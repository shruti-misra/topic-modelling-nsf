"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"1159236","IRFP: Metalanguage Identification for Interactive Language Technologies","OISE","IRFP","07/01/2013","07/25/2012","Shomir Wilson","MD","Wilson                  Shomir","Fellowship","Cassandra M. Dudka","02/28/2015","$98,100.00","","","","College Park","MD","207403155","","O/D","5956","5946, 5956","$0.00","The International Research Fellowship Program enables U.S. scientists and engineers to conduct nine to twenty-four months of research abroad. The program's awards provide opportunities for joint research, and the use of unique or complementary facilities, expertise, and experimental conditions abroad.  <br/><br/>This award will support a twenty-month research fellowship by Dr. Shomir Wilson to work with Prof. Jon Oberlander at the University of Edinburgh in Scotland and Prof. Alan Black at Carnegie Mellon University in Pittsburgh, Pennsylvania.  <br/><br/>Metalanguage is a crucial linguistic mechanism which allows us to communicate information about language itself. It is essential for many language activities, including attributing statements, explaining meaning, assigning names, clarifying assumptions, and correcting misunderstandings. The ability to understand metalanguage is a skill that humans employ frequently in conversation and assume in fellow interlocutors, regardless of context or topic. However, in spite of its centrality to linguistic communication, metalanguage has received little attention in the development of interactive language technologies, leaving such technologies unable to cope with or exploit occurrences of the phenomenon in human-computer dialog.  <br/><br/>This research is the first effort to study the use of computational methods to identify occurrences of metalanguage in informal English contexts. This project consists of three stages. The first is the creation of corpora of instances of metalanguage in informal contexts, particularly from spoken conversations and informal blog texts. The second is the detection and delineation of the phenomenon, using features of metalanguage that are peculiar to the phenomenon. Finally, the third stage is the practical evaluation of metalanguage detection and processing within the context of a spoken dialog system deployed for use by the general public.  <br/><br/>The project will fill a long-standing and consequential research gap in artificial intelligence and computational linguistics. The practical applications of spoken dialog systems - such as voice menus, car GPS voice interfaces, and accessibility software for the blind - are expanding rapidly, as improvements are made to speech recognition, natural language processing, and other technologies that support conversational interaction with computers. Studying metalanguage will remedy a missing element in the design these of dialog systems, bringing them further in-line with users' expectations and thus making them more natural to use. Moreover, this research will lead to new international collaborations between the principal investigator and the two hosts' research groups, the Institute for Language, Cognition, and Computation (ILCC) at the University of Edinburgh and the Language Technologies Institute (LTI) at Carnegie Mellon University."
"1338054","CAREER: Computation and Approximation in Structured Learning","IIS","Robust Intelligence","01/01/2013","06/24/2014","Ali Farhadi","WA","University of Washington","Standard Grant","Weng-keen Wong","05/31/2017","$419,879.00","Emily Fox","afarhad2@gmail.com","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7495","1045, 1187, 7495","$0.00","Machine learning is transforming the way many fields make sense of data, from engineering and science to medicine and business. Machine learning has vastly improved speech recognition, machine translation, robotic navigation and many other prediction tasks. A crucial goal of machine learning is automating intelligent processing of information: this project will focus on automatically describing videos by detecting objects, people, actions and interactions between them, and parsing documents by extracting entities, events and relationships between them.  All these prediction tasks require more than just true-false or multiple-choice answers, but have an exponential number of possible answers to consider. Breaking these joint predictions up into independent decisions (for example, translating each word on its own, recognizing a phoneme at a time, detecting each object separately) ignores critical correlations and leads to poor accuracy.<br/><br/>Structured models, such as grammars and graphical models, can capture strong dependencies but at considerable computational costs. The barrier to improving accuracy in such structured prediction problems is the prohibitive cost of inference.  Structured prediction problems present a fundamental trade-off between approximation error and inference error due to computational constraints as we consider models of increasing complexity. This trade-off is poorly understood but is constantly encountered in machine learning applications.<br/><br/>The primary outcome of this project will be a framework for addressing very large scale structured prediction using a novel coarse-to-fine architecture. This architecture will enable explicit, data-driven control of the approximation/computation trade-off.  It promises to drastically advance state-of-the-art accuracy in computer vision and natural language applications and greatly enhance search and organization of documents, images, and video. The PI's plan includes an active role in the machine learning community, disseminating results through tutorials, code and data and organizing workshops."
"1321053","RI: Small: Deep Learning: Theory, Algorithms, and Applications","IIS","Robust Intelligence","08/15/2013","06/16/2014","Pierre Baldi","CA","University of California-Irvine","Continuing grant","Weng-keen Wong","07/31/2016","$457,999.00","","pfbaldi@ics.uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","CSE","7495","7495, 7923, 9251","$0.00","The ability to learn is essential to the survival and robustness of biological systems. There is also growing evidence that learning is essential to build robust artificial intelligent systems and solve complex problems in many application domains.  However, solutions to complex problems ranging from recognizing faces to understanding speech, cannot be implemented in a single step.  Instead they require multiple processing stages, for instance to extract increasingly more abstract and refined features from an input image.  <br/><br/>Thus computers and brains are both faced with the problem of deep learning --- how to simultaneously optimize the parameters of a hierarchy of processing stages in order to solve complex tasks and display intelligent behavior. In the past few years there has been remarkable progress in computer science to address the deep learning problem, and deep learning methods now claim state-of-the-art performance in several application areas.  The next generation of machine learning methods holds the promise to not only approach human performance in tasks previously impossible for computers, but also to exceed it.  However, our theoretical understanding of deep learning remains limited and there are several important areas where deep learning has not yet been applied systematically. This project addresses these challenges and opportunities by furthering formal understanding of the theory and algorithms behind deep learning, and by applying deep learning methods to new problems in the life sciences.  Because deep learning can be used in almost any domain, the results have the potential for broadly impacting science, engineering, and technology across multiple areas.  <br/><br/>The project has educational and outreach components, ranging from courses to virtual 3D world interactions, for undergraduate and graduate students at UCI, as well as talented students from local high schools, and underrepresented minority students from local colleges. Scientific results, data, and software resulting from the project will be disseminated in scientific journals and over the web.<br/><br/>The project has three main thrusts: theory, algorithms, and applications. From a theoretical standpoint, the project develops better mathematical understanding of deep architectures, including stacks of autoencoder networks, and their properties.  These theoretical results will inform the design of deep-learning architectures.  From an algorithmic standpoint, the project investigates, formally and through simulations, several deep learning algorithms, including the dropout algorithm and the PI's deep targets algorithm.  From an application standpoint, the project uses the new theoretical and algorithmic knowledge in application to the life sciences, for instance for protein structure prediction, and in predicting chemical reactions.  Advancing the state-of-the-art for any one of these thrusts will have a significant impact in computer science, artificial intelligence, statistical machine learning, and the corresponding application field."
"1247360","SBIR Phase I:  Data-Driven Guiding Technology: Transforming Training and Therapy","IIP","SBIR Phase I","01/01/2013","11/08/2012","John Nosek","PA","SenseMaking Technologies Corporation","Standard Grant","Glenn H. Larsen","06/30/2013","$150,000.00","","johnnosek@verizon.net","721 Dresher Road Suite 2400","Horsham","PA","190442218","6096059273","ENG","5371","5371, 8031, 8032","$0.00","The innovation is an agent-based, data-driven guiding technology that will augment the human facility to evaluate task performance by overcoming human limitations to recall complex processes that require high expertise. The cloud-based, mobile technology will assist teams of varying expertise and capabilities to more effectively instruct, coordinate, and assess learning progress of students, especially those with developmental delays. The technology will provide natural, hands-free support for the instructor who guides the student in each attempt within a complex program of study. The technology will 1) select the task, 2) provide pre-evaluation guidance on setup, prompts, target response and instructions, 3) using speech recognition, record task performance evaluations, 4) provide post-evaluation guidance on what reinforcement to provide, if any, and 5) select the next task. The enabling of automatic capture of rich, high quality data for each task attempt will facilitate the use of advanced artificial intelligence techniques, such as associative data mining, to provide more customized programs of instruction. This project will overcome instructor expertise and process complexity barriers and eliminate data collection burdens while increasing data and program fidelity that will result in better, faster learning. <br/><br/>The broader/commercial impact of this innovation will occur in both Applied Behavior Analysis (ABA) and non-ABA areas. Initially it will help tackle the crisis in providing cost effective, quality therapy to individuals with autism. Autism affects more than 1% of the world?s population. ABA is the gold standard in treating autism. Up to fifty percent of children with autism who undergo early treatment improve their IQs and developmentally progress so they can mainstream into standard classrooms with no educational assistants. Without early intervention, there is almost no chance. ABA is most effective when the team of certified ABA analysts, non-certified ABA therapists, educational assistants and family members coordinate instruction. However, expertise and process complexity barriers experienced by instructors, coupled with the burdens of data collection, limit its effectiveness, while the shortfall of trained therapists and its high costs (60K/child/year) limit its availability. The technology will increase available ABA therapy by 1) accelerating training, 2) enabling team members to instruct more easily and effectively with greater data and program fidelity and 3) lowering costs. This innovation will accelerate the availability of transformational, affordable technology that will improve the lives of many and ameliorate a major world health problem."
"1321015","RI: Small: Multi-View Learning of Acoustic Features for Speech Recognition Using Articulatory Measurements","IIS","ROBUST INTELLIGENCE","09/01/2013","07/01/2015","Karen Livescu","IL","Toyota Technological Institute at Chicago","Continuing grant","Tatiana Korelsky","08/31/2017","$444,859.00","","klivescu@ttic.edu","6045 S Kenwood Ave","Chicago","IL","606372803","7738340409","CSE","7495","7495, 7923","$0.00","This project explores techniques for learning acoustic features for speech recognition, based on multi-view learning using acoustic and articulatory recordings.  Recent work has shown recognition improvements using this strategy via linear and nonlinear canonical correlation analysis, in which transformations of acoustic features are learned so as to maximize correlation with (transformations of) articulatory measurements.  Prior work has been limited to a single database and a single language.<br/><br/>The main goals of this project are to learn better universal features for arbitrary speakers and languages and to develop improved multi-view techniques.  Project activities include: learning time-varying projections; multi-view techniques based on neural networks; ""many-view"" learning using articulation, video, labels, etc.; efficient implementations; new input features such as spectro-temporal filters; and visualization tools for related research and education.<br/><br/>A critical component of automatic speech recognition is a representation of the audio signal that encapsulates useful information while discarding acoustic noise, speaker identity, and so on.  This project aims to automatically learn improved representations using statistical analysis of audio recordings paired with positions of the speech articulators (lips, tongue, etc.) and other measurements.  The project starts with basic statistical techniques, and develops new techniques that address challenges and opportunities specific to speech and related signals.<br/><br/>The project's impact extends beyond speech processing.  Applications of multi-view representation learning include neurology, meteorology, chemometrics, computer vision, and text processing; all of these can benefit from the improved techniques.  The work impacts education by generating materials for a Speech Technologies course and visualization tools for speech and other signals."
"1159008","IRFP: Deep Neural Networks for Perception and Action Integration in Robotic Control","OISE","IRFP-Inter Res Fellowship Prog","06/01/2013","05/27/2015","Alan Lockett","TX","Lockett                 Alan           J","Fellowship Award","Cassandra Dudka","07/31/2016","$183,808.00","","","","Austin","TX","787490000","","O/D","5956","5950, 5956","$0.00","The International Research Fellowship Program enables U.S. scientists and engineers to conduct nine to twenty-four months of research abroad. The program's awards provide opportunities for joint research, and the use of unique or complementary facilities, expertise and experimental conditions abroad. <br/><br/>This award will support a twenty-four-month research fellowship by Dr. Alan Lockett to work with Professor Juergen Schmidhuber at the Instituto dalle Molle di Studi sull'Intelligenza Artificiale (IDSIA) in Lugano, Switzerland. <br/><br/>This project explores methods for training deep neural networks to control a physical robot with humanoid hands and arms. The recent development of new methods for training deep artificial neural networks has resulted in breakthroughs on a number of benchmarks in artificial intelligence. Deep neural networks currently hold the record for benchmark predictive tasks including handwriting recognition (MNIST) and object recognition (NORB, CIFAR-10). <br/><br/>This project is developing methods for training deep neural network controllers with thousands or millions of parameters using an array of massively multiprocessor GPUs with over 4,096 processors. Deep network controllers in this research are trained using neuroevolution, reinforcement learning, and combinations of the two. Such controllers are being used to train a humanoid iCub robot to manipulate objects for the AAAI Small-Scale Manipulation Challenge.<br/><br/>The research is being performed at the Instituto dalle Molle di Studi sull'Intelligenza Artificiale (IDSIA) in Lugano, Switzerland in conjunction with Professor Juergen Schmidhuber. IDSIA is a leading research institution in the study of deep neural networks, artificial evolution, reinforcement learning, and robotic control. <br/><br/>This research seeks to advance our understanding of robotic control in general. The focus on deep neural networks that integrate hierarchical perception and action modules has the potential to result in breakthroughs in control of complex robotic systems that would enable the deployment of computer and robotic systems that operate with greater autonomy than is currently possible.<br/><br/>The deployment of robotic technologies over the course of the next century is likely to mirror the rapid introduction of computer technology in the past century.  Behind the success of these robots will be deep hierarchical control systems that integrate perception and action at a high level of abstraction, as studied in this research. This research examines technologies that hold the potential to transform and improve our lives in innumerable ways. In the future, self-driving cars will co-ordinate with each other and with an active roadway to minimize accidents and improve efficiency. Advanced autopilot technology will finally make personal flying vehicles a reality. Autonomous robotic miners will reduce risk to humans while improving access to raw materials and resources. Robotic surgeons will perform complex operations with new levels of precision. Each of these technologies depends critically on the availability of deep integration of perceptual analysis and hierarchical controllers. The use of deep neural networks like the ones studied in the proposed research constitutes a promising approach to bringing these new technologies out of the lab and into our daily lives."
"1339552","I-Corps:  Commercializing the Integration of Human and Artificial Intelligence for Large Scale Multimedia Analysis","IIP","I-Corps","05/01/2013","04/17/2013","Gerald Friedland","CA","International Computer Science Institute","Standard Grant","Rathindra DasGupta","10/31/2013","$50,000.00","","fractor@icsi.berkeley.edu","1947 CENTER ST STE 600","Berkeley","CA","947044115","5106662900","ENG","8023","","$0.00","Currently, crowdsourcing platforms are only being used to perform tasks that are easy for humans. Researchers have developed methods for using these systems to do tasks that are difficult for humans. They have developed methods to accomplish difficult tasks and with these techniques can produce solutions that are more accurate and efficient than any currently present in the marketplace. This method is a hybrid approach that combines artificial intelligence systems with low-cost crowdsourced labor enabling a flexible approach to the end user that enables them to analyze a wide variety of events with high accuracy while still achieving the time and cost savings associated with artificial intelligence-based systems.<br/><br/>Multimedia content is a major part of people's ever day lives, and the ability to understand the data that is being acquired at a rapid pace will have a huge impact on society. Researchers aim to aid this by developing new techniques for the processing of large scale multimedia databases, creating methods for using existing crowdsourcing tools in a significantly more efficient and accurate way to produce high quality results, based on a n hybrid system of machine learning and human intelligence. This hybrid system could have an great public impact, leveraging the advantages of machine learning and human annotation simultaneously (having the machine learn from human crowdsourcers) providing a high accuracy solution at a lower cost than is currently possible."
"1346564","CIF: Medium: Collaborative Research: Information Theory and Statistical Inference from Large-Alphabet Data","CCF","Comm & Information Foundations","01/01/2013","09/12/2013","Mokshay Madiman","DE","University of Delaware","Standard Grant","richard brown","10/31/2016","$351,672.00","","madiman@udel.edu","210 Hullihen Hall","Newark","DE","197160099","3028312136","CSE","7797","7924, 7935","$0.00","Statistical analysis is key to many challenging applications such as text classification, speech recognition, and DNA analysis. However, often the amount of data available is comparable or even smaller than the set of symbols (alphabet) constituting the data. Unfortunately, not much is known about optimal inference in this so-called large-alphabet domain. Recently, several promising approaches have been developed by different scientific communities, including Bayesian nonparametrics in statistics and machine learning, universal compression in information theory, and the theory of graph limits in mathematics and computer science.<br/><br/>The investigators study the problem drawing from these multiple perspectives, but with a particular focus on developing the information theoretic approach. The research studies analytical properties of the ""pattern maximum likelihood'' estimator, which performs well in practice but is not understood theoretically, and also explores computational speedups. Moreover, it attempts to delineate which problem classes are better handled by Bayesian nonparametric techniques and which by the pattern approach, and explores links between these approaches. The investigators use the resulting theory for automatic document classification, allowing for more automation in storing, retrieving, and analyzing data. Furthermore, the investigators use the theory to study genetic variations, whose link with disease diagnosis is a crucial step in the systematic quantification of biology that is playing an increasingly important role in medical advancement. The research also brings new courses to the classroom, with a special outreach effort to involve women and under-represented minorities, including through the Native Hawaiian Science and Engineering Mentorship Program."
"1318427","Computation of crowded geodesics on the universal Teichmueller space for planar shape matching in computer vision","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","08/01/2013","05/30/2014","Akil Narayan","MA","University of Massachusetts, Dartmouth","Continuing Grant","Leland Jameson","10/31/2015","$198,300.00","Alfa Heryudono","akil@sci.utah.edu","285 Old Westport Road","North Dartmouth","MA","027472300","5089998953","MPS","1266, 1271","9263","$0.00","Quantifying the (dis)similarity between two shapes is a central problem in computer vision. One distance metric on the space of planar shapes is realized by identifying this space as a subset of the Universal Teichmueller Space, and equipping it with the Weil-Petersson metric. This results in a metric that is scale- and translation-invariant on shapes, and has unique geodesic flow between two shape endpoints. The work of this proposal develops robust computational methods for the computation of metric distances and geodesics between shapes on this space. The major difficulty lies in computations involving ""crowded"" shapes, i.e., those with elongated, winding, or extended protrusions. Such shapes stymie finite-precision computations because direct algorithms suffer from severe roundoff error. The major thrusts of this proposal develop algorithmic methodologies to address roundoff error and related issues: The Zipper conformal mapping algorithm will be augmented to produce accurate conformal maps for crowded shapes. The velocity field representation on a geodesic will be rewritten into a form that is resistant to roundoff error. The geodesic equation will be transformed into a expression that takes advantage of the aforementioned velocity field transformation, and can effectively flow between crowded shapes. The final phase of this project will demonstrate accurate geodesic flow and distance computations between crowded shapes. The methods developed under this project can be applied to several related problems in scientific computing: solutions to differential equations on irregular geometries through conformal mapping, conservative integration methods with ill-conditioned particle systems, and moving-mesh kernel approximations.<br/><br/>The work of this project can contribute to far-reaching applications in scientific and computer vision problems: automated object recognition (e.g. projectile identification), outline classification (determination of an animal's species), medical imaging (usage of MRI to diagnose dementia and related diseases), and artificial intelligence (visual recognition and interpretation) to name a few. All computational deliverables (computer code, example simulations, documentation) will be made publicly available. Through the engagement of students in related research tasks, this project will contribute to the educational development of future engineers, mathematicians, and computer scientists."
"1242748","AAAI Fall Symposium: AI for Gerontechnology Student Travel Support","IIS","Smart and Connected Health","02/01/2013","09/12/2013","Narayanan Chatapuram Krishnan","WA","Washington State University","Standard Grant","Sylvia Spengler","01/31/2014","$15,000.00","Diane Cook, Parisa Rashidi","ckn@eecs.wsu.edu","280 Lighty","PULLMAN","WA","991641060","5093359661","CSE","8018","7556, 8018","$0.00","This  project is providing  support for graduate students to attend a workshop organized in conjunction with the fall 2012 annual symposium of the Association for Advancement in Artificial Intelligence. The workshop entitled Artificial Intelligence for Gerontechnology was held November 2nd to November 4th, 2012, in Arlington, Virginia.  Gerontechnology  is an interdisciplinary academic and professional field combining the study of aspects of aging and technology to address the challenges arising from the demographic changes due to aging society.   Artificial intelligence techniques are considered to be among the key components of the solutions within the gerontechnology domain supporting care for elders. Intellectual Merit: The symposium advances the state of the art in artificial intelligence by considering problems and solutions that are inspired by the challenges in developing gerontechnology. Specifically, the topics addressed include the following: 1) novel approaches for dealing with sparsely annotated data, 2) innovative machine learning techniques that can improve the generalization of technological solutions across a wide range of real-world settings and 3) design of modeling algorithms that take advantage of domain knowledge. Broader Impacts: While the goal of the symposium was to advance the field of AI with the focus on elder care applications, the solutions resulting from the symposium can also be applied to other health care areas such as assisting chronic patients in hospitals and homes. In addition, the educational aspect of this workshop provides the opportunity to educate future workforce with expertise at the intersection of computer science, artificial intelligence and care for the elders."
"1344017","Machine Learning Summer School Pittsburgh 2014","IIS","Info Integration & Informatics, Robust Intelligence","09/01/2013","08/28/2013","Alexander Smola","PA","Carnegie-Mellon University","Standard Grant","Sylvia Spengler","08/31/2014","$40,000.00","Zico Kolter","smola@cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7364, 7495","7364, 7495, 7556","$0.00","Machine learning has many important applications in science and industry. Modern machine learning uses a mix of insights from different disciplines, most notably artificial intelligence, statistics and optimization - areas that traditionally have not had much overlap. The participants will take part in tutorials given by experts from several different areas of machine learning - an opportunity that many students do not have at their home institutions. <br/><br/>The project supports student participation in a Machine Learning Summer School to be held at Carnegie Mellon University in Pittsburgh during June 16-27, 2014. The summer school emphasizes big data and scalable machine learning algorithms. It features speakers from academia and industry with established experience in large scale data analysis. Approximately 50 graduate students from around the U.S. are expected to participate in person. The content will be streamed live as well as archived online making it possible for a much larger number of students from academia and industry to benefit from the summer school. In addition to in-depth tutorial lectures given by leading researchers, the summer school will include exercise sessions that provide the participants hands-on experience with large scale data (using the Kaggle platform and Amazon cloud services). <br/><br/>Broader Impact: The Summer School provides state-of-the art knowledge of machine learning and big data analytics to graduate students -  an opportunity that many students do not have at their home institutions. Thus, it would not only help train an new generation of machine learning and big data analytics researchers, but also reduce the barrier to entry of researchers who want to apply state-of-the-art machine learning techniques to applications in areas such as social network analytics, bioinformatics etc."
"1231620","SHB: Type I (EXP): Algorithms for Unsupervised and Online Learning of Hierarchy of Features for Tuning Cochlear Implants for the Hearing Impaired","IIS","Smart and Connected Health","01/01/2013","08/31/2012","Bonny Banerjee","TN","University of Memphis","Standard Grant","Sylvia Spengler","12/31/2016","$298,203.00","Lisa Mendel","bbnerjee@memphis.edu","Administration 315","Memphis","TN","381523370","9016783251","CSE","8018","8018, 8061, 9150","$0.00","Since noteworthy events happen only occasionally in any data, it is imperative for smart sensors to learn the norms in data so that authorities can be alerted and appropriate action can be taken at the occurrence of an abnormal or noteworthy event. The aim of this project is to develop algorithms that can learn the norm in terms of a hierarchy of meaningful features from data in an unsupervised and online manner. The application testbed is the problem of automatically tuning cochlear implants (CIs) of patients with severe-to-profound hearing loss by continuously monitoring their speech output. The working hypothesis is that deficiencies in hearing for people with significant hearing loss are reflected in their speech production. This project will develop and use unsupervised, online, and biologically plausible machine learning algorithms to learn feature hierarchies from the speech output data of severely-to-profoundly hearing-impaired patients. The learned feature hierarchy from the speech of a patient will be compared to those learned from the speech of a comparable normal hearing population. Deficiencies in the patient's hearing will be ascertained by identifying the missing or distorted features. Algorithms will be developed to map this information into the signal processing strategies used in CIs to enhance the audibility of speech.<br/><br/>The proposed project promises transformative changes to three major interdisciplinary fields: machine learning and artificial intelligence, healthcare, and sensors. It will transform the traditional ways in which the clinical needs of patients are met. For example, the results of this project will provide doctors with evidence-based practices that will better address the specific needs of individual patients by monitoring each patient around the clock at minimal effort and cost.<br/><br/>Hearing loss is the most common birth defect in the U.S. with slightly over 15,000 new pediatric cases each year and societal losses amounting to $4.6 billion over a lifetime. A proven technology for CI tuning would make a significant difference to the lives of over 1.2 million CI candidates in the U.S. and many more around the world, thereby leading to substantial health and economic benefits to society. Other than CI tuning, the proposed algorithms will be applicable to a variety of monitoring applications within healthcare, such as blood pressure, cerebrospinal fluid pressure, intracavitary pressure of the bladder, etc., and beyond healthcare, such as web, machine health, traffic, etc. Continuous monitoring with wearable and implantable body sensors will increase early detection of emergency conditions and diseases in at-risk patients and also provide a wide range of healthcare services for people with various degrees of cognitive and physical disabilities. Not only the elderly and chronically ill, but also the families in which both parents have to work will benefit from these systems to provide high-quality care services for their babies and children. Finally, the proposed project will integrate diversity by promoting teaching, learning, and interdisciplinary research among underrepresented groups."
"1306133","Collaborative Research:Transfer Learning for Chemical Analyses from Laser-Induced Breakdown Spectroscopy","CHE","Chemical Measurement & Imaging","09/15/2013","09/08/2013","Melinda Dyar","MA","Mount Holyoke College","Standard Grant","Kelsey D. Cook","08/31/2016","$141,129.00","","mdyar@mtholyoke.edu","50 College Street","South Hadley","MA","010756456","4135382000","MPS","6880","7433, 8084","$0.00","With support from the Chemical Measurements and Imaging program, Professors Melinda Dyar of Mt. Holyoke College and Sridhar Mahadevan of University of Massachusetts at Amherst and their students will use laser-induced breakdown spectroscopy (LIBS) measurements, including laboratory investigations of standard materials at varying experimental conditions, to develop numerical methods that will address limitations to the broad application of LIBS imposed by matrix effects and plasma variability. State-of-the-art dimensionality reduction and transfer learning methods from machine learning and statistics will be used to build innovative LIBS-based predictive models. These investigations will extend classical methods in statistics for dealing with multiple paired data sets, such as canonical correlational analysis, to deal with unlabeled data, and extract nonlinear low-dimensional regularities in the data. The project includes the design of a suite of model-building tools that can deal with a range of problems and optimization objectives, including different types of correspondence information available across datasets, diversity of global objectives ranging from preserving local to global geometry, and producing linear or nonlinear mappings to lower-dimensional factors. <br/><br/>Laser-induced breakdown spectroscopy (LIBS) is a chemical analysis tool that uses the light emitted by a sample when a focused laser pulse generates a plasma at the sample surface. LIBS has a number of features that make it particularly useful for field use, including rapid analysis, minimal sample preparation and suitability for stand-off, that is remote, detection. Moreover, LIBS can detect and quantify light elements that are not always measured using other methods. Consequently, LIBS is well-suited to many applications including, defense interests (e.g., military explosive detection, illegal drug detection, airport security), in-situ analysis of archeological sites, field work at hazardous waste sites, and geological resource exploration. However, utilization of LIBS measurements is limited by signal variability with measurement and sample conditions. This project launches an integrated research program to couple state of the art LIBS instrumentation at Mount Holyoke College to equally state of the art numerical methodology in artificial intelligence and machine learning at the nearby University of Massachusetts to increase the utility of LIBS measurements. This project will provide an interdisciplinary training environment that includes undergraduate, graduate and post-doctoral researchers."
"1307179","Collaborative Research: Transfer Learning for Chemical Analyses from Laser-Induced Spectroscopy","CHE","Chemical Measurement & Imaging","09/15/2013","09/08/2013","Sridhar Mahadevan","MA","University of Massachusetts Amherst","Standard Grant","Kelsey D. Cook","08/31/2016","$158,872.00","","mahadeva@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","MPS","6880","7433, 8084","$0.00","With support from the Chemical Measurements and Imaging program, Professors Melinda Dyar of Mt. Holyoke College and Sridhar Mahadevan of University of Massachusetts at Amherst and their students will use laser-induced breakdown spectroscopy (LIBS) measurements, including laboratory investigations of standard materials at varying experimental conditions, to develop numerical methods that will address limitations to the broad application of LIBS imposed by matrix effects and plasma variability. State-of-the-art dimensionality reduction and transfer learning methods from machine learning and statistics will be used to build innovative LIBS-based predictive models. These investigations will extend classical methods in statistics for dealing with multiple paired data sets, such as canonical correlational analysis, to deal with unlabeled data, and extract nonlinear low-dimensional regularities in the data. The project includes the design of a suite of model-building tools that can deal with a range of problems and optimization objectives, including different types of correspondence information available across datasets, diversity of global objectives ranging from preserving local to global geometry, and producing linear or nonlinear mappings to lower-dimensional factors. <br/><br/>Laser-induced breakdown spectroscopy (LIBS) is a chemical analysis tool that uses the light emitted by a sample when a focused laser pulse generates a plasma at the sample surface. LIBS has a number of features that make it particularly useful for field use, including rapid analysis, minimal sample preparation and suitability for stand-off, that is remote, detection. Moreover, LIBS can detect and quantify light elements that are not always measured using other methods. Consequently, LIBS is well-suited to many applications including, defense interests (e.g., military explosive detection, illegal drug detection, airport security), in-situ analysis of archeological sites, field work at hazardous waste sites, and geological resource exploration. However, utilization of LIBS measurements is limited by signal variability with measurement and sample conditions. This project launches an integrated research program to couple state of the art LIBS instrumentation at Mount Holyoke College to equally state of the art numerical methodology in artificial intelligence and machine learning at the nearby University of Massachusetts to increase the utility of LIBS measurements. This project will provide an interdisciplinary training environment that includes undergraduate, graduate and post-doctoral researchers."
"1319680","SHF: Small: RUI: Generating High Quality Trace Links through Intelligent Composition of Tracing Features","CCF","Software & Hardware Foundation, SOFTWARE ENG & FORMAL METHODS","08/01/2013","05/29/2015","Jane Huang","IL","DePaul University","Standard Grant","Sol Greenspan","07/31/2017","$529,383.00","","JaneClelandHuang@nd.edu","1 East Jackson Boulevard","Chicago","IL","606042287","3123627595","CSE","7798, 7944","7923, 7944, 9229, 9251","$0.00","Software traceability serves a critical role in ensuring that software systems operate correctly. It is used to support a wide variety of software engineering activities such as change management, compliance verification, and safety analysis. Unfortunately current practices fall far short of delivering cost-effective traceability, primarily because creating and managing trace links in large and/or complex systems is time-consuming, arduous, and error-prone. These problems were highlighted in a recent report entitled 'Critical Code: Software Producibility for Defense' commissioned by the Department of Defense. The report stressed the need for the research community to develop cost-effective and accurate traceability solutions.  While state of the art tracing techniques offer significant promise for reducing the cost and effort of tracing, they fall short of meeting industrial needs primarily because the quality of the generated links is imprecise. This work will investigate ways to integrate techniques from feature modeling, product line development, artificial intelligence and machine learning to deliver a dynamically configurable trace infrastructure.  The framework will then be used to investigate and integrate a broad set of novel tracing techniques which are expected to significantly improve the quality of generated trace links.<br/><br/>The results of the project will contribute towards the development of software intensive systems, especially safety-critical ones in which traceability is mandatory. Technology transfer will be facilitated by delivering solutions on the TraceLab platform, disseminated via the Center of Excellence for Software Traceability (CoEST.org) and through training materials targeted at industrial users. Ongoing research opportunities will be provided for a diverse group of undergraduate and graduate students, and pedagogical materials will be developed and made publicly available for use in a variety of courses on requirements engineering, software engineering and software architecture."
"1417991","CI-P:Collaborative Research: Visual entailment data set and challenge for the language and vision communities","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","08/01/2013","05/12/2014","Tamara Berg","NC","University of North Carolina at Chapel Hill","Standard Grant","Jie Yang","06/30/2015","$50,815.00","","tlberg@cs.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","7359","7359","$0.00","Vision and language provide fundamental means to interpret, learn, and communicate about the world around us. A primary goal of computer vision and natural language processing research is therefore to automatically uncover and analyze the information that images and video, or text and speech, convey about the world.  Both communities are concerned with tasks that require increasingly deeper understanding, including the ability to reason with and draw inferences from this information.  Since vision and language are complementary modalities, there is now also an increasing amount of work at the interface of both fields. However, progress in multimodal analysis requires a tighter collaboration between the two communities, since each currently relies on its own set of techniques, datasets and evaluation criteria. <br/><br/>This community planning grant explores the need for, feasibility, and usefulness of a ""visual entailment"" corpus and associated visual entailment recognition task. In natural language, entailment recognition is the problem of determining whether a particular statement can be inferred from a text document. This project explores a novel related problem - visual entailment - where the goal is to determine whether a statement in natural language can be inferred from an image or video.  The outcomes of the project include a novel dataset and prototype research challenge, as well as increased collaboration between the vision and language communities."
"1308462","Modeling Neural Activity: Statistics, Dynamical Systems and Networks","DMS","STATISTICS, MATHEMATICAL BIOLOGY","04/01/2013","03/25/2013","Robert Kass","PA","Carnegie-Mellon University","Standard Grant","Xiaoming Huo","03/31/2014","$12,000.00","","kass@stat.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","MPS","1269, 7334","7556","$0.00","The workshop ""Modeling Neural Activity: Statisics, Dynamical Systems, and Networks (MONA)"" will be held June 26-28, 2013, in Lihue, Hawaii. Computational neuroscience has grown, in distinct directions, from the success of biophysical models neural activity, the attractiveness of the brain-as-computer metaphor, and the increasing prominence of statistical and machine learning methods throughout science. This has helped create a rich set of ideas and tools associated with ``computation'' to studying the nervous system, but it has also led to a kind of balkanization of expertise. There is, especially, very little overlap between mathematical and statistical research in this area.  Important breakthroughs in computational neuroscience could come from research strategies that are able to combine what are currently largely distinct approaches.  A workshop ""Modeling Neural activity: Statisics, Dynamical Systems, and Networks (MONA)"" will be held June 26-28, 2013, in Lihue, Hawaii, with the purpose of exploring fruitful interactions of modeling ideas that come from mathematics, statistics, and biophysics.  An additional purpose of the workshop is to bring together U.S. and Japanese researchers in this area. While computational neuroscience is represented strongly in both the U.S. and Japan there has been too little concrete communication and interaction between research groups across our two countries.  Interaction across American and Japanese researchers should facilitate the advance of cross-disciplinary work.<br/><br/>Many disorders, such as ADHD, autism, and schizophrenia, as well as stroke and various neurodegenerative diseases, are thought to involve dysfunction of neural networks. Because computational neuroscience aims to supply principles for understanding the activity of individual and collective neural firing patterns, its successes can help in formulating mechanistic descriptions of pathophysiology. The workshop ``Modeling Neural Activity: Statisics, Dynamical Systems, and Networks (MONA)"" is at the interface between mathematics, statistics, and neural network analysis, and has as its goal to generate new and productive lines of research."
"1316223","RCN Proposal: Macroecology of Infectious Disease","DEB","Ecology of Infectious Diseases","09/01/2013","06/01/2020","Patrick Stephens","GA","University of Georgia Research Foundation Inc","Continuing Grant","Samuel Scheiner","10/31/2020","$499,451.00","Sonia Altizer, Katherine Smith, Alonso Aguirre, Robert Poulin","prsteph@uga.edu","310 East Campus Rd","ATHENS","GA","306021589","7065425939","BIO","7242","7242, EGCH","$0.00","Scientists from multiple disciplines, including wildlife biology, public health, veterinary parasitology and biomedical sciences, have compiled a wealth of data on the distribution and impacts of infectious diseases, with most studies focused on particular locations or single host-pathogen interactions. This wealth of data creates the opportunity to address a pressing need, namely to explore the patterns and drivers of infectious disease emergence in humans and natural ecosystems at global scales. This project will create a Research Coordination Network to bring together internationally recognized experts from ecology, conservation medicine, parasitology and computational sciences to quantify and explore the drivers of global scale patterns of pathogen biodiversity. Participants will work together to assemble data sets of unprecedented size, including information about disease occurrence in host species ranging from insects to humans. With help from experts in machine learning (artificial intelligence) and geographic information systems (GIS), they will work to build predictive models that can be used to understand the changing distributions of infectious diseases and identify future hotspots of novel disease emergence in humans and wildlife.<br/><br/>Emerging infectious diseases, especially those that jump from wildlife to livestock and humans, threaten public health around the world. Using state of the art computational methods, the RCN will be able to answer critical questions such as: How and why do certain pathogens successfully move from one host species to another? Are hotspots of pathogen biodiversity in wildlife the same areas as hotspots of disease emergence in humans and livestock? To share its findings, the RCN will develop educational products such as webcasts and workshops aimed at educational levels from high school to post graduate, and will train students from under-represented groups through a summer research program. The RCN will also develop and maintain databases of global infectious disease biodiversity and make these data freely available to the academic community and the general public."
"1318374","AF: Small: Identifying sampling problems with efficient algorithms","CCF","ALGORITHMS","09/01/2013","06/21/2013","Daniel Stefankovic","NY","University of Rochester","Standard Grant","Tracy Kimbrel","08/31/2018","$399,703.00","","stefanko@cs.rochester.edu","518 HYLAN, RC BOX 270140","Rochester","NY","146270140","5852754031","CSE","7926","7923, 7926","$0.00","Sampling problems are fundamental in many areas of science and engineering, for example, statistics, artificial intelligence (machine learning and vision), and biology (phylogeny). Efficiency of sampling has impact on other important algorithms, for example, the speed of statistical tests and the reliability of estimators and classifiers. This project seeks to characterize combinatorial problems that admit efficient sampling algorithms. The research under this award addresses both sides of the characterization for a broad class of ubiquitous graphical models: 1) complexity theoretic obstacles to efficient sampling and 2) efficient sampling algorithms.<br/><br/>Recent results established a useful characterization for antiferromagnetic 2-spin models (for example, weighted independent sets and Ising model) in terms of the behavior of the model on regular trees. This created a connection to problems and techniques studied in the statistical physics community. This project aims to use the connection to the statistical physics and its techniques (for example, belief propagation recurrences) to generalize the characterization to models with more than two spins (multi-spin models, such as the Potts model, frequently occur in the applications). As a first step towards this generalization a relation between the efficiency of commonly used Markov chains (Glauber dynamics, Swendsen-Wang algorithm) and the behavior of the models on regular trees will be explored. Beside Markov chains the PI will also investigate other approaches to sampling problems (for example, dynamic programming and utilizing strong spatial mixing).<br/><br/>The product of the research will be efficient sampling algorithms and an improved understanding of obstacles to efficient sampling. The algorithmic  problems, concepts, and techniques will be transferred to both undergraduate and graduate teaching (in the form of guided problems and implementation challenges). The award will support the training of two PhD students in the area of sampling and counting algorithms at the University of Rochester."
"1318971","RI: Small: Hierarchical Feature Learning by Heterogeneous Networks with Application to Face Verification","IIS","ROBUST INTELLIGENCE","09/01/2013","06/12/2014","Thomas Huang","IL","University of Illinois at Urbana-Champaign","Continuing grant","Jie Yang","08/31/2016","$408,612.00","","huang@ifp.uiuc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7495","7495, 7923","$0.00","Learning good features is a key to computer vision problems such as recognizing human faces, and understanding scenes. Many computer vision researchers learn features by providing a semantic label for each image in a large database, limiting the amount of information per image to a few bits. Others learn features by identifying common patterns found in images such as lines, blobs, and more complicated shapes, but ignoring semantic information. This project develops algorithms to learn features that are common in images and also predict the semantics of images at various spatial scales using a new type of deep neural network called Heterogeneous Networks. The developed algorithms allow the incorporation of semantic information at intermediate layers.  The algorithms developed can not only change how features are learned but also indicate how to scale feature learning to giant datasets of millions of images.  The research team addresses challenging problems in human face verification using NCSA's petascale supercomputer, Blue Waters, and two large scale (millions of images) image data sets. <br/><br/>The research of this projected is integrated with both undergraduate and graduate education. The results obtained from this project are applicable to a wide range of applications in computer vision and pattern recognition. The research team plans to release algorithms and face data sets collected in this project to research communities once they are finished."
"1250793","BIGDATA: Small: DA: DCM: Labeling the World","IIS","INFORMATION TECHNOLOGY RESEARC, Big Data Science &Engineering","04/01/2013","04/11/2013","Steven Seitz","WA","University of Washington","Standard Grant","Sylvia J. Spengler","03/31/2017","$750,000.00","Luke Zettlemoyer","seitz@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","1640, 8083","7433, 7923, 8083","$0.00","The project aims to  leverage the massive corpus of online photos, text, and maps to create a semantic 3D labeled model of the world, e.g., detailed representations of the world's top cultural and historical sites.  While breakthroughs in computer vision enable creating detailed 3D models from millions of online 2D images,  the resulting models capture only geometry. Consequently, they lack semantics; they don't provide information about the contents of the scene.  The vast treasure trove of online text such as Wikipedia meticulously catalogs the scenes that are captured in  photos and models.  Modern Natural Language Processing (NLP) techniques can now process such data, opening up the opportunity to extract knowledge from the online text corpus and use it to label 3D geometry.  This project seeks to jointly analyze the massive corpus of online text, maps, and photos to create labeled 3D models of the world's sites.  Achieving this goal will require fundamental research advances at the interface of natural language processing and computer vision that impact both the scientific research community and the world at large. <br/><br/>The project addresses two key technical challenges: (1) automatic scene labeling:  mapping semantics onto geometry, and (2) solving the 3D jigsaw puzzle:  mapping pieces of geometry into the world.  Many clues to these mapping problems lie in the text and other online datasources such as floorplans.  Other clues lie in the content of the photos.  Decoding this mapping therefore involves an interplay between NLP and computer vision.  The key research advances center around new ways to jointly leverage computer vision and NLP to solve problems to solve challenging problems in both fields, specifically, 1) recognizing objects through joint NLP and 3D visual analysis, 2) placing objects in the world by correlating geometry with spatial text in maps and webpages, and 3) using semantics to improve geometry by augmenting visual cues with textual spatial relations.<br/><br/>Broader Impacts:  The primary research outcomes are: (1) technology for creating labeled 3D models at a massive scale, and (2) labeled models for many top tourist sites.  Both the algorithms and models will be made freely available for the research community.  These algorithms and models will provide the foundation for a range of exciting applications of major practical impact on the world at large.  The resulting tools could make it possible for resources such as Wikipedia to link the text directly to 3D models and vice-versa, with attendant benefits to online learning and education.  The same technology could enable automated labeling of 2D photographs.  In the context of real-time applications (e.g., augmented reality), the technology could provide visual overlays and instant feedback on what you are currently looking at, and enable augmented reality-style guided tours.  Other applications include using labeled geometry for navigation (walking directions), and converting images to text for the visually impaired. The research is tightly integrated into education and training of students at the University of Washington. Additional information about the project can be found at: http://grail.cs.washington.edu/projects/label3d/"
"1353694","EAGER: Diverse M-Best Predictions from Probabilistic Models","IIS","Robust Intelligence","09/15/2013","09/06/2013","Dhruv Batra","VA","Virginia Polytechnic Institute and State University","Standard Grant","Jie Yang","08/31/2015","$184,415.00","","dbatra@vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","CSE","7495","7495, 7916","$0.00","Computer Vision systems must deal with significant levels of ambiguity - from inter- and intra-object occlusion and varying appearance, lighting, and pose. Probabilistic models provide a principled framework for dealing with uncertainty and for converting evidence into a posteriori belief about the world. Typically, a vision system uses this belief to predict the ""most likely"" or maximum a-posteriori hypothesis. Unfortunately, our current models are inaccurate and this single-best hypothesis is often incorrect. <br/><br/>This project explores a novel way to allow vision systems to hedge against uncertainty by producing multiple plausible hypotheses. Specifically, this project develops techniques for finding a diverse set of high-probability solutions from probabilistic models. The project focuses on (a) interactive object cutout (where multiple segmentations are shown to the user to expedite convergence to an acceptable result); (b) semantic segmentation (where multiple plausible scene labelings are propagated to subsequent stages of a cascade for higher-order processing); (c) person/object tracking (where multiple localization hypotheses on each frame reduce the search space of a sequence tracker). <br/><br/>This project is producing new scientific knowledge in the context of probabilistic reasoning and advancing the state of art in computer vision. The techniques developed are useful for other AI domains such as Speech and Natural Language Processing. The PI and his students are broadly disseminating produced work by organizing workshops, tutorials, and journal special issues, and publicly sharing code and results. The project is engaging undergraduate students and women in computer science research."
"1320956","RI: Small: Open Vision - Tools for Open Set Computer Vision and Learning","IIS","Robust Intelligence","09/01/2013","08/21/2013","Terrance Boult","CO","University of Colorado at Colorado Springs","Standard Grant","Jie Yang","12/31/2017","$449,894.00","","tboult@vast.uccs.edu","1420, Austin Bluffs Parkway","Colorado Springs","CO","809183733","7192553153","CSE","7495","7495, 7923","$0.00","When humans ""recognize"" things one of answers can always be ""unknown"" or ""that's new.""  Existing vision and machine learning research has made great progress but have done so in a closed set paradigm - which explicitly minimizes risk/errors over what is known.  As a computer vision system is moved toward real problems, it must face up to an open world. This project develops technologies for a new fundamental theory of ""open vision"" and corresponding set of tools that are explicitly designed to address open set recognition. At the heart of this research are three key concepts: 1) extending classical learning theory to include the risks of labeling open/unknown spaces, and then building classifiers that balance empirical risk, smoothness and open space risk; 2) meta-recognition - bringing a statistically-grounded probabilistic interpretation to classifiers, improving their ability to produce ""confidence"" in their answers; 3) operational adaptation - developing new approaches to address, at operation/run time, missing data or new data incorporating both open set and meta-recognition technologies. The work is also developing new approaches for open set evaluation, addressing problems in face recognition and visual object recognition as well as adapting classical machine learning datasets.<br/><br/>The open vision paradigm is embodied in open source tools that provide  performance at or significantly advancing the state of the art  while providing greater protection form unknown unknowns.   Since most science is exploring the unknown, providing easy to use open source learning/recognition tools design for both known and unknown data, the project have broad impact to many different applications."
"1254029","CAREER: Programmable Smart Machines","CNS","CSR-Computer Systems Research, Software & Hardware Foundation, PROGRAMMING LANGUAGES","06/01/2013","06/12/2017","Jonathan Appavoo","MA","Trustees of Boston University","Continuing grant","Marilyn McClure","12/31/2018","$595,021.00","","jappavoo@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","7354, 7798, 7943","1045","$0.00","Faster computers have enabled advances in science, commerce and daily life.  Unfortunately, computers have also become complex and more and more difficult to program efficiently.  This trend threatens the sustainability of future advances.  Perhaps, however, we can draw upon biologically inspired learning techniques to shed light into a new model of hybrid computers, a ``Programmable Smart Machine'', that inherently learns from its past behavior to automatically improve its performance without the burden of more complex programming.  Specifically this work explores the addition of a smart memory to a computer that gives it the abilities to learn, store and exploit patterns in past execution to improve its performance.<br/><br/>Central to this work is the introduction of a new kind of global long-term machine learning based 'cache' that can be viewed as an auto-associative memory.  The 'cache' is fed raw low-level traces of execution, from which it extracts and stores commonly occurring patterns that can be recognized and predicted.  The core execution process is modified to send the trace to the 'cache' and to exploit its feedback to enact acceleration.  The long-term goal is a system whose performance improves with the size and contents of the 'cache', which can be constructed with local associative memory devices and a shared online repository that is contributed to and leveraged by many systems.  In this way a kind of shared computational history is naturally created and exploited.<br/><br/>This work experimentally explores questions with respect to concretizing the ``Programmable Smart Machine'' model.  What are useful and tractable traces for detecting patterns in execution?  Can current unsupervised deep learning techniques detect, store and recall useful patterns?  How can the predictions from the machine learning based memory be utilized to automatically improve performance?  How big does the machine learning based memory need to be to yield useful predictions and acceleration?  This work explores these questions using simulation and controlled workload experiments to create complete traces including all instructions, register values, and I/O events.  Using the traces, at least two deep learning approaches will be evaluated with respect to the number and size of patterns they recognize.  The resulting trained models will be integrated into the published auto-parallelization methodology that established preliminary results for this work.  The simulation infrastructure, trace data and experimental results will be made publicly available to enable broader study.<br/><br/>This work produces unique trace data of computer operation.  The PI has found that visual and audio presentations of the preliminary data reflect the kind of intuition that computer scientists develop about how computers work.  This aspect will be leveraged to develop both a seminar, ``From Bits to Chess to Supercomputers'' and an associated<br/>``Computing Intuition'' website that engages K-12 students with computing.<br/>"
"1319846","RI: Small: RUI: AIR: Automatic Idiom Recognition","IIS","ROBUST INTELLIGENCE","08/01/2013","03/01/2017","Anna Feldman","NJ","Montclair State University","Standard Grant","Tatiana Korelsky","01/31/2018","$176,514.00","Jing Peng","feldmana@mail.montclair.edu","1 Normal Avenue","Montclair","NJ","070431624","9736556923","CSE","7495","7495, 7923, 9229","$0.00","The main goal of this research project is to develop a language independent method for automatic idiom recognition. Idiomatic expressions, such as 'a blessing in disguise' and 'kick the bucket' are plentiful in everyday language, though they remain mysterious, as it is not clear exactly how people learn and understand them. There is no single agreed-upon definition of idiom that covers all members of this class, but idioms tend to be relatively fixed in grammatical form and meaning, but with relatively little predictability in the relation between form and meaning. Also, many idiomatic expressions can appear with both literal, i.e. fully predictable, interpretations given their form -- compare 'The little girl made a face at her mother.' (idiomatic) vs. 'The little girl made a face on the snowman using a carrot and two buttons.' (literal) As a result, idioms present great challenges for a variety of natural language processing applications, including machine translation systems, which often do not detect idiomatic language. To address these challenges, an algorithm is proposed that neither relies on target idiom types, lexicons, or large manually annotated corpora, nor limits the search space by a particular type of linguistic construction. The starting point is that idioms are semantic outliers that violate cohesive structure, especially in local contexts. The following properties are quantified and are incorporated into the outlier detection algorithm: 1) lack of compositionality comparing to literal expressions or other types of collocations; 2) violation of local cohesive ties, so that they tend to be semantically distant from the local topics; 3) while not all semantic outliers are idioms, non-compositional semantic outliers are likely to be idiomatic; 4) idiomaticity is not a binary property; rather, idioms fall on the continuum from being compositional to being partly unanalyzable to completely non-compositional.<br/><br/>This research contributes to the better understanding of idiomatic language, to the computational treatment of such phenomena and, with the creation of high quality, publicly available linguistic resources annotated for idioms, to the facilitation of machine learning research and big data science. Additional benefits include efficient algorithms for computing compositionality and topicality from large corpora, interesting new generalizations about the nature of figurative language, and the training of a cadre of undergraduate and graduate students in highly practical work on a difficult interdisciplinary problem."
"1302668","Collaborative Research: RI: Processing Opinion Sharing Dialogue in Social Media","IIS","ROBUST INTELLIGENCE","09/01/2013","06/28/2016","Marilyn Walker","CA","University of California-Santa Cruz","Continuing grant","Tatiana Korelsky","08/31/2018","$1,070,556.00","Jean Fox Tree, Pranav Anand, Steve Whittaker","mawalker@ucsc.edu","1156 High Street","Santa Cruz","CA","950641077","8314595278","CSE","7495","7495, 7924, 9251","$0.00","When people make decisions or form beliefs, they often discuss them with others, seeking out others' opinions and sharing their own. Recently such conversations are occurring online, providing a public source of information of interest to companies, the military, the government, public policy bodies, and educators. Moreover these dialogs occur at scale, allowing researchers in natural language processing access to large-scale dialog datasets for the first time. However, automatically processing such dialogs is challenging, because current tools are targeted to traditional language resources such as newspaper articles. This project develops innovative algorithms for automatically processing and identifying important phenomena in such dialogs including: (a) stance - participants' views on a topic; (b) subjective dialog acts - including sarcasm and humor; and (c) central propositions - core ideas in the dialog, by combining methods of crowd-sourced annotation, bootstrapping and machine learning, and cognitive science. A critical project output is a new  corpus, including  annotations and dialogic summaries. <br/><br/>Longer term impacts include public policy, providing government and the military with methods to discover what ""the man on the street"" is saying about current topics. Educators can re-use the corpora and tools to expose children to compelling arguments about important issues. Greater understanding of opinion sharing dialog enables new cognitive experiments and theory: automatically identifying compelling arguments allows political science and social psychology researchers to examine learning and opinion formation. The project trains undergraduate and graduate students in interdisciplinary research combining social media, human computer interaction, computational linguistics and natural language processing."
"1352440","EAGER: PARTIAL: An Exploratory Study on Practical Approaches for Robust NLP Tools with Integrated Annotation Languages","IIS","Robust Intelligence","09/01/2013","08/19/2013","Noah Smith","PA","Carnegie-Mellon University","Standard Grant","Tatiana Korelsky","08/31/2014","$100,000.00","Chris Dyer","nasmith@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7495","7495, 7916","$0.00","In order to develop natural language processing (NLP) technologies for text in a wider range of languages, dialects, genres, and styles, this Early Grant for Exploratory Research investigates a novel methodological approach.  Conventionally, linguistic experts are employed to create gold-standard linguistically annotated datasets to which supervised machine learning algorithms are applied.  This project frees annotators from the requirement that annotations be complete by moving more of the burden to learning algorithms. Algorithms are developed that are robust to partial evidence, annotator variation, and noise due to errors.  As a result, any language enthusiast (not just trained experts) can provide annotations so that NLP can be developed for more kinds of text in more languages for less money.  In this exploration, the focus is on dependency parsing, a fundamental NLP component that predicts the grammatical relationships between words in sentences, with experimentation on data in English (two genres), Chinese, and Farsi.  The formal basis for the approach is a framework called Graph Fragment Language (GFL).   The project assesses the quality of parsers learned from GFL and the productivity of annotators accorded this new flexibility.<br/><br/>Beyond documentation and assessment of the new methodology, this project produces open-source software tools for gathering annotated data and constructing NLP tools using the data.  It emphasizes the usability of these tools in classrooms, contributing exercises that can be used in NLP and linguistics courses to allow students to engage directly with data, with the models that make use of the data, and with the technological goals that data annotation supports."
"1256596","SBIR Phase II:  Use of Machine Learning Techniques for Robust Crop and Weed Detection in Agricultural Fields","IIP","SBIR Phase II","04/15/2013","09/22/2015","Lee Redden","CA","Blue River Technology Inc","Standard Grant","Muralidharan Nair","02/28/2018","$999,998.00","","leeredden@gmail.com","575 N Pastoria Ave","Sunnyvale","CA","940852916","4087187457","ENG","5373","165E, 5373, 6840, 8035, 9139, HPCC","$0.00","This Small Business Innovation Research (SBIR) Phase II project seeks to further develop a novel computer vision based plant identification system for commercialization in agricultural weed control. This system will provide a cost competitive alternative to chemical herbicides, a global $20B market. Existing computer vision based approaches can segment a 'splotch' of green vegetation from a brown background but are unable to provide the selectivity and precision necessary for mechanized, automated weeding. This project's objective is to create software algorithms that match the capability of the human eye and brain to quickly and reliably classify plants into crops and weeds in real-time. The project team will build a computer vision algorithm based on a hierarchical classifier. This classifier will utilize a field customized support vector machine (SVM) that uses point-of-interest rather than shape-based methods, a novel approach to visual object identification. The result of this research will be the creation of an algorithm integrated into an automated weeding system.<br/><br/>The broader impact/commercial potential of this project is significant, as the development of an alternative to chemical intensive agricultural weed control will impact technological understanding, create commercial opportunity, and positively impact society. Technologically, the project will advance the fields of computer vision and machine learning through development of a real-time, automated plant identification system based on point-of-interest and SVMs. Commercially, the system will offer conventional farmers an effective and chemical-free method to eliminate weeds, and it will offer organic farmers the first truly precise organic weed control method. The addressable market for weed control in food production is estimated to be $4B in the U.S. The system's ability to eliminate the use of chemical herbicides has a profound societal effect. U.S. farmers apply over 250M pounds of herbicide annually on corn and soybeans alone, with many unintended and detrimental side effects. Chemical concentrations in rivers, lakes and groundwater are rising, and the prevalence of herbicide resistant weeds is growing exponentially. An alternative to these chemicals limits society's exposure while protecting environmental integrity."
"1248638","SBIR Phase I: A Cloud-Based Service for Audio Access to News and Blogs","IIP","SMALL BUSINESS PHASE I","01/01/2013","12/11/2012","Radhika Thekkath","CA","AgiVox, Inc.","Standard Grant","Glenn H. Larsen","06/30/2013","$149,999.00","","rthekkath@agivox.com","440 N. Wolfe Road","Sunnyvale","CA","940853869","6509960224","ENG","5371","5371, 8032, 8039","$0.00","The innovation improves access and discovery of online written content when content is automatically converted to an audio format such as mp3. In general, synthesized audio from random written text often delivers a poor listening experience. The technical effort is motivated by the absence of applications that provide a user preferred news articles and blogs with high quality synthesized audio that is phonetically correct for the visually impaired person or the multitasking visually busy person such as a car driver. This work uses techniques such as textual processing motivated by text understanding and content analysis by domain knowledge and machine learning. Machine learning techniques are used to improve speech synthesis and to incorporate auto-discovery of user preferences into listenable news. Since content scanning by listening is a slower process than visually scanning for relevant responses, this technical work will improve this auditory search process by combining user input with information retrieval for a smoother user experience. The resulting technology infrastructure is expected to provide an array of compelling commercial products with far-reaching implications.<br/><br/>The broader/commercial impact of this technical work comes from the cloud-based infrastructure that can process online written text into high-quality audio. This cloud software has advantages of unlimited storage and computing capacity, and uses this to support content retrieval, machine learning, text preprocessing, content discovery, natural language processing, and interaction with commercial Text-to-Speech servers. The first version of this technology will focus on news and blogs, a sufficiently large corpus of information that provides a challenge while also providing considerable commercial interest. The cloud infrastructure can support a range of client-side applications that work on smart-phones, tablets, and desktops. These applications will have access to high quality synthesized audio useable in an ""eyes-busy"" situation including the low-vision community. The apps will provide customizable access to user-preferred content via intelligent information retrieval. While there is commercial potential in such client applications, the greater value is from licensing the server technology. The societal impact of such a product is tremendous since neither the blind community nor the general public have such easy listening access to the large corpus of online content that is curated with user preferences and with an application control mechanism that is entirely via voice and finger gestures."
"1320344","III: Small: Collaborative Research: Solving Matching Problems in Machine Learning with Non-commutative Harmonic Analysis","CCF","SIGNAL PROCESSING","08/01/2013","08/07/2013","Imre Kondor","IL","University of Chicago","Standard Grant","Phillip Regalia","07/31/2018","$221,993.00","","risi@cs.uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","7936","7923, 7936","$0.00","The problem of matching representations of one set of objects, e.g., their images, to representations of another set of objects that achieves an optimal global measure of overlap (goodness of match) is ubiquitous in computer science, and remains a fundamental challenge in areas such as machine learning, computer vision, and computational biology. While some cases are solvable in polynomial time, a majority of those encountered in practice are computationally intractable - NP-hard. This research will exploit the fact that in many matching problems of interest the space to be optimized over has the (algebraic) structure of group, which allows one to leverage an entire spectrum of ideas from abstract algebra, including non-commutative harmonic analysis and fast Fourier transforms on groups. In addition to yielding efficient optimization schemes in several important cases, this algebraic approach has the potential to serve as a basis for developing novel matching algorithms and suggest new approaches for certain classes of combinatorial optimization problems. <br/><br/>The proposed research has four main goals: to design faster general purpose harmonic analysis-based quadratic assignment problem (QAP) solvers and apply these to alignment and matching problems; to develop ""tailored"" QAP solution methods by coupling them to a learning component, which leverages training data to solve subsequent QAP instances much more efficiently; to design multiresolution analysis-based algorithms which yield global solutions to multi-object tracking and matching problems; and, to implement a flexible open-source library which offers a wide variety of harmonic analysis functionality (with support for wavelet and other transforms) to encourage experimentation on a broad class of inference and optimization problems. This project will yield a powerful set of algorithms and open-source software that can be used by researcher in areas of machine learning, computer vision, and optimization."
"1320755","III: Small: Collaborative Research: Solving Matching Problems in Machine Learning with Non-commutative Harmonic Analysis","CCF","SIGNAL PROCESSING","08/01/2013","08/07/2013","Vikas Singh","WI","University of Wisconsin-Madison","Standard Grant","John Cozzens","07/31/2017","$202,212.00","","VSINGH@CS.WISC.EDU","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","7936","7923, 7936","$0.00","The problem of matching representations of one set of objects, e.g., their images, to representations of another set of objects that achieves an optimal global measure of overlap (goodness of match) is ubiquitous in computer science, and remains a fundamental challenge in areas such as machine learning, computer vision, and computational biology. While some cases are solvable in polynomial time, a majority of those encountered in practice are computationally intractable - NP-hard. This research will exploit the fact that in many matching problems of interest the space to be optimized over has the (algebraic) structure of group, which allows one to leverage an entire spectrum of ideas from abstract algebra, including non-commutative harmonic analysis and fast Fourier transforms on groups. In addition to yielding efficient optimization schemes in several important cases, this algebraic approach has the potential to serve as a basis for developing novel matching algorithms and suggest new approaches for certain classes of combinatorial optimization problems. <br/><br/>The proposed research has four main goals: to design faster general purpose harmonic analysis-based quadratic assignment problem (QAP) solvers and apply these to alignment and matching problems; to develop ""tailored"" QAP solution methods by coupling them to a learning component, which leverages training data to solve subsequent QAP instances much more efficiently; to design multiresolution analysis-based algorithms which yield global solutions to multi-object tracking and matching problems; and, to implement a flexible open-source library which offers a wide variety of harmonic analysis functionality (with support for wavelet and other transforms) to encourage experimentation on a broad class of inference and optimization problems. This project will yield a powerful set of algorithms and open-source software that can be used by researcher in areas of machine learning, computer vision, and optimization."
"1315554","SBIR Phase I:  Linguistic Analysis of Web Content for 21st Century Inquiry Learning","IIP","SBIR Phase I","07/01/2013","07/31/2014","Eleni Miltsakaki","PA","Choosito!","Standard Grant","Glenn H. Larsen","08/31/2014","$183,924.00","","eleni@choosito.com","462 Ballytore Rd","Wynnewood","PA","190962309","6106421816","ENG","5371","5371, 8031, 8039, 9177","$0.00","This SBIR Phase II innovation is twofold.  First, advanced natural language processing techniques are used to analyze the content of websites and identify features of the text that determine processing difficulty for the individual student. After the initial evaluation of text difficulty, the system becomes adaptive and automatically updates its decision for the suitability of a website by keeping track of the user's reading experience.  This machine learning technology enables differentiated learning and, thus, enables fair access of web content to English Language Learners and special education groups. Second, the content of websites is characterized by advanced text analysis and classification to detect school subject areas. Elementary school students, for example, will find science content written at a level that they can comprehend, thus, being enabled to laser focus their attention to accessible content and engage with it critically.  The proposed technology addresses the critical problem of information management and helps teachers teach their students the skills that they need to find, evaluate, and take advantage of available information in any context. <br/><br/>The broader/commercial impact of the proposed technology will be in the integration of the recently articulated common core standards (21st century learning) in the K-12 classroom. The development of the proposed technology will offer unprecedented opportunity for differentiated learning and give students a life-long tool to help them understand and critically engage with the information available to them. Early integration of information literacy and inquiry based learning is enabled not only by identifying age appropriate resources but, also, by offering a total project management platform. Teachers and curriculum developers will save valuable time and effort because they will have community space to find curated up-to-the-minute resources and core projects, which they can edit, save, and share, assign them to students, and receive submitted work, all in one window. Students, too, have personal space to keep notes, automatically keep track of resources for later review and citation, collaborate, prepare, and submit their reports. The platform can integrate with school library resources, thus, becoming a powerful tool for librarians. Application Programming Interfaces can be developed for collaboration with publishers enabling them to smoothly integrate core standards in their curricula and update them for 21st century learning."
"1252951","CAREER: Computational mechanisms of rapid visual categorization: Models and psychophysics","IIS","CAREER: FACULTY EARLY CAR DEV, Robust Intelligence","09/15/2013","09/08/2013","Thomas Serre","RI","Brown University","Standard Grant","Kenneth Whang","08/31/2019","$500,001.00","","Thomas_Serre@brown.edu","BOX 1929","Providence","RI","029129002","4018632777","CSE","1045, 7495","1045, 9150","$0.00","Primates can recognize objects embedded in complex natural visual scenes at a glance. Despite the ease with which we see, visual recognition -- one of the key issues addressed in computer vision -- is quite difficult for machines. Understanding which computations are performed by the visual cortex would give scientists a powerful tool to uncover key mechanisms of human perception and cognition as well as to create a new generation of 'seeing' machines. <br/><br/>The PI's central research goal is to identify the perceptual principles and model the neural mechanisms underlying rapid visual categorization. By forcing processing to be fast, rapid visual categorization paradigms help isolate the very first pass of visual information before more complex visual routines take place. Hence, understanding 'vision at a glance' is arguably a necessary first step before studying natural everyday vision where eye movements and attentional shifts are known to play a key role.<br/><br/>Specifically, this proposal will lead to the development of a computational neuroscience model of rapid visual recognition in the primate visual system, which is both consistent with physiological properties of cells in the visual cortex and able to predict behavioral responses (both correct and incorrect responses as well as reaction times) from human participants across a range of conditions. The proposed model will integrate recent developments in computational models of vision and decision making with large-scale machine learning techniques. New stimulus sets will be generated, which are optimally tailored for testing among alternative visual representations and computations against human psychophysics data. These experiments will, in turn, enable the refinement of computational models.<br/><br/>The computational models developed as part of this proposal will be integrated in courses and disseminated broadly via a web graphical interface. Overall the interdisciplinary nature of the proposal will give students the opportunity to experience a research environment that crosses traditional boundaries across disciplines and departments. Increased undergraduate participation in computational neuroscience will help integrate this area into the mainstream computer science and neuroscience curricula."
"1310496","AIS: Learning  from  Initially Labeled Nonstationary Streaming Data","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","09/01/2013","08/01/2013","Robi Polikar","NJ","Rowan University","Standard Grant","Radhakisan Baheti","08/31/2018","$297,542.00","","polikar@rowan.edu","Office of Sponsored Programs","Glassboro","NJ","080281701","8562564057","ENG","7607","1653","$0.00","The objective of this research is to develop a novel framework for analysis of large volumes of streaming data, where data characteristics change over time, and all new data are unlabeled and unstructured. The proposed work will be seminal for analyzing streaming nonstationary and unlabeled data while removing the unrealistic assumptions and simplifications made by existing approaches. The proposed approach uses small initial training data to label the currently unlabeled new data, creates an envelope around this data and shrinks the envelope to determine the core support region of the data. Samples are extracted from this region to serve as future training data to iteratively label the new incoming unlabeled drifting data. Once initialized, this approach never needs labeled data, and can indefinitely track the changes in data distribution. <br/>The primary intellectual merit is a new framework, addressing arguably one of the most challenging learning problems, accomplished by strategic integration of machine learning and computational geome-try. Significant fundamental knowledge will be obtained through formal development of this framework, which will then benefit many real-world applications, currently not properly addressed under existing ap-proaches. <br/>Broader Impacts: The proposed framework promises to bring us closer to truly adaptive and intelligent (brain-like) learning, and allow proper analysis of data drawn from aforementioned scenarios, whose ap-plications include network intrusion, cyber security, web-usage analysis, natural language processing, anomaly detection, climate change and energy demand analysis. Project's educational component will form Integrated Research and Learning Communities, drawing undergraduate students to whom this field has been mostly inaccessible."
"1319645","HCC: Small: Modeling and Supporting Creativity During Collaborative STEM Activities","IIS","HCC-Human-Centered Computing","08/01/2013","04/25/2014","Winslow Burleson","AZ","Arizona State University","Standard Grant","William Bainbridge","04/30/2015","$515,998.00","Danielle McNamara, Kasia Muldner","win@arizona.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","7367","7367, 7923, 9251","$0.00","This research will advance a novel technological approach that relies on machine learning techniques in general and Natural Language Processing (NLP) in particular to develop models and support for creativity during collaborative science, technology, engineering, and mathematics (STEM) educational activities. We will extend existing educational software with NLP capabilities to automatically assess and subsequently support creativity during collaborative tasks. The research questions are: (1) Which factors influence moment-by-moment creativity during collaborative problem solving activities? (2) How can NLP be used to build student models that detect those factors? (3) How can an ITS use this information to create personalized interventions to support creativity?<br/> <br/>The first phase in this research will collect data from students solving problems in pairs with an educational application to identify factors that are relevant to creativity processes and outcomes.  These data will be used to derive computational student models for automatically assessing student creativity in terms of both moment-to-moment processes and outcomes through machine learning methodologies focusing on an NLP approach. In addition to providing automatic assessment, the models will also inform factors that influence creativity during collaboration through educational data mining techniques. The final phase of the work will design and test a set of interventions to foster creativity during collaborative activities.<br/> <br/>Using data corresponding to pairs of students solving open-ended STEM-based problems, this research will develop a rich and nuanced understanding of creativity processes and outcomes in collaborative contexts, and how these relate to knowledge, affect and creative thinking styles. Relying on that understanding, it will develop and evaluate novel student models that recognize salient, creativity-related events through NLP techniques, as well as personalized support for creativity during collaborative activities and evaluating that support through an experiment with university students. This project will pave the way for a new class of collaborative cyberlearning technologies to both assess and foster creativity, through just-in-time personalized support based on easily deployed NLP-based student models."
"1317372","Collaborative Research: CDS&E-MSS: Robust Algorithms for Interpolation and Extrapolation in Manifold Learning","DMS","CDS&E-MSS, CDS&E","09/01/2013","07/14/2013","Hongyuan Zha","GA","Georgia Tech Research Corporation","Standard Grant","Yong Zeng","08/31/2017","$170,000.00","","zha@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","MPS","8069, 8084","9263","$0.00","The objective of this proposal is to develop robust algorithms for reconstructing or synthesizing highly structured high-dimensional data based on a low-dimensional representation learned from a training dataset, i.e., the interpolation and extrapolation problems in manifold learning. The project will address the elusive issue of computing a usually not well-defined low-dimensional parametrization in the setting of various interpolation and extrapolation problems for manifold learning, emphasizing the notion of physically meaningful paramterizations. It will develop innovative computational methodology for flexibly learning a low-dimensional parametrization together with other physically important variables in the context of both unsupervised and semi-supervised learning and especially active learning settings, for learning and synthesis of dynamic data, and for manifold extrapolation based on transfer learning. Included in the project is a development of a publicly available software package which will disseminate the research results and promote applications of nonlinear dimension reduction methodology to real-world problems.<br/><br/>The discoveries from this proposed research are expected to impact a wide range of areas of applications. Computing compact representation of high-dimensional data represents a very challenging statistical learning problem, and manifold learning has become a very active research field aiming at discovering hidden structures from the statistical and geometric regularity inherent in many high-dimensional data. Reconstruction and synthesis of high-dimensional data in the context of interpolation and extrapolation will have significant applications in image and video processing, computer vision, video surveillance for homeland security, computational biology, and scientific visualization. The proposed theoretical tools and computational methods have the promise of significantly expanding the applicability and functionality of existing and new manifold learning methods and thus advancing the state of the art in nonlinear dimension reduction research. The proposed research lies at the interface between applied mathematics, computational science, and machine learning applications and provides an ideal setting for research cross-fertilization and collaboration as well as training of graduate students in interdisciplinary research."
"1317424","Collaborative Research: CDS&E-MSS: Robust Algorithms for Interpolation and Extrapolation in Manifold Learning","DMS","CDS&E-MSS, CDS&E","09/01/2013","07/14/2013","Qiang Ye","KY","University of Kentucky Research Foundation","Standard Grant","Yong Zeng","08/31/2017","$139,907.00","","qye3@uky.edu","109 Kinkead Hall","Lexington","KY","405260001","8592579420","MPS","8069, 8084","9150, 9263","$0.00","The objective of this proposal is to develop robust algorithms for reconstructing or synthesizing highly structured high-dimensional data based on a low-dimensional representation learned from a training dataset, i.e., the interpolation and extrapolation problems in manifold learning. The project will address the elusive issue of computing a usually not well-defined low-dimensional parametrization in the setting of various interpolation and extrapolation problems for manifold learning, emphasizing the notion of physically meaningful paramterizations. It will develop innovative computational methodology for flexibly learning a low-dimensional parametrization together with other physically important variables in the context of both unsupervised and semi-supervised learning and especially active learning settings, for learning and synthesis of dynamic data, and for manifold extrapolation based on transfer learning. Included in the project is a development of a publicly available software package which will disseminate the research results and promote applications of nonlinear dimension reduction methodology to real-world problems.<br/><br/>The discoveries from this proposed research are expected to impact a wide range of areas of applications. Computing compact representation of high-dimensional data represents a very challenging statistical learning problem, and manifold learning has become a very active research field aiming at discovering hidden structures from the statistical and geometric regularity inherent in many high-dimensional data. Reconstruction and synthesis of high-dimensional data in the context of interpolation and extrapolation will have significant applications in image and video processing, computer vision, video surveillance for homeland security, computational biology, and scientific visualization. The proposed theoretical tools and computational methods have the promise of significantly expanding the applicability and functionality of existing and new manifold learning methods and thus advancing the state of the art in nonlinear dimension reduction research. The proposed research lies at the interface between applied mathematics, computational science, and machine learning applications and provides an ideal setting for research cross-fertilization and collaboration as well as training of graduate students in interdisciplinary research."
"1319830","III: Small: Probabilistic Hashing for Efficient Search Learning","IIS","Info Integration & Informatics","09/01/2013","08/21/2013","Ping Li","NY","Cornell University","Continuing grant","Sylvia Spengler","10/31/2013","$316,562.00","","pingli@stat.rutgers.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7364","7364, 7923","$0.00","Numerous applications involve massive, high-dimensional datasets. For example, the search industry routinely deals with billions of web pages, where each page is often represented as a binary vector in 2^64 dimensions. In computer vision, images are often represented as non-binary vectors in millions of dimensions. Algorithms which are capable of efficiently compressing, retrieving, and mining these datasets are of high practical importance. Mathematically rigorous and computationally efficient hashing methods will be developed to dramatically reduce ultra-high-dimensional datasets. These algorithms will be integrated with a variety of learning techniques including classification, clustering, near-neighbor search, matrix factorizations, etc. <br/><br/>The project builds on and extends minwise hashing, and b-bit minwise hashing which are standard hashing techniques in search applications. The project aims to (i) rigorously analyze b-bit minwise hashing and develop, analyze, and apply significantly more efficient (and more accurate) to problems in search and learning; (ii) develop  a unified framework of probabilistic hashing which essentially consists of one permutation followed by (at most) one random projection; (iii) develop a unified theory of summary statistics under a variety of engineering constraints (storage space, computational speed, indexing capability, adaptation to streaming, etc.). <br/><br/>Hashing algorithms developed under this framework are expected to be substantially much more efficient and more accurate than existing popular algorithms such as random projections and minwise hashing.  This general framework allows the design algorithms to accommodate many different data types (sparse or dense data, binary or real-valued data, static or streaming data), many different engineering needs (computing inner products or lp distances, kernel learning or linear learning), and different storage requirements. Anticipated results of the proposed research include rigorous and computationally efficient hashing algorithms for dealing with ultra-high-dimensional datasets, the integration of the resulting hashing algorithms into with a variety of learning techniques for classification, clustering, near-neighbor search, singular value decompositions, matrix factorization, etc; and rigorous experimental evaluation of the resulting methods on big (e.g., TeraByte or potentially PetaByte) data of the order of up to 2^64 dimensions. <br/><br/>Broader Impacts: Effective approaches to building predictive models from extremely high dimensional data can impact many areas of science that rely on machine learning as the primary methodology for knowledge acquisition from data. The PI's education and outreach efforts aim to broaden the participation of women and underrepresented groups. The publications, software, and datasets resulting from the project will be freely disseminated to the larger scientific community."
"1360971","III: Small: Probabilistic Hashing for Efficient Search Learning","IIS","Info Integration & Informatics","08/28/2013","07/14/2014","Ping Li","NJ","Rutgers University New Brunswick","Continuing grant","Sylvia Spengler","08/31/2018","$475,149.00","","pingli@stat.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7364","7364, 7923","$0.00","Numerous applications involve massive, high-dimensional datasets. For example, the search industry routinely deals with billions of web pages, where each page is often represented as a binary vector in 2^64 dimensions. In computer vision, images are often represented as non-binary vectors in millions of dimensions. Algorithms which are capable of efficiently compressing, retrieving, and mining these datasets are of high practical importance. Mathematically rigorous and computationally efficient hashing methods will be developed to dramatically reduce ultra-high-dimensional datasets. These algorithms will be integrated with a variety of learning techniques including classification, clustering, near-neighbor search, matrix factorizations, etc. <br/><br/>The project builds on and extends minwise hashing, and b-bit minwise hashing which are standard hashing techniques in search applications. The project aims to (i) rigorously analyze b-bit minwise hashing and develop, analyze, and apply significantly more efficient (and more accurate) to problems in search and learning; (ii) develop  a unified framework of probabilistic hashing which essentially consists of one permutation followed by (at most) one random projection; (iii) develop a unified theory of summary statistics under a variety of engineering constraints (storage space, computational speed, indexing capability, adaptation to streaming, etc.). <br/><br/>Hashing algorithms developed under this framework are expected to be substantially much more efficient and more accurate than existing popular algorithms such as random projections and minwise hashing.  This general framework allows the design algorithms to accommodate many different data types (sparse or dense data, binary or real-valued data, static or streaming data), many different engineering needs (computing inner products or lp distances, kernel learning or linear learning), and different storage requirements. Anticipated results of the proposed research include rigorous and computationally efficient hashing algorithms for dealing with ultra-high-dimensional datasets, the integration of the resulting hashing algorithms into with a variety of learning techniques for classification, clustering, near-neighbor search, singular value decompositions, matrix factorization, etc; and rigorous experimental evaluation of the resulting methods on big (e.g., TeraByte or potentially PetaByte) data of the order of up to 2^64 dimensions. <br/><br/>Broader Impacts: Effective approaches to building predictive models from extremely high dimensional data can impact many areas of science that rely on machine learning as the primary methodology for knowledge acquisition from data. The PI's education and outreach efforts aim to broaden the participation of women and underrepresented groups. The publications, software, and datasets resulting from the project will be freely disseminated to the larger scientific community."
"1335035","Geometry and Statistics on Spaces of Dynamical Systems for Pattern Recognition in High-Dimensional Time Series","CMMI","APPLIED MATHEMATICS, DYNAMICAL SYSTEMS","09/01/2013","07/27/2013","Rene Vidal","MD","Johns Hopkins University","Standard Grant","Jordan Berg","08/31/2016","$391,000.00","Laurent Younes","rvidal@cis.jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","ENG","1266, 7478","034E, 035E, 036E, 1057, 1266, 7478, 8024, 9102, CVIS","$0.00","The objective of this project is to develop a mathematical framework for efficiently comparing dynamical systems identified from high-dimensional time-series data as well as algorithms for clustering, classification, and statistical analysis of such data. Dynamical systems are widely used for the analysis, verification, and control of physical, mechanical, thermal, chemical and biological processes. However, there are many emerging applications in which one also needs to ""compare the dynamics of two processes"". In computer vision, for example, one can use dynamical models to describe kinematic and video data of human motion. While different people move differently, the dynamical models of two people performing the same task (e.g., walking) should be ""closer"" to each other than the models of two people performing different tasks (e.g., walking vs running). Framework will be developed for spaces of linear dynamical systems whose quotient structure is defined by the action of a group on a smooth manifold. A family of efficiently computable distances in the ambient space will be used to define a family of ""group-action-induced distances"" in the quotient space. Such distances will be used to develop methods for performing classification, clustering and statistical analysis on spaces of dynamical systems. These methods will be evaluated on kinematic and video data of human activities.<br/><br/>The development of methods for comparing dynamical models can impact both basic science and the society at large. In control theory, such methods can impact system identification and robust control. In computer vision, such techniques can be used to discriminate human and crowd activities in video data, which is relevant to many applications in surveillance, security, traffic monitoring, sports coverage/broadcast, human-computer interaction, etc. This project will train engineers and scientist in multidisciplinary research that will need concepts from differential geometry, machine learning, and computer vision. As such, it can potentially impact many other related fields. This project will also impact many diversity outreach activities, including ongoing REU programs, the Women in Science and Engineering (WISE) program and summer camps for K-12 outreach. Datasets and code will be made publicly accessible for research and educational purposes."
"1252624","CAREER: Flexible Network Estimation from High-Dimensional Data","DMS","STATISTICS, Division Co-Funding: CAREER","07/01/2013","06/09/2017","Daniela Witten","WA","University of Washington","Continuing Grant","Gabor Szekely","06/30/2020","$400,000.00","","dwitten@u.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","MPS","1269, 8048","1045","$0.00","This research involves the development of new statistical methods and theory for graphical modeling on the basis of high-dimensional data, in which the number of features exceeds the number of observations. In certain applications, such as the estimation of transcriptional regulatory networks on the basis of gene expression data, existing techniques are inadequate for two reasons: the assumptions that underlie these techniques are insufficient for accurate network recovery in the face of such high dimensionality, and furthermore the assumptions that are made may be unrealistic for the data. To address these two problems, the investigator proposes to study (a) a set of techniques for more effectively learning one or more Gaussian graphical models by making more effective and structured assumptions about the topology of the true conditional dependence networks, via convex penalties and other techniques; and (b) more flexible frameworks for estimating conditional dependence relationships without the usual Gaussianity assumptions.<br/><br/>In recent years, new technologies and fast computers have resulted in the generation and availability of vast amounts of data in fields as diverse as molecular biology, marketing, finance, sociology, linguistics, and computer vision. Unfortunately, analyzing this type of ""big data"" poses severe statistical challenges, and the classical statistical toolset cannot be applied. Therefore, developing effective statistical machine learning techniques for making sense of very large-scale data sets is crucial for progress in many areas of science as well as industry, in order to bridge the gap between the data that is being collected and the scientific and industrial questions that are being asked about the data. As an example, being able to estimate gene networks on the basis of genomic data has important implications for understanding biological processes, and for making progress towards the treatment of cancer and other disease. This proposal involves (1) developing techniques for improved network estimation on the basis of high-dimensional data sets; (2) disseminating the resulting techniques to the statistical and biomedical communities via publications, seminars, and the public release of software;  (3) training PhD students in statistical machine learning techniques for big data; and (4) increasing the exposure of high school students, undergraduates, and members of underrepresented groups to statistical machine learning and big data challenges via short courses, conference presentations, and other activities."
"1252725","CAREER: Efficient Statistical Inference using Neuroimaging data for Sample Enrichment and Optimizing Power","IIS","STATISTICS, Robust Intelligence","04/15/2013","03/24/2020","Vikas Singh","WI","University of Wisconsin-Madison","Continuing Grant","Jie Yang","02/28/2021","$493,976.00","","VSINGH@CS.WISC.EDU","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","1269, 7495","1045, 7495, 9251","$0.00","Hypothesis testing on neuroimaging data traditionally has made use of classical statistical tests (on uni-variate response variables). This makes sub-optimal use of the structure of images, particularly problematic if the two groups being tested have weak differences to begin with. Failure to detect statistically significant differences may imply failure of the experiment itself. Acquiring more images is expensive but also occasionally infeasible. This project develops technologies to address these problems (particularly those dealing with differential analysis of brain images) via the lens of computer vision and machine learning. The algorithmic component of this project is (1) a suite of  convex optimization based multi-modal learning schemes to seamlessly leverage a spectrum of brain imaging data, (2) new multi-resolution representations for inference with surface/network based signals (data derived from structural/functional brain images), and (3) using these mechanisms for boosting statistical power even in experiments with small sample sizes.<br/><br/>The project has broad scientific impact.  Extending the operating range of statistical image analysis methods for neuroimaging will foster a new inter-disciplinary area at the interface of computer vision, biostatistics, and machine learning, which is highly intellectually stimulating. The research team brings real neuroimaging research data for undergraduate/graduate students to explore and study. The project goals also include training and mentoring of students, increased involvement of under-represented groups, seminars, and an extensive set of outreach activities. In addition, the resultant software tools drive the analysis of neuroscience studies, which has clear broad societal impact."
"1338042","MRI: Development of an Instrument that Monitors Behaviors Associated with Obsessive-Compulsive Behaviors and Schizophrenia","CNS","Special Projects - CNS, IUCRC-Indust-Univ Coop Res Ctr, NRI-National Robotics Initiati","10/01/2013","05/30/2019","Nikolaos Papanikolopoulos","MN","University of Minnesota-Twin Cities","Standard Grant","Rita Rodriguez","09/30/2020","$630,000.00","Kelvin Lim, Gail Bernstein, Tasoulla Hadjiyanni, Arindam Banerjee","npapas@cs.umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","1714, 5761, 8013","1189, 1714, 9178, 9251","$0.00","Proposal #: 13-38042<br/>PI(s):  Papanikolopoulos, Nikolaos<br/>  Banerjee, Arindam; Bernstein, Gail; Hadjiyanni, Tassoulla; Lim, Kelvin, K.<br/>Institution: University of Minnesota-Twin Cities<br/>Title:   MRI/Dev.: Instrument that Monitors Behaviors Associated with OCD and Schizophrenia <br/>Project Proposed:<br/>This project, developing a new instrument to facilitate data collection associated with clinical assessment of complex mental health disorders such as obsessive-compulsive behaviors (known as OCD), aims to enable long-term research advances in computer vision, activity recognition and tridimensional reconstruction algorithms to automate the identification of behaviors typical of OCD subjects. The immersion of selected subjects in a virtual reality room (CAVE) is used to trigger specific behaviors to be captured and analyzed. The sophisticated sensor system under development will serve to collect these data and provide intelligent data processing capabilities that would enable future exploration and testing of new diagnostic and therapeutic protocols, leading to the establishment of the basis for, and utility in, seeking early at-risk markers in children and adolescents. This instrument initiative is based on the premise that expertise can accurately identify useful diagnostic markers and on the belief that technologies can now be developed to collect massive behavioral data in ways not previously done and discover behavioral patterns.<br/>The instrument is expected to<br/>- Provide extensive data collection associated with subjects diagnosed with the respective disorders (data useful not only to clinicians but also to computer vision and machine learning researchers among others),<br/>- Capture interactions, behaviors, and physiological reactions to real and/or synthetic multimodal stimuli (optical, acoustical, etc.),<br/>- Allow computer and information scientists to develop computational tools and algorithms to generate quantitative, adequate, and cost-effective norms for screening a broad population, <br/>- Enhance Cognitive Behavior Therapy (CBT) procedures and diagnostic protocols by integration of technologies that can excite or inhibit triggers for schizophrenic or OCD episodes,<br/>- Assess particular Augmented and Virtual Environments (AE/VEs) and social media devices (smart phones and tablets) and their impacts on the cognitive presence of normal versus afflicted subjects, and<br/>- Evaluate whether an enhanced cognitive presence via an AE/VE can increase or suppress (habituate) the intensity of behavioral symptoms detectable by sensors. <br/>Broader Impacts: Among these we have:<br/>- Creation of large and complex datasets that will enable computer scientists to apply the newest computational tools on them,<br/>Development of a potentially transformative technology-driven instrument for detecting early risk markers of OCD,<br/>- Exploration of a platform well-suited to new directions for a better characterization of mental disorders,<br/>- Systematic database development of quantified, multimodal data and a sounder and more precise basis for earlier detection,<br/>- Reduction of overall costs and a parallelizing reduction in the long-term costs due to previously delayed or incorrect diagnoses,<br/>- Reduction of anxiety, disruption, stress, and sometimes real tragedy on patients and their families, <br/>- Earlier detection and reduced need for drug-based, later stage interventions enabled by the ease of testing, and associated societal benefits, and<br/>- Student education and training in the use of the instrument."
"1360566","RI: Small: Unsupervised Object Class Discovery via Bottom-up Multiple Class Learning","IIS","Robust Intelligence","07/01/2013","06/12/2014","Zhuowen Tu","CA","University of California-San Diego","Continuing Grant","Jie Yang","08/31/2016","$420,029.00","","zhuowen.tu@gmail.com","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7495","7495, 7923","$0.00","This project develops an integrated framework to perform simultaneous object discovery and detector training in an unsupervised setting. It takes advantages of large amount (millions or even billions) of well-organized internet images to automatically learn rich image representations for a wide range of objects. The main activities in this project include the following. (1) The central component of this project is a formulation to turn unsupervised data into weakly-supervised ""noisy input"" through which commonalities are explored for rich object representation using a new learning method. (2) A large dictionary of mid-level image representations will be learned on a large scale number of images retrieved using thousands of object words through the internet search engine. (3) A new flexible object representation is developed to deal with articulated/non-rigid objects.<br/><br/>The project advances computer vision and machine learning fields by developing an unsupervised paradigm to explore a large scale of internet images. The learned mid-level and high-level representations from images retrieved using thousands of words can significantly enhance the object representation power and benefit researchers in the object recognition field. The formulations, algorithms, and methods resulted from this project are also helpful to researchers in other fields such as medical imaging and data mining. The project dissemination plan includes the source code and learned mid-level and high-level representations."
"1360568","CAREER:  Holistic 3D Brain Image Parsing by Integrating Implicit and Explicit Models","IIS","Robust Intelligence","07/01/2013","09/24/2013","Zhuowen Tu","CA","University of California-San Diego","Continuing Grant","Kenneth Whang","06/30/2015","$197,514.00","","zhuowen.tu@gmail.com","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7495","1045, 1187, 9215, HPCC","$0.00","<br/>Designing automated algorithms to extract and analyze anatomical brain structures from neuro-images is of significant scientific and clinical importance in detecting abnormal brain patterns, analyzing various brain diseases, and studying the brain growth.<br/><br/>This project will develop a general statistical modeling/computing framework to perform 3D holistic brain image understanding.  The framework emphasizes rigorous, efficient, and effective learning-based statistical models to integrate the complex appearances, varying 3D shapes, and the large spatial configuration of anatomical brain structures.<br/><br/>Implicit models through discriminative approaches have the advantages of fusing a large amount of information and obtaining decisions quickly. Explicit models through generative approaches can directly represent the information and thus, better explain the structure and model the transformation and scale change. The PI explores harmonic relationships between discriminative and generative models for 3D image parsing by combining implicit and explicit models along several directions: (1) learning-based models with rich appearance, and implicit shape and context; (2) integrating skeleton with surfaces for 3D shapes; (3) effective 3D shape representation and similarity measure; (4) component-based simultaneous registration and segmentation.<br/><br/>This research will contribute to automating the process of extracting a large number of anatomical structures, and enhancing the shape analysis needed for detecting brain diseases, monitoring health conditions, studying drug effects, and discovering brain functions. The scope of the proposed model goes beyond medical image analysis and can be applied in other problems of statistical modeling/computing, computer vision, multi-variate labeling in machine learning. <br/><br/>"
"1332566","Enriching Computing Curricula through HPC Teaching and Research","HRD","Hist Black Colleges and Univ, Information Technology Researc, Special Projects - CNS","07/01/2013","09/23/2019","Suxia Cui","TX","Prairie View A & M University","Standard Grant","Claudia Rankins","06/30/2020","$430,783.00","Yonghui Wang, Lin Li, Lei Huang","sucui@pvamu.edu","P.O. Box 519","Prairie View","TX","774460519","9362611689","EHR","1594, 1640, 1714","041Z, 9178","$0.00","The goal of this project at Prairie View A&M University is to improve undergraduate computing education through enhanced courses and research opportunities that incorporate high performance computing (HPC) training that will stimulate students' interest in computing and strengthen their computational problem-solving skills. This project leverages faculty expertise, institutional commitment, industry and federal support to elevate students' computing-related skills and education.<br/><br/>This project is a collaboration of faculty members from three departments - Electrical & Computer Engineering, Computer Science, and Engineering Technology - to develop a diverse high performance computing environment and use it to improve HPC-related education and training at the institution.  Six courses will be redesigned to include both embedded and cluster HPC platforms and integrate research topics of the project faculty members. In addition, faculty members will develop new courses in the areas of parallel computing, machine learning, and computer vision. The proposed HPC platforms can easily be expanded to support other disciplines in science technology, engineering, and mathematics. <br/><br/>This project addresses a national need to provide trained graduates in the computing sciences in order to help the U.S. retain its technological competitiveness. Almost 500 undergraduate students, the majority of whom are African American, will be impacted during the three project years. Each year, high school teachers and university faculty members will receive training.  Research data, source code, and curriculum materials generated by this project will be posted online and disseminated in conferences and publications for the research and educational community."
"1337866","MRI Collaborative: Development of iRehab, an Intelligent Closed-Loop Instrument for Adaptive Rehabilitation","CNS","Information Technology Researc","10/01/2013","09/16/2013","Margrit Betke","MA","Trustees of Boston University","Standard Grant","Rita Rodriguez","09/30/2017","$200,109.00","","betke@cs.bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","1640","1189","$0.00","Proposal #: 13-38118 Collaborative                            Proposal #: 13-37866 <br/>PI(s): Makedon, Fillia S collaboration with                   PI(s): Betke, Margrit <br/>Athitsos, Vassilis; Gatchel, Robert J; Huang, Heng; Romero-Ortega, Mario I <br/>Institution: University of Texas-Arlington collaboration with Institution: Boston Univeristy <br/>Title: MRI/Dev :Collab Dev. of iRehab, an Intelligent Closed-loop Instrument for Adaptive Rehabilitation <br/>Project Proposed: <br/>This project, developing of an instrument referred to as iRehab, aims to enable personalized rehabilitation therapy for individuals suffering from brain injury, motor disabilities, cognitive impairments, and/or psychosocial symptoms. The instrument, a modular rehabilitation device, in its simplest form consists of a computer, a camera, and adaptive software for assessment and training of cognitive functions. In its final, most complex form, the instrument will integrate data from a 4-degree-of-freedom robotic-arm with gimbals and torque sensing, a Kinect sensor, multiple cameras, an eye-tracking device, a touch screen, a microphone, and an fNIRS brain imaging sensor. <br/>The instrument will be developed in two phases. In the first phase, the investigators develop a Barrett robot arm. In the second phase, the instrument will extend to a Kinect sensor, multiple cameras, an eye-tracking device, and related low-cost components, along with the assessment software for assessing motor function and cognitive, emotional, and personality functioning. <br/>iRehab consists integrates multidisciplinary methodologies and sensors to assess and assist the cognitive and physical rehabilitation of persons affected by various impairments. This work highly interdisciplinary work follows a cyber-physical approach. It provides new research opportunities across the fields of human-centered computing, computer vision, assistive technology, robotics, machine learning, and neuroimaging. This work advances research in human brain activity mapping, personalized medicine, and big data. <br/>Broader Impacts: <br/>The proposed instrument exhibits potential for large broader impact as it directly contributes to future healthcare and human wellbeing improving accessibility to affordable rehabilitation for a broad range of patients. The instrument is likely to accelerate the recovery of a large spectrum of injuries and diseases including those causing motor, neurological, and cognitive disorders. An education plan includes course development, internships, workshops and tutorials, and an on-line resource center. In addition to many educational impacts, impact will be felt on the fundamental research in the areas addressed."
"1320410","Integrating low-level speech features into a model of speech perception","BCS","Perception, Action & Cognition, Robust Intelligence","09/01/2013","09/16/2013","Naomi Feldman","MD","University of Maryland College Park","Standard Grant","Betty Tuller","08/31/2016","$179,724.00","","nhf@umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","SBE","7252, 7495","7252, 7495, 9178, 9251","$0.00","Why are humans so much better than machines at recognizing speech?  This research aims to measure differences between humans and machines in how they compute similarities between sounds.  A computational model of speech perception will be trained on speech production data, using several types of features that are typically used in speech recognitions systems.  It will then be tested on its ability to predict human listeners' responses in speech sound discrimination tasks.  Results are expected to provide information about how the speech that listeners hear shapes their perception of sounds, as well as how well the information used by automatic speech recognition systems matches the information used by human listeners.<br/><br/>By allowing us to compare how humans and speech recognition systems use information when perceiving speech, this research will provide a tool that can help make speech recognition systems more human-like.  Reverse engineering human perception can improve the way these systems generalize to new dialects, talkers, and noise conditions.  This has the potential to facilitate the construction of systems for low-resource languages, broadening the impact of speech recognition technologies.<br/><br/>[Supported by SBE/BCS/PAC and CISE/IIS/RI]"
"1338118","MRI Collaborative: Development of iRehab, an Intelligent Closed-Loop Instrument for Adaptive Rehabilitation","CNS","INFORMATION TECHNOLOGY RESEARC, SPECIAL PROJECTS - CISE","10/01/2013","04/26/2018","Fillia Makedon","TX","University of Texas at Arlington","Standard Grant","Rita Rodriguez","09/30/2018","$879,890.00","Heng Huang, Vassilis Athitsos, Mario Romero-Ortega, Robert Gatchel","makedon@cse.uta.edu","701 S Nedderman Dr, Box 19145","Arlington","TX","760190145","8172722105","CSE","1640, 1714","1189, 9178, 9251","$0.00","Proposal #: 13-38118  Collaborative  Proposal #: 13-37866<br/>PI(s):  Makedon, Fillia S    PI(s):  Betke, Margrit<br/>Athitsos, Vassilis; Gatchel, Robert J; Huang, Heng; Romero-Ortega, Mario I<br/>Institution:      University of Texas-Arlington              INstitution:     Boston Univeristy    <br/>Title:  MRI/Dev :Collab Dev. of iRehab, an Intelligent Closed-loop Instrument for Adaptive Rehabilitation<br/>Project Proposed:<br/>This project, developing of an instrument referred to as iRehab, aims to enable personalized rehabilitation therapy for individuals suffering from brain injury, motor disabilities, cognitive impairments, and/or psychosocial symptoms. The instrument, a modular rehabilitation device, in its simplest form consists of a computer, a camera, and adaptive software for assessment and training of cognitive functions. In its final, most complex form, the instrument will integrate data from a 4-degree-of-freedom robotic-arm with gimbals and torque sensing, a Kinect sensor, multiple cameras, an eye-tracking device, a touch screen, a microphone, and an fNIRS brain imaging sensor. <br/>The instrument will be developed in two phases. In the first phase, the investigators develop a Barrett robot arm. In the second phase, the instrument will extend to a Kinect sensor, multiple cameras, an eye-tracking device, and related low-cost components, along with the assessment software for assessing motor function and cognitive, emotional, and personality functioning. <br/>iRehab consists integrates multidisciplinary methodologies and sensors to assess and assist the cognitive and physical rehabilitation of persons affected by various impairments. This work highly interdisciplinary work follows a cyber-physical approach. It provides new research opportunities across the fields of human-centered computing, computer vision, assistive technology, robotics, machine learning, and neuroimaging. This work advances research in human brain activity mapping, personalized medicine, and big data.<br/>Broader Impacts:  <br/>The proposed instrument exhibits potential for large broader impact as it directly contributes to future healthcare and human wellbeing improving accessibility to affordable rehabilitation for a broad range of patients. The instrument is likely to accelerate the recovery of a large spectrum of injuries and diseases including those causing motor, neurological, and cognitive disorders. An education plan includes course development, internships, workshops and tutorials, and an on-line resource center. In addition to many educational impacts, impact will be felt on the fundamental research in the areas addressed."
"1266183","I/UCRC:  Identification Technology Research (CITeR) - UB Site","CNS","IUCRC-Indust-Univ Coop Res Ctr, , , , , ","06/01/2013","08/27/2018","Venugopal Govindaraju","NY","SUNY at Buffalo","Continuing Grant","Behrooz Shirazi","05/31/2019","$407,852.00","Srirangaraj Setlur, Ifeoma Nwogu","govind@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","5761, N510, O317, O530, Q287, R321","170E, 5761, 8039, 8237","$0.00","Center for Identification Technology Research (CITeR) <br/><br/>1266183 SUNY at Buffalo; Venugopal Govindaraju <br/><br/>The proposed site, SUNY at Buffalo, requests funding to establish a partner site of the Center for Identification Research (CITeR) currently comprised of the Clarkson University (lead institution), West Virginia University and the University of Arizona as research partners. <br/><br/>The proposed site will advance the science of biometric technologies for both civilian and homeland security applications by integrating pattern recognition and machine learning algorithms with sensors technology. SUNY at Buffalo site will bring together faculty expertise, not only in core biometric modalities such as fingerprints and face recognition, and soft biometrics such as gender, age, mannerisms, and emotion, but also in related areas such as advanced computer vision, robotics, cryptography, and theoretical computer science that will allow CITeR to pursue research in the development of innovative biometric applications. CITeR affiliates will also benefit from being able to benchmark and assess the effectiveness of various hard and soft biometric technologies in real applications. The proposed site's faculty strength in Chemistry, Physics and Optics will also help CITeR pursue novel modulaties such as odor biometrics and better sensors to assist in identification technologies. <br/><br/>The proposed site will further CITeR's objective to serve as a comprehensive academic center serving the growing identification technology research, undergraduate and graduate education, and outreach needs of the public and private sectors. This will be accomplished by providing a mechanism for the funding of pooled interdisciplinary faculty and student talent to address the challenges in identification technology. Under the auspices of BEAM (Buffalo Area Engineering Awareness for Minorities) the proposed site plans to attract the participation of local high school and undergraduate students using innovative events built around biometrics applications that incorporate treasure hunts and other fun elements to educate and interest them in STEM areas. The faculty at SUNY Buffalo are members of the NSF-AGEP (The Alliance for Graduate Education and the Professoriate) program whose goal is to significantly increase the number of underrepresented minorities obtaining graduate degrees in STEM areas. CITeR meetings will also serve as an excellent opportunity for students to interact with potential employers to understand challenges in the field."
"1340151","I-Corps:  Real-Time Traffic Congestion Detection from Surveillance Videos","IIP","I-Corps","05/01/2013","04/17/2013","John Cavazos","DE","University of Delaware","Standard Grant","Rathindra DasGupta","10/31/2013","$50,000.00","","cavazos@cis.udel.edu","210 Hullihen Hall","Newark","DE","197160099","3028312136","ENG","8023","9150","$0.00","This I-Corps team proposes the development of a real-time traffic congestion detection system from surveillance videos. The proposed technology for detecting traffic congestion involves two major components: (a) vehicle detection and tracking; and (b) event classification. The research team will use computer vision techniques for detecting vehicles in single frames. By analyzing consecutive frames, vehicle speed and relative locations will be estimated. For the classification of events, the team will use their expertise in graph kernels and machine learning. A sequence of consecutive frames can be used to create a graph, where the nodes represent vehicles labeled with local features, such as speed and location. Neighboring nodes in the graph are connected by edges labeled with the distance between their respective vehicles. A fast kernel function for graphs will be developed and used by a binary classifier which will be trained for the task of recognizing traffic congestion. As an extension, a multi-class classifier can also be trained to distinguish different types of traffic events, such as, high, moderate, or low traffic congestion, accident, or normal traffic.<br/><br/>The proposed product is a great opportunity for transforming state-of-the-art computational techniques into new technologies that can directly have societal and commercial impacts. A real time traffic congestion detection system has as their primary targets government agencies and departments across the nation. With a real time detector of congestion, one will be able to track simultaneously hundreds or thousands of cameras at the same time, discovering incidents that can enhance traffic management. Decision makers can use the proposed product for dynamic traffic assignment, incident discovery, and improved management of evacuation systems. Society may be impacted as a whole with savings in commuting time and delay costs. This product can also be used for developing web applications or apps for mobile devices, bringing real time traffic information to motorists."
"1312718","I-Corps:  Wake-Up-Word Speech Recognition Software Toolkit","IIP","I-Corps","01/15/2013","01/06/2013","Veton Kepuska","FL","Florida Institute of Technology","Standard Grant","Rathindra DasGupta","06/30/2014","$50,000.00","Richard Fox, Jacob Zurasky","vkepuska@fit.edu","150 W UNIVERSITY BLVD","MELBOURNE","FL","329016975","3216748000","ENG","8023","","$0.00","The technology developed through this project is a specific way of using speech recognition to ""trigger"" required and necessary action from a computing system, using only speech instead of push-to-talk buttons. Although speech recognition software is widely available, this technology has the potential to advance the industry. The project team has worked on this technology for over a decade. The proposed system will be used as a software tool-kit for software developers developing applications that fall into one of two categories: 1) the access to the system is restricted or; 2) access to the system is prohibited.<br/><br/>This software tool-kit has the potential to make an impact in helping directly or indirectly impaired users such as quadriplegics, surgeons, operators wearing HAZMAT suits, remotely operated vehicles, etc., where the same operation would be very difficult and/or very expensive to perform or not possible at all."
"1311085","NSF East Asia and Pacific Summer Institute (EAPSI) for FY 2013 in China","OISE","EAPSI","06/01/2013","05/16/2013","Daniel Brenner","AZ","Brenner Daniel S","Fellowship","Anne L. Emig","05/31/2014","$5,070.00","","","","Tucson","AZ","857195300","","O/D","7316","5978, 7316, 9200","$0.00","This action funds Daniel Brenner of the University of Arizona to conduct a research project in the Social, Behavioral and Economic Sciences area during the summer of 2013 at the Chinese Academy of Social Sciences in Beijing. The project title is ""Mandarin Tone Perception in Conversational and Read Speech."" The host scientist is Dr. HU Fang.<br/><br/>In Mandarin, words are distinguished not only by segments ([ba] 'to pull out' vs. [pa] 'to climb'), but also by tone, the pattern of vocal pitch overlaid on the syllable ([pa] with rising pitch, 'to climb'; [pa] with falling pitch, 'to fear'). This project describes the relationship between tones as produced in conversation to those in careful speech, and how those differences affect listeners' perception of tones in the two speech styles. In the first phase of the project, recordings are made of Mandarin speakers in conversation and reading a list of words, and the acoustics of the two kinds of tone productions will be compared. In a second phase, Mandarin-speaking subjects perform a perception task on those words, either with full acoustic information, or with a filtered version that removes segmental detail from the recordings. In this way, a measurement of the contributions of tone and segmental information to the identification of words can be made. Further understanding of tone and conversational speech enable developments in second language Mandarin pedagogy, where a great deal of emphasis is placed on learning to produce and detect tones, an endeavor that does not come easily to many American students. The details of tonal realizations are also vital for progress in human language technologies such as automatic speech recognition, where the ultimate goal is to program computers to understand naturally produced speech, as in conversation.<br/><br/>Broader impacts of an EAPSI fellowship include providing the Fellow a first-hand research experience outside the U.S.; an introduction to the science, science policy, and scientific infrastructure of the respective location; and an orientation to the society, culture and language. These activities meet the NSF goal to educate for international collaborations early in the career of its scientists, engineers, and educators, thus ensuring a globally aware U.S. scientific workforce. Furthermore, publication of the results following completion of the study will disseminate the findings widely to linguists and researchers in related fields."
"1305365","CI-ADDO-NEW: Collaborative Research: The Speech Recognition Virtual Kitchen","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","09/01/2013","06/04/2013","Florian Metze","PA","Carnegie-Mellon University","Standard Grant","Tatiana Korelsky","08/31/2017","$542,371.00","","fmetze@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7359","7359","$0.00","The Speech Recognition Virtual Kitchen <br/><br/>Performing successful research on end-to-end speech processing problems requires the integration of many individual tools (e.g. for data cleaning, acoustic model training, language modeling, data analysis, real-time audio, decoding, parsing, synthesis, etc.). It is difficult for new researchers to get started in the field, simply because a typical lab environment consists of a hodgepodge of tools suited to a particular computing set-ups. This environment is hard to recreate, because few people are experts in the theory and practice of all these fields, and can debug and replicate experiments from scratch. <br/><br/>This research infrastructure project creates a ""kitchen"" environment based on Virtual Machines (VMs) to promote community sharing of research techniques, and provides solid reference systems as a tool for education, research, and evaluation. We liken VMs to a ""kitchen"" because they provide an environment into which one can install ""appliances"" (e.g., toolkits), ""recipes"" (scripts for creating state-of-the art systems using these tools), and ""ingredients"" (spoken language data). The kitchen even holds ""reference dishes"" in the form of complete experiments with baseline runs, log-files, etc., together with all that is needed to recreate and modify them. <br/><br/>The project is developing a community and repository by (a) building pilot VMs, (b) engaging the community in using and continuing to develop them on its own, and (c) evaluating the impact of providing VMs for education and research. We envision researchers as well as students downloading a VM, reproducing the baseline experiment, implementing changes, posting their results in the community, discussing with other users who have worked on the same VM, merging improvements back into the VM, which get re-distributed, and finally publishing easily reproducible results. Work with curriculum and project development will support the creation of engaging activities to specifically encourage students at undergraduate and graduate levels."
"1305319","CI-ADDO-NEW: Collaborative Research: The Speech Recognition Virtual Kitchen","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","09/01/2013","06/04/2013","Eric Fosler-Lussier","OH","Ohio State University","Standard Grant","Tatiana Korelsky","08/31/2017","$382,082.00","","fosler@cse.ohio-state.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","7359","7359","$0.00","The Speech Recognition Virtual Kitchen <br/><br/>Performing successful research on end-to-end speech processing problems requires the integration of many individual tools (e.g. for data cleaning, acoustic model training, language modeling, data analysis, real-time audio, decoding, parsing, synthesis, etc.). It is difficult for new researchers to get started in the field, simply because a typical lab environment consists of a hodgepodge of tools suited to a particular computing set-ups. This environment is hard to recreate, because few people are experts in the theory and practice of all these fields, and can debug and replicate experiments from scratch. <br/><br/>This research infrastructure project creates a ""kitchen"" environment based on Virtual Machines (VMs) to promote community sharing of research techniques, and provides solid reference systems as a tool for education, research, and evaluation. We liken VMs to a ""kitchen"" because they provide an environment into which one can install ""appliances"" (e.g., toolkits), ""recipes"" (scripts for creating state-of-the art systems using these tools), and ""ingredients"" (spoken language data). The kitchen even holds ""reference dishes"" in the form of complete experiments with baseline runs, log-files, etc., together with all that is needed to recreate and modify them. <br/><br/>The project is developing a community and repository by (a) building pilot VMs, (b) engaging the community in using and continuing to develop them on its own, and (c) evaluating the impact of providing VMs for education and research. We envision researchers as well as students downloading a VM, reproducing the baseline experiment, implementing changes, posting their results in the community, discussing with other users who have worked on the same VM, merging improvements back into the VM, which get re-distributed, and finally publishing easily reproducible results. Work with curriculum and project development will support the creation of engaging activities to specifically encourage students at undergraduate and graduate levels."
"1305215","CI-ADDO-NEW: Collaborative Research: The Speech Recognition Virtual Kitchen","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","09/01/2013","05/08/2018","Dean Kelley","MN","Minnesota State University, Mankato","Standard Grant","Marilyn McClure","08/31/2018","$145,364.00","","dean.kelley@mnsu.edu","Research and Sponsored Programs","Mankato","MN","560016066","5073895275","CSE","7359","7359","$0.00","The Speech Recognition Virtual Kitchen <br/><br/>Performing successful research on end-to-end speech processing problems requires the integration of many individual tools (e.g. for data cleaning, acoustic model training, language modeling, data analysis, real-time audio, decoding, parsing, synthesis, etc.). It is difficult for new researchers to get started in the field, simply because a typical lab environment consists of a hodgepodge of tools suited to a particular computing set-ups. This environment is hard to recreate, because few people are experts in the theory and practice of all these fields, and can debug and replicate experiments from scratch. <br/><br/>This research infrastructure project creates a ""kitchen"" environment based on Virtual Machines (VMs) to promote community sharing of research techniques, and provides solid reference systems as a tool for education, research, and evaluation. We liken VMs to a ""kitchen"" because they provide an environment into which one can install ""appliances"" (e.g., toolkits), ""recipes"" (scripts for creating state-of-the art systems using these tools), and ""ingredients"" (spoken language data). The kitchen even holds ""reference dishes"" in the form of complete experiments with baseline runs, log-files, etc., together with all that is needed to recreate and modify them. <br/><br/>The project is developing a community and repository by (a) building pilot VMs, (b) engaging the community in using and continuing to develop them on its own, and (c) evaluating the impact of providing VMs for education and research. We envision researchers as well as students downloading a VM, reproducing the baseline experiment, implementing changes, posting their results in the community, discussing with other users who have worked on the same VM, merging improvements back into the VM, which get re-distributed, and finally publishing easily reproducible results. Work with curriculum and project development will support the creation of engaging activities to specifically encourage students at undergraduate and graduate levels."
"1330937","Workshop: How the Brain Accommodates Variability in Linguistic Representations; July, 2013 - University of Michigan","BCS","Linguistics, Perception, Action & Cognition, Robust Intelligence","07/01/2013","06/19/2013","T. Florian Jaeger","NY","University of Rochester","Standard Grant","William Badecker","12/31/2014","$11,903.00","Victor Ferreira","fjaeger@bcs.rochester.edu","518 HYLAN, RC BOX 270140","Rochester","NY","146270140","5852754031","SBE","1311, 7252, 7495","1311, 7252, 7495, 7556","$0.00","When we listen, we rapidly and reliably decode speakers' intentions and we mostly do so independently of whom were are talking to. Yet, anyone who has interacted with an automated speech recognition system (e.g., while booking a flight) is painfully aware that speech recognition is a computationally hard problem: although we hardly ever become aware of it, the physical signal corresponding to, for example, one speaker's ""b"" can be identical to another speaker's ""p"", making it hard for computers to distinguish between them. How then does the human brain accomplish this task with such apparent ease? <br/><br/>This NSF funded workshop brings together researchers from computer sciences, linguistics, and the cognitive sciences to discuss and investigate how the brain achieves robust language understanding despite variability. The invited speakers are internationally-known experts. Representatives from both industry and academia will present on the state of the art in automated speech recognition, implicit learning during language understanding, and the neural systems underlying speech perception. The workshop will take place in conjunction with the 2013 Linguistic Society of America's Summer Institute--the largest international linguistics summer school--and will thereby provide training to a large number of young language researchers."
"1320892","RI: Small: Speaker Independent Acoustic-Articulator Inversion for Pronunciation Assessment","IIS","LINGUISTICS, ROBUST INTELLIGENCE","08/01/2013","12/21/2016","Mike Johnson","WI","Marquette University","Continuing grant","Tatiana Korelsky","07/31/2018","$481,643.00","Jeffrey Berry","mike.johnson@uky.edu","P.O. Box 1881","Milwaukee","WI","532011881","4142887200","CSE","1311, 7495","7495, 7923, 9251","$0.00","To support an integrated global economy, it is essential that people of all backgrounds be able to function together effectively despite language barriers, and development of Computer Aided Language Learning (CALL) and accent modification tools is a key part of making this possible. In order to support effective learning and provide specific, useful pronunciation feedback to users, systems for pronunciation correction must be able to capture and accurately describe errors in articulation. Accurate acoustic-to-articulator inversion, the estimation of articulatory trajectories from an acoustic signal, has the potential to significantly improve the accuracy and specificity of such feedback to language learners, and enhance methods for in-depth study of both native speaker and second language learner articulatory patterns.<br/><br/>This research addresses the problem of robust speaker-independent acoustic-to-articulator inversion, which is a challenging problem due to the complexity of articulation patterns and significant inter-speaker differences. To overcome this difficulty, a novel speaker-independent inversion approach called Parallel Reference Speaker Weighting is being developed, which uses parallel acoustic-articulator adaptation to create speaker-specific models for new speakers without kinematic training data, represented in a normalized articulatory working space. The new approach is being evaluated on the Marquette University EMA-MAE Corpus of parallel acoustic / 3-D electromagnetic articulography data including both American English and Mandarin Accented English speakers. <br/><br/>The primary impact of this work focuses on the improvement of pronunciation assessment and accent modification systems, with potential for contribution to numerous other speech technologies, including speech recognition, speech coding, and audio and video synthesis."
"1319941","EAGER: Aggregating Online Information in Science","IIS","ROBUST INTELLIGENCE","09/15/2013","09/10/2013","Bruce Buchanan","CA","Association for the Advancement of Artificial Intelligence","Standard Grant","Hector Munoz-Avila","08/31/2015","$71,832.00","","buchanan@cs.pitt.edu","2275 E BAYSHORE RD STE 160","East Palo Alto","CA","943033224","6503283123","CSE","7495","7495, 7916","$0.00","This project continues the development of the AITopics information portal, a service to the Artificial Intelligence research community and an educational resource for the general public.  The portal can be found at www.aitopics.org. This site was originally conceived as a compendium of introductory and historical articles about artificial intelligence for use by the AI community as well as the general public. The initial prototype was a manual effort conducted by volunteers at the Association for the Advancement of Artificial Intelligence (AAAI). Following this, an NSF-funded effort was undertaken in 2012 to reduce the time required of volunteers without reducing the quality of information provided. This project advances the sophistication of the portal by  automating the time-consuming process of selecting content from the literature to post on the website, and by enhancing a deployed AI news finder program employed to feed the site with fresh content. The ultimate goal of this work is to build a generally useful content management framework that would be applicable to such outreach effort across all sciences. <br/><br/>The project activities include A) development of the NewsFinder program to browse online AI journals as well as current periodicals to find interesting overview articles; B) creation of additional content management technology including tools for user interaction and feedback; creation of meta-data for search engines, creation of summary descriptions; automatically learning criteria for interesting items; and creation of tools to identify new topics as the field changes; C) continued curating of online versions of classic books and papers, including scanning material only available in hard copy; D) extension of NewsFinder to find articles describing new applications in each Applications area; E) as well as to provide useful information to practitioners in a new area of technology, including classification of IAAI papers by industry and type of problem; and F) expansion of the use of social media and mobile devices to refine and deliver information."
"1319365","RI: Small: Reinforcement Learning with Predictive State Representations","IIS","Robust Intelligence","08/01/2013","07/24/2014","Satinder Baveja","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Rebecca Hwa","07/31/2018","$450,000.00","","baveja@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7495","7495, 7923","$0.00","Like animals and humans, artificial autonomous agents that are able to predict short-term and long-term consequences of their actions can then plan their behavior, act more intelligently, and achieve greater reward. Agents that can learn such predictive models from experience can be more robust in their intelligence than agents that rely on pre-built models. The PI and graduate students are focused on the particularly challenging but natural case where observations from the agent's sensors far in the past can continue to influence the predictions of consequences of actions long into the future. (For example, the observation of where you park the car in the morning will help predict where you will see the car later in the day.) There are two broad classes of approaches to learning predictive models in such 'partially observable' settings. Finite-history models use short-term history of observations to predict future observations conditioned on actions; these are fast to learn but are limited because they cannot capture the effects of long-term history. Latent-variable models can capture the effects of long-term history by positing hidden or latent variables that capture the true state of the environment (e.g., the location of the car), but such models are difficult to learn because the latent variables have to be inferred from data. <br/><br/>This project builds on previous work by the PI and others on a third approach, called Predictive State Representations (or PSRs), in which the agent maintains predictions of future observations conditioned on future actions as a summary-representation of history; these models can both be fast to learn and capture the effect of long-term history. This project develops new PSR-based methods and algorithms for hierarchical models, rich-feature-based models, and local and modular models.  The project applies the new methods to challenging applications from active perception and robotics. In addition, theoretical understanding of these richer and newer methods will be developed. Altogether the project significantly expands the applicability of PSR-methods as well as their theoretical foundations and algorithms. <br/><br/>Broader Impacts: New methods that allow artificial agents to robustly build predictive models would advance the state of knowledge across the fields of artificial intelligence, reinforcement learning, control, operations research, psychology, and neuroscience. The PI is co-leading an effort to create a new undergraduate degree in Data Sciences at the University of Michigan to be jointly managed by Computer Science & Engineering and Statistics. This future degree as well as other current undergraduate research programs will be targeted to recruit, mentor, and train students for this project."
"1343599","Doctoral Mentoring Consortium at at the 23rd International Joint Conference on Artificial Intelligence (IJCAI 2013)","IIS","ROBUST INTELLIGENCE","07/01/2013","06/20/2013","Maria Gini","MN","University of Minnesota-Twin Cities","Standard Grant","Hector Munoz-Avila","06/30/2014","$25,000.00","","gini@cs.umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","7495","7484, 7495, 7556","$0.00","This grant supports student travel for select students participating in the Doctoral Consortium at the International Joint Conference on Artificial Intelligence (IJCAI 2013), that will be held in Beijing, China, August 3-9, 2013.  The biannual IJCAI conference is the premier international conference spanning all topics in AI across a fully international research community.  <br/><br/>This consortium is oriented on research and career development for students who have identified their PhD topics and are just embarking on that independent research.  This consortium is covering topics that are important and relevant to the success of these students, including, ""funding and strings attached to them, use of subjects in research (including IRB), data collection, maintaining privacy and integrity of data, authorship (single and multi-author submissions, ghost writing), plagiarism and self-plagiarism, proper use of citations, reviewing of papers and proposals, and how to get the best out of attending a conference.""<br/><br/>Student participation in this conference has direct impact through encouraging promising U.S. students to engage in internationally-competitive research.  This program will also enhance the broader scientific community through exposure to emerging research topics.  IJCAI is the major international conference that will figure prominently in the research careers of these young investigators.  Students gain valuable research insights from the exchange of technical ideas in this broader venue.  In the process, they make valuable connections with potential collaborators from around the world.  In addition, students participate in the main IJCAI conference activities.  This allows them to attend presentations of the leading research in AI, as well as avail themselves of the available tutorials, workshops, and demonstrations."
"1313847","WORKSHOP: Student Consortium at the 2013 ACM Conference on Intelligent User Interfaces","IIS","HCC-Human-Centered Computing","02/01/2013","01/17/2013","Henry Lieberman","MA","Massachusetts Institute of Technology","Standard Grant","Ephraim Glinert","01/31/2015","$10,368.00","","lieber@media.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7367","7367, 7556","$0.00","This is funding to provide financial support for 10 graduate students (at least 8 of them registered at U.S. universities and working towards either their Master's degree or a Doctorate) to attend the 2013 International Conference on Intelligent User Interfaces (IUI 2013), to be held March 19-22 in Santa Monica, California, as participants in a special Student Consortium (workshop), as presenters in the main conference, and as attendees at the conference for general training purposes.  Sponsored by ACM, the annual IUI conferences represent the growing interest in next-generation intelligent and interactive user interfaces; they are the premier forum where researchers from academia and industry, who work at the intersection of Human-Computer Interaction (HCI) and Artificial Intelligence (AI), come together to exchange complementary insights and to present and discuss outstanding research and applications whose goal is to make the computerized world a more amenable place.  Unlike traditional AI the focus is not so much on making the computer smart all by itself, but rather on making the interaction between computers and people smarter.  Unlike traditional HCI, there is a focus on solutions that involve large amounts of knowledge and emerging technologies such as natural language understanding, brain computer interfaces, and gesture recognition. To this end, IUI encourages contributions not only from computer science but also from related fields such as psychology, cognitive science, computer graphics, the arts, etc.  IUI 2013 will be the 16th conference in the series; topics of interest this year include: intelligent interactive interfaces, systems, and devices; ubiquitous interfaces; smart environments and tools; human-centered interfaces; mobile interfaces; multimodal interfaces; pen-based interfaces; spoken and natural language interfaces; conversational interfaces; affective and social interfaces;  tangible interfaces; collaborative multi-user interfaces; adaptive interfaces; sensor-based interfaces; user modeling and interaction with novel interfaces and devices; interfaces for personalization and recommender systems; interfaces for plan-based systems; interfaces that incorporate knowledge- or agent-based approaches; help interfaces for complex tasks; example- and demonstration-based interfaces; interfaces for intelligent generation and presentation of information; intelligent authoring systems; synthesis of multimodal virtual characters and social robots; interfaces for games and entertainment; for learning-based interactions and for health informatics; empirical studies and evaluations of IUI interfaces; and new approaches to designing intelligent user interfaces.  More information about the conference is available online at http://iuiconf.org/. <br/><br/>The IUI 2013 Student Consortium will build on the success of the first such event last year.  The heart of the Consortium will be a full-day workshop on March 19, immediately preceding the conference, and will be structured to give student trainees exposure to their new research community by giving a 20-30 presentation on their work and receiving feedback from peers and a panel of 4-5 senior researchers.  Group lunch and dinner will encourage social interaction among the student cohort and informal personal interaction with the mentors.  The students' work will also be featured during the main conference in a poster session, where they will gain additional experience explaining their work to others in the field.  The IUI conference organizers will pay for audio-visual services, two coffee breaks, and space for accommodating attendees in the student session; no funds are requested for these items from NSF.<br/><br/>Broader Impacts:  This funding will enable attendance at the IUI conference by students who might otherwise be unable to do so for financial reasons.  It will enhance the educational experience of funded participants, by bringing them into contact with leading researchers in the field and by exposing them to the lively discussion during the course of the conference that often leads to opportunities for career advancement.  The quality of the conference itself will be enhanced as well, thanks to a broadening of the base of institutions represented and increased diversity of participants.  The rich exchange of ideas at IUI has previously proven to be a valuable source of ideas for future research, as well as leading to collaborative efforts; this funding will extend the opportunities for collaboration and provide intellectual stimulus to programs that have previously sent few or no representatives to this conference.  The organizing committee has undertaken to proactively recruit student participants from schools that have not traditionally been well represented in the IUI community.  Women, minority students, the disabled, and veterans all will be encouraged to participate.  To further assure diversity, no more than one student will be accepted from any given institution."
"1415879","WORKSHOP: Student Consortium at the 2014 ACM Conference on Intelligent User Interfaces","IIS","HCC-Human-Centered Computing","12/15/2013","12/18/2013","Joyce Chai","MI","Michigan State University","Standard Grant","Ephraim Glinert","02/29/2016","$19,250.00","","chaijy@umich.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","CSE","7367","7367, 7556","$0.00","This is funding to provide financial support for 10 graduate students (all from U. S. universities and working towards either their Master's degree or a Doctorate) to attend the 2014 International Conference on Intelligent User Interfaces (IUI 2014), to be held February 24-27 in Haifa, Israel, as participants in a special Student Consortium (workshop), as presenters in the main conference, and as attendees at the conference for general training purposes.  Sponsored by ACM, the annual IUI conferences represent the growing interest in next-generation intelligent and interactive user interfaces; they are the premier forum where researchers from academia and industry, who work at the intersection of Human-Computer Interaction (HCI) and Artificial Intelligence (AI), come together to exchange complementary insights and to present and discuss outstanding research and applications whose goal is to make the computerized world a more amenable place.  Unlike traditional AI the focus is not so much on making the computer smart all by itself, but rather on making the interaction between computers and people smarter.  Unlike traditional HCI, there is a focus on solutions that involve large amounts of knowledge and emerging technologies such as natural language understanding, brain computer interfaces, and gesture recognition. To this end, IUI encourages contributions not only from computer science but also from related fields such as psychology, cognitive science, computer graphics, the arts, etc.  IUI 2014 will be the 17th conference in the series; topics of interest this year include: intelligent interactive interfaces, systems, and devices; ubiquitous interfaces; smart environments and tools; human-centered interfaces; mobile interfaces; multimodal interfaces; pen-based interfaces; spoken and natural language interfaces; conversational interfaces; affective and social interfaces;  tangible interfaces; collaborative multi-user interfaces; adaptive interfaces; sensor-based interfaces; user modeling and interaction with novel interfaces and devices; interfaces for personalization and recommender systems; interfaces for plan-based systems; interfaces that incorporate knowledge- or agent-based approaches; help interfaces for complex tasks; example- and demonstration-based interfaces; interfaces for intelligent generation and presentation of information; intelligent authoring systems; synthesis of multimodal virtual characters and social robots; interfaces for games and entertainment; for learning-based interactions and for health informatics; empirical studies and evaluations of IUI interfaces; and new approaches to designing intelligent user interfaces.  More information about the conference is available online at http://iuiconf.org/. <br/><br/>The IUI 2014 Student Consortium will build on the success of the previous two such events.  The heart of the Consortium will be a full-day workshop on February 24, immediately preceding the conference, and will be structured to give student trainees exposure to their new research community by giving a 20-30 presentation on their work and receiving feedback from peers and a panel of senior researchers.  Group lunch and dinner will encourage social interaction among the student cohort and informal personal interaction with the mentors.  The students' work will also be featured during the main conference in a poster session, where they will gain additional experience explaining their work to others in the field.  The IUI conference organizers will pay for audio-visual services, two coffee breaks, and space for accommodating attendees in the student session; no funds are requested for these items from NSF.<br/><br/>Broader Impacts:  This funding will enable attendance at the IUI conference by students who might otherwise be unable to do so for financial reasons.  It will enhance the educational experience of funded participants, by bringing them into contact with leading researchers in the field and by exposing them to the lively discussion during the course of the conference that often leads to opportunities for career advancement.  The quality of the conference itself will be enhanced as well, thanks to a broadening of the base of institutions represented and increased diversity of participants.  The rich exchange of ideas at IUI has previously proven to be a valuable source of ideas for future research, as well as leading to collaborative efforts; this funding will extend the opportunities for collaboration and provide intellectual stimulus to programs that have previously sent few or no representatives to this conference.  The organizing committee has undertaken to proactively recruit student participants from schools that have not traditionally been well represented in the IUI community.  Women, minority students, the disabled, and veterans all will be encouraged to participate.  To further assure diversity, no more than one student will be accepted from any given institution."
"1337085","EAAI-13: The Fourth Annual Symposium on Educational Advances in Artificial Intelligence","IIS","INFORMATION TECHNOLOGY RESEARC, ROBUST INTELLIGENCE","05/15/2013","05/16/2013","Laura Brown","CA","Association for the Advancement of Artificial Intelligence","Standard Grant","James Donlon","04/30/2014","$12,000.00","","lebrown@mtu.edu","2275 E BAYSHORE RD STE 160","East Palo Alto","CA","943033224","6503283123","CSE","1640, 7495","7495, 7556","$0.00","This grant provides travel support for selected students attending the annual Symposium on Combinatorial Search (SoCS). Student participation in SoCS encourages young investigators in their research in combinatorial search. This symposium carries on a tradition of bringing together researchers and students around issues of pattern-database heuristics, inconsistent heuristics, real-time search, path-finding for physical robots and computer-game agents, parallel search, and search using external memory. These search techniques have a significant place in broader AI application in areas such as robotics, computer games, planning, and bioinformatics. In addition, SoCS 13 includes a special focus on graph search engineering."
"1340163","Support for Doctoral Students from U.S. Universities to Attend the AIED 2013 and EDM 2013 Conferences","IIS","Information Technology Researc, Cyberlearn & Future Learn Tech","07/01/2013","05/24/2013","Sidney D'Mello","IN","University of Notre Dame","Standard Grant","christopher hoadley","06/30/2014","$19,860.00","","sidney.dmello@gmail.com","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","CSE","1640, 8020","1640, 7556, 8045","$0.00","The International Conference on Artificial Intelligence and Education (AIED 2013; http://aied2013.memphis.edu) and the International Conference on Educational Data Mining (EDM 2013; http://edm2013.memphis.edu) provide professional opportunity for researchers from around the world to share results of cutting-edge research from the fields of artificial intelligence (AI), data mining, computer science, cognitive and learning sciences, psychology, and educational technology that focuses on the design and effective use of advanced learning technologies. AIED researchers aim to design new technologies and advance understanding of how to use those technologies and integrate them into learning environments so that their potential is fulfilled. EDM researchers focus on working towards better use of technology for collecting, analyzing, sharing, and managing data to shed light on learning, promoting learning, and designing learning environments. Researchers from both communities aspire to better understand how people learn with technology and how technology can be used productively to help people learn, through individual use and/or through collaborations mediated by technology. <br/><br/>This project supports travel for advanced graduate students from US universities to attend these two conferences, held in Memphis, Tennessee, AIED 2013 from July 6 to 8, 2013, and EDM 2013 from July 10 to 12, 2013. Participating graduate students join the Doctoral Consortium (DC) tracks of the two conferences and are paired with a senior member of the AIED or EDM community for one-on-one mentoring throughout the conferences. The DC tracks of the conferences and mentor pairing are designed to provide young researchers with mentoring beyond what they get at their home institutions to help them transition from graduate school to a fruitful research career. DC track activities include structured poster sessions where students present their work, meetings with peers who have related interests, and interactions with senior members of the field. Each young researcher's one-on-one mentor will be senior members of the AIED/EDM community who shares research interests with the young researcher and who comes from a different university and has a different approach than the young researcher experiences in his/her home institution. It is expected that conversations between peers and between mentors and mentees will continue throughout each young researcher's career.<br/><br/>This activity supports the mission of NSF to train more advanced professionals in science, technology, engineering, and mathematics. Attending conferences is expensive for graduate students; funding their travel allows them to present their work to the larger community, speak individually with leaders in the field, and receive both support and advice from both senior researchers and peers. The AIED conference is special in its synthesis and cross-fertilization across three STEM capacities: building cutting-edge learning technologies, investigating pedagogical methods that are theoretically grounded in the cognitive, social, and learning sciences, and rigorously testing the learning environments for their effectiveness at promoting learning (in STEM and other disciplines) among K-12, college, and workplace populations. The EDM conference is special in its focus on learning how to use data collected as learners interact with learning technologies to assess learner understanding and capabilities so as to personalize feedback and advice."
"1262814","CI-ADDO-EN: Smart Home in a Box: Creating a Large Scale, Long Term Repository for Smart Environment Technologies","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","08/01/2013","07/19/2013","Diane Cook","WA","Washington State University","Standard Grant","Sylvia Spengler","07/31/2017","$900,000.00","","cook@eecs.wsu.edu","280 Lighty","PULLMAN","WA","991641060","5093359661","CSE","7359","7359, 7364, 7918","$0.00","There is considerable current interest in developing smart environment technologies. Such efforts present research challenges in, and integration of research outcomes from, diverse disciplines including artificial intelligence, pervasive computing, robotics, interfaces, middleware, and sensor networks. The lack of availability of large-scale sharable data sets from smart environments is a major stumbling block for rapid advances in this area. Against this background, this project aims to develop and deploy a data and tool repository needed by the smart environment research community.<br/><br/>The anticipated results of this infrastructure project include 1) a streamlined, do-it-yourself smart home kit, 2) a web interface to upload, access, and annotate smart environment data, 3) meta data including functional assessment scores and energy usage, and 4) software tools to recognize, visualize, and analyze home-based behaviors. The investigators aim to assess the impact of the  resulting repository (CASAS) using measures such as number and diversity of researchers utilizing the repository, number of datasets and tools contributed to the repository, research, education, and commercial advances related to the repository, and publication citations to the repository.<br/><br/>Broader impacts of the project include (i) the do-it-yourself smart home toolkit, data sets and software tools that enable research and educational efforts by a large community of researchers in artificial intelligence, pervasive computing, robotics, interfaces, middleware, and sensor networks; (ii) enhanced opportunities for researchers in cognitive psychology, gerontology, and sociology to contribute to interdisciplinary research in smart environments; and (iii) enhanced research-based training opportunities for students from underrepresented groups. The datasets, software tools and educational materials that result from this work will be made available as part of the CASAS repository at http://ailab.wsu.edu/casas/."
"1259780","The 17th International Conference on Cognitive and Neural Systems (ICCNS), - May-June, 2013 - Boston, MA","BCS","DECISION RISK & MANAGEMENT SCI, COGNEURO, PERCEPTION, ACTION & COGNITION","02/01/2013","01/31/2013","Stephen Grossberg","MA","Trustees of Boston University","Standard Grant","Betty H. Tuller","01/31/2014","$20,000.00","","steve@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","SBE","1321, 1699, 7252","7556","$0.00","Major themes of the International Conference on Cognitive and Neural Systems (ICCNS) are: How does the brain control behavior? How can technology emulate biological intelligence? The conference is aimed at researchers and students in experimental and theoretical cognitive science and neuroscience, neural networks, computer science, neuromorphic engineering, artificial intelligence, mathematics, and physics. It includes invited lectures by distinguished scientists and engineers (with good representation of female scholars), and contributed lectures and posters by students and experts on the biology and technology of how the brain and other intelligent systems adapt to a changing world. There is an emphasis on enabling young scientists and students to present their work."
"1315432","SBIR Phase I: Virtualization Tolerant, High Performance Computing in Servers, Using Compute Intensive Multicore Accelerators","IIP","SMALL BUSINESS PHASE I","07/01/2013","01/23/2014","Jeff Brower","TX","Signalogic, Inc.","Standard Grant","Peter Atherton","06/30/2014","$178,840.00","","dspinfo@signalogic.com","9617 Wendell Rd","Dallas","TX","752435510","2143495551","ENG","5371","5371, 8032, 8039, 8040","$0.00","The innovation addresses problems of data I/O inefficiency and virtualization in High Performance Computing (HPC). Conventional High Performance Computing (HPC) methods are both inefficient and incompatible with virtualization methods commonly used in servers, creating a roadblock to supercomputing server farms needed for practical AI (Artificial Intelligence) applications. GPU (Graphics Processor Unit) and ""many integrated x86 cores"" (many-x86) accelerators, while offering high performance, generate excessive heat, are physically large, and do not offer direct, high-speed, low-latency I/O. For example, GPU and many-x86 accelerator boards are full-length, double-wide, consume up to 300W, and do not connect high-speed, low-latency I/O directly between their compute cores and external networks. This project describes research and development efforts based on a novel approach combining arrays of high performance, low heat compute intensive multicore CPU accelerators with network I/O connected directly to the cores, and a superset of the popular OpenMP standard for multicore programming. Results will demonstrate the first fundamentally virtualization tolerant accelerator available on the market, using a single 1U server with (a) 2.5 Teraflop acceleration, (b) 450 W total power consumption, and (c) virtualization-compatible, OpenMP based programming model with high degree of ease-of-use and suitable for rapid adoption by AI application developers and programmers.<br/><br/>The broader/commercial impacts of the innovation include increased High Performance Computing (HPC) efficiency and virtualization compatibility in supercomputing server farm applications and enabling new Artificial Intelligence (AI) server farm applications. Commercial examples include practical AI applications that require real-time network data I/O, such as mobile device speech and face recognition, fast and automated analysis of drone and surveillance video (for example detecting human behavior and divining human intent in real-time), financial data modeling and trading network risk checks applied directly at the network edge, and real-time social media data analytics. Breakthroughs in HPC efficiency and virtualization will lead to greater scientific understanding of heterogeneous CPU systems, in this case x86 and ARM server motherboards combined with compute intensive multicore accelerators. The OpenMP programming model will be enhanced to allow compute-intensive, direct I/O cores and general-purpose x86 cores to coexist within a unified platform, under a standards-based model. A practical, scalable server acceleration paradigm will be demonstrated with four (4) compute intensive multicore CPU accelerators inserted in a 1U server, running video analytics and computational finance application examples."
"1340162","Support for 18th SIGART/AAAI Doctoral Consortium","IIS","INFORMATION TECHNOLOGY RESEARC, ROBUST INTELLIGENCE","06/01/2013","05/15/2013","Ayanna Howard","CA","Association for the Advancement of Artificial Intelligence","Standard Grant","James Donlon","05/31/2014","$16,994.00","","ah260@gatech.edu","2275 E BAYSHORE RD STE 160","East Palo Alto","CA","943033224","6503283123","CSE","1640, 7495","7556","$0.00","This grant provides travel support for selected students attending the 8th SIGART/AAAI Doctoral Consortium. AAAI (Association for the Advancement of Artificial Intelligence) is the major professional association for AI. At the Doctoral Consortium (DC), PhD students who are pursuing work on AI-related topics present their proposed research and receive feedback from a panel of established researchers, as well as from other student participants. This provides the students with invaluable exposure to outside perspectives on their work at a critical time in their research, and also enables them to explore their career objectives."
"1349355","Workshop: Robot Planning in the Real World: Research Challenges and Opportunities","IIS","Robust Intelligence","08/15/2013","06/19/2015","Ron Alterovitz","NC","University of North Carolina at Chapel Hill","Standard Grant","jeffrey trinkle","07/31/2016","$48,058.00","Sven Koenig, Maxim Likhachev","ron@cs.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","7495","7495, 7556","$0.00","Planning is one of the key technologies in robotics. Yet, robots are deployed in only a small number of niche areas, and most deployed robots have very minimal planning capability. This workshop discusses how the field of robot planning should progress to make robots deployable more widely, performing more novel tasks and relying less on human supervision. It brings together researchers from robotics, artificial intelligence, and related research disciplines to discuss the state of the art in planning, its use in various robotic applications and current research challenges. By studying planning research across different applications, analyzing planning challenges as part of complete robot architectures, and discussing the interaction of planning with other robot modules (such as perception, control, and user interfaces), the workshop participants will gain new insights into how planning can help robots become more robust and efficient. The workshop consists of invited talks, breakout sessions, panels and a final discussion aiming to converge on the roadmap for the field of robot planning that will be summarized in a report. The report and all presentations will be posted on the workshop website. The workshop is expected to stimulate future research towards robot planning in the real world and have strong potential to enable advances in all areas of robotics, from home assistance to medicine to exploration to manufacturing."
"1414935","RI:  Small:  Understanding Value-based Multiagent Learning and Its Applications","IIS","Robust Intelligence","07/01/2013","02/19/2014","Michael Littman","RI","Brown University","Standard Grant","Hector Munoz-Avila","01/31/2016","$157,019.00","","mlittman@cs.brown.edu","BOX 1929","Providence","RI","029129002","4018632777","CSE","7495","7495, 7923, 9150","$0.00","This project explores the behavior of value-based learning methods in multi-agent environments. Value-based methods make decisions by using experience to estimate the utility impact of alternatives and choosing those with high predicted value. Because they evaluate components of behavior instead of treating behaviors as atomic units, they are computationally and statistically efficient. While these methods have been used in computational experiments for many years, only recently have researchers begun to formally characterize their behavior. Our own preliminary work is finding that some value-based methods exhibit super-Nash behavior, making them particularly worthy of study.<br/><br/>More specifically, we are analyzing, mathematically and experimentally, how value-based algorithms perform in several classes of simulated games of varying complexity from the artificial intelligence community, multi-agent engineering applications drawn from the wireless networking area, and as models of human and animal decision making in collaboration with cognitive neuroscientists. Where possible, we are refining existing value-based algorithms to work more efficiently, robustly, and generally than existing algorithms. We are also designing educational outreach activities, including creating entertaining instructional videos on how to promote cooperative behavior in real-life social dilemmas."
"1319966","RI: Small: Any-Angle Search","IIS","Robust Intelligence","08/01/2013","07/20/2018","Sven Koenig","CA","University of Southern California","Standard Grant","James Donlon","07/31/2019","$452,979.00","","skoenig@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7495","7495, 7923, 9251","$0.00","In this project, the PI studies any-angle search methods. Any-angle search methods are variants of the heuristic search method A* that interleave the search with path optimizations by propagating information only along grid edges (to achieve small runtimes) but without constraining the paths to grid edges (to find short ""any-angle"" paths, namely paths whose headings can change by any angle). The objective of this project is to broaden any-angle search from a few isolated search methods to a well-understood framework and to extend its applicability. To this end, the PI is developing new any-angle search methods and analyzing their properties, which is complicated by the fact that even base properties often do not transfer from A* to them. The team will also evaluate all new and existing any-angle search methods against each other and against alternative search methods, for example, to understand how they trade off among runtime, path length and memory consumption.<br/><br/>Any-angle search is a recent search paradigm that promises to result in a new class of powerful path-planning methods for mobile robots, including underwater and aerial vehicles. The project includes dissemination activities to raise awareness of any-angle search in artificial intelligence and robotics (such as via tutorials, open-source code and web applets) and offers research opportunities to both graduate and undergraduate students."
"1301542","Advanced Study Institute on Global Healthcare Grand Challenges and Opportunities Conference on July 21-26, 2013 in Antalya, Turkey","CBET","Engineering of Biomed Systems","08/01/2013","04/28/2016","Metin Akay","TX","University of Houston","Standard Grant","Michele Grimm","07/31/2017","$25,000.00","","makay58@gmail.com","4800 Calhoun Boulevard","Houston","TX","772042015","7137435773","ENG","5345","004E, 017E, 028E, 137E, 138E, 7237, 7479","$0.00","PI: Metin Akay<br/>Proposal ID: 1301542<br/><br/>The National Academy of Engineering announced the 14 Grand Challenges for Engineering at the annual meeting of the American Association for the Advancement of Science. The committee also praised biomedical and biological engineering as the research field to fulfill the promise of personalized medicine. Included among these 14 challenges were Reverse Engineering the Brain, Engineer better Medicines and Advance Health Informatics. An important way of exploiting such information would be through the development of methods that allow doctors to forecast the benefits and side effects of potential treatments or cures. ""Reverse Engineering"" the Brain, is an emerging discipline that helps us to understand how the brain works and treats several diseases. It furthermore helps us to develop computerized artificial intelligence. Advanced computer intelligence, in turn, should enable automated diagnosis and prescriptions for treatment. Computerized catalogs of health information will enhance the medical system's ability to track the spread of disease and analyze the comparative effectiveness of different approaches to prevention and therapy. Finally, engineering new medicines will help us fight the growing danger of attacks from novel disease-causing agents. For instance, certain deadly bacteria have repeatedly evolved new properties, conferring resistance against even the most powerful antibiotics. New viruses arise with the power to kill and spread more rapidly than disease-prevention systems are designed to counteract.<br/><br/>Intellectual Merits: The main objective of the ""Advanced Summer Institute on Global Healthcare--Challenges and Opportunities"" is to highlight and discuss these emerging grand challenges, mainly focused on the latest advances in the areas of science, engineering, technology and medicine. The institute provides a unique environment to discuss the emerging research areas, challenges and opportunities which lead to very fruitful discussions.<br/><br/>Broader Impacts: It exposes the attendees with biology and medicine backgrounds to the latest developments in these emerging enabling technologies. It is also helpful to those with engineering and science background who are interested in doing research in bionanoscience and nanomedicine, neuroscience and engineering since the advanced institute provides exceptional insights into the fundamental challenges in biology and medicine."
"1344768","SCH: INT: Collaborative Research: FITTLE+: Theory and Models for Smartphone Ecological Momentary Intervention","IIS","Smart and Connected Health","10/01/2013","05/12/2017","Joy Ying Zhang","PA","Carnegie-Mellon University","Standard Grant","Sylvia Spengler","09/30/2017","$846,572.00","Ole Mengshoel","joy.zhang@sv.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","8018","8018, 8062, 9251","$0.00","Many health conditions are caused by unhealthy lifestyles and can be improved by behavior change. Traditional behavior-change methods (e.g., weight-loss clinics; personal trainers) have bottlenecks in providing expert personalized day-to-day support to large populations for long periods. There is a pressing need to extend the reach and intensity of existing successful health behavior change approaches in areas such as diet and fitness. Smartphone platforms provide an excellent opportunity for projecting maximally effective interventions for behavior change into everyday life at great economies of scale. Smartphones also provide an excellent opportunity for collecting rich, fine-grained data necessary for understanding and predicting behavior-change dynamics in people going about their everyday lives. The challenge posed by these opportunities for detailed measurement and intervention is that current theory is not equally fine-grained and predictive. <br/><br/>This interdisciplinary project investigates theory and methods to support fine-grained behavior-change modeling and intervention integrated via smartphone into the daily lives of individuals and groups.  Fittle+ develops a new and transformative form of smartphone-delivered Ecological Momentary Intervention (EMI) for improving diet and physical activity. This approach will provide social support and autonomously planned and personalized coaching that builds on methods from mobile sensing, cognitive tutoring, and evidence-based social design. The foundation for this new approach will require new predictive computational theories of health behavior change. Current coarse-grained conceptual theories of individual health behavior change will be refined into fine-grained predictive computational models. These computational models will be capable of tracking moment-by-moment human context, activity, and social patterns based on mobile sensing and interaction data. Using these monitoring capabilities, Fittle+'s computational models will support assessment of, and predictions about, individual users and groups based on underlying motivational, cognitive, and social mechanisms. These predictive models will also be used to plan and optimize coaching actions including detailed diagnostics, individualized goals, and contextually and personally adapted interventions. <br/><br/>The collaborative team of researchers works with weight-loss interventionists at one of nation's largest health organization's facility in Hawaii. The team includes expertise in mobile sensing, artificial intelligence, computational cognition, social psychology, human computer interaction, computer tutoring, and measurement theory."
"1319974","HCC: Small: Collaborative Research: Integrating Cognitive and Computational Models of Narrative","IIS","HCC-Human-Centered Computing","08/01/2013","06/27/2014","Joseph Magliano","IL","Northern Illinois University","Continuing Grant","William Bainbridge","07/31/2017","$146,950.00","","jmagliano@gsu.edu","301 Lowden Hall","De Kalb","IL","601152828","8157531581","CSE","7367","7367, 7923","$0.00","The primary objective of this research is to develop new, cognitively informed computational models of the generation of narrative that is told within three-dimensional virtual environments.  Motivated by theoretic models of narrative structure and psychological models of narrative comprehension, techniques will be developed for creating accounts of sequences of events and the techniques needed to convey them to users. These techniques will use these models to search for narratives that are at once coherent and effective at communicating the underlying event structure.  The project will explore how computational models of the mental processes performed by people when experiencing film or machinima can inform an automatic process used to generate the films themselves.  Extensive empirical studies will provide a comprehensive evaluation of the effectiveness of the models.<br/><br/>The research program has three major thrusts: (1) Integrating generative models of character plans with narrative theoretic structural models to create storylines that reflect both rich character goal structures and recognizable narrative elements. (2) Developing methods for shot sequence selection that build on pragmatic models from linguistic communication to effectively convey characters' plans and goals. (3) Developing and then evaluating a system that integrates these parts to search for narratives that are both coherent and effective.<br/><br/>The project will contribute to the infrastructure of science and education by training new researchers (graduate research assistants) in an area that is broadly multidisciplinary (computer science, cognitive science and psychology). These new researchers will gain from the project a unique integrated view of the contributing disciplines.  Team members will participate in the dissemination of results through journal articles and presentations at national and international conferences on creativity, artificial intelligence, human-computer interaction and psychology. It is expected that the work will have a significant impact on the theory and understanding of creativity, particularly in the context of narrative, serving as a foundation for a new generation of tools that support the creative process."
"1319912","HCC: Small: Collaborative Research: Integrating Cognitive and Computational Models of Narrative","IIS","HCC-Human-Centered Computing","08/01/2013","07/14/2014","Robert Young","NC","North Carolina State University","Continuing Grant","William Bainbridge","10/31/2016","$352,696.00","","young@cs.utah.edu","2701 Sullivan DR STE 240","Raleigh","NC","276950001","9195152444","CSE","7367","7367, 7923","$0.00","The primary objective of this research is to develop new, cognitively informed computational models of the generation of narrative that is told within three-dimensional virtual environments.  Motivated by theoretic models of narrative structure and psychological models of narrative comprehension, techniques will be developed for creating accounts of sequences of events and the techniques needed to convey them to users. These techniques will use these models to search for narratives that are at once coherent and effective at communicating the underlying event structure.  The project will explore how computational models of the mental processes performed by people when experiencing film or machinima can inform an automatic process used to generate the films themselves.  Extensive empirical studies will provide a comprehensive evaluation of the effectiveness of the models.<br/><br/>The research program has three major thrusts: (1) Integrating generative models of character plans with narrative theoretic structural models to create storylines that reflect both rich character goal structures and recognizable narrative elements. (2) Developing methods for shot sequence selection that build on pragmatic models from linguistic communication to effectively convey characters' plans and goals. (3) Developing and then evaluating a system that integrates these parts to search for narratives that are both coherent and effective.<br/><br/>The project will contribute to the infrastructure of science and education by training new researchers (graduate research assistants) in an area that is broadly multidisciplinary (computer science, cognitive science and psychology). These new researchers will gain from the project a unique integrated view of the contributing disciplines.  Team members will participate in the dissemination of results through journal articles and presentations at national and international conferences on creativity, artificial intelligence, human-computer interaction and psychology. It is expected that the work will have a significant impact on the theory and understanding of creativity, particularly in the context of narrative, serving as a foundation for a new generation of tools that support the creative process."
"1346066","SCH: INT: Collaborative Research: FITTLE+: Theory and Models for Smartphone Ecological Momentary Intervention","IIS","Smart and Connected Health","10/01/2013","06/04/2015","Peter Pirolli","CA","Palo Alto Research Center Incorporated","Standard Grant","Sylvia J. Spengler","10/31/2017","$1,231,070.00","Gregory Michael Youngblood","ppirolli@ihmc.us","3333 Coyote Hill Road","Palo Alto","CA","943041314","6508124070","CSE","8018","8018, 8062, 9251","$0.00","Many health conditions are caused by unhealthy lifestyles and can be improved by behavior change. Traditional behavior-change methods (e.g., weight-loss clinics; personal trainers) have bottlenecks in providing expert personalized day-to-day support to large populations for long periods. There is a pressing need to extend the reach and intensity of existing successful health behavior change approaches in areas such as diet and fitness. Smartphone platforms provide an excellent opportunity for projecting maximally effective interventions for behavior change into everyday life at great economies of scale. Smartphones also provide an excellent opportunity for collecting rich, fine-grained data necessary for understanding and predicting behavior-change dynamics in people going about their everyday lives. The challenge posed by these opportunities for detailed measurement and intervention is that current theory is not equally fine-grained and predictive. <br/><br/>This interdisciplinary project investigates theory and methods to support fine-grained behavior-change modeling and intervention integrated via smartphone into the daily lives of individuals and groups.  Fittle+ develops a new and transformative form of smartphone-delivered Ecological Momentary Intervention (EMI) for improving diet and physical activity. This approach will provide social support and autonomously planned and personalized coaching that builds on methods from mobile sensing, cognitive tutoring, and evidence-based social design. The foundation for this new approach will require new predictive computational theories of health behavior change. Current coarse-grained conceptual theories of individual health behavior change will be refined into fine-grained predictive computational models. These computational models will be capable of tracking moment-by-moment human context, activity, and social patterns based on mobile sensing and interaction data. Using these monitoring capabilities, Fittle+'s computational models will support assessment of, and predictions about, individual users and groups based on underlying motivational, cognitive, and social mechanisms. These predictive models will also be used to plan and optimize coaching actions including detailed diagnostics, individualized goals, and contextually and personally adapted interventions. <br/><br/>The collaborative team of researchers works with weight-loss interventionists at one of nation's largest health organization's facility in Hawaii. The team includes expertise in mobile sensing, artificial intelligence, computational cognition, social psychology, human computer interaction, computer tutoring, and measurement theory."
"1311610","WORKSHOP: The 2013 HRI Pioneers Workshop at the 2013 ACM/IEEE International Conference on Human-Robot Interaction","IIS","HCC-Human-Centered Computing","02/01/2013","01/17/2013","Peter Kahn","WA","University of Washington","Standard Grant","Ephraim Glinert","01/31/2014","$34,575.00","","pkahn@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7367","7367, 7556","$0.00","This is funding to support a Young Pioneers Workshop (doctoral consortium) of approximately 20 graduate students and post-docs (including 15 U.S. participants) from diverse research communities (e.g., computer science and engineering, psychology, cognitive science, robotics, human factors, human-computer interaction design, and communications), along with distinguished research faculty.  NSF funding will be used solely to cover travel, housing, and subsistence for eligible U.S. attendees.  The event will take place on Sunday, March 3, 2013, immediately preceding the Eighth Annual Human Robot Interaction Conference (HRI 2013), to be held March 3-6, 2013, in Tokyo, Japan, and which is jointly sponsored by ACM and IEEE.  HRI is a single-track, highly selective annual international conference that seeks to showcase the very best inter- and multi-disciplinary research in human-robot interaction with roots in social psychology, cognitive science, HCI, human factors, artificial intelligence, robotics, organizational behavior, anthropology and many more, and invites broad participation.  The theme of HRI 2013 is ""Holistic Human-Robot Development.""  Robotic solutions are increasingly applied to real world problems such as our aging society, renewable energy, climate control, emergency response, education and exploration. These societal problems require a holistic approach to the design and development of robots that meet human needs, address technical challenges, and foster acceptance in everyday settings.  More information about the conference is available online at http://humanrobotinteraction.org/2013.<br/><br/>The Young Pioneers Workshop is designed to complement the conference, by providing a forum for students and recent graduates in the field of HRI to share their current research with their peers and a panel of senior researchers in a setting that is less formal and more interactive than the main conference. During the workshop, participants will talk about the important upcoming research themes in the field, encouraging the formation of collaborative relationships across disciplines and geographic boundaries. To these ends, the workshop format will include oral presentations from 2 student attendees, poster presentations from all student attendees, a hands-on breakout session with group presentations, a keynote, and a panel presentation by senior researchers who will share their expertise and insights on how to address the interdisciplinary challenge of HRI.  The afternoon breakout session will involve small groups of 3-5 attendees with diverse backgrounds, working to design an integrative HRI project, thereby encouraging group participation and the cultivation of cross-disciplinary ideas.  The presentation session afterward will allow each breakout group to present a summary to the entire workshop; the organizers anticipate that the discussions will continue during dinner. <br/><br/>Broader Impacts:  This workshop will afford a unique opportunity for the best of the next generation of researchers in human-robot interaction to be exposed to and discuss current and relevant topics as they are being studied in several different research communities (including but not limited to computer science and engineering, psychology, robotics, human factors and ergonomics, and HCI).  This is important for the field, because it has been recognized that transformative advances in research in this fledgling area can only come through the melding of cross-disciplinary knowledge and multinational perspectives. Participants will be encouraged to create a social network both among themselves and with senior researchers at a critical stage in their professional development, to form collaborative relationships, and to generate new research questions to be addressed during the coming years.  Participants will also gain leadership and service experience, as the workshop is largely student organized and student led.  The PI has expressed his strong commitment to recruiting women and members from other under-represented groups.  To further ensure diversity, the event organizers will consider an applicant's potential to offer a fresh perspective and point of view with respect to HRI, and no more than one applicant will be accepted from any given institution."
"1317815","NRI: Small: Collaborative Planning for Human-robot Science Teams","IIS","NRI-National Robotics Initiati","10/01/2013","09/18/2013","Gaurav Sukhatme","CA","University of Southern California","Standard Grant","David Miller","09/30/2017","$482,252.00","","gaurav@cs.usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","8013","7923, 8086","$0.00","This project envisions the future of scientific exploration as a collaborative endeavor between human scientists and autonomous robotic systems. The key challenge to materializing this vision lies in combining the expert knowledge of the scientist with the optimization capabilities of the autonomous system. The scientist brings specialized knowledge and experience to the table, while the autonomous system is capable of processing and evaluating large quantities of data. This research leverages these complementary strengths to develop a collaborative system capable of guiding scientific exploration and data collection by integrating input from scientists into an autonomous learning and planning framework. This is achieved by combining probabilistic planning with inverse reinforcement learning to integrate human input and prior knowledge into a unified optimization framework in the context of scientific exploration. The project team is validating the approach in the challenging domain of autonomous underwater ocean monitoring. This domain is particularly well suited for the testing of human-robot collaboration due to the limited communication available underwater and the necessary supervised autonomy capabilities. By integrating feedback from the human user into an algorithmic planning framework, the goal is to improve the efficiency of scientific data collection and gather data about phenomena that were previously outside the reach of scientific investigation. The use of autonomous vehicles for scientific data collection is becoming increasingly prominent; however, the research community lacks a foundational understanding of the interactions between scientists and autonomous vehicles. This work focuses on principled methods for integrating human input into algorithmic optimization techniques moving towards the goal of supervised autonomy for robots.<br/><br/>This project has the potential to change the way scientific data are collected through the development of a foundational framework for human-robot scientific collaboration. Such a framework is expected to have broad implications throughout the fields of human-robot interaction and artificial intelligence. The proposed research is being integrated into the robotics and computer science curriculum at both the graduate and undergraduate levels. It is also being utilized for K-12 robotics outreach programs in Los Angeles. The algorithms created in this research are transitioned to field tests and operations via ongoing collaborations with the Monterey Bay Aquarium Research Institute (MBARI) and the Southern California Coastal Ocean Observing System (SCCOOS)."
"1308505","Doctoral Mentoring Consortium at the Twelfth International Conference on Autonomous Agents and Multi-Agent Systems","IIS","Information Technology Researc, Info Integration & Informatics","05/15/2013","05/03/2013","Anita Raja","NC","University of North Carolina at Charlotte","Standard Grant","Sylvia Spengler","04/30/2014","$20,000.00","","anita.raja@hunter.cuny.edu","9201 University City Boulevard","CHARLOTTE","NC","282230001","7046871888","CSE","1640, 7364","1640, 7364, 7556","$0.00","This award supports the travel of US doctoral students to, and their participation in the Twelfth International Joint Conference on Autonomous Agents and Multi-Agent Systems (AAMAS 2013), and the doctoral consortium to be held in conjunction with it.  <br/><br/>AAMAS is the premier international conference devoted to all aspects of agent technology and its applications in the World Wide Web, electronic commerce, digital libraries, search, personalization, etc. The Doctoral Consortium offers the students an opportunity to present their research  and receive in-depth feedback from  senior researchers. Students will benefit from  attending  the conference, with its rich technical program of contributed and invited research talks, complemented by focused workshops, tutorials and other events. <br/><br/>The student travel awards will help enrich the eduation and training of a diverse group of young researchers, including women and members of underrepresented groups, working in Artificial Intelligence in general, and Autonomous Agents and Multi-Agent Systems in particular."
"1343507","Doctoral Consortium Support for ICAPS 2013","IIS","ROBUST INTELLIGENCE","07/01/2013","06/19/2013","Mark Boddy","CA","Association for the Advancement of Artificial Intelligence","Standard Grant","Hector Munoz-Avila","06/30/2014","$16,000.00","","mark.boddy@adventiumlabs.com","2275 E BAYSHORE RD STE 160","East Palo Alto","CA","943033224","6503283123","CSE","7495","7495, 7556","$0.00","This grant supports student participation in a doctoral mentoring consortium at the International Conference on Automated Planning and Scheduling (ICAPS). The ICAPS Doctoral Consortium (DC) is a primary method for broadening participation and improving retention of doctoral researchers in the field of automated planning and scheduling. This DC at ICAPS 2013, June 2013 in Rome, Italy, is the eleventh occurrence of the event.<br/>This symposium is carrying on a multiyear tradition of holding DCs and summer schools, providing an important venue to encourage students and junior researchers in automated planning and scheduling. Activities include research panel discussions, lunch with mentors, and poster session held as part of the main conference. The DC brings together a broader community of researchers in planning and scheduling, promoting integration with other areas of AI and computer science, and encouraging junior researchers with a more focused attention to this area than is otherwise possible in other venues. Of particular note is the potential to involve participants who might not have attended an AI conference before. This is especially true for those outside of the usual AI research community: those at smaller institutions who use AI in teaching but not in research; those from smaller/liberal arts institutions; and those from institutions that are poorly represented in STEM disciplines."
"1311059","NSF East Asia and Pacific Summer Institute (EAPSI) for FY 2013 in Japan","OISE","EAPSI","06/01/2013","05/16/2013","Erik Steinmetz","MN","Steinmetz               Erik           S","Fellowship Award","Anne Emig","05/31/2014","$5,070.00","","","","Minneapolis","MN","554182128","","O/D","7316","5921, 5978, 7316","$0.00","This action funds Erik Stefan Steinmetz of The University of Minnesota, Department of Computer Science and Engineering, to conduct a research project in the Computer and Information Science and Engineering area during the summer of 2013 at The Nara Institute of Science and Technology (NIST) in Nara, Japan.  The project title is ""Improving Opening Strategy in Computer Go Players."" The host scientist is Kenichi Matsumoto.<br/><br/>This project explores a method of improving the artificial intelligence search technique known as Monte Carlo search. The avenues being explored in this research allow the application of expert knowledge to a technique which normally relies on not being restricted by contextual knowledge (to allow randomness to properly explore a search tree). Preliminary results showed one such measure which improved play of a computer Go engine that used Monte Carlo search, and this project develops further measures along with discovering their effectiveness and limitations. By studying these modifications which touch the very nature of Monte Carlo search, this project explores the applicability and limitations of Monte Carlo search in general and whether or not hybrid methods offer an opportunity for their improvement. This research will be directly applicable to other problems using the Monte Carlo search method such as military simulation software, along with some large online simulation software systems.<br/><br/>Broader impacts of an EAPSI fellowship include providing the Fellow a first-hand research experience outside the U.S.; an introduction to the science, science policy, and scientific infrastructure of the respective location; and an orientation to the society, culture and language.   These activities meet the NSF goal to educate for international collaborations early in the career of its scientists, engineers, and educators, thus ensuring a globally aware U.S. scientific workforce.  Furthermore,  the demonstration of the results of this project will be exceptionally helpful in a classroom setting. It shows a simple method to increase the effectiveness of an algorithm in a topic which is interesting to undergraduate students. By showing how key ideas such as this research can be applied directly to these interesting topics, students can be engaged and motivated at a very early stage of their undergraduate computer science careers."
"1338935","Planning Grant:  I/UCRC for Semantic Computing","IIP","IUCRC-Indust-Univ Coop Res Ctr","08/15/2013","08/09/2013","Mihaela van der Schaar","CA","University of California-Los Angeles","Standard Grant","Thyagarajan Nandagopal","01/31/2015","$11,500.00","William Zame, Abeer Alwan, Hu Xiao","mihaela@ee.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","ENG","5761","5761, 8039","$0.00","Semantic Computing (SC) is an emerging field that addresses computing technologies which allow users to search, create, manipulate and connect computational resources. The research efforts will be anchored by the University of California-Irvine as the lead institution, partnered with the University of California-Los Angeles and the University of California-San Diego. <br/><br/>Semantic Computing includes the computing technologies (e.g., artificial intelligence, natural language, software engineering, data and knowledge engineering, computer systems, signal processing, etc.), and their interactions, that may be used to extract or process computational content and descriptions. The proposed I/UCRC will focus on providing technologies that lead to a Semantic Problem Solving (SPS) network where distributed resources can be easily connected based on semantics for the purpose of problem solving. While some areas of Semantic Computing have appeared as isolated pieces in individual disciplines, Semantic Computing glues these pieces together into an integrated theme with synergetic interactions. It addresses not only the analysis and transformation of signals (e.g., pixels, words) into useful information, but also how such information can be accessed and used to synthesize new signals. The three proposed campuses have complementary areas of strength and have emphasized different aspects of Semantic Computing systems. All participating faculty are prominent researchers in one or more areas related to Semantic Computing. <br/><br/>The technologies developed by the proposed I/UCRC may support new inter-connectivity that has not been realized by the Internet today. It therefore may facilitate the transition of the Internet into Web 3.0 and stimulate new business models for a better economy that everyone can benefit. The technologies developed by the proposed I/UCRC will be delivered to the public via the Internet, and the PIs expect to create broader impacts in areas other than the five selected. The five areas are chosen as they characterize different aspects of problem solving, e.g., creativity (IT, entertainment), learning (education) and decision making (health, finance), which are applicable to many other areas such as manufacturing, science, engineering, and the humanities."
"1347075","EAGER: Human-Centric Predictive Analytics of Cyber-Threats: a Temporal Dynamics Approach","CNS","Secure &Trustworthy Cyberspace","09/01/2013","08/15/2013","H. Brinton Milward","AZ","University of Arizona","Standard Grant","Deborah Shands","08/31/2016","$200,000.00","Ronald Breiger, Jerzy Rozenblit, Loukas Lazos","milward@email.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","CSE","8060","7434, 7916","$0.00","Cybersecurity is paramount to protecting national interests in domains that include but extend well beyond defense and finance. However, current state-of-the-art cyber-defenses have severely limited predictive and attribution capabilities. Detecting and understanding cyber attacks is not sufficient--we can liken that to ""studying symptoms instead of a disease.""<br/>           <br/>This research carries out a synergistic approach to integrating cyber data forensics with human-centric social network analysis under a common framework. The three major activities of this project are as follows: (a) comprehensive models of cyber-attack characteristics are developed using feature extraction techniques on diverse data sources, (b) adversarial groups are classified according to their feature similarities, and (c) group classification is enhanced using analytic techniques from social network science.<br/>           <br/>To accomplish these activities: (1) different models for constructing joint representations of computer and social networks are investigated within a multi-mode graph framework; (2) data reduction and feature extraction techniques are developed for associating large datasets with this unified graph model; (3) an existing system is leveraged to discover invisible and missing links between adversarial networks of individuals; and (4) social network models and tools, as well as case studies, are applied to infer adversarial group typology.<br/>           <br/>This research is expected to benefit computer science, cyber security, and social sciences by improving detection methods for cyber attacks. Applying time-series analysis in creative ways to multi-mode networks should contribute to the field of artificial intelligence. This joint work should apply also in fields such as health care, marketing, or forecasting technology adoption trends."
"1337300","XPS: DSD: Collaborative Research: NeoNexus: The Next-generation Information Processing System across Digital and Neuromorphic Computing Domains","CCF","Exploiting Parallel&Scalabilty","09/15/2013","09/06/2013","Qinru Qiu","NY","Syracuse University","Standard Grant","Tao Li","08/31/2017","$275,884.00","","qiqiu@syr.edu","OFFICE OF SPONSORED PROGRAMS","SYRACUSE","NY","132441200","3154432807","CSE","8283","","$0.00","The explosion of ""big data"" applications imposes severe challenges of data processing speed and scalability on traditional computer systems. The performance of traditional Von Neumann machines is greatly hindered by the increasing performance gap between CPU and memory, motivating the active research on new or alternative computing architectures. By imitating brain's naturally massive parallel architecture with closely coupled memory and computing as well as the unique analog domain operations, neuromorphic computing systems are anticipated to deliver superior speed for applications in image recognition and natural language understanding.<br/><br/>The objective of this research is to establish the fundamental framework and design methodology for NeoNexus -- the next-generation information processing system inspired by human neocortex. It integrates neuromorphic computing accelerators with conventional computing resources by leveraging large scale inference-based data processing and computing acceleration technique atop memristor crossbar arrays. The computation and data exchange will be carefully coordinated and supported by the innovative interconnect architecture, i.e., a hierarchical network-on-chip (NoC). The software-hardware co-design platform will be developed to address the various design challenges. The project will help computer architecture and high-performance computing communities to overcome the ever-increasing technical challenges of traditional architectures and accelerate the fusion between conventional computing technology and cognitive computing model. It will also promote the applications of artificial intelligence technology advances in modern computer architectures and motivate the inventions at both software and hardware levels. Undergraduate and graduate students involved in this research will be trained for the next-generation semiconductor industry workforce."
"1337198","XPS: DSD: Collaborative Research: NeoNexus: The Next-generation Information Processing System across Digital and Neuromorphic Computing Domains","CCF","Exploiting Parallel&Scalabilty","09/15/2013","09/06/2013","Hai Li","PA","University of Pittsburgh","Standard Grant","tao li","06/30/2017","$450,000.00","Yiran Chen","hai.li@duke.edu","300 Murdoch Building","Pittsburgh","PA","152603203","4126247400","CSE","8283","","$0.00","The explosion of ""big data"" applications imposes severe challenges of data processing speed and scalability on traditional computer systems. The performance of traditional Von Neumann machines is greatly hindered by the increasing performance gap between CPU and memory, motivating the active research on new or alternative computing architectures. By imitating brain's naturally massive parallel architecture with closely coupled memory and computing as well as the unique analog domain operations, neuromorphic computing systems are anticipated to deliver superior speed for applications in image recognition and natural language understanding.<br/><br/>The objective of this research is to establish the fundamental framework and design methodology for NeoNexus -- the next-generation information processing system inspired by human neocortex. It integrates neuromorphic computing accelerators with conventional computing resources by leveraging large scale inference-based data processing and computing acceleration technique atop memristor crossbar arrays. The computation and data exchange will be carefully coordinated and supported by the innovative interconnect architecture, i.e., a hierarchical network-on-chip (NoC). The software-hardware co-design platform will be developed to address the various design challenges. The project will help computer architecture and high-performance computing communities to overcome the ever-increasing technical challenges of traditional architectures and accelerate the fusion between conventional computing technology and cognitive computing model. It will also promote the applications of artificial intelligence technology advances in modern computer architectures and motivate the inventions at both software and hardware levels. Undergraduate and graduate students involved in this research will be trained for the next-generation semiconductor industry workforce."
"1262805","REU Site: CAAR: Combinatorial Algorithms Applied Research","CCF","RSCH EXPER FOR UNDERGRAD SITES, COMPUT GAME THEORY & ECON","05/01/2013","04/19/2013","Samir Khuller","MD","University of Maryland College Park","Standard Grant","Anindya Banerjee","04/30/2016","$292,154.00","William Gasarch","samir.khuller@northwestern.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","1139, 7932","7932, 9250","$0.00","This funding will establish a new CISE Research Experiences for Undergraduates (REU) Site at the University of Maryland-College Park.  This Site focuses on research in combinatorial algorithms, which are algorithms that work on discrete objects such as numbers, graphs, or data where one needs to optimize over a space of possible solutions.  To help create a climate in which students learn new techniques, understand a few real world problems, and work on designing and implementing algorithms, the students will be involved in applied algorithms research projects that will be jointly advised by members of the UMD theory group in conjunction with faculty in different research areas, such as systems, artificial intelligence, and databases.<br/><br/>This REU Site will support eight undergraduate students for ten weeks each year.  The Site will recruit computer science, computer engineering, and mathematics majors, with a focus on students from Historically Black Colleges and Women's colleges.  The Site will help bridge the gap between theory and practice, which will benefit both as it gives theorists new and exciting problems and non-theorists tools they can use.  The students will have the opportunity to see what research is like, obtain a solid grounding in algorithms, and encourage them to go to graduate school."
"1338934","Planning Grant:  I/UCRC for Semantic Computing","CNS","IUCRC-Indust-Univ Coop Res Ctr","08/15/2013","08/09/2013","Shlomo Dubnov","CA","University of California-San Diego","Standard Grant","Thyagarajan Nandagopal","07/31/2015","$11,499.00","","sdubnov@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","5761","5761, 8039","$0.00","Semantic Computing (SC) is an emerging field that addresses computing technologies which allow users to search, create, manipulate and connect computational resources. The research efforts will be anchored by the University of California-Irvine as the lead institution, partnered with the University of California-Los Angeles and the University of California-San Diego. <br/><br/>Semantic Computing includes the computing technologies (e.g., artificial intelligence, natural language, software engineering, data and knowledge engineering, computer systems, signal processing, etc.), and their interactions, that may be used to extract or process computational content and descriptions. The proposed I/UCRC will focus on providing technologies that lead to a Semantic Problem Solving (SPS) network where distributed resources can be easily connected based on semantics for the purpose of problem solving. While some areas of Semantic Computing have appeared as isolated pieces in individual disciplines, Semantic Computing glues these pieces together into an integrated theme with synergetic interactions. It addresses not only the analysis and transformation of signals (e.g., pixels, words) into useful information, but also how such information can be accessed and used to synthesize new signals. The three proposed campuses have complementary areas of strength and have emphasized different aspects of Semantic Computing systems. All participating faculty are prominent researchers in one or more areas related to Semantic Computing. <br/><br/>The technologies developed by the proposed I/UCRC may support new inter-connectivity that has not been realized by the Internet today. It therefore may facilitate the transition of the Internet into Web 3.0 and stimulate new business models for a better economy that everyone can benefit. The technologies developed by the proposed I/UCRC will be delivered to the public via the Internet, and the PIs expect to create broader impacts in areas other than the five selected. The five areas are chosen as they characterize different aspects of problem solving, e.g., creativity (IT, entertainment), learning (education) and decision making (health, finance), which are applicable to many other areas such as manufacturing, science, engineering, and the humanities."
"1338936","Planning Grant:  I/UCRC for Semantic Computing","CNS","IUCRC-Indust-Univ Coop Res Ctr","08/15/2013","08/09/2013","Phillip Sheu","CA","University of California-Irvine","Standard Grant","Thyagarajan Nandagopal","07/31/2015","$15,624.00","","","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","CSE","5761","5761, 8039","$0.00","Semantic Computing (SC) is an emerging field that addresses computing technologies which allow users to search, create, manipulate and connect computational resources. The research efforts will be anchored by the University of California-Irvine as the lead institution, partnered with the University of California-Los Angeles and the University of California-San Diego. <br/><br/>Semantic Computing includes the computing technologies (e.g., artificial intelligence, natural language, software engineering, data and knowledge engineering, computer systems, signal processing, etc.), and their interactions, that may be used to extract or process computational content and descriptions. The proposed I/UCRC will focus on providing technologies that lead to a Semantic Problem Solving (SPS) network where distributed resources can be easily connected based on semantics for the purpose of problem solving. While some areas of Semantic Computing have appeared as isolated pieces in individual disciplines, Semantic Computing glues these pieces together into an integrated theme with synergetic interactions. It addresses not only the analysis and transformation of signals (e.g., pixels, words) into useful information, but also how such information can be accessed and used to synthesize new signals. The three proposed campuses have complementary areas of strength and have emphasized different aspects of Semantic Computing systems. All participating faculty are prominent researchers in one or more areas related to Semantic Computing. <br/><br/>The technologies developed by the proposed I/UCRC may support new inter-connectivity that has not been realized by the Internet today. It therefore may facilitate the transition of the Internet into Web 3.0 and stimulate new business models for a better economy that everyone can benefit. The technologies developed by the proposed I/UCRC will be delivered to the public via the Internet, and the PIs expect to create broader impacts in areas other than the five selected. The five areas are chosen as they characterize different aspects of problem solving, e.g., creativity (IT, entertainment), learning (education) and decision making (health, finance), which are applicable to many other areas such as manufacturing, science, engineering, and the humanities."
"1262901","REU Site:  Undergraduate Research in Computational Biology at Mississippi State University","DBI","RSCH EXPER FOR UNDERGRAD SITES","04/01/2013","03/13/2013","Andy Perkins","MS","Mississippi State University","Standard Grant","Amanda Simcox","03/31/2017","$286,304.00","","perkins@cse.msstate.edu","PO Box 6156","MISSISSIPPI STATE","MS","397629662","6623257404","BIO","1139","1228, 9150, 9178, 9250","$0.00","A Research Experience for Undergraduates (REU) award has been made to Mississippi State University that will provide research training for 10 students, for 10 weeks during the summers of 2013-2015. Undergraduate students will work on interdisciplinary research projects in computational biology. Students will work closely with their faculty mentors on projects in the areas of genome analysis, functional genomics, artificial intelligence, scientific visualization, and bioinformatics algorithms, among others. Recruitment will be targeted at colleges and universities within Mississippi and the surrounding states with the goal of providing a meaningful research experience to promising undergraduate students who otherwise would not have had the opportunity to engage in research. There is a particular focus on increasing the participation of underrepresented groups in computational biology research. Educational and professional development sessions will be conducted to train students in the basics of computational biology and to provide information on graduate school and pursuing scientific careers. Students will also receive training and mentoring in the responsible conduct of research. The program will culminate with a poster session where the students will present their research findings to the computational biology community at Mississippi State University. Program objectives will be assessed through the use of a common web-based assessment tool available to BIO-funded REU PIs. Students will be tracked to document their educational and professional path through periodic communication with students via email and social media. More information about this program can be obtained by contacting the project PI, Dr. Andy Perkins (perkins@cse.msstate.edu) at (662) 325-0004, or by visiting the project website at http://www.cse.msstate.edu/~compbio/."
"1309708","Collaborative Research: High Performance Cellular Simultaneous Recurrent Network based Pattern Recognition","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","06/01/2013","05/20/2013","Tarek Taha","OH","University of Dayton","Standard Grant","Radhakisan Baheti","05/31/2017","$146,630.00","","ttaha1@udayton.edu","300 COLLEGE PARK AVE","DAYTON","OH","454690104","9372292919","ENG","7607","102E, 1653","$0.00","This is a collaborative proposal between a neural network researcher, addressing the issues of face recognition and image recognition in general, and a researcher on a new class of electronic chip based on memristors.<br/>       In recent years, new world records have been set in image recognition by convolutional neural networks, funded <br/>at other universities through the EFRI/COPN topic at NSF. At times, those systems have outperformed humans in those tasks. On the neural network side, this team plans to use a more general class of neural networks, the Cellular Simultaneous Neural Network (CSRN), to address benchmark challenges in face recognition where computers have yet to outperform humans. The CSRN may be viewed as<br/>a generalization of the convolutional network to add a kind of real-time recurrence or feedback, a kind of recurrence which is known to be crucial to the powers of biological brains.<br/>       On the electronic hardware side, this proposal addresses a crucial challenge in continuing Moore's Law. The speed of computing chips is not expected to grow as fast as it did in the past, but thanks to breakthroughs in lithography and the recent work in memristors, we can still expect progress towards thousand or even millions of active processors on a chip. In order to make full use of this emerging new capability, new efforts are needed to integrate device work and systems level work together, in developing new architectures of real practical use. If successful, this project could be an important step forward in that effort. Memristors for use in memory are already being well-funded by industry, but the extension to active processing and learning is more of a high risk<br/>breakthrough activity.<br/>     This project also includes a substantial component of education and outreach, including development of systems to stimulate K-8 children."
"1310353","Collaborative Research: High Performance Cellular Simultaneous Recurrent Network based Pattern Recognition","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","06/01/2013","07/01/2015","Khan Iftekharuddin","VA","Old Dominion University Research Foundation","Standard Grant","Radhakisan Baheti","05/31/2017","$277,369.00","","kiftekha@odu.edu","4111 Monarch Way","Norfolk","VA","235082561","7576834293","ENG","7607","092E, 102E, 1653, 9102, 9251","$0.00","This is a collaborative proposal between a neural network researcher, addressing the issues of face recognition and image recognition in general, and a researcher on a new class of electronic chip based on memristors.<br/>       In recent years, new world records have been set in image recognition by convolutional neural networks, funded <br/>at other universities through the EFRI/COPN topic at NSF. At times, those systems have outperformed humans in those tasks. On the neural network side, this team plans to use a more general class of neural networks, the Cellular Simultaneous Neural Network (CSRN), to address benchmark challenges in face recognition where computers have yet to outperform humans. The CSRN may be viewed as<br/>a generalization of the convolutional network to add a kind of real-time recurrence or feedback, a kind of recurrence which is known to be crucial to the powers of biological brains.<br/>       On the electronic hardware side, this proposal addresses a crucial challenge in continuing Moore's Law. The speed of computing chips is not expected to grow as fast as it did in the past, but thanks to breakthroughs in lithography and the recent work in memristors, we can still expect progress towards thousand or even millions of active processors on a chip. In order to make full use of this emerging new capability, new efforts are needed to integrate device work and systems level work together, in developing new architectures of real practical use. If successful, this project could be an important step forward in that effort. Memristors for use in memory are already being well-funded by industry, but the extension to active processing and learning is more of a high risk<br/>breakthrough activity.<br/>     This project also includes a substantial component of education and outreach, including development of systems to stimulate K-8 children."
"1344302","INSPIRE Track 1: Imaging Neuronal Network Activity using Voltage-Gated Optical Transitions in Graphene","DMR","INSTRUMENTAT & INSTRUMENT DEVP, OFFICE OF MULTIDISCIPLINARY AC, ELECTRONIC/PHOTONIC MATERIALS, Cross-BIO Activities, BIOMATERIALS PROGRAM, Modulation, BIOSENS-Biosensing, INSPIRE","09/15/2013","06/13/2014","Feng Wang","CA","University of California-Berkeley","Continuing Grant","Z.  Ying","08/31/2018","$1,000,000.00","Bianxiao Cui","fengwang76@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1108, 1253, 1775, 7275, 7623, 7714, 7909, 8078","142E, 7573, 8653","$0.00","This INSPIRE award is partially funded by the Electronic and Photonic Materials Program and the Biomaterials Program in the Division of Materials Research in the Directorate for Mathematical and Physical Sciences; the Neural Systems Program in the Division of Integrative Organismal Systems in the Directorate for Biological Sciences; the Instrument Development for Biological Research Program in the Division of Biological Infrastructure in the Directorate for Biological Sciences; and the Nano-Biosensing Program in the Division of Chemical, Bioengineering, Environmental, and Transport Systems in the Directorate for Engineering.<br/><br/>Technical Description: This research project aims to develop a novel technique to detect electric activities of a neuronal network using voltage-gated optical transitions in graphene. A neuron receives, analyzes and conveys information in the form of electrical signals such as action potentials. The decoding of neural data requires accurate recording of those electrical signals.  This project exploits unique electrical and optical properties of graphene to detect electric activities of a neuronal network in a highly parallel and non-invasive manner. It advances scientific understanding in several important fronts, including (1) fundamental understanding of graphene as a unique electronic and optical material for high-sensitivity bio-imaging, (2) interfacing graphene with neuron cells, (3) non-invasive optical detection of action potential, and (4) highly parallel imaging of neuron activity in a neuronal network. This project builds on the complimentary expertise of Professor Wang and Professor Cui on graphene optical spectroscopy and neural electrophysiology, respectively.<br/><br/>Non-technical Description: This project captures the tremendous opportunities provided by graphene for optical imaging of neural network activities, which has a potential to become an important tool in neuroscience.  The research requires extensive collaboration and exchange of students between Cui and Wang labs, which provides a unique opportunity for graduate and undergraduate students to be exposed to sciences in different disciplines including biology, engineering, materials science, optics, and physics."
"1311213","US-German Collaboration: Toward a quantitative understanding of navigational deficits in aging humans","IIS","Engineering of Biomed Systems, International Research Collab, CRCNS-Computation Neuroscience","09/15/2013","03/15/2019","Ila Fiete","TX","University of Texas at Austin","Continuing Grant","Kenneth Whang","07/31/2019","$447,150.00","","fiete@mit.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","5345, 7298, 7327","5345, 5936, 5980, 7298, 7327","$0.00","The goal of this project is to combine computational modeling with behavioral and neuroimaging studies to characterize the mechanisms of navigational abilities in humans and understand how they decline with age. The PIs will focus on an important navigational circuit in mammals, which consists of the hippocampus and associated areas, and includes grid cells of the entorhinal cortex as well as place cells. Place cells have highly location-specific responses, turning on at one location in an environment and firing little elsewhere; grid cells by contrast fire at multiple locations within an environment, with periodically separated activity blobs in a striking triangular lattice pattern. Studies in rodents have detailed the properties of grid and place cells, and led to neural network models whose additional predictions have often been borne out by single-unit neuron recordings. However, much less is known about grid cells and place cells in humans, and the nature of interactions between different parts of the navigation circuit remains unclear, in rodents and humans. <br/><br/>In this project, the PIs bring to bear virtual-reality-based behavioral experiments, ultra-high-resolution fMRI recordings during virtual navigation, and neural network modeling, to better understand the circuit for spatial navigation in humans. The PIs plan a three-pronged approach to these questions. The first is to characterize phenomenologically the characteristic errors made by humans, through navigation environments with and without accurate external landmark cues, and under other externally varying conditions, in aged and non-aged subjects. The second is to employ neural network models of grid cells, to model the network parameters that could give rise to the observed deficits, and in turn test the predictions of these models with the neuroimaging experiments. The experimental setup will permit systematic variation in the fidelity of external sensory cues, to probe the relative contributions of the complementary computations of dead-reckoning (path integration) versus landmark-based navigation, and uncover their potential neural substrates in humans. The results will help to develop models of how parallel streams of spatial information are combined and processed across brain areas to aid in navigation. The third component is to develop accurate algorithms for extracting spatial information from high-resolution fMRI data from regions and sub-regions of the entorhinal-hippocampal complex. The aim is to map the distribution of location information across areas and learn where it is most compromised in old age.<br/><br/>This award is being co-funded by NSF's Office of the Director, International Science and Engineering.  A companion project is being funded by the German Ministry of Education and Research (BMBF)."
"1310704","NSF East Asia and Pacific Summer Institute (EAPSI) for FY 2013 in Japan","OISE","EAPSI","06/01/2013","05/15/2013","Agnieszka Szymanska","CA","Szymanska               Agnieszka      A","Fellowship Award","Anne Emig","05/31/2014","$5,070.00","","","","Irvine","CA","926175307","","O/D","7316","5921, 5978, 7316","$0.00","This action funds Agnieszka Szymanska of the Center for Biomedical Signal Processing and Computation, University of California, Irvine to conduct a research project in Engineering during the summer of 2013 at the Laboratory of Chemical Pharmacology, Graduate School of Pharmaceutical Sciences, University of Tokyo in Tokyo, Japan. The project title is ""An Algorithm for Neuron Characterization In Vivo."" The host scientist is Dr. Yuji Ikegaya, Associate Professor, Graduate School of Pharmaceutical Sciences, University of Tokyo, Japan.<br/><br/>While many techniques exist to study the functional organization of neurons, few can use extracellular action potentials (EAPs) to extract crucial neuron parameters such as the location, size, dendritic tree shape, and type of neurons being studied. This study develops a novel framework to monitor neural activity via both extracellular recording and functional multi-neuron calcium imaging (fMCI). Extracellular recording uses micro-sensors placed in neural tissue to detect action potentials from nearby neurons, whereas calcium imaging detects the flow of calcium ions into a cell, causing the cell to effectively light up during an action potential. Extracellular recordings from rat brain slices are analyzed using a new statistical signal processing algorithm to extract the location, size, and type of multiple neurons, while fMCI data provides confirmation of the results. Because extracellular recording is one of the only ways to study neural activity in live animals (in vivo) where direct imaging cannot be performed, the algorithm being tested here can ultimately improve studies of neuronal migration, plasticity, and brain computer interfacing. These kinds of studies can then lead to a better understanding of brain development, healing, and diseases such as dementia. <br/><br/>Broader impacts of an EAPSI fellowship include providing the Fellow a first-hand research experience outside the U.S.; an introduction to the science, science policy, and scientific infrastructure of the respective location; and an orientation to the society, culture and language. These activities meet the NSF goal to educate for international collaborations early in the career of its scientists, engineers, and educators, thus ensuring a globally aware U.S. scientific workforce. Furthermore, once tested, the analysis algorithm developed in this study will be freely disseminated as a software package to the neuroscience community at large. This will vastly improve in vivo studies of neural functionality, and therefore have long lasting impact on the scientific community. Neuron activity movies made during the study will also be presented in an outreach program to teach local elementary and high school students about neural network dynamics in an engaging way."
"1361274","CAREER: Semantic Interpretation with Monolingual and Cross-lingual Evidence","IIS","Linguistics, Robust Intelligence","09/01/2013","09/26/2013","Rada Mihalcea","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Tatiana Korelsky","05/31/2015","$116,141.00","","mihalcea@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","1311, 7495","0000, 1045, 1187, 7495, 9215, HPCC, OTHR","$0.00","Word meanings are central to the semantic interpretation of texts.<br/>Although much work to date has focused on statistical approaches that often ignore the explicit understanding of the text, recent research work has begun to challenge this simplification, demonstrating that semantic interpretation is indeed essential for a number of language processing applications.<br/><br/>The key observation underlying this CAREER project is that word meaning distinctions differ from one lexical resource to another and that the optimality of word meaning representations should be dictated by the target application. The project is exploring rich and flexible word meaning representations that combine the benefits of multiple monolingual and cross-lingual lexical resources and that can be adapted to the context and to the target application. In particular, the multilingual nature of these representations allows for an effective exploitation of the knowledge and resources available in different languages. The project also explores the role played by these word meaning representations and the corresponding monolingual and cross-lingual knowledge sources in several natural language processing tasks including lexical substitution, word and text translation, and text-to-text semantic similarity.<br/><br/>Another aim of the project is to integrate natural language processing into educational applications, and explore the use of the word meaning interpretation models to build a comprehension-assistant tool for students of English as a second language (ESL) and English as a foreign language (EFL).  The educational program also fosters increased awareness about research in multilingual natural language processing among college, undergraduate, and graduate students, through a college outreach program and a new course on multilingual computational linguistics, as well as increased exposure of students to international experiences through international collaborations.<br/>"
"1264982","GOALI: 2D Optoelectronic Graphene Nanoprobes for Neural Network","CBET","GOALI-Grnt Opp Acad Lia wIndus, BIOSENS-Biosensing","07/01/2013","06/25/2015","Yaqiong Xu","TN","Vanderbilt University","Continuing Grant","Chenzhong Li","06/30/2017","$300,000.00","Qi Zhang, Miles Li, George Peeters","yaqiong.xu@vanderbilt.edu","Sponsored Programs Administratio","Nashville","TN","372350002","6153222631","ENG","1504, 7909","142E, 1504, 9150","$0.00","1264982 - Xu<br/><br/>The objective of this research is to investigate, across submicron to centimeter size scales, the principle of signal processing from single synapse to multi-neuron circuitry, via a 2D free-standing optoelectronic graphene platform. The approach is to grow neurons directly on a large-area suspended graphene membrane, which is part of an ultra-sensitive graphene transistor in a liquid environment. Through graphene photoconductance measuring, patch-clamp recording, and optical imaging, the electrical, chemical, and optical changes in axons, dendrites and synapses across a neuronal circuit will be investigated.<br/><br/>General statement:<br/>Neuronal activity is typically mapped using electrode arrays that stimulate neuronal cells and record their electrical responses.  This system has several limitations: the number of electrodes is typically small and placement of cells in relation to electrodes is not well defined.  This proposal will deploy novel material, grapheme, to be used as the surface for culturing neurons.  Grapheme has optoelectrical effect, meaning that electrical activity of cells may be monitored optically using a microscope.  This would circumvent limitations of electrode arrays and would allow for monitoring activity of a much larger number of neuronal cells."
"1319212","TWC: Small: Unsupervised and Statistical Natural Language Processing Techniques for Automatic Phishing and Opinion Spam Detection","CNS","Secure &Trustworthy Cyberspace","10/01/2013","07/17/2018","Rakesh Verma","TX","University of Houston","Standard Grant","Wei-Shinn Ku","09/30/2019","$408,167.00","","rverma@uh.edu","4800 Calhoun Boulevard","Houston","TX","772042015","7137435773","CSE","8060","7434, 7923","$0.00","In phishing, an attacker tries to steal sensitive information, e.g., bank/credit card account numbers, login information, etc., from Internet users. The US society and economy are increasingly dependent on the Internet and the web, which is plagued by phishing. One popular phishing method is to create a site that mimics a good site and then attract users to it via email, which is by far the most popular medium to entice unsuspecting users to the phishing site. Because of this modus operandi and the damages caused by phishing, it is important to design efficient and effective classifiers for emails and web sites.  <br/><br/>In this project, new techniques, inspired from natural language processing methods, are being designed for phishing email and web site detection. They are then implemented and validated rigorously on realistic data sets. They are also applied to automatic detection of opinion spam. <br/><br/>Proposed research is expected to:  (i) be useful in pushing the envelope of natural language processing techniques, and (ii) yield new applications of these techniques in cyber security. In the past, the PI has been very successful in involving women and minorities including underrepresented minorities in his research and this effort will be continued in this project. University of Houston has been recognized as a Hispanic-serving institution and the PI will continue his past successful efforts to involve underrepresented minorities including African Americans and Hispanics in this research. This research will be moved into the classroom and broadly disseminated through publications and software on the web."
"1352249","EAGER: Generating and Understanding Narratives for Dynamic Environments","IIS","Robust Intelligence","09/01/2013","08/29/2013","Hanna Hajishirzi","WA","University of Washington","Standard Grant","Hector Munoz-Avila","02/29/2016","$149,857.00","","hannaneh@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7495","7495, 7916","$0.00","Narrative generation is the process of generating textual descriptions of action in dynamic environments such as movies, sports events and educational programs. On-line narration of a dynamic environment is beneficial in a wide range of contexts, from entertainment to training and education. For example, successfully narrating a video would allow blind and visually-impaired people to follow visual cues that are important to understanding the video.  Key to attaining this goal is the ability to translate natural language into a form that is understandable by computers.  A particular challenge is to do this not for a specifically chosen domain, but in a general way that is suitable for adaptation to  a wide range of natural dynamic environments.  This project explores new directions to tackle these extremely challenging, yet crucial, issues, undertaking exploratory research towards building essential components of a domain-adaptive framework that learns to understand and generate narratives on-line for natural dynamic environments with minimal supervision by human experts.  This research explores methods to generate narratives on-line by learning the natural dynamics of the environment, automatically forming templates, and deciding when and what to mention. <br/><br/>Many natural language applications are concerned with recognition of paraphrases and semantic understanding. The software and data resulting from this project are potentially useful for semantic analysis in natural language processing, and is being made available for research purposes. This work is designed for significant social impact through a broad range of applications including educational, entertainment, and accessibility.  A narrative generation system could be beneficial to visually-impaired people to better understand videos over the internet. In addition, such a system can help broadcasting companies to report news or sports events with customized commentaries for different users. This project also provides research and collaborative work experience to undergraduate and graduate students including under-represented and minority groups. <br/>"
"1251131","BIGDATA: Small: DA: Big Multilinguality for Data-Driven Lexical Semantics","IIS","Big Data Science &Engineering","08/01/2013","06/20/2013","Noah Smith","PA","Carnegie-Mellon University","Standard Grant","Maria Zemankova","09/30/2015","$249,994.00","Chris Dyer","nasmith@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","8083","7433, 7923, 8083","$0.00","A key challenge in natural language processing is defining the computational representation of words.  Data-driven distributional approaches use corpora to induce vector-space representations for words, based on the contexts they occur in.  This project goes beyond traditional approaches (e.g., latent semantic analysis; Deerwester et al., 1990), which use words that tend to occur near a word in corpora to define the context, by extending the types of contexts used in constructing semantic vectors.  First, this project incorporates translation contexts, i.e., words readily available in multilingual parallel corpora, alongside traditional monolingual corpora.  This allows evidence-sharing across languages, most importantly from resource-rich languages with large corpora to more resource-poor languages.  Second, this project incorporates social context inferable from social network platforms, captured through author, time, geographic, and social connection metadata.  Taken together, these additional features give a broader definition of a word's context and lead to a more unified approach to the distributional approach to modeling human language, moving in the direction of a language-independent semantics.  The project focuses on ten typologically diverse languages representing several major language families (English, Arabic, Chinese, Spanish, Russian, German, Portuguese, Swahili, Malagasy, and Farsi). A key emphasis is scaling up algorithms for inferring distributional representations to web-scale corpora and dealing with much larger contextual vectors representing the expanded notion of context.  The approach also leverages noisy syntactic processing to enable syntactic information, rather than just information about neighboring words, to be considered when defining context.<br/><br/>In addition to improving the quality of the learned lexico-semantic representations by including richer contextual information, this project creates lexical semantic representations that link word types across languages. These have direct use in text processing applications such as text categorization, machine translation, information extraction, and semantic analysis of text, and they will enable the construction of robust lexical semantic resources in lower-resource languages that benefit from the richness of resources in languages they are paired with.  The multilingual vector representations produced will be released to the research community and will be used in undergraduate class projects.  The project provides integrated educational and research experience for two graduate students in a dynamic research environment.  The project website (http://www.ark.cs.cmu.edu/BigMultilinguality) will be used for dissemination of results."
"1330214","TWC SBE: Option: Frontier: Collaborative: Towards Effective Web Privacy Notice and Choice: A Multi-Disciplinary Prospective","CNS","Secure &Trustworthy Cyberspace","09/01/2013","05/28/2020","Thomas Norton","NY","Fordham University","Continuing Grant","Nina Amla","02/28/2021","$500,340.00","","tnorton1@fordham.edu","441 E. Fordham Road","Bronx","NY","104585149","7188174086","CSE","8060","025Z, 7434, 8087","$0.00","Natural language privacy policies have become a de facto standard to address expectations of notice and choice on the Web. Yet, there is ample evidence that users generally do not read these policies and that those who occasionally do struggle to understand what they read. Initiatives aimed at addressing this problem through the development of machine implementable standards or other solutions that require website operators to adhere to more stringent requirements have run into obstacles, with many website operators showing reluctance to commit to anything more than what they currently do. This project offers the prospect of overcoming the limitations of current natural language privacy policies without imposing new requirements on website operators.<br/><br/>This frontier project builds on recent advances in natural language processing, privacy preference modeling, crowdsourcing, formal methods, and privacy interfaces to overcome this situation.  It combines fundamental research with the development of scalable technologies to semi-automatically extract key privacy policy features from natural language website privacy policies and present these features to users in an easy-to-digest format that enables them to make more informed privacy decisions as they interact with different websites.  Work in this project also involves the systematic collection and analysis of website privacy policies, looking for trends and deficiencies both in the wording and content of these policies across different sectors and using this analysis to inform ongoing public policy debates. An important part of this project is to work closely with stake holders in industry to enable the transfer of these technologies to industry for large-scale deployment."
"1330141","TWC SBE: Option: Frontier: Collaborative: Towards Effective Web Privacy Notice and Choice: A Multi-Disciplinary Prospective","CNS","Secure &Trustworthy Cyberspace","09/01/2013","07/29/2016","Barbara van Schewick","CA","Stanford University","Continuing grant","Dan Cosley","02/28/2018","$490,409.00","","schewick@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","8060","7434, 8087","$0.00","Natural language privacy policies have become a de facto standard to address expectations of notice and choice on the Web. Yet, there is ample evidence that users generally do not read these policies and that those who occasionally do struggle to understand what they read. Initiatives aimed at addressing this problem through the development of machine implementable standards or other solutions that require website operators to adhere to more stringent requirements have run into obstacles, with many website operators showing reluctance to commit to anything more than what they currently do. This project offers the prospect of overcoming the limitations of current natural language privacy policies without imposing new requirements on website operators.<br/><br/>This frontier project builds on recent advances in natural language processing, privacy preference modeling, crowdsourcing, formal methods, and privacy interfaces to overcome this situation.  It combines fundamental research with the development of scalable technologies to semi-automatically extract key privacy policy features from natural language website privacy policies and present these features to users in an easy-to-digest format that enables them to make more informed privacy decisions as they interact with different websites.  Work in this project also involves the systematic collection and analysis of website privacy policies, looking for trends and deficiencies both in the wording and content of these policies across different sectors and using this analysis to inform ongoing public policy debates. An important part of this project is to work closely with stake holders in industry to enable the transfer of these technologies to industry for large-scale deployment."
"1311851","AIR Option 1:  Technology Translation - Feasibility Analysis of Self-Administered Eye Tactile Tonometer","IIP","Accelerating Innovation Rsrch","06/01/2013","05/06/2015","Eniko Enikov","AZ","University of Arizona","Standard Grant","Barbara H. Kenny","11/30/2015","$186,000.00","","enikov@engr.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","ENG","8019","116E, 8019, 9231, 9251","$0.00","This PFI: AIR Technology Translation project focuses on translating micro-electromechanical (MEMS) tactile sensing and signal processing science to fill a non-invasive eye-pressure measurement technology gap.  The translated technology has the following unique features: the ability to detect minute changes in the physical characteristics of soft materials and tissues, the ability to detect and measure pressure inside closed cavities in tissues and organs such as the human eye, and the ability to recognize changes that might occur due to aging processes.  The translated technology provides exemplary cost savings, convenience, and efficacy when compared to the leading competing corneal tonometry in this market space.  <br/><br/>The project accomplishes this goal by utilizing the latest advances in sensor miniaturization, soft tissue biomechanical modeling, and application of artificial neural network (ANN) signal processing techniques, resulting in an easy-to-use self-administered home tonometer.  The partnership engages Arch Partners LLC, EPV Sensors LLC, Arizona Retinal Specialists LLC, and the University of Arizona to provide guidance in the home tonometry market space and other aspects: evaluation and testing of the device, financing, commercialization, and marketing as they pertain to the potential to translate the tactile sensing technology along a path that may result in a competitive commercial reality.  <br/><br/>The potential economic impact is expected to be over $2B in world-wide sales in the next 5 to 10 years, which will contribute to the U.S. competitiveness in the home tonometry market space.   The societal impact, long term, will be better management of glaucoma affecting over 4.3 M Americans each year, reduced disability rates, lower healthcare delivery costs, and greater access to eye diagnostics and care."
"1304398","Collaborative Proposal: The Role of Arctic Amplification in Modifying Mid-latitude Atmospheric Circulation and Promoting Extreme Weather Events","OPP","ARCSS-Arctic System Science","07/01/2013","06/27/2018","Stephen Vavrus","WI","University of Wisconsin-Madison","Standard Grant","Gregory Anderson","06/30/2019","$380,922.00","","sjvavrus@wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","GEO","5219","1079","$0.00","Intellectual Merit: recent work has suggested and demonstrated a mechanism connecting Arctic Amplification (AA ? enhanced Arctic warming) with an increased probability of extreme weather events in northern hemisphere mid-latitudes, as well as a feedback to arctic wind patterns that drive further ice loss. This study will extend this work using output from reanalyses, model simulations from the CMIP5 archive, and experimental CESM model runs to investigate how and why the ongoing loss of arctic sea ice and high-latitude terrestrial snow cover may cause mid-latitude weather patterns to change. Past and projected patterns in 500 hPa heights will be analyzed using a combination of Self-Organizing Maps (SOMs) ? a neural-network-based technique that reduces large data sets to their representative patterns ? a new meridional circulation index, and an extreme weather index to address the hypothesis that Arctic Amplification of past and future global climate change promotes circulation patterns that favor extreme weather events in middle latitudes of the northern hemisphere. The PIs suggest that changes in the energy budget of the arctic surface, because of sea-ice loss and earlier snow melt on high-latitude land, reduce meridional thickness gradients, which induce generally weaker zonal flow aloft and higher amplitude circulation features. They think these upper-level flow patterns slow the eastward progression of large-scale atmospheric Rossby waves, favoring more persistent weather conditions that increase the probability of extreme weather such as cold-air outbreaks, heat waves, droughts, and heavy precipitation. They believe the primary physical mechanism driving the change is an enhanced and seasonally varying arctic heating: in fall/winter it is ocean-based associated with substantial sea ice loss, while in warmer months it is land-based due primarily to earlier snow melt and reduced soil moisture. Higher-amplitude flow trajectories may also exacerbate sea-ice and snow-cover loss, thereby constituting a positive feedback loop.<br/><br/>Because extreme weather events, such as temperature and precipitation extremes, are often caused by high-amplitude, slow-moving upper-level circulation patterns, this analysis will focus on detecting and characterizing these patterns, interpreting the weather conditions associated with them, and identifying regions that are likely to be affected. This study attempts to connect arctic change with the rest of the global climate system.<br/><br/>Broader Impacts: Changes in extreme weather as a result of greenhouse gas accumulation will directly and dramatically affect millions of people across the globe, thus a better understanding and more accurate future projection of changes in the frequency, location, and severity of droughts, floods, and temperature extremes are urgently needed by decision-makers at all levels of society. Preliminary work has already attracted world-wide attention via articles in prominent newspapers, radio interviews, interviews for the Weather Channel, magazine articles, and numerous online blogs. This will also be an opportunity for a new post-doc. The PIs will prepare a module for the NSF-supported ?Beyond Penguins and Polar Bears? project for K-5 teachers. Both PIs make frequent public and school presentations on climate-change related issues, and because extreme weather is so relevant to society at large, results from the proposed study will figure prominently into these presentations."
"1319318","RI: Small: Closing the Loop: Inducing High-Precision Grammars for Generating Disambiguating Paraphrases","IIS","ROBUST INTELLIGENCE","09/01/2013","11/07/2017","Michael White","OH","Ohio State University","Continuing grant","Tatiana Korelsky","11/30/2018","$307,982.00","","mwhite@ling.ohio-state.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","7495","7495, 7923, 9251","$0.00","This project investigates trainable methods of paraphrasing natural language sentences to effectively disambiguate their meaning, using precise, bidirectional grammars induced from corpora to ""close the loop""' between parsing and generation.  The approach generalizes previous work on probabilistically avoiding ambiguity in natural language generation to a broad coverage setting, disambiguating only as necessary in order to better balance clarity and readability. Generating disambiguating paraphrases in a broad coverage setting makes it possible to explore ways of adapting parsers to new domains using crowd-sourced judgments of meaning similarity.  Accordingly, the project explores methods of (1) inducing OpenCCG grammars from the dependency output of parsers such as the C&C parser, (2) generating paraphrases with OpenCCG that explicitly aim to avoid likely distractor interpretations, (3) collecting meaning similarity judgments between the original sentence and paraphrases of its most likely interpretations, and (4) retraining the parser using the collected judgments. To evaluate the approach while also conducting outreach, the project involves data collection and experimentation at Ohio State's language research pod at the COSI science museum, in addition to the use of Amazon's Mechanical Turk.<br/><br/>By closing the loop between interpretation and generation, the project  promises to dramatically enhance the prospects for using crowd-sourcing to adapt natural language processing tools to new domains.  The project will also enable international collaborations with the University of Sydney, and help to educate the public about language science and technology, providing an inspirational example of science in action to the children who attend COSI."
"1218683","RI: Small: Collaborative Research: Detecting Abnormalities in Images","IIS","ROBUST INTELLIGENCE","05/15/2013","05/07/2013","Ali Farhadi","WA","University of Washington","Standard Grant","Kenneth C. Whang","04/30/2016","$120,000.00","","afarhad2@gmail.com","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7495","7495, 7923","$0.00","Computer interpretation of images has taken huge strides in recent years, but even the most modern algorithms can't come close to matching human capabilities on simple visual tasks. For example, in a brief glance at an image, people reflexively classify the objects in it in terms of the categories they belong to--people, animals, tools, and other significant classes. This allows us to understand the objects' meaning in the image, for example understanding that a scene with many pieces of food might be a dinner table. Because even modern computer vision systems can't make such a classification, they can't automatically detect when an object in a scene doesn't belong, that is, when it is abnormal relative to the categories present in the scene. Detecting such ""oddball"" or atypical objects is essential to understanding visual scenes, because objects that don't belong are often the ones that play the most important role and require immediate action (like a cat on the dinner table). Studies of human subjects have shown that humans are indeed especially adept at detecting atypical items, which often draw our visual attention even before we become consciously aware of them.<br/><br/>This project aims at developing algorithmic techniques to endow computer visions systems with the same ability. By adapting modern vision techniques to mimic the way human observers classify visual atypicality, researchers will develop computer systems that can examine an image and automatically detect abnormal objects, as well as identifying the nature of the abnormality and quantifying the degree of abnormality. The project involves a collaboration among researchers at multiple universities and multiple scientific specialties, including both computer vision and human vision.  The result will be a new and useful class of computer vision techniques that can be applied to visual image understanding in many contexts."
"1218872","RI: Small: Collaborative Research: Detecting Abnormalities in Images","IIS","ROBUST INTELLIGENCE","05/15/2013","05/07/2013","Ahmed Elgammal","NJ","Rutgers University New Brunswick","Standard Grant","Kenneth C. Whang","04/30/2017","$339,962.00","Jacob Feldman","elgammal@cs.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7495","7495, 7923","$0.00","Computer interpretation of images has taken huge strides in recent years, but even the most modern algorithms can't come close to matching human capabilities on simple visual tasks. For example, in a brief glance at an image, people reflexively classify the objects in it in terms of the categories they belong to--people, animals, tools, and other significant classes. This allows us to understand the objects' meaning in the image, for example understanding that a scene with many pieces of food might be a dinner table. Because even modern computer vision systems can't make such a classification, they can't automatically detect when an object in a scene doesn't belong, that is, when it is abnormal relative to the categories present in the scene. Detecting such ""oddball"" or atypical objects is essential to understanding visual scenes, because objects that don't belong are often the ones that play the most important role and require immediate action (like a cat on the dinner table). Studies of human subjects have shown that humans are indeed especially adept at detecting atypical items, which often draw our visual attention even before we become consciously aware of them.<br/><br/>This project aims at developing algorithmic techniques to endow computer visions systems with the same ability. By adapting modern vision techniques to mimic the way human observers classify visual atypicality, researchers will develop computer systems that can examine an image and automatically detect abnormal objects, as well as identifying the nature of the abnormality and quantifying the degree of abnormality. The project involves a collaboration among researchers at multiple universities and multiple scientific specialties, including both computer vision and human vision.  The result will be a new and useful class of computer vision techniques that can be applied to visual image understanding in many contexts."
"1330596","TWC SBE: Option: Frontier: Collaborative: Towards Effective Web Privacy Notice and Choice: A Multi-Disciplinary Prospective","CNS","Special Projects - CNS, Secure &Trustworthy Cyberspace","09/01/2013","12/13/2018","Norman Sadeh","PA","Carnegie-Mellon University","Continuing Grant","Nina Amla","08/31/2019","$3,443,159.00","Lorrie Cranor, Alessandro Acquisti, Noah Smith, Travis Breaux","sadeh@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","1714, 8060","025Z, 7434, 8087, 9178, 9251","$0.00","Natural language privacy policies have become a de facto standard to address expectations of notice and choice on the Web. Yet, there is ample evidence that users generally do not read these policies and that those who occasionally do struggle to understand what they read. Initiatives aimed at addressing this problem through the development of machine implementable standards or other solutions that require website operators to adhere to more stringent requirements have run into obstacles, with many website operators showing reluctance to commit to anything more than what they currently do. This project offers the prospect of overcoming the limitations of current natural language privacy policies without imposing new requirements on website operators.<br/><br/>This frontier project builds on recent advances in natural language processing, privacy preference modeling, crowdsourcing, formal methods, and privacy interfaces to overcome this situation.  It combines fundamental research with the development of scalable technologies to semi-automatically extract key privacy policy features from natural language website privacy policies and present these features to users in an easy-to-digest format that enables them to make more informed privacy decisions as they interact with different websites.  Work in this project also involves the systematic collection and analysis of website privacy policies, looking for trends and deficiencies both in the wording and content of these policies across different sectors and using this analysis to inform ongoing public policy debates. An important part of this project is to work closely with stake holders in industry to enable the transfer of these technologies to industry for large-scale deployment."
"1260844","Doctoral Dissertation Improvement:  The evolution and genetic basis of primate brain cortical gyrification in a pedigreed Papio population","BCS","Biological Anthropology","02/01/2013","02/22/2013","James Cheverud","MO","Washington University","Standard Grant","Rebecca Ferrell","01/31/2015","$30,400.00","Elizabeth Atkinson","jcheverud@luc.edu","CAMPUS BOX 1054","Saint Louis","MO","631304862","3147474134","SBE","1392","1392, 9179, SMET","$0.00","Elevated cognition, related to the dramatic increase in brain volume, is a hallmark of the primates. Cognition is directly related to the number of neurons in the cerebral cortex and the connectivity network underlying information processing in the brain. Increased cortical folding (gyrification) allows for more neurons to be contained within the volume of the braincase and the arrangement of folds and ridges across the cerebral cortex is an indication of the underlying neural network connecting regions. This dissertation project by Elizabeth Atkinson (Washington University), under the supervision of Dr. James Cheverud, uses a pedigreed baboon population to assess the genetic basis and structure of cortical folding in primates. It then further investigates the genomic regions identified in baboons to determine the role of natural selection in altering the DNA sequence of genes across the primate family tree. Questions addressed include: Is the organizational structure of the cerebral cortex modular? Are brain regions that develop together in utero or work together throughout an organism's life also inherited together? How susceptible to developmental noise is the cerebral cortex? What chromosomal regions and genes affect variation in brain function and cortical traits? Is there evidence that genes affecting cortical traits have been selectively altered at an evolutionary scale? Overall, this project addresses critical questions in human and primate brain evolution, including the genetic architecture of brain morphology, differing cognitive strategies across the primate clade, and the evolutionary mechanisms responsible for their formation.<br/><br/>In addition to addressing issues of the genetics of brain morphology and cognition in primates, this project also will involve mentoring undergraduates and local high school students from ethnicities and socioeconomic groups typically underrepresented in the sciences, teaching evolutionary concepts at inner-city St. Louis schools, furthering the efforts of a graduate student-run program for science outreach, and disseminating the notable findings from this project to other scientists at meetings and conferences. Participating in this project will further the scientific career and curriculum vitae of an underprivileged St. Louis high school student, mentored closely over the course of the summer of 2013, who otherwise would have few opportunities to engage in scientific research at this early point in their academic development."
"1418815","ACL 2013 Student Research Workshop","IIS","Robust Intelligence","09/01/2013","12/20/2013","Steven Bethard","AL","University of Alabama at Birmingham","Standard Grant","Tatiana Korelsky","02/28/2014","$19,000.00","","bethard@email.arizona.edu","AB 1170","Birmingham","AL","352940001","2059345266","CSE","7495","7495, 7556","$0.00","The Association for Computational Linguistics (ACL) is the primary international organization in the field of natural language processing and computational linguistics. The ACL's annual conference is the major international conference in this field. This project is to subsidize travel, conference and housing expenses of students selected to participate in the ACL Student Research Workshop, which will take place during the main ACL conference on August 4-9, 2013 in Sofia, Bulgaria. The Student Research Workshop accepts papers in two categories: thesis/research proposal and general research papers. The thesis/research proposal can have only one author who must be a student. The research papers can have multiple authors, with the first author being a student. The workshop is organized and run by students.<br/><br/>The Student Research Workshop provides a valuable opportunity for the next generation of natural language processing researchers to enter the research community. It allows the students in the field to take an important step towards becoming professional computational linguists by receiving critical feedback on their work from experts outside of their dissertation committee, and by making contacts with other students and senior researchers in their field. The students who are involved in running and reviewing for the student workshop also gain valuable opportunities for professional growth and interaction with the researchers on the organizing committee of the main conference. The ACL Student Research Workshop contributes to the maintenance and development of a skilled and diverse computational linguistics and natural language processing community."
"1343068","NAACL-HLT 2013 Student Research Workshop","IIS","ROBUST INTELLIGENCE","07/01/2013","06/19/2013","Eric Ringger","UT","Brigham Young University","Standard Grant","Tatiana D. Korelsky","06/30/2014","$15,000.00","","ringger@cs.byu.edu","A-285 ASB","Provo","UT","846021231","8014223360","CSE","7495","7495, 7556, 9150","$0.00","The Association for Computational Linguistics (ACL) is the primary international organization in the field of natural language processing and computational linguistics. The NAACL HLT conference is the ACL's major conference held annually in North America. This project subsidizes travel, conference registration, and housing expenses of students selected to participate in the NAACL HLT Student Research Workshop, which takes place during the main conference held on June 10-12 in Atlanta and in a one-day workshop on June 13th. The workshop accepts papers in two categories: thesis proposals and general research papers.  The thesis/research proposal can have only one author who must be a student.  The research papers can have multiple authors, with the first author being a student. The workshop is organized and run by students.<br/><br/>The Student Research Workshop provides a valuable opportunity for the next generation of natural language processing researchers to enter the research community. It allows the students in the field to take an important step toward becoming professional computational linguists by receiving critical feedback on their work from experts outside of their dissertation committee, and by making contacts with other students and senior researchers in their field. The students who are involved in running and reviewing for the workshop also gain valuable opportunities for professional growth and interaction with the researchers on the organizing committee of the main conference. The NAACL HLT Student Research Workshop contributes to the maintenance and development of a skilled and diverse computational linguistics and natural language processing community."
"1321834","ACL 2013 Student Research Workshop","IIS","Robust Intelligence","03/01/2013","02/20/2013","Steven Bethard","CO","University of Colorado at Boulder","Standard Grant","Tatiana Korelsky","01/31/2014","$19,000.00","","bethard@email.arizona.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","7495","7495, 7556","$0.00","The Association for Computational Linguistics (ACL) is the primary international organization in the field of natural language processing and computational linguistics. The ACL's annual conference is the major international conference in this field. This project is to subsidize travel, conference and housing expenses of students selected to participate in the ACL Student Research Workshop, which will take place during the main ACL conference on August 4-9, 2013 in Sofia, Bulgaria. The Student Research Workshop accepts papers in two categories: thesis/research proposal and general research papers. The thesis/research proposal can have only one author who must be a student. The research papers can have multiple authors, with the first author being a student. The workshop is organized and run by students.<br/><br/>The Student Research Workshop provides a valuable opportunity for the next generation of natural language processing researchers to enter the research community. It allows the students in the field to take an important step towards becoming professional computational linguists by receiving critical feedback on their work from experts outside of their dissertation committee, and by making contacts with other students and senior researchers in their field. The students who are involved in running and reviewing for the student workshop also gain valuable opportunities for professional growth and interaction with the researchers on the organizing committee of the main conference. The ACL Student Research Workshop contributes to the maintenance and development of a skilled and diverse computational linguistics and natural language processing community."
"1338378","CNIC:US-UAE Planning Visit: Development of Research Collaborations on Spatio-temporal Modeling and Analysis of Mobile Sensor Data in Evaluating Environmental Exposures","OISE","Catalyzing New Intl Collab","10/01/2013","08/28/2013","Wan Bae","WI","University of Wisconsin-Stout","Standard Grant","Anne Emig","09/30/2014","$34,598.00","Cheng Liu","baew@uwstout.edu","PO Box 790","Menomonie","WI","547510790","7152321123","O/D","7299","5976, 7406","$0.00","1338378<br/>Bae<br/>This project will support a team headed by Dr. Wan Bae, University of Wisconsin-Stout, Menomonie, WI for a two week visit to the United Arab Emirates (UAE) for the establishment of new international research collaborations between researchers in the two countries. This visit will enable the PI and Dr. Cheng Liu and two undergraduate students from the UWI-Stout, Dr. Petr Vojtchovsky from the University of Denver, and Dr. Shashi Shekhar from the University of Minnesota, to meet with Dr. Shayma Alkobaisi, Dr. Ahmed Al Faresi, Dr. Mohammad Masud, Dr. Fatma Maskari and their students from the United Arab Emirates University, and Dr. Ibrahim Kamel from the University of Sharjah to develop a research framework for modeling and analysis of individual exposure to various environmental conditions. The research will focus on developing data models and computing algorithms for effectively mapping individuals? environmental exposure to their health conditions, and implementing Map/Reduce methods for efficiently processing iterative computations of the proposed models and algorithms. As a result, the research team will submit a subsequent grant proposal targeted for the NSF Smart Health and Wellbeing (SHB) program.<br/>Intellectual Merit: Relations between negative health effects like asthma and lung cancer and elevated levels of the environmental factors, such as air pollution, tobacco smoke and humidity, have been detected in several large scale exposure studies. Evaluating environmental exposures often requires the ability to track, monitor, store, and analyze individual moving trajectories along with several environmental conditions the individual is exposed to in order to identify relationships among these data. Challenges arise due to spatio-temporal uncertainty, data size, and iterative computations of commonly used data modeling algorithms such as the Back propagation neural network algorithm. The main objectives of this research are: (1) to develop novel data models to map individuals environmental exposures to health levels, (2) to design a new technique for implementing the proposed models on the Map/Reduce paradigm of the Hadoop system, (3) to develop data analysis algorithms to characterize behaviors in learned models and interpret the data for estimating their effects on human health, (4) to build an evaluation system for Asthma patients as a case study. The research team, consisting of mathematicians, computer and information scientists, engineers and medical expertise, is capable of carrying out the planned tasks.<br/>Broader Impacts: The project will support two U.S. undergraduate students to be actively involved in scientific research.  Their involvement is designed to integrate research and education through various activities. Gaining experience with inter-cultural collaboration is one of the mutual benefits to both the U.S. and UAE students. The also promotes diversity with the involvement by students from the U.S. who may be first-generation college students and UAE national students. The U.S. and UAE researchers will build new relationships that are the basis for future collaborations in research and education. Further, this project will broaden the understanding of the impact of the environment on public health and the importance of individual-based health care for patients, doctors, and healthcare providers."
"1304097","Collaborative Proposal: The Role of Arctic Amplification in Modifying Mid-latitude Atmospheric Circulation and Promoting Extreme Weather Events","OPP","ARCSS-Arctic System Science","07/01/2013","06/20/2016","Jennifer Francis","NJ","Rutgers University New Brunswick","Standard Grant","Gregory Anderson","06/30/2018","$315,807.00","","jfrancis@whrc.org","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","GEO","5219","1079","$0.00","Intellectual Merit: recent work has suggested and demonstrated a mechanism connecting Arctic Amplification (AA ? enhanced Arctic warming) with an increased probability of extreme weather events in northern hemisphere mid-latitudes, as well as a feedback to arctic wind patterns that drive further ice loss. This study will extend this work using output from reanalyses, model simulations from the CMIP5 archive, and experimental CESM model runs to investigate how and why the ongoing loss of arctic sea ice and high-latitude terrestrial snow cover may cause mid-latitude weather patterns to change. Past and projected patterns in 500 hPa heights will be analyzed using a combination of Self-Organizing Maps (SOMs) ? a neural-network-based technique that reduces large data sets to their representative patterns ? a new meridional circulation index, and an extreme weather index to address the hypothesis that Arctic Amplification of past and future global climate change promotes circulation patterns that favor extreme weather events in middle latitudes of the northern hemisphere. The PIs suggest that changes in the energy budget of the arctic surface, because of sea-ice loss and earlier snow melt on high-latitude land, reduce meridional thickness gradients, which induce generally weaker zonal flow aloft and higher amplitude circulation features. They think these upper-level flow patterns slow the eastward progression of large-scale atmospheric Rossby waves, favoring more persistent weather conditions that increase the probability of extreme weather such as cold-air outbreaks, heat waves, droughts, and heavy precipitation. They believe the primary physical mechanism driving the change is an enhanced and seasonally varying arctic heating: in fall/winter it is ocean-based associated with substantial sea ice loss, while in warmer months it is land-based due primarily to earlier snow melt and reduced soil moisture. Higher-amplitude flow trajectories may also exacerbate sea-ice and snow-cover loss, thereby constituting a positive feedback loop.<br/><br/>Because extreme weather events, such as temperature and precipitation extremes, are often caused by high-amplitude, slow-moving upper-level circulation patterns, this analysis will focus on detecting and characterizing these patterns, interpreting the weather conditions associated with them, and identifying regions that are likely to be affected. This study attempts to connect arctic change with the rest of the global climate system.<br/><br/>Broader Impacts: Changes in extreme weather as a result of greenhouse gas accumulation will directly and dramatically affect millions of people across the globe, thus a better understanding and more accurate future projection of changes in the frequency, location, and severity of droughts, floods, and temperature extremes are urgently needed by decision-makers at all levels of society. Preliminary work has already attracted world-wide attention via articles in prominent newspapers, radio interviews, interviews for the Weather Channel, magazine articles, and numerous online blogs. This will also be an opportunity for a new post-doc. The PIs will prepare a module for the NSF-supported ?Beyond Penguins and Polar Bears? project for K-5 teachers. Both PIs make frequent public and school presentations on climate-change related issues, and because extreme weather is so relevant to society at large, results from the proposed study will figure prominently into these presentations."
"1416201","Developing and Testing New Geospatial Approaches in Paleoanthropology","BCS","GEOGRAPHY AND SPATIAL SCIENCES, Biological Anthropology","11/08/2013","01/13/2014","Robert Anemone","NC","University of North Carolina Greensboro","Standard Grant","Rebecca Ferrell","08/31/2015","$110,880.00","","robert.anemone@uncg.edu","1111 Spring Garden Street","Greensboro","NC","274125013","3363345878","SBE","1352, 1392","1352, 1392","$0.00","Paleontologists search for fossils today in very nearly the same ways that our predecessors have since the beginnings of the discipline in the nineteenth century. They study geological and topographic maps in order to locate places where fossils of a certain age may be found, and then they walk long distances with their eyes scouring the ground for hints of eroding fossils.  As a result, many important paleontological sites are literally stumbled upon, and chance and luck continue to play a large role in the success or failure of many paleontological expeditions.<br/><br/>This interdisciplinary research utilizes state-of-the-art imaging methods and analytical techniques from remote sensing and the spatial sciences to develop and test new predictive models for determining where paleontologists should concentrate their efforts in the field in order to maximize their effectiveness at finding productive fossil-bearing localities.  The investigators will use Landsat imagery as well as high-resolution, commercially available satellite imagery to determine the spectral characteristics of known productive localities in the Eocene deposits of Wyoming's Great Divide Basin.  A number of analytical approaches will be tested for their ability to identify the spectral signatures of productive localities, including artificial neural network analysis and geographic object-based image analysis.  The research team will spend two summer field seasons searching those areas on the ground that are predicted to have a high potential to be fossil-bearing, in order to statistically evaluate the success of the predictive models.<br/><br/>The international research team includes specialists from vertebrate paleontology, paleoanthropology, geology and geography, and their aim is to stimulate the application of new approaches from the geographic and spatial sciences to the sciences of paleoanthropology and paleontology. Broader impacts include the training of undergraduate and graduate students in this cross-disciplinary approach to field-based anthropological science, and development of web-based tools for education and outreach."
"1332030","STTR Phase I: Predictive Control Systems for Nickel Zinc Flow Assisted Systems","IIP","STTR PHASE I","07/01/2013","06/25/2013","Valerio DeAngelis","NY","Urban Electric Power Inc.","Standard Grant","Muralidharan S. Nair","06/30/2014","$224,901.00","Sanjoy Banerjee","valerio@uepinc.com","Greeley Sq Station, 39 W 31st St","New York","NY","100019994","6504502680","ENG","1505","1505, 6840, 8035, 9139, HPCC","$0.00","This Small Business Technology Transfer Research (STTR) Phase I project aims to improve predictive battery models and control systems for grid-scale energy storage applications. A typical grid-scale battery system is composed of thousands of individual batteries that may have initial material and manufacturing variations that tend to increase over time and reduce overall string efficiency, with the result that energy storage installations must be significantly over-specified, making them too expensive for many customers. There is a great opportunity to develop an integrated predictive battery modeling and control system that can determine the exact performance of each battery in operation and optimize string function. The research objectives of this SBIR project are to develop Neural Network (NN) based predictive models of battery performance using information gathered early in the life of each cell like Electrochemical Impedance Spectroscopy measurements in addition to current, voltage, and temperature measurements that can be taken throughout the life of the battery, in order to accurately estimate the state of charge and state of health of each battery in the battery string. The NN models will be incorporated into the control strategy to operate the battery string safely but aggressively, thereby decreasing the total system cost and required volume.<br/><br/>The broader impact/commercial potential of this project is to enable grid-scale energy storage by reducing system costs. The main barrier to the adoption of energy storage on the grid is its high cost. Recent advancements in energy storage technology have resulted in lower cost, longer-life batteries capable of meeting grid requirements, though there have not been the analogous transformative improvements to battery management systems to optimize system efficiency and cost-effectiveness. The innovations supported by this SBIR will enhance scientific and technical understanding of battery function and failure modes, resulting in improved battery performance and lifetimes. The addition of energy storage to the grid will have an enormous societal impact, as storage is required to firm zero-carbon renewable sources such as wind and solar and can reduce energy prices by time-shifting energy loads. While the market for these types of stationary battery systems is currently less than $5 billion, this sector is expected to surge to approximately $100 billion in the next ten years. The dominant battery management system technology for these systems has not yet been established. Commercial advanced battery controls are the key to unlocking this market and represent the next step toward a lower carbon, more sustainable energy future."
"1248867","SBIR Phase I: Enhancing the Viability of Emergency Alerting over Social Media via a Collaborative Computer-Aided System for Handling Incoming Citizen Messages","IIP","SMALL BUSINESS PHASE I","01/01/2013","05/23/2013","Hisham Kassab","MD","MobiLaps","Standard Grant","Glenn H. Larsen","12/31/2013","$155,000.00","","hkassab@mobilaps.com","PO Box 34657","Bethesda","MD","208270657","3016088222","ENG","5371","5371, 8032","$0.00","The innovation is a software engine that empowers emergency managers to analyze and process the high volume of incoming citizen messages (e.g., inquiries, calls for help, etc.) on the emergency manager's social media site(s) (e.g., Facebook, Twitter), typically experienced after an emergency alert is issued over one or more social media sites. The core of the Internet-connected software engine consists of two algorithms: a multivariate matching algorithm and a natural language processing algorithm. The matching algorithm optimally selects, based on a number of parameters, other emergency managers who can collaborate with the affected jurisdiction's emergency manager on handling the incoming messages. The natural language processing algorithm is employed to reorder incoming messages based on an intelligent keyword-driven prioritization method. The reprioritization is necessary since certain messages warrant more immediate attention than others.<br/><br/>The broader/commercial impact of the innovation will be the societal benefit of enhancing the nation's emergency notification capability and effectiveness, which in turn bolsters the nation's emergency preparedness and response efforts, potentially saving lives during life-threatening emergencies. Emergency managers currently using social media for emergency alerting will attest to its power as an alerting tool in terms of reaching targeted citizens quickly, inexpensively, and with detailed information; while immediately establishing two-way communication with citizens for the purpose of receiving requests for assistance as well as eyewitness and first-hand accounts of situational developments. Yet, the same emergency managers will warn that the already large number of incoming messages is growing as more citizens join their social media accounts, which will eventually make it unviable/unsustainable for emergency managers to manually process all the incoming messages, especially during period after the alert is issued, when their efforts are focused on responding to the emergency incident. The proposed activity is aimed at developing an intelligent collaboration-based system that will keep the social media message-processing workload manageable during emergency response periods. The insurance-inspired business model for the service ensures affordability to emergency managers and commercial success for the business."
"1223825","TWC: Small: Assessing Online Information Exposure Using Web Footprints","CNS","Secure &Trustworthy Cyberspace","01/15/2013","08/28/2012","Lisa Singh","DC","Georgetown University","Standard Grant","nan zhang","12/31/2016","$499,996.00","Micah Sherr, Grace Hui Yang","singh@cs.georgetown.edu","37th & O St N W","Washington","DC","200571789","2026250100","CSE","8060","7434, 7923, 9102","$0.00","This research project studies a new area of research - exposure detection - that is at the intersection of data mining, security, and natural language processing. Exposure detection refers to discovering components/attributes of a user's public profile that reduce the user's privacy. To help the public understand the privacy risks of sharing certain information on the web, this research project focuses on developing efficient algorithms for modeling how an adversary learns information using incomplete and schemaless public data sources. Theoretically sound and efficient techniques for identifying accurate web footprints are introduced, including: new methods for data matching using a novel probabilistic join operator on multi-granular data, automated approaches for generating inference rules, and new solutions for identifying missing information and unifying mismatched vocabulary using lightweight natural language processing and text mining. The research activities also investigate methods for quantifying and adjusting exposure and risk, facilitating a better understanding of individuals' vulnerability on the web. These techniques not only advance the state of the art in re-identification, probabilistic reasoning and inference logic, and natural language understanding, but also serve as a foundation for exposure detection."
"1316829","Comparative Analysis of a Critical Brain Structure: The Basal Ganglia","BCS","Biological Anthropology","08/01/2013","06/25/2013","Mary Ann Raghanti","OH","Kent State University","Standard Grant","Rebecca Ferrell","07/31/2017","$225,615.00","","mraghant@kent.edu","OFFICE OF THE COMPTROLLER","KENT","OH","442420001","3306722070","SBE","1392","1392, 9178, 9179","$0.00","The neocortex has been both the theoretical and the experimental focal point of evolutionary studies dedicated to identifying unique features of the human brain; however, the neocortex is only one element of a neural network underlying human intellectual abilities. It is directly connected to and dependent upon other subcortical structures, especially the basal ganglia. Traditionally, the basal ganglia were known to regulate motor functions. Recently, however, they have been revealed to be active in advanced cognition, and their regulation of prefrontal cortical activity demonstrates that they are as important as the neocortex in processes such as language, learning, and memory. The goal of the present research is to examine the role of the basal ganglia in human cognitive development by comparing metabolic and modulatory components across a wide range of primate species, including humans and our closest living relative, the chimpanzee. The methods will combine advanced stereology with immunohistochemistry in order to quantify neuron to glia ratios, neuromodulatory neurons, and neuromodulator axons within key basal ganglia areas. This study will provide new knowledge about these subcortical structures, including any reorganization that may have occurred during human evolution. This will help distinguish our basal ganglia from those of other primates, promote understanding of how they changed during primate evolution, and how phylogenetic differences may relate to cognitive processing.<br/> <br/>   The research also may contribute substantially to understanding the etiology of major neuropathologies, such as Huntington's and Parkinson's diseases, by identifying how humans differ from other species in the basal ganglia regions most affected by these diseases. Histological sections and slides of many primate species will be generated, archived, and made available to other researchers for additional comparative studies. This work also will provide training for graduate and undergraduate students, including those enrolled in the McNair Scholars Program, which prepares individuals from under-represented groups for doctoral study."
"1256526","OPUS:  Understanding Ecosystem Processes in Response to Climate Variation in a Subalpine Forest","DEB","ECOSYSTEM STUDIES","02/01/2013","03/11/2013","Russell Monson","AZ","University of Arizona","Standard Grant","Henry L. Gholz","01/31/2015","$147,482.00","","russmonson@email.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","BIO","1181","1181, 7565, 9169, EGCH","$0.00","This project involves an integrative synthesis of 52 studies published by Dr. Monson over the past 14 years on how a subalpine forest exchanges carbon dioxide and water with the atmosphere.  The project will enhance the utility of this body of work, improving its potential to inform stakeholders and land managers about the causes of several environmental challenges that have emerged in the Western US. These include an epidemic pine beetle outbreak, mass tree die-off due to persistent drought, high frequencies of wildfires, and uncertainty in the contribution of forests to regional carbon budgets. The investigator plans to: extract new, longer-term insights from a 14-year data set of forest carbon and water fluxes; use the longer-term insights to re-assess conclusions from the previously published papers; and disseminate the synthesis to those attempting to understand and manage forest ecosystems of the Western US.  The synthesis will be conducted using an advanced computer modeling procedure known as Artificial Neural Network (ANN) analysis. The ANN analysis will be conducted in a manner that elucidates how changes in weather and climate affect the ability of the forest to exchange carbon and water with the atmosphere. <br/><br/>This project will support the work of a Graduate Research Assistant (GRA) to implement the ANN analysis. The results will be disseminated to the through: (1) an article intended for professional ecologists; (2) an invited Spotlight Article intended for foresters and land managers; (3) a book chapter intended for beginning graduate students; and (4) a chapter intended for college undergraduate students in a Wiki and hardbound book."
"1248768","SBIR Phase I: Quantifying Consumer Rationale Expressed in Free Text Online Discussions","IIP","SMALL BUSINESS PHASE I","01/01/2013","06/25/2013","Paul Nemirovsky","NY","dMetrics Inc.","Standard Grant","Muralidharan S. Nair","12/31/2013","$180,000.00","","paul.nemirovsky@gmail.com","181 North 11th St","Brooklyn","NY","112111175","6176427163","ENG","5371","4080, 5371, 8032, 9139, ","$0.00","This Small Business Innovation Research (SBIR) Phase I project aims at developing a targeted semantic processing framework for the analysis of online conversations. A particular focus is given to improving the state-of-the art in the design of semi-supervised syntactic and semantic parsers capable of processing informal conversations about consumer products and services across industry verticals. This project will address some of the key shortcomings of existing natural language processing (NLP) frameworks when applied to noisy language typical of user-generated content, where linguistic phenomena is significantly divergent from the carefully edited content such as news reports, including colloquialisms, misspellings, grammatical errors, and incomplete sentences. Given the wide proliferation, amount, and richness of available user-generated content, combined with the limitations of the currently available shallow textual representations, creation of better NLP models is critical for extracting useful information from text that is largely informal, and for which no annotated data is available. Successful completion of this work will fulfill the urgent need for the development of NLP models accurate across multiple domains, introduce novel information extraction models for the extraction of consumer decisions, experiences, and rationale, while advancing our capability to model, quantify, and understand the reasons underlying product adoption and attrition.<br/><br/>The broader impact/commercial potential of this project relates to the accuracy of automation that can be achieved when analyzing large amounts of unstructured text. Millions of Americans use the Internet to share, search and communicate information about products and services. Whether considering a new product, relaying past product experience, or analyzing product performance, consumers and companies alike are faced with an overwhelming range of informational sources, making the process of gathering information about products both slow and costly. Using in-depth text processing technology to analyze product reports found across social networks, forums, blogs, and company-driven websites will both increase the value of consumer-driven product reports and improve companies' and regulators' ability to learn from real-world product usage data to improve performance and public perception of marketed products and services. If successful, this project will lead to advancements in knowledge discovery, enable the automation of manual processes associated with large-scale consumer studies and change the dominant methodologies for conducting market research."
"1302338","HCC: Medium: Combining Crowdsourcing and Computer Vision for Street-level Accessibility","IIS","HCC-Human-Centered Computing","05/01/2013","05/16/2013","Jon Froehlich","MD","University of Maryland College Park","Standard Grant","Ephraim Glinert","04/30/2019","$1,199,034.00","David Jacobs","jonf@cs.washington.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","7367","7367, 7924","$0.00","Despite comprehensive civil rights legislation for Americans with disabilities, many city streets, sidewalks, and businesses remain inaccessible.  The problem is not just that street-level accessibility affects where and how people travel in cities but also that there are few, if any, mechanisms to determine accessible areas of a city a priori.  Traditionally, sidewalk assessment has been conducted via in-person street audits, which are labor intensive and costly, or via citizen call-in reports, which are done on a reactive basis.  And while efforts exist for visualizing the walk-ability, bike-ability, and availability of public transport in cities, there are no analogous efforts for accessibility.  Thus, wheelchair users, for example, often avoid going to new areas of a city where they don't know about accessible routes.  The PI plans to address this problem by means of a two-pronged approach in which he will first develop scalable data collection methods for acquiring sidewalk accessibility information using a combination of crowd-sourcing, computer vision, and online map imagery; he will then use the new data to develop and evaluate a novel set of navigation and map tools for accessibility.  To these ends, the PI and his team will collect and analyze interview and survey data both from mobility impaired persons and from ADA streetscape design experts, and will seek to understand how people with mobility impairments can make use of interactive mapping information to enhance mobility.  They will study methods for efficiently and effectively crowd-sourcing map labeling tasks, evaluating existing approaches empirically and designing novel, more effective approaches.  They will develop new computer vision algorithms for the analysis of street scenes, which will be used to help scale the data collection by focusing human labeling efforts on locations that are most likely to contain significant problems.   And they will design, implement and evaluate new accessible-aware map-based tools to aid people with mobility impairments in navigating their cities.  As appropriate for each phase of the research, user evaluations will include both lab and field studies.<br/><br/>Broader Impacts:  Roughly 30.6 million individuals in the United States have physical disabilities that affect their ambulatory activities, and nearly half of these individuals report using an assistive aid such as a wheelchair, cane, crutches, or walker.  The outcomes from this research will have a significant impact on the ability of these Americans to travel independently, by transforming the ways in which accessibility information is collected and visualized for every sidewalk, street, and building faade in America.  Project outcomes will include a publicly accessible web site where both the labeled data collected during this work and the new prototype tools developed will be made available for general use.   Furthermore, the PI and Co-PI will advise and mentor both graduate and undergraduate students throughout the course of the project, including two PhDs and two MS students who will obtain a cross-disciplinary education in human-computer interaction and computer vision."
"1311043","NSF East Asia and Pacific Summer Institute (EAPSI) for FY 2013 in China","OISE","EAPSI","06/01/2013","05/15/2013","Samuel Dodge","AZ","Dodge Samuel F","Fellowship","Anne L. Emig","05/31/2014","$5,070.00","","","","Scottsdale","AZ","852541950","","O/D","7316","5978, 7316, 9200","$0.00","This action funds Samuel Fuller Dodge of Arizona State University to conduct a research project in the Computer and Information Science and Engineering area during the summer of 2013 at Peking University in Beijing, China. The project title is ""Biologically Inspired Computer Vision: Using Attention to Efficiently Process Visual Entities."" The host scientist is Dr. Yizhou Wang.<br/><br/>This project studies the usefulness of biologically inspired visual attention models in computer vision applications. The human visual system is able to process a large amount of information using relatively few physiological resources by selectively focusing attention on the most salient areas of a scene. This same manner of information processing can be useful in computer vision systems that are under similar computational constraints. To this aim, this project investigates the relative influence of simple bottom-up attention and more complicated top-down attention for the applications of object recognition and scene understanding. Bottom-up attention is formed from low-level features that attract our attention such as a red dot in a sea of blue dots. Top-down attention stems from higher level cognitive processing, such as detecting a face in the scene. Together these two factors can yield a more appropriate attention model for use in computer vision recognition tasks.<br/><br/>Broader impacts of an EAPSI fellowship include providing the Fellow a first-hand research experience outside the U.S.; an introduction to the science, science policy, and scientific infrastructure of the respective location; and an orientation to the society, culture and language. These activities meet the NSF goal to educate for international collaborations early in the career of its scientists, engineers, and educators, thus ensuring a globally aware U.S. scientific workforce. Furthermore, the results of this research will be disseminated in a top computer vision conference, so that this research can find use in practical application areas such as automation, healthcare, and intelligent systems. The fellow is working with Chinese graduate and undergraduate students to improve their professional English, so that they will be more prepared for conferences."
"1314778","HCC: Large: Social-Computational Support of Civic Engagement in Public Policymaking","IIS","HCC-Human-Centered Computing","09/15/2013","06/29/2016","Claire Cardie","NY","Cornell University","Standard Grant","William Bainbridge","08/31/2017","$2,279,876.00","Susan Fussell, Cynthia Farina, Gilly Leshed","cardie@cs.cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7367","7367, 7925, 9251","$0.00","The overarching goals of this work are to understand and to provide socio-computational support for improving the entire cycle of technology-enabled civic engagement: (1) recruitment of people with a stake in the issues; (2) deliberative discussion in which they learn about the policy issues, engage with each other, voice questions and recount experiences; and (3) consensus building in which participants move toward collaborative content-creation, summarization of the knowledge that has emerged in discussion and the development of agreement around key points. In practice, efforts to use social media for citizen policy consultations often fell far short of their knowledge-generating and democracy-reinforcing goals. There thus is a crucial need to discover how to design civic engagement spaces that leverage the potential of social media, so that they support not simply more participation but rather better participation that will benefit both the policymakers seeking input and the citizens who participate in the discussion. <br/><br/>To achieve this goal, the project integrates computer science research on natural language learning for social-computational systems, human-computer interaction research on online communities, social media design, and social science research on motivation and individual and group deliberative processes. The research will advance behavioral science understanding of the relationship between individual characteristics and successful e-deliberation; the communicative processes that characterize successful e-deliberation; and the group processes and moderator behaviors that promote a shift from open discussion to consensus building. It will advance the state-of-the-art in natural language processing by developing joint human-computer text analysis techniques to (1) promote on-line civic engagement in policy discussions and (2) facilitate deliberative moderation in this collaborative online setting. It will add to human-computer interaction by advancing recommender systems, online communities, and social media research to support mentoring activities and engagement with alternate points of view. Finally, it will extend scientific understanding of how to motivate and support broader, better citizen participation in public policymaking.<br/><br/>The work will have at least five broader impacts: (1) increase understanding of, and infrastructure for, e-participation in policy-making, and provide annotated datasets of civic deliberations for use by other researchers; (2) enhance education through graduate and undergraduate mentoring and development of a new interdisciplinary course on Online Civic Engagement; (3) promote STEM education diversity with programs for middle and high school girls; (4) provide community and government outreach activities; (5) benefit society by improved civic engagement in policymaking in general."
"1321408","Group Travel Grant for the Doctoral Consortium of the IEEE Conference on Computer Vision and Pattern Recognition","IIS","INFORMATION TECHNOLOGY RESEARC, ROBUST INTELLIGENCE","05/15/2013","05/03/2013","Philippos Mordohai","NJ","Stevens Institute of Technology","Standard Grant","Jie Yang","04/30/2014","$15,050.00","","Philippos.Mordohai@stevens.edu","CASTLE POINT ON HUDSON","HOBOKEN","NJ","070305991","2012168762","CSE","1640, 7495","7495, 7556","$0.00","This project supports the Doctoral Consortium held with the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) in Portland, Oregon, in June 2013. CVPR is the premier annual conference with about 2000 senior and student participants in computer vision, held in North America and attended by members of the international research community. The goal of the Doctoral Consortium is to highlight the work of senior Ph.D. students, who are close to finishing their degrees, or recent graduates, and to give these students the opportunity to discuss their research with senior researchers matched with their expertise. NSF support covers some of the costs for the selected US graduate students to attend the conference. <br/><br/>The intellectual merit of this project rests in the selection of top-quality Ph.D. students whose research is showcased and to whom feedback is provided by senior researchers. The opportunity to receive advice on their research and career plans from experts from different institutions, and with potentially different perspectives, in many cases is not available internally within one's own institution. The broader impacts of this project include supporting the career development of some of the brightest junior researchers in computer vision, contributing to the research community in general by drawing attention to an important aspect of graduate student development, and potentially increasing the number of active researchers and teachers in STEM. The doctoral consortium event aims to have representation from a diverse group of participants in terms of gender, ethnic background, academic institution and geographic location."
"1248603","SBIR Phase I:  Development of Cohort Identification Tool","IIP","SBIR Phase I","01/01/2013","11/27/2012","Daniel Riskin","CA","Health Fidelity, Inc.","Standard Grant","Muralidharan Nair","06/30/2013","$150,000.00","","grants@verantos.com","204 2nd Avenue #517","San Mateo","CA","944013963","6508884956","ENG","5371","4080, 5371, 8032, 9139, 9150, HPCC","$0.00","This Small Business Innovation Research (SBIR) Phase I project seeks to address the most significant and challenging software need in healthcare: Cohort identification. A cohort is a group of patients with a common medical condition. Cohorts underpin modern medical care, defining treatment algorithms, measuring quality improvement, supporting government initiatives, and representing the core organization for research trials. While manual techniques have been developed to identify a cohort within a healthcare organization's electronic medical record (EMR), all rely on a physician or coder identifying and marking every record for every applicable medical condition. This manual process is inaccurate and only addresses the most common conditions. The suggested novel and revolutionary approach is to use big data techniques, utilizing the detailed unstructured narrative notes recorded on every patient for every encounter in every healthcare institution.  The core technology required to extract and make unstructured data usable in healthcare is natural language processing (NLP) combined with coded representations of clinical concepts (ontologies).  This proposal brings together industry leading teams and technologies to tackle the greatest data problem in healthcare, which offers a unique opportunity to significantly influence care for decades to come.<br/><br/>The broader impact/commercial potential of this project includes creating the foundational infrastructure for the next generation of data-driven healthcare.  Just as Google and Yahoo required advanced information extraction and search indexing techniques to make the vast amount of internet data usable, healthcare requires similar enabling technology.  The healthcare challenge is even more complex given the multitude of natural language descriptions used by physicians and the complex logic that defines potential cohorts and algorithms.  To address these issues, healthcare requires the category of technologies used in Google and Yahoo, but specialized for the healthcare domain.  In healthcare, quality improvement requires recognizing at risk cohorts in a population. Missing these cohorts and inadequately treating them can increase mortality by an order of magnitude, as in the case of deep vein thrombosis (DVT) in acute care. For quality measures being implemented by the federal government, defining and identifying cohorts is always the first step of tracking and reporting. Current processes are manual, limited, and inaccurate. By bringing evidence derived from clinical documentation which is created in current workflow to real-time and population based treatment decisions, this intervention will form a foundation for data-driven care, supporting improved outcomes, shorter hospitalizations, and reduced direct medical costs."
"1341772","RI: Small: Debugging Machine Visual Recognition via Humans in the Loop","IIS","Robust Intelligence","02/11/2013","05/24/2013","Devi Parikh","VA","Virginia Polytechnic Institute and State University","Standard Grant","Jie Yang","08/31/2015","$30,483.00","","parikh@vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","CSE","7495","7495, 7923","$0.00","The problem of visual recognition is fundamental towards the goal of automatic image understanding. While a large number of efforts have been made in the computer vision community, machine performance at these tasks remains significantly inferior to human ability. <br/>The overarching goal of this project is to leverage the best known visual recognition system - the human visual recognition system. This project employs a ""Human Debugging"" paradigm to replace various components of a machine vision pipeline with human subjects, and examines the resultant effect on recognition performance. Meaningful comparisons provide valuable insights and pinpoint aspects of the machine vision pipeline that are performance bottlenecks and require future research efforts. Specifically, the project considers the problems of image classification and object detection, and explores the roles of local and global information, as well part-detection, spatial modeling and contextual reasoning (including non-maximal suppression) for these problems respectively. <br/>This project touches on a wide range of problems in visual recognition including object recognition, scene recognition and object detection. This novel paradigm of identifying weak links in computational models via humans in the loop is also applicable to other vision problems, as well as other sub-fields in AI. By sharing all collected data and results, and through organized conferences and workshops, this project will initiate and fuel a dialogue with the research community about leveraging humans to advance computer vision. More broadly, this work encourages the involvement of young women and undergraduate students in computer science research."
"1302438","CIF: Medium:Collaborative Research: Nonasymptotic Analysis of Feature-Rich Decision Problems with Applications to Computer Vision","CCF","Comm & Information Foundations, SIGNAL PROCESSING","07/01/2013","06/03/2015","Maxim Raginsky","IL","University of Illinois at Urbana-Champaign","Continuing grant","Phillip Regalia","06/30/2018","$669,212.00","Svetlana Lazebnik","maxim@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7797, 7936","7924, 7936","$0.00","This project deals with theory and efficient algorithms for statistical decision problems that are radically different from those that have been studied to date in two key aspects: First, the decision-maker may choose among a large class of observation channels (features) of varying complexity and quality; and second, the total cost of computational resources that can be used prior to arriving at a decision is limited. Computer vision is a paradigmatic source of such feature-rich decision problems, requiring the use of multiple heterogeneous feature types, integration of diverse sources of contextual information, and possibly even human interaction.<br/><br/>This project entails the development of a rigorous mathematical framework for feature-rich decision problems in accordance with three specific aims: (1) structural characterization of features as stochastic belief-refining filters; (2) universal cost-sensitive criteria for numerical comparison of features in terms of expected information gains; and (3) optimal value-of-information criteria for sequential feature selection that take into account both feature extraction costs and terminal decision losses. As corollaries, this research investigates connections to asymptotic information-theoretic characterizations of optimal feature selection rules and decisions. The fourth specific aim of the project is the development of practical algorithms for two challenging computer vision problems: active visual search and fine-grained categorization. This component of the project leverages theoretical aims (1) and (2) to develop practical cost- and loss-sensitive feature compression techniques. Theoretical aim (3) targets algorithms that function as autonomous decision-making agents. Faced with an inference task on an image, they apply cost-sensitive non-myopic value- of-information criteria to decide at each time step whether to extract a new feature from the image or to stop and declare an answer."
"1302588","CIF: Medium: Collaborative Research: Nonasymptotic Analysis of Feature-Rich Decision Problems with Applications to Computer Vision","CCF","Comm & Information Foundations, SIGNAL PROCESSING","07/01/2013","06/03/2015","Tara Javidi","CA","University of California-San Diego","Continuing Grant","Phillip Regalia","06/30/2019","$396,691.00","","tara@ece.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7797, 7936","7924, 7936","$0.00","This project deals with theory and efficient algorithms for statistical decision problems that are radically different from those that have been studied to date in two key aspects: First, the decision-maker may choose among a large class of observation channels (features) of varying complexity and quality; and second, the total cost of computational resources that can be used prior to arriving at a decision is limited. Computer vision is a paradigmatic source of such feature-rich decision problems, requiring the use of multiple heterogeneous feature types, integration of diverse sources of contextual information, and possibly even human interaction. <br/><br/>This project entails the development of a rigorous mathematical framework for feature-rich decision problems in accordance with three specific aims: (1) structural characterization of features as stochastic belief-refining filters; (2) universal cost-sensitive criteria for numerical comparison of features in terms of expected information gains; and (3) optimal value-of-information criteria for sequential feature selection that take into account both feature extraction costs and terminal decision losses. As corollaries, this research investigates connections to asymptotic information-theoretic characterizations of optimal feature selection rules and decisions. The fourth specific aim of the project is the development of practical algorithms for two challenging computer vision problems: active visual search and fine-grained categorization. This component of the project leverages theoretical aims (1) and (2) to develop practical cost- and loss-sensitive feature compression techniques. Theoretical aim (3) targets algorithms that function as autonomous decision-making agents. Faced with an inference task on an image, they apply cost-sensitive non-myopic value- of-information criteria to decide at each time step whether to extract a new feature from the image or to stop and declare an answer."
"1414931","RI: Medium: Collaborative Research: Teaching Computers to Follow Verbal Instructions","IIS","Robust Intelligence","07/01/2013","09/17/2015","Michael Littman","RI","Brown University","Standard Grant","Weng-keen Wong","08/31/2016","$514,356.00","","mlittman@cs.brown.edu","BOX 1929","Providence","RI","029129002","4018632777","CSE","7495","7924","$0.00","The goal of this research is to develop techniques that will permit a computer or robot to learn from examples to carry out multipart tasks specified in natural language on behalf of a user.  It will study each of these components in isolation, but a significant focus will be on integrating them into a coherent system.  The project will also leverage this technology to provide an entry point to educate non- or pre-computer science students about the capabilities and utility of computers as tools.<br/><br/>Our approach uses three main subcomponents, each of which requires innovative research to solve its portion of the overall problem.  In addition, the integrated architecture is a novel contribution of this work.  The three components are (1) recognizing intention from observed behavior using extensions of inverse reinforcement learning, (2) translating instructions to task specifications using novel techniques in the area of natural language processing, and (3) creating generalized task specifications to match user intentions using probabilistic methods for creating and managing abstractions.<br/><br/>The goal of the work is develop technology for an improved ability for human users to interact with intelligent agents, the incorporation of novel AI research insights and activities into education and outreach activities, and the development of resources for the AI educator community.  In addition to permitting intelligent agents to be developed and trained in the future for a broad range of complex application domains, the interactive agents that we will develop will be used for outreach and student learning."
"1320654","AF: Small: Computational Aspects of Markets, Equilibria, and Fixed Points","CCF","ALGORITHMS","09/01/2013","08/06/2013","Mihalis Yannakakis","NY","Columbia University","Standard Grant","Tracy J. Kimbrel","08/31/2016","$499,985.00","","mihalis@cs.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","7926","7923, 7926, 7932","$0.00","The overall goal of this project is to advance the computational theory and algorithms for equilibria and fixed points.  Many problems from different areas can be formulated as the problem of computing a fixed point of a suitable function F. Examples include the computation of Nash equilibria of games, price equilibria in markets,  the value and optimal strategies for stochastic and other dynamic games, the analysis of various basic stochastic models (branching processes, stochastic context-free grammars, recursive Markov chains, and others) that arise in many areas. In some cases (e.g., Nash and market equilibria) one wishes to compute any fixed point, while in several others (e.g., stochastic models and games), the function F is monotone and one wishes to compute a specific fixed point, the least fixed point.<br/><br/>The  project will build on recent progress to advance the theory and algorithms on two fronts: market equilibria, and least fixed point problems. In the first area, it will seek to develop a more systematic methodology and show general results that characterize what features make the market equilibrium problem hard and what features make it easy; it will advance our understanding of the computation of  equilibria, both on the hardness side and on the algorithmic side;  and it will try to resolve open questions regarding specific types of markets,  and limitations of price adjustment schemes. In the second area, the project will leverage recent powerful positive results to address and solve in a unified way basic problems on stochastic context-free grammars, quasi-birth-death processes, and other stochastic models that require the solution of more general classes of monotone fixed point equations.<br/><br/>The problems and models studied in this project are fundamental in various disciplines (including economics, game theory, biology, and various areas of computer science such as verification and natural language processing), and they have been studied and are used extensively. The research of the project will provide a systematic,  unified treatment of the underlying fundamental questions,  and will result in algorithms and insights that are useful in the various relevant areas."
"1319378","III: Small: TwitterHealth: Learning Fine-Grained Models of Health Influences and Interactions From Social Media","IIS","Info Integration & Informatics, Smart and Connected Health","09/01/2013","05/14/2015","Henry Kautz","NY","University of Rochester","Standard Grant","Sylvia Spengler","08/31/2017","$497,939.00","","kautz@cs.rochester.edu","518 HYLAN, RC BOX 270140","Rochester","NY","146270140","5852754031","CSE","7364, 8018","7364, 7923, 8018, 9251","$0.00","Current techniques for answering questions about the influence of behaviorial and environmental factors on public health are based on surveys, which are costly and subject to response bias, or simulations, which rely on possibly incorrect or simplistic assumptions.  The TwitterHealth project is developing techniques to extract reliable public health information from social media.  In essence, the online population becauses a vast organic sensor network.  Statistical natural language processing techniques are employed to classify tweets (or other social media postings) as self-reports of disease or particular behaviors of interest.  GPS information included in postings made from cell phones allow a variety of behavioral information to be inferred about each user, such as the venues visited and the other individuals from the data set who are encountered.<br/><br/>Major technical challenges for using social media in this manner are the highly noisy nature of the information channel, scaling to a large number of different health conditions, and the need to discover causal influences as well as correlations between behavioral and environmental factors and health.  The challenge of noise is approached by learning dynamic relational models of health states, which generalize classical epidemiological models but support individual as well as aggregate predictions.  The scaling challenge is dealt with by knowledge transfer techniques, which reduce data and computational requirements by transfering information between models for different health conditions.  Specific knowledge transfer techniques are cascaded training of a target classifier starting with a given classifier for a related but different disease, and the use of ensembles of general and specific classifiers.  The challenge of inferring casuality is addressed by temporal-lag methods, which identify changes in behaviorial or environmental conditions that consistently precede changes in health.  For example, the inference that a venue is a cause (vector) of disease spread is accomplished by tracing backward in time the GPS trails of users who post social media reports of illness.  TwitterHealth employs two approaches for validating its results: first, comparing the aggregate predictions of the model against CDC statistics; second, comparing individuals' behavior in reporting or not reporting disease symptoms in status updates against the behavior predicted by the models.  The project also includes planning for clinic based evaluations, in which subjects identified by their social media postings would provide swabs that would be tested for disease agents.<br/><br/>The TwitterHealth approach to collecting and analyzing health information has the potential to improve public health, by making detailed data about health, behavior, social structure, and geographic influences available in real time and at almost no cost.  While it will not completely replace traditional methods of gathering health information, it provides an important complementary information channel, which emphases speed, reach, and scale.  The project includes outreach expert medical professionals in order to plan future clinical validation.  The outreach interaction provides a forum for exchange of computer science and medical expertise between researchers and students in the two fields. Information about the project is available online at http://www.cs.rochester.edu/u/kautz/twitterhealth."
"1319700","III: Small: Integrating Crowd Sourcing, Volunteer Computing and Expert Observation to Robustly Classify Massive Quantities of Avian Nesting Video","IIS","Info Integration & Informatics, EPSCoR Co-Funding","08/15/2013","06/04/2015","Travis Desell","ND","University of North Dakota Main Campus","Continuing grant","Sylvia Spengler","07/31/2017","$497,263.00","Susan Ellis-Felege","tjdvse@rit.edu","264 Centennial Dr Stop 7306","Grand Forks","ND","582027306","7017774151","CSE","7364, 9150","7364, 7923, 9150","$0.00","New camera technology is allowing avian ecologists to perform detailed studies of avian behavior, nesting strategies and predation in areas where it was previously impossible to gather data. Unfortunately, studies have shown mechanical triggers and a variety of sensors to be inadequate in capturing footage of small predators (e.g., snakes, rodents) or events in dense vegetation. Because of this, continuous camera recording is currently the most robust solution for avian monitoring, especially in ground nesting species. However, continuous video footage results in a data deluge, as monitoring enough nests to make biologically significant inferences results in massive amounts of data which is unclassifiable by humans alone.  This project will develop a citizen science project which combines volunteer computing, where people volunteer their computers to automatically analyze video with computer vision strategies, and crowd sourcing, where people volunteer their brain power by streaming the videos and reporting observations, to analyze over a hundred thousand of hours of avian nesting video.<br/> <br/>This collaborative proposal will address the data deluge in avian research, where data acquisition rates are greatly outpacing the ability to process that data, by gathering, storing, and analyzing nest video at unprecedented scales for evaluating hypotheses about avian reproductive ecology and predator-prey interactions.  The team will develop computer vision techniques based on the scale-invariant feature transform (SIFT) and speeded up robust features (SURF) algorithms and their variants  capable of identifying events involving animals with cryptic coloration in uncontrolled outdoor settings for this analysis.  In addition,they will use the nesting video to develop a large human annotated archival video resource for ecologists and computer vision researchers alike, generated using crowd sourced volunteer observations validated against each other and further refined by a scientific web portal for expert analysis of the volunteered responses. An enduring citizen science project combining crowd sourcing and volunteer computing to perform the analysis of the video and use it to foster public interest and involvement from K-12 classrooms, stimulating online education in STEM disciplines is also planned."
"1318563","TWC: Small: Semantics Aware Approaches to Automated Reverse Engineering Unknown Application Protocols","CNS","Secure &Trustworthy Cyberspace","09/01/2013","08/21/2013","Alex Liu","MI","Michigan State University","Standard Grant","Nina Amla","12/31/2017","$455,000.00","","alexliu@cse.msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","CSE","8060","7434, 7923","$0.00","Many application protocols on the Internet, especially those used by malware, are proprietary and have no publicly released specifications. According to the Internet2 NetFlow weekly reports on backbone traffic, more than 40% of Internet traffic belongs to unidentified application protocols. Therefore, it is critical for network security solutions to understand the specifications of these unknown application protocols. For instance, protocol specifications are needed for parsing unknown application protocol in advanced intrusion prevention systems. Protocol specifications are also useful for many other applications such as vulnerability discovery and system integration. Furthermore, even for some application protocols with known specifications, protocol inference is also needed sometimes for identifying implementation details and bugs that are not unambiguously specified. Inferring protocol specification for unknown application protocols is therefore fundamental to network security.<br/><br/>The objective of this project is to develop schemes for automatically inferring the protocol specification of unknown applications from their network traces. The PI proposes a semantics aware approach that takes network traces as the input and automatically outputs the inferred protocol message format. This project represents the first effort towards developing semantics aware approaches to protocol inference, a fundamental building block of many network security solutions. This project is potentially transformative research with high-impact. It will enable a spectrum of new network security applications and solutions. The proposed project is interdisciplinary in nature as it applies natural language processing techniques to network security problems."
"1321802","Collaborative Research: Accounting for the Emergence, Persistence, and Media Coverage of Social Action Organizations","SES","SOCIOLOGY","09/01/2013","08/28/2013","Patrick Rafail","LA","Tulane University","Standard Grant","Marie Cornwall","08/31/2016","$89,898.00","","prafail@tulane.edu","6823 ST CHARLES AVENUE","NEW ORLEANS","LA","701185698","5048654000","SBE","1331","9150, 9251","$0.00","SES-1321802 Tulane University <br/><br/>Patrick Rafail <br/><br/>SES-1322568 <br/>John McCarthy <br/>University of Southern California <br/><br/>This project aims to complete the collection of a six year time series of social action mobilization occurring between 2009 and 2014. Building on the results from previous research, the investigators propose to address three important theoretical questions. The first aims to understand what social structural variation (e.g. income inequality, rates of unemployment) across U.S. states and local communities increases the likelihood that social action will occur and recur. The second asks what community contexts facilitate the founding of social action organization as well as the factors influencing the organizational linkages and survival of these groups over time. The third, asks which factors influence the intensity, tone, and content of newspaper coverage of such groups. <br/><br/>To address these questions, the researchers will assemble a database of social action events as well as organizational foundings, other local activities such as meetings or demonstrations, and organization survival by combining multiple sources of data including internet listings, systematic searches of 432 local newspapers, and content coding of newspaper stories of covered events. The research will geographically situate each event and organization, which will be linked to a larger database of social structural indicators, information on election cycle outcomes, and newspaper characteristics. Statistical approaches to modeling the processes will include hierarchical regression, survival analysis, web scraping, text mining, and natural language processing. <br/><br/>Broader Impact<br/>The proposed project has enormous potential for having broader impact in several ways. First, it will provide a comprehensive, systematic analysis of social action groups. Second, the investigators will create a public use data set to allow diverse additional investigations of the causes and consequences of social action group mobilization from a wide variety of perspectives beyond our own. And, finally, the researchers will be able to chronicle these social movements from infancy into maturity as well as signs of their decline. Additionally, the project incorporates specific research experiences for both graduate and undergraduate students."
"1322568","Collaborative Research: Accounting for the Emergence, Persistence, and Media Coverage of Social Action Organizations","SES","SOCIOLOGY","09/01/2013","08/28/2013","John McCarthy","PA","Pennsylvania State Univ University Park","Standard Grant","Joseph Whitmeyer","08/31/2018","$168,273.00","","jxm516@psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","SBE","1331","","$0.00","SES-1321802<br/>Patrick Rafail<br/>Tulane University<br/><br/>SES-1322568<br/>John McCarthy<br/>University of Southern California<br/><br/>In this project we aim to complete the collection of a six year time series of social action mobilization occurring between 2009 and 2014. Building on the results from previous research, we propose to address three important theoretical questions: The first aims to understand what social structural variation (e.g. income inequality, rates of unemployment) across U.S. states and local communities increases the likelihood that social action will occur and recur. The second asks what community contexts facilitate the founding of social action organization as well as the factors influencing the organizational linkages and survival of these groups over time. Third, we ask which factors influence the intensity, tone, and content of newspaper coverage of such groups.<br/><br/>To address these questions, we will assemble a database of social action events as well as organizational foundings, other local activities such as meetings or demonstrations, and organization survival by combining multiple sources of data including internet listings, systematic searches of 432 local newspapers, and content coding of newspaper stories of covered events. We will then geographically situate each event and organization, which will be linked to a larger database of social structural indicators, information on election cycle outcomes, and newspaper characteristics. Statistical approaches to modeling the processes will include hierarchical regression, survival analysis, web scraping, text mining, and natural language processing.<br/><br/>Broader Impact<br/>The proposed project has enormous potential for having broader impact in several ways. First, it will provide a comprehensive, systematic analysis of social action groups. Second, we will create a public use data set to allow diverse additional investigations of the causes and consequences of social action group mobilization from a wide variety of perspectives beyond our own. And, finally, we will be able to chronicle these social movements from infancy into maturity as well as signs of their decline."
"1315617","SBIR Phase I: Transparent Cloud Acceleration of Mobile Applications via Compressive Sensing","IIP","SMALL BUSINESS PHASE I","07/01/2013","06/17/2013","CHIT-KWAN LIN","NJ","UpShift Labs, Inc.","Standard Grant","Rajesh Mehta","12/31/2013","$150,000.00","","ck@upshiftlabs.com","70 GREENE ST SUITE 2308","JERSEY CITY","NJ","073027598","6463978187","ENG","5371","153E, 5371, 8032","$0.00","This Small Business Innovation Research Program (SBIR) Phase I project seeks to develop a software system that transparently and dynamically offloads computations from mobile devices to the cloud for the purpose of accelerating mobile applications. The need is clear, as software requirements are already outstripping the hardware capabilities of smartphones and tablets. However, such a system has remained out of reach since current technologies cannot practically achieve the tight device-cloud coupling required for high application responsiveness, especially under the processor, battery and network constraints of modern mobile devices. Compressive replication, a novel continuous data replication method based on compressive sensing, can fill this technology gap by enabling just such a tight coupling. To elevate compressive replication to a level suitable for use in a commercial product and service, technical work will be undertaken to survey and improve its encoding/decoding mechanisms, tailor efficient network protocols to it, and evaluate its real-world performance in a prototype. The anticipated result of this feasibility study is a system that will demonstrate significant speed-up of mobile applications through transparent cloud acceleration.<br/><br/>The broader impact/commercial potential of this project will be a transformation of the mobile computing landscape. Application developers will be afforded a simple, principled method and a general, reusable framework for developing mobile applications that are tightly coupled to the vast resources of the cloud. In turn, this will unleash a new generation of mobile applications that rival, and even surpass, current desktop or web-based applications in sophistication, features and engagement. For end-users at large, it could mean greater productivity through more sophisticated business apps and higher quality entertainment through more immersive games. For disabled persons, cloud acceleration would make assistive user interfaces -- particularly those that combine computationally intensive natural language processing, gesture recognition, and predictive algorithms -- practical on mobile devices. The proposed system will also be a key driver of new economic activity in the mobile space, as real-time synchronization with the cloud opens up new opportunities in an extensive array of market verticals including, e.g., mobile user engagement analytics, mobile advertising, and even machine-to-machine (M2M) platform development. In short, the proposed cloud acceleration technology will have significant societal and economic impacts on many fronts."
"1266184","Creativity through Collaborative Human-Machine Interactions: A Formal Approach to Design Crowd Sourcing","CMMI","SYS-Systems Science","08/01/2013","04/27/2015","Panos Papalambros","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Rich Malak","07/31/2017","$642,574.00","Yi Ren, Richard Gonzalez, Honglak Lee","pyp@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","ENG","8085","067E, 068E, 073E, 1327, 6856","$0.00","This research project will create a theoretical and computational infrastructure to design new technology using creativity from crowds and individuals in combination with machine learning.  A crowd could be a collection of experts within an organization, a classroom of students, or a large number of people online. Earlier research used machine learning working with individual engineers to help with simple design problems.  This research will extend the earlier work to more complex configuration design problems, and will add crowd sourcing.  Design representations will be graphs instead of vectors, and the design space will not be defined ahead of time. Machine learning may prove to be an important improvement over design evolution methods, and will provide insight into which design features are important.  Machine learning also generates a model of subjective human judgment and preference, leading to more efficient and perhaps more innovative design synthesis. <br/><br/>If successful, this research will provide a platform for new models of innovation with input from multiple stakeholders: A machine supported by crowd-sourced knowledge and real-time interaction with humans will be able to produce unique and creative structures that were beyond the imagination of the humans involved. This research will also result in algorithmic advances in inference, learning, and related optimization techniques in representation learning for structured data and interactive human-machine collaboration. It will also provide a mathematical framework for innovation in massive system design problems involving thousands of designers such as the design of a jet fighter. The technology developed will be possible to implement in a variety of environments from complex engineering system design decisions to ideas for new products on commercial sites. In an educational context, this research, if successful, will allow students and teachers to experiment with design tools, individually, as part of a classroom experience, or as a national endeavor to both learn and possibly to generate new design ideas collectively."
"1253942","CAREER: Differentially-Private Machine Learning with Applications to Biomedical Informatics","IIS","Robust Intelligence, Secure &Trustworthy Cyberspace","07/01/2013","05/30/2017","Kamalika Chaudhuri","CA","University of California-San Diego","Continuing Grant","Rebecca Hwa","06/30/2019","$490,625.00","","kamalika@cs.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7495, 8060","1045, 7434","$0.00","Machine learning on large-scale patient medical records can lead to the discovery of novel population-wide patterns enabling advances in genetics, disease mechanisms, drug discovery, healthcare policy, and public health. However, concerns over patient privacy prevent biomedical researchers from running their algorithms on large volumes of patient data, creating a barrier to important new discoveries through machine-learning. <br/><br/>The goal of this project is to address this barrier by developing privacy-preserving tools to query, cluster, classify and analyze medical databases. In particular, the project aims to ensure differential privacy --- a formal mathematical notion of privacy designed by cryptographers which has gained considerable attention in the systems, algorithms, machine-learning and data-mining communities in recent years.  The primary challenge in applying differentially-private machine learning tools to biomedical informatics is the lack of statistical efficiency, or the large number of samples required.<br/><br/>The project will overcome this challenge by drawing on insights obtained from the PI's expertise to develop differentially-private and highly statistically-efficient machine learning tools for classification and clustering. The proposed research will advance the state-of-the-art in privacy-preserving data analysis by combining insights from differential privacy, statistics, machine learning, and database algorithms. <br/><br/>The proposed research is closely tied to the development of the undergraduate and graduate curricula at UCSD, feeding into the PI's new undergraduate machine learning class, a new graduate learning theory class, and updates to an algorithm design and analysis class.  The corresponding materials will be publicly disseminated through the PI's website.  The PI is strongly committed to increasing the participation of women and minorities, and will engage in outreach activities to attract and retain women in computer science."
"1319749","III: Small: Effective Convex Solvers for Machine Learning","IIS","Info Integration & Informatics, Robust Intelligence","09/01/2013","07/08/2014","Daniel Boley","MN","University of Minnesota-Twin Cities","Continuing grant","Sylvia Spengler","08/31/2018","$453,466.00","","boley@cs.umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","7364, 7495","7364, 7495, 7923, 9251","$0.00","Many large scale machine learning problems are formulated as optimization problems, in which some measure of error or loss is to be minimized over a suitable training corpus.  Real problems have too many data points to fit in a single computer.  Hence the data and/or computation must be distributed over a network of computers.  Often the only practical methods for extremely large problems are so-called splitting methods, but their convergence properties are extremely variable: sometimes very fast, sometimes very slow, in ways that can be hard to predict.  The goal of this project is to gain a better understanding of the convergence behavior and to use this understanding to construct accelerated algorithms with more consistent convergence properties.  This will allow the application of machine learning techniques to a much wider class of problems.<br/><br/>Splitting methods (or more precisely alternating direction methods) are based on the idea that a general convex optimization problem can be split into two or more parts, each of which can be solved much more easily compared to the problem as a whole.  The methods cycle through all the variables in turn, optimizing over each subset of variables leaving the rest fixed.  The proposed work builds on a preliminary analysis of a simple model problem using the eigen-structure of certain matrix operators.  The project is devoted to extending this analysis to more general problems, as well as developing faster solvers using well-established computational technologies for the matrix eigenvalue problem.  Success will be measured in terms of the generality of the theory developed and the improvements in the observed convergence behavior on real problems.<br/><br/>With faster solvers, discovery of major regions of influence in a global-scale social network (e.g. Facebook or Twitter) could become practical on modest computer platforms.  The same holds for tracking disease propagation and people in video sequences.  With efficient solvers, tracking software could be deployed on local hardware without the need for high-powered central servers.  This will lead to advances in countless areas such data mining, compressive sensing, recommender systems, signal processing, missing data imputation, analysis of large scale social, biological or computer networks, image reconstruction, denoising and classification.<br/><br/>The results of this research are to be disseminated in papers in the principal journals and conferences in machine learning, data mining, and optimization as well as in the form of software packages via the WWW (http://www-users.cs.umn.edu/~boley/ML-Optimization).<br/><br/>The project depends on the interaction between different disciplines and applications areas, which will attract students from a variety of backgrounds at both the graduate and undergraduate level.  Some research tasks are suitable as projects in classes on linear algebra, optimization, data mining, machine learning and are to be developed for both undergraduate and graduate students.  Undergraduate students, including women and members of underrepresented groups, will see the value of mathematical algorithms to solve real problems of interest to them."
"1346800","Workshop for Women in Machine Learning","IIS","Info Integration & Informatics, Robust Intelligence, NRI-National Robotics Initiati","09/01/2013","08/19/2013","Katherine Heller","NC","Duke University","Standard Grant","Jie Yang","09/30/2016","$40,002.00","","kheller@gmail.com","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","7364, 7495, 8013","7495, 7556","$0.00","Since 2006, the annual workshop for Women in Machine Learning (WiML) has brought together female researchers in industry and academia, postdoctoral fellows, and graduate students from the machine learning community to exchange research ideas and build mentoring and networking relationships. The one-day workshop has been especially beneficial for junior graduate students, giving them a supportive environment in which to present their research (in many cases, for the first time) and enabling them to meet peers and more senior researchers in the field of machine learning. The networking opportunities provided by the workshop have also helped senior graduate students find jobs following graduation. <br/><br/>Intellectual Merit: This workshop will advance machine learning knowledge and foster collaboration within the machine learning community. As invited speakers, established researchers at top universities and research labs will teach workshop participants about cutting-edge ideas from diverse areas of machine learning. Students will present their own research and receive valuable feedback from both senior researchers and their peers. By enabling women at all stages of their careers in machine learning to exchange research ideas and form new relationships, we expect that new connections and research collaborations will be established, thereby advancing the state-of-the-art of the field. <br/><br/>Broader Impact: This workshop will provide a forum for female graduate students, postdoctoral fellows, junior and senior faculty, and industry and government research scientists to exchange research ideas and establish networking and mentoring relationships. Undergraduates, particularly those who are interested in pursuing graduate school or industry positions in machine learning, are also welcome to attend. Bringing together women from different stages of their careers gives established researchers the opportunity to act as mentors, and enables junior women to find female role models working in the field of machine learning. The workshop will also benefit the wider machine learning community: Firstly, the WiML website, which lists all previous workshop presenters, serves as a useful resource for organizations looking for female invited speakers. Secondly, co-locating with a major machine learning conference enhances the visibility of female researchers among the wider machine learning community. Thirdly, travel funding provided to workshop participants also facilitates their travel to the co-located conference, which for some participants would otherwise not be possible. Finally, all workshop materials (slides, abstracts, etc.) will be made available on the workshop website in order to ensure broad dissemination."
"1341268","30th International Conference on Machine Learning (ICML 2013)","IIS","Info Integration & Informatics, Robust Intelligence","06/01/2013","05/31/2013","Charles Isbell","GA","Georgia Tech Research Corporation","Standard Grant","Sylvia Spengler","05/31/2014","$30,000.00","Vishwanathan Swaminathan","isbell@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7364, 7495","7364, 7495, 7556","$0.00","The International Conference on Machine Learning (ICML) is the permier conference in Machine Learning. ICML encompasses topics on all facets of Machine Learning, and solicits papers on problem areas, research topics, learning paradigms, and approaches to evaluation. In addition to rigorously refereed conference papers, it features poster sessions, tutorials, workshops and invited talks. This award provides funds for travel scholarships for students from US institutions to help cover their travel and costs of attending the conference.  The selection of students to be funded is based on a review by the selection committee of their financial needs and alignment of research areas. The student scholarships are very important for encouraging student participation in this premier conference and for shaping the future of the field as a whole. Special attention will be paid to broadening the participation of students from groups that are traditionally underrepresented in Computer Science  in general and Machine Learning research in particular and  students from institutions with limited Machine Learning expertise who would benefit from the opportunity to interact with researchers from around the world.<br/><br/>Attendance at the ICML represents a unique opportunity for students interested in Machine Learning research. The poster sessions at the conference are designed enable students to recieve feedback from leading Machine Learning researchers from around the world and to help them become part of the larger Machine Learning research community. The award contributes to the education and training of the next generation of researchers and educators in an increasingly important area. It also helps broaden the participation of underrepresented groups and women in Machine Learning research."
"1342739","Participant Support for attendants to the program Mathematics of Machine Learning (Barcelona)","DMS","CDS&E-MSS","09/01/2013","06/24/2013","Alexander Rakhlin","PA","University of Pennsylvania","Standard Grant","Andrew Pollington","08/31/2014","$32,000.00","","rakhlin@mit.edu","Research Services","Philadelphia","PA","191046205","2158987293","MPS","8069","7556, 9263","$0.00","The research program ``Mathematics of Machine Learning'', hosted by the Centre de Recerca Matematica (Barcelona, Spain), will take place from April 7th to July 14th, 2014. The aim of the program is to bring together leading researchers from the fields of Statistics, Optimization, and Computer Science. The dialogue between these fields has been crucial to the understanding of modern machine learning problems. The research program will focus on several aspects of theoretical machine learning, including the interplay of computation and statistics, connections between learning and optimization, as well as the theory of sequential prediction methods. Over the course of fourteen weeks, the investigators will organize three thematic 3-day workshops, one large 5-day workshop on learning theory, as well as many short and long-term visits. The program will also be co-located with the conference Journees de Statistique and the Conference on Learning Theory.<br/><br/>Machine learning approaches to dealing with large scale data ultimately rely on our understanding of computational and statistical demands. By attracting the leading experts in the respective fields, the program organizers are aiming to focus on the theoretical issues of modern machine learning problems. As machine learning methods form an essential part of many computerized systems, the research is likely to have a positive impact on technology and society through the development of faster algorithms with better performance. The program naturally integrates research and education. The short and long-term visitors will be given a unique opportunity to conduct cutting-edge research, while the thematic workshops during this period will provide a forum for learning and the exchange of ideas with the broader audience. In particular, student participation will be crucial to fostering the next generation of researchers that study both computational and statistical aspects of learning. Conference web page is located at http://stat.wharton.upenn.edu/~rakhlin/crm/"
"1323306","Full-Scale Development: Collaborative Research Advancing Informal STEM Learning Through Scientific Alternate Reality Games","DRL","AISL","10/01/2013","09/06/2016","Kari Kraus","MD","University of Maryland College Park","Continuing grant","Arlene de Strulle","09/30/2018","$684,766.00","June Ahn","kkraus@umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","EHR","7259","9150, 9177, SMET","$0.00","Full-Scale Development: Collaborative Research Advancing Informal STEM Learning and Computational Thinking through Scientific Alternate Reality Games<br/><br/>In partnership with the Smithsonian Institution, the Computer History Museum, NASA, plus leading game designers, educators, and scientists, Brigham Young University and the University of Maryland will design, research and iteratively test two large-scale Alternate Reality Games (ARGs) to engage thousands of youth aged 13-15 in scientific inquiry. Aimed at attracting girls and other groups historically underrepresented in science and technology, the games will focus on computational thinking and deep-time sciences in areas of astrobiology and astronomy.  ARGs are interactive experiences in which players collaboratively hunt for scientific data, make sense of disparate data and information, contribute content, and solve questions to advance a science-based narrative woven into the fabric of the real world. Inquiry activities within each ARG will be based upon the Next Generation Science Standards (NGSS). Unlike video games or virtual reality experiences, ARGs lead players to use a variety of media (social media, text messages, websites, videos, audio recordings). Because players participate as themselves ARGs afford learners with intensive, self-driven, scaffolded, scientific learning. <br/><br/>Combining data from web and social media analytics, player interviews, surveys, and user-generated content, researchers will establish the properties of ARGs that most effectively advance informal STEM learning outcomes. Additionally, by comparing open-ended and closed-ended ARGs, they will be able to assess the relative strengths and weaknesses of two distinct approaches to Alternate Reality Game design. The project team will test the hypothesis that open-ended, user-generated content will support inquiry-based learning, peer-to-peer learning, and life-wide and life-deep learning, while close-ended, narrative-rich ARGs will support specific transfer of STEM knowledge, collaboration, and problem solving. To help ensure that the games appeal to their target audiences, the project team will adopt co-design methods, enlisting the creative input of participating teens at each stage of the design process. Supplementary materials and lesson plans developed in close consultation with teachers, librarians, teens, and other key experts and stakeholders will enable the ARGs to be widely and effectively used in museums, classrooms, libraries, and after-school programs.<br/><br/>The overarching goal of the project is to help realize the potential of ARGs as effective and perpetually innovative tools for informal STEM learning. By co-designing with teens, consulting with leading experts in science, technology, and education, authentically embedding learning assessments within the games, and creating ARGs with longevity and reusability in mind, researchers hope to increase learners' scientific inquiry skills, ability to use computational thinking for problem solving, understanding of deep-time science, and positive engagement with other STEM topics. Additionally, the ARGs will serve as templates and inspiration for future ARGs aimed at increasing STEM outcomes and other 21st century literacies and skills."
"1302518","AF: Medium: Towards Provable Bounds for Machine Learning","CCF","ALGORITHMIC FOUNDATIONS, ALGORITHMS","09/01/2013","07/14/2016","Sanjeev Arora","NJ","Princeton University","Continuing grant","jack snoeyink","08/31/2017","$900,000.00","Moses Charikar","arora@cs.princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","7796, 7926","7924, 7927","$0.00","Many areas of machine learning (ML) currently rely on heuristic algorithms with no performance guarantees, and, in fact, the underlying problems are computationally intractable.  The proposed project will lead to new machine learning algorithms with provable guarantees.  The PIs will leverage theoretical ideas from average case analysis, semi-random models, approximation, solution stability, and approximate linear algebra, and develop new tools and techniques.  Benefits from this endeavor will occur across fields.  Theoretical computer science will benefit by gaining new problems and research agendas, and further development of algorithmic and mathematical tools.  Machine learning will benefit by gaining new models (hopefully, more tractable than the ones currently in use), new modes of thinking, and new algorithms.  The task of proving bounds on algorithms will provide insight into when they do or do not work, as well as suggest modifications to existing heuristics.<br/><br/>The project will involve training a new generation of graduate students and postdocs who will be fluent both in theoretical algorithms and machine learning.  The PIs gained experience in delivering this kind of training and mentoring during the past couple of years and will continue this, including working with the undergraduate students.  The PIs will disseminate the ideas of this project by lecturing, teaching, and creating new course materials, including videos.  One specialized workshop will also be organized, devoted to the topics of study.  Open-source software implementation will be released based upon any new learning algorithms that are discovered."
"1331915","CyberSEES:Type 2: Precipitation Estimation from Multi-Source Information using Advanced Machine Learning","CCF","CyberSEES","09/15/2013","09/10/2013","Soroosh Sorooshian","CA","University of California-Irvine","Standard Grant","Eva Zanzerkia","08/31/2018","$1,060,000.00","Xiaogang Gao, Kuolin Hsu, Alexander Ihler","soroosh@uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","CSE","8211","","$0.00","This project will develop a cyber-enabled, data-driven modeling system that will use the vast amount of earth and environmental observational data to estimate precipitation, with the goal to further freshwater resource sustainability. One of the major objectives of this research is to innovatively apply advanced machine learning techniques to predict complex natural phenomena. Identification of current machine learning algorithms' representational and computational limitations when applied to earth and environmental data that are required for accurately estimating precipitation will also be explored. Specifically, this project will explore image-based feature extraction techniques and so-called ""deep belief"" models for interpreting important features within weather and climate data to estimate and predict precipitation. Features to be investigated include appearance, texture, shape, dynamics, and regional weather and climate signatures. The project will use data from ground-based radar, satellite-based sensors, and Numerical Weather Prediction (NWP) models, as well as physical characteristics from land surface datasets. Image-based extraction techniques will produce feature maps, which in turn will be used as input into a Deep Boltzmann Machine (DBM) model, a modern version of neural networks. A DBM represents a probability model over a collection of visible units and hidden units and produces as output a target variable (in this case, precipitation). This model is particularly suitable for the proposed modeling approach and for large-scale data. Once this system is developed, we will utilize a wide variety of earth science data to provide accurate, high-resolution global precipitation estimates. Verification methods to be used for evaluating precipitation estimates will include general statistics, precipitation intensity distribution, and regional analysis methods used by the atmospheric and hydrological sciences. The project will focus on verification over the United States, where high-resolution ground-based radar data are available. <br/><br/>In recent decades, extreme flooding and droughts have become more frequent and severe. Changing patterns in precipitation, which are attributed to climate variability, are responsible for these hydrologic extremes and contribute to uncertainties in freshwater resource management and planning. In the face of the planet's growing population and stresses on water resources, it is important to minimize these uncertainties and the social impacts of these natural hazards. These goals can only be accomplished through accurate precipitation measurements and forecasts. Satellite platforms and advanced numerical models produce massive amounts of global high temporal and spatial resolution data that can be used for this purpose, but analyzing these data remains a challenge. Recent innovations in computational sciences and machine learning have extended our capability to harvest from remotely-sensed data critical information that is essential to understanding cloud-precipitation systems. This project will adapt and improve satellite-based precipitation estimation algorithms by using computational science, statistical modeling techniques (machine learning), remote-sensing observations, and numerical models to develop cyber-enabled modeling systems that can effectively analyze the massive amount of observational data to improve the global estimation of precipitation at high spatial and temporal resolutions."
"1320651","RI: Small: Statistical Perceptual Inference in Visual Cortical Neural Circuits","IIS","Robust Intelligence","09/15/2013","09/06/2013","Tai Sing Lee","PA","Carnegie-Mellon University","Standard Grant","Kenneth Whang","08/31/2018","$499,961.00","","tai@cnbc.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7495","7495, 7923","$0.00","This interdisciplinary research project seeks to elucidate the computational machinery and algorithms in our brain that enables us to perceive 3-dimensional surfaces of objects in visual scenes based on the 2D images projected on our retinae.  The first conjecture of the project is that the neural circuitry underlying perceptual inference in our brain can be predicted by the statistical structures in our natural environment.  To prove this conjecture, statistical studies on 3D natural scenes will be carried out and their predictions on neural connectivity will be compared with the functional connectivity and tuning properties of depth-sensitive neurons in the primate visual cortex obtained using large-scale multi-electrode recording techniques. This will provide deeper insights into how the brain represents and builds models of the structures of the external world to enable perceptual inference. To understand what such circuits can compute, the investigator conjectures that neural circuits realize a class of generative models in computer vision and computational neuroscience called Markov random fields and Boltzmann machine. This second conjecture will be evaluated by exploring the theoretical link between the neural circuits and these computational models, by comparing experimental neural observations with behaviors of these computational models, and by evaluating the computational performance of the inferred neural circuits in solving stereo computation and surface interpolation problems in real world data. The research combines techniques in machine and statistical learning, computer vision, neural networks and neurophysiology to dissect neural circuits from a functional perspective. By linking real circuits to a powerful class of computational models in statistical inference, the project will have broader impacts by providing novel evidence and fundamental insight to the neural mechanisms and computational algorithms underlying statistical perceptual inference in the brain.  This interdisciplinary project will be a catalyst for the development of educational initiatives to bring bringing computer science and neuroscience together for undergraduates and graduate students, and to promote the awareness and involvement of students from multiple disciplines, including under-represented groups, in the field of computational neuroscience."
"1316996","NSF East Asia and Pacific Summer Institute (EAPSI) for FY 2013 in Taiwan","OISE","EAPSI","06/01/2013","05/16/2013","Chen-Ping Yu","NY","Yu Chen-Ping","Fellowship","Anne L. Emig","05/31/2014","$5,070.00","","","","Stony Brook","NY","117944400","","O/D","7316","5924, 5978, 7316","$0.00","This action funds Chen-Ping Yu of Stony Brook University to conduct a research project in the Computer and Information Science and Engineering area during the summer of 2013 at Perception and Attention Lab of National Taiwan University in Taipei, Taiwan. The project title is ""Modeling Human Visual Attention using Object-based Attention."" The host scientist is Dr. Su-Ling Yeh.<br/><br/>This project is developing a visual attention model as measured by shift in gaze, which is based on object-based attention theory and takes object parts (proto-objects) into account in addition to the usual spatial features. Current visual attention models predict where people look next based on salient regions of a scene, such as noticeable objects or just features that are prominent somewhere in the image. This corresponds to spatially-based attention, such that attention is driven by interesting locations in the scene. However, recent work suggests that objects can serve as the attention selection basis, in a way that when a person is already attending at an object, changes that occur on the attending object is noticed faster than the same changes that occur away from the object, albeit the same spatial distances. This object-based attention has not yet been captured in predicting human visual attention, and it is the goal for this project to incorporate object-based attention processing with existing spatially-based models in order to predict gaze shift that are more consistent with human behavior. The research model predicts that the shift of eye fixation utilizes the theory of object-based attention, and it requires three major components for processing: objects, the associated object parts (proto-objects), and the saliency map. The top-down object information is being obtained by applying state-of-the-art object detectors in computer vision; proto-objects are being segmented by merging coherent pixel fragments (superpixels) that are similar in low level features; and the saliency map is being generated using multi-scale blob detector on the image, with blob strength denoting the degree of spatial saliency. Initial fixation is set at the most salient region, and whenever an object is selected as being fixated, object-based attention suggests that consequent fixations may prioritize on the associate proto-objects before moving to another spatial salient location, in contrast from existing models that may immediately release from the attending object if other spatial location is deemed more salient. The performance of the eye movement model will be validated against a human fixation dataset using 1000 images from the PASCAL VOC2008 dataset and 104 images from the SUN 2009 dataset. <br/><br/>Broader impacts of an EAPSI fellowship include providing the Fellow a first-hand research experience outside the U.S.; an introduction to the science, science policy, and scientific infrastructure of the respective location; and an orientation to the society, culture and language. These activities meet the NSF goal to educate for international collaborations early in the career of its scientists, engineers, and educators, thus ensuring a globally aware U.S. scientific workforce. Furthermore, this project facilitates the learning exchange between the fields of Psychology (human vision) and Computer Science (computer vision). The model can also help and improve visual aids design for the general public, and the underrepresented groups that may require visual representations to be more intuitively perceived. This impact naturally leads to more partnerships between research, education, and the industry, which is all based on the better understanding of human visual attention."
"1319501","CSR: Small: Power Efficient Emerging Heterogeneous Platforms","CNS","Special Projects - CNS, CSR-Computer Systems Research","10/01/2013","06/05/2014","Gunar Schirner","MA","Northeastern University","Standard Grant","Marilyn McClure","09/30/2017","$466,000.00","David Kaeli","schirner@ece.neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","1714, 7354","7354, 7923, 9178, 9251","$0.00","This project addresses the problem of design space exploration for high-performance embedded computing platforms.  Such systems  are increasingly heterogeneous, and include powerful multi-core microprocessors (CPUs), many-core Graphics Processing Units (GPUs), and Digital Signal Processors (DSPs). Despite the availability of these powerful platforms, many barriers exist for application developers and platform designers to reap the full benefits of these devices.  Some of these challenges include finding effective mappings of applications to a wide range of heterogeneous compute platforms and defining/exploring the most appropriate memory interfaces.  <br/><br/>The goal of the project is development of a Heterogeneous Design Environment (HDE), that will allow one to define algorithms at a high level, and automate the search of the design space to select hardware and software implementations to meet desired objectives. Towards this co-design goal, the research includes investigation of methods of expressing algorithms at a conceptual, functional level; exploration of static methods of analyzing parallelism of applications; and extending simulation frameworks to support and accurately model power and performance on CPUs, GPUs, and DSPs at different levels of abstraction. The target application domain for proof of this concept is computer vision, one of the fastest growing market sectors within embedded high-performance computing, in which applications tend to be data-driven, compute-hungry and power-constrained.<br/><br/>The Heterogeneous Design Environment is expected to simplify development of power-efficient embedded computing applications across a set of heterogeneous devices.  By providing a rich set of design exploration tools in the context of industry-standard programming frameworks, such as Multi2Sim and OpenCL, the project aims to accelerate the pace at which new computer vision applications are developed. Integrated education and outreach activities include new development of new courseware, delivery of tutorials, involvement of undergraduates, participation in a summer program for middle-school students, and use of the HDE in undergraduate and graduate courses."
"1321119","CGV: Small: Towards Computational Stereoscopic Cinematography","IIS","GRAPHICS & VISUALIZATION","10/01/2013","07/07/2015","Feng Liu","OR","Portland State University","Continuing Grant","Jie Yang","09/30/2017","$270,784.00","","fliu@pdx.edu","1600 SW 4th Ave","Portland","OR","972070751","5037259900","CSE","7453","7453, 7923","$0.00","Stereoscopic 3D provides an immersive viewing experience and has evoked a tremendous amount of interest. However, stereoscopic content must be properly produced to avoid ""3D fatigue,"" such as blurring vision, eyestrain, and headache. The difficulty in producing stereoscopic content has become a bottleneck that limits its impact. <br/><br/>This research develops computational stereoscopic cinematography technologies to assist stereoscopic content production by integrating computer graphics, computer vision, stereoscopic cinematography, and stereo perception. This project explores fundamental and unique problems in computational stereoscopic cinematography: how to edit disparity to deliver a pleasant viewing experience; how disparity interacts with other aspects of stereoscopic content like monocular depth cues and motion; how to optimize disparity during stereoscopic content manipulation and authoring, including warping, collage authoring, and video stabilization; how to coordinate the process of the left and right view with no or unreliable stereo correspondences; and how to extend monocular content manipulation and authoring technologies to stereoscopic content in a principled way. In short, this project solves key problems in computational stereoscopic cinematography and establishes general principles for developing stereoscopic content processing and authoring technologies. <br/><br/>This computational stereoscopic cinematography research facilitates high-quality stereoscopic content production, which is critical in sustaining and boosting the impact of stereoscopic 3D in a variety of applications, including virtual reality, human computer interaction, scientific visualization, educational training, and medical rehabilitation. The research results, including the technical writings, the stereoscopic image/video library, and the code infrastructure, are disseminated through paper publications and online sharing."
"1444234","CAREER: Toward a General Framework for Words and Pictures","IIS","ROBUST INTELLIGENCE","08/28/2013","05/29/2015","Tamara Berg","NC","University of North Carolina at Chapel Hill","Continuing grant","Jie Yang","05/31/2017","$269,926.00","","tlberg@cs.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","7495","1045, 1187","$0.00","Pictures convey a visual description of the world directly to their viewers. Computer vision strives to design algorithms to extract the underlying world state captured in the camera's eye, with an overarching goal of general computational image understanding. To date much vision research has approached image understanding by focusing on object detection, only one perspective on the image understanding problem.  This project looks at an additional, complimentary way to collect information about the visual world -- by directly analyzing the enormous amount of visually descriptive text on the web to reveal what information is useful to attach to, and extract from pictures. This project presents a comprehensive research program geared toward modeling and exploiting the complimentary nature of words and pictures. One main goal is studying the connection between text and images to learn about depiction -- communication of meaning through pictures. This goal is addressed through 3 broad challenges: 1) Developing a richer vocabulary to describe the information provided by depiction. 2) Developing image representations that can visually capture this more nuanced vocabulary. 3) Constructing a comprehensive joint words and pictures framework. <br/><br/>This project has direct significance to many concrete tasks that access images on the internet including: image search, browsing, and organization, as well as commercial applications such as product search, and societally important applications such as web assistance for the blind. Additionally, outputs of this project, including progress toward a natural vocabulary and structure for visual description, have great potential for cross-cutting impact in both the computer vision and natural language communities."
"1445409","RI: Medium: Integrating Humans and Computers for Image and Video Understanding","IIS","ROBUST INTELLIGENCE","08/21/2013","05/04/2015","Tamara Berg","NC","University of North Carolina at Chapel Hill","Continuing grant","Jie Yang","04/30/2017","$764,930.00","","tlberg@cs.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","7495","7495, 7924","$0.00","In this project, the research team explores several research challenges to exploit the relationship between images, video, and the people viewing this visual imagery.  Areas of exploration include: 1) behavioral experiments to better understand the relationship between human viewers and imagery, 2) development of human-computer collaborative systems for image and video understanding that utilize automatic computer vision algorithms in conjunction with active and passive cues from human viewers, and 3) implementing retrieval and collection organization applications using our collaborative models.<br/><br/>Billions of images and millions of videos are now available online via the infrastructure of amazingly successful companies from Google to Microsoft to Facebook. This wealth of visual data is creating considerable opportunities for communication and community, and tightening the social fabric of our world.  In parallel to this explosion in online imagery, there is also an increasing proliferation of cameras viewing the user, from the ever present webcams peering out at us from our laptops, to cell phone cameras carried in our pockets wherever we go. This record of a user's viewing behavior, particularly of their eye, body movements, or descriptions, can provide enormous insight into how people interact with images or video, and can inform construction of more effective visual applications such as image or video retrieval.  In addition, understanding what people recognize, attend to, or describe about an image or video is a necessary step toward high level goals of human centric image understanding that will have research benefits to many diverse fields, including computer vision and behavioral science."
"1253146","CAREER: A Rigidity Theory for Multi-Robot Formations","IIS","CAREER: FACULTY EARLY CAR DEV, IIS Special Projects, Robust Intelligence","10/01/2013","09/12/2013","Audrey St. John","MA","Mount Holyoke College","Standard Grant","Erion Plaku","09/30/2019","$411,531.00","","astjohn@mtholyoke.edu","50 College Street","South Hadley","MA","010756456","4135382000","CSE","1045, 7484, 7495","1045, 7484, 7495, 9229","$0.00","This proposal connects foundational research for multi-robot formations with the development of empowering experiences for women undergraduates in the classroom and beyond. The theoretical nature of the research is complemented by a firm grounding in hardware and computer vision fundamentals, integrated throughout a comprehensive education plan. The PI will develop an understanding of geometric formations of multi-robot systems, such as swarms in both 2- and 3-dimensions. Sensor and communication costs will be integral to modeling and algorithmic considerations, as minimizing power consumption is increasingly important for the design of lightweight and agile robot platforms. The PI will establish mathematical foundations and develop algorithms in three fundamental directions: (1) understanding a formation's structural properties, (2) producing optimal control architectures, and (3) predicting a formation?s internal motions. Developing a unifying theory from both theoretical and applied perspectives will produce a wealth of new directions, such as actuating a formation as if it were a single traditional robot.<br/><br/>The research contributions have the potential to significantly impact cutting-edge technology for the control and coordination of multi-robot systems. The PI is junior faculty at Mount Holyoke College, a liberal arts college for women, where a recent growth in enrollments has led to an average of 15 computer science majors a year (surpassing the peak of 2002). She will engage this vibrant community of budding computer scientists through her proposed education plan. Two courses will be developed, designed to simultaneously educate undergraduates through core computer science principles and expose them to exciting research problems challenging the field. Students will have additional opportunities for experiences outside the classroom through highly visible robotics and computer vision projects on campus, producing role models for generations to come. By working closely with the student-run CS Club, the PI will establish a supportive environment that fosters growing interest in technology from traditionally under-represented groups. She will also actively involve students in research by supervising two undergraduates each summer through a compelling research experience."
"1246422","Small Business-ERC Collaborative Opportunity: In-Situ Information Display for Ultrasound Guided Interventions","EEC","ERC-Eng Research Centers","02/15/2013","07/14/2014","Philipp Stolka","MD","Clear Guide Medical, Inc.","Fixed Price Award","Deborah Jackson","07/31/2016","$199,850.00","Gregory Hager, Ralph Etienne-Cummings, Emad Boctor Mikhail","stolka@clearguidemedical.com","3600 Clipper Mill Road","Baltimore","MD","212112146","4105046540","ENG","1480","130E, 131E","$0.00","Image guidance is a key enabling technology for millions of interventional procedures annually in medicine. Biopsies, ablative procedures, energy-based therapies, as well as minimally invasive and open procedures all rely on real-time feedback from imaging in order to be performed safely and effectively. In current practice, this feedback is predominantly provided by ultrasound (US) as it offers safe, real-time imaging for low cost.  However, ultrasound often has poor image quality and is difficult to manage, particularly for interventions requiring high precision. These issues have led many research groups and several companies to investigate methods for enhanced image guidance.  <br/><br/>The vision of this proposal is to advance ultrasound-guided cancer therapy with an innovative, low-cost, and self-contained navigation device which is being developed by Clear Guide Medical LLC, a small business spun out of the CISST ERC.  The proposed device will enable the system to perform dynamic registration to a preoperative diagnostic image. The result will be a ""Body GPS"" system that is able to localize the probe in ""body coordinates.""  This platform will then be further developed to provided targeting information via real-time, on patient targeting display.<br/><br/>The principle objectives are: 1) to develop algorithms to calibrate the elements of the system; 2) to provide surface representations of the patient via structured light stereo; 3) to develop projection modes for on-patient targeting display; and 4) to evaluate the resulting projection methods in a simulated biopsy environment.  Objectives 1 and 2 will be performed in collaboration with the CISST ERC; while objectives 3 and 4 will be led by Clear Guide Medical.<br/><br/>One of the core research thrusts of the CISST ERC is the development of new technologies for percutaneous therapy (that is medical procedures where access to inner organs or other tissue is done via needle-puncture of the skin, rather than by using an ""open"" approach where inner organs or tissue are exposed). To create a viable device, it will be necessary to combine advanced methods in computer vision and ultrasound imaging with new methods for on-patient targeting display. The project team is led by PI Stolka, the head of engineering for Clear Guide Medical and an expert on ultrasound systems, and ERC PI Etienne-Cummings, and expert on compact computational imaging system. They are assisted by co-PI Boctor, an expert in ultrasound imaging and image guidance and co-PI Hager, CEO of Clear Guide Medical and an expert in computer vision."
"1343402","AIR Option 1: Technology Translation:  Automated Targeted Destination Recognition for the Blind with Motion Deblurring","IIP","Accelerating Innovation Rsrch","09/15/2013","05/27/2014","YingLi Tian","NY","CUNY City College","Standard Grant","Barbara H. Kenny","02/28/2017","$161,997.00","","ytian@ccny.cuny.edu","Convent Ave at 138th St","New York","NY","100319101","2126505418","ENG","8019","116E, 8019, 9251","$0.00","This PFI: AIR Technology Translation project focuses on translating the research advances in computer vision technologies to fill the gap between research and the real needs in assistive technology for blind people. The goal of this project is to explore solutions for the challenges of designing wearable camera-based wayfinding assistants for blind persons from fundamental research to produce a portable proof-of-concept prototype to help blind users recognize targeted destinations from complex environments. The project accomplishes this goal by 1) developing robust motion deblurring methods to handle the irregular motions of blind users who are wearing the camera and 2) recognizing targeted destinations based on the requests of blind users for wayfinding and navigation. The proof-of-concept prototype system will be developed with visual information captured via a wearable camera on sunglasses/hat and a portable computer (mini laptop or cell phone) for data analysis, while the speech/sound outputs will be provided to a blind user via a Bluetooth earpiece. The performance of the newly developed algorithms and the proof-of-concept prototype will be evaluated by blind subjects.<br/>The partnership engages different areas of expertise including university research, industry real system design and development, and blind user study from organizations for the blind and visually handicapped to provide solutions for the most challenging problems (motion blur and query-based targeted destination recognition) in the design of wearable camera-based wayfinding and navigation aids for blind users. The interdisciplinary areas of expertise pertain to the potential to translate the computer vision based assistive technology along a path that may result in a competitive commercial reality. This will lead to new revolutionary design concepts of cost-effective and portable camera-based assistive devices to help visually impaired people in achieving functional mobility comparable to people with normal vision. The potential economic impact is expected to be $120 million in the next 10 years, which will contribute to the U.S. competitiveness in developing technology for blind wayfinding and navigation.   This research has a direct impact to benefit the visually impaired, improve their inclusion and integration into society, and enhance their future employment opportunities, success in the workplace, independent living, and economic and social self-sufficiency."
"1320910","Sparse 3D-Data Representations from Compactly Supported Atoms for Rigid Motion Invariant Classification with Applications to Neuroscience Imaging","DMS","COMPUTATIONAL MATHEMATICS","08/01/2013","09/10/2015","Emanuel Papadakis","TX","University of Houston","Continuing Grant","Rosemary Renaut","05/31/2017","$229,993.00","Ioannis Kakadiaris, Demetrio Labate","mpapadak@math.uh.edu","4800 Calhoun Boulevard","Houston","TX","772042015","7137435773","MPS","1271","9263","$0.00","The quantitative characterization of neuronal morphology is currently among the most fundamental objectives in neuroscience, as it is essential to precisely correlate structure, activity and neuronal communication at the cellular level. It has been long established that neurons respond to external stimuli through significant structural changes. Hence, the ability to quantitatively capture and track these changes is fundamental for understanding the cell-level biology of brain functions.  Dendritic spines, in particular, are are sub-cellular structures, which play a key-role in how neurons talk to each other; these structural changes are referred to as synaptic plasticity. Although, this type of microanatomical plasticity has been associated with drug addiction or even with autism, the change in the morphology and density of spines in addiction, autism and in neurodegenerative diseases is still unclear. Recent developments of high-resolution confocal single photon and multi-photon microscopy, together with the ability to mark subcellular structures, provide a new window for the observation of live single or small groups of neurons which never existed before. However, extracting useful information using this novel imaging is a challenging task. In particular, observing the variations of spine populations, which are in the order of several thousands for even a single neuron, categorizing them in different types and maintaining the timeline of changes are largely laborious manual tasks that are subject to the inevitable inaccuracies native to repetitive and tedious manual work.<br/><br/>Our project aims to develop the algorithmic foundations for a new generation of software tools for extracting global spine morphometric characteristics and population dynamics from high-resolution confocal single photon and multi-photon microscopy images with minimal human intervention. Such tools will have a transformative effect in the logistics of spine studies, because they slash the required labor cost. To achieve our goal, we will make contributions to both mathematical analysis and computer vision. We aim to develop robust methods for the detection, identification of type and estimation of volume regardless of the position and spatial orientation of spines in a 3D image of a neuron acquired with the said microscopes. Finally, our project offers educational opportunities to graduate students in a blend of abstract and computational mathematics and computer vision. We also plan to continue our outreach activities to local high schools located in disadvantaged areas of Houston, offering to top students sneak peeks of the life of the mathematician, the computer scientist and the biologist researcher."
"1254041","CAREER: An Information-Theoretic Approach to Communication-Constrained Statistical Learning","CCF","Comm & Information Foundations, COMM & INFORMATION THEORY","02/01/2013","11/25/2019","Maxim Raginsky","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Phillip Regalia","01/31/2021","$518,435.00","","maxim@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7797, 7935","1045, 7935","$0.00","This project aims to develop an information-theoretic approach to communication-constrained statistical learning problems involving multiple learning agents located at the nodes of a large network. This approach will build on the recently introduced coordination paradigm within network information theory, which looks at multiterminal problems in terms of optimal use of communication resources in order to establish some desired statistical correlations between the nodes of a network. The main theoretical goal is to explicitly identify the effect of bandwidth limitations, losses, delays, and lack of central coordination on the performance of statistical learning algorithms over networks. The project will systematically explore the fundamental limits of learning in multiterminal settings and design efficiently implementable and robust coding/decoding schemes. The theory developed under this project will be a novel synthesis of probabilistic techniques from machine learning (such as empirical process theory) and of multiterminal information theory (such as distributed lossy source coding).<br/><br/>As a broader impact, this project will provide key enabling technologies for large-scale, distributed applications of machine learning in such domains as smart grids, health-care informatics, transportation networks, and cybersecurity. Statistical machine learning is emerging as a dominant paradigm for making accurate predictions on the basis of empirical observations in the presence of significant model uncertainty. Most of the research activity in this field, however, has taken place in isolation from the realities of complex networks and all the attendant limitations on information transmission and processing: it is frequently assumed that the data needed for learning are available instantly, with arbitrary precision, and at a single location. However, given the fact that most data fed to machine learning algorithms are increasingly generated, exchanged, stored and processed over large-scale networks, there is a pressing need to dispense with this assumption and thus take network effects into consideration. The theory and the algorithms developed as part of this project will ensure that the relevant data are delivered over the network to the right decision-makers, while securing accurate decisions made on the basis of the received information. The research component of the project is tightly integrated with an education and outreach plan, including development and teaching of new courses on machine learning aimed specifically at engineering students."
"1252648","CAREER: Self-adjusting Models as a New Direction in Machine Learning","IIS","Info Integration & Informatics","03/01/2013","02/23/2017","Murat Dundar","IN","Indiana University","Continuing Grant","Sylvia Spengler","02/28/2019","$499,998.00","","dundar@cs.iupui.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","CSE","7364","1045, 7364","$0.00","Machine learning algorithms are now routinely used to build predictive models from data in wide range of applications. However, current approaches to machine learning have an important limitation: They assume that the set of classes observed in a training data set is exhaustive and that new data samples originate from one of the existing classes represented in the training data set. This assumption is unrealistic in many real-world applications in which previously unobserved classes of interest emerge.  <br/><br/>This study explores a new class of machine learning algorithms that produce self-adjusting models that can accommodate new classes observed in data in offline as well as online learning scenarios. The project aims to (i) use non-parametric models to dynamically incorporate the changing number of classes; (ii) develop new online and offline inference techniques to accommodate new classes as they emerge (iii) automatically associate newly discovered classes with higher-level groups of classes in an attempt to identify potentially interesting class formations, and (iv) develop partially-observed tree models containing observed and unobserved nodes, where observed nodes represent existing classes and unobserved nodes are introduced online to fill the gaps in the existing data hierarchy that become evident only with the arrival of new data.<br/><br/>The broader impacts of  this work could  extend to several real world applications: Bio-security and bio-surveillance, information retrieval, and remote sensing among others in settings where all of the classes are not known a priori. The educational plan includes outreach to K-12 students and enhanced research opportunities for undergraduate and graduate students in computer science as well as at the intersection of computational and life sciences. All the software, publications, and data sets resulting from the project will be freely disseminated to the larger research and educational community. Additional information about the project can be accessed through the project website at http://www.cs.iupui.edu/~dundar/career.html"
"1347208","EAGER: Viewpoint Tracking via Acceleration Stabilized with Computer Vision","IIS","GRAPHICS & VISUALIZATION","08/15/2013","08/06/2013","Frederick Brooks, Jr","NC","University of North Carolina at Chapel Hill","Standard Grant","Ephraim P. Glinert","07/31/2014","$75,000.00","","brooks@cs.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","7453","7453, 7916","$0.00","This project addresses a problem which has kept virtual reality from widespread use. Some 15 years ago high-capability graphics cards in PCs reduced the cost of computing for virtual environments from hundreds-of-thousands of dollars to (today) hundreds of dollars. Low-cost head-mounted displays have just appeared. The similar advance in viewpoint tracking has not occurred; accurate, low-latency wide-area viewpoint tracking remains very costly. Virtual reality demands stereo 60 frames per second per eye and system latencies below 50 ms. This research is developing a novel system to provide accurate, low-latency viewpoint tracking to meet these requirements with consumer-cost components. The research is based upon a recently demonstrated proof-of-concept system. A standard RGB-Depth camera sits on the user's head. Pose is calculated by matching images against an environment model. A Kalman filter integrates rotational velocity and linear acceleration from a cheap high-speed inertial measurement unit (IMU) to update the pose estimate many times between frames. This not only gives low-latency pose readings, it also improves initial values for the next camera calculation. The depth images and reconstruction software are concurrently used to incrementally build/update the depth model of the environment for the camera matching. <br/><br/>The current research is demonstrating the system's potential. To work completely successfully, both conceptual and algorithmic advances are in process. IMU calibrations are being improved. Temperature and dynamic bias must be compensated in the calibration to improve stimation and reduce jitter. Using multiple cameras to reduce overall noise and handle difficult cases (such as blank walls) are being addressed with new algorithms and evaluated. The merging of new and modeled data is computationally expensive. The feasibility demo uses two GPUs, one for rendering; one for tracking. Ways are being invented to do it on one.  Additional future research includes tracking dynamic objects and incorporating object recognition (e.g., such as a desk or chairs) to improve estimates. Widespread access to virtual reality may well open new, unexpected creative uses of the technology. The research is inventing the proof-of-concept system forward to one that can make this exciting leap."
"1338509","BCC: Organizing Multi-Disciplinary Communities to Conduct Data-Intensive Research on Education and Learning","SMA","International Research Collab, REAL","09/15/2013","04/21/2015","Erin Peters-Burton","VA","George Mason University","Standard Grant","John Cherniavsky","08/31/2016","$538,403.00","Mika Seppala, Colleen Ganley, Sara Hart","epeters1@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","SBE","7298, 7625","5935, 5979, 7433, 9179","$0.00","The PIs of this project are building the capacity for research within the data-intensive environments of massive open online courses. The PIs are using as an organizing example the World Education Portals (WEPS) massive open online course (MOOC) that focuses on calculus and has been taught through Florida State University and the University of Helsinki for over 8 years. The PIs are building a community of researchers who are studying ways to improve student achievement measures, pedagogical aspects and the extensions to mobile platforms. A specific set of research foci for this community include the elements of design-based experimentation in MOOCs, motivational and affective aspects that can be studied within such environments, the development of new statistical and analytics methods, ethical issues that surround access to data elements, and economic aspects of MOOCs. Multiple workshops bring the community together, and Mendeley.com (an academic social network) is being used to coordinate the growing research agendas around MOOCs in the time between the workshops. The project also includes an invitational conference to organize participant papers for a set of proceedings and reports. <br/><br/>This project builds on work that was funded as part of a suite of projects collaborating virtually with institutes of higher education in Finland. The project provides a vehicle to move the development and initial research on MOOCs to a wider community. The project leverages individuals who are experts in mathematics learning together with experts in analytics and educational data mining to provide a deep connection to learning in the MOOC virtual learning environments. <br/><br/>A criticism of the learning analytics and educational data mining research agenda is that a focus of the deep learning about STEM content is not well represented in the research and development community. Too frequently, the focus is on the technical development of the infrastructure and the statistical processes that are necessary to analyze the large amounts of data. What has been missing from the research agenda is the focus on how students are learning in MOOCs and what aspects of the learning environments are supporting that learning. This study brings the connection of the study of the learning of mathematics into the analysis of the adequacy of the MOOC learning environment."
"1323787","Full-Scale Development: Collaborative Research Advancing Informal STEM Learning Through Scientific Alternate Reality Games","DRL","AISL","10/01/2013","07/31/2015","Derek Hansen","UT","Brigham Young University","Continuing grant","Arlene de Strulle","09/30/2018","$1,249,459.00","Steven Shumway","dlhansen@byu.edu","A-285 ASB","Provo","UT","846021231","8014223360","EHR","7259","9150, 9177, SMET","$0.00","Full-Scale Development: Collaborative Research Advancing Informal STEM Learning and Computational Thinking through Scientific Alternate Reality Games<br/><br/>Brigham Young University and the University of Maryland, in partnership with the Smithsonian Institution, the Computer History Museum, and NASA, plus leading game designers, educators, scientists, and researchers, will conduct research on the design and development of two large-scale Alternate Reality Games (ARGs) based on deep-time science in astrobiology, astrophysics, and interplanetary space travel. The project will iteratively design and test two distinct types of ARGs (closed- and open-ended) to study the effects of these ARGs on STEM learning. <br/><br/>The ARGs will be based upon the Next Generation Science Standards (NGSS), affording learners with intensive, self-driven, and scaffolded scientific learning and will be aimed at attracting girls and other groups historically underrepresented in science and technology. Each ARG will be designed by NASA scientists, educators and education researchers, and game-based learning experts and will be highly interactive: engaging learners in collaborative investigations in real and virtual worlds to collect scientific data, conduct data analysis, and contribute scientific evidence that will help solve scientific questions within a science-based narrative derived from real world problems that will develop learners? computational thinking skills in a collaborative, participatory virtual learning environment. <br/><br/>Combining data from web and social media analytics, player interviews, surveys, and user-generated content, researchers, and evaluation experts at UXR who will provide an outcomes-based evaluation, including front-end, formative, remedial, and summative evaluations, will establish the properties of ARGs that most effectively advance informal STEM learning outcomes. By comparing open-ended and closed-ended ARGs, the PIs will be able to assess the relative strengths and weaknesses of two distinct approaches to Alternate Reality Game design. The project team will test the hypothesis that open-ended, user-generated content will support inquiry-based learning, peer-to-peer learning, and life-wide and life-deep learning, while close-ended, narrative-rich ARGs will support specific transfer of STEM knowledge, collaboration, and problem solving. To help ensure that the games appeal to their target audiences, the project team will adopt co-design methods, enlisting the creative input of participating teens at each stage of the design process. Supplementary materials and lesson plans developed in close consultation with teachers, librarians, teens, and external stakeholders will enable the ARGs to be widely and effectively used as a model in museums, classrooms, libraries, and after-school programs. <br/><br/>The proposed ARGs represent a unique environment to test learning principles that enable players to bridge their learning through transmedia across multiple contexts and test the effects of collaboration with massive numbers of concurrent players. As a result, the project should yield insights on how learning principles can be adopted and re-appropriated for emerging learning environments, including those that that might be crowd-sourced. The research is well grounded in the literature and the PIs do an excellent job of mapping ARG design principles to the pertinent learning science research, providing a clear sense of the particular affordances of the genre that should lead to new understandings. The approach has profound implications for the way we might teach the next generation of students. The ability to mix problem solving and learning in virtual spaces with experiences and data derived from the physical world could dramatically change how we understand the role of technology in education."
"1307801","The Dark Reaction Project:   A Machine Learning Approach to Materials Discovery","DMR","OFFICE OF MULTIDISCIPLINARY AC, Comp&Data Driven Mat Res(CDMR), CDS&E","09/15/2013","08/28/2013","Joshua Schrier","PA","Haverford College","Standard Grant","Daryl Hess","08/31/2017","$299,998.00","Alexander Norquist, Sorelle Friedler","jschrier@fordham.edu","370 Lancaster Avenue","Haverford","PA","190411336","6108961000","MPS","1253, 8029, 8084","7433, 8084, 9177, 9216","$0.00","Technical Summary:<br/><br/>Hydrothermal synthesis reactions of organically-templated inorganic solids are an ideal test case for data-driven materials chemistry, as just a few reactants (one or two inorganic components, one or two organic components and solvent) and a few reaction conditions (pH, temperature, reaction time) yield a diversity of products with numerous applications.   Despite this simplicity, the formation of crystalline products depends sensitively on the quantities of reagents used and the reaction conditions, which makes this a demanding test case for predicting success or failure of the reaction.  Moreover, unlike other systems such as metal organic frameworks (MOFs), the many different types of intermolecular interactions that are present result in highly diverse crystal structures which cannot be predicted a priori.  <br/>Rather than predicting a final crystal structure, we aim to address the simpler problem of whether a reaction will yield any crystalline product or not.  Our project will address this with three strategies: We propose constructing a searchable online repository for ""dark reactions"", the chemical reactions that have been performed and recorded in laboratory notebooks, but never reported in the literature. This begins with putting our own reactions online, then the reactions of selected experimental collaborators, and finally creating a web-accessible public repository for depositing, retrieving, and utilizing reaction information.  Using this data, we propose using machine learning to derive predictions to increase the success rate of performing novel reactions.   From the experimental reaction data, we use cheminformatics calculations to predict >200 computed properties of the individual reagents (e.g., van der Waals surface areas, polar surface areas as a function of pH, number of hydrogen bond donors and acceptors, etc.)  and compute 50 stoichiometric descriptors (e.g., ratios of organic and inorganic components, weighted by hydrogen bond donor/acceptors as a function of pH, etc.). Based on a preliminary dataset of 506 reactions, we have been able to train a decision tree model to achieve an 87% success rate in predicting whether a crystalline product is formed or not. During this project, we will make the improved model publicly available (via the web), address weaknesses in the physicochemical model, and integrate this with databases of commercially available starting materials.   Finally, we will perform experimental validation to demonstrate a proof-of-principle synthesis of new compounds, address structural holes in the dataset, and engage with the broader research community to guide experiments in other laboratories for the synthesis of new materials and addressing limitations in the chemical space of our model.  Besides the impact on this specific area of materials synthesis, the software architecture that we develop will serve as a starting point for others to begin similar projects and we commit to freely distributing our work to others by open-sourcing our code under a license that will allow its free use in academic settings. <br/> <br/><br/>Non-technical Summary:  <br/><br/>Organically-templated metal oxide framework compounds have outstanding structural and chemical diversity, which lends them to applications for industrial catalysis, gas separation, and optical engineering.  Yet, despite several decades of experimental effort, making new examples of these materials is a time-consuming trial-and-error process.  Most of the chemical reactions that have been performed are deemed ""unsuccessful"" because they do not result in a crystalline product, and are never reported in the literature.  There is no forum for collecting these experiments, nor a means for deriving value from them.  Nevertheless, these ""dark reactions"" are valuable because they define the bounds on the reaction conditions needed to successfully produce a product. By providing a searchable online repository for reaction data, we will enable better management and sharing of these dark reactions.  Moreover, we will use this data as a resource to train machine learning (aka statistical learning or data-mining) algorithms that predict the success of reactions ahead of time.  Based on the machine learning predictions, we will perform experimental validation to test the predictions of the model.<br/>Our project will provide a mechanism for collecting the dark reactions and then using them to guide future reactions to be more successful, reducing the researcher time and cost of reagents needed to synthesize new materials.  This will accelerate and lower the cost (in researcher time and materials) of discovering new materials.  This directly addresses the call of the White House Office of Science and Technology Policy's 2011 Materials Genome Initiative, specifically finding ways to use computation to bring functional materials to market more quickly.  Second, this project will serve as a model for collaboration between chemists and computer scientists that can be directly transferred to a wide range of other disciplines and avenues of investigation.  Third, we will provide a cohesive, comprehensive, interdisciplinary and sustained research experience for undergraduate students, thus contributing to the scientific workforce.  Fourth, our outreach activities will foster interest in data-driven techniques, create a network of collaborating laboratories and provide the software infrastructure to others wishing to initiate related projects."
"1320538","RI: Small: Bayesian Thinking on Your Feet---Embedding Generative Models in Reinforcement Learning for Sequentially Revealed Data","IIS","ROBUST INTELLIGENCE","08/01/2013","06/25/2015","Jordan Boyd-Graber","MD","University of Maryland College Park","Continuing grant","Donald T. Langendoen","07/31/2017","$500,000.00","Hal Daume","jbg@umiacs.umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","7495","7495, 7923","$0.00","Machine learning algorithms cannot ""think on their feet"".  When applied in practice, most approaches developed using traditional machine learning techniques wait for an entire input to arrive before they are able to provide an answer or react.  While sufficient for some tasks, this is inappropriate for a large class of problems that require more immediate or incremental responses.  This project develops new algorithms to address machine learning problems that require an algorithm to ""think on its feet"".  These algorithms combine guesses about what input is likely appear in the future with actions that the algorithm should take now to provide useful, effective output in a timely fashion.<br/><br/>One application of these new methods is simultaneous translation.  This is the problem of taking problem of ""observing"" a sentence one word at a time in a foreign language, such as German, and providing a real-time running translation in a target language (like English).  This is particularly difficult for language pairs that have significant syntactic divergences, such as object-verb order differences between foreign languages like German or Japanese (verb final) and target languages like English (verb medial).  Like human simultaneous translators, machine learning algorithms must learn to predict the words that will appear at the end of a sentence.  The project facilitates this prediction using a framework that combined word prediction and machine translation system.<br/><br/>The project also uses the newly developed  algorithms in academic settings to provide significant outreach to high school students and undergraduates, particularly in underrepresented communities."
"1321168","CGV: Small: A Patch-based Framework for Capturing a World in Motion","IIS","GRAPHICS & VISUALIZATION","10/01/2013","07/16/2015","Pradeep Sen","CA","University of California-Santa Barbara","Continuing grant","Jie Yang","09/30/2017","$499,719.00","","psen@ece.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","CSE","7453","7453, 7923","$0.00","Although the real world is dynamic, many techniques used to image/capture it are fundamentally sequential in nature.  For example, capturing a high-dynamic range (HDR) image of a scene (which contains a wide range of illumination) without special hardware involves taking a set of sequential images at different exposures, each one measuring a small range of illumination.  However, this approach has problems when reconstructing the HDR image of dynamic scenes with moving subjects, since the individual frames are not registered correctly.  Problems like this appear in many research areas, from medical imaging to computer vision.<br/><br/>In this project, the PI and his team are developing a common framework that addresses artifacts from motion for a variety of different applications.  Their key insight is that patch-based optimization can be used to handle motion inconsistencies without explicitly solving the challenging problem of accurate motion estimation.  This produces results that are reconstructed from different inputs in a consistent manner without motion artifacts.  The PI is exploring how this framework can be applied to several important research areas, from high-quality imaging to computer vision applications such as the reconstruction of dynamic scenes.<br/>  <br/>Improved capture of dynamic scenes has the potential to transform the way certain imaging procedures (such as medical imaging) are done.  Scientific results of this work are disseminated through technical publications at top venues in the graphics/vision communities, and the PI plans to release the algorithms developed online.  Finally, this project involves high school and under-represented students into the research effort."
"1253538","CAREER: Combinatorial Inference and Learning for Fusing Recognition and Perceptual Grouping","IIS","Robust Intelligence","10/01/2013","08/11/2017","Charless Fowlkes","CA","University of California-Irvine","Continuing Grant","Jie Yang","09/30/2019","$507,903.00","","fowlkes@ics.uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","CSE","7495","1045","$0.00","When presented with a novel image, humans typically have little problem providing a consistent interpretation of the scene in terms of contours, surfaces, junctions, and the relations between them. This process of perceptual organization is closely coupled with recognition of familiar shapes and materials. Perceptual organization can aid recognition by reducing the complexity of a cluttered scene to a small number of candidate surfaces while recognition can help resolve ambiguities in grouping based on local image cues.   This project is developing a computational framework that fuses top-down information provided by recognition with bottom-up perceptual organization in order to automatically produce a coherent scene interpretation. This research includes (1) identifying local image features that provide cues to grouping and figure-ground, (2) developing libraries of composable detectors that capture the appearance of objects, parts and their spatial relations, and (3) designing models and efficient inference routines that explicitly reason about occlusion and the binding of image regions and contours into object shapes.<br/><br/>Integrated models of grouping and recognition have direct significance to expand the computer vision capabilities of robotics and assistive technologies that must operate in complex, cluttered environments.  The framework being developed also has applications in automating biological image analysis where top-down shape information are useful in resolving noisy local measurements. The computational tools developed by the project along with dissemination and educational efforts are aimed at forming an interdisciplinary bridge between biological imaging and cutting-edge computer vision research."
"1254840","CAREER:  New Directions in Spatial Statistics","DMS","STATISTICS, Division Co-Funding: CAREER","07/01/2013","07/01/2014","Debashis Mondal","IL","University of Chicago","Continuing grant","Gabor J. Szekely","02/28/2015","$152,239.00","","debashis.mondal@oregonstate.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","MPS","1269, 8048","1045","$0.00","The  de Wijs process (also known as the  Gaussian free field in statistical physics) is a fundamental spatial process that arises as the scaling limit of lattice based Gaussian Markov random fields and generalizes Brownian motion in two-dimensions. However, at present,  there is a wide gap between the theory of Gaussian free field (including the subsequent theory of random fields) in statistical physics and modern probability, and the current practice of spatial statistics via lattice based Gaussian Markov random fields. Thus, there is great need to bridge this gap to develop a principled framework for statistics and inference of spatial models  and to pursue novel computations that make such inferences feasible.  This  project will consider formulating appropriate functionals of the de Wijs process to construct useful random fields and novel matrix-free computations via conjugate gradient and other methods, and will focus on developing new areas of scientific applications. The proposed research will also shed new light on and allow deeper understanding of theoretical and computational issues discussed by many researchers in spatial statistics in the past decades.  Novel matrix-free computations  will provide further impetus to study parametric bootstrap methods and multi-scale modeling, and to construct a new class of non-Gaussian random fields.  The project will contribute  to obtaining enhanced scientific understanding in studies of environmental bioassays, arsenic contamination of groundwater and distributions of galaxies. <br/><br/>Advances in the field of spatial statistics are important because new statistical methods can be applied to a wide range of scientific questions in fields such as astronomy, agriculture, biomedical imaging, computer vision, climate and environmental studies, epidemiology and geology. The de Wijs process is one fundamental spatial process that generalizes Brownian motion from time to space.  Using the de Wijs process as a fundamental building block, this project will develop novel mathematics and derive fast, efficient and large-scale statistical computations so that various scientific questions can be answered in a practical way. This will lead to new developments  for the analysis of continuum spatial data  and spatial point patterns, and will allow us to obtain enhanced scientific understanding in studies of environmental bioassays, arsenic contamination of groundwater and distributions of galaxies.  The statistics and the computations that will be developed in this project will also be particularly relevant for various research problems that arise in environmental or global change, and in health studies. Finally, this project will integrate research and educational activities through the development of new graduate and undergraduate courses and will also provide valuable training and learning opportunities for students at graduate and undergraduate levels."
"1305302","II-NEW: Shared High Performance Data Center","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","10/01/2013","09/20/2013","Yijuan Lu","TX","Texas State University - San Marcos","Standard Grant","Wendy Nilsen","09/30/2018","$375,759.00","Xiao Chen, Anne Ngu, Apan Qasem, Byron Gao","lu@txstate.edu","601 University Drive","San Marcos","TX","786664616","5122452314","CSE","7359","7359","$0.00","The creation, management, and access of data is key to the advancement of the state-of-the-art in computer science. Currently many research and educational activities in social network, web mining, multimedia retrieval, and high performance computing, just to name a few, need to involve big data. However, existing infrastructure of the Computer Science Department at Texas State University falls far behind its increasing research and educational needs.<br/><br/>This project builds a shared high-performance data center, providing the fundamental facilities to collect, process, and manage large volumes of data. The new data center supplies the necessary computational power and storage to support the exploration and development of new research and technologies within and beyond the department in the fields of computer vision, wireless network, information retrieval, data mining, human computer interaction, software engineering, high performance computing, and security. The data center also supports quality undergraduate and graduate student research experience, enables the integration of research and education, encourages inter-departmental and cross-university collaborations, and promotes the Ph.D. program development in the Computer Science Department. The developed infrastructure will significantly enhance the current research and educational capabilities of the department, university, and community.<br/>"
"1320229","CGV: Small: A General Framework for Expressing, Navigating, and Querying Uncertainty in Data Analysis and Visualization Tasks","IIS","GRAPHICS & VISUALIZATION","09/01/2013","06/23/2014","Kwan-Liu Ma","CA","University of California-Davis","Continuing grant","Maria Zemankova","08/31/2018","$498,196.00","","ma@cs.ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","CSE","7453","7453, 7923","$0.00","This project addresses fundamental challenges in incorporating and conveying uncertainty in the process of data analysis and visualization. In order to compute uncertainty, the project aims to develop a general model for uncertainty analysis that is independent of the visualization method and application domain. An important and novel aspect of the technical approach is the division of uncertainty into high and low conceptual levels, with separate but complementary methods of analysis for each level.  The ultimate goal is to allow assessment of uncertainty at the level of tasks and queries, aided by effective visualization. The project focuses on three principal research problems: (1) development of a hybrid probability-possibility uncertainty analysis framework for representing low-level computational uncertainty, along with methods for displaying this uncertainty in various visual modalities; (2) formulation of a fuzzy analysis for representing high-level, task-related uncertainty that handles human input and visual/perceptual uncertainty, while bridging the gap between low-level uncertainty and high-level uncertainty; and (3) investigation into ways to visually display the evolution of uncertainty in computation, enabling uncertainty navigation, in which exploration and modification of uncertainty can occur in the same context. The resulting framework is expected to effectively enable verifiable visualization of uncertainty in data analysis.<br/><br/>This project draws from many fields of research outside of visualization, including management of uncertainty, fuzzy logic, information theory, data analysis, simulation, computer vision, human-computer interaction, computer graphics and high performance computing and its potential impact extends to these areas and beyond. The ultimate goal of verifiable visualization is beneficial to visualization and visual analytics, but also facilitates the adoption of visualization in other fields, such as medical imaging, computational biology, and visual analytics to name a few. The project results will be disseminated to the visualization community and beyond through annual conferences, workshops, and tutorials, and also through the project website (http://vis.cs.ucdavis.edu/NSF/IIS1320229), which will include project status updates and deliverables such as images, videos, and prototype software. Complementing the proposed research is an educational agenda, consisting of integration of research results into teaching, arrangement of summer internships for participating students at the collaborating scientists' laboratories, and involvement of graduate and undergraduate students in research."
"1319598","HCC: Small: Head Activated Technology for Off-the-Shelf Mobile Devices","IIS","HCC-Human-Centered Computing","09/15/2013","09/09/2013","Kenneth Barner","DE","University of Delaware","Standard Grant","Ephraim Glinert","08/31/2017","$496,020.00","Jingyi Yu","barner@eecis.udel.edu","210 Hullihen Hall","Newark","DE","197160099","3028312136","CSE","7367","7367, 7923, 9150","$0.00","Mobile computing devices offer increasingly rich human-computer interaction opportunities through the use of new sensor technology such as multi-touch surfaces, microphones, and cameras.  These advances provide users with richer sets of interaction modalities and motivate new and novel human-centric interfaces.  Among the new input modalities, touch-based interaction is the most advanced and widely deployed, while speech is gaining increased attention.  However, even with recent breakthroughs in computer vision and depth and motion sensing technology, the imaging modality remains the least developed for mobile devices.  And yet image-based sensing (a) would permit richer interaction opportunities for a general user population and (b) holds special promise for individuals with disabilities who find touch-based interfaces very challenging to use.<br/> <br/>This project focuses on the development of Head-Activated Technology for off-the- shelf Mobile Devices (HAT-MD).  HAT-MD systems will employ the next-generation imaging technology that will be included in a wide array of mobile devices.  Advanced algorithms will detect and track user head and facial features, and map specific movements to specific interface controls and application actions.  HAT-MD is initially targeted towards individuals with physical disabilities, and will both (a) extend to mobile platforms the head control interaction techniques that are often employed on desktop computers and (b) broaden the set of head and face movements that can be employed.<br/><br/>Project deliverables include:  (a) HAT-MD Algorithms:  Next-generation imaging sensors will be used for detection, tracking, and 3D reconstruction.  Head position and facial features will be detected, tracked, and mapped to interface controls.  Computation will be distributed between the mobile devices and cloud computing services.  (b) HAT-MD Applications:  Head activated interfaces will be developed to address the challenges that individuals with physical disabilities have when using contemporary mobile platforms.  (c) HAT-MD Evaluations:  Rigorous human subject studies will evaluate the effectiveness and ease-of-use of the general HAT-MD methodology as well as specific interface and applications that are developed.  Algorithms will be benchmarked to current state-of-the-art detection, tracking, and reconstruction methods.<br/><br/>Broader Impacts:  The increased accessibility provided by the HAT-MD project will create new opportunities for people to interact with mobile devices, especially for individuals with physical disabilities who currently have limited independent control of such devices.  The project will also feature direct involvement by individuals with disabilities at the research, development, and evaluation phases in order to focus the intellectual development in a truly useful direction that will complement the broader impact."
"1263011","REU Site: Advances of Machine Learning in Theory and Applications (AMALTHEA)","IIS","RSCH EXPER FOR UNDERGRAD SITES","04/01/2013","03/31/2017","Georgios Anagnostopoulos","FL","Florida Institute of Technology","Standard Grant","Wendy Nilsen","03/31/2018","$360,000.00","Adrian Peter","georgio@fit.edu","150 W UNIVERSITY BLVD","MELBOURNE","FL","329016975","3216748000","CSE","1139","9250","$0.00","The Advances of MAchine Learning in THEory & Applications (AMALTHEA) REU Site aims to provide top quality educational experiences to a diverse community of undergraduate students through research participation in the area of Machine Learning (ML). The relevance and importance of ML is not limited to specialized technological innovations, as it was in the past. Nowadays, it also increasingly influences everyday life through its contributions to applications such as voice/face recognition, credit fraud detection, intelligent recommendation systems and many others. Furthermore, ML is inherently multi-disciplinary as it draws from advances in disciplines such as computing, statistics, mathematics, physics, biology and engineering, to a name a few major ones. The project's thrust area is the theory of ML and how it can be integrated and applied to important real-life problems, thus exposing participants to both theory and applications. AMALTHEA involves 10 undergraduate students per year from a broad spectrum of disciplines for 10 weeks in the summer. These participants perform supervised research, whose results are going to impact the field of ML itself, as well as how ML is applied in other scientific disciplines. For this purpose, the faculty mentors have ample expertise in ML and past experience of effectively engaging undergraduate students into state-of-the-art research.<br/><br/>Over its lifespan, the project will directly impact a diverse group of 30 motivated students, the majority of which may not have access to such research participation opportunities otherwise. The participants will be exposed to cutting-edge ML research, as well as professional development activities, such as technical seminars and career-related workshops. Furthermore, AMALTHEA aims to involve overall 10 graduate students in undergraduate teaching and mentoring activities during the summer experiences. Finally, the project's research and education endeavors are supported by its advisory board, which consists of seasoned educators, ML industry practitioners and researchers. Research results will be published in interdisciplinary conferences, and, potentially, technical journals.<br/><br/>The REU Site web site (http://www.amalthea-reu.org) provides additional information for students as well as educators and researchers in machine learning."
"1307178","Statistical Analysis of High Dimensional Manifold Data","DMS","STATISTICS","07/01/2013","05/29/2015","Sungkyu Jung","PA","University of Pittsburgh","Continuing Grant","Gabor Szekely","06/30/2016","$110,000.00","","sungkyu@pitt.edu","300 Murdoch Building","Pittsburgh","PA","152603203","4126247400","MPS","1269","","$0.00","Manifold-valued data appear frequently in shape and image analysis, computer vision, biomechanics and many others. Medical imaging data in studies of the variability of human organ shapes lie on a fairly high dimensional nonlinear manifolds, where the challenge is two-fold: high dimensionality with a low sample size (data are expensive to gather) and naturally imposed non-Euclidean geometry. The proposed research aims to answer scientific questions arising in object shape analysis and to provide solid mathematical basis for more complex problems. First, the investigator takes and extends the strategy of backward dimension reduction with regularization framework. Sparse representation of shape and directional data are proposed and their properties are studied. In the regression context, polynomial regression for manifold-valued response to model and test non-geodesic trends is proposed. An extension of local polynomial modeling is also considered. The investigator also explores efficient computational methods. <br/><br/>The study of object shape is crucial for understanding the population of human anatomical objects and revealing the interplay between biomarkers/clinical outcomes and object shape variations. Due to the advanced technology, the modern object shape data become big and complex, but conventional methods lack considerations on the special geometric structure of the data types. This research project aims to provide new statistical methodologies for exploratory and confirmatory analysis of the large-scale non-standard data types, including the object shapes. The project will also produce statistical tools, which may be applied in many fields including biomechanics, computer vision, medical studies and biological sciences."
"1252318","CAREER: Annotating the Microbiome using Machine Learning Methods","IIS","Info Integration & Informatics","03/01/2013","01/31/2013","Huzefa Rangwala","VA","George Mason University","Standard Grant","Sylvia Spengler","09/30/2019","$550,000.00","","hrangwal@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","CSE","7364","1045, 1187","$0.00","This project addresses an important challenge of developing sophisticated and novel machine learning techniques for complex real-world problems. New technologies allow us to determine the genomes  of organisms co-existing within various ecosystems ranging from ocean, soil and human-body. Several researchers have embarked on studying the pathogenic role played by the microbiome, defined as the collection of microbial organisms within the human body, with respect to human health and disease conditions. <br/><br/>The research activities in this CAREER project will develop approaches for the  identification of taxonomy, function and metabolic potential from the collective genomes samples.  A key contribution will be the  development of  multi-task learning approaches that  combine  information across multiple hierarchical databases associated with the annotation problems. During research, the PI will investigate the best ways to capture the underlying hierarchical structure, prevalent within different annotation databases. The rationale underlying this proposed  research is that there is a wealth of complementary information that exists across several manually curated biological databases.  Associating microbiome with phenotype requires integration of  various  high-throughput omic data sources (genomic, metabolic, proteomic)  that may  not be uniformly available across all samples.  The PI will develop data fusion classifiers within the multi-task learning  paradigm to integrate  heterogeneous, incomplete data sources for predicting phenotypes. This project will lead to the following  key contributions:   (i) Improved metagenome annotation models by  integration of  multiple  prediction  tasks and associated  databases.   (ii) Incorporation of  hierarchical information within regularized multi-task learning. (iii) Integration of  diverse and  incomplete information sources. (iv) Scalable algorithms that use hash based feature representations and improve the learning rates.<br/><br/>This  project is interdisciplinary and spans the fields of machine learning,  bioinformatics, metagenomics,  microbiology and environmental ecology.  This project will foster the the synergy between teaching and research by providing an environment for all students to develop intellectually and professionally.   The project integrates the research with an education plan focused on mentoring of  high school, undergraduate and graduate students, curriculum development and laboratory  visits.  Planned activities include training of inter-disciplinary researchers, integration of microbiome analysis related projects within the classes, curriculum enhancement and implementation of new learning strategies. Open source software and tools will be developed as part of this project, that will enhance scientific understanding and discovery amongst a broad and diverse group of researchers."
"1343756","NUE: Nanotechnology LINK: An integrated approach for nanotechnology education: End-of-life management of nanomaterial-containing wastes","EEC","NANOTECHNOLOGY UNDERGRAD EDUCA","09/01/2013","08/27/2013","Nicole Berge","SC","University of South Carolina at Columbia","Standard Grant","Mary Poats","08/31/2016","$197,943.00","Charles Pierce, Juan Caicedo, Fabio Matta, Yeomin Yoon","berge@engr.sc.edu","Sponsored Awards Management","COLUMBIA","SC","292080001","8037777093","ENG","7219","113E, 9150, 9178","$0.00","This NUE in Engineering program entitled, ""NUE: Nanotechnology LINK: An integrated approach for nanotechnology education: End of life management of nanomaterial-containing wastes"", at the University of South Carolina (USC), under the direction of Dr. Nicole Berge, has as its goal to develop an integrated undergraduate nanotechnology theme within the currently existing Civil and Environmental Engineering (CEE) curriculum at USC that focuses on the environmental implications associated with the end-of-life management of nanomaterial-containing products, materials, and nanomaterial manufacturing waste streams to produce a more informed and competitive CEE workforce.<br/><br/>To accomplish this goal, the project team plans to develop nanotechnology problem-based hands-on modules following a pedagogical approach referred to as Environments for Fostering Effective Critical Thinking (EFFECTs). EFFECTs use student-centered learning strategies to promote deep learning, enhance conceptual understanding, and stimulate growth in critical thinking skills. The EFFECTs approach has become institutionalized within the USC CEE curriculum. However, even though EFFECTs have been developed and implemented in a significant number of courses, these EFFECTs are independent and unrelated. Dr. Berge and her team propose to create an EFFECT LINK (Learning Integration of New Knowledge) for teaching and learning of nanotechnology content across the curriculum, which is referred to as the Nanotechnology LINK. As part of this integrated approach, students will assemble nanotechnology-themed electronic portfolios, building content knowledge as they advance through a sequence of courses. In addition, students will have the option of participating in an undergraduate nanotechnology-based research experience and graduate with a Leadership Distinction in Research.<br/><br/>The development and implementation of the proposed Nanotechnology LINK framework will result in the generation and refinement of a transformative educational approach of linking student learning across a curriculum to enhance student learning of other techniques/concepts. In addition, knowledge associated with the environmental implications associated with the end-of-life management of nanomaterial-containing wastes will be advanced. To date, there has been little research in this area. This work will also result in better-prepared civil and environmental engineers."
"1245410","Collaborative Research: Distributing the load: using the Structure-Behavior-Function framework to inform instructional design in introductory biology","DUE","S-STEM-Schlr Sci Tech Eng&Math, TUES-Type 1 Project","07/01/2013","06/17/2013","Elena Bray Speth","MO","Saint Louis University","Standard Grant","Ellen Carpenter","06/30/2017","$158,571.00","","elena.brayspeth@slu.edu","221 N. Grand Blvd.","St Louis","MO","631032006","3149773925","EHR","1536, 7513","9178, SMET","$0.00","This collaborative project transforms learning in introductory biology courses through a technology-supported, active-learning pedagogy that is informed by theory and evidence about learning systems. Specifically, this project overcomes students' problems in understanding complex biological systems and distributes learning in a way that actively engages students in and out of class. This project evaluates the effectiveness of the pedagogy, provides research on how introductory biology students articulate complex processes within and across biological systems, and investigates development of reflective and deep learning approaches.<br/><br/>The intellectual merit of this project results from a technology-rich approach, allowing instructors to communicate selected content outside of class and replacing traditional lectures with promising active learning strategies in class. An approach adapted from engineering that analyzes systems in terms of their Structure, Behavior, and Function is the theoretical framework employed to facilitate learning and understanding of biological systems. <br/><br/>This project creates broader impacts by building and strengthening community within and across institutions. At the collaborating institutions alone, several hundred students are expected to benefit from the enhanced instructional framework. Dissemination of practices and curricular materials should impact students at multiple institutions and levels in the biology curriculum. Research findings are expected to be disseminated as oral and poster communication at local and national meetings of professional societies. Further, outcomes of this research are also expected to be disseminated via manuscripts targeted for science education journals. Contributions to the STEM education research literature should yield insights to the broader academic and science education research communities."
"1245362","Collaborative Research: Distributing the load: using the Structure-Behavior-Function framework to inform instructional design in introductory biology","DUE","S-STEM-Schlr Sci Tech Eng&Math, TUES-Type 1 Project","07/01/2013","06/17/2013","Jennifer Momsen","ND","North Dakota State University Fargo","Standard Grant","Ellen Carpenter","06/30/2017","$40,997.00","","jennifer.momsen@ndsu.edu","Dept 4000 - PO Box 6050","FARGO","ND","581086050","7012318045","EHR","1536, 7513","9150, 9178, SMET","$0.00","This collaborative project transforms learning in introductory biology courses through a<br/>technology-supported, active-learning pedagogy that is informed by theory and<br/>evidence about learning systems. Specifically, this project overcomes students'<br/>problems in understanding complex biological systems and distributes learning in a way<br/>that actively engages students in and out of class. This project evaluates the<br/>effectiveness of the pedagogy, provides research on how introductory biology students<br/>articulate complex processes within and across biological systems, and investigates<br/>development of reflective and deep learning approaches.<br/><br/>The intellectual merit of this project results from a technology-rich approach, allowing<br/>instructors to communicate selected content outside of class and replacing traditional<br/>lectures with promising active learning strategies in class. An approach adapted from<br/>engineering that analyzes systems in terms of their Structure, Behavior, and Function is<br/>the theoretical framework employed to facilitate learning and understanding of biological<br/>systems.<br/><br/>This project creates broader impacts by building and strengthening community within<br/>and across institutions. At the collaborating institutions alone, several hundred students<br/>are expected to benefit from the enhanced instructional framework. Dissemination of<br/>practices and curricular materials should impact students at multiple institutions and<br/>levels in the biology curriculum. Research findings are expected to be disseminated as<br/>oral and poster communication at local and national meetings of professional societies.<br/>Further, outcomes of this research are also expected to be disseminated via<br/>manuscripts targeted for science education journals. Contributions to the STEM<br/>education research literature should yield insights to the broader academic and science<br/>education research communities."
"1244807","Vertical Integration of Raman Spectroscopy into the Chemistry Curriculum","DUE","S-STEM-Schlr Sci Tech Eng&Math, TUES-Type 1 Project","02/01/2013","01/17/2013","Evonne Rezler","FL","Florida Atlantic University","Standard Grant","Tom Higgins","01/31/2017","$199,803.00","Jerome Haky, Andrew Terentis","erezler@fau.edu","777 GLADES RD","BOCA RATON","FL","334316424","5612970777","EHR","1536, 7513","9178, SMET","$0.00","The faculty in the Chemistry and Biochemistry Department at Florida Atlantic University are vertically integrating Raman spectroscopy experiments and experiences into the undergraduate chemistry curriculum.  The project is addressing key, course-specific, student learning outcomes as well as fundamental, anchoring concepts ('big ideas').  The project is accomplishing this by acquiring a sophisticated Raman spectrometer and is using this instrument to introduce and reinforce increasingly more complex chemistry concepts as a student progresses through the course sequence.  This project is part of a restructured chemistry curriculum that includes a 'spectroscopy-to-learn' approach.   Raman spectroscopy is aiding students' deep learning of molecular structure, quantitative analysis, the elucidation of reaction mechanisms, and how spectroscopy aids research.<br/><br/>The project is demonstrating and establishing the methods by which a sophisticated spectroscopic instrument can be utilized throughout the entire chemistry curriculum to promote understanding of anchoring concepts at all levels.  Students are engaging with increasingly more advanced Raman instrument-based experiments as they progress through the chemistry curriculum enabling them to: (1) connect the theory taught in lectures with authentic research and industry practice; (2) become more active and responsible participants in their own learning; and (3) reinforce and extend their understanding of concepts previously learned.  Independent consultants are guiding the evaluation of the effectiveness of this effort aimed at the over-arching ideas of improving student understanding of chemical principles and their attitudes towards learning.<br/><br/>The student body of the FAU campus ensures a diverse population is experiencing a hands-on encounter with state-of-the-art instrumentation.  The investigators are disseminating their work through presentations and a dedicated web site.  In addition, an illustrated handbook featuring the novel undergraduate Raman experiences and experiments is being prepared for publication.  An instructor's manual that contains both pre- and post-measures of learning outcomes is also being developed."
"1350763","EAGER: Towards Human Centered Visual Understanding: Exploring the Intended and Interpreted Meaning of Images in Social Multimedia","IIS","ROBUST INTELLIGENCE","09/15/2013","06/09/2017","Gang Hua","NJ","Stevens Institute of Technology","Standard Grant","Jie Yang","08/31/2018","$199,170.00","","gang.hua@stevens.edu","CASTLE POINT ON HUDSON","HOBOKEN","NJ","070305991","2012168762","CSE","7495","7495, 7916","$0.00","This project explores a new direction in computer vision, which is to model the context dependent visual semantics associated with images in social multimedia. The context dependent visual semantics, e.g., the intended and perceived sentiment of an image in social multimedia, are dynamically formed based on the various contextual information associated with it. This is different from the static visual semantics that conventional computer vision research focused on studying, such as the object category presented in the image.<br/><br/>The project develops a set of new networked and context aware probabilistic latent semantic models, which integrate situated contextual information into visual content analysis for modeling context dependent visual semantics. The research team is verifying two hypotheses: 1) the context dependent semantics needs to be holistically modeled and jointly inferred from a collection of related images; and 2) related context dependent visual semantics, such as intended and perceived meaning of an image, also needs to be jointly modeled for more robust recognition.<br/><br/>The project is integrated with education through training graduate and undergraduate students. The outcome of the research can be applied to many domains, such as targeted online advertisements; open source information analysis and social event prediction; and social multimedia security."
"1307505","Topological methods for Azumaya algebras","DMS","ALGEBRA,NUMBER THEORY,AND COM, TOPOLOGY","08/01/2013","07/16/2013","David Antieau","CA","University of California-Los Angeles","Standard Grant","Linda Chen","10/31/2013","$101,279.00","","benjamin.antieau@gmail.com","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","MPS","1264, 1267","","$0.00","The PI will engage in several projects at the border of algebraic geometry and algebraic topology. Three projects aim to use topological methods to understand the Brauer group, Azumaya algebras, and more generally torsors on schemes. (1) The PI will study the extent to which the foundational results of Jackowski, McClure, and Oliver on maps between classifying spaces of complex algebraic groups can be extended to finite approximations to these classifying spaces. Progress on this problem will enable the solution of a host of problems about when torsors for complex algebraic groups extend from the generic point of a scheme to the entire scheme. In low dimensions, early progress on this problem has been used by the PI and Ben Williams to settle an old question of Auslander and Goldman on the existence of Azumaya maximal orders in unramified division algebras, where it transpires that there are purely topological obstructions to the existence of these Azumaya maximal orders. (2) The PI will work toward computing the Chow groups and singular cohomology of the classifying spaces of special linear groups by various central subgroups. This has been done in special cases by Vezzosi and Vistoli. However, greater generality is needed for most applications. These Chow groups are fundamental objects in algebraic geometry, controlling the characteristic classes associated to certain torsors of fundamental importance in the study of the Brauer group. The computations will be directly useful to the first project, and to the following project. (3) The PI and Ben Williams previously formulated the topological period-index problem and established first results. They will continue this study, especially as it relates to the algebraic period-index conjecture. In particular, their results in low dimensions suggest a method for disproving the period-index conjecture, which would be a fundamental advance. Following this idea to its conclusion is the major aspiration of the first set of projects. A fourth project aims to continue to build a bridge between higher category theory and classical algebraic geometry, bringing the formidable techniques of the former to bear on various questions in the arithmetic of derived categories. For example, the PI is developing a toolbox using higher category theory that will allow a purely derived-category proof of Panin's computations of the K-theory of projective homogeneous spaces, once the existence of certain exceptional objects on the split forms of these spaces is known.<br/><br/>The PI proposes work in algebraic geometry and algebraic topology, two areas of modern mathematics. Algebraic geometry is an ancient subject with many connections to real-world problems. Its goal is to understand the geometry of solutions sets of polynomial equations, equations of central importance in various disciplines, such as theoretical physics, cryptography, and the modeling of dynamical systems like weather. Algebraic topology on the other hand developed more recently, in the 19th century, and aims to study a general notion of shape, less rigid than the idea of shape studied in geometry. It has found striking applications in the last decade, for instance to the analysis of large data sets that occur in computer vision and cancer research, frequently finding patterns that more traditional methods of data analysis fail to find. The proposal of the PI will bring the considerable machinery and insight of algebraic topology to bear on several questions in algebraic geometry which have been identified by the community as among the most important."
"1247658","BIGDATA: Mid-Scale: DA: Distribution-based machine learning for high dimensional datasets","IIS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS, Big Data Science &Engineering","01/01/2013","08/25/2013","Aarti Singh","PA","Carnegie-Mellon University","Continuing grant","Sylvia Spengler","12/31/2016","$1,000,000.00","Timothy Verstynen, Barnabas Poczos","aartisingh@cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","1253, 1269, 8083","7433, 7924, 8083","$0.00","Many applications call for representation and analysis of 'distributional' data sets where each data point is a collection of samples from a high dimensional distribution (as opposed to valuations of a typically vector valued random variable). In this setting, each data point can be modeled by a collection of distributions, one for each measured attribute. A concrete example of distributional data arises in the context of brain connectivity mapping.  The human brain contains around a hundred billion neurons with several hundred trillion physical connections. Neuroimaging approaches, like Diffusion Spectrum Imaging attempt to visualize the underlying anatomical architecture of neural pathways by creating 3D probability distributions of water diffusion along nerve fiber bundles, called orientation distribution functions. <br/><br/>The project aims to develop new statistical and algorithmic approaches to natural generalizations of a class of standard machine learning problems (where multi-dimensional vector valued data points are replaced by distributions), including techniques for measuring distances and inner products between distributional data points, estimating variants of entropy, mutual information, conditional mutual information, clustering distributional data, constructing low-dimensional embeddings of distributional data, and learning classifiers and function approximators from distributional data. The resulting methods will be evaluated on large diffusion scan imaging data sets (where the data point for each patient  consists of 500,000 distributions). <br/><br/>The novel machine learning approaches for descriptive and predictive modeling of distributional data resulting from this project are expected to benefit other scientific fields where data points can be naturally modeled by sets of distributions, which is a common situation in physics, psychology, economics, epidemiology, medicine, and social network-analysis.  New distributional data set to be obtained at CMU to augment the data available from NTU are likely to allow other research groups to engage in research on big data analytics from distributional data. Release of open source software,  video tutorials, research-training of graduate students contribute to the broader impacts of the project. Additional information about the project can be found at: http://www.autonlab.org/autonweb/20928.html."
"1358832","Topological methods for Azumaya algebras","DMS","ALGEBRA,NUMBER THEORY,AND COM, TOPOLOGY","07/01/2013","09/04/2013","David Antieau","WA","University of Washington","Standard Grant","Joanna Kania-Bartoszynska","11/30/2014","$101,279.00","","benjamin.antieau@gmail.com","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","MPS","1264, 1267","","$0.00","The PI will engage in several projects at the border of algebraic geometry and algebraic topology. Three projects aim to use topological methods to understand the Brauer group, Azumaya algebras, and more generally torsors on schemes. (1) The PI will study the extent to which the foundational results of Jackowski, McClure, and Oliver on maps between classifying spaces of complex algebraic groups can be extended to finite approximations to these classifying spaces. Progress on this problem will enable the solution of a host of problems about when torsors for complex algebraic groups extend from the generic point of a scheme to the entire scheme. In low dimensions, early progress on this problem has been used by the PI and Ben Williams to settle an old question of Auslander and Goldman on the existence of Azumaya maximal orders in unramified division algebras, where it transpires that there are purely topological obstructions to the existence of these Azumaya maximal orders. (2) The PI will work toward computing the Chow groups and singular cohomology of the classifying spaces of special linear groups by various central subgroups. This has been done in special cases by Vezzosi and Vistoli. However, greater generality is needed for most applications. These Chow groups are fundamental objects in algebraic geometry, controlling the characteristic classes associated to certain torsors of fundamental importance in the study of the Brauer group. The computations will be directly useful to the first project, and to the following project. (3) The PI and Ben Williams previously formulated the topological period-index problem and established first results. They will continue this study, especially as it relates to the algebraic period-index conjecture. In particular, their results in low dimensions suggest a method for disproving the period-index conjecture, which would be a fundamental advance. Following this idea to its conclusion is the major aspiration of the first set of projects. A fourth project aims to continue to build a bridge between higher category theory and classical algebraic geometry, bringing the formidable techniques of the former to bear on various questions in the arithmetic of derived categories. For example, the PI is developing a toolbox using higher category theory that will allow a purely derived-category proof of Panin's computations of the K-theory of projective homogeneous spaces, once the existence of certain exceptional objects on the split forms of these spaces is known.<br/><br/>The PI proposes work in algebraic geometry and algebraic topology, two areas of modern mathematics. Algebraic geometry is an ancient subject with many connections to real-world problems. Its goal is to understand the geometry of solutions sets of polynomial equations, equations of central importance in various disciplines, such as theoretical physics, cryptography, and the modeling of dynamical systems like weather. Algebraic topology on the other hand developed more recently, in the 19th century, and aims to study a general notion of shape, less rigid than the idea of shape studied in geometry. It has found striking applications in the last decade, for instance to the analysis of large data sets that occur in computer vision and cancer research, frequently finding patterns that more traditional methods of data analysis fail to find. The proposal of the PI will bring the considerable machinery and insight of algebraic topology to bear on several questions in algebraic geometry which have been identified by the community as among the most important."
"1320752","CSR: Small: Profiling and Optimizing Embedded Software for Soft Real-Time Behavior and Responsiveness","CNS","Computer Systems Research (CSR","10/01/2013","09/20/2013","Santosh Pande","GA","Georgia Tech Research Corporation","Standard Grant","M. Mimi McClure","09/30/2017","$477,724.00","","santosh@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7354","7354, 7923","$0.00","This project aims to develop a framework for performance characterization, representation, optimization and debugging for the emerging domain of soft real time computing. Key elements include a new metric, called ""variance"", that characterizes the timing properties of such workloads, and a ""variant characterization graph"" (VCG) that represents the relationships among the variance contributing functions in a software system. The VCG is subjected to a combination of static analysis, dynamic profiling and statistical analysis. The results of these analyses are applied to the problems of debugging performance bottlenecks, and dynamic performance optimization making use of predictive methods and contextual information obtained at run time. <br/><br/>Interactive, response-time sensitive applications such as video games, computer vision, and multi-media encoder/decoders are an important, growing class of emerging applications on mobile devices such as the smart-phones. A proliferation of powerful new devices is driving up the scale and pervasiveness of applications. The diversity of new devices and the number of kinds of resources present in them, accompanied by a market-driven demand for shorter application development cycles, are making performance tuning more challenging and more essential. Success of this project would provide a practical conceptual backbone for converting the optimization of such soft real time systems from an art to a science. Transition of the results of this research to practice is furthered through open source distribution of software tools implementing and applying the VCG-based analyses, and an on-line graduate course titled ""Optimizations for Interactive Embedded Software""."
"1251031","BIGDATA: Small: DA: Collaborative Research: Real Time Observation Analysis for Healthcare Applications via Automatic Adaptation to Hardware Limitations","IIS","Big Data Science &Engineering","07/01/2013","06/25/2013","Rong Jin","MI","Michigan State University","Standard Grant","Sylvia J. Spengler","05/31/2016","$270,453.00","","rongjin@cse.msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","CSE","8083","7433, 7923, 8083","$0.00","This research seeks to develop novel machine learning algorithms that enable real-time video and sensor data analysis on large data streams given limited computational resources. The work focuses on healthcare as an application domain where real-time video analysis can prevent user-errors in operating medical devices or provide immediate alerts to caregivers about dangerous situations.  The research will develop algorithms to automatically adapt data analysis approaches to maximize accuracy of analysis within a short time period despite limited available computing resources. Today's healthcare environment is significantly more technologically sophisticated than ever before. Many medical devices are now frequently used in patient's homes, ranging from simple equipment such as canes and wheelchairs to sophisticated items such as glucose meters, ambulatory infusion pumps and laptop-sized ventilators. The rapidly growing home health industry raises new safety concerns about devices being used inappropriately in the home setting. The proposed research is designed to reduce medical device related use-errors by developing computational algorithms that perform real-time video analysis and alert the patient or caregiver when medical devices are not used appropriately. The real-time video and sensor data analysis is also critical to the healthcare systems that monitor the activities of the elderly or those with disabilities in order to allow a caregiver to react immediately to an incident. <br/><br/>New machine learning theories and algorithms will automatically adapt to hardware limitations, with the aim to learn from a large number of training examples, a prediction function that (i) is sufficiently accurate in making effective predictions and (ii) can be run efficiently on a specified computer system to deliver time critical results. Three types of prediction models are studied to address the problem of automatic hardware adaptation, including a vector-based model, a matrix-based model, and a prediction model based on a function from a Reproducing Kernel Hilbert Space (RKHS).  A general framework and multiple optimization techniques are being developed to learn accurate prediction models that match limited memory and computational capacity. The new learning algorithms will be evaluated in several medical scenarios through real-time prediction of a patient's activities from observations in the large video archives collected by several healthcare related projects.  The intellectual merit of the proposed work is in bridging the gap between the high complexity of a prediction model and limited computational resources, a scenario that is encountered in many application domains besides healthcare. The proposed research in machine learning algorithms and theories will make it possible to run complicated prediction algorithms on big data within the limitation of a given computing infrastructure. The developed techniques for automatic hardware adaptation will be applied to a large dataset of continuous video and sensor recordings for medically-critical activity recognition.  The project's broader impacts include providing medical experts with algorithms and tools supporting novel approaches to analyzing observational data in their quest to recognize and characterize human behavior. Surveillance systems with continuous observations will be able to categorize salient events with co-located, limited hardware. Researchers with complex data from continuous streams will be able to explore their domains with greater accuracy within constrained time using their available computing resources. Similarly, large archives can be exploited as rapidly as possible with limited hardware."
"1361062","EXP: Developing a tutor to guide students as they invent deep principles with contrasting cases","IIS","Cyberlearn & Future Learn Tech","09/15/2013","09/30/2013","Catherine Chase","NY","Teachers College, Columbia University","Standard Grant","John Cherniavsky","08/31/2017","$548,676.00","","chase@tc.edu","525 West 120th Street","New York","NY","100276625","2126783000","CSE","8020","8045, 8841","$0.00","This is a project to develop a tutor that will teach the skills of innovation through invention.  The Invention Tutor will simulate the guidance of a well-trained inquiry teacher, who asks critical questions and gives feedback just at the right time, to push students' thinking forward. By tracking student trajectories through the Invention process, the Invention Tutor will respond adaptively to student moves. Most importantly, the tutor will eliminate the constraint of needing a large teacher: student ratio to implement the Invention method successfully. This work will have broader impact by scaling up a successful instructional technique, with the ultimate goal of enhancing deep learning and transfer in science domains, for both  high and low-achieving student populations. The project also has significant intellectual merit. The research will identify effective forms of support for Invention tasks and expand our understanding of the Invention process itself. More generally, findings will inform the field's understanding of the process of discovery and how best to guide it. In addition, the Invention Tutor will represent a new kind of tutor that guides open-ended discovery tasks, as opposed to the more typical tutor that guides students to solve well-defined problems.<br/><br/>Innovation is an important 21st century skill. The Invention tutor strives to teach that skill without the high teacher:student ratio typically required when teaching such soft skills. Most intelligent tutoring systems rely on a fixed set of rules that guide how the tutor interacts with students that are derived from a study of the tutor's discipline and a cognitive task analysis of what is required to learn the subject. The Invention tutor has no such underlying cognitive task analysis and the challenge of building the tutor will lie in uncovering the rules that guide tutor-student interactions and building upon previous tutor-student successful interactions. The intelligent tutoring system will be disseminated through free web access and the data collected in the project will be available for secondary analyses."
"1262292","Collaborative Proposal: ABI Innovation: Rapid, Interactive, Visual Mining of Biological Motion","DBI","ADVANCES IN BIO INFORMATICS","08/01/2013","06/23/2015","Anna Dornhaus","AZ","University of Arizona","Continuing Grant","Peter McCartney","07/31/2016","$279,288.00","Jeremiah Hackett","dornhaus@email.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","BIO","1165","","$0.00","Across the biological sciences, high-quality, high frame-rate images and video are widely used in analysis of sub-cellular organelles to individual organisms. The use of video has become so pervasive, that many research teams accumulate much more data than can be reasonably analyzed by current methods. This project will develop algorithms to allow the rapid and affordable mining of very large video sets for motion behaviors of interest in the study of the collective behavior and emergent properties of complex systems of organisms. This work is motivated by the needs of domain experts in various areas of biological motion analysis and incorporates research in computer vision, image processing, user interfaces, and visualization. A key part of the project is a fast, semi-automated biological object tracker that leverages the computational power of off-the-shelf hardware and incorporates simple user interactions to dramatically improve tracking accuracy in the types challenging cases that frequently arise in biological image analysis. Domain experts will be able to search motion databases through visual query by selecting an example motion of interest. The system will be evaluated on three biological research applications: honeybee behavior, ant colony networks, and cell motion analysis.<br/> <br/>Groups of organisms can display a rich and sophisticated behavioral repertoire and are able to make complex collective decisions. Studying these interactions provides insight into collective decision-making, adaptive networks, and division-of-labor. Understanding these complex interactions over large time scales requires new methods of data analysis that this project will address with an interdisciplinary collaboration between biologists and computer scientists. The outreach components of this work include demonstrations and explanations of the complex behavior of groups of organisms and how these behaviors can be studied using computing.   The system is intended to be widely-applicable and will be disseminated to other researchers by making the software available for download online."
"1262472","Collaborative Proposal: ABI Innovation: Rapid, Interactive, Visual Mining of Biological Motion","DBI","ADVANCES IN BIO INFORMATICS","08/01/2013","06/11/2015","Min Shin","NC","University of North Carolina at Charlotte","Continuing grant","Peter H. McCartney","07/31/2017","$743,426.00","Richard Souvenir, Stanley Schneider","mcshin@uncc.edu","9201 University City Boulevard","CHARLOTTE","NC","282230001","7046871888","BIO","1165","","$0.00","Across the biological sciences, high-quality, high frame-rate images and video are widely used in analysis of sub-cellular organelles to individual organisms. The use of video has become so pervasive, that many research teams accumulate much more data than can be reasonably analyzed by current methods. This project will develop algorithms to allow the rapid and affordable mining of very large video sets for motion behaviors of interest in the study of the collective behavior and emergent properties of complex systems of organisms. This work is motivated by the needs of domain experts in various areas of biological motion analysis and incorporates research in computer vision, image processing, user interfaces, and visualization. A key part of the project is a fast, semi-automated biological object tracker that leverages the computational power of off-the-shelf hardware and incorporates simple user interactions to dramatically improve tracking accuracy in the types challenging cases that frequently arise in biological image analysis. Domain experts will be able to search motion databases through visual query by selecting an example motion of interest. The system will be evaluated on three biological research applications: honeybee behavior, ant colony networks, and cell motion analysis.<br/> <br/>Groups of organisms can display a rich and sophisticated behavioral repertoire and are able to make complex collective decisions. Studying these interactions provides insight into collective decision-making, adaptive networks, and division-of-labor. Understanding these complex interactions over large time scales requires new methods of data analysis that this project will address with an interdisciplinary collaboration between biologists and computer scientists. The outreach components of this work include demonstrations and explanations of the complex behavior of groups of organisms and how these behaviors can be studied using computing.   The system is intended to be widely-applicable and will be disseminated to other researchers by making the software available for download online."
"1248809","SBIR Phase I: Autonomous 3D Scanner for Building Interiors and Exteriors","IIP","SMALL BUSINESS PHASE I","01/01/2013","11/08/2012","Oskar Skrinjar","GA","Scientific Imaging and Visualization, LLC","Standard Grant","Glenn H. Larsen","06/30/2013","$146,716.00","","oskar@scientificiv.com","1996 Wellesley Trace","Atlanta","GA","303383088","4048632371","ENG","5371","5371, 8034, 8039","$0.00","The innovation of this project is the development of a system that can autonomously scan building interiors and exteriors and construct corresponding detailed, photo-realistic 3D models with minimal interaction of an operator that does not need to have specialized skills. The idea is to use a small unmanned aerial vehicle (UAV) equipped with a wireless camera and a laser line that can almost fully autonomously fly inside and around the building, video record all the interior and exterior structures, and send the video and other recorded information to the base station where the 3D model is constructed using computer vision techniques. The proposed system requires the development of two innovative technologies: (1) automated construction of texture-mapped 3D models of structures video recorded from a flying vehicle, and (2) autonomous flying of an UAV inside and around a building based on the recorded video and range information. While 3D scanning is an existing technology and small UAVs have already been developed, this proposal will combine these technologies and enhance their capabilities to deliver new value. In addition, computer, smartphone and tablet software for interactive visualization and exploration of the constructed 3D models will be developed.<br/><br/>The broader/commercial impact of this project is in the area of marketing of businesses and institutions, virtual reality applications as well as in rescue and law-enforcement applications. Having detailed, photo-realistic 3D models of building interiors and exteriors is useful in a number of applications. However, the existing approaches for the construction of such models are labor intensive and require highly trained people and expensive equipment, which make the entire process prohibitively expensive for most applications. The proposed system would reduce the cost of construction of such models by at least an order of magnitude compared to the existing solutions and would automate most of the process. The proposed system could be used to relatively inexpensively construct detailed, photo-realistic 3D models of interiors and exteriors of: real estate properties, restaurants, sport clubs, museums, historic buildings, architectural sites, touristic sites, university campuses, amusement parks, water parks, zoos, apartment complexes, resorts, hotels, hospitals, shopping malls, etc. The constructed 3D models can then be explored online using computers, smartphones and tablets. Furthermore, the system can be used for mapping or exploring buildings in situations where it is dangerous or impractical for people to enter, which is often the case in rescue and law-enforcement applications."
"1251187","BIGDATA: Small: DA: Collaborative Research: Real Time Observation Analysis for Healthcare Applications via Automatic Adaptation to Hardware Limitations","IIS","Big Data Science &Engineering","07/01/2013","06/25/2013","Alexander Hauptmann","PA","Carnegie-Mellon University","Standard Grant","Sylvia Spengler","06/30/2017","$459,901.00","","alex@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","8083","7433, 7923, 8083","$0.00","This research seeks to develop novel machine learning algorithms that enable real-time video and sensor data analysis on large data streams given limited computational resources. The work focuses on healthcare as an application domain where real-time video analysis can prevent user-errors in operating medical devices or provide immediate alerts to caregivers about dangerous situations.  The research will develop algorithms to automatically adapt data analysis approaches to maximize accuracy of analysis within a short time period despite limited available computing resources. Today's healthcare environment is significantly more technologically sophisticated than ever before. Many medical devices are now frequently used in patient's homes, ranging from simple equipment such as canes and wheelchairs to sophisticated items such as glucose meters, ambulatory infusion pumps and laptop-sized ventilators. The rapidly growing home health industry raises new safety concerns about devices being used inappropriately in the home setting. The proposed research is designed to reduce medical device related use-errors by developing computational algorithms that perform real-time video analysis and alert the patient or caregiver when medical devices are not used appropriately. The real-time video and sensor data analysis is also critical to the healthcare systems that monitor the activities of the elderly or those with disabilities in order to allow a caregiver to react immediately to an incident. <br/><br/>New machine learning theories and algorithms will automatically adapt to hardware limitations, with the aim to learn from a large number of training examples, a prediction function that (i) is sufficiently accurate in making effective predictions and (ii) can be run efficiently on a specified computer system to deliver time critical results. Three types of prediction models are studied to address the problem of automatic hardware adaptation, including a vector-based model, a matrix-based model, and a prediction model based on a function from a Reproducing Kernel Hilbert Space (RKHS).  A general framework and multiple optimization techniques are being developed to learn accurate prediction models that match limited memory and computational capacity. The new learning algorithms will be evaluated in several medical scenarios through real-time prediction of a patient's activities from observations in the large video archives collected by several healthcare related projects.  The intellectual merit of the proposed work is in bridging the gap between the high complexity of a prediction model and limited computational resources, a scenario that is encountered in many application domains besides healthcare. The proposed research in machine learning algorithms and theories will make it possible to run complicated prediction algorithms on big data within the limitation of a given computing infrastructure. The developed techniques for automatic hardware adaptation will be applied to a large dataset of continuous video and sensor recordings for medically-critical activity recognition.  The project's broader impacts include providing medical experts with algorithms and tools supporting novel approaches to analyzing observational data in their quest to recognize and characterize human behavior. Surveillance systems with continuous observations will be able to categorize salient events with co-located, limited hardware. Researchers with complex data from continuous streams will be able to explore their domains with greater accuracy within constrained time using their available computing resources. Similarly, large archives can be exploited as rapidly as possible with limited hardware."
"1316934","NRI: Small: Multirobot-Human Coordination for Visual Scene Understanding","IIS","International Research Collab, IIS Special Projects, NRI-National Robotics Initiati","09/01/2013","04/22/2019","Amit Roy Chowdhury","CA","University of California-Riverside","Standard Grant","David Miller","08/31/2019","$821,934.00","Anastasios Mourikis, Jay Farrell","amitrc@ece.ucr.edu","Research & Economic Development","RIVERSIDE","CA","925210217","9518275535","CSE","7298, 7484, 8013","5920, 7923, 8086","$0.00","The objective of this research is to enable the development of teams of robots, equipped with vision and other sensors, capable of working alongside humans in critical missions, such as search and rescue. Key requirements are situational awareness and coordinated action. The approach is to develop mathematical frameworks and algorithms to enable such a team of robots to coordinate their paths, share and analyze their sensor data, maintain communications, and interact effectively and safely with humans. The project brings together experts in computer vision, robotics, estimation theory and controls.<br/><br/>Intellectual Merit. Realizing the above goals will require advances in several inter-related domains. Specifically, the sensing, estimation, and trajectory control tasks must seamlessly integrate visual analysis with navigation and control strategies, as well as inputs from humans. Novel distributed estimation strategies must be developed to accommodate difficult and dynamic environments. Efficient human-robot coordination necessitates methodologies for joint exploration and mapping, identifying important visual information, and robots? operation at different levels of autonomy.<br/><br/>Broader Impact. The success of this project will be a major step towards the deployment of teams of robots to assist humans in dangerous and complex tasks like disaster response. Search-and-rescue experts will advise the team in developing a prototype system, and evaluating it in situations that mimic operational conditions. The developed software tools will be disseminated to other researchers so they can build on the results. Undergraduates from UCR's highly diverse student population will gain valuable experience working alongside graduate student researchers."
"1320252","EXP: Developing a tutor to guide students as they invent deep principles with contrasting cases","IIS","Cyberlearn & Future Learn Tech","09/15/2013","09/04/2013","Catherine Chase","PA","Carnegie-Mellon University","Standard Grant","John Cherniavsky","10/31/2013","$548,676.00","Vincent Aleven","chase@tc.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","8020","8045, 8841","$0.00","This is a project to develop a tutor that will teach the skills of innovation through invention.  The Invention Tutor will simulate the guidance of a well-trained inquiry teacher, who asks critical questions and gives feedback just at the right time, to push students' thinking forward. By tracking student trajectories through the Invention process, the Invention Tutor will respond adaptively to student moves. Most importantly, the tutor will eliminate the constraint of needing a large teacher: student ratio to implement the Invention method successfully. This work will have broader impact by scaling up a successful instructional technique, with the ultimate goal of enhancing deep learning and transfer in science domains, for both  high and low-achieving student populations. The project also has significant intellectual merit. The research will identify effective forms of support for Invention tasks and expand our understanding of the Invention process itself. More generally, findings will inform the field's understanding of the process of discovery and how best to guide it. In addition, the Invention Tutor will represent a new kind of tutor that guides open-ended discovery tasks, as opposed to the more typical tutor that guides students to solve well-defined problems.<br/><br/>Innovation is an important 21st century skill. The Invention tutor strives to teach that skill without the high teacher:student ratio typically required when teaching such soft skills. Most intelligent tutoring systems rely on a fixed set of rules that guide how the tutor interacts with students that are derived from a study of the tutor's discipline and a cognitive task analysis of what is required to learn the subject. The Invention tutor has no such underlying cognitive task analysis and the challenge of building the tutor will lie in uncovering the rules that guide tutor-student interactions and building upon previous tutor-student successful interactions. The intelligent tutoring system will be disseminated through free web access and the data collected in the project will be available for secondary analyses."
"1342379","BRIGE: Effect of Microstructure and Interface Layer on the Fracture and Thermal Behavior of Nanocomposites","EEC","ENG DIVERSITY ACTIVITIES, EPSCoR Co-Funding","10/01/2013","07/31/2013","Addis Kidane","SC","University of South Carolina at Columbia","Standard Grant","James Moore","09/30/2015","$174,986.00","","kidanea@cec.sc.edu","Sponsored Awards Management","COLUMBIA","SC","292080001","8037777093","ENG","7680, 9150","7715, 9150","$0.00","Technical Description the Project<br/><br/>This research program focusses on understanding and exploiting the effect of microstructure and interface layer on the fracture and thermal properties of particle reinforced polymer nanocomposites. Well characterized and functionalized nanocomposites will be fabricated from different particles and polymer systems. Both micro-scale and macro-scale fatigue crack growth experiments using microscopic stereo vision and digital image correlation will be conducted. Dynamic fracture experiment will also be conducted using the modified split Hokinson pressure bar technique.  The thermal conductivity of the nanocomposites as-fabricated and after subjected to mechanical loading will be measured experimentally and numerically. Finally, combining the experimental results with a modified micromechanics model, the effective thermal and fracture behavior of the nanocomposite as a function of individual component properties will be determined. <br/><br/>Non-Technical Description of the Project's Signficance<br/><br/>Understanding the fracture and thermal properties in nanocomposites as a function of individual material properties of the particles is essential and needed from small industry to large research centers for a better optimized design of nanocomposites.  The outcome of this study is important especially in structures and applications where the performance of the components depends mainly on the effectiveness of providing cooling for a device hot spot.  Furthermore, the findings can lead to the development of novel nanocomposite materials that can report damage and micro cracking by sensing changes in thermal properties before irreversible damage occurs. <br/><br/>Broadening Participation of Underrepresented Groups in Engineering<br/><br/>The project supports the effort to broaden the participation of underrepresented groups by directly involving students in the proposed research and through the outreach activities. As a part of continuous outreach effort, a set of laboratory demonstration events, using polymer and nanocomposites, will be organized for local high school students, of which one-third are from underrepresented groups.  The students will be exposed to and familiarized with the current nanotechnology, 3D computer vision and digital image correlation techniques. The PI will also work with Historically Black Colleges and Universities in South Carolina and nearby states, through the Center for Science Education at the USC as well as the SCienceLab program there. A collaboration with a faculty member at Benedict College has already been established and will be facilitated through this BRIGE award.<br/><br/>This research has been funded through the Broadening Participation Research Initiation Grants in Engineering solicitation, which is part of the Broadening Participation in Engineering Program of the Engineering Education and Centers Division.<br/><br/>The research is also funded through the Experimental Program to Stimulate Competitive Research (EPSCoR), which is part of the Office of International and Integrative Activities."
"1254071","CAREER: Estimation and Decisions in Graphical Models","IIS","Robust Intelligence","07/01/2013","06/22/2017","Alexander Ihler","CA","University of California-Irvine","Continuing Grant","Rebecca Hwa","06/30/2019","$442,040.00","","ihler@ics.uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","CSE","7495","1045, 7495","$0.00","Probabilistic graphical models are central to automated reasoning.  The past decade has seen significant progress on two basic reasoning tasks: combinatorial optimization (maximization or minimization) and marginalization (summation) tasks.  Maximization queries are often used to generate predictions from a given model, such as image denoising or finding stereo correspondence.  Summation queries are common in model learning, for computing and optimizing the data likelihood during fitting.  <br/><br/>A key point is that for these tasks the model is treated homogeneously; all variables are either maximized (or minimized) or summed over.  However, many important reasoning and inference tasks require a mixture of these where some variables are maximized (or minimized), while others are summed over.  Such mixed problems occur in optimal estimation, decision making in single- and multi-agent systems, and worst-case or antagonistic problems that arise in robust estimation and games.  Far less progress has been made on these more difficult query types.<br/><br/>The goals of this Faculty Early Career Development (CAREER) award are to develop a new framework for exact and approximate methods for such advanced computational reasoning problems.  The project includes both theoretical and practical algorithm pieces, and studies their use in estimation and learning from data.   The project extends the abilities of intelligent systems to reasoning and decision-making under uncertainty.  It applies and tests these methods on a variety of application domains, including sensor networks and computer vision.  The project supports graduate, undergraduate, and high-school student research, and it contributes to open and online course development.  The project increases impact and algorithm adoption by deploying open-source tools, developing open standards and benchmark problems in these domains, and it encourages additional progress through open comparisons and competitions."
"1316225","Innovate to Mitigate: A Crowdsourced Carbon Challenge","DRL","Discovery Research K-12","09/01/2013","06/03/2015","Gillian Puttick","MA","TERC Inc","Standard Grant","David Haury","08/31/2017","$494,215.00","Brian Drayton","gilly_puttick@terc.edu","2067 Massachusetts Avenue","Cambridge","MA","021401339","6178739600","EHR","7645","9177","$0.00","This project is designing and conducting a crowd-sourced open innovation challenge to young people of ages 13-18 to mitigate levels of greenhouse gases. The goal of the project is to explore the extent to which the challenge will successfully attract, engage and motivate teen participants to conduct sustained and meaningful scientific inquiry across science, technology and engineering disciplines.  Areas in which active cutting edge research on greenhouse gas mitigation is currently taking place include, among others,  biology (photosynthesis, or biomimicry of photosynthesis to sequester carbon) and chemistry (silicon chemistry for photovoltaics, carbon chemistry for decarbonization of fossil fuels). Collaborating in teams of 2-5, participants engage with the basic science in these areas, and become skilled at applying scientific ideas, principles, and evidence to solve a design problem, while taking into account possible unanticipated effects. They refine their solutions based on scientific knowledge, student-generated sources of evidence, prioritized criteria, and tradeoff considerations. <br/><br/>An interactive project website describes specifications for the challenge and provides rubrics to support rigor. It includes a library of relevant scientific resources, and, for inspiration, links to popular articles describing current cutting-edge scientific breakthroughs in mitigation.  Graduate students recruited for their current work on mitigation projects provide online mentoring.  Social networking tools are used to support teams and mentors in collaborative scientific problem-solving. If teams need help while working on their challenges, they are able to ask questions of a panel of expert scientists and engineers who are available online.  At the end of the challenge, teams present and critique multimedia reports in a virtual conference, and the project provides awards for excellence. <br/><br/>The use of open innovation challenges for education provides a vision of a transformative setting for deep learning and creative innovation that at the same time addresses a problem of critical importance to society. Researchers study how this learning environment improves learning and engagement among participants. This approach transcends the informal/formal boundaries that currently exist, both in scientific and educational institutions, and findings are relevant to many areas of research and design in both formal and informal settings.  Emerging evidence suggests that open innovation challenges are often successfully solved by participants who do not exhibit the kinds of knowledge, skill or disciplinary background one might expect.  In addition, the greater the diversity of solvers is, the greater the innovativeness of challenge solutions tends to be.  Therefore, it is expected that the free choice learning environment, the nature of the challenge, the incentives, and the support for collaboration will inspire the success of promising young participants from underserved student populations, as well as resulting in innovative solutions to the challenge given the diversity of teams."
"1327236","IEEE Workshop on Multimodal and Alternative Perception for Visually Impaired People (MAP4VIP)","IIS","Robust Intelligence","03/15/2013","03/19/2013","Zhigang Zhu","NY","CUNY City College","Standard Grant","Jie Yang","02/28/2014","$19,250.00","Tony Ro, YingLi Tian","zzhu@ccny.cuny.edu","Convent Ave at 138th St","New York","NY","100319101","2126505418","CSE","7495","7495, 7556","$0.00","This travel grant supports students and domain experts to attend the workshop on ""Multimodal and Alternative Perception for Visually Impaired People"" in July, 2013 in conjunction with the International Conference on Multimedia & Expo (ICME). The workshop brings researchers and practitioners from multiple disciplines (computer vision, neuroscience, multimedia computing, sensor technologies and assistive technology applications) to discuss fundamental issues in visual perception, computational intelligence, neuroscience and visual prosthesis for helping blind and visually impaired people and people working in visually challenged environments. The workshop papers are included along with the IEEE ICME 2013 proceedings. The workshop homepage is used to disseminate results of the panel discussions and paper presentations as well as the report."
"1319987","AF: Small: Counting and Sampling Cuts and Paths in Planar and Lattice Graphs","CCF","ALGORITHMIC FOUNDATIONS, ALGORITHMS","06/01/2013","04/28/2017","Ivona Bezakova","NY","Rochester Institute of Tech","Standard Grant","Rahul Shah","05/31/2018","$128,357.00","","ib@cs.rit.edu","1 LOMB MEMORIAL DR","ROCHESTER","NY","146235603","5854757987","CSE","7796, 7926","7923, 7926, 9251","$0.00","This award targets counting and sampling variants of a number of cut and path problems in several restricted graph classes including planar graphs, the 8-connected 2D grid, and the 3D grid. These graph classes are primarily motivated by applications in computer vision where several recent results renewed interest in the minimization variants of these problems. The importance of the corresponding counting and sampling variants stems from, among others, automated learning of the parameters of the underlying probabilistic models such as the Markov Random Field.<br/><br/>The proposed research builds on the PI's recent works on counting and sampling minimum (s,t)-cuts and minimum single-source-multi-sink cuts with connectivity priors, both in planar graphs. These problems are used in image segmentation applications and form an initial step towards the ultimate goal of computing the partition function for weighted cuts, i.e., the summation of the cut weights across all (s,t)-cuts; this quantity is essential for model parameter learning via the maximum likelihood principle. The PI will study a variety of cut and path problems that generalize the initial results from different perspectives. These include studying other connectivity priors, moving beyond planarity, and beyond minimum cuts and shortest paths.<br/><br/>The research will actively involve undergraduate and master students. Related educational materials are projected to annually impact over 600 students, about 10 of them from the National Technical Institute for the Deaf. In addition to standard dissemination venues like conferences and journals, the PI will also present the results at the annual Imagine RIT festival. This community outreach festival attracted about 35,000 visitors in 2012, many of them middle and high school students."
"1348152","EAGER: Hierarchical Topic Modeling by Nonnegative Matrix Factorization for Interactive Multi-scale Analysis of Text Data","IIS","GRAPHICS & VISUALIZATION","08/15/2013","08/15/2013","Haesun Park","GA","Georgia Tech Research Corporation","Standard Grant","Ephraim P. Glinert","07/31/2017","$175,000.00","","hpark@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7453","7453, 7916","$0.00","EAGER: Hierarchical Topic Modeling by Nonnegative Matrix Factorization for Interactive Multi-scale Analysis of Text Data<br/><br/>Nonnegative matrix factorization (NMF) has proven to be an important tool of choice for numerous data analytic problems in text, imaging, and computer vision. It provides advanced mathematical methods for improvements in dimensionality reduction, clustering, etc.  A distinguishing feature of the NMF is the requirement of non-negativity in the factors that represent the matrix in a lower rank.  This property greatly enhances the interpretability and modeling capability for many applications, where preserving non-negativity is important. This project is studying foundational properties of the NMF, producing new algorithmic methods using the framework of NMF for efficient and effective hierarchical clustering and topic modeling of large scale text data for multi-scale analysis, generating labels for the topics, and interactive analysis.  In addition, an interactive visual analytic system for the proposed methods is being developed to make these theoretical and algorithmic discoveries readily available to the research and applications communities. New multi-scale hierarchical methods for generating clusters and discovering topics in the documents and detection of topic changes over time are being explored to enable computationally efficient and perceptually effective ways of exploring text data and discovering latent group structure. Visual analytic systems are also being developed based on this foundational work to enable more effective and informed discovery of topics in a large-scale document collection.<br/><br/>This project will have a significant impact on the analysis and development of NMF algorithms and new modeling of problems for applications utilizing the NMF (e.g., 'Big Data').  The project is yielding effective computational methods with solid analysis that will enhance the analysis of high-dimensional data in broad areas of science, engineering, medicine, and business disciplines beyond the application areas being considered within this project."
"1319799","CIF: Small: Sampling Rate Distortion","CCF","COMM & INFORMATION THEORY","09/01/2013","06/25/2013","Prakash Narayan","MD","University of Maryland College Park","Standard Grant","Phillip Regalia","08/31/2018","$499,670.00","","prakash@eng.umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","7935","7923, 7937","$0.00","This research takes an information theoretic approach to understanding principles that govern a coordinated rate-efficient sampling of multiple signals and centralized compression of the sampled subset. The goal is to reconstruct the entirety of the signals within acceptable distortion levels. The research has three main components: an integrated analysis of sampling and rate distortion behavior, and their tradeoffs; sampling rate distortion theory for Markov random fields; and sampling rate distortion performance for signals with memory. Expected outcomes are a characterization of fundamental performance limits of optimum sampling rate and lossy compression rate and their interplay, together with the best choice of sampling mechanisms and attendant processing for reconstruction.<br/><br/>The investigators' technical approach involves the development of a principle of ""sampling rate distortion"" which lies at the intersection of specialities in information theory and signal processing, and has the larger objective of elucidating material connections between sampling and rate distortion performance. The performance of specific sampling and rate distortion processing schemes will be investigated. Specific groups of open problems chosen for investigation address a general class of multisignal models for sampling and lossy data compression. These are motivated by potential applications including dynamic thermal management for on-chip temperature control during runtime; network function computation; and image restoration, surface reconstruction and visual integration in computer vision."
"1314823","TWC: Medium: Collaborative: HIMALAYAS: Hierarchical Machine Learning Stack for Fine-Grained Analysis of Malware Domain Groups","CNS","Secure &Trustworthy Cyberspace","10/01/2013","08/30/2013","Guofei Gu","TX","Texas A&M Engineering Experiment Station","Standard Grant","Shannon Beck","09/30/2018","$250,000.00","","guofei@cse.tamu.edu","400 Harvey Mitchell Pkwy S","College Station","TX","778454645","9798626777","CSE","8060","7434, 7924","$0.00","The domain name system (DNS) protocol plays a significant role in operation of the Internet by enabling the bi-directional association of domain names with IP addresses.  It is also increasingly abused by malware, particularly botnets, by use of:  (1) automated domain generation algorithms for rendezvous with a command-and-control (C&C) server, (2) DNS fast flux as a way to hide the location of malicious servers, and (3) DNS as a carrier channel for C&C communications.<br/>This project explores the development of a scalable, hierarchical machine-learning stack, called HIMALAYAS, which specializes in algorithms for automatically mining DNS data for malware activity. In particular, we are interested in isolating both ordered and unordered sets of malware domain groups whose access patterns are temporally and logically correlated.  <br/><br/>HIMALAYAS performs a task of increasing complexity at each level ? starting from scalable clustering and feature selection at lower levels, to more advanced malware domain subsequence identification algorithms at higher levels. It has multiple benefits, including speed, accuracy, interpretability, and ability to use domain knowledge, which makes it very well suited for malware analysis and related tasks. The analysis by HIMALAYAS should accelerate the identification and takedown of malware domains on the Internet and improve services such as Google SafeSearch. <br/><br/>The machine-learning stack developed as part of the HIMALAYAS project has broader application to many important data mining problems, e.g., in financial data analysis, and mining user patterns from web access logs.  The project provides opportunities for students to participate in the development and transition of the technology."
"1314956","TWC: Medium: Collaborative: HIMALAYAS: Hierarchical Machine Learning Stack for Fine-Grained Analysis of Malware Domain Groups","CNS","Secure &Trustworthy Cyberspace","10/01/2013","08/30/2013","Vinod Yegneswaran","CA","SRI International","Standard Grant","Shannon Beck","09/30/2018","$596,095.00","Shalini Ghosh","vinod@csl.sri.com","333 RAVENSWOOD AVE","Menlo Park","CA","940253493","7032478529","CSE","8060","7434, 7924","$0.00","The domain name system (DNS) protocol plays a significant role in operation of the Internet by enabling the bi-directional association of domain names with IP addresses.  It is also increasingly abused by malware, particularly botnets, by use of:  (1) automated domain generation algorithms for rendezvous with a command-and-control (C&C) server, (2) DNS fast flux as a way to hide the location of malicious servers, and (3) DNS as a carrier channel for C&C communications. This project explores the development of a scalable, hierarchical machine-learning stack, called HIMALAYAS, which specializes in algorithms for automatically mining DNS data for malware activity. In particular, we are interested in isolating both ordered and unordered sets of malware domain groups whose access patterns are temporally and logically correlated.  <br/><br/>HIMALAYAS performs a task of increasing complexity at each level - starting from scalable clustering and feature selection at lower levels, to more advanced malware domain subsequence identification algorithms at higher levels. It has multiple benefits, including speed, accuracy, interpretability, and ability to use domain knowledge, which makes it very well suited for malware analysis and related tasks. The analysis by HIMALAYAS should accelerate the identification and takedown of malware domains on the Internet and improve services such as Google SafeSearch. <br/><br/>The machine-learning stack developed as part of the HIMALAYAS project has broader application to many important data mining problems, e.g., in financial data analysis, and mining user patterns from web access logs.  The project provides opportunities for students to participate in the development and transition of the technology."
"1302698","III: Medium: Collaborative Research: Scaling Machine Learning to Massive Datasets---A Logic Based Approach","IIS","Info Integration & Informatics","09/01/2013","07/25/2016","Tyson Condie","CA","University of California-Los Angeles","Continuing Grant","Sylvia Spengler","08/31/2017","$667,000.00","Carlo Zaniolo","tconde@cs.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","7364","7364, 7924","$0.00","Machine learning (ML) algorithms have become ubiquitous across applications as diverse as science, engineering, business, finance, education and healthcare. However, development of ML software that can scale to massive datasets and that are also easy-to-use remains a challenge in part due to the fact that developing an ML tool currently requires the implementation of a deep software stack, from the actual runtime (i.e., how an ML algorithm is executed) to the API exposed to the users.<br/><br/>This  project aims to develop DeML, a system to support the authoring and execution of ML tools. Specifically, DeML would allow ML algorithms to be formulated in the form of a declarative query over the training dataset. DeML  optimizes the execution of the query over a computing platform (e.g., Amazon EC2 or SQL Azure), taking into account the characteristics of the algorithm, the data, and the available computational resources. Adoption of DeML would greatly reduce the effort required to develop scalable implementations of ML algorithms. The project is organized around three thrusts: (i) Development of a declarative query language, based on extensions of Datalog; (ii) Analysis of runtime of DeML queries; (iii) Optimization of dataflow of DeML queries based on the characteristics of data sources and the capabilities of the underlying execution platform. The resulting open source DeML prototype implementation will be made freely available to the community through the project web page at: http://deml.cs.ucla.edu.<br/><br/>The availability of the DeML could greatly lower the effort needed to author scalable implementations of ML algorithms for analysis of massive datasets, which in turn would increase the availability of such tools to the broader community. Experience gained by implementing and deploying ML algorithms at scale over modern cloud-computing platforms, could help inform critical design choices in the development of future cloud computing platforms for big data analytics, and hence impact a broad range of scientific, engineering, national security, healthcare and business applications of big data analytics. The project offers enhanced opportunities for research-based advanced training of graduate and undergraduate students, including members of groups that are currently under-represented in computer science, in databases, machine learning, and cloud computing."
"1314560","TWC: Medium: Collaborative: HIMALAYAS: Hierarchical Machine Learning Stack for Fine-Grained Analysis of Malware Domain Groups","CNS","Secure &Trustworthy Cyberspace","10/01/2013","08/30/2013","Arindam Banerjee","MN","University of Minnesota-Twin Cities","Standard Grant","Nina Amla","09/30/2017","$252,545.00","","banerjee@cs.umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","8060","7434, 7924","$0.00","The domain name system (DNS) protocol plays a significant role in operation of the Internet by enabling the bi-directional association of domain names with IP addresses.  It is also increasingly abused by malware, particularly botnets, by use of:  (1) automated domain generation algorithms for rendezvous with a command-and-control (C&C) server, (2) DNS fast flux as a way to hide the location of malicious servers, and (3) DNS as a carrier channel for C&C communications.<br/>This project explores the development of a scalable, hierarchical machine-learning stack, called HIMALAYAS, which specializes in algorithms for automatically mining DNS data for malware activity. In particular, we are interested in isolating both ordered and unordered sets of malware domain groups whose access patterns are temporally and logically correlated.  <br/><br/>HIMALAYAS performs a task of increasing complexity at each level ? starting from scalable clustering and feature selection at lower levels, to more advanced malware domain subsequence identification algorithms at higher levels. It has multiple benefits, including speed, accuracy, interpretability, and ability to use domain knowledge, which makes it very well suited for malware analysis and related tasks. The analysis by HIMALAYAS should accelerate the identification and takedown of malware domains on the Internet and improve services such as Google SafeSearch. <br/><br/>The machine-learning stack developed as part of the HIMALAYAS project has broader application to many important data mining problems, e.g., in financial data analysis, and mining user patterns from web access logs.  The project provides opportunities for students to participate in the development and transition of the technology."
"1250985","BIGDATA: Small: DA: Statistical Machine Learning Methods for Scalable Data Analysis","IIS","Big Data Science &Engineering","07/01/2013","07/01/2016","Tong Zhang","NJ","Rutgers University New Brunswick","Standard Grant","Sylvia J. Spengler","06/30/2017","$738,971.00","","tzhang@stat.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","8083","7433, 7923, 8083","$0.00","Big Data has become ubiquitous in modern industrial and scientific applications where the size and dimensionality of data are becoming so large as to require  new statistical tools for efficient data analysis. This collaborative project involving researchers at Rutgers University and Microsoft Research focuses on the theoretical and algorithmic development of advanced computational methods for big data analytics. While the problems to be investigated are motivated by various Internet applications, the resulting solutions are expected to be broadly applicable to other domains. <br/><br/>The project considers three interrelated main themes in big data analytics: (a) effective sampling of big datasets to filter out unreliable data source and improve statistical analysis;  (b) dimensionality reduction techniques that can best preserve information via hashing and sparse random projection techniques; and (c) large scale optimization techniques for machine learning that can directly handle large datasize. Anticipated results of this work include new theoretical results, new data analytics algorithms, and their open source software implementations.<br/><br/>Broader impacts of the research include broadly disseminated open source implementations of scalable data analytics algorithms, research-based training and education of graduate and undergraduate students, and academic-industrial collaborations resulting in an interplay between fundamental research in machine learning and industrial applications."
"1253935","CAREER: Next Generation Patient Simulators","IIS","HCC-Human-Centered Computing","02/01/2013","02/27/2017","Laurel Riek","IN","University of Notre Dame","Continuing Grant","Ephraim Glinert","02/28/2018","$625,160.00","","lriek@eng.ucsd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","CSE","7367","1045, 7367, 9251","$0.00","It is estimated that in the United States many thousands of people are killed each year and billions of dollars lost due to medical errors.  The PI argues that one way to reduce the incidence of such errors is through education involving human patient simulator (HPS) systems.  Although perhaps the most commonly used android robots in America, a critical technology gap is that none of the commercially available HPS systems exhibit realistic facial expressions, gaze, or mouth movements, despite the vital importance of these cues in how providers assess and treat patients.  The PI's goal in this project is to address this shortcoming by developing novel expression synthesis algorithms and social control methods, thereby advancing the fields of affective computing and human-robot interaction.  To these ends the PI will model facial features characteristic of 3 pathologies (stroke, cerebral palsy and dystonia, the latter being a neurological disorder in which the muscles of the trunk, shoulders, and neck go into spasm so that the head and limbs are held in unnatural positions), and 2 affective states (pain and drowsiness).  She will synthesize these facial features on a new HPS system and create a model of shared social control for operators of expressive robots, and evaluate their impact on educational and task outcomes.  <br/><br/>Broader Impacts:  This research will transform the state of the art in HPS technology by enabling educators to run simulations currently impossible with commercially available technology, thereby leading to more realistic training experiences for doctors, nurses, and combat medics, which will ultimately improve healthcare.  It will create new facial models of stroke, dystonia and cerebral palsy, which may impact fields such as computer vision and biometrics while also enhancing our understanding of these disorders and providing a means for educating people how to better interact with those suffering from these disabilities and/or to quickly recognize signs of stroke.  The PI will also conduct substantial mentoring activities for undergraduates and outreach activities for K-12 students."
"1329891","CPS: Synergy: Collaborative Research: Mutually Stabilized Correction in Physical Demonstration","CNS","CPS-Cyber-Physical Systems","10/01/2013","09/16/2013","Todd Murphey","IL","Northwestern University","Standard Grant","Radhakisan Baheti","09/30/2018","$699,999.00","Brenna Argall","t-murphey@northwestern.edu","750 N. Lake Shore Drive","Chicago","IL","606114579","3125037955","CSE","7918","7918","$0.00","Objective: How much a person should be allowed to interact with a controlled machine?  If that machine is safety critical, and if the computer that oversees its operation is essential to its operation and safety, the answer may be that the person should not be allowed to interfere with its operation at all or very little. Moreover, whether the person is a novice or an expert matters.  <br/><br/>Intellectual Merit: This research algorithmically resolves the tension between the need for safety and the need for performance, something a person may be much more adept at improving than a machine. Using a combination of techniques from numerical methods, systems theory, machine learning, human-machine interfaces, optimal control, and formal verification, this research will develop a computable notion of trust that allows the embedded system to assess the safety of the instruction a person is providing.  The interface for interacting with a machine matters as well; designing motions for safety-critical systems using a keyboard may be unintuitive and lead to unsafe commands because of its limitations, while other interfaces may be more intuitive but threaten the stability of a system because the person does not understand the needs of the system.  Hence, the person needs to develop trust with the machine over a period of time, and the last part of the research will include evaluating a person's performance by verifying the safety of the instructions the person provides.  As the person becomes better at safe operation, she will be given more authority to control the machine while never putting the system in danger.<br/><br/>Broader Impacts:  The activities will include outreach, development of public-domain software, experimental coursework including two massive online courses, and technology transfer to rehabilitation. Outreach will include exhibits at the Museum of Science and Industry and working with an inner-city high school. The algorithms to be developed will have immediate impact on projects with the Rehabilitation Institute of Chicago, including assistive devices, stroke assessment, and neuromuscular hand control. Providing a foundation for a science of trust has the potential to transform rehabilitation research."
"1302690","III: Medium: Collaborative Research: Scaling Machine Learning to Massive Datasets---A Logic Based Approach","IIS","Info Integration & Informatics","09/01/2013","07/25/2014","Neoklis Polyzotis","CA","University of California-Santa Cruz","Continuing grant","Sylvia Spengler","08/31/2017","$333,000.00","","alkis.polyzotis@gmail.com","1156 High Street","Santa Cruz","CA","950641077","8314595278","CSE","7364","7364, 7924","$0.00","Machine learning (ML) algorithms have become ubiquitous across applications as diverse as science, engineering, business, finance, education and healthcare. However, development of ML software that can scale to massive datasets and that are also easy-to-use remains a challenge in part due to the fact that developing an ML tool currently requires the implementation of a deep software stack, from the actual runtime (i.e., how an ML algorithm is executed) to the API exposed to the users.<br/><br/>This  project aims to develop DeML, a system to support the authoring and execution of ML tools. Specifically, DeML would allow ML algorithms to be formulated in the form of a declarative query over the training dataset. DeML  optimizes the execution of the query over a computing platform (e.g., Amazon EC2 or SQL Azure), taking into account the characteristics of the algorithm, the data, and the available computational resources. Adoption of DeML would greatly reduce the effort required to develop scalable implementations of ML algorithms. The project is organized around three thrusts: (i) Development of a declarative query language, based on extensions of Datalog; (ii) Analysis of runtime of DeML queries; (iii) Optimization of dataflow of DeML queries based on the characteristics of data sources and the capabilities of the underlying execution platform. The resulting open source DeML prototype implementation will be made freely available to the community through the project web page at: http://deml.cs.ucla.edu.<br/><br/>The availability of the DeML could greatly lower the effort needed to author scalable implementations of ML algorithms for analysis of massive datasets, which in turn would increase the availability of such tools to the broader community. Experience gained by implementing and deploying ML algorithms at scale over modern cloud-computing platforms, could help inform critical design choices in the development of future cloud computing platforms for big data analytics, and hence impact a broad range of scientific, engineering, national security, healthcare and business applications of big data analytics. The project offers enhanced opportunities for research-based advanced training of graduate and undergraduate students, including members of groups that are currently under-represented in computer science, in databases, machine learning, and cloud computing."
"1309410","Cracking the Color Code of DNA-stabilized Metal Nanoclusters with Rapid Optical Array Characterization and Machine Learning","DMR","BIOMATERIALS PROGRAM, Comp&Data Driven Mat Res(CDMR)","09/15/2013","05/31/2016","Elisabeth Gwinn","CA","University of California-Santa Barbara","Standard Grant","Daryl Hess","03/31/2018","$315,000.00","","bgwinn@physics.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","MPS","7623, 8029","7237, 7573, 9177, 9251","$0.00","Technical Abstract <br/><br/>Elisabeth Gwinn of the University of California, Santa Barbara is supported by an award from the Computational and Data Driven Materials Research program for research to combine computational machine learning tools with strategic data obtained from fast, array format optical characterization, with the goal of discovering and developing a versatile new class of photonic nanomaterial.  Specifically, the work will investigate fluorescent, DNA-stabilized, few-atom metal nanoclusters, or DNA-mNCs. DNA-mNCs based on silver clusters are already beginning to be used in innovative imaging, molecular logic, and selective sensor applications. The recent discovery of copper-based DNA-mNCs suggests that the formation of fluorescent clusters in DNA hosts may generalize to other coinage metals.<br/><br/>The PI's prior work revealed the special sensitivity of silver cluster fluorescence to the sequence and secondary structure of the host DNA. Studies of small sets of various DNA strands have found DNA-AgNCs with fluorescence colors spanning 500 - 900 nm. Together with the compact sizes of DNA-mNCs, which are compatible with the finest resolutions accessed by DNA nanotechnology, this wide color space may open new arenas beyond current solution applications, in nanoscale photonic arrays for information processing and signaling of biochemical and physical events. However, there is essentially no current understanding of how the properties of DNA-mNCs relate to the sequence of the host DNA.  Even for the relatively well-studied DNA-AgNCs, there are no identified sequence motifs that govern fluorescence color, brightness or stability. This is despite the fundamental importance of these properties to all applications, and to the underlying materials science. But, only ~100 strands have been examined as potential hosts for chemically stable DNA-AgNCs. This is a miniscule sampling of the space of possible sequences. <br/><br/>This research aims to crack the code for the sequence characteristics that govern the properties of DNA-mNCs, by applying machine learning tools to much larger, strategically selected data sets. The data will be acquired by robotic synthesis and rapid array optical characterization of Ag-based DNA-mNCs. Strand selection will leverage the knowledge of DNA-AgNCs developed in the PI's prior work.  To elucidate the role of the specific metal from which the cluster is formed, experiments on other coinage metals, including copper, will also be made.<br/><br/>The undergraduate and graduate students who participate in the work will be trained in advanced techniques encompassing materials science, computer science and nanotechnology. High school students will be exposed to aspects of the work through UCSB's School for Scientific Thought (SST).<br/><br/><br/>Non-Technical Abstract<br/><br/>This work focuses on tiny clusters composed of just a few atoms of metal, that are made stable in water by wrapping the clusters up in short strands of DNA.  It has been known for many years that ""bare"" clusters made of a few metal atoms have very interesting optical properties.  In particular, they can be fluorescent, meaning that the clusters emit photons after they are placed in an excited state. These DNA-encapsulated, few-atom metal nanoclusters may have many potential uses, such as fluorescent sensing of toxic ions and of targeted DNA and RNA strands.  <br/><br/>The most fascinating and potentially useful feature of these materials is  the fact that different DNA sequences can produce clusters of different color.   Cracking the code for the DNA sequence characteristics that govern  this color is the focus of this work.    The primary focus will be on silver clusters, building on previous work, but to elucidate the role of the specific metal from which the cluster is formed, experiments on other metals, including copper, will also be carried out. <br/><br/>The undergraduate and graduate students who participate in the work will be trained in advanced techniques encompassing materials science, computer science and nanotechnology. High school students will be exposed to aspects of the work through UCSB's School for Scientific Thought (SST)."
"1320746","AF:Small: Divide-and-Conquer Numerical Methods for Analysis of Massive Data Sets","CCF","ROBUST INTELLIGENCE, NUM, SYMBOL, & ALGEBRA COMPUT","09/01/2013","08/07/2013","Inderjit Dhillon","TX","University of Texas at Austin","Standard Grant","Jack Snoeyink","08/31/2017","$491,044.00","","inderjit@cs.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7495, 7933","7923, 7933","$0.00","Data is being generated at a tremendous rate in diverse applications, such as health care, genomics, energy management and social network analysis. Indeed, the recent moniker of Big Data emphasizes that massive volumes of data are ubiquitous. Thus, there is a great need for developing scalable and sophisticated methods for analyzing these data sets. This project is aimed towards one aspect of this challenge, namely, developing scalable and state-of-the-art numerical methods for modern problems that arise in machine learning. <br/><br/>This project will aim to develop divide-and-conquer methods for representative, concrete problems that arise in contemporary applications. These include (a) classification: kernel support vector machines, (b) regression: kernel regression and high-dimensional sparse approximation, (c) structure learning: graphical model estimation, (d) spectral approximation: multi-scale SVD computation, and (e) missing value estimation: matrix factorization. The project will develop specialized algorithms for each of these problems, in particular, developing tailored ways of dividing the problem into subproblems, solving the subproblems, and finally conquering the subproblems. Thus, general principles for applying the divide-and-conquer approach to other problems in large-scale machine learning will be uncovered. The project will lead to software for large-scale data analysis that will be efficient on modern multi-core computers. Impact of the new algorithms on various application areas, such as bioinformatics and network analysis, will be studied. Within computer science and applied mathematics, the project will have a broad impact on research in a variety of disciplines, including numerical analysis, numerical optimization, statistics, machine learning, data mining and parallel computing."
"1320894","RI: Small: Collaborative Research: Statistical ranking theory without a canonical loss","IIS","Robust Intelligence","08/01/2013","08/08/2013","Pradeep Ravikumar","TX","University of Texas at Austin","Standard Grant","Weng-keen Wong","08/31/2017","$223,572.00","","pradeepr@cs.cmu.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7495","7495, 7923","$0.00","The problem of ranking objects occupies a central place in key technologies such as web search and recommendation systems. These technologies have a tremendous daily impact on the lives of millions of people. Moreover, the enormous scale of data on the web makes the use of machine learning especially attractive in constructing ranking algorithms.  A huge amount of research effort has been devoted to developing efficient ranking algorithms that can deal with a variety of data sets encountered in web search and recommendation systems.<br/><br/>This project develops unifying mathematical theory that will provide a basis for understanding and categorizing existing algorithms and, more importantly, lead to deeper insights and new algorithms for the problem of learning to rank. The investigators also apply ranking algorithms to new domains.  For example, ranking chemical reactions based on their plausibility will help chemists discover much-needed reaction bases for technologies such as carbon dioxide reduction, and conversion of natural gas into gasoline. <br/><br/>Fundamental advances in the statistical theory of ranking will be incorporated into undergraduate and graduate courses. Data sets and software developed will be made freely available to the scientific community. The investigators will also organize a workshop with a focus on interdisciplinary participation and involvement of under-represented groups in computer science and statistics.<br/><br/>The primary technical challenge in developing statistical ranking theory is the absence of a universally agreed-upon loss functions for ranking. This is in contrast to classic machine learning problems such as classification and regression, where there are only a few natural possibilities for the loss function and these are well-understood theoretically. The project addresses this gap by investigating how different loss functions for ranking affect fundamental theoretical properties such as learnability, and by creating a theory of convex surrogates that is applicable when loss functions abound.  The project re-examines existing statistical literature on ranking with a computational lens.  This will enable development of flexible and efficient plug-in decision rules that model the conditional probability of labels given inputs.<br/><br/>By incorporating the results of this research into courses and survey articles, the PIs help train a new generation of machine learning researchers and practitioners who will view ranking as a learning problem on par with classification and regression in mathematical depth as well as practical importance.  Theoretical guidance for practitioners formulating new algorithms for ranking will improve the most common applications on the web."
"1308777","Differential Geometry of Curves and Surfaces","DMS","GEOMETRIC ANALYSIS","09/01/2013","09/05/2013","Mohammad Ghomi","GA","Georgia Tech Research Corporation","Standard Grant","Christopher W. Stark","08/31/2017","$176,000.00","","ghomi@math.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","MPS","1265","","$0.00","Abstract<br/><br/>Award: DMS 1308777, Principal Investigator: Mohammad Ghomi <br/><br/>The principal investigator proposes to continue his work on the theory of curves and surfaces in Euclidean space, and more generally on Riemannian submanifolds of low dimension or codimension. He specializes in applying contemporary methods such as curvature flows and h-principle theory to solve classical problems which often have simple intuitive statements, while their solutions may require sophisticated techniques. The PI's research in this area spans a wide range of topics including isometric embeddings, isoperimetric problems, geometric knot theory, polyhedral approximations, and connections with real algebraic geometry. Some recurring themes throughout these investigations are various notions of convexity or optimization, and the interaction between geometric and topological concepts, or local versus global properties of submanifolds. More specifically, a typical problem is how restrictions on curvature, intrinsic metric, or various boundary conditions, influence the global shape of a curve or a hypersurface, or even allow an embedding of that object in a Euclidean space of low codimension.  A fundamental problem in this area is that of isometric rigidity of surfaces: can one continuously deform a smooth closed surface in Euclidean space without changing its intrinsic metric?  We also consider a number of related problems involving the self-linking number or vertices of closed curves, spherical images of closed surfaces, and various deformations of submanifolds which preserve the sign or magnitude of the curvature. Other projects include inequalities for mean chord lengths of submanifolds, regularity of real algebraic hypersurfaces, and unfoldings of convex polyhedra.<br/><br/>Curves and surfaces are to geometry what numbers are to algebra. They form the basic ingredients of our visual perception and inspire the development of far reaching mathematical tools. Yet despite centuries of pure study, and an abundance of potential applications, there are still many fundamental open problems in this area which are strikingly intuitive and elementary to state.  Studying these problems may stimulate useful developments in pure mathematics, or lead to wider applications in science and technology. For instance, the PI's work on rigidity problem for surfaces may have applications for stability of complicated domes in modern architecture, or various physical frameworks. The polyhedral approximation techniques which the PI is proposing could be useful in computer aided design, and the emerging field of discrete differential geometry. The related studies of the Gauss maps of surfaces could be useful in computer vision and optics, while studying isoperimetric problems has been a significant source of enrichment in calculus of variations and mathematical physics. Further, folding-unfolding problems have numerous applications ranging from deployment of satellite dishes in space to implantation of stents in human arteries. Another impact of the proposed activity would be development of connections between various fields, as in the PI's work on tangent cones, which combines concepts from geometric measure theory, algebraic geometry, and convex analysis. Finally, these problems are ideal for introducing the general public to the exciting world of modern day mathematics, and arousing the interest of beginning students in Geometry."
"1248731","SBIR Phase I: A Novel Platform for Automatic Geo-Tagging of Images","IIP","SMALL BUSINESS PHASE I","01/01/2013","02/15/2013","Tsung-Lin Yang","NY","TaggPic, Inc.","Standard Grant","Muralidharan S. Nair","06/30/2013","$146,581.00","","tlytaggpic@gmail.com","210 Summerhill Drive","Ithaca","NY","148502848","5202470143","ENG","5371","4080, 5371, 8032, 9139, ","$0.00","This Small Business Innovation Research (SBIR) Phase I project aims to develop computer software that automatically recognizes buildings, structures, and landmarks in digital photographs, as well as the precise location where photos were taken. The advent of the digital age has led to hundreds of millions of photos being shared on the Web every day. This influx of user-generated content is leading to significant challenges in organizing and understanding large photo collections from the many thousands of photographs captured by a family over several years, to the billions of photos shared online each month. For instance, annotating photos is currently a burdensome task; users of photo-sharing services have no accurate, automated method for labeling landmarks and other structures present in their pictures, instead relying on manual tagging. Using newly invented computer vision technology, this project aims to automatically transform photographs of locations into geo-tagged user engagement portals storyboards rich with pixel-level annotations and hyper-relevant information. The innovations leverage a unique database of 3D models, yielding very accurate locations and tags. The project objectives include developing algorithms for world-scale location-based image content recognition, an accurate annotation method, and an application programming interface for the system for use by application developers.<br/><br/>The broader impact/commercial potential of this project are significant; as digital photo-sharing has become a major online activity, there will be considerable market interest in the ability to identify and link photos to hyper-local and highly relevant information, including advertising, for the tourism and hospitality market sector, as well as for small local businesses. Additionally, the technology will enable companies in the social media market sector to increase user engagement through interactive and contextualized photo browsing. Furthermore, the proposed technology can transform photos into image-powered platform for the mobile phone market sector, allowing for a seamless transition between digital photos and online maps and thus greatly enhancing user navigation of new physical environments using mobile devices. Finally, the technology can also provide key analytics to companies in social media monitoring, by determining aggregate patterns in where and why people take photos at locations of interest."
"1319810","RI: Small: Collaborative Research: Statistical ranking theory without a canonical loss","IIS","Robust Intelligence","08/01/2013","08/08/2013","Ambuj Tewari","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Weng-keen Wong","07/31/2016","$245,594.00","","tewaria@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7495","7495, 7923","$0.00","The problem of ranking objects occupies a central place in key technologies such as web search and recommendation systems. These technologies have a tremendous daily impact on the lives of millions of people. Moreover, the enormous scale of data on the web makes the use of machine learning especially attractive in constructing ranking algorithms.  A huge amount of research effort has been devoted to developing efficient ranking algorithms that can deal with a variety of data sets encountered in web search and recommendation systems.<br/><br/>This project develops unifying mathematical theory that will provide a basis for understanding and categorizing existing algorithms and, more importantly, lead to deeper insights and new algorithms for the problem of learning to rank. The investigators also apply ranking algorithms to new domains.  For example, ranking chemical reactions based on their plausibility will help chemists discover much-needed reaction bases for technologies such as carbon dioxide reduction, and conversion of natural gas into gasoline. <br/><br/>Fundamental advances in the statistical theory of ranking will be incorporated into undergraduate and graduate courses. Data sets and software developed will be made freely available to the scientific community. The investigators will also organize a workshop with a focus on interdisciplinary participation and involvement of under-represented groups in computer science and statistics.<br/><br/>The primary technical challenge in developing statistical ranking theory is the absence of a universally agreed-upon loss functions for ranking. This is in contrast to classic machine learning problems such as classification and regression, where there are only a few natural possibilities for the loss function and these are well-understood theoretically. The project addresses this gap by investigating how different loss functions for ranking affect fundamental theoretical properties such as learnability, and by creating a theory of convex surrogates that is applicable when loss functions abound.  The project re-examines existing statistical literature on ranking with a computational lens.  This will enable development of flexible and efficient plug-in decision rules that model the conditional probability of labels given inputs.<br/><br/>By incorporating the results of this research into courses and survey articles, the PIs help train a new generation of machine learning researchers and practitioners who will view ranking as a learning problem on par with classification and regression in mathematical depth as well as practical importance.  Theoretical guidance for practitioners formulating new algorithms for ranking will improve the most common applications on the web."
"1329683","CPS: Synergy: Collaborative Research: Mutually Stabilized Correction in Physical Demonstration","CNS","CYBER-PHYSICAL SYSTEMS (CPS)","10/01/2013","09/16/2013","Magnus Egerstedt","GA","Georgia Tech Research Corporation","Standard Grant","Radhakisan S. Baheti","09/30/2017","$299,999.00","","magnus@ece.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7918","7918","$0.00","Objective: How much a person should be allowed to interact with a controlled machine?  If that machine is safety critical, and if the computer that oversees its operation is essential to its operation and safety, the answer may be that the person should not be allowed to interfere with its operation at all or very little. Moreover, whether the person is a novice or an expert matters.  <br/><br/>Intellectual Merit: This research algorithmically resolves the tension between the need for safety and the need for performance, something a person may be much more adept at improving than a machine. Using a combination of techniques from numerical methods, systems theory, machine learning, human-machine interfaces, optimal control, and formal verification, this research will develop a computable notion of trust that allows the embedded system to assess the safety of the instruction a person is providing.  The interface for interacting with a machine matters as well; designing motions for safety-critical systems using a keyboard may be unintuitive and lead to unsafe commands because of its limitations, while other interfaces may be more intuitive but threaten the stability of a system because the person does not understand the needs of the system.  Hence, the person needs to develop trust with the machine over a period of time, and the last part of the research will include evaluating a person's performance by verifying the safety of the instructions the person provides.  As the person becomes better at safe operation, she will be given more authority to control the machine while never putting the system in danger.<br/><br/>Broader Impacts:  The activities will include outreach, development of public-domain software, experimental coursework including two massive online courses, and technology transfer to rehabilitation. Outreach will include exhibits at the Museum of Science and Industry and working with an inner-city high school. The algorithms to be developed will have immediate impact on projects with the Rehabilitation Institute of Chicago, including assistive devices, stroke assessment, and neuromuscular hand control. Providing a foundation for a science of trust has the potential to transform rehabilitation research."
"1307164","Rigidity Phenomena in Geometry and Dynamics","DMS","GEOMETRIC ANALYSIS, TOPOLOGY","07/01/2013","06/15/2015","Ralf Spatzier","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Christopher Stark","06/30/2017","$295,192.00","","spatzier@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","MPS","1265, 1267","9251","$0.00","The research proposed lies between dynamical systems, group theory and geometry. Principally, the investigator plans to study dynamical and geometric structures of ""higher rank"" systems in these areas. These appear naturally in seemingly quite separate areas, for example in number theory or in studying the spectrum of the Laplacian. The investigator will work on rigidity properties of actions of higher rank abelian and semi-simple Lie groups and their lattices striving to classify such systems under suitable geometric or dynamical hypotheses.  In particular, he will study higher rank hyperbolic abelian actions and their cocycles on tori and homogeneous spaces as well as general Cartan actions of rank 2. These special cases provide tests for more general conjectures.  The investigator will also study actions by semi-simple groups and their lattices preserving projective, affine and other geometric structures.  In addition, the investigator will analyze Riemannian manifolds (especially those of higher spherical rank) and more general singular spaces and their geodesic flows. Geometric, dynamical and group theoretic tools will be used in this research.<br/><br/>Dynamical systems and ergodic theory investigate the evolution of a physical or mathematical system over time, such as turbulence in a fluid flow. New ideas and concepts such as chaos and fractals have changed our understanding of the world. Dynamics and ergodic theory provide excellent mathematical tools, and have a strong impact on the sciences and engineering. Symbolic dynamics for instance has been instrumental in developing efficient and safe codes for computer science. Tools and ideas from smooth dynamics are used as far afield as cell biology and meteorology. Geometry is a highly developed and ancient field in mathematics of amazing vigor.  It studies curves, surfaces and their higher dimensional analogues, their shapes, shortest paths, and maps between such spaces.  Differential geometry had its roots in cartography, starting with Gauss in the nineteenth century. It is closely linked with physics and other sciences and applied areas such as computer vision. Geometry and dynamics are closely related.  Indeed, important dynamical systems come from geometry, and vice versa geometry provides tools to study dynamical systems. One main goal of this project studies when two dynamical systems commute, i.e. when one system is unaffected by the changes brought on by the other. Important examples of such systems arise from geometry when the space contains many flat subspaces.  Group theory finally enters both dynamics and geometry by studying the group of symmetries of a geometry or dynamical situation, or by investigating the dynamical and geometric behavior of the group of symmetries acting on a space."
"1305878","Connecting Women in Mathematics: AWM Research Symposium 2013 and Workshop 2014","DMS","INFRASTRUCTURE PROGRAM","02/15/2013","02/04/2013","Georgia Benkart","RI","Association for Women in Mathematics, Inc.","Standard Grant","Jennifer Slimowitz Pearl","01/31/2015","$49,861.00","Ruth Charney, Kathryn Leonard","benkart@math.wisc.edu","201 Charles St","Providence","RI","029042213","4014554086","MPS","1260","7556","$0.00","The Association for Women in Mathematics (AWM) is developing a series of interconnected research conferences and workshops designed to create ongoing networks, encourage research collaborations, and build a stronger sense of community among women in the mathematical sciences. This project includes the first two events in this series, AWM Research Symposium 2013 and AWM Workshop at JMM 2014.  The symposium, to be held March 16-17, 2013 at Santa Clara University, will include three distinguished plenary speakers, special sessions on a broad array of topics in pure and applied mathematics, contributed paper sessions, poster presentations, and a panel discussion. All presenters will be women, but the event will be open to the entire mathematical community and will be advertised widely. Mathematics of Planet Earth 2013, an initiative launched by the mathematical sciences institutes of North America to showcase the relevance of the mathematical sciences in solving the challenges facing our planet, will be featured in one of the invited addresses and in several of the special sessions.  The AWM Workshop at the Joint Mathematics Meetings in Baltimore in January 2014, will be aimed at women early in their careers. It will concentrate on diverse aspects of the mathematics of shape modeling and will feature a daylong special session on this subject, a general topic poster session, and a reception with a mentoring component. The need for mathematical shape modeling and image analysis arises in fields such as computer vision, computer animation, and biomedical engineering. Questions from shape analysis often lead to rich mathematical problems in areas like statistics, variational methods, optimization, and geometry.<br/><br/>While the problem of under representation of women in the mathematical sciences is persistent and complex and has no single solution, events that bring together women at various stages of their careers for networking, mentoring and research collaboration create a synergy that goes beyond that of a standard research conference. Such events can have a significant impact on the aspirations and future career trajectories of young women.  AWM's planned series of conferences and workshops is designed to build community and to promote networking, natural mentoring relationships, and research collaborations among women in mathematics, especially among those just beginning their careers. This project will help fund the first two of these events, a broad-based research symposium and a focused workshop for early-career women."
"1344670","SCH: INT: Collaborative Research: A Self-Adaptive Personalized Behavior Change System for Adolescent Preventive Healthcare","IIS","Smart and Connected Health","10/01/2013","09/16/2013","Elizabeth Ozer","CA","University of California-San Francisco","Standard Grant","Sylvia Spengler","09/30/2018","$1,043,173.00","","ozere@peds.ucsf.edu","1855 Folsom St Ste 425","San Francisco","CA","941034249","4154762977","CSE","8018","8018, 8062","$0.00","The majority of morbidity and mortality during adolescence is preventable and related to behaviors such as substance use and vehicle-related injuries. Most adolescents visit a healthcare provider once a year, providing an ideal opportunity to integrate behavioral health screening into clinical care. Although the majority of adolescent health problems are amenable to behavioral intervention, few health information technology interventions have been integrated into adolescent care. With complementary theoretical advances (social-cognitive theories of behavior change) and technology advances (intelligent narrative-centered learning environments, user modeling, and machine learning), the field is now well positioned to design health behavior change systems that can realize significant impacts on behavior change for adolescent preventive health. <br/><br/>Computationally-enabled models of behavior change hold significant promise for adolescent healthcare. The objective of the proposed research is to design, implement, and investigate INSPIRE, a self-adaptive personalized behavior change system for adolescent preventive health. INSPIRE will utilize a social-cognitive theory of behavior change built around a tight feedback loop in which a narrative-centered behavior change environment will produce improved behaviors in patients, and the resulting patient outcome data will be used by a reinforcement learning optimization system to learn refined computational behavior change models. With a focus on risky behaviors and an emphasis on substance use, adolescents will interact with INSPIRE to develop an experiential understanding of the dynamics and consequences of their substance use decisions. A unique feature of INSPIRE afforded by recent advances in machine learning will be its ability to optimize health behavior change at both the individual and population levels. At the individual level, INSPIRE will utilize a patient behavior model to personalize its behavior change narratives for individual adolescents. It will customize interactions based on an adolescent's goals and affective models. At the population level, INSPIRE will utilize reinforcement learning to adapt its narrative generation system to systematically increase its ability to improve two types of outcomes: behavior change and self-efficacy. The project will culminate with an experiment conducted with a fully implemented version of INSPIRE at outpatient clinics within the UC San Francisco Department of Pediatrics, Benioff Children's Hospital. <br/><br/>It is anticipated that INSPIRE interventions will yield two types of outcomes: 1) improved health behavior through significant reductions in adolescent risky behavior, relative to standard of care; and 2) increased self-efficacy with respect to adolescents' ability to make good decisions about their health behaviors, relative to standard of care. Designed for natural integration into clinic workflow, interoperability with EHR and patient portal systems, and security and privacy requirements, INSPIRE will report patient behavior change summaries to healthcare providers. Through multi-platform deployments supporting laptop, desktop, tablet, and mobile computing devices, INSPIRE will serve as an empowering tool for adolescents, making them full participants in their own wellbeing. It will also enable researchers to run behavior analytics to investigate which properties of alternate interventions contribute most effectively to behavior change outcomes. Going forward, it is anticipated that INSPIRE will provide a testbed for a broad range of behavior change research and serve as the foundation for next-generation personalized preventive healthcare through computationally-enabled behavior change."
"1245540","Motivating First-Year Calculus with Robotics","DUE","TUES-Type 1 Project","09/15/2013","09/12/2013","Jason Cantarella","GA","University of Georgia Research Foundation Inc","Standard Grant","Ron Buckmire","08/31/2017","$174,696.00","","cantarel@math.uga.edu","310 East Campus Rd","ATHENS","GA","306021589","7065425939","EHR","7513","9178, SMET","$0.00","The PI is developing curricular materials based on educational robotics, gaming, and computer vision for the first-year calculus course. An example project is based on a throwing robot. By spinning an arm at a controlled speed and releasing a ball bearing at a precise angle, the robot can accurately toss the bearing several feet into a coffee cup. Students are using calculus to compute rotation speed and release angle. The intellectual merit of the project comes from 1) providing new technology for calculus education, 2) motivating students with robotics activities which give them a better understanding of the uses of calculus, and 3) contributing to the emerging body of scholarship on educational robotics in the university mathematics classroom. The broader impacts of the project include 1) providing instruction to several hundred students with the new materials, 2) training future teachers from the UGA College of Education in technology-based mathematics education, and 3) training faculty from seven campuses with the new materials. The project is providing a new robot platform for students to use in investigating the problem of throwing a ball onto a target, software to track thrown objects for teaching projectile motion, and an interface to a Bluetooth-controlled toy car with accelerometer to learn about acceleration, to calculus classes on seven campuses. The new materials are being evaluated by an external team, using the Student Mathematics Motivation assessment, to see if they increase student engagement with mathematics."
"1319788","AF: Small: Learning and Testing Classes of Distributions","CCF","ALGORITHMS","06/01/2013","06/03/2013","Rocco Servedio","NY","Columbia University","Standard Grant","Tracy J. Kimbrel","05/31/2016","$471,875.00","","rocco@cs.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","7926","7923, 7926","$0.00","A long and successful line of research in machine learning deals with algorithms that learn from ""labeled"" data, where a target function is assumed to provide a label for each data point. A major focus of theoretical work has been to develop efficient algorithms for learning different classes of target functions. Recent years have witnessed a data explosion across many domains of science and society, but much of this newly available data consists simply of example points (DNA sequences, sensor readings, smartphone user locations, etc) without any labels. A natural model of such scenarios is that data points are generated according to some unknown probability distribution (typically over an extremely large domain). The goal of the proposed work is to study the learnability of different classes of probability distributions given access to samples drawn from the distributions. This is closely analogous to the framework of learning from labeled data sketched above, but with probability distributions playing the role of functions as the objects to be learned.<br/><br/>In this project, the PI will perform theoretical research on developing computationally efficient algorithms for learning and testing various natural types of probability distributions over extremely large domains. (Testing algorithms are algorithms which, instead of trying to accurately model an unknown distribution, have the more modest goal of testing whether or not the distribution has some property of interest.) Specific problems the PI will address include: (1) Developing efficient algorithms to learn and test univariate probability distributions that satisfy various natural kinds of ""shape constraints"" on the underlying probability density function. Preliminary results suggest that dramatic improvements in efficiency may be possible for algorithms that are designed to exploit this type of structure. (2) Developing efficient algorithms for learning and testing complex distributions that result from the aggregation of many independent simple sources of randomness.<br/><br/>The algorithms that the PI will work to develop can provide useful modelling tools in data-rich environments and may serve as a ""computational substrate"" on which large-scale machine learning applications can be developed for real-world problems spanning a broad range of application areas. Other important focuses of the grant are to train graduate students through research collaboration, disseminate research results through seminar talks, survey articles and other publications, and to continue ongoing outreach activities aimed at increasing interest in theoretical computer science topics in elementary school students."
"1254159","CAREER:   Automated extraction of vessel data from images to construct new models for vascular networks in plants and animals","DBI","ADVANCES IN BIO INFORMATICS","06/01/2013","06/07/2017","Van Savage","CA","University of California-Los Angeles","Continuing Grant","Peter McCartney","05/31/2019","$796,325.00","","vsavage@ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","BIO","1165","1045","$0.00","Blood is pumped from the heart to the capillaries through a highly branched and interconnected network known as the cardiovascular system, with vessel sizes that range from microns to centimeters and blood flow speeds that differ by a factor of 1000. Similar types of vascular networks are essential for the flow of resources in nearly all multicellular organisms, including plants (xylem networks), insects (tracheal networks), and mammals. Understanding which evolutionary principles and environmental factors drive the structure of vascular networks, and what constrains the flow through them, could help lead to a better understanding of organismic structure and function with implications for systems as diverse as forests, food webs, and even tumors. This research project will supply the data needed to directly test existing models and to develop new, more realistic models. Outputs will be new software for extracting vascular data from images (e.g., Magnetic Resonance Imaging (MRI), Computed Tomography (CT)), a large database for vascular measurements in plants and animals, identification of branching patterns common to specific taxa or tissues, and new theory. Measurements will include vessel radii, lengths, branching ratios, and branching angles. Existing models are contradicted by some preliminary results, including asymmetric branching (two daughter vessels of different sizes and different flow) and deviations from self-similarity (similar branching patterns recurring across scales). New models will be constructed to incorporate these findings and used to predict connections between the geometry of and flow through vascular networks. A key prediction of models will be scaling exponents that describe how the number of capillaries changes with network volume. In an attempt to enable quick translation of branching geometry into predictions for flow rate, techniques will be adapted to classify vascular networks according to a suite of these characteristic scaling exponents.<br/><br/>Closely integrated with these research objectives are three educational goals: 1) teach students how to use image recognition software to extract data from images across biological fields, including museum collections and labs, 2) teach students how to translate empirical results into equations and test specific, mechanistic hypotheses, and 3) motivate data sharing with the public and scientific community via websites for large, comprehensive databases that will accelerate scientific research and community outreach. High school, undergraduate, and graduate students, as well as postdoctoral researchers will all be trained during this research. Notably, summer research experiences for three high school students will be provided for each year of the project. A critical aspect of this training is in teaching students how to combine theory and empirical data and how to disseminate research findings through publications, websites, curricular materials, and talks at professional meetings as well as outreach. The PI has experience educating and training students at all levels and will actively recruit students from under-represented groups to engage in informatics research. Publications, databases, code, and curricular materials will all be made available through websites. Together, this work will provide an example of how computer vision techniques can be used to extract data from biological images around the world."
"1265480","CAREER: The neural mechanisms underlying visual target and task switching","BCS","Cognitive Neuroscience","08/01/2013","09/23/2019","Nicole Rust","PA","University of Pennsylvania","Continuing grant","Kurt Thoroughman","07/31/2020","$482,895.00","","nrust@sas.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","SBE","1699","1045, 1699","$0.00","One of the hallmarks of human intelligence is our ability to flexibly adapt our thinking and behavior based on the overall context of a situation, a trait known as ""cognitive flexibility."" Among the simplest behavioral tasks that require cognitive flexibility are visual ""target switching"" paradigms that involve viewing the same visual stimuli in the context of searching for different targets. As an extension, ""task switching"" paradigms often involve searching for a target based on one stimulus property (e.g., match the shape regardless of color) followed by searching for a different property (e.g., match the color regardless of shape). While interactions between brain areas that lie in the visual, temporal and frontal lobes are thought to play important roles in flexibly switching between targets and tasks, the specific neural mechanisms that allow for cognitive flexibility remain little-understood. Understanding these neural mechanisms has proven to be a considerable challenge, at least in part because the neural activity in the brain areas involved reflect heterogeneous and difficult-to-understand mixtures of different types of information.  With support from the National Science Foundation, Dr. Nicole Rust and colleagues will record neural signals as subjects perform target and task switching paradigms and they will then use newly-developed computational data analysis techniques to tease apart the neural mechanisms that the brain uses to flexibly switch between targets and tasks.  <br/><br/>An array of disorders including obsessive compulsive disorder and autism have been linked to deficits in the mechanisms underlying cognitive flexibility, thus developing a basic understanding of how these mechanisms function is likely to be important for developing treatments to address the disorders that arise when these mechanisms go awry. Additionally, a basic understanding of the neural mechanisms underlying cognitive flexibility has the potential to benefit the robotics and computer vision communities interested in constructing artificial systems that can flexibly switch between targets and tasks to assist humans.  Motivated by the notion that the experience of scientific discovery is one that cannot be fathomed through classroom experiences alone, the results of this project will also be incorporated into an undergraduate educational course focused on analyzing neural data.  Finally, Dr. Rust will participate in the big data effort by making the data available to support other coordinated NSF efforts that aim to make use of real data in the teaching of STEM related courses and to enable participation in discovery science by those who would otherwise have no access to such data."
"1344803","SCH: INT: Collaborative Research: A Self-Adaptive Personalized Behavior Change System for Adolescent Preventive Healthcare","IIS","Smart and Connected Health","10/01/2013","07/05/2016","James Lester","NC","North Carolina State University","Standard Grant","Sylvia Spengler","09/30/2018","$984,818.00","","lester@csc.ncsu.edu","2701 Sullivan DR STE 240","Raleigh","NC","276950001","9195152444","CSE","8018","8018, 8062, 9251","$0.00","The majority of morbidity and mortality during adolescence is preventable and related to behaviors such as substance use and vehicle-related injuries. Most adolescents visit a healthcare provider once a year, providing an ideal opportunity to integrate behavioral health screening into clinical care. Although the majority of adolescent health problems are amenable to behavioral intervention, few health information technology interventions have been integrated into adolescent care. With complementary theoretical advances (social-cognitive theories of behavior change) and technology advances (intelligent narrative-centered learning environments, user modeling, and machine learning), the field is now well positioned to design health behavior change systems that can realize significant impacts on behavior change for adolescent preventive health. <br/><br/>Computationally-enabled models of behavior change hold significant promise for adolescent healthcare. The objective of the proposed research is to design, implement, and investigate INSPIRE, a self-adaptive personalized behavior change system for adolescent preventive health. INSPIRE will utilize a social-cognitive theory of behavior change built around a tight feedback loop in which a narrative-centered behavior change environment will produce improved behaviors in patients, and the resulting patient outcome data will be used by a reinforcement learning optimization system to learn refined computational behavior change models. With a focus on risky behaviors and an emphasis on substance use, adolescents will interact with INSPIRE to develop an experiential understanding of the dynamics and consequences of their substance use decisions. A unique feature of INSPIRE afforded by recent advances in machine learning will be its ability to optimize health behavior change at both the individual and population levels. At the individual level, INSPIRE will utilize a patient behavior model to personalize its behavior change narratives for individual adolescents. It will customize interactions based on an adolescent's goals and affective models. At the population level, INSPIRE will utilize reinforcement learning to adapt its narrative generation system to systematically increase its ability to improve two types of outcomes: behavior change and self-efficacy. The project will culminate with an experiment conducted with a fully implemented version of INSPIRE at outpatient clinics within the UC San Francisco Department of Pediatrics, Benioff Children's Hospital. <br/><br/>It is anticipated that INSPIRE interventions will yield two types of outcomes: 1) improved health behavior through significant reductions in adolescent risky behavior, relative to standard of care; and 2) increased self-efficacy with respect to adolescents' ability to make good decisions about their health behaviors, relative to standard of care. Designed for natural integration into clinic workflow, interoperability with EHR and patient portal systems, and security and privacy requirements, INSPIRE will report patient behavior change summaries to healthcare providers. Through multi-platform deployments supporting laptop, desktop, tablet, and mobile computing devices, INSPIRE will serve as an empowering tool for adolescents, making them full participants in their own wellbeing. It will also enable researchers to run behavior analytics to investigate which properties of alternate interventions contribute most effectively to behavior change outcomes. Going forward, it is anticipated that INSPIRE will provide a testbed for a broad range of behavior change research and serve as the foundation for next-generation personalized preventive healthcare through computationally-enabled behavior change."
"1319140","CIF: Small: Sampling and Inference Methods for Spatiotemporal Single-Photon Imaging","CCF","SIGNAL PROCESSING","07/01/2013","06/19/2013","Yue Lu","MA","Harvard University","Standard Grant","Richard Brown","12/31/2017","$416,517.00","","yuelu@seas.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","7936","7923, 7936","$0.00","Recent advances in materials, devices and fabrication technologies have led to an emerging class of  solid-state sensors that can detect individual photons in space and time. Thanks to their single-photon sensitivity, ultra-fast speed, and rapidly increasing spatial resolutions, these new single photon sensors (SPS) have great potentials in a wide range of applications, including fluorescence-based bio-imaging, time-of-flight 3D computer vision, high-speed videography, and astronomy. The goal of this project is to build rigorous signal processing foundations for optical imaging based on the emerging SPS scheme.<br/><br/>Analogous to silver-halide grains in photographic film, each pixel of the SPS has a binary response, revealing only one-bit and stochastic information of the local light intensity. With an array of pixels and high temporal sampling rates, the SPS generates a massive spatiotemporal volume of bits that sample and encode the original visual information. The investigator studies models, theory, and algorithms in signal sampling and inference to address the challenges associated with the SPS. Specific objectives of this project involve: (1) establishing sampling theorems for the SPS in acquiring light intensity fields of given spatiotemporal bandwidths; (2) identifying performance bounds and the precise tradeoffs between imaging performance and key device metrics; (3) designing adaptive sensing schemes to improve imaging performances; and (4) developing both offline and online image formation algorithms that can efficiently ""decode"" the massive bitstreams generated by the SPS."
"1302700","RI: Medium: Collaborative Research: Object and Activity Recognition as the Maximum Weight Subgraph Problem with Mutual Exclusion Constraints","IIS","Robust Intelligence","09/01/2013","06/20/2016","Sinisa Todorovic","OR","Oregon State University","Continuing grant","Jie Yang","08/31/2018","$500,000.00","","sinisa@eecs.oregonstate.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","CSE","7495","7495, 7924","$0.00","It has been widely acknowledged that recognizing objects in images, and human activities in video - the basic problems in computer vision - can be significantly improved by accounting for object (activity) parts, context, and their spatiotemporal relationships. This is because these constraints facilitate resolving ambiguous hypotheses in the face of uncertainty. Since parts and contexts can be efficiently modeled by graphical models (e.g., Conditional Random Field), object and activity recognition are often formulated as probabilistic inference of graphical models. The project develops a new theoretical framework of graphical models that explicitly encodes high-order, spatiotemporal, hierarchical, and contextual interactions among objects (activities) as Quadratic Mutual-Exclusion Constraints (QMCs), for the purposes of object and activity recognition in images and video.<br/><br/>The key contributions of the project work include: 1) Approaches to view-invariant object and activity recognition; 2) Formulations of learning and inference of graphical models representing objects and human activities, as finding a maximum weight subgraph (MWS) under the QMCs; 3) Polynomial-time algorithms for solving the MWS problem subject to QMCs; and 4) Explicit performance bounds and theoretical guarantees of tightness and convergence of the proposed learning and inference algorithms. <br/><br/>The project framework encodes hard constraints from the domain of interest that have never been used in prior work, and uses principled, polynomial-time algorithms for learning and inference. The research of this project advances the state of the art in object and activity recognition, and enables new applications including video surveillance, retrieval from large datasets, and perception of mobile robots."
"1302164","RI: Medium: Collaborative Research: Object and Activity Recognition as the Maximum Weight Subgraph Problem with Mutual Exclusion Constraints","IIS","Robust Intelligence","09/01/2013","03/22/2016","Longin Jan Latecki","PA","Temple University","Continuing Grant","Jie Yang","08/31/2019","$525,800.00","","latecki@temple.edu","1801 N. Broad Street","Philadelphia","PA","191226003","2157077547","CSE","7495","7495, 7924, 9251","$0.00","It has been widely acknowledged that recognizing objects in images, and human activities in video - the basic problems in computer vision - can be significantly improved by accounting for object (activity) parts, context, and their spatiotemporal relationships. This is because these constraints facilitate resolving ambiguous hypotheses in the face of uncertainty. Since parts and contexts can be efficiently modeled by graphical models (e.g., Conditional Random Field), object and activity recognition are often formulated as probabilistic inference of graphical models. The project develops a new theoretical framework of graphical models that explicitly encodes high-order, spatiotemporal, hierarchical, and contextual interactions among objects (activities) as Quadratic Mutual-Exclusion Constraints (QMCs), for the purposes of object and activity recognition in images and video.<br/><br/>The key contributions of the project work include: 1) Approaches to view-invariant object and activity recognition; 2) Formulations of learning and inference of graphical models representing objects and human activities, as finding a maximum weight subgraph (MWS) under the QMCs; 3) Polynomial-time algorithms for solving the MWS problem subject to QMCs; and 4) Explicit performance bounds and theoretical guarantees of tightness and convergence of the proposed learning and inference algorithms. <br/><br/>The project framework encodes hard constraints from the domain of interest that have never been used in prior work, and uses principled, polynomial-time algorithms for learning and inference. The research of this project advances the state of the art in object and activity recognition, and enables new applications including video surveillance, retrieval from large datasets, and perception of mobile robots."
"1419433","CAREER: Toward Discovering the 3D Geometrical and Semantic Structure of Objects and Scenes","IIS","Robust Intelligence","10/01/2013","05/04/2015","Silvio Savarese","CA","Stanford University","Continuing grant","Jie Yang","12/31/2015","$259,472.00","","ssilvio@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","7495","1045","$0.00","This project develops a novel framework for jointly understanding the 3D spatial and semantic structure of complex scenes from images. The state-of-the-art computer vision methods deal with these two tasks separately. Methods for object recognition typically describe the scene as a list of class labels, but are unable to account for the 3D spatial structure. Methods for scene 3D modeling produce accurate metric reconstructions but are unable to infer the semantic content of its components. This project seeks to fill this gap and creates the foundations for a new framework for coherently describing objects, object components and their 3D spatial arrangement in the scene's physical space.  The research of this project makes two main contributions. First, novel models for representing the intrinsic multi-view nature of object categories and for measuring critical object geometrical attributes are explored. Second, a new coherent probabilistic formulation that is capable to use these measurements for simultaneously estimating the most likely 3D configuration of scene elements and the critical semantic phenomena of the scene are investigated. This research has potential to play a transformative role in many strategic areas such as autonomous navigation, robotics, and 3D automatic modeling of urban environments. Moreover, it is crucial in designing technology for assisting people with reduced functional capabilities.  The project integrates research and education by involving undergraduates or high school students in projects whose primary application goal is to develop technology for people with disabilities."
"1248967","SBIR Phase I: Real-time, Low Cost Point-and-Shoot 3D Camera","IIP","SMALL BUSINESS PHASE I","01/01/2013","07/11/2013","Christopher Slaughter","TX","Lynx Laboratories Inc.","Standard Grant","Muralidharan S. Nair","06/30/2013","$179,990.00","","chris.c.slaughter@gmail.com","3925 Braker Lane","Austin","TX","787593925","6508234072","ENG","5371","5371, 6840, 8033, 9139, HPCC","$0.00","This Small Business Innovation Research (SBIR) Phase I project aims to build a low cost, real-time 3D camera. The research objectives are to prototype a portable, compact and low power camera capable of capturing 3D models of indoor scenes. This research will use techniques from computer vision, robotics and high dimensional statistics and signal processing in order to achieve its goals. The novelty of the research is in bringing together cutting-edge ideas from these diverse disciplines to solve real-world problems in 3D modeling. The anticipated technical results include a working prototype that enables real-time operation without any human intervention.<br/><br/>The broader impact/commercial potential of this project includes the architecture, engineering and construction (AEC) industry. An industry where mistakes can prove costly both in terms of money and lives, our research can enable real-time tracking of the construction process, enabling workers, architects and engineers to ensure that mistakes are detected and corrected promptly during construction, and <br/>historical monuments are preserved over time as 3D models. Additionally, this technology can aid educational activities in classrooms, help with remote surveillance etc., with multitude of applications to impact people from all walks of life."
"1301171","RET in Engineering and Computer Science Site for Machine Learning, Big Data and Computer Science Principles","CNS","RES EXP FOR TEACHERS(RET)-SITE","07/01/2013","03/01/2013","Jeffrey Popyack","PA","Drexel University","Standard Grant","Harriet Taylor","12/31/2016","$499,989.00","Mary Jo Grdina","Popyack@Drexel.edu","1505 Race St, 10th Floor","Philadelphia","PA","191021119","2158955849","CSE","1359","1359","$0.00","This award establishes a new Research Experiences for Teachers site at Drexel University. The Drexel team plans to create a summer research institute in computer science for STEM high school teachers and 2-year college faculty in the City of Philadelphia and Greater Philadelphia Metropolitan Area.   Teachers will spend the time performing research with faculty and students in the associated labs and initiating work on educational modules for use at their home institutions. They will prepare poster presentations and videos summarizing their projects, initial accomplishments, further expectations and descriptions of their modules.   Participants will remain engaged year-round by participating in Teacher2Teacher discussion groups, facilitated by the Math Forum@Drexel. They will also visit campus for quarterly meetings, culminating in a 1-day showcase event in the spring to present results, materials and posters, and to which their colleagues and students are invited.  The objectives of the program are to build partnerships between high schools, community colleges and the university, introduce teachers to cutting edge research in the computer science community, inform and excite them about computer science principles and computational thinking, produce learning materials for use in high school and community college STEM curricula, and expand the pipeline of students studying STEM and computing curricula in college.<br/><br/>Intellectual Merit: The intellectual merit is in the strong research expertise of the participating faculty in research as well as their significant experience with pre-college education.  The focus is on important themes of big data and machine learning which are areas that are current and of importance all citizens. The translation of the research experiences into modules compatible with Computer Science Principles should also add to the significant body of work linking fundamental computer science to classroom practice and applications.   <br/><br/>Broader Impacts: The project makes significant outreach to the greater Philadelphia metropolitan area, with a population exceeding 5 million, and the eighth largest school district in the USA. The Math Forum involves a substantial community of mathematics teachers and professionals, providing a variety of services and resources. Through the partnership with these organizations and dissemination through other widely used services, the team expects to provide significant impact for these activities."
"1320083","RI: Small: Functional Scene Representation for Image Understanding","IIS","Robust Intelligence","08/15/2013","08/19/2013","Abhinav Gupta","PA","Carnegie-Mellon University","Standard Grant","Jie Yang","07/31/2017","$450,000.00","","abhinavg@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7495","7495, 7923","$0.00","What does it mean to ""understand"" an image? One popular answer is simply naming the objects seen in the image. During the last decade most computer vision researchers have focused on this ""object naming"" problem. While there has been great progress in detecting things like ""cars"" and ""people"", such a level of understanding still cannot answer even basic questions about an image such as ""What is the geometric structure of the scene?"" ""Where in the image can I walk""?<br/> <br/>The goal of this project is to develop a geometric and functional representation of our visual world for scene understanding. This project aims to develop this functional representation by learning relationships between the physical/visual representation of the scene and the space of the interactions an agent can perform in that scene. The key advantage of functional scene representation is that it is subjective, explicitly task-based and takes into account the agent?s capabilities.<br/> <br/>This project is anticipated to result in major advances within the image understanding community, bringing it closer to researchers in robotics. It is anticipated to result in improvements in: (a) 3D Scene Understanding; (b) Recognition; (c) Human Activity Understanding, and hence could be a critical enabling technology for applications such as autonomous systems, surveillance, and personal robotics. This project is also expected to contribute to education through course development, student projects, workshops, and tutorials involving a broader audience as well as using popular online media (e.g., YouTube) and interactive web demos to involve young children."
"1344201","INSPIRE Track 1: UDiscoverIt: Integrating Expert Knowledge, Constraint-Based Reasoning and Learning to Accelerate Materials Discovery","IIS","OFFICE OF MULTIDISCIPLINARY AC, Information Technology Researc, SOLID STATE & MATERIALS CHEMIS, Info Integration & Informatics, INSPIRE","09/15/2013","07/16/2014","Carla Gomes","NY","Cornell University","Continuing grant","Sylvia Spengler","08/31/2017","$699,986.00","Francis DiSalvo, Bart Selman, Robert van Dover","gomes@cs.cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","1253, 1640, 1762, 7364, 8078","1640, 7364, 8653","$0.00","This INSPIRE award is partially funded by the Information Integration and Informatics Program in the Division of Information and Intelligent Systems in the Directorate for Computer and Information Science and Engineering and the Solid State and Materials Chemistry  Program in the Division of Materials Research and the Office of Multidisciplinary Activities in the Directorate for Mathematical and Physical Sciences.<br/><br/>The past two decades have seen a rapid development in experimental high-throughput experimentation (HTE) methodologies that would be extremely valuable for (i) the discovery of new applied materials with high complexity and (ii) the generation of deep understanding of structure/function, structure/activity and structure/performance relationships. Especially high photon flux X-ray techniques have enormous transformative potential in materials discovery. The research team leverages the data being collected by the Cornell High Energy Synchrotron Source (CHESS) and at Caltechs Joint Center for Artificial Photosynthesis (JCAP). While high-throughput inorganic library synthesis is relatively well-established, high-throughput structure determination, which is at the heart of the proposed research, is in its infancy. X-ray diffraction is well-suited for rapidly collecting information on the atomic arrangements in an inorganic sample, but the data do not immediately reveal a crystal structure. The development of  data analysis, data mining and interpretation methodologies has not kept pace with the development of experimental capability. Consequently, data acquired in a week can take many months of traditional analysis by researchers. Automation and machine-intelligent processing of the data are absolutely necessary to maximise the impact of complex multidimensional datasets. <br/><br/>This project addresses this state of affairs head-on; It investigates computational techniques that allow dealing with the multiparameter space associated with HTE structure determination of materials libraries, through constraint guided search adn optimization, statistical machine learning, and inference techniques in combination with  direct human input into the process. Anticipated advances include new probabilistic methods and computational discovery tools that integrate soft and hard constraints that capture the complex background knowledge from the underlying physics and chemistry of materials with insights gained from high throughput data analytics and machine learning.   If the project succeeds in achieving the anticipated enormous efficiency gains in complex structure determination, it could have have a transformative impact on  materials discovery and complex solid state chemistry and physics. <br/><br/>The ability to reduce complex materials dicovery and optimization from  timeframes of months or years to hours or days could lead to a paradigm shift in the development of products benefiting society, with technological advances as well as commercial impact on energy, sustainability, health and quality of life. The planned free dissemination of data sets and computational tools to the larger scientific community is likely to enhance the broader impacts of the project. The project facilitates increased interdisciplinary interactions between computer scientists and material scientists at Cornell  University and offer enhanced opportunities for training of a new generation of researchers at the interface between the two disciplines."
"1300720","A Crowdsourced Knowledge Base for the Damage Assessment of Extreme Events","CMMI","HDBE-Humans, Disasters, and th","07/01/2013","04/16/2015","Thomas Oommen","MI","Michigan Technological University","Standard Grant","Robin Dillon-Merrill","06/30/2018","$335,030.00","","toommen@mtu.edu","1400 Townsend Drive","Houghton","MI","499311295","9064871885","ENG","1638","036E, 041E, 042E, 043E, 116E, 1576, 9102, 9178, 9231, 9251","$0.00","The use of crowdsourced volunteers to analyze remote sensing imagery is a relatively new damage assessment approach, developed in the wake of the 2008 Sichuan earthquake, and formalized during the 2010 Haiti and 2011 New Zealand earthquakes. This approach is enabled by the advent of Web 2.0 technologies and the ubiquity of free remote sensing images that are synoptic with high spatial-, spectral- and temporal-resolutions. The demonstrated benefit was a speedup by a factor of two or three in the delivery of damage estimates. However, the success of this manual crowdsourced approach for damage assessment is dependent upon the size and reliability of the crowd. This research will focus on a new framework called BACKBOnE (Building A Crowdsourced Knowledge Base of Extreme Events) for extreme event damage assessment utilizing remotely sensed images that automatically finds and classifies damages, and builds a data-driven knowledge base of damage characteristics that can be reused during future events. BACKBOnE is a transformation of the manual crowdsourced approach for damage assessment and is unprecedented in the disasters community, combining the power of crowdsourcing with state-of-the-art methods from computer science and image processing. It replaces the manual effort with automated methods for object-based change detection and classification that increase the speed, reduce the cost of damage assessment, and scale well to increases in data volume. It shifts the crowd from its task of manual annotation to quality assurance feedback on the performance of automated methods via crowdsourced active learning. This improves assessment accuracy, while decoupling the framework's success from the size and reliability of the crowd because feedback is solicited from annotators scored favorably, and only on difficult cases.  It also incorporates a multitude of remote sensing products and performs data fusion to unify their outputs into a common map of damage.  This is a must-have characteristic of next-generation damage assessment as data volumes and products proliferate. The use of diverse data products, particularly imagery from high spatial resolutions and non-visible bands that are less sensitive to weather and solar illumination, will better discriminate certain damage types.<br/><br/>The broader impact of the research is the reduction of the overall human and financial cost of extreme events by contributing new methods for rapid and accurate damage estimates used for Post-Disaster Needs Assessment (PDNA). The curation of a knowledge base builds effective models quickly when a disaster strikes, refines damage predictions in event simulations that assess vulnerability, and fosters better land-use planning that encourages the growth of disaster resilient communities. The work also includes a web-based damage assessment simulator that maps remotely sensed earthquake images from recent earthquake events to engage the greater public in disaster mitigation. In addition, the investigators will actively recruit graduate and undergraduate students from under-represented groups and mentor them within a multi-disciplinary collaboration. Machine learning students will learn about remote sensing and damage assessment, and geoengineering students will learn fundamentals of machine learning and statistical data analysis."
"1308340","Eigenvectors of random graphs, random matrices and triple collisions","DMS","PROBABILITY","08/15/2013","07/30/2013","Soumik Pal","WA","University of Washington","Standard Grant","Tomek Bartoszynski","07/31/2017","$145,002.00","","soumikpal@gmail.com","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","MPS","1263","","$0.00","The PI proposes two major themes of research for the next three years. One is the study of eigenvectors of sparse but large random graphs. Although such eigenvectors are of both theoretical and applied interest, very few results are rigorously known about them. The PI proposes to use techniques from Random Matrix Theory and combinatorics to continue his investigation of spectral properties of sparse random graphs. A particular goal will be to study the effect of cycles in the graph on the localization/ delocalization property of the eigenvectors. The other line of work is related to models used in mathematical finance. Particle system models from statistical physics have been recently successfully used to explain observed financial data and construct portfolios that can be thought of as arbitrage opportunities with respect to the market index. The PI proposes, on the one hand, to further study the properties of these processes as particle systems, and on the other, collaborate with industry groups to help them build more efficient portfolios.  <br/><br/>The mathematics we study has the following broader impact. Random graphs and networks are popular in diverse areas such as social networks, models for the internet, computer vision, and number theory. Several natural optimization problems on graphs (e.g., figuring out clusters, or ranking algorithms such as Google PageRank) involve what are called eigenvectors of the graph. If the network grows randomly, its eigenvectors are random, and it is of interest how they behave. The proposed work in finance is useful in building portfolios that gain from the presence of volatility in financial market. As such it is of great interest to practitioners who would like to harvest growth out of market fluctuations. The proposed research is a further study in an area which is already being applied by portfolio managers, some of whom are in consultation with the PI and his students."
"1407174","quantizing Schur functors","DMS","Combinatorics","09/01/2013","04/08/2014","Jonah Blasiak","PA","Drexel University","Standard Grant","Tomek Bartoszynski","06/30/2016","$73,923.00","","jblasiak@gmail.com","1505 Race St, 10th Floor","Philadelphia","PA","191021119","2158955849","MPS","7970","9251","$0.00","Geometric complexity theory is an approach to P versus NP and related problems in complexity theory using algebraic geometry and  representation theory.  A fundamental problem in representation theory, believed to be important for this approach, is the Kronecker problem, which asks for a positive combinatorial formula for decomposing the tensor product of two irreducible representations of the symmetric group into irreducibles.  The theory of quantum groups and crystal bases, which has been actively developed over the last several decades, is a powerful tool for connecting combinatorics and representation theory.  In the last decade, there have been several attempts, by Berenstein, Zwicknagl, Mulmuley, Sohoni, and others, to use this tool to study the Kronecker and related plethysm problems.  Recently, the investigator and his collaborators Mulmuley and Sohoni have obtained the beginnings of a theory of crystal bases for the Kronecker problem.  The main goal of this project is to push this approach further.  More broadly, this project aims to further explore the potentially deep connections between complexity theory and positivity in algebraic combinatorics as has been initiated by geometric complexity theory.<br/><br/>Objects arising in algebra are typically complicated, mysterious, and have many symmetries.  Algebraic combinatorics is the study of counting and organizing such objects.  This project will explore some surprising connections between this area and complexity theory (the study of algorithms and their limitations).  The tensor decomposition problem is an important and difficult problem that shows up in many fields, including algebraic combinatorics, complexity theory, and statistics, and has applications in medicine, computer vision, chemistry, and fast matrix multiplication.  Essentially, it is the problem of recovering individual signals from a mixture of signals.  This project offers potential new insights into this problem by applying powerful tools from algebraic combinatorics."
"1360562","Hybrid 4-Dimensional Augmented Reality Environments for Ubiquitous Markerless Context-Aware AEC/FM Applications","CMMI","CIVIL INFRASTRUCTURE SYSTEMS","08/01/2013","09/26/2013","Mani Golparvar-Fard","IL","University of Illinois at Urbana-Champaign","Standard Grant","Dennis Wenger","07/31/2015","$273,840.00","","mgolpar@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","ENG","1631","029E, 036E, 039E","$0.00","The objective of this project is to test whether a framework proposed by the PIs can in near real-time read, write, and receive feedback from a model which fuses pictures from mobile devices and Building Information Models for the purpose of providing ubiquitous and marker-less contextual awareness for Architecture/ Engineering/ Construction and  Facility Management (AEC/FM) applications. According to the framework, field personnel can use mobile devices to take pictures that include specific project elements (e.g., column), touch or click on the elements in the image, and be presented with (or be able to add) a detailed list of information, such as architectural/structural plan related to the physical elements. The mobile device can use onboard GPS and other sensors to perform a rough calculation of the device's field-of-view and location. Initial image processing is done on the mobile device to extract and send feature points/descriptors, field-of-view, and location to the Hybrid 4-dimensional Augmented Reality (HD4AR) server. Based on a new computer vision method, the server uses this information from the phone to derive the mobile device's position at a resolution that is an order of magnitude more accurate than with current approaches based solely on GPS. The server uses the derived high-precision camera position to determine what cyber-information is in view of the device's camera. The extracted information, along with pixel coordinates of where each cyber-information item should appear in the photo, is returned to the mobile device and visualized in augmented reality format. <br/><br/>If successful, the results of this research will provide the first feasible platform for context aware applications which does not require reliable and high accurate GPS/sensor-based location and orientation tracking and works based on existing image collections. It further assists field personnel through visualization of queried plan and actual site information in form of augmented reality, and supports interactions among project personnel and field information. By providing immediate access to information, the proposed framework automatically provides inexpensive, global and frequent reports from the field activities, and in turn can reduce downtime, rework, waste, and ultimately cost overrun. This project also involves educational and outreach activities to promote teaching and learning, engage undergraduate and graduate students, and reach out to underrepresented groups, K-12 students, and industry professionals. These activities include development of two course modules of ""visual sensing for civil infrastructure engineering and management"" and ""mobile cyber-physical systems,"" as well as creating new software tools and hands-on outreach materials for context aware AEC/FM applications, which will be widely distributed among research and professional communities."
"1347308","Information Procuration via Adaptive Algorithms","CCF","ALGORITHMS","09/01/2013","08/21/2013","Ramamoorthi Ravi","PA","Carnegie-Mellon University","Standard Grant","Balasubramanian Kalyanasundaram","08/31/2014","$99,714.00","","ravi@cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7926","7916, 7926","$0.00","The Principal Investigator (PI) formulates the information procuration problem of converting unstructured data into structured information as one of using limited resources (such as processing time and collection costs) among several available strategies for information acquisition, extraction, collation and aggregation in a sequential and adaptive manner. The proposal aims to build a Markov decision process (MDP) for which both the states and the rewards will be learned, and from which an optimal adaptive strategy for effective information procuration will be extracted.<br/><br/>Recent methods for designing adaptive strategies for multi-armed bandit problems and budgeted learning approaches by the PI will be extended for this purpose, as well as techniques from inverse reinforcement learning. Moreover, given the intended size of the application data sets, the focus will be on on scalable algorithms for these problems. Due to the centrality of the problem, new approaches to making better sense of unstructured data will have much impact both in terms of developing new methods and in practice. The proposed synthesis of methods from Operations Research, Approximation Algorithms and Machine Learning is novel in this context. This proposal will increase the cross-fertilization of ideas between Operations Research and Machine Learning, via a collaboration team formed at this intersection."
"1343860","EAGER: Nonlinear and Data-Adaptive Compressive Sampling for Big Data Processing","ECCS","COMMS, CIRCUITS & SENS SYS","09/01/2013","06/17/2013","Konstantinos Slavakis","MN","University of Minnesota-Twin Cities","Standard Grant","Chengshan Xiao","03/31/2016","$160,000.00","","kslavaki@buffalo.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","ENG","7564","154E, 7916, 8084","$0.00","As pervasive sensors continuously collect and record massive amounts of<br/>high-dimensional data from communication, social, and biological networks,<br/>and growing storage as well as processing capacities of modern computers<br/>have provided new and powerful ways to dig into such huge quantities of<br/>information, the need for novel analytic tools to comb through these ""big<br/>data"" becomes imperative. The objective of this project is to develop a<br/>novel framework for nonlinear, data-adaptive (de)compression algorithms to<br/>learn the latent structure within large-scale, incomplete or corrupted<br/>datasets for compressing and storing only the essential information, for<br/>running analytics in real time, inferring missing pieces of a dataset, and<br/>for reconstructing the original data from their compressed renditions. <br/><br/>The intellectual merit lies in the exploration of the fertile but largely<br/>unexplored areas of manifold learning, nonlinear dimensionality reduction,<br/>and sparsity-aware techniques for compression and recovery of missing and<br/>compromised measurements. Capitalizing on recent advances in machine<br/>learning and signal processing, differential geometry, sparsity, and<br/>dictionary learning are envisioned as key enablers. Effort will be put also<br/>into developing online and distributed (non)linear dimensionality reduction<br/>algorithms to allow for streaming analytics of sequential measurements<br/>using parallel processors. <br/><br/>The broader impact is to contribute to the development of novel <br/>computational methods and tools useful for data inference, cleansing, <br/>forecasting, and collaborative filtering, with direct impact to <br/>statistical signal processing and machine learning applications<br/>to large-scale data analysis, including communication, social, and<br/>biological networks."
"1308872","Structural-Information Enhanced Inference for Large-Scale and High-Dimensional Data","DMS","STATISTICS","08/01/2013","07/11/2013","Chunming Zhang","WI","University of Wisconsin-Madison","Standard Grant","Gabor J. Szekely","07/31/2016","$130,001.00","","cmzhang@stat.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1269","","$0.00","A fundamental research issue for large-scale and high-dimensional inference procedures is how can we incorporate the structural information from the data to enhance large-scale computing, machine learning and statistical inference. Learning and exploiting such structure is crucial towards better analysis of complex datasets. This proposal aims to incorporate important structures of the data and association among feature variables and the response variable into devising efficient experimental approaches and algorithms for large scale problems and understanding theoretical properties of the procedures. Project 1 develops a feature screening approach in the high dimension, low sample size paradigm which takes into account the correlation structure among the features. The PI proposes a framework of inference for selecting feature variables relevant to the response variable. In the context of large-scale simultaneous inference, the hypotheses are often accompanied with certain structural prior information. Project 2 proposes a new multiple testing procedure, which maintains control of the false discovery rate while incorporating the prior information. Large-scale multiple testing tasks often exhibit dependence, and leveraging the dependence among individual tests is an important but challenging problem in statistics. Project 3 proposes a multiple testing procedure which allows general dependence structures and heterogeneous dependence parameters.<br/><br/>This proposal aims to tackle some challenging research problems, arising from frontiers of biological, medical and scientific research, with a common theme of exploiting structural information in high-dimensional data. New tools for stochastic modeling, computational algorithms, parameter learning, and statistical inference applied to large-scale and high-dimensional data, for example, brain fMRI imaging data and datasets from genome-wide association studies on breast cancer, will be developed. Dissemination of these developments will enhance new knowledge discoveries, and strengthen interdisciplinary collaborations. The research will also be integrated with educational practice through multi-disciplinary courses on the contemporary state-of-the-art data mining and machine learning, and benefit the training and learning of undergraduate, graduate students and underrepresented minorities."
"1331620","Rational randomness: Search, sampling and exploration in children's causal learning.","BCS","DS -Developmental Sciences","09/15/2013","07/18/2016","Alison Gopnik","CA","University of California-Berkeley","Standard Grant","Chalandra Bryant","08/31/2017","$446,815.00","Thomas Griffiths","gopnik@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","SBE","1698","1698","$0.00","How do young children learn so much about the world so quickly and accurately? And how can they learn so much when, at the same time, they often seem so irrational and unpredictable? The research in this proposal will help answer these questions by bringing together ideas from computer science with research on very young children. The basic idea is that young children learn in some of the same ways as the most powerful machine-learning programs. Both the children and the computers explore a wide range of more and less likely possibilities. Moreover, children may sometimes actually explore more widely than adults and so be smarter or at least more open-minded learners. Some of their apparently irrational play, like their wide-ranging pretend play, may really reflect powerful learning methods.<br/><br/>This work should have significant broader impact for educational practice. If we understand children's basic natural rational learning mechanisms we can use those mechanisms to help teach science more effectively. In particular, there are significant practical questions about how we can leverage children's spontaneous play to help them learn.  Similarly, this research has impact for studies of developmental disabilities such as autism and mental retardation. There is reason to think that children with these syndromes may have particular difficulty with the kind of learning about possibilities that is facilitated by pretend play, and understanding that learning may help us understand and remedy these difficulties."
"1302216","SHF: Medium: Collaborative: Transfer Learning in Software Engineering","CCF","SOFTWARE ENG & FORMAL METHODS","07/01/2013","04/19/2013","Timothy Menzies","WV","West Virginia University Research Corporation","Continuing Grant","Sol Greenspan","01/31/2015","$271,553.00","","timm@ieee.org","P.O. Box 6845","Morgantown","WV","265066845","3042933998","CSE","7944","7924, 7944, 9150","$0.00","The goal of the research is to enable software engineers to find software development best practices from past empirical data. The increasing availability of software development project data, plus new machine learning techniques, make it possible for researchers to study the generalizability of results across projects using the concept of transfer learning. Using data from real software projects, the project will determine and validate best practices in three areas: predicting software development effort; isolating software detects; effective code inspection practices. <br/><br/>This research will deliver new data mining technologies in the form of transfer learning techniques and tools that overcome current limitations in the state-of-the-art to provide accurate learning within and across projects. It will design new empirical studies, which apply transfer learning to empirical data collected from industrial software projects. It will build an on-line model analysis service, making the techniques and tools available to other researchers who are investigating validity of principles for best practice. <br/><br/>The broader impacts of the research will be to make empirical software engineering research results more transferable to practice, and to improve the research processes for the empirical software engineering community.  By providing a means to test principles about software development, this work stands to transform empirical software engineering research and enable software managers to rely on scientifically obtained facts and conclusions rather than anecdotal evidence and one-off studies. Given the immense importance and cost of software in commercial and critical systems, the research has long-term economic impacts."
"1252412","CAREER: Distilling information structure from big and dirty data: Efficient learning of clusters and graphs in modern datasets","IIS","Info Integration & Informatics","03/01/2013","04/29/2015","Aarti Singh","PA","Carnegie-Mellon University","Continuing grant","Sylvia Spengler","02/28/2018","$500,000.00","","aartisingh@cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7364","1045, 7364","$0.00","This CAREER project aims to advance the state-of-the-art in theory and methods for extracting clusters and graphs from big and dirty datasets arising in modern application domains. Clusters and graphs provide a meaningful representation of the structure of information contained in data, e.g. in neuroscience and health care domains, clustering patients with similar phenotypes and genotypes helps identify target groups for drug design, clustering fiber tracks generated by high-resolution Digital Surface Imaging (DSI) scans of  brains help identify significant neural pathways, and graph structures can reflect connectivity between brain regions. The results of this work will significantly enhance the ability to exploit such modern datasets through new methods for learning clusters and graphs from data that is large-scale, high-dimensional, under-sampled, corrupted, and often only available in a compressed or streaming representation. <br/><br/>Specifically, this project will develop computationally efficient and principled methods for learning clusters and graphs that can (i) perform unsupervised feature selection to discard irrelevant features in high dimensions, (ii) leverage feedback based on intelligent adaptive queries that focus resources on most informative variables and features, (iii) use compressive measurement design that adapts to the information structure for measurement and computation efficiency, and (iv) be able to handle noisy streaming data. The algorithms will be accompanied with performance guarantees in the form of a precise characterization of the mis-clustering rate and graph recovery error. Additionally, the project will investigate the tradeoffs between number of measurements, computational complexity and robustness in these problems. The methods and theory developed will be evaluated through simulations as well as their applicability to real datasets in neuroscience and healthcare domain, in collaboration with practitioners from these fields. <br/><br/>The results of this research could potentially transform many application domains that involve grouping similar variables and learning complex interactions between them, based on big and dirty datasets. In particular, the neuroscience and healthcare applications are likely have very direct and significant implications for society. Accurately mapping neural pathways will help diagnose and treat brain pathologies at an early stage, and help understand brain functioning. Clustering patients and discovering disease spreading pathways based on few measurements of relevant genetic features or indicators could help prevent and cure diseases, and also minimize healthcare costs. The  research activities will be tightly integrated with education efforts that aim to develop a diverse workforce that is better equipped with cross-disciplinary tools to address the challenges of modern datasets. The education plan includes development of two inter-disciplinary courses, and enhancement of the joint Statistics & Machine Learning PhD program at Carnegie Mellon University (CMU). Outreach activities include promoting undergraduate research, broadening participation of women and underrepresented groups in STEM fields through OurCS (Opportunities for Undergraduate Research in Computer Science), Andrew?s Leap (a summer enrichment program for area high school and middle school students) and CS4HS program aimed at High School and K-8 teachers at Carnegie Mellon University. The results of this project (including publications, data sets, and software) will be disseminated online at http://www.cs.cmu.edu/~aarti/research_projects/.<br/>"
"1302662","RI: AF: Medium: Learning and Matrix Reconstruction with the Max-Norm and Related Factorization Norms","IIS","Robust Intelligence, NUM, SYMBOL, & ALGEBRA COMPUT","06/01/2013","08/30/2018","Nathan Srebro","IL","Toyota Technological Institute at Chicago","Continuing grant","Rebecca Hwa","03/31/2019","$915,986.00","Yury Makarychev","nati@ttic.edu","6045 S Kenwood Ave","Chicago","IL","606372803","7738340409","CSE","7495, 7933","7495, 7924, 7933, 9251","$0.00","Matrix learning is fundamental in many learning problems.  These include problems that can be directly formulated as learning some unknown matrix, as well as a broader class of learning problems involving a matrix of parameters.  The most direct matrix learning problem is matrix completion, completing unseen entries in a partially observed matrix. Matrix completion has recently received much attention both in practice in collaborative filtering (notably through the Netflix challenge), and theoretical analysis as an extension to compressed sensing.  Matrix learning has also been used for clustering, transfer and multi-task learning, and similarity learning.<br/><br/>The dominant approach to matrix learning in recent years, especially in the context of matrix completion, has used the matrix trace norm (developed in part by the PI on this award).  Indeed, trace norm-based methods enjoy much success in a variety of applications.  This project develops and studies alternative matrix norms to the trace-norm, most importantly the promising max-norm.<br/><br/>Learning with the max-norm was initially presented in 2004 (along with the trace norm), but has not received the same attention, despite many theoretical and empirical advantages.  This project identifies domains where the max-norm and related norms can be beneficial, develops computational methods for using these norms, and promotes the adoption of these norms.  A central aim is to develop optimization methods for max-norm regularized problems that are as efficient as the corresponding methods for trace-norm regularized problems, such as singular value thresholding and LR-type methods.  Beyond matrix completion, the project applies the max-norm both to problems where the trace-norm has been previously applied, and in novel settings.  Novel applications include clustering, binary hashing, crowdsourcing, modeling rankings by a population, and similarity learning.<br/> <br/>Research under this project links the machine learning and theory-of-computation research communities (where SDP relaxations essentially corresponding to the max-norm have played a significant role in recent years).  The project forms bridges between the communities, enabled in part by cross-disciplinary tutorials.  Through collaboration with sociologists the PIs reach out to the social sciences, and increase the broad impact of the work by presenting it in an approachable and useable way to this audience."
"1348109","CAREER: A Multiagent Teacher/Student Framework for Sequential Decision Making Tasks","IIS","ROBUST INTELLIGENCE","01/02/2013","05/06/2015","Matthew Taylor","WA","Washington State University","Standard Grant","James Donlon","08/31/2018","$421,894.00","","taylorm@eecs.wsu.edu","280 Lighty","PULLMAN","WA","991641060","5093359661","CSE","7495","1045, 7495, 9251","$0.00","Physical (robotic) agents and virtual (software) agents are becoming increasingly common in industry, education, and domestic environments. Although recent research advances have enabled agents to learn how to complete tasks without human intervention, little is known about how best to have humans teach agents or agents teach other agents or even how agents might teach humans. Considering the full matrix of agent/human learning, in which either an agent or a human can play the role of teacher or student, would increase the potential benefits of leveraging human and agent expertise and knowledge. <br/><br/>This project aims to study agent/human learning in the context of sequential decision-making problems, a class of central importance for real-world agent systems. This project aims to develop a novel teacher/student framework that integrates autonomous learning with teaching by another agent or a human. The project plans to develop and evaluate a set of core algorithms to allow: (1) agents to teach agents, thus enabling robust knowledge sharing among agents; (2) humans to teach agents, thus allowing humans to share or transfer common sense or domain-specific knowledge with agents; and (3) agents to teach humans, thus helping humans better understand how to perform or recast sequential decision-making tasks already understood or performed by autonomous agents. In all cases, the goal is to develop methods that significantly improve learning performance relative to learning without guidance from a teacher. Issues to be explored include mismatch between teacher/student abilities, learning from multiple teachers, and shared knowledge representation between teacher/student. The PI plans to focus on several scenarios, each with different sets of assumptions about the knowledge or skill of the student or teacher and the kind of interaction possible between them (e.g., whether the teacher can tell the student what action to take). The techniques developed in the project will be evaluated in a variety of tests domains and will involve simulations as well as actual robots.<br/><br/>The teacher/student framework will enable agents to teach other agents and humans, as well as integrate autonomous learning with agent and human teaching. Understanding how to best teach agents is of key importance in developing deployable agent systems. The platform- and domain-independent approach incorporates ideas from multiagent systems, machine learning, human-computer interaction, and human-robot interaction communities, and has the potential to impact each of these areas. This work takes a step towards transitioning agents from specialized systems usable only by experts into useful tools and teammates for people without programming expertise. <br/><br/>This project has a strong educational component. The PI teaches at an undergraduate college and undergraduate students will play a crucial role throughout the project. Furthermore, the research produced by this project will be incorporated into five of the PI's courses, providing exciting new material to attract and retain computer science majors. The PI will also continue outreach to secondary school students as well as to underrepresented groups via Lafayette College's S-STEM and Higher Achievement programs."
"1302169","SHF: MEDIUM: Collaborative Research: Transfer Learning in Software Engineering","CCF","Software & Hardware Foundation, SOFTWARE ENG & FORMAL METHODS","07/01/2013","06/01/2016","Lucas Layman","MD","Fraunhofer Center for Experimental Software Engineering","Continuing Grant","Sol Greenspan","06/30/2017","$482,852.00","Lucas Layman","laymanl@uncw.edu","5700 Rivertech Court","Riverdale","MD","207371250","3013146070","CSE","7798, 7944","7924, 7944, 9150","$0.00","The goal of the research is to enable software engineers to find software development best practices from past empirical data. The increasing availability of software development project data, plus new machine learning techniques, make it possible for researchers to study the generalizability of results across projects using the concept of transfer learning. Using data from real software projects, the project will determine and validate best practices in three areas: predicting software development effort; isolating software detects; effective code inspection practices. <br/><br/>This research will deliver new data mining technologies in the form of transfer learning techniques and tools that overcome current limitations in the state-of-the-art to provide accurate learning within and across projects. It will design new empirical studies, which apply transfer learning to empirical data collected from industrial software projects. It will build an on-line model analysis service, making the techniques and tools available to other researchers who are investigating validity of principles for best practice. <br/><br/>The broader impacts of the research will be to make empirical software engineering research results more transferable to practice, and to improve the research processes for the empirical software engineering community.  By providing a means to test principles about software development, this work stands to transform empirical software engineering research and enable software managers to rely on scientifically obtained facts and conclusions rather than anecdotal evidence and one-off studies. Given the immense importance and cost of software in commercial and critical systems, the research has long-term economic impacts."
"1320693","EXP: Digital Lofts: Online Learning Environments for Real-World Innovation","IIS","TUES-Type 2 Project, Cyberlearn & Future Learn Tech","09/15/2013","04/21/2015","Matthew Easterday","IL","Northwestern University","Standard Grant","Elliot Douglas","08/31/2016","$564,253.00","Elizabeth Gerber","easterday@northwestern.edu","750 N. Lake Shore Drive","Chicago","IL","606114579","3125037955","CSE","7511, 8020","8045, 8841, 9251","$0.00","In this Cyberlearning: Transforming Education EXP project, researchers focus on developing the civic innovators of the future. To become civic innovators, learners must gain experience tackling complex, ill-structures design challenges that are not easily solved by a single individual within a fixed time frame. Such education is challenging, and these researchers take advantage of Web 2.0, crowdsourcing, badges, and social media to design means of supporting the learning of young engineers as they tackle civic problems.  Supporting learning from design and complex problem solving activities includes providing support for successfully solving problems and achieving goals as well as providing support for reflecting on those experiences to grasp the collaboration and communication skills and begin to learn strategies and tactics for innovating. The Digital Loft being designed for this purpose includes facilities for identifying peer experts who might collaborate on new problems, assigning expertise badges to make it easy to identify potential experts, extracting key design principles from design and problem solving cases and illustrating and indexing them in a case library that is made available as a resource, helping participants identify when they need instruction, and providing means of finding appropriate mentors to provide necessary help. Research focuses on the identifying important characteristics of the human-machine learning ecosystem that promote successful design and problem solving, successful learning, successful innovation, and sustainability of the infrastructure. The research will produce empirically-grounded principles for designing Digital Lofts for civic innovation education and advance understanding of the roles that digital badges, crowd-sourcing, learning by cases, design practice, and social networking can play in promoting innovation learning. Research and development is being done in the context of Design for America, a multi-university collaborative focused on promoting civic innovation among undergraduate engineering students.<br/><br/>There is an urgent need for educating civic innovators who can solve our greatest societal challenges. This project explores the feasibility of a Digital Loft for supporting such education. In such a learning environment, engineering students at the undergraduate level participate in addressing community issues together with both local students and participants in other locations who are solving problems in their own communities. Software tools support the kinds of interactions between student engineers that are needed to promote success and learning and the creation of shared resources that will allow participants to build community knowledge that will help them become better innovators. Research addresses issues in the design and integration of several software functions for successfully promoting innovation education."
"1320586","III: Small: Is Imprecise Supervision Useful? Leveraging Ambiguous, Incomplete or Conflicting Data Annotations","IIS","Info Integration & Informatics","09/01/2013","05/06/2015","Jinbo Bi","CT","University of Connecticut","Continuing grant","Sylvia Spengler","08/31/2018","$351,836.00","","jinbo.bi@uconn.edu","438 Whitney Road Ext.","Storrs","CT","062691133","8604863622","CSE","7364","7364, 7923, 9251","$0.00","Supervised machine learning approaches to building predictive models from data traditionally rely on labeled samples. However, in many real-world applications, samples are either unlabeled or labeled imprecisely labeled, i.e., labels are often ambiguous, conflicting, or incomplete. This presents the problem of learning predictive models under imprecise supervision.<br/><br/>This project aims to develop effective algorithms to address three different scenarios that lead to imprecise supervision (1) multiple labelers with varying expertise are employed to annotate samples; (2) annotated labels are associated with a set of samples instead of an individual sample; (3) annotations are derived by modeling multiple expert assessments. The project introduces a general bi-convex programming, minimax optimization, and multi-objective optimization based framework for learning predictive models from imprecisely labeled data. The resulting algorithms will be evaluated on a number of real-world applications. <br/><br/>Broader Impacts: The results of this research are likely to impact a range of biomedical applications, including medical image labeling, longitudinal behavioral studies, genomics, and drug safety. The project offers enhanced opportunities for curriculum development and research-based advanced training of grauduate amd undergraduate students in machine learning and its applications. Dissemination of open source software implementation of algorithms resulting from the project also contribute to the project's broader impact. Additional information about the project can be found at: http://www.labhealthinfo.uconn.edu/home/MachineLearning.jsp"
"1250956","BIGDATA: Small: Big Data for Everyone","IIS","Big Data Science &Engineering","08/01/2013","07/24/2013","William Cohen","PA","Carnegie-Mellon University","Standard Grant","Sylvia Spengler","05/31/2017","$548,417.00","Tom Mitchell","wcohen@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","8083","7433, 7923, 8083","$0.00","Although big data has had a huge impact in several areas, this impact is limited by the high cost and poor quality of analyzing unstructured data, and the costs of integrating data of multiple types. Lowering these costs will bring the benefits of big data based research to many new areas. Against this background, this project aims to develop machine-learning methods that read, analyze, and integrate web-scale collections of text and other data. The project can be expected to yield fundamental advances in data integration, machine learning, natural language understanding, and automated inference. <br/><br/>The project includes research thrusts in (1) robust semi-supervised bootstrap learning algorithms that can cope with ambiguity in text, (2) algorithms for detecting and aligning the schemas implicit in semi-structured sources relative to a shared common ontology, (3) NLP algorithms that perform deeper analysis on text to extract infrequently mentioned yet important facts, and (4) targeted reading agents  capable of pursuing specific queries or conjectures based on the scientist's current focus. <br/><br/>Anticipated results of the project include fundamental advances in each of the research thrusts and their synergistic integration into software system (NESSIE) designed to help scientists in exploring scientific hypotheses in their respective domains of interest, by supporting targeted extraction of knowledge from large amounts of textual  sources in relevant areas. <br/><br/>Broader impacts of the research include advanced techniques for  extracting and organizing structured knowledge from text, and integrate the learned information with existing structured knowledge in multiple domains. The Additional broader impacts of the research include enhanced opportunities fore advanced research-based training of graduate students. The softare and data resulting from the research will be made freely available to the larger scientific community."
"1320402","RI: Small: Large-scale Probabilistic Forecasting for Energy Systems","IIS","Robust Intelligence","08/01/2013","06/24/2014","Zico Kolter","PA","Carnegie-Mellon University","Continuing grant","Todd Leen","07/31/2015","$236,946.00","","zkolter@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7495","7495, 7923","$0.00","How much power is a set of wind farms likely to generate over the next 24 hours? How will occupants in a commercial building interact to consume energy? Being able to answer prediction questions like these is vital to developing a more sustainable energy infrastructure: If we can predict renewable energy production and demand ahead of time, we can schedule energy resources more efficiently and reliably, leading to significant reductions in greenhouse gas emissions. Unfortunately, these are also inherently uncertain quantities we need to predict; for example, no matter how good our algorithms are, we can't predict human behavior with perfect accuracy. In order to use such predictions, we need to be able to properly model the uncertainty inherent in these domains. We need to make predictions that are not only correct on average, but which capture the complex random fluctuations and correlations between predicted quantities.  Only then can we schedule energy resources in a way that accounts for these uncertainties.<br/><br/>This project develops and uses a recently-proposed framework for modeling --- sparse Gaussian conditional random fields --- a generalization of the commonly used Markov random field. This framework efficiently models high-dimensional distributions by exploiting sparsity in the inverse covariance matrix.  The project extends the state of the art by greatly accelerating model learning, by extending existing theory to understand when these models can effectively learn high-dimensional predictors, and by generalizing the predictions to the non-Gaussian setting through copula methods. The project uses these algorithms to build forecasting models in four crucial domains in the energy sector: energy demand, wind power, user occupancy in homes and commercial buildings, and personal energy consumption from smart meters.<br/><br/>The project has exemplary broader impacts. First, the research deals directly with application domains crucial to efficient energy management, where even small advances can have a sizable impact on sustainability. Second, the PI leverages the research to bring the power systems and machine learning communities closer together, disseminating the results at both machine learning and power systems venues, and releasing material and video lectures to practitioners in energy. Finally, the project harnesses the research to increase diversity within STEM fields by advising under-represented minorities at the graduate and undergraduate level, and by engaging High School students and teachers with talks illustrating how computation can be used to address problems in sustainability."
"1251274","BIGDATA: Small: DA: Classification Platform for Novel Scientific Insight on Time-Series Data","OAC","Big Data Science &Engineering","08/01/2013","07/31/2013","Joshua Bloom","CA","University of California-Berkeley","Standard Grant","Robert Chadduck","07/31/2018","$733,536.00","Fernando Perez","joshbloom@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","8083","7923, 8083","$0.00","BIGDATA: Small: DA: Classification Platform for Novel Scientific Insight on Time-Series Data<br/><br/>Abstract<br/><br/>The deepest insights into the nature of complex physical systems arise from the measurement of how observables of those systems change with time. Such dynamism - witnessed on scales ranging from atomic to Universal - reveals the underlying forces that govern the interaction of the constituents of those systems. The temporal sampling of data from sensors and from simulations, then, may be seen as a primary vector towards the deepest scientific insight. In this respect, mechanisms to quickly and robustly extract and mine knowledge from diverse time-series data can be fundamental tool of modern data-driven science. <br/>     <br/>This project will build a webservice portal for scientific teams to train state-of-the-art machine-learning algorithms on existing data and receive autonomously generated classification statements on new data, whatever the scale. Massive data storage and the scaling/parallelism of computational algorithms (using commodity cloud services) will be abstracted from the end users. The envisioned framework will act both to simplify the algorithm selection and application processes as well as to educate the broad user base in modern machine-learning approaches. <br/>          <br/>This project will lead to the implementation of novel and efficient feature extraction algorithms on irregularly sampled time-series data, and will make them available in the context of a robust and scalable platform integrated with classification and cross-validation, that will lead to informed use of the algorithms for reliable scientific insight. This learning and prediction platform will accelerate data-intensive decision-making, and will be a new data analytics tool for the autonomous discovery of knowledge across a diverse range of scientific disciplines. Geo-scientists may use it to find new robust earthquake trigger algorithms, enabling on-the-fly decision-making to improve emergency response times. Astronomers may rapidly detect anomalies, identifying a class of new variable stars buried within data from a time-domain imaging survey. Neuroscientists could incorporate improved real-time feedback and prediction into prosthetics control systems. As an intelligent agent, the platform could be used as an automated annotator for streaming biomedical data.<br/>     <br/>This work will deliver a new open-source toolkit and web platform that can serve as a fundamental tool for time-domain science. By design, it will grow organically as user-contributed code is integrated into the platform. With burgeoning adoption among some data-driven science disciplines the webservice will emerge as an educational platform in the use of learning algorithms for time-series data and as a societal service that can be used by anyone (even outside of traditional scientific disciplines) to test hypotheses on large scales with minimal effort. The website will also act as a public repository for large, well-described datasets useful for validating new time-series classification and prediction algorithms. A series of short and semester-long courses will be developed (and broadly disseminated) to teach a new generation of scientists how to use the platform (and other widely available resources) as central 21st century research instruments."
"1251267","BIGDATA: Small: DA: DCM: Measurement and Learning in Large-Scale Social Networks","IIS","Big Data Science &Engineering","09/01/2013","09/10/2013","Animashree Anandkumar","CA","University of California-Irvine","Standard Grant","Sylvia Spengler","08/31/2016","$746,783.00","Carter Butts","a.anandkumar@uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","CSE","8083","7433, 7923, 8083","$0.00","In the context of social networks, ""big data"" generally involves information on very large social systems whose elements of interest display complex dependence.  State-of-the-art statistical models for such systems require the use of computationally expensive stochastic simulation techniques to capture this dependence; these techniques do not generally scale well to the large-population case.  One potential solution to this problem is to focus detailed modeling efforts on smaller subpopulations (e.g., groups, communities, etc.) extracted from the larger system.  While scalability of the subsystem models is less challenging in this case, one must have appropriate methods for sampling from large networks in such a manner as to permit principled inference, and modeling techniques that recognize the coupling between local subpopulations and the broader network in which they are embedded.<br/><br/>The PI will bridge the gap between expensive, highly detailed models and the limits of computability imposed by Big Data by combining expertise from machine learning and social network modeling within a unifying exponential family framework.  The research will develop novel methods for the scalable measurement and analysis of large social networks, validating these techniques by deploying them in the context of dynamic data collection from online social networks. Specifically, the researchers will combine probabilistic graphical models and exponential family random graph models (ERGMs) to: (i) identify models with low computational requirements by exploiting limited-range dependence; (ii) develop machine learning techniques for identifying weakly coupled regimes in large networks to facilitate sampling and subgraph modeling; and (iii) develop integrated sampling and modeling strategies for inference from subgraphs of large networks that capture coupling to the structures in which they are embedded.  This proposal investigates these questions in both the cross-sectional and dynamic contexts, for networks with and without vertex attributes.  The sampling techniques created via this project will be deployed as an extension of a broader infrastructure for data collection in online social networks developed and maintained by one of the PIs, allowing for evaluation in a practical setting.<br/><br/>The methods developed via this research will allow for analysis of data relating to many problems of public interest, including epidemiological, security, and emergency management applications; data collection and analysis activities within the project will include applications in the natural hazard context, with the potential to inform policies that can save lives and property during disasters.  The project will be integrated with graduate and undergraduate education, as well as postdoctoral mentoring.  Tools developed via this project will be released as part of a widely used open-source toolkit for statistical network analysis (statnet), allowing widespread dissemination to researchers and practitioners in a range of fields."
"1309658","EAGER: Localization in Ad-Hoc Wireless Networks: Investigation into Fusing Dempster-Shafer Theory and Support Vector Machines","ECCS","COMMS, CIRCUITS & SENS SYS","02/01/2013","01/17/2013","Vijaya Kumar Devabhaktuni","OH","University of Toledo","Standard Grant","chengshan xiao","01/31/2016","$150,000.00","","vjdev@pnw.edu","2801 W Bancroft St., MS 218","TOLEDO","OH","436063390","4195302844","ENG","7564","153E, 7916","$0.00","Objective:<br/>The objective of this project is to perform exploratory investigation into accurate wireless node localization for wireless as hoc networks in realistic network environments, where signal obstruction due to multipath or shadowing is a major concern. To address such practical challenges, this project explores a new and reliable localization technique that mathematically fuses range estimates derived from Received Signal Strength (RSS) and Time Difference Of Arrival (TDOA) measurements.<br/><br/>Intellectual merit:<br/>The intellectual merit lies in the design and implementation of an efficient, hybrid algorithm integrating Dempster-Shafer theory with a state-of-the-art machine-learning tool referred to as Support Vector Machine (SVM). The hypothesis is that SVM-based signal propagation models will possibly help improve the accuracy of range measurements. The proposal entails high-risk as it is applied to real-time environments that are unpredictable and dynamic. If successful, there will be a high-reward in terms of accurately mapping RSS and TDOA signal information into distances, and offering precise node-position with no hardware upgrades whatsoever.<br/><br/>Broader impacts:<br/>The broader impacts are reflected in several aspects. This exploratory research will benefit US-based business enterprises that thrive on wireless network applications, particularly in such strategic areas as emergency services and wildlife habitat monitoring. The node localization challenge will be investigated from an interdisciplinary perspective combining the fields of Computer Science and Communications, Geomatics Engineering, and Applied Mathematics. Finally, the proposed research will lead to (i) Creation of new interdisciplinary graduate curriculum for students including minorities and underrepresented groups and (ii) Promotion of interdisciplinary NSF research."
"1251049","BIGDATA: Small: DA: Collaborative Research: From Data to Users: Providing Interpretable and Verifiable Explanations in Data Mining","IIS","Information Technology Researc","09/15/2013","09/13/2013","Suresh Venkatasubramanian","UT","University of Utah","Standard Grant","Sylvia Spengler","08/31/2017","$500,000.00","Preston Thomas Fletcher","suresh@cs.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","1640","7433, 7923, 8083, 9150","$0.00","The fruits of data mining pervade every aspect of our lives.  We have books and movies recommended; we are given differential pricing for insurance; screened for potential terror threats; diagnosed with various diseases; and targeted for political advertising. The ability to sift through massive data sets with sophisticated algorithms has resulted in applications with impressive predictive power.  And yet there is still a gap between what such tools can deliver, and what the users of data mining really need. It is often hard to interpret the answers produced by a learning algorithm, due to its sophistication and the use of large data sets to build models. The results of mining are often ""one-size-fits-all"", and convincing a user that results are actually relevant to them is difficult.  Finally, there is the important problem of validation. As the results of data mining affect more and more of our lives, the more crucial it is that the user be able to validate decisions made on their behalf and that affect them. The common theme tying these issues together is a user-centric perspective on the problems of data mining. Rather than asking ""What patterns can be found in this mountain of data?"" this work instead asks ""What structures in this data affect me?"" These issues arise precisely because of the vast amounts of data we now have the ability to mine, and the sophisticated methods at our disposal to analyze this data. In this research, the PIs develop a computational framework and key tools for user-centric data mining. A central theme in this research is the idea of interaction. In both machine learning and in the foundations of complexity theory, interaction has been used to allow a (weaker) entity to probe a much more powerful system and determine answers that it lacks the resources to compute directly itself. The PIs use formal interaction mechanisms both from the perspective of a user interacting with a powerful algorithm, as well as a client interacting with a computing source with access to large data, in order to enable the user to interpret and validate the results of data mining. <br/><br/>The goal of this project is to develop a computational framework for user-centric data mining  that enables existing users to tailor data analysis to their needs and facilitates the use of data mining in new areas where existing The team proposes interactive mechanisms that start with the results of a learning process and, via interaction with the user, produce an explanation expressed in terms of meaningful features, drawing on ideas from active learning, feature selection, and domain adaptation. 2. Locality: Answers that are relevant. Here, the focus is on providing information that depends more on a user?s local neighborhood, achieved via a new local notion of stability. 3. Verifiability: Answers you can check. The team proposes a framework for the validation of computationally-intensive data mining by the computationally-weak user, with ideas from interactive proof theory and stream algorithms. Tools for analyzing patient medical data have become more sophisticated and individual medical profiles play a far more significant role in diagnosis and treatment.The research examines user-centric data mining via three core primitives (classification, regression and clustering), and studies the three problems of interpreting results, providing local explanations, and validating the results of data mining. Firstly, the research draws on ideas from active learning, feature selection and domain adaptation to build interpretable results via interaction with users. Secondly, it introduces local notions of stability as a way of validating predictions for a specific user. Finally, it develops a general framework for validation of an analysis by a computationally-weak user, by drawing on ideas from the theory of interactive proofs and streaming algorithms."
"1251110","BIGDATA: Small: DA: Collaborative Research: From Data To Users: Providing Interpretable and Verifiable Explanations in Data Mining","IIS","INFORMATION TECHNOLOGY RESEARC","09/15/2013","09/13/2013","Andrew McGregor","MA","University of Massachusetts Amherst","Standard Grant","Sylvia J. Spengler","08/31/2017","$250,000.00","","mcgregor@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","1640","7433, 7923, 8083","$0.00","The fruits of data mining pervade every aspect of our lives.  We have books and movies recommended; we are given differential pricing for insurance; screened for potential terror threats; diagnosed with various diseases; and targeted for political advertising. The ability to sift through massive data sets with sophisticated algorithms has resulted in applications with impressive predictive power.  And yet there is still a gap between what such tools can deliver, and what the users of data mining really need. It is often hard to interpret the answers produced by a learning algorithm, due to its sophistication and the use of large data sets to build models. The results of mining are often ""one-size-fits-all"", and convincing a user that results are actually relevant to them is difficult.  Finally, there is the important problem of validation. As the results of data mining affect more and more of our lives, the more crucial it is that the user be able to validate decisions made on their behalf and that affect them. The common theme tying these issues together is a user-centric perspective on the problems of data mining. Rather than asking ""What patterns can be found in this mountain of data?"" this work instead asks ""What structures in this data affect me?"" These issues arise precisely because of the vast amounts of data we now have the ability to mine, and the sophisticated methods at our disposal to analyze this data. In this research, the PIs develop a computational framework and key tools for user-centric data mining. A central theme in this research is the idea of interaction. In both machine learning and in the foundations of complexity theory, interaction has been used to allow a (weaker) entity to probe a much more powerful system and determine answers that it lacks the resources to compute directly itself. The PIs use formal interaction mechanisms both from the perspective of a user interacting with a powerful algorithm, as well as a client interacting with a computing source with access to large data, in order to enable the user to interpret and validate the results of data mining. <br/><br/>The goal of this project is to develop a computational framework for user-centric data mining  that enables existing users to tailor data analysis to their needs and facilitates the use of data mining in new areas where existing The team proposes interactive mechanisms that start with the results of a learning process and, via interaction with the user, produce an explanation expressed in terms of meaningful features, drawing on ideas from active learning, feature selection, and domain adaptation. 2. Locality: Answers that are relevant. Here, the focus is on providing information that depends more on a user?s local neighborhood, achieved via a new local notion of stability. 3. Verifiability: Answers you can check. The team proposes a framework for the validation of computationally-intensive data mining by the computationally-weak user, with ideas from interactive proof theory and stream algorithms. Tools for analyzing patient medical data have become more sophisticated and individual medical profiles play a far more significant role in diagnosis and treatment.The research examines user-centric data mining via three core primitives (classification, regression and clustering), and studies the three problems of interpreting results, providing local explanations, and validating the results of data mining. Firstly, the research draws on ideas from active learning, feature selection and domain adaptation to build interpretable results via interaction with users. Secondly, it introduces local notions of stability as a way of validating predictions for a specific user. Finally, it develops a general framework for validation of an analysis by a computationally-weak user, by drawing on ideas from the theory of interactive proofs and streaming algorithms."
"1258471","EAGER: SAVI: Dynamic Digital Text: An Innovation in STEM Education","IIS","REAL, Cyberlearn & Future Learn Tech","01/01/2013","09/06/2012","Sadhana Puntambekar","WI","University of Wisconsin-Madison","Standard Grant","Lee Zia","12/31/2015","$247,933.00","Clifford Shaffer, N Narayanan","puntambekar@education.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","7625, 8020","7625, 7916, 8045","$0.00","This is an EAGER proposal supported under the SAVI initiative that is conducting research on knowledge organization techniques for visualizing and presenting STEM content digitally so as to engage students and engender deep learning. The PIs are applying the results of their research (in an iterative fashion) toward the development of new presentation, interaction, and navigation techniques for digital STEM content, and then evaluating these digital artifacts in the context of an inquiry-based pedagogy designed around dynamic digital text. In the long term, the team envisions a cloud-based service architecture that allows anytime, anywhere, and as-needed access to dynamic digital content covering an extensive set of topics for school and college level STEM education that is integrated with a proven pedagogy that makes effective use of digital STEM content. Collaboration with researchers in Finland is a key component of this work, as the team is testing its theoretical framework for dynamic digital texts with content in science and computing, for middle school and college, in Finnish and English. As such, this project is a first attempt to systematically evaluate the use and utility -- across multiple content domains, learner levels, and cultures -- of dynamic digital texts founded on concept map-based organizations of domain knowledge. In keeping with the spirit of the EAGER mechanism, this project is exploratory in nature; and while its multi-faceted nature presents risks in execution, there is a potentially transformative payoff to this work in that it can lay an excellent technological foundation for dynamic digital texts of the future."
"1254218","CAREER: Structured Learning of Distribution Spaces","CCF","Comm & Information Foundations, SIGNAL PROCESSING","04/01/2013","04/20/2017","Raviv Raich","OR","Oregon State University","Continuing Grant","Phillip Regalia","03/31/2019","$468,077.00","","raich@eecs.oregonstate.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","CSE","7797, 7936","1045, 7936","$0.00","A rapid acceleration in both volume and complexity of public domain and scientific data presents new and exciting challenges. This project aims to develop a theoretical framework for structured learning of distribution spaces and study tools for identifying and utilizing probabilistic structure in high-dimensional large volume data. This project lies within the intersection of multiple disciplines: signal processing, pattern recognition, machine learning, probability and statistics, and thus will foster collaboration among these disciplines. The application of the proposed framework to data-driven medical diagnosis and ecological research will further the impact of this project beyond the realm of computational data analysis. Additionally, this research sets a goal to enrich the quality of education for both undergraduate and graduate students, through exciting integration of research, application, and new curriculum.<br/><br/>The research framework consists of geometrically-constrained probabilistic modeling and efficient optimization approaches for inference of multiple instance data. The project sets forth the following tasks i) confidence-constrained joint estimation of multiple discrete probability models, ii) joint learning of multiple distribution based geometrically-constrained maximum-entropy models, and iii) direct application of the developed framework to the analysis of clinical flow cytometry data for medical diagnosis and in-situ bioacoustics data for ecological research."
"1332597","CAREER: Branch Prediction","CCF","COMPUTING PROCESSES & ARTIFACT","01/01/2013","03/12/2013","Daniel Jimenez","TX","Texas A&M Engineering Experiment Station","Continuing Grant","Hong Jiang","03/31/2014","$19,315.00","","djimenez@acm.org","400 Harvey Mitchell Pkwy S","College Station","TX","778454645","9798626777","CSE","7352","1045, 1187, 7941, 9218, HPCC","$0.00","Microprocessors use devices called branch predictors to predict the near-term behavior of a program so that work on future instructions may begin early, reducing the amount of time the program takes to run.  Branch predictors must be highly accurate, and a small improvement in accuracy can give a large benefit for performance.  This project is a principled approach to continuing the study of branch prediction.   Several new ways to understand and improve branch prediction will be explored:<br/><br/>1) Exploring the limits of the potential of branch prediction to improve performance by developing a model of an idealistic branch predictor given reasonable assumptions;<br/><br/>2) Improving technologies for running computer programs on real computer systems so that these programs will have better branch prediction accuracy;<br/><br/>3) Discovering ways of improving the communication between computer programs and computer systems such that information available to a computer program can be used to improve the accuracy of branch prediction in a computer system; and<br/><br/>4) Working on new branch predictor designs for future computer systems, incorporating techniques from other disciplines such as machine learning, i.e., the study of how computer systems can learn by observing data.<br/><br/>In each of these areas, technological constraints on branch prediction will be taken into account.  In particular, a branch predictor must act very quickly to deliver its prediction in time to improve performance, and it should do so in an energy-efficient way.  This research will be brought  to the classroom with a special seminar class on the interaction of research into computer systems and research on machine learning.<br/>"
"1319050","Accelerated Algorithms for a Class of Saddle Point problems and Variational Inequalities","DMS","COMPUTATIONAL MATHEMATICS","09/01/2013","07/08/2013","Yunmei Chen","FL","University of Florida","Standard Grant","Leland Jameson","08/31/2017","$160,000.00","Guanghui Lan","yun@math.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","MPS","1271","9263","$0.00","This project will develop novel theories and optimal numerical methods for solving certain classes of deterministic and stochastic saddle point and variational inequality problems arising from large-scale data analysis in various disciplines. The proposed accelerated primal-dual (APD) algorithm is based on the integration of a multi-step acceleration scheme with the primal-dual method, and expected to exhibit an optimal rate of convergence as the one obtained by Nesterov for a different scheme. The proposed stochastic APD algorithm is also expected to possess an optimal rate of convergence for solving stochastic saddle point problems, while no stochastic primal-dual algorithms have been developed in the literature. Moreover, the research will be extended to the development of optimal methods for solving a class of composite variational inequalities (VI) that includes the class of the saddle point problems to be studied as a special case. This study provides some important insights on the decomposition of a general VI problem to potentially accelerate its solution. Furthermore, the theoretical analysis on optimal convergence rate, and optimal estimation of the bound for duality gap, especially, the dependence on the distance between the initial and saddle points (or the diameter of the feasible set, if they are bounded), of all the proposed algorithms will be investigated. The project will investigate and develop backtracking strategies for the proposed algorithms to enhance their practical performance.  The new methods will be applied to several image reconstruction and machine learning problems. <br/><br/>The class of the deterministic and stochastic saddle point and variational inequality problems studied in this proposal has been considered as a framework of ill-posed inverse problems regularized by a non-smooth functional in many data analysis problems, such as image reconstruction, compressed sensing and machine learning.  The success of the proposed research will significantly advance non-smooth convex optimization solvers by enriching solver's abilities in accelerating computation with good theoretical performance guaranteed. Therefore, this project is expected to greatly increase the applicability of many emerging technologies, such as partially parallel imaging and dynamic multi-tracer PET. Those imaging methods can significantly reduce scan time and improve image quality. However, their clinical applications have been hindered by our incapability to efficiently solve the large-scale ill-posed and ill-conditioned inverse image reconstruction problems. Moreover, the development of stochastic APD algorithms will greatly enhance learning power. For instance, these optimal methods will enable researchers to build high-level, class specific feature detectors from massive datasets. The new methods to be developed have a wide range of applications in large-scale data analysis problems from various disciplines. Therefore, the research will contribute to the research communities and industry with mutual interest. The algorithms developed during the research will be made freely available on the World Wide Web. The graduate students of the PIs will be involved in all aspects of the research, both theoretical analysis as well as practical implementation of algorithms. The research will be made accessible to more graduate and senior undergraduate students through seminars and course developments. The PIs intend to teach courses based on the proposed research."
"1314631","SBE TTP: Medium: Securing Cyber Space: Understanding the Cyber Attackers and Attacks via Social Media Analytics","SES","Secure &Trustworthy Cyberspace","09/01/2013","03/04/2016","Daniel Zeng","AZ","University of Arizona","Standard Grant","Sara Kiesler","08/31/2018","$1,301,944.00","Ronald Breiger, Salim Hariri, Thomas Holt","zeng@email.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","SBE","8060","0000, 7434, 7924, 9178, 9179, 9251, OTHR, SMET","$0.00","As society becomes more dependent on cyber infrastructure, the security of networks and information technologies has become a growing concern. Individuals, businesses, and governmental organizations are now common victims of cyber-attacks that seek to steal private data, gain remote control over remote systems, and cause harm to networks and systems through other malicious means. Additionally, critical infrastructures such as smart power grids and communication networks are facing an increasing number of cyber-based threats. As a result, many researchers and security practitioners have begun to investigate cyber attacker communities in order to learn more about cyber attacker behaviors, emerging threats, and the cybercriminal supply chain. Unfortunately, there is a lack of established science for cyber security research. The lack of literature is problematic for researchers wanting to learn more so that they may contribute to and advance the current state of cyber security research. For example, many cyber attacker communities take careful measures to hide themselves by employing anti-crawling measures. This would be a challenge for many researchers and security practitioners. Furthermore, some may find cyber attacker community discussion difficult to interpret due to cyber attacker jargon, advanced security concepts, or foreign contents belonging to cyber attacker groups spanning across different countries or regions.<br/><br/>For these reasons, research studying hacker communities is greatly needed, as well as research that advances others? capacity to understand and investigate contents from such communities. Specifically, the development of automated tools and analyses increases the potential for more cyber security research. Web mining and machine learning technologies can be used in tandem with social science methodologies to help answer many questions related to hacker behaviors and culture, illegal markets and covert networks, cybercriminal supply chain, malware analysis, emerging security threats, and other matters. There are many opportunities for extending current cyber security research by combining hacker community data with social science methodologies, computational techniques, and security analysis. <br/>   <br/>In this research, important questions about hacker behaviors, markets, community structure, community contents, artifacts, and cultural differences are explored. Automated techniques to collect and analyze data from forums, Internet Relay Chat, and honeypots will be developed. The development of such tools will help further proactive approaches for preventing cyber-based threats, rather than taking the traditional approach of reacting when something ""bad"" happens.  Better understanding of hacker communities across multiple geopolitical regions will support a better understanding of cybercriminal behavior, and improved and safer practices for security researchers and practitioners.<br/><br/>The proposed integrated computational framework and the resulting algorithms and software will also allow social science researchers and security practitioners to closely examine how cyber attacker groups form, develop, and spread their ideas; identify important and influential cyber criminals in the online world; and develop the means to recognize online hacker identities through their communication and interaction styles. Knowing more about cyber criminals, hackers, and their illegal black markets can help policy makers and security professionals make better decisions about how to prevent or respond to attacks. <br/><br/>The proposed work also contributes to the educational and professional development of the student research associates who contribute to it.  They will learn sound research methods, and how to write about and present their work for scientific and other professional audiences."
"1322174","EAGER: Pilot Investigation of Using Gaze in a Reading Tutor","IIS","Cyberlearn & Future Learn Tech","03/01/2013","03/20/2013","David Mostow","PA","Carnegie-Mellon University","Standard Grant","Janet L. Kolodner","12/31/2013","$300,000.00","Jessica Nelson-Taylor","mostow@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","8020","7916, 8045","$0.00","The big question the PIs are addressing in this project is how to unobtrusively track silent reading of novice readers so as to be able to use an intelligent tutoring system to aid reading comprehension. This EAGER project focuses on the first steps in answering that question. This pilot project builds on previous work in vision and speech technology, sensor fusion, machine learning, user modeling, intelligent tutors, and eye movements in an effort to identify the feasibility of using eye tracking techniques, along with other information collected from an intelligent reading tutor, to predict reading difficulties of novice/young readers. In particular, the work plan includes collecting gaze data in real-world conditions in the context of using the existing Reading Tutor, designing software to display those traces so that accuracy can be guauged, testing gazepoint accuracy and detecting gaze-speech discrepancies, and using that data to develop heuristics for detecting tracking errors in real time and calibrating eye tracking data to noisy school environments, a primary environment where the augmented Reading Tutor would ultimately be used. The intellectual merit of this project is in identifying and addressing challenges in relating children's gaze data to their silent reading, in making technical contributions to calibrating eye trackers so that they can be used in normal everyday applications, and in setting the stage for intelligent tutors across diverse domains to exploit gaze more broadly.<br/><br/>The project's most important potential broader impacts is in establishing a foundation for exploiting gaze input to build intelligent computing systems that can be used to help children with reading difficulties learn to read and read to learn. If successful, the PIs will develop a larger project that will extent the successful Project Listen Reading Tutor so that it can track readers as they are reading silently and help them with their comprehension -- both comprehension of text itself and strategies for coming to deep understanding."
"1302134","III: Medium: Algorithms and Software Tools for Epigenetics Research","IIS","ADVANCES IN BIO INFORMATICS, Information Technology Researc, Cross-BIO Activities, Info Integration & Informatics","09/15/2013","06/09/2014","Stefano Lonardi","CA","University of California-Riverside","Continuing Grant","Sylvia Spengler","08/31/2017","$994,370.00","Karine Le Roch","stelo@cs.ucr.edu","Research & Economic Development","RIVERSIDE","CA","925210217","9518275535","CSE","1165, 1640, 7275, 7364","7364, 7924, 8750","$0.00","This project will develop a new computational framework to advance the understanding of epigenetic gene regulation in the human malaria parasite.  Epigenetics is the study of heritable changes in gene expression or cellular phenotype caused by mechanisms other than changes in the underlying DNA sequence.At the core of the computational framework is the ability to solve a set of hard computational questions, which are the focus of the research plan. The computational challenges require the study of novel combinatorial optimization problems, the development of new time- and space-efficient algorithms, and ultimately the implementation and deployment of user-friendly web-based software tools. The ability to analyze the epigenome of the human malaria parasite will improve our comprehension of its biology and possibly enable molecular biologists to identify new antimalarial strategies.  The proposed computational framework will also enable life scientists to make novel epigenetic discoveries and ultimately improve the understanding of the complex mechanisms that drive gene expression inother eukaryotic organisms. Software tools will be placed into the public domain, which will benefit researchers and the public worldwide, and potentially lead to new international and industrial collaborations. This project will support two graduate students and one post-doc in a highly interdisciplinary environment.<br/><br/>Most eukaryotic genomes have a second layer of information which is embedded on chemical marks added to DNA and to the protruding tail of special proteins that package DNA into a complex called the nucleosome. One of the most astonishing discoveries in molecular biology of the past decades is that this ""covert"" layer, called the epigenome, affects a variety of cellular and metabolic processes. Epigenetic marks not only controls what genes are accessible in each type of cell, but also determine when the accessible genes may be activated. Molecular biologists have also confirmed that the epigenome is affected by the interactions of the organism with the environment and that changes to the epigenetic marks induced by these interactions are inherited across cell division, despite not being encoded directly in DNA. <br/><br/>This project will study a set of computational challenges that will be brought about by the increasing number of epigenome projects. Specifically, the goal is to develop methods and software tools for (1) the analysis nucleosome and methylation maps(using a modified Gaussian mixture model and expectation maximization); (2) the study of dynamics of nucleosome positioning, histone tail modifications and DNA methylation patterns (using graph theoretical approaches, e.g., k-partite matching); (3) the analysis of DNA motifs for stable nucleosomes and specific histone modifications (using combinatorial optimization approaches); (4) the discovery of new genes using nucleosome or methylation landscapes (using machine learning classifiers); (5) the identification of statistically significant genome-wide correlations between nucleosome positioning, histone modifications, DNA methylation patterns and gene expression (using dynamic Bayesian networks).  These five computational tasks will require the study of novel combinatorial optimization and machine learning problems, the development of new time- and space-efficient algorithms, and ultimately the implementation and deployment of user-friendly web-based software tools.<br/><br/>The ""platform"" on which the algorithms will be developed is P. falciparum, the parasite responsible each year for 350-500 million cases of malaria, and between one and three million of human deaths world-wide. There is no vaccine against malaria (one is currently on clinical trials) and the parasite is developing resistances to almost all drugs currently available. The methods and tools developed will not be malaria-specific, and will scale to a variety of other eukaryota with much larger/complex genomes.<br/>Updates and additional information about this project will be made available at http://www.cs.ucr.edu/~stelo/iis13.htm"
"1353606","CAREER:  A Scalable, Declarative, Imprecise Database Management System","IIS","Info Integration & Informatics","07/01/2013","05/06/2015","Christopher Re","CA","Stanford University","Continuing grant","Maria Zemankova","04/30/2017","$345,950.00","","chrismre@cs.stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","7364","1045, 1187, 7364, 9251","$0.00","The unprecedented amounts of data available to individuals, companies, governments, and scientists promises to revolutionize the way entertainment, business, governance, and science operate. And while data are cheap and plentiful, much of this data is lower quality than the precise data that has been managed for the last 30 years. Building an application that processes this imprecise data is difficult: it requires that developers handle both standard data management challenges (e.g., concurrency and scalability), while at the same time coping with imprecise and incomplete data, which is typically done using statistical or machine learning techniques (e.g., interpolation and classification). The Hazy project addresses this challenge by building a system that integrates the paradigms of relational database management systems with statistical machine learning techniques. This project conducts the following major tasks: (I) designing a language to integrate these techniques with standard SQL, (II) proposing an algebra to implement this language along with support for automatic optimization (similar to a standard RDBMS), and (III) discovering techniques to efficiently maintain the statistical models as the underlying data are changed or updated. The end goal is a system that makes it as easy to develop scalable applications that use imprecise data as it is to develop their precise counterparts. Hazy allows users to process larger amounts of data with more sophisticated statistical processing than ever before. In turn, this enables new applications in a divese set of areas, such as life and physical science sensing applications, health-care and environmental monitoring, and enterprise-based and Web-based information extraction.<br/><br/>The research of this project is used to develop the data and infrastructure for new practicum-style courses that are under development at the University of Wisconsin-Madison. In addition, this infrastructure will be used as part of an outreach effort to enable high school students to gain access to data analysis tools. The source code of Hazy is released into open source and the results are disseminated on the project Web site (http://www.cs.wisc.edu/hazy/)."
"1253670","CAREER: Hardware and Algorithmic Architectures for Analyzing Physically-complex Systems: embedding inference capabilities in ultra-low-power sensors","CCF","Comm & Information Foundations, Software & Hardware Foundation, SIGNAL PROCESSING, DES AUTO FOR MICRO & NANO SYST","03/01/2013","04/19/2017","Naveen Verma","NJ","Princeton University","Continuing grant","Sankar Basu","02/28/2019","$445,734.00","","nverma@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","7797, 7798, 7936, 7945","1045, 7936, 7945, 9251","$0.00","The scientific aim of this work is to study how small electronic devices can function at a high level in the face of increasingly-severe physical complexities. The complexities can originate from the physical signals of interest in a sensing system or from non-ideal device and algorithmic behaviors within the electronics itself, which are becoming unavoidable due to technology and system scaling. As an example, this study focuses on analyzing physiological signals that are available through low-power medical sensors. Though such signals are highly indicative, extracting medical information of value requires high-order models of the underlying physiological processes when in fact no tractable analytical models generally exist. This study also focuses on errors within the hardware that occur due to unpredictable but inevitable technological defects and variations, leading to high levels of errors in the data being processed. These challenges are approached through algorithmic methods emerging from the domain of machine learning that construct models for interpreting data from the data itself. The large amount of data that is available through small-scale sensors can thus be leveraged as an extensive knowledgebase; but the problem is that these methods are not well supported by low-power electronics, in terms of their computational energy, memory requirements, network interactions, etc. This research starts with the kernel computations used in machine-learning frameworks, and it investigates kernel formulations, structured hardware architectures, and algorithms to overcome the physical complexities associated with application signals and technological non-idealities. The principles are studied through hardware and software experimental demonstrations.<br/><br/>The broader impact of this research is to enable greater value of electronic systems in critical applications and to establish an interdisciplinary educational program that teaches students to connect fundamentals from computer science, low-power electronics, and clinical applications. While electronics presents tremendous capabilities, its impact on real-world challenges such as in healthcare depends on high-value interactions with physical systems. This program emphasizes clinical applications and collaborations to understand the role that electronics can play in enabling preemptive medical harm detection and chronic-disease management over large patient populations: something that is infeasible with today's methods. This program also emphasizes interactions with the semiconductor industry, to transfer principles and architectures both for advanced sensing platforms and for algorithmic approaches to hardware resilience; with hardware errors having been identified by the industry as one of the critical challenges, methods that overcome the need for traditional forms of design margining are being urgently pursued. New interdisciplinary courses, student projects, and outreach activities will expose students to external collaborators and will drive an educational program that ties together engineering fundamentals from multiple domains through an application-driven pursuit of systems to overcome critical challenges in healthcare decision support."
"1318751","NeTS: Small: Collaborative Research: Distributed Robust Spectrum Sensing and Sharing in Cognitive Radio Networks","CNS","Networking Technology and Syst","10/01/2013","08/19/2013","Jie Yang","MI","Oakland University","Standard Grant","Min Song","11/30/2014","$150,149.00","","jyang5@fsu.edu","530 Wilson Hall","Rochester","MI","483094401","2483704116","CSE","7363","7923","$0.00","The future Cognitive Radio Networks (CRNs) will consist of heterogeneous devices such as smartphones, tablets and laptops moving dynamically. Accurate and robust spectrum sensing and identification of unauthorized spectrum usage are essential components of spectral efficiency in future radio systems. This project aims to utilize consensus-based cooperation featuring self-organizable and scalable network structure to capture the swarming behaviors of spectrum users and providing cooperative spectrum sensing in a fully distributed manner. By using a combination of control theory and machine learning techniques, the project designs secure weighted average consensus for cooperative spectrum sensing that can not only capture the swarming behaviors in CRNs with heterogeneous devices, but also is robust to practical channel conditions. Robust localization approaches are developed grounded on dynamic signal strength mapping, which have the capability to localize multiple malicious users. Additionally, the new techniques are validated using an actual testbed with on-campus deployment and system demonstration to industrial collaborators. The integration of control theory with dynamic spectrum access will enable a new revolution in the way for enhancing spectrum efficiency in CRNs. The project serves as a pioneer in exploiting multi-disciplinary knowledge (e.g., control systems and machine learning techniques) to achieve a more efficient spectrum usage in future radio systems, aiming to alleviate the increasing crowdness of the spectrum occupancy and support the co-existence of heterogeneous devices. This project also carries out a broad range of education and outreach activities to encourage students to pursue careers in the fields of science and engineering. Research results will be disseminated to academia and industry through presentations and publications in meetings, conferences and journals."
"1337158","XPS: CLCCA: On the Hunt for Correctness and Performance Bugs in Large-scale Programs","CCF","Information Technology Researc, Software & Hardware Foundation, Exploiting Parallel&Scalabilty","09/15/2013","04/24/2015","Milind Kulkarni","IN","Purdue University","Standard Grant","Anindya Banerjee","08/31/2016","$292,331.00","Michael Gribskov, Saurabh Bagchi","milind@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","1640, 7798, 8283","7943, 9251","$0.00","The scale of computing applications has been dramatically increasing over the past several years. As applications in domains such as computational genomics, data mining, and machine learning are let loose on ever-more-complex problems, the scale of the inputs to these applications has shot up. And as the pursuit of parallelism has led to increasing core counts for servers, and increasing numbers of servers and racks for data centers, the scale of the systems that these applications must run on has also dramatically risen. A critical problem in developing large scale applications is detecting and debugging scaling issues, which are problems with program behavior that emerge only as a program scales up. Scaling issues show up as correctness bugs or performance bottlenecks. Unfortunately, detecting bugs that arise at large scales is difficult. Manually poring through logs or performance profiling individual application processes is not practical. Moreover, the developer may not have access to the inputs and systems necessary to run the application at large scales. This research project aims to develop automated techniques to detect and diagnose correctness and performance bugs for large-scale programs using program behavior modeling, training at small scale runs, and extrapolating to large-scale runs.<br/><br/>To achieve our objectives, we build statistical models that incorporate scale. By relating program scale to program behavior, we can predict how a program behaves at large scales, without ever seeing correct behavior at that scale, and use those predictions to detect and diagnose bugs. The project is structured around three thrusts, each using the computational genomics applications for context. In the first, we build statistical models of program behavior that incorporate scale. In the second, we build statistical techniques for detecting when there is an error and then drilling down to identify potential root causes in the software. In the third, we build a testing tool which will allow us to uncover such scaling issues in an accelerated manner.  In aggregate, the project combines in innovative ways applications of static analysis, dynamic instrumentation, modeling, and machine learning-based data analysis.  The project will use computational genomics applications, such as Blast, Bowtie, Trinity/Butterfly, and Margin, to evaluate the approach."
"1302269","SHF:Medium:Overcoming the Intuition Wall: Automatic Graphical Analysis of Programs to Discover and Program New Computer Architectures","CCF","IIS Special Projects, COMPUTER ARCHITECTURE","09/15/2013","09/09/2013","L Sethumadhavan","NY","Columbia University","Standard Grant","tao li","12/31/2016","$400,654.00","Gail Kaiser, Tony Jebara","simha@cs.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","7484, 7941","7924, 7941","$0.00","Workload characterization is central to development of new computer architectures. The rise of the mobile-cloud paradigm has increased the diversity and rate at which applications are created thus challenging computer architects' ability to build optimized systems for them. In the past, architects have been able to examine software codes of interest (often through slow laborious manual inspection if necessary) when releases were far and few in between to derive intuition necessary to make architectural and microarchitectural discoveries. But this method does not scale to emerging applications that are literally hammered out in the hundreds by the day. Further, new languages and platforms have behaviors that are quite different from legacy codes and there is an urgent need for intuition on these applications. Without new methods to characterize emerging workloads, computer architects risk running into an intuition wall. This risk might prove calamitous if unmitigated, given the added reliance on (micro)architects to develop more energy efficient designs to compensate for the losses due to slowdowns in Dennard's scaling.<br/><br/>Advances in machine learning provide an opportunity to overcome the intuition wall. In the last decade there have been many major advances in machine learning on graphs motivated by need/benefits of mining behaviors in social networks and enabled by cheap commodity computing. In this project, the PIs plan to leverage these advances to discover and program new computer architectures. By viewing program execution as a graph, clustering these graphs, and mining them for similarities, the PIs plan to discover new behaviors that architects and microarchitects can use to develop new on-chip acceleration structures. The PIs also plan to study how legacy code can semi-automatically be converted to execute on the architectures with the new accelerators."
"1311833","Mathematical problems from materials science","DMS","APPLIED MATHEMATICS","10/01/2013","09/18/2017","Robert Kohn","NY","New York University","Continuing Grant","Pedro Embid","09/30/2019","$1,121,590.00","","kohn@cims.nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","MPS","1266","","$0.00","Kohn<br/>1311833<br/><br/>     This project has three main scientific thrusts: <br/>(a) The first and broadest thrust concerns elastic-energy-driven pattern formation in thin elastic sheets.  The investigator studies stress-driven patterns involving wrinkles, folds, delamination, and other defects, with particular emphasis on situations where the energy-minimizing pattern develops fine-scale structure as the sheet thickness tends to zero.  His approach is to focus on how the minimum energy scales with respect to the sheet thickness and other relevant physical parameters. <br/>(b) The second thrust concerns surface-energy-driven coarsening of two-phase mixtures.  The investigator studies a family of nonlocal evolutions from the physics literature, which generalize the relatively well-understood model of ""Cahn-Hilliard dynamics.""  The focus here is on understanding the large-time coarsening rate. <br/>(c) The third thrust concerns prediction with expert advice (a topic from the machine learning literature).  The investigator's goal is a fresh perspective on some regret-minimization-based algorithms for prediction.  He views regret minimization as a robust control problem and considers a suitable scaling limit in which the associated value function solves a differential equation. <br/><br/>     The investigator studies three interdisciplinary topics.  The first two lie at the interface where mathematics meets physics and materials science, while the third lies at the interface with machine learning.  In each area, challenges from applications drive the development of new mathematical methods.  For example, the work on thin elastic sheets is helping develop a theory of energy-minimizing patterns, in much the same way that consideration of soap bubbles and soap films led to the theory of minimal surfaces a generation ago.  It is of course a familiar fact that thin sheets often wrinkle or fold: our skin wrinkles and our clothes wrinkle; leaves, flowers, and hanging drapes have folds.  Physical experiments in controlled settings can quantify such phenomena, and numerical simulations can demonstrate within a model how the patterns develop.  But neither experiment nor simulation can tell us ""why"" a system chooses a particular pattern.  The project provides a valuable complement to other methods, by showing that elastic energy minimization requires types of patterns.  The project provides training opportunities for graduate students."
"1347844","CAREER: Nonparametric Models Building, Estimation, and Selection with Applications to High Dimensional Data Mining","DMS","STATISTICS","07/01/2013","08/14/2013","Hao Zhang","AZ","University of Arizona","Continuing Grant","Gabor Szekely","06/30/2014","$96,133.00","","hzhang@math.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","MPS","1269","0000, 1045, 1187, OTHR","$0.00","Nonparametric methods are increasingly applied to regression, classification and density estimation, both in statistics and other related areas such as data mining and machine learning. However, a key difficulty with nonparametric models is model fitting for high dimensional data due to the curse of dimensionality. Another difficulty is model inference and interpretation, i.e., how to evaluate or test individual variable effects on the complex surface fit. For heterogeneous data with complicated covariance structure, nonparametric model estimation is even more challenging. The objectives of this proposal are to develop novel and widely applicable procedures to simultaneous model selection and estimation for nonparametric models and their related paradigms in data mining. In the framework of reproducing kernel Hilbert space (RKHS), the PI proposes a host of new regularization techniques for several families of models: smoothing spline ANOVA models for correlated data, semiparametric regression models, support vector machines for supervised and semi-supervised learning. The proposed methodologies constitute key advances over standard methods through their unified framework for achieving model sparsity and function smoothing altogether, their tractable theoretical properties, and their easy adaptation to high dimensional problems. The PI will study asymptotic behaviors of the proposed estimators, explore data-driven procedures for tuning regularization parameters, and develop computation algorithms and softwares to implement the proposed procedures. The PI will also examine finite sample performance of new methods via extensive simulation studies and real data analysis.<br/><br/>In the current information era, the volume and complexity of scientific and industrial databases have been exponentially expanding. As a consequence, the data form keeps gaining higher and higher dimensionality. Analysis of such data poses new challenges to statisticians and is becoming one of the most important research topics in modern statistics. The purpose of this project is to significantly increase the available tools for analyzing complex high dimensional data. In this project, the PI aims to accomplish the following three goals: (1) meet the challenges of nonparametric model estimation and selection within a unified mathematical framework; (2) develop flexible methods with desired statistical properties and high-performance statistical softwares for mining massive data; (3) integrate research opportunities and findings from the above two activities into disciplinary and interdisciplinary statistical education at graduate, undergraduate and high school levels. This research will broaden traditional understanding of nonparametric inferences <br/>and model selection, provide a broad range of researchers and practitioners in various fields including sociology, economics, environmental, biological and medical sciences with state-of-the-art data analysis tools, and help to prepare the next-generation students with the necessary modern statistical perspectives. <br/> <br/>"
"1319979","CIF: Small: Optimal Iterative Estimation in Signal Processing, Information Theory and Machine Learning","CCF","ALGORITHMS, SIGNAL PROCESSING","06/01/2013","05/31/2013","Andrea Montanari","CA","Stanford University","Standard Grant","Phillip Regalia","05/31/2018","$416,160.00","","montanari@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","7926, 7936","7923, 7936","$0.00","Modern imaging devices, sensors, data acquisition systems allow to gather data with unprecedented speed and<br/>accuracy. Most of the times, however, we are not interested in accumulating data per se, but rather to<br/>uncover some hidden patterns in the data. For instance, given a large network, we might want to discover <br/>a small subset of notes that are tightly connected to each other. Such highly connected substructures<br/>are of interest in biological datasets, but also in social network analysis, and in signal processing. <br/>Finding such patterns requires highly efficient algorithms that can process large amount of data and<br/>uncover tenuous statistical signatures. The investigators develop new algorithms that simultaneously optimize<br/>both metrics: statistical efficiency and computational efficiency.<br/><br/>Consider in particular the problem of finding an anomalous submatrix in a large data matrix with independent <br/>random entries. If the anomalous submatrix has entries with a different distribution, this can be done via <br/>principal component analysis, as long as the submatrix has dimensions of the  order of the square root of<br/>the ambient dimensions. The investigators introduce a class of first order methods with linear complexity,<br/>and determine the optimal algorithm within this class. This appears to provably outperform existing approaches. <br/>The same framework is generalized to several other classes of high-dimensional estimation problems. <br/>Optimal iterative procedures are developed  under strict computational constraints."
"1262603","ABI Development: Continued Development of RaptorX Server for Protein Structure and Functional Prediction","DBI","ADVANCES IN BIO INFORMATICS","07/01/2013","05/15/2013","Jinbo Xu","IL","Toyota Technological Institute at Chicago","Standard Grant","Jennifer Weller","06/30/2017","$551,238.00","","j3xu@ttic.edu","6045 S Kenwood Ave","Chicago","IL","606372803","7738340409","BIO","1165","","$0.00","High-throughput sequencing has been producing millions of protein sequences without solved structures and functional annotations, which raise demand for computational tools especially user-friendly web servers for protein structure and functional elucidation. This project will transform RaptorX, a popular protein structure modeling web server, to one that can also annotate functions of a protein sequence and the quality of a theoretical protein model in the absence of the corresponding native structure. The resultant new server will greatly facilitate the interpretation and proper usage of a theoretical protein model, just like what E-value does for homology search. The server will also predict functions of a protein sequence with coverage beyond what can be reached by native-structure-based methods and accuracy much higher than sequence-based methods. Ultimately, the project will deliver a long-term sustainable cyber-infrastructure for protein sequence, structure and functional analysis that enables transformative biological and biomedical research. This project will also advance protein structure and functional prediction by developing several sophisticated computational methods for model quality assessment and functional prediction. <br/><br/>Proteins play fundamental roles in all biological processes. Complete description of protein structures and functions is a fundamental step towards understanding biological life. This project will benefit a broad range of biological/biomedical applications, such as the study of plant metabolic pathways, drug design, and bio-energy development. The research results will be communicated to the broader community through a variety of venues (wiki, talks, papers and posters). The software will be freely available to the public. Since its first release in August 2011, RaptorX has processed dozens of thousands of protein modeling and analysis jobs for more than 3500 users around the world. After the new RaptorX is implemented, it will contribute much more to the broader community. This project will also contribute to computer science by studying machine learning problems inspired from protein bioinformatics. This project shall enrich and disseminate knowledge on protein bioinformatics, machine learning and web programming. It will also train minority students, future K-12 science teachers and nationwide students in the Illinois online bioinformatics program. All involved students will receive training in the intersection of computer science, molecular biology, biophysics, and biochemistry. The research results will be integrated into course materials, which will be used in the classes and also freely available to the public."
"1318748","NeTS: Small: Collaborative Research: Distributed Robust Spectrum Sensing and Sharing in Cognitive Radio Networks","CNS","Networking Technology and Syst","10/01/2013","08/19/2013","Yingying Chen","NJ","Stevens Institute of Technology","Standard Grant","wenjing lou","09/30/2016","$304,840.00","Yi Guo","yingche@scarletmail.rutgers.edu","CASTLE POINT ON HUDSON","HOBOKEN","NJ","070305991","2012168762","CSE","7363","7923","$0.00","The future Cognitive Radio Networks (CRNs) will consist of heterogeneous devices such as smartphones, tablets and laptops moving dynamically. Accurate and robust spectrum sensing and identification of unauthorized spectrum usage are essential components of spectral efficiency in future radio systems. This project aims to utilize consensus-based cooperation featuring self-organizable and scalable network structure to capture the swarming behaviors of spectrum users and providing cooperative spectrum sensing in a fully distributed manner. By using a combination of control theory and machine learning techniques, the project designs secure weighted average consensus for cooperative spectrum sensing that can not only capture the swarming behaviors in CRNs with heterogeneous devices, but also is robust to practical channel conditions. Robust localization approaches are developed grounded on dynamic signal strength mapping, which have the capability to localize multiple malicious users. Additionally, the new techniques are validated using an actual testbed with on-campus deployment and system demonstration to industrial collaborators. The integration of control theory with dynamic spectrum access will enable a new revolution in the way for enhancing spectrum efficiency in CRNs. The project serves as a pioneer in exploiting multi-disciplinary knowledge (e.g., control systems and machine learning techniques) to achieve a more efficient spectrum usage in future radio systems, aiming to alleviate the increasing crowdness of the spectrum occupancy and support the co-existence of heterogeneous devices. This project also carries out a broad range of education and outreach activities to encourage students to pursue careers in the fields of science and engineering. Research results will be disseminated to academia and industry through presentations and publications in meetings, conferences and journals."
"1317105","NSF East Asia and Pacific Summer Institute (EAPSI) for FY 2013 in Japan","OISE","EAPSI","06/01/2013","05/16/2013","Yves Petinot","NJ","Petinot Yves","Fellowship","Anne L. Emig","05/31/2014","$5,070.00","","","","New Brunswick","NJ","089014508","","O/D","7316","5921, 5978, 7316","$0.00","This action funds Yves Petinot of Columbia University to conduct a research project in the Computer and Information Science and Engineering area during the summer of 2013 at the Institute of Statistical Mathematics in Tokyo, Japan. The project title is ""Hierarchical Language Models for Natural Language Generation and Web Summarization."" The host scientist is Professor Daichi Mochihashi.<br/><br/>This research project aims at developing and evaluating hierarchical models of natural language that can support the statistical generation of human-readable sentences, and in particular, of Web gists. Web gists are static, typically single-sentence long summaries of Web Uniform Resource Locators (URLs). The project explores statistical and machine learning approaches to automatically predict how elements of language appear in Web gists. Organically available Web resources, including hierarchy-based Web directories, are being used as a basis for learning and experimental validation.<br/><br/>Broader impacts of an EAPSI fellowship include providing the Fellow a first-hand research experience outside the U.S.; an introduction to the science, science policy, and scientific infrastructure of the respective location; and an orientation to the society, culture and language. These activities meet the NSF goal to educate for international collaborations early in the career of its scientists, engineers, and educators, thus ensuring a globally aware U.S. scientific workforce. Furthermore, this research project aims at facilitating the large-scale, efficient and non-biased discovery of information and services accessible through the Web both by human and machine agents."
"1254206","CAREER: A Novel Framework for Knowledge Discovery from Time Series Data in Biology and Climate Science","IIS","Info Integration & Informatics","03/15/2013","04/30/2020","Yan Liu","CA","University of Southern California","Continuing Grant","Sylvia Spengler","02/28/2021","$510,385.00","","yanliu.cs@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7364","1045, 7364","$0.00","Recent advances in sensors and high throughput data acquisition technologies have made it possible to collect massive amount of data, and especially time series data in a number of domains (e.g., climate sciences, biological sciences). While a wide range of techniques have been developed for clustering and mining such data, there has been limited progress on  scalable algorithms for extracting causal relationships from time series data. This project aims to develop novel machine learning models based on Granger causality to uncover the complex dependence structures from high-dimensional time series. The resulting algorithms will be evaluated in the context of two real-world applications (climate change, computational biology).<br/><br/>The project aims to address three fundamental challenges of data analysis from time series data, including: (1) developing the theoretical foundations of causality analysis from time series data to quantify the gap between Granger causality and true causality,  (2) developing a unified framework to incorporate different types of domain knowledge in data analysis, and (3) examining effective solutions to important but usually overlooked practical issues, including irregular nature of the time series and scalability. The resulting algorithms will be evaluated on two real applications, i.e., gene regulatory network discovery in immune systems and climate change attribution, by collaborating with researchers in biology and climate science.<br/><br/>The proposed research could impact multiple application domains where discovery of causal relationships from high dimensional time series data is of interest. The project is expected to advance the theoretical foundations of data analytic techniques for time-series data and provide a unified framework that can easily integrate domain knowledge.  The results of this project can be expected to significantly advance the current state of the art in eliciting insights regarding causal relationships from time series data. In addition to the core research advances, this project contributes easy-to-use software based on workflows for teaching  machine learning to students, researchers and practitioners with a broad range of backgrounds. Educational and outreach activities include  new interdisciplinary courses,  workshops,  tutorials, and high-school visits. Software and data resulting from this work will be freely disseminated to the broader research and educational community. Additional information about the project can be found at: http://www-bcf.usc.edu/~liu32/uscTimeSeries.htm."
"1350035","CSR: EAGER: Multi-physiological Signal Processing Architectures for Seizure Detection","CNS","CSR-Computer Systems Research","09/15/2013","09/06/2013","Tinoosh Mohsenin","MD","University of Maryland Baltimore County","Standard Grant","Marilyn McClure","08/31/2016","$99,804.00","","tinoosh@umbc.edu","1000 Hilltop Circle","Baltimore","MD","212500002","4104553140","CSE","7354","7354, 7916","$0.00","The objective of this project is to overcome the limitations of sensor artifacts (noise), false detection, and energy/power constraints by combining the analysis of multiple physiological signals through specialized hardware which implements a multi-layer classification technique comprised of a unique sequence of signal processing and machine learning functions to distill time series data. The hypothesis is that a hybrid architecture can leverage common operations and communication patterns between DSP and machine learning to support these computations more efficiently than traditional digital signal processors and general purpose processors.  This hypothesis is explored in the context of wearable seizure detection, using traces of EEG and other physiological sensor data obtained from the Epilepsy Center at University of Maryland Medical Center. <br/><br/>Success of this exploratory research could have a significant impact for robust and efficient monitoring and use of continuous multi-physiological data for patients. Just in the context of epilepsy, it could enable seizure detection and caregiver alerts, which is important at night when seizures can happen without someone to help nearby.  Longer term potential impacts extend to human-centered cyber-physical systems, cyber-security, and unmanned vehicles."
"1320725","SHF: Small: Compiling Custom Hardware Accelerators from Graph Algorithms","CCF","DES AUTO FOR MICRO & NANO SYST","07/01/2013","06/28/2013","James Hoe","PA","Carnegie-Mellon University","Standard Grant","Sankar Basu","06/30/2017","$450,000.00","","jhoe@cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7945","7923, 7945","$0.00","Many emerging applications in machine learning and data mining can be cast as graph computations. Efficient low-power implementations of graph computations promise disruptive capabilities for the increasingly ubiquitous embedded and mobile platforms. The project is building the GraphGen compiler to overcome the complexity and difficulty of creating graph computation hardware accelerators that are needed to meet the demanding power and performance constraints of embedded and mobile systems.  The GraphGen compiler is a general-purpose compiler (within the graph computation domain) to handle arbitrary graph applications based on varying graph structures (e.g., grid-shaped, planar, natural graphs) that may be static or dynamically changing (e.g., updated by streaming data) following different execution strategies (e.g., synchronous vs. asynchronous).  GraphGen implementation mapping makes use of reusable hardware implementation templates to allow the same graph computation specification to be efficiently mapped onto different target platforms.  Overall, the GraphGen compiler captures knowledge from both application developers (in graph specifications) and hardware designers (in the reusable implementation templates), and bridges the gap between the two camps through automatic mapping of a specification to a template to yield a highly efficient embedded implementation tuned to the application developer's design objectives.<br/><br/>The continued exponential increase in transistors-per-die, coupled with advances in sensors and breakthrough algorithms in machine learning and data mining, have resulted in a revolution in the embedded and mobile application space. Graph computation is an important enabling computation paradigm for many of these emerging applications. The GraphGen compiler can facilitate rapid adoption of these applications into embedded and mobile devices by allowing domain experts to automatically translate their graph computation algorithms onto efficient FPGA-accelerated embedded platforms. This new capability has the potential to spark new research in graph computation and embedded hardware architectures by providing a common design automation environment that bridges the gap between application domain experts and hardware designers, thus benefiting industry."
"1311875","AIR Option 1:  Technology Translation - Smart Power Protection Devices for Photovoltaic Installations","IIP","Accelerating Innovation Rsrch","05/01/2013","04/28/2013","Bradley Lehman","MA","Northeastern University","Standard Grant","Barbara H. Kenny","10/31/2015","$149,998.00","","lehman@ece.neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","ENG","8019","8019","$0.00","This PFI: AIR Technology Translation project focuses on translating smart fault detection methods for photovoltaic (PV) installations to fill the technology gap of identifying and protecting against dangerous electrical faults in solar PV installations.   The translated science/technology has the following unique features: 1) it uses machine learning to identify fault type and fault location in the PV arrays; 2) it will be able to clear faults that traditional fuses or other fault protection devices cannot presently clear; 3) it will be a fully integrated fuse holder and box that has user interface allowing the user to monitor the PV array performance. The new solar PV protection technology will be able to isolate, clear, and identify the type and location of PV fault. This includes several types of faults that presently pose fire hazards to PV installations, such as line-to-line faults that may occur when branches or wires accidentally connect two different PV arrays in a big installation.  The new, smart fuse protection adds exemplary safety, reliability, and even increased energy delivery when compared to the leading competing science/technology, such as traditional fuses or circuit breakers in this market space. <br/><br/>The project accomplishes this goal by running smart fault detection algorithms that utilize machine learning, nonlinear model prediction of PVs, intelligent measurements of PV currents and voltages, resulting in a method to identify hazardous fault location and types.  Prototype smart over-current protection devices will be built with miniaturized self-contained electronics. The protection devices will be tested in PV installations to demonstrate their commercial viability and benefits.<br/><br/>The partnership engages Mersen USA Newburyport-LLC to provide guidance in the fuse/over-current protection manufacturing and commercial market. Mersen will help manufacture the prototype smart fuse holders and test them in their lab PV facilities. Throughout the project, Mersen will lead product testing, customer surveys and other aspects of commercialization and financing as they pertain to the potential to translate the smart PV fuse protection science/technology along a path that may result in a competitive commercial reality.  The potential economic impact is expected to be 1) the creation of a new product market in smart PV protection fuses projected to reach more than $750k annually within 2 to 3 years of the project conclusion and 2) the further acceptance of solar PV energy in the consumer marketplace by making the PV installations safer to the consumer. The smart PV protection devices will contribute to the U.S. competitiveness in this PV installation market, since each installation of solar PV array must utilize (usually multiple) over-current protection devices, such as fuses.   The societal impact, long term, will be improved safety of any PV installation, increased energy yield due to immediate knowledge of fault, and better diagnosis in real-time of any reductions of energy yield caused by PV arrays that may need repair."
"1315882","SBIR Phase I: A Smartphone Incentive System to Align Electricity Demand with Intermittent Supply","IIP","SMALL BUSINESS PHASE I","07/01/2013","06/20/2013","Roderick Hinman","HI","Ikehu Natural, LLC","Standard Grant","Prakash Balan","12/31/2013","$150,000.00","","rod@auroraresearch.com","75-5660 Kopiko St., Suite C7154","Kailua Kona","HI","967403611","5038039194","ENG","5371","153E, 5371, 8032, 9150","$0.00","This Small Business Innovation Research Program (SBIR) Phase I project will address challenges that electric utilities face in maintaining stability of their grids as more and more intermittent renewable energy generating systems are added to them. Solar and wind are variable and thus complicate the process of balancing energy generation with consumption. The objective of this project is to show the viability of a software-based solution to assist with this balance, reducing the need for expensive energy storage. This service will provide real-time incentives to consumers via their smart phones to reduce or increase electricity usage in response to a request. Another objective is to demonstrate that machine-learning algorithms can help the service estimate consumer response rate based upon environmental, locational, and personal factors. The project will include a preliminary trial with residential electricity consumers where they will be sent requests in response to simulated conditions; some will be able to adjust loads from their smart phones. Electricity usage and other data gathered from the trial will be used to train and test the machine-learning estimators and to evaluate methods that validate whether the consumer performed as promised. <br/><br/>The broader impact/commercial potential of this project will be to increase utilities' ability to add renewable electricity generation to their grids while minimizing the need for expensive compensating solutions like gas turbine peaking plants or energy storage. It will also lower peak electricity demand, further reducing the need for peaking plants and therefore reducing costs for utilities and consumers. The end result will be less reliance on fossil fuel for electricity generation, improving the nation's energy security and reducing greenhouse gas emissions. A second impact will be to improve customers' education and engagement with their energy consumption. As a result, consumers may be more likely to purchase and install energy-efficient appliances and home improvements, further reducing energy consumption. An increased technical understanding of that relationship, particularly given the proliferation of smartphones and related applications, will improve the ability to create incentives to foster grid friendly energy consumption."
"1258485","IGERT-CIF21: Big Data U: A Program for Integrated Multidisciplinary Education and Research for Big Data Science","DGE","OFFICE OF MULTIDISCIPLINARY AC, IGERT FULL PROPOSALS, Special Projects - CNS, , IIS Special Projects, CDS&E-MSS, CIF21-IGERT","09/01/2013","07/31/2014","Magdalena Balazinska","WA","University of Washington","Continuing Grant","John Weishampel","08/31/2019","$2,800,000.00","E. Virginia Armbrust, Andrew Connolly, Carlos Guestrin, Magdalena Balazinska, David Beck","magda@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","EHR","1253, 1335, 1714, 1798, 7484, 8069, 8090","1335, 7433, 9179, SMET","$0.00","This Integrative Graduate Education and Research Traineeship (IGERT) award provides Ph.D. students at the University of Washington with multidisciplinary training in computer science, statistics, and domain sciences (oceanography, astronomy, chemistry, and genome sciences). Through this blended approach, trainees will learn how to manage, analyze, and visualize increasingly large amounts of data (known as ?Big Data?), thereby being prepared to address the challenges of cyberinfrastructure in the 21st century. <br/><br/>Intellectual Merit: By developing a new Ph.D. program that involves partnerships with 11 leading companies and national labs in the field of Big Data, this program provides trainees with a collaborative approach to processing, scaling, and modeling massive and complex data sets for the scientific community. Trainees learn to create new statistical and machine learning techniques needed to manage large data sets. Additionally, this program builds an open-source system for scientists worldwide to access and analyze Big Data through a Cloud service.<br/> <br/>Broader Impacts: This IGERT traineeship program aims to create a new Big Data curriculum that will be delivered online and through University of Washington outreach initiatives. The program also prepares a new generation of scientists with the interdisciplinary tools to approach  problems that will arise in the field of cyberinfrastructure. Moreover, this program promotes the development of a diverse STEM workforce by recruiting and training underrepresented groups, women, and students with disabilities, particularly through a partnership with the AccessComputing Alliance and the University of Washington DO-IT program.<br/><br/>IGERT is an NSF-wide program intended to meet the challenges of educating U.S. Ph.D. scientists and engineers with the interdisciplinary background, deep knowledge in a chosen discipline, and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to establish new models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries, and to engage students in understanding the processes by which research is translated to innovations for societal benefit."
"1320527","III: Small: Statistical Learning Algorithms for Micro-Event Time Series Data","IIS","Info Integration & Informatics","10/01/2013","07/06/2015","Padhraic Smyth","CA","University of California-Irvine","Continuing grant","Sylvia Spengler","09/30/2018","$499,888.00","","smyth@ics.uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","CSE","7364","7364, 7923","$0.00","This project is focused on the development of new data analysis tools for analyzing personal data archives, namely, the streams of digital data that are routinely recorded reflecting different aspects of individuals' daily lives. Examples of such data include keystrokes, email histories, text messages, social media interactions, microblogs, as well as records of physical activity, diet, and sleep. As sensors become more accurate and cheaper and as data storage becomes effectively zero cost, there is increasing demand for data analysis tools that allow individuals to analyze and gain insight into their own personal data. This research project is developing new statistical machine learning algorithms for analyzing these types of data. The project has a particular focus on the development of models and algorithms to handle personal archives in the form of event time-series data, consisting of logs of time-stamped events involving interactions with other individuals as well as textual and other metadata. Testbed data sets being used to support this research include publicly-available archives of email histories, software development discussions, Twitter microblogs, Wikipedia editing interactions, and physical proximity data. In terms of broader societal impact, the data analysis tools being developed by this project have the potential to significantly transform how individuals analyze their personal data to better understand and monitor their physical and mental health."
"1419210","BIGDATA: Small: DA: A Random Projection Approach","IIS","Big Data Science &Engineering","12/01/2013","04/23/2018","Ping Li","NJ","Rutgers University New Brunswick","Standard Grant","Sylvia Spengler","04/30/2019","$447,380.00","","pingli@stat.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","8083","7433, 7923, 8083","$0.00","With the advent of Internet, numerous applications in the context of network traffic, search, and databases are faced with very large, inherently high-dimensional, or naturally streaming datasets. To effectively tackle these extremely large-scale practical problems (e.g.,  building statistical models from massive data, real-time network traffic monitoring and anomaly detection), methods based on statistics and probability have become increasingly popular. This proposal aims at developing theoretical, well-grounded statistical methods for massive data based on random projections, including data stream algorithms, quantized projection algorithms, and sparse projection algorithms.<br/><br/>Massive data are often generated as high-rate streams. Network traffic is a typical example. Effective measurements (and updates) of network traffic in real-time using small storage space are crucial for detecting anomaly events, for example the DDoS (Distributed Denial of Service) attacks. For many applications such as databases and machine learning, appropriate quantization of random projections will substantially improve the accuracies (in terms of variance per bit) and provide efficient indexing and dimension reductions to facilitate efficient search and learning. The proposed research will tackle a series of mathematically challenging problems in the development of random projections. A wide range of statistical learning and numerical linear algebra algorithms will be re-engineered to take advantage of the state-the-art projection methods.<br/><br/>These days, many industries such as search are in urgent demand for statistical algorithms which can effectively handle massive data. It is expected that algorithms to be developed in this proposal will be integrated with parallel platforms, to solve truly large-scale real-world problems. Research results will be disseminated to practitioners through publications, conference presentations, industry visits and collaborations, tutorials, and open-source distributions. Many of the proposed research problems involve statistical analysis and may continue to help attract statisticians/mathematicians to work on area of big data. The proposed research activities will engage both undergraduate and graduate students in statistics and engineering, through innovative curriculum and research training."
"1250914","BIGDATA: Small: DA: A Random Projection Approach","IIS","Big Data Science &Engineering","05/01/2013","04/17/2013","Ping Li","NY","Cornell University","Standard Grant","Balasubramanian Kalyanasundaram","02/28/2014","$462,509.00","","pingli@stat.rutgers.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","8083","7433, 7923, 8083","$0.00","With the advent of Internet, numerous applications in the context of network traffic, search, and databases are faced with very large, inherently high-dimensional, or naturally streaming datasets. To effectively tackle these extremely large-scale practical problems (e.g.,  building statistical models from massive data, real-time network traffic monitoring and anomaly detection), methods based on statistics and probability have become increasingly popular. This proposal aims at developing theoretical, well-grounded statistical methods for massive data based on random projections, including data stream algorithms, quantized projection algorithms, and sparse projection algorithms.<br/><br/>Massive data are often generated as high-rate streams. Network traffic is a typical example. Effective measurements (and updates) of network traffic in real-time using small storage space are crucial for detecting anomaly events, for example the DDoS (Distributed Denial of Service) attacks. For many applications such as databases and machine learning, appropriate quantization of random projections will substantially improve the accuracies (in terms of variance per bit) and provide efficient indexing and dimension reductions to facilitate efficient search and learning. The proposed research will tackle a series of mathematically challenging problems in the development of random projections. A wide range of statistical learning and numerical linear algebra algorithms will be re-engineered to take advantage of the state-the-art projection methods.<br/><br/>These days, many industries such as search are in urgent demand for statistical algorithms which can effectively handle massive data. It is expected that algorithms to be developed in this proposal will be integrated with parallel platforms, to solve truly large-scale real-world problems. Research results will be disseminated to practitioners through publications, conference presentations, industry visits and collaborations, tutorials, and open-source distributions. Many of the proposed research problems involve statistical analysis and may continue to help attract statisticians/mathematicians to work on area of big data. The proposed research activities will engage both undergraduate and graduate students in statistics and engineering, through innovative curriculum and research training."
"1320498","SHF:Small: Accurate and Computationally Efficient Predictors of Java Memory Resource Consumption","CCF","PROGRAMMING LANGUAGES","09/01/2013","08/12/2013","J. Eliot Moss","MA","University of Massachusetts Amherst","Standard Grant","Anindya Banerjee","08/31/2017","$450,000.00","Benjamin Marlin","moss@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","7943","7923, 7943","$0.00","The Java programming language is widely-used and of great commercial and economic significance. It is favored in part because it features automatic management of the computer memory resources it uses, simplifying such management for the programmer.  Memory management in Java (and other managed languages) has reached a plateau in cost and effectiveness because most current techniques are tuned based on a small number of coarse-grained measures gathered while programs run.  Substantial improvement might be gained from using more accurate estimation of current and near-future memory use to drive better memory management decisions.  This would reduce the time, memory, and energy requirements to run Java programs. This is of significance to the full range of Java applications from small embedded systems through laptops and desktops to large servers.  There is therefore an urgent need for techniques to derive better online predictors of Java memory use.<br/><br/>The long-term goal of the research program this award will support is to substantially improve memory allocation and garbage collection effectiveness by using better online predictors to drive more sophisticated allocator and collector decisions.  The objective of this particular project is to develop machine learning techniques that induce accurate and computationally efficient predictors of characteristics of Java memory allocation that influence memory manager performance.  Examples include predicting the volume of objects that become ""garbage"" (can be reclaimed and reused for future allocations), as well as objects that will be in use for a long time and will not become garbage soon.  The approach is to learn models that predict memory usage based on features compiled from observable run-time events like calls to particular methods or allocations of certain objects. Data to learn models will be obtained from analysis of detailed program execution traces. Features will be selected that are both informative of memory use and computable with low space and time overheads.  Programs will then be modified to compute these features as they run, and real-time predictive models will be used to predict future memory usage as programs execute.  These predictions will be used to improve memory management performance.  This will be accomplished by, for example, improving the timing of garbage collection so that it occurs at points during program execution that result in higher memory reclamation with lower effort."
"1337614","MRI: Acquisition of an Electroencephalography (EEG) System for Integrated Cognitive, Perceptual, and Social Neuroscience Research at Colgate University","BCS","MAJOR RESEARCH INSTRUMENTATION","09/01/2013","08/28/2013","Bruce Hansen","NY","Colgate University","Standard Grant","John E. Yellen","08/31/2016","$199,307.00","Arnold Ho, Spencer Kelly, Douglas Johnson, Caroline Keating","bchansen@colgate.edu","13 Oak Drive","Hamilton","NY","133461398","3152287457","SBE","1189","1189","$0.00","With support from the Major Research Instrumentation Program, Dr. Bruce C Hansen and his collaborators will purchase a state-of-the-art electroencephalography (EEG) system from Electrical Geodesics Incorporated (GES 300 system) for shared use by faculty and undergraduate students in Colgate University's Department of Psychology and Neuroscience Program. The EEG technique itself involves placing surface electrodes on the scalp of a human participant and recording electrical signals generated by the brain in real-time, thereby allowing for a wide variety of analyses focused on the temporal localization of different brain signals.  The system will enable this group to adopt an integrated model for understanding human behavior by blending traditional psychological methodology with functional neuroelectric activity in humans.  <br/><br/>The scientists involved in this proposal are all active researchers from a broad range of disciplines including cognitive, perceptual, and social psychology.  Five research projects (each consisting of several studies) are proposed. Project 1 uses machine learning for the classification of visual evoked potentials (VEPs) to investigate the time course of the brain's recognition and categorization of complex visual scenes in order to understand how such representations guide actions in different environments.  Project 2 examines how biases in perception of novel social categories (e.g., multiracial groups), as well as individual differences in opposition to equality, contribute to the perpetuation of group-based social inequality (e.g., racial inequality).  It will use event related potentials (ERPs) to explore how social motivations (e.g., anti-egalitarianism) and social contexts (e.g., economic progress for ethnic minorities) influence the way people react to multiracial individuals.  Project 3 proposes to combine EEG frequency band power analyses with behavioral paradigms in order to establish a more direct and conclusive indicator of whether encoding or retrieval based memory processes determine the impact of changing task demands on development of expertise.  Project 4 investigates how social power evokes self-deception and, as a consequence, enhances persuasive abilities. Specifically, the project combines traditional behavioral measures with ERP analysis to trace the timing of brain signals that selectively unleash changes in awareness. In essence, it aims to elucidate how lying to others may begin with lying to the self.  Project 5 will utilize ERPs to explore whether embodied language instruction (i.e., speech, gesture, facial expression, eye gaze, etc.) is effective for inducing neural changes in second-language (L2) learning in two different contexts: face-to-face versus online instruction. The project will focus on components that reflect early perceptual and late semantic processes in the learning of novel speech sounds and new words.<br/><br/>A shared EEG system at Colgate will allow this group to directly engage their students in laboratory techniques that unite psychology and neuroscience into one cohesive field of study, thereby fostering non-traditional research connections that should spur fresh insights and creative new areas of study.  Such an approach will no doubt yield students who are better prepared for graduate research labs at an early stage (most undergraduates at other schools will not have this sort of highly technical experience), thereby guaranteeing the rapid advancement of the broader field of science.  Lastly, the majority of psychology and neuroscience concentrators at Colgate are female, and the enhanced training made possible by a shared EEG system will therefore increase the competitive representation of women pursuing advanced degrees in a STEM field."
"1253908","CAREER: Robust Processing of Data Streams in Real Time","CNS","Special Projects - CNS, CSR-Computer Systems Research","03/01/2013","01/10/2017","Shrideep Pallickara","CO","Colorado State University","Continuing Grant","Marilyn McClure","02/28/2019","$504,000.00","","shrideep@cs.colostate.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","CSE","1714, 7354","1045, 7354, 9178, 9251","$0.00","This project investigates the problem of scheduling the processing of collections of streams of medical sensor data, with a goal of providing high-confidence per-packet service guarantees that are robust to variability in the stream generation and concomitant changes in the loads at the distributed set of resources where streams are processed.  Given the NP-Hard complexity of optimal stream scheduling and the need to handle streams that are stochastic in nature, the service guarantees are probabilistic.  The approach makes use of statistical and machine learning techniques, harnessing a mix of application-independent and application-dependent features to adaptively orchestrate stream processing over a collection of resources with dynamic utilization profiles while retaining the ability to prioritize processing under heavy load, for multiple concurrent applications. Data from clinical and assisted living settings are used to evaluate the efficacy of solutions.<br/><br/>Health care and homeland security can benefit from this research, as well as experimental science.  Skyrocketing healthcare costs have coincided with the proliferation of electronic monitoring devices in medical and assisted living environments, which generate data streams of patient data.  Timely monitoring and analysis of these streams can detect emergencies early and improve patient outcomes, but failure can be fatal. Improvements in the efficiency and robustness of automated medical data stream processing translate to lower costs and improved outcomes. There are analogous opportunities in homeland security, where chemical and biological sensor data must be processed in real-time for threat evaluation. Open-source software produced as part of this research, which can be configured over an arbitrary number of machines to process a large number of streams in a variety of settings, lowers entry barriers for scientists who need to process observational data in their applications.  The project will also provide educational opportunities for students, and middle school outreach activities targeted at improving assimilation of mathematical concepts among Native American students."
"1322079","Workshop on Integrating Approaches to Computational Cognition","BCS","PERCEPTION, ACTION & COGNITION, ROBUST INTELLIGENCE","03/15/2013","03/11/2013","Matthew Jones","CO","University of Colorado at Boulder","Standard Grant","Betty H. Tuller","02/28/2014","$42,423.00","","mcj@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","SBE","7252, 7495","7556, 7252","$0.00","This project is to host a workshop focused on identifying frontiers for collaborative research integrating mathematical and computational modeling of human cognition with machine learning and machine intelligence. The workshop will bring together leading researchers who are working at the intersection of the fields. The goal is to initiate dialogue between people working within different computational frameworks, to identify points of synthesis where new scientific insights or breakthroughs are most likely, and to identify gaps and obstacles to collaboration between the two communities. A summary report will be produced that details the products of the discussion, highlighting prospects identified for future collaborative research."
"1315878","SBIR Phase I: Automatic Extraction of Financial Data from Text","IIP","SMALL BUSINESS PHASE I","07/01/2013","12/27/2013","Hassan Alam","CA","BCL Technologies","Standard Grant","Peter Atherton","06/30/2014","$179,999.00","","hassana@bcltechnologies.com","3031 Tisch Way","San Jose","CA","951282533","4085572080","ENG","5371","5371, 8032, 8039","$0.00","The innovation is the development of a linguistically-driven machine learning system that will extract financial data from financial text such as 10-Q documents, with an accuracy of over 85%. To be useful to the analysts, financial data needs to be a triple of ""Financial Concept"", ""Numeric Value"" and ""Date Range."" Because of the complexity of sentences in the financial domain, detecting the Financial Concept and attaching it to the correct Numeric Value and Date Range remains a challenge. Current financial extraction systems record an accuracy of less than 50%. The proposed method will use a combination of Financial Named Entity Recognition, Semantic Nearest Neighbor location and Support Vector Machines to improve Financial Concept detection, attachment and semantic tagging to 85%. By combining these methods in its Phase II Research, the innovation is the development of an end-to-end 'Automatic Extraction of Financial Data from Text' system that is usable by computerized systems. At the end of Phase I, the proposed method will demonstrate the feasibility of financial data extraction on the Notes section of 10-Q documents. The Phase II system will be designed to scale up to handle very large data sets, including non-American English documents in near real-time.<br/><br/>The broader/commercial impact of Automatic Extraction of Financial Data from Text system is the availability of relevant financial information in computer-readable format with high accuracy in near real time. Currently, data embedded in financial text are extracted manually by hundreds of people working for data warehouses. This manual effort takes on the order of weeks making the bulk of the data unavailable in easily computer-usable forms in real time. The benefit of Automatic Extraction of Financial Data from Text will be in three areas: 1. Algorithmic Trading programs will be able to use all data published worldwide immediately after the data is published; 2. Financial data warehouses will be able to provide much larger types of data concepts - there are 18,498 concepts in the US Generally Accepted Accounting Principles taxonomy versus less than 180 available in commercial data warehouses; 3. There will be increased transparency in the financial market as financial information embedded in the text becomes computer readable. The algorithmic trading was estimated to reach over $5 Trillion with 750 Billion shares traded, generating a profit of over $600 Million in 2012. The impact of financial transparency is an intangible benefit that will improve financial market efficiency."
"1404673","SHB: Medium: Collaborative Research: Crafting a Human-Centric Environment to Support Human Health Needs","IIS","Info Integration & Informatics, Smart and Connected Health","08/31/2013","04/01/2016","Sajal Das","MO","Missouri University of Science and Technology","Standard Grant","Sylvia Spengler","08/31/2017","$251,919.00","","sdas@mst.edu","300 W 12th Street","Rolla","MO","654096506","5733414134","CSE","7364, 8018","7364, 7924, 8018, 9150, 9251","$0.00","Researchers and providers alike are recognizing that human-centric smart environments can provide health monitoring services and support aging in place through adaptive interventions. The need for the development of such technologies is underscored by the aging of the population, the cost of formal health care, and the importance that individuals place on remaining independent in their own homes. The goal of this project is to design, implement, and evaluate in-home techniques for generating reports of activities and social interactions that are useful for monitoring well being and for automating intervention strategies for persons with dementia.  The plan is to design machine learning techniques that make effective use of sensor data to perform automated activity monitoring and prompting-based interventions that are beneficial for the residents as well as for their caregivers and family. The environment is human-centric because it learns information about its human residents and uses this information to provide activity-aware monitoring and intervention services. By transforming everyday environments into smart environments, many older adults with cognitive and physical impairment can lead independent lives in their own homes. A key component of this project is an evaluation of the technologies in actual homes with volunteer older adults and thus will assess the technologies for acceptance with the target population. <br/><br/>This project addresses NSF?s Smart Health and Wellbeing goal of leveraging computational expertise leading to fundamental advances in the development of algorithms to create improvements in safe, effective, and patient-centered health and wellness services. Through design of a Gerontechnology class we are training  students to design and use these technologies. This effort includes REU and IGERT students in the research project, which involves students from underrepresented groups in this multidisciplinary, collaborative effort.  To facility community-wide use, comparison and collaboration, all of our datasets, tools, and course materials will be disseminated from our project web page."
"1301976","AF: Medium: Algorithmic Crowdsourcing Systems","CCF","ALGORITHMIC FOUNDATIONS, COMPUT GAME THEORY & ECON","09/01/2013","09/04/2015","David Parkes","MA","Harvard University","Continuing grant","Tracy Kimbrel","08/31/2018","$999,977.00","Yiling Chen, Yaron Singer","parkes@eecs.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","7796, 7932","7924, 7926, 7932","$0.00","Crowdsourcing refers to the paradigm of eliciting work, typically in small pieces, from a global population of workers, each making dynamic decisions about which task to complete next, and for whom. Crowdsourcing systems are inherently algorithmic because the coordination task would otherwise be overwhelming. Algorithms take the place of the management and incentive structure of traditional firms, responsible for optimizing workflows, determining payments, matching workers with tasks, and learning models of workers and tasks.<br/><br/>With the potential to transform the way in which productive effort is allocated, crowdsourcing is already finding application across a broad range of domains, such as citizen science, the transcription of documents, and collecting training data for use in machine learning for bridging the gap to human-level intelligence.The goal of this project is to develop a cohesive theory of algorithms and incentives for the design of crowdsourcing systems.<br/><br/>The proposed research addresses key challenges in harnessing decentralized resources and capabilities for productive work, and seeks to develop a theoretical framework to guide the design of crowdsourcing systems that align incentives, are adaptive and robust, and achieve high performance for low cost.The investigators are interested in the roles of matching and pricing, in developing methods for eliciting high quality, unverifiable contributions, and in embracing realistic models of human behavior.<br/><br/>The investigators will  organize workshops on social computing at upcoming Conferences on Electronic Commerce (EC13 and EC14) and they continue to recruit women and other under-represented groups to join the project."
"1353019","EAGER: Automatically Generating Formal Human-Computer Interface Designs From Task Analytic Models","IIS","HCC-Human-Centered Computing","09/15/2013","09/04/2013","Matthew Bolton","IL","University of Illinois at Chicago","Standard Grant","Anthony Hornof","02/28/2014","$149,976.00","","mbolton@buffalo.edu","809 S. Marshfield Avenue","Chicago","IL","606124305","3129962862","CSE","7367","7367, 7916","$0.00","The concurrent nature of human-computer interaction (HCI) can result in situations unanticipated by designers.  Usability may not always be properly maintained or human operators may not be able to complete the task goals that a system was designed to support.  This can result in poor adoption of the system, decreased productivity with its use, or unsafe operating conditions.  Mathematical tools and techniques called ""formal methods"" exist for modeling and providing proof-based evaluations of different elements of HCI including the human-computer interface, the human operator's task analytic behavior, and usability.  Unfortunately, these approaches require the creation of formal models of interface designs, something that is non-standard practice and prone to modeling error.  This project will show that a formal-methods approach can be used to automatically generate formal human-computer interface designs that are guaranteed to adhere to usability properties and to support human operator tasks.  Specifically, a system that uses the L* machine learning algorithm will be created that will generate formal interface designs using task analytic behavior models and formal representations of usability properties.<br/><br/>The researchers will implement an interface generation system, test its performance with a suite of benchmark examples, and evaluate its ability to generate an interface for a realistic application.  To implement the generator, the researchers will first construct an oracle system capable of accepting or rejecting interface state transition sequences based on analyst-specified task models and usability properties.  This oracle system will be connected to an implementation of the L* algorithm that will progressively learn a formal interface model by observing how generated sequences of interface state transitions are accepted or rejected by the oracle.  Artificial test cases that exploit the different features of the system will be used to generate interface designs, and formal verification will be used to check that the designs exhibit the intended properties.  The system will be used to generate the human-computer interface for programming a patient controlled analgesia pump, a medical device that automatically delivers pain medication to patients intravenously.  The generated interface will then be compared against the formal interface design standard that exists for these devices.<br/><br/>The automatic generation of human-computer interface designs from task analytic models and usability properties constitutes a novel approach to user-centered design.  By using this method in the creation of interfaces, designs will be guaranteed to always exhibit certain properties.  This will potentially help ensure that designs will be accepted by users, improve the associated system's efficiency, and facilitate safer operation.  The formal representation of user interfaces that result from the implementation of this method will also permit HCI designers to pursue formal analysis and verification of other interface properties, and will facilitate the automated generation of test cases for usability verification and certification purposes.<br/><br/>Broader Impacts:  The proposed research has the potential to significantly change the way human-computer interfaces are designed.  By guaranteeing that generated interfaces are always usable, this research could improve the usability and safety of user interfaces across many domains.  The performance guarantees of the generated designs could allow development and testing times to be reduced, thus decreasing development and software costs.  This work will also enhance the education and research experience of UIC's diverse engineering student body.  The computational resources acquired for this work will be made available to student for research projects and study results will be incorporated into the curriculum of the PI's graduate and undergraduate courses.  Project results will be presented at conferences by student researchers and published with open access in high quality journals.  A dedicated website will be used to rapidly disseminate results and tools produced during this effort."
"1308045","Cognitive Prediction-Enabled Online Intelligent Fault Diagnosis and Prognosis for Wind Energy Systems","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","06/01/2013","05/09/2013","Wei Qiao","NE","University of Nebraska-Lincoln","Standard Grant","Radhakisan Baheti","05/31/2017","$359,852.00","","wqiao@engr.unl.edu","151 Prem S. Paul Research Center","Lincoln","NE","685031435","4024723171","ENG","7607","155E, 1653, 9150","$0.00","The objective of this research is to study the use of cognitive prediction paradigms for online fault diagnosis and prognosis to enable condition-based smart maintenance for wind energy systems.  The approach is to:  (1) study the use of time and frequency domain data mining methods to effectively extract the features of faults in a wind turbine from the signals acquired from the wind turbine condition monitoring system; and (2) study the use of artificial neural networks and machine learning for intelligently diagnosing and prognosing faults, predicting the lifetime, and quantitatively evaluating the physical condition of the wind turbine using the extracted fault features.<br/><br/>Intellectual Merit:  This project will create innovative cognitive prediction-based models and computational algorithms to enhance condition awareness of geographically distributed wind energy systems.  The findings of this research are highly transformable and will provide capabilities for enabling condition-based intelligent maintenance for other energy conversion and engineered systems.<br/><br/>Broader Impacts:  The outcome of this project will further exploit the benefits of wind power by successfully reducing cost and improving reliability of wind energy systems and, therefore, will make wind energy a reliable, cost-competitive source of clean electricity.  The increasing use of wind power will benefit various sectors of the nation's economy and contribute to sustainable development of society.  Multiple fields covered by this project are areas where a talent shortage is projected in the United States, particularly in the Midwest.  The proposed activities will provide a unique learning platform for young individuals to become skilled professionals."
"1312903","Functional Data Analysis for Synoptic Time-Domain Astronomy","AST","STELLAR ASTRONOMY & ASTROPHYSC, EXTRAGALACTIC ASTRON & COSMOLO","09/15/2013","07/14/2016","Thomas Loredo","NY","Cornell University","Continuing Grant","Nigel Sharp","07/31/2019","$560,614.00","David Ruppert","loredo@astro.cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","MPS","1215, 1217","1206, 7433, 7480, 7569","$0.00","Cosmic phenomena display a broad range of time-dependent behavior.  Until recently, resources were sufficiently limited that only selected populations could be extensively monitored in the time domain, but modern telescopes and detectors can survey cosmic volumes much more quickly than before.  As a consequence, synoptic time-domain astronomy - the study of cosmic variability across sizeable populations - is becoming a dominant mode of study.  Such surveys provide not simply more data, but a different kind of data, requiring a new approach to statistical learning capable of extracting information from large ensembles of multivariate, irregularly and asynchronously sampled light curves.  Astronomy is not the only discipline facing this change in the nature of data.  Functional data analysis (FDA) is a rapidly growing area of statistics that addresses inference from datasets that sample ensembles of related functions.  The present interdisciplinary project will use FDA concepts and techniques to develop new methods to address several problem areas of survey data analysis, from astrophysical modeling of variability using catalog data, to the detection of variable and transient sources in images.  One main application will be the analysis of light curves of periodic variable stars using functional mixed effects models.  In particular, Cepheid variable stars are a foundation of the cosmic distance scale, and the hope is that these new functional models can provide more accurate brightness estimates and therefore ease limits on how accurately important cosmological parameters can be measured.  Other problems well suited to FDA methods include the detection of dim intermittent signals, and the classification of sources from features in their light curves.<br/><br/>The team includes experts in astronomy and in statistics, and will pursue innovative research in both disciplines.  This research will enhance their partnership by direct collaboration, by dissemination of results in both communities, and by training a graduate student in information sciences to work on problems in astronomy.  The methods produced will improve the science return on the large investments being made in astronomical surveys.  The investigators are also involved with the Penn State University's interdisciplinary Summer School in Statistics for Astronomers, helping to train young astronomers in advanced statistics and machine learning methods."
"1253731","CAREER: Towards Context-Aware, Self-Organizing Wireless Small Cell Networks","CNS","CAREER: FACULTY EARLY CAR DEV, Networking Technology and Syst","01/01/2013","05/05/2014","Walid Saad","FL","University of Miami","Continuing grant","Thyagarajan Nandagopal","10/31/2014","$191,250.00","","walids@vt.edu","1320 S. Dixie Highway Suite 650","CORAL GABLES","FL","331462926","3052843924","CSE","1045, 7363","1045","$0.00","Providing seamless, high quality wireless service anytime and anywhere requires substantial structural changes in today's macro-cellular networks. One such change, introducing small cell base stations, is seen as a highly promising solution. However, it requires meeting fundamental challenges: 1) nodes? self-organization, 2) network heterogeneity, and 3) high sensitivity of resource allocation to the system parameters. The proposed research addresses these challenges by exploring a dimension that has often been overlooked: the user's context. To achieve this goal, first, machine learning techniques are proposed to extract context from three dimensions: device, geo-location, and social metrics. Then, context-aware resource management schemes are developed by advancing novel techniques from matching theory - a powerful tool from economics and game theory. Subsequently, the learned context is leveraged to devise cooperative small cell models using new tools from coalitional game theory. Comprehensive evaluation is done via testbed implementation and software simulations. <br/><br/><br/>The developed analytical tools will lay the foundations of context-aware, self-organizing small cell networks and will impact multiple disciplines such as communications, game theory, and social sciences. The generated results will provide fresh ideas for developing new small cell products. The research is fully integrated into the educational plan via incorporation in new and existing courses as well as training students via mentoring, participation in testbed development, and internships at industrial labs. A developed small cell educational tool will foster this integration via new hands-on activities and demonstrations to the community. Specialized outreach activities will contribute to increasing the participation of minority high school students in science and engineering."
"1332693","WORKSHOP ON TRAINING STUDENTS TO EXTRACT VALUE FROM BIG DATA","DMS","INFRASTRUCTURE PROGRAM, STATISTICS, INFORMATION TECHNOLOGY RESEARC, CDS&E-MSS","08/15/2013","08/08/2013","Michelle Schwalbe","DC","National Academy of Sciences","Standard Grant","Gabor J. Szekely","07/31/2015","$150,000.00","","mschwalbe@nas.edu","500 FIFTH STREET NW","Washington","DC","200012721","2023342254","MPS","1260, 1269, 1640, 8069","1640, 7433, 7556, 8083, 9263","$0.00","The National Academies will plan and organize a cross-disciplinary public workshop to explore perspectives on the training that students from diverse fields need in order to extract value from Big Data. The workshop will identify key skills that are needed to prepare students for analyzing Big Data. It will consider the needs of industry, academia, and government and build on the experience gained from emerging courses and curricula for this topic. Invited participants to the workshop will include experts in statistics, machine learning, databases, large-scale computing systems, streaming computing, user domains, health and biological informatics, and industry.<br/><br/>Many traditional methods for data analysis do not work, or do not work well, with the massive amounts of data emerging in numerous endeavors. In order to reliably extract insight from Big Data, students need to learn new skills, and many of those skills cut across multiple disciplines and, thus, are not necessarily accessible through standard courses and curricula. During the academic year 2012-2013, a number of courses were introduced at various universities to impart the needed understanding. The planned workshop will enable educators to share insights gained from these courses and make adjustments. A summary report of the workshop will make these insights available more broadly."
"1313415","NSF CDSE:   Enabling Precise Constraints on Dark Energy","AST","EXTRAGALACTIC ASTRON & COSMOLO, SPECIAL PROGRAMS IN ASTRONOMY, OFFICE OF MULTIDISCIPLINARY AC","09/01/2013","03/08/2016","Robert Brunner","IL","University of Illinois at Urbana-Champaign","Continuing grant","Richard E. Barvainis","02/28/2018","$667,173.00","","bigdog@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","MPS","1217, 1219, 1253","1206, 1515, 7433, 8084, 9251","$0.00","This goal of this proposal is to develop advanced techniques to calculate accurate and computationally efficient photometric redshift distance estimators, and to mitigate photometric uncertainties and systematics on clustering measures for galaxies found in large numbers in current and upcoming surveys.  The computer codes will incorporate machine learning techniques with a combination of supervised and unsupervised learning algorithms. The methods developed will be applicable to the massive surveys to be undertaken by the future Large Synoptic Survey Telescope, which will, among other things, place precise constraints on Dark Energy.  <br/><br/>Broader impacts of the work include training of a postdoc, a graduate student, and undergraduate students, and development of a course in Data Science to be available on line.   All computational tools developed will be made available to the general astronomical community."
"1306697","NSF Postdoctoral Fellowship in Biology FY 2013","DBI","Inters Biol and Math and Phys","07/01/2013","05/14/2013","Carl Boettiger","CA","Boettiger Carl","Fellowship","Michael Vanni","06/30/2015","$138,000.00","","","","Davis","CA","956162954","","BIO","8054","7137, 7174","$0.00","Postdoctoral Research Fellowship in Biology: Management for an uncertain world: robust decision theory in face of regime shifts<br/><br/>This research will explore methods for more robust management of ecosystems when the underlying dynamics are uncertain. This work seeks to bridge mathematical and biological methods that have hitherto been developed largely in isolation, such as optimal control (from decision-theoretic work) and early warning signals (from resilience work) as well as machine learning approaches (from statistics and computer science). To anchor this work in the biology of a real world problem, examples and applications will come from the ecosystem dynamics and economic concerns of marine fisheries. The three main objectives of this project are: (1) Develop an approach to integrate early warning signals into a decision-theory framework (2) explore how nonparametric Bayesian inference can account for structural uncertainty in ecological dynamics, (3) explore how active learning approaches in each of these areas could be developed to design adaptive management policies that actively decrease uncertainty.<br/><br/>Effective decision-making in face of deep uncertainty is a fundamental challenge across many disciplines. Methods that can improve the ability to select robust actions under such circumstances may have far reaching value. Such real-world applications will only emerge as they are percolated through and developed by a larger community of researchers and managers. With this in mind, this project also seeks to promote a more immediate impact to the scientific community, the science education community, and the public at large through the use and development of an online open laboratory notebook. The online notebook will chronicle the daily advances in this research while also exploring mechanisms and technology that can advance scientific infrastructure, collaboration and dissemination of results. See http://carlboettiger.info/lab-notebook."
"1405259","NRI-Small: Improved safety and reliability of robotic systems by faults/anomalies detection from uninterpreted signals of computation graphs","IIS","NRI-National Robotics Initiati","10/01/2013","09/09/2017","Andrea Censi","MA","Massachusetts Institute of Technology","Standard Grant","Ralph Wachter","06/30/2017","$669,683.00","","censi@MIT.EDU","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","8013","7923, 8086","$0.00","One of the main challenges to designing robots that can operate around humans is to create systems that can guarantee safety and effectiveness, while being robust to the nuisances of unstructured environments, from hardware faults to software issues, erroneous calibration, and less predictable anomalies, such as tampering and sabotage. However, the fact that the streams of observations and commands possess coherence properties suggests that many of these disturbances could be detected and automatically mitigated with general methods that imply very low design efforts.  Currently, robotic systems are developed as a set of components realizing a directed ""computation graph"". This project focuses on theoretical methods, applicable designs, and reference implementation of a faults/anomalies detection mechanism for low-level  robotic sensorimotor signals. The system, without any prior information about the robot configuration, should learn a model of the robot and the environment by passive observations of the signals exposed in the computation graph, and, based on this model, instantiate faults/anomalies detection components in an augmented computation graph.<br/><br/>The project engages undergraduate and graduate students in advanced robotics design and development.  It is expected the research results will have a significant impact on future robotic systems and machine learning."
"1314406","Workshop on Numerical Linear Algebra and Optimization","DMS","COMPUTATIONAL MATHEMATICS","07/01/2013","05/13/2013","Ioana Dumitriu","WA","University of Washington","Standard Grant","Junping Wang","06/30/2014","$23,900.00","","dumitriu@u.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","MPS","1271","7556, 9263","$0.00","This grant provides travel support for US scientists to a three-day ""Workshop on Numerical Linear Algebra and Optimization"", to be held August 8-10, 2013 at the University of British Columbia, Vancouver, Canada. The workshop has the following three objectives: (1) to bring together the community of linear algebra and optimization in order to foster interaction and explore the rich area at the intersection of these two research fields; (2) to seek application of these problems in fields like machine learning and control; and (3) to promote and discuss the issue of actively using large-scale numerical linear algebra tools in scientific and industrial optimization. One of the main areas the workshop will focus on is eigenvalue optimization, which has received a lot of attention recently and which is studied by the two communities from different perspectives.<br/><br/>Optimization is a large field of research, with applications ranging from economics and engineering to national security, energy, and climate understanding. Examples of problems that can be solved with the help of optimization include maximizing profit or minimizing waste when manufacturing a variety of goods under raw material constraints; computer-based learning to classify objects (e.g., distinguishing tanks from trees); designing stronger buildings subject to volume and environmental constraints; and making planes safer by avoiding certain undesirable vibration frequencies. Solutions to such optimization problems need to make use of linear algebra quantities (like eigenvalues), and extensive calculations of such quantities must be done with care and in the most efficient way. This is a central task in numerical linear algebra, the tools of which have been adopted and adapted by various optimization communities to suit their needs. There is a very rich area of research at the interface between these two fields of optimization and numerical linear algebra, and the organizers of this workshop aim to see both communities work together toward addressing the important problems that arise in industry."
"1345006","EAGER: Sub-second human-robot synchronization","IIS","HCC-Human-Centered Computing","07/01/2013","08/23/2013","Gil Weinberg","GA","Georgia Tech Research Corporation","Standard Grant","Ephraim Glinert","06/30/2015","$155,973.00","","gil.weinberg@coa.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7367","7367, 7916","$0.00","The Principal Investigator (PI) will explore and test a number of hardware platforms and software algorithms whose goal is to facilitate sub-second human-robot synchronization.  To this end the PI will utilize the medium of music, one of the most time-demanding media where accuracy in milliseconds is critical because asynchronous operations of more than 10 ms are noticeable by listeners.  Specifically, the PI will develop up to three different kinds of robotic devices intended to allow a drummer, whose arm was recently amputated from the elbow down, to play and synchronize between his organic functioning arm and the newly developed robotic devices.  In addition, he will develop and investigate the efficiency of novel anticipation algorithms designed to foresee human actions before they take place and trigger robotic actions with low-latency and in a timely manner.  This research will advance our understanding in a variety of areas, including the biomechanics of limb operations, machine-learning techniques for the anticipation and prediction of human gestures, and highly accurate myoelectric robotic devices.<br/><br/>Broader Impacts:  This project will ultimately benefit a large population of amputees whose quality of life could improve through the use of low-latency robotic limbs with sub-second synchronization.  Facilitating such accurate sub-second human-robot synchronization could also improve efficiency and flow in other human-robot interaction scenarios where humans and robots must collaborate to achieve time-demanding common goals.  The novel solenoid-based robotic device(s) created in this research should also benefit musicians in general (that is to say, who are not disabled), who will be able to explore novel drumming techniques and create novel musical results."
"1255018","CAREER: Robust Bipedal Locomotion in Real-World Environments","CMMI","CONTROL SYSTEMS","01/01/2013","12/18/2012","Katie Byl","CA","University of California-Santa Barbara","Standard Grant","Irina Dolinskaya","12/31/2017","$400,000.00","","katiebyl@ece.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","ENG","1632","030E, 031E, 034E, 1045, 9102","$0.00","The objective of this Faculty Early Career Development (CAREER) Program grant is to develop tools for analyzing and optimizing quasi-periodic biped gaits for high-dimensional models of both humans and humanoid devices.  From watching Olympic athletes to taking a hike along a dirt path, our experiences demonstrate that biped locomotion provides a highly agile and robust means of mobility.  However, today's humanoid robots are far less capable than their fictional, Hollywood counterparts.  The same inverted-pendulum dynamics that make upright walking highly maneuverable under desired control inputs also make it highly susceptible to destabilization.  Unlike classic inverted pendulum problems (for example, a rocket propelled by thrusters at its base stabilized during continuous flight), the process of walking is complicated by the discontinuities of impulsive footsteps that vary in both width and height, resulting in a 'quasi-periodic' gait.  In particular, work focuses on the challenges of walking on terrain that is not flat and includes intermittent obstacles.  The approach exploits the observation that step-to-step 'snapshots' of the position and velocity states of the joints tend to lie on two-dimensional manifolds within a much higher dimensional state space for such systems.  This dimensionality reduction enables the use of machine learning techniques for control policy evaluation and improvement to quantifiably estimate fall rates, energy use, and speed for a given combination of biped walker and stochastic terrain.<br/><br/>If successful, this work will provide important tools to analyze and/or optimize a variety of real-world systems.  From evaluation of the risk of falling for a stroke survivor who walks with an impaired gait, to the design of smart lower-limb prostheses for injured veterans, the modeling approach developed will provide a means of quantifying reliability for systems with such high dimensionality and complexity that traditional guarantees of stability cannot be made.  Additionally, robotics research provides a natural gateway for both K-12 and university STEM education.  This project includes several outreach elements to encourage this interest, from hosting on-campus field trips for local FIRST Robotics participants and predominantly Hispanic elementary school students to sponsorship of a new Robotics Club at UC Santa Barbara."
"1319302","CSR:  Small:  Realism in Activity Recognition for Long Term Sensor Network Deployments","CNS","CSR-Computer Systems Research","09/01/2013","08/21/2013","John Stankovic","VA","University of Virginia Main Campus","Standard Grant","Marilyn McClure","08/31/2017","$423,263.00","","jas9f@virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","7354","7923","$0.00","Research in wireless sensor networks has been very successful in creating academic testbeds and short term real deployments for many application areas such as home health care, saving energy in buildings, infrastructure monitoring, agriculture, and various environmental science applications. However, significant new problems arise for long term deployments in uncontrolled environments. It is also important to note that most of these applications perform activity recognition. Yet, these activity recognition solutions are not always robust enough for long term deployments. Consequently, the goal of this work is to develop robust and reliable activity recognition for in-home deployments that address the realism of long term deployments. To accomplish this goal requires new research results in: obtaining labeled ground truth for training activity recognition systems, recognizing overlapping activities, detection of activities that occur across rooms of a home, handling missing sensor events and sensor failures, addressing the issues of multiple person homes and visitors, and handling the evolution of human behaviors. These solutions must be combined in a holistic manner. In addition, the utility of activity recognition often depends on recognizing anomalies from typical human behaviors. Anomaly detection can also suffer from the realisms of long term deployments and, therefore, is also addressed. The basic research approach includes employing data mining, machine learning, and other techniques in robust ways that account for realisms in long term deployments. Demonstration of the utility of the solutions spans from lab experiments to realistic long term deployments for 9 months or longer.<br/><br/>The broad significance of this work occurs because developing robust activity recognition schemes for wireless sensor networks that operate for long time periods implies improvement in applications such as home health care and saving energy in homes and buildings. Home health care can save lives, provide improved life style and greater independence for the elderly and chronically ill, lower medical costs, and via longitudinal studies, increase understanding of the causes of diseases. Energy is a scarce resource and improved activity recognition can be used to perform control actions that save energy. This energy savings can save money and lower the impact of global warming.<br/>"
"1313551","Supporting US-Based Students to Attend the 2013 IEEE International Conference on Data Mining (ICDM 2013)","IIS","Information Technology Researc, Info Integration & Informatics","08/01/2013","05/03/2013","Diane Cook","WA","Washington State University","Standard Grant","Sylvia Spengler","07/31/2014","$26,001.00","Bhavani Thuraisingham","cook@eecs.wsu.edu","280 Lighty","PULLMAN","WA","991641060","5093359661","CSE","1640, 7364","1640, 7364, 7556","$0.00","This award provides travel support for U.S. based graduate student participants to attend the 2013 International Conference on Data Mining (ICDM 2013), which will be held in Dallas, Texas, from December 9-12, 2013. ICDM has established itself as the world's premier research conference in data mining. The total number of ICDM participants in the past has been in excess of 500, with a majority of the participants coming from the U.S., followed by Europe and Asia. The award provides travel scholarships to 16 U.S. based graduate student participants to attend the ICDM conference and participate in the Doctoral Forum.<br/><br/>ICDM  provides an international forum for presentation of original research results, as well as exchange and dissemination of innovative and practical development experiences. The conference covers all aspects of data mining, including algorithms, software and systems, and applications, as well as related areas such as data management, machine learning and bioinformatics. The conference proceedings are published by IEEE. The conference seeks to continuously advance the state-of-the-art in data mining. With the growth of the Web, the Internet, and data intensive technologies such as sensor networks and bioinformatics, data mining has become an extremely important area in information technology. Besides the technical program, the conference features workshops, tutorials, panels, data mining contest. Since 2007 the conference has included a data mining contest, and since 2011, it has also included a Ph.D. forum that provides valuable feedback on preliminary research methods and results.<br/><br/>A strong representation of U.S. researchers, especially graduate students, in ICDM 2013 is critical for maintaining U.S. competitiveness and for nurturing the next generation of young researchers in an increasingly important area. The award also helps broaden the participation of women and members of underrepresented groups in data mining research."
"1252819","EAGER: Identifying Graphic Design Principles for Reports and Presentations","IIS","GRAPHICS & VISUALIZATION","01/01/2013","08/27/2012","Maneesh Agrawala","CA","University of California-Berkeley","Standard Grant","Maria Zemankova","12/31/2013","$100,000.00","","maneesh@cs.stanford.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","7453","7453, 7916","$0.00","Reports and presentations are frequently used to share information. In business, science, education, law, and government, such documents are ubiquitous. When they are well-designed reports and presentations combine text with charts, tables and diagrams to visually highlight important findings. Good designers carefully choose colors, fonts, layout and composition to clarify and emphasize key ideas. Yet, creating an effective graphic design is challenging; most people who need to produce reports and presentations do not have the relevant training in art and design. Even with the proper training, creating effective designs can be extremely time-consuming. The result is that all too often reports and presentations are poorly designed. The goal of this exploratory research is to develop new tools for improving the graphic design of reports and presentations to highlight the desired information content. The initial work centers on the problem of identifying the principles of effective visual communication and graphic design for generating high-quality reports and diagrams. The challenge is to identify the most significant dimensions of visual design (e.g. spatial layout, fonts, colors, etc.) based on a large corpus of reports and presentations. A combination of crowdsourcing, image analysis and machine learning techniques will be used to determine the visual properties that characterize good graphic designs. <br/><br/>The expected result will lead to better understanding of how graphic design contributes to the clarity and usability of a report or presentation. This work will produce an explicit set of principles for creating effective graphic design and web-based materials for teaching these principles to improve design literacy in society at large. The project provides research experience to students and design principles will also be presented to high school and college students. The project Web site (http://vis.berkeley.edu/projects/graphicDesignPrinciples ) is used to disseminate results, including software; a corpus of reports and presentations from web-based sources; papers and technical reports; and educational materials."
"1302435","CIF: Medium: Collaborative Research: New Approaches to Robustness in High-Dimensions","CCF","Comm & Information Foundations, SIGNAL PROCESSING","07/01/2013","08/27/2015","Sujay Sanghavi","TX","University of Texas at Austin","Continuing Grant","Phillip Regalia","06/30/2019","$695,369.00","Constantine Caramanis","sanghavi@mail.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7797, 7936","7924, 7936","$0.00","Rapid development of large-scale data collection technology has<br/>ignited research into high-dimensional machine learning.  For<br/>instance, the problem of designing recommender systems, such as those<br/>used by Amazon, Netflix and other on-line companies, involves<br/>analyzing large matrices that describe users' behavior in past<br/>situations.  In sociology, researchers are interested in fitting<br/>networks to large-scale data sets, involving hundreds or thousands of<br/>individuals.  In medical imaging, the goal is to reconstruct<br/>complicated phenomena (e.g., brain images; videos of a beating heart)<br/>based on a minimal number of incomplete and possibly corrupted<br/>measurements.  Motivated by such applications, the goal of this<br/>research is to develop and analyze models and algorithms for<br/>extracting relevant structure from such high-dimensional data sets in<br/>a robust and scalable fashion.<br/><br/><br/>The research leverages tools from convex optimization, signal<br/>processing, and robust statistics.  It consists of three main thrusts:<br/>(1) Model restrictiveness: Successful methods for high-dimensional<br/>data exploit low-dimensional structure; however, many real-world<br/>problems fall outside the scope of existing models.  This proposal<br/>significantly extends the basic set-up by allowing for multiple<br/>structures, leading to computationally efficient algorithms while<br/>eliminating negative effects of model mismatch.  (2) Non-ideal data:<br/>Missing data are prevalent in real-world problems, and can cause major<br/>breakdowns in standard algorithms for high-dimensional data. The<br/>second thrust devises relaxations and greedy approaches for these<br/>non-convex problems.  (3) Arbitrary Outliers: Gross errors can arise<br/>for various reasons, including fault-prone sensors and manipulative<br/>agents.  The third thrust proposes efficient and randomized algorithms<br/>to address arbitrary outliers."
"1356918","AF:III:Small:Collaborative Research: New Frontiers in Join Algorithms: Optimality, Noise, and Richer Languages","CCF","ALGORITHMS","09/01/2013","09/10/2013","Christopher Re","CA","Stanford University","Standard Grant","Rahul Shah","08/31/2016","$173,898.00","","chrismre@cs.stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","7926","7923, 7926","$0.00","The relational join is central to relational database processing, which is the dominant way data is processed today. The join also models problems in biological and social networks, coding theory, compressed sensing, machine learning, and constraint satisfaction. Recently,the investigators described the first ever worst-case optimal algorithm (the NPRR algorithm) for join queries.These new results open a line of new tools to attack a diverse set of fundamental problems related to the join. This project aims to further exploit the new algorithmic techniques developed for NPRR to address the following three classes of problems:<br/><br/>(1) Optimal Join algorithms. Developing algorithms that are instance optimal when the data are stored in either traditional database indexes or new indexing structures is a goal of this project. (2) Coping with and Leveraging Noise. This project will extend the latest work to handle and leverage both worst-case and statistical noise models, bridging to coding theory and compressed sensing.(3) Expressive Query Languages. The project will explore a series of extensions to join queries that will pave the way to overcome challenges in motif finding, search, databases with functional dependencies, and more powerful classes of queries and join operations.<br/><br/>If successful, the results of this grant will apply to a variety of pattern extraction problems in modern massive, dynamic, and noisy data sets, which have a wide range of applications in complex network analysis, coding theory, and compressive sensing."
"1255965","CSR: EAGER: Design and Implementation of a Fine-Grained Appliance Energy Profiling System for Green Building","CNS","Special Projects - CNS","01/01/2013","09/17/2012","Nirmalya Roy","WA","Washington State University","Standard Grant","Krishna Kant","08/31/2013","$265,292.00","Behrooz Shirazi","nroy@umbc.edu","280 Lighty","PULLMAN","WA","991641060","5093359661","CSE","1714","7916","$0.00","Green building applications need efficient and fine-grained determination of power consumption pattern of a wide variety of consumer-grade appliances through non-intrusive load monitoring (NILM) techniques for an effective adaptation and percolation of demand response model down to the consumer level appliances. A key inhibitor to the widespread adoption of such demand response policy at the consumer grade appliances for intelligent building energy management, is the inability of smart plug to efficiently determine, control or infer the power consumption pattern of multiple devices in tandem.  In practice, deploying smart plug based NILM and acquiring the low-level power measures of a large number of devices is often difficult or impossible due to the deployment complexity and varying characteristics of devices and thus must instead be employed at the circuit-level and inferred through the incorporation of novel usage-based measurement and probabilistic level-based disaggregation algorithm. But the challenges in deploying non-intrusive load monitoring algorithm involve disaggregating individual device?s consumption from the aggregate power measurement, as well as modeling and incorporating the usage based prediction. Thus in this project we will focus on advanced machine learning and data analytics algorithms that capture the measurement based approach and circuit level NILM with the autonomous profiling and prediction logic to enable the deployment of flexible and fungible smart plug and the evolvability of future DR model in green building applications."
"1319460","AF: Small: New Perspectives on Special Methods for Graph Algorithms","CCF","ALGORITHMS","09/01/2013","06/20/2013","Lorenzo Orecchia","MA","Massachusetts Institute of Technology","Standard Grant","Tracy Kimbrel","07/31/2015","$177,392.00","","orecchia@uchicago.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7926","7923, 7926","$0.00","Classical algorithms for many important graph primitives were designed at a time when the conventional notion of efficiency was polynomial running time.  However, many of today's applications involve graphs consisting of millions, or even billions, of nodes. On these massive inputs, practical algorithms must run in time that is as close to linear as possible in the size of the input.<br/><br/>The spectral approach to designing graph algorithms views the instance graph as a matrix and makes use of the algebraic properties of the corresponding linear operator. Recently, research in this area has led to the design of faster spectral algorithms for many essential graph problems, such as electrical flow, maximum flow and graph partitioning in undirected graphs.  The goal of this project is to develop a novel algorithmic approach by combining spectral methods and the idea of regularization from optimization.  Regularization is a mechanism for modifying a given optimization problem to make it more amenable to known algorithms without changing its salient characteristics. Surprisingly, many of the recent breakthroughs in the design of fast spectral algorithms can be viewed as applying different types of regularization. This research aims to exploit this interpretation to design algorithms that are faster, simpler to analyze and easier to implement. Another aim of this research is to integrate different perspectives on regularization from Machine Learning, Statistics and Convex Optimization, to create new bridges between these fields and the design of algorithms.<br/><br/>Due to the practical importance of the graph problems under consideration, the work will also focus on empirically evaluating the algorithms designed.  These evaluations will be disseminated to the relevant audiences to maximize the impact of the award. Moreover, because this project aims to develop new fundamental techniques in the design of algorithms, a particular effort will be devoted to incorporating material from this research into the PI's teaching activity and to preparing educational material for the public."
"1331176","BSF:2012304:Methods for Preprocessing Population Sequence Data","CCF","SPECIAL PROJECTS - CCF","09/01/2013","09/11/2013","Eleazar Eskin","CA","University of California-Los Angeles","Standard Grant","Tracy Kimbrel","08/31/2018","$40,000.00","","eeskin@cs.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","2878","2878, 7796","$0.00","This project is funded as part of the United States-Israel Collaboration in Computer Science (USICCS) program.  Through this program, NSF and the United States - Israel Binational Science Foundation (BSF) jointly support collaborations among US-based researchers and Israel-based researchers.<br/><br/>In recent years, many genetic studies have been performed, revealing many new associations between human genetic variation and complex diseases. These studies, referred to as genome-wide association studies, are limited to common genetic variants because the technology which collected the genetic variation was limited to only collecting common variants.  There is evidence suggesting that rare variants have an important role in disease architectures.  Recently, sequencing technologies have been introduced which are capable of collecting both genetic common and rare genetic variation. Sequencing technologies generate enormous amounts of data, raising new computational challenges.  In this project, the PIs will develop methods for addressing these computational challenges including the design of efficient algorithms and the modeling of the sequencing process.  In addition, the researchers will develop methods for incorporating rare variants into the analysis of genetic studies. The immediate broader impact of our project is the availability of these tools for general use by geneticists, leading to an improved understanding of the disease genetics. Particularly, the PIs will apply their methods to studies of non-Hodgkin's lymphoma, bipolar, dyslipidemia, neurodegenerative dementia, and Tourette syndrome, which will result in a direct impact on our understanding of these particular conditions.<br/><br/>Current computational methods for the analysis of sequencing data exist, however they are limited to the analysis of a single sample. In this project the PIs will design efficient computational methods for the analysis of sequence data across a population. For population samples, the tremendous size of the data requires the design of highly efficient algorithms in terms of memory and runtime.  Specifically, the PIs propose to design algorithms for the compression of sequencing data, for the search of regions identical by descent across multiple samples, and for high-resolution haplotype inference from sequence data. The PIs will explicitly model rare variants and the sequencing process, and use machine learning techniques and convex optimization to estimate the model parameters efficiently. These methods will allow for a fine-scale analysis of population data, resulting in improved understanding of complex diseases and human history.  The collaborative nature of the project will expose the students involved in the project to the medical and genetics worlds, both in Israel and in the US, and it will improve their abilities to design and implement solutions to complex algorithmic problems.  The methods developed in this project will be part of the teaching material of courses in UCLA and Tel-Aviv, and these materials will be made publicly available."
"1331813","Collaborative Research: CyberSEES: Type 2: A New Framework for Crowd-Sourced Green Infrastructure Design","CCF","INFORMATION TECHNOLOGY RESEARC, CyberSEES","10/01/2013","08/10/2016","Lawrence Band","NC","University of North Carolina at Chapel Hill","Standard Grant","richard brown","09/30/2017","$572,182.00","Jack Snoeyink, Mary Whitton","lband@virginia.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","1640, 8211","8211","$0.00","This project develops a novel computational Green Infrastructure (GI) design framework that integrates interactive, neighborhood-scale, collaborative design by multiple stakeholders (""crowd-sourced"" design) with multi-scale models of ecosystem and human impacts. The following research questions are being addressed: (1) How well does coupling of site-scale ecohydrology with catchment-scale hydraulic routing improve predictions? (2) How well can stakeholder preferences be predicted using design image feature extraction and machine learning? (3) What interactive optimization and visualization techniques lead to the most rapid and complete consensus among diverse stakeholders? (4) Do stakeholders using interactive cyberinfrastructure tools consider more options and explore more of the GI design space?<br/><br/>A ""crowd-sourced"" design framework is developed to enable stakeholders to interactively create and evaluate potential GI designs that reflect consideration of the full breadth of social, economic, and environmental criteria. The research questions are evaluated in diverse neighborhoods within three urban catchments in the Baltimore Ecosystem Study, working closely with environmental non-governmental organizations to ensure that the results will provide significant benefits to community stakeholders. The models developed in this project are the first to integrate criteria for human and ecosystem wellbeing with site- and watershed-scale hydrologic processes, a key advance for improving understanding and implementation of GI design. Map and image visualization identifies which visualization approaches best support improving stakeholder engagement for achieving consensus using interactive collaborative design. This makes technical advances in interactive optimization and model parameterization accessible to the broad range of stakeholders, from regulators and planners to contractors and homeowners."
"1262547","Collaborative Research: ABI Innovation: Breaking through the taxonomic barrier of the fossil pollen record using bioimage informatics","DBI","ADVANCES IN BIO INFORMATICS","08/01/2013","07/01/2015","Charless Fowlkes","CA","University of California-Irvine","Continuing grant","Peter McCartney","07/31/2017","$254,176.00","","fowlkes@ics.uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","BIO","1165","","$0.00","The practice of identifying pollen has a large number of scientific applications and is used in fields as diverse as archaeology, biostratigraphy (the dating of rocks), and forensic science. Pollen and spores play a particularly important role in paleontology, because they form the most abundant and extensive record of plant diversity, dating back hundreds of millions of years. However, the most critical hypotheses in plant ecology and evolution (e.g. the assembly of plant communities, speciation and extinction) cannot be fully tested with pollen data due to the extreme difficulty of recognizing species from pollen and spore material. This project develops new methods to probe the shape and fine structural and textural properties of the grains using high-throughput, super-resolution structured illumination microscopy and automated image analysis in order to transform species identification from a subjective, by-eye procedure to a quantitative, computational practice. Since it is not known a priori which morphological features are phylogenetically meaningful, new machine learning techniques are being developed to model pollen images at multiple scales, identify aspects of shape and texture that are statistically informative, and infer their relation to the underlying phylogenetic structure. <br/><br/>The project has the ambitious long-term goal of creating a high-throughput system for analyzing pollen data that incorporates meaningful characterizations of pollen and spore morphology, provides testable hypotheses of biological affinity, and is open and available to the entire scientific community. This will allow researchers to break through the current taxonomic limitations of pollen identification and fundamentally change current practices in the discipline on many levels, from the basic task of identification and counting to the interpretation and use of these data in global climate-vegetation models. The project brings together a diverse, interdisciplinary team including international collaborators at the Smithsonian Tropical Research Institute in Panama and will train graduate and undergraduate students from multiple scientific disciplines and backgrounds in an emerging area of interdisciplinary research. A public outreach component is in development that will include a virtual microscopy web site using images generated by this research to introduce non-experts to the beauty, complexity, and relevance of pollen morphology. Additional information about this project can be found at: http://www.life.illinois.edu/punyasena"
"1318205","AF:III:Small:Collaborative Research: New Frontiers in Join Algorithms: Optimality, Noise, and Richer Languages","CCF","ALGORITHMS","09/01/2013","08/07/2013","Christopher Re","WI","University of Wisconsin-Madison","Standard Grant","Balasubramanian Kalyanasundaram","10/31/2013","$173,898.00","","chrismre@cs.stanford.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","7926","7923, 7926","$0.00","The relational join is central to relational database processing, which is the dominant way data is processed today. The join also models problems in biological and social networks, coding theory, compressed sensing, machine learning, and constraint satisfaction. Recently,the investigators described the first ever worst-case optimal algorithm (the NPRR algorithm) for join queries.These new results open a line of new tools to attack a diverse set of fundamental problems related to the join. This project aims to further exploit the new algorithmic techniques developed for NPRR to address the following three classes of problems:<br/><br/>(1) Optimal Join algorithms. Developing algorithms that are instance optimal when the data are stored in either traditional database indexes or new indexing structures is a goal of this project. (2) Coping with and Leveraging Noise. This project will extend the latest work to handle and leverage both worst-case and statistical noise models, bridging to coding theory and compressed sensing.(3) Expressive Query Languages. The project will explore a series of extensions to join queries that will pave the way to overcome challenges in motif finding, search, databases with functional dependencies, and more powerful classes of queries and join operations.<br/><br/>If successful, the results of this grant will apply to a variety of pattern extraction problems in modern massive, dynamic, and noisy data sets, which have a wide range of applications in complex network analysis, coding theory, and compressive sensing."
"1262134","Collaborative Research: ABI Innovation: Genome-Wide Inference of mRNA Isoforms and Abundance Estimation from Biased RNA-Seq Reads","DBI","ADVANCES IN BIO INFORMATICS","09/01/2013","08/31/2013","Xinshu Grace Xiao","CA","University of California-Los Angeles","Standard Grant","Peter H. McCartney","08/31/2017","$200,158.00","","gxxiao@ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","BIO","1165","","$0.00","The University of California, Riverside and University of California, Los Angeles are awarded collaborative grants to identify mRNA isoforms on a genome-wide basis. Due to alternative splicing events in eukaryotic cells, the identification of mRNA isoforms (or transcripts) is a difficult problem in molecular biology. Traditional experimental methods for this purpose are time-consuming and cost  ineffective. The emerging RNA-Seq technology provides a possible effective way to address this problem. This project aims to develop efficient and accurate methods for inferring isoforms and estimating their abundance levels from RNA-Seq data where the reads may be sampled non-uniformly due to the existence of various biases including positional, sequencing and mappability biases. In particular, a novel statistical framework based on quasi-multinomial distributions will be introduced and a companion expectation-maximization (EM) algorithm developed for estimating isoform abundance levels that can handle all above biases in RNA-Seq data. The algorithms will be implemented efficiently in C++, tested extensively on both simulated and real RNA-Seq data in human, mouse and drosophila, and made available to the public for free. The performance of the algorithms will be evaluated extensively using both simulated and real RNA-Seq data. In the latter case, perturbations to some important splicing factors will be introduced into selected cell lines to induce widespread alteration of splicing events. RNA-Seq data of these cells, combined with quantitative RT-PCR validation, will provide an enriched dataset to assess the performance of the algorithms in  predicting both isoform abundance and relative variation. In addition, the  validation results may provide insight on the regulatory functions of the splicing  factors and serve as a testbed for further improvement of the algorithms.<br/><br/>The broader impact of this project is twofold. First, RNA-Seq data analysis is a timely topic in bioinformatics due to the recent rapid advance in next generation sequencing (NGS) technologies and its potential impact in life sciences and medicine. Despite the success of many RNA-Seq applications, several challenges remain in the analysis of RNA-Seq data, one of which comes from the understanding and handling of biases in RNA-Seq reads. The approaches proposed in this project for treating RNA-Seq biases combine unique techniques from statistics, machine learning and combinatorial algorithms. Moreover, the experimental validation results may shed light on the regulatory functions of some important splicing factors. Second, the project will provide an excellent opportunity for the training of two computer science PhD students, a postdoc and two biology undergraduate students in the interdisciplinary field of computational biology and bioinformatics. Since many of the involved students are female, the research will also help improve the representation of women in science and engineering."
"1309623","Exploring Power Network Attributes for Information Forensics","ECCS","CCSS-Comms Circuits & Sens Sys, Secure &Trustworthy Cyberspace","09/01/2013","08/12/2013","Min Wu","MD","University of Maryland College Park","Standard Grant","akbar sayeed","08/31/2017","$360,000.00","","minwu@eng.umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","ENG","7564, 8060","154E, 7434, 9102","$0.00","Objective:                   <br/>The objective of this project is to investigate the scientific and technological foundations on time, location and integrity of sensor recordings, by exploiting novel intrinsic fingerprints in the environment.  An example is the small random-like fluctuations of the electricity frequency known as the Electric Network Frequency (ENF). These environmental fingerprints reflect the attributes and conditions of the power grid and become naturally ""embedded"" into video, audio or other types of sensor signals at the time of recording. They carry time and location information and may facilitate integrity verification of the primary sensing data. Answering questions about the time, location, and integrity of sensor recordings find important applications in crime solving, counter-terrorism, journalism, infrastructure monitoring, smart grid management, and other commercial operations.<br/><br/>Intellectual merit:       <br/>The intellectual merit lies on exploring the nearly unchartered research area of forensic evidences in the environment, by drawing synergy across multiple disciplines including signal processing, machine learning, sensor circuits and systems, power engineering, and information forensics/security. The research exploration and technology development of this project are transformative and interdisciplinary in nature.<br/> <br/>Broader impacts:        <br/>The broader impacts are two folds:  through engaging students, especially female and under-represented ones, in exploratory research, the project's seamless integration of research with education and outreach will prepare the future generation of engineering workforce with interdisciplinary strength; through new technologies and tools developed to advance information forensics, the project has a high potential to benefit a number of applications of global and societal importance."
"1252795","CAREER: Deformations in statistics, cosmology and image analysis","DMS","STATISTICS, Division Co-Funding: CAREER","07/01/2013","05/19/2020","Ethan Anderes","CA","University of California-Davis","Continuing Grant","Gabor Szekely","06/30/2021","$400,004.00","","anderes@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","MPS","1269, 8048","1045","$0.00","Smooth invertible transformations, or deformations, are fast becoming important tools in modern data analysis. The nonlinear nature of deformations makes these objects extremely powerful while at the same time making them challenging to estimate and theoretically explore. This proposal is dedicated to the development and theoretical understanding of deformations applied to three specific areas of research: statistics, cosmology and image analysis. The theoretical properties of estimated deformations for generating nonparametric and semiparametric statistical estimates are analyzed through a surprising connection with Stein's method. In addition, the investigator focuses on recent results found in the theory of optimal transport, which has the potential to provide a rigorous theoretical foundation for deformable templates. The computational aspects of estimated deformations will utilize a new Euler-Lagrange characterization of a penalized maximum likelihood estimate, which can significantly relieve the typical computational burden associated with estimation. One consequence will be to make these methods available for widespread use by statistical practitioners in a broad range of problems: nonparametric and semiparametric density estimation, estimating gravitational lensing in cosmology and posterior sampling techniques, to name a few. Another intellectual merit of this proposal are the scientific ramifications of two new proposed deformation estimates of weak lensing of the cosmic microwave background (CMB): a wavelet/Slepian quadratic estimator and a new Bayesian lensing estimator. Gravitational lensing studies have become one of the most successful tools for probing the nature of dark matter. The precise estimation of lensing is important for a number of reasons including, but not limited to, understanding cosmic structure, constraining cosmological parameters and detecting gravity waves. The investigator proposes to uses wavelets and Slepian multi-tapers to adapt the quadratic estimate to local foreground contaminants and sky cuts, which are ubiquitous features in most modern cosmological surveys. The investigator proposes a new Bayesian estimator which has the potential to dramatically change the way gravitational lensing studies are done and how they are integrated within other astronomical surveys.<br/><br/>Smooth invertible transformations, or deformations, are fast becoming important tools in modern data analysis.  They have been used with spectacular success in the field of computational anatomy where time varying vector field flows which generate deformations are used to statistically analyze medical fMRI images and quantify abnormal morphological structure. In cosmology, deformations are used to model gravitational distortions of the cosmic microwave background from dark matter density fluctuations, and have resulted in a deeper understanding of cosmic structure. Even though these important tools are becoming integrated in modern scientific methods, the statistical properties of estimated deformations have been largely unexplored. This proposal is dedicated to the development and theoretical understanding of deformations applied to three specific areas of research: statistics, cosmology and image analysis. The tools resulting from this project will be useful, not only in statistics, but also in other branches of science and technology ranging from genetics to machine learning. In the field of physics, for example, the potential scientific progress resulting from gravitational lensing estimation could a have broad impact on scientific understanding and the future of scientific research. Moreover, it is becoming increasingly important to train graduate and undergraduate students with the tools necessary to successfully navigate interdisciplinary work, and who are prepared for independent research. The interdisciplinary nature of the proposal will foster a culture of collaboration that will reach the fundamentals of statistical education and will deepen ties with statistics and other physical sciences. In addition, through the integration of research and education, the proposal will teach both graduate and undergraduate students research skills. The result will be two fold. First, it will train graduate students to become creative independent researchers who can contribute within an academic environment. Second, it will educate undergraduates to navigate a work environment which values creative independent investigation."
"1358182","Workshop on Control, Computing, and Signal Processing Challenges in Future Power Systems.  To be Held in Arlington,VA in November, 2013","ECCS","CCSS-Comms Circuits & Sens Sys, Comm & Information Foundations","11/15/2013","11/14/2013","Sairaj Dhople","MN","University of Minnesota-Twin Cities","Standard Grant","Zhi Tian","10/31/2014","$50,000.00","Steven Low, Georgios Giannakis","sdhople@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","ENG","7564, 7797","153E, 155E, 7935","$0.00","Transformative research activities in the last decade focused on sustainable, robust, and reliable power systems have presented several paradigms such as smart grids, self-healing grids, and distributed generation. This workshop seeks to address the Control, Computing, and Signal Processing Challenges in Future Power Systems within the context of developing the tools for a system-level understanding of the impact of these emerging paradigms.  It will promote cross fertilization of ideas with the aim of developing the underlying foundational science to realize next-generation electric generation, transmission, and distribution systems.<br/><br/>Intellectual Merit:<br/><br/>The workshop will bring together prominent researchers in relevant disciplines to address key questions relevant to future power systems, including: 1) What emerging control paradigms will improve management of renewables and charging of electric vehicles, with build-in resilience against failures and malicious attacks on critical infrastructure? 2) What is the role of optimization, big data analytics, and grid informatics in revitalizing power delivery subsystems and improving consumer satisfaction? 3) How can advances in statistical signal processing and machine learning be leveraged to improve situational awareness and system reliability? 4) How can we educate next-generation engineers about system-level challenges in future power systems?<br/><br/>Broader Impacts:<br/><br/>This workshop will engage several engineering disciplines to map out the foundational science for research in future power systems. The workshop will also emphasize the related educational aspects, primarily by leveraging the long tradition of excellence in power and energy systems education that the University of Minnesota has established through the NSF-supported Consortium of Universities for Sustainable Power."
"1343760","EarthCube Building Blocks:  A Cognitive Computer Infrastructure for Geoscience","ICER","EarthCube","09/15/2013","09/18/2017","Christopher Re","WI","University of Wisconsin-Madison","Standard Grant","Eva Zanzerkia","08/31/2018","$1,497,798.00","Miron Livny, Shanan Peters, Christopher Re","chrismre@cs.stanford.edu","21 North Park Street","MADISON","WI","537151218","6082623822","GEO","8074","7433","$0.00","This is an era when access to information and data is often less of a problem than the ability<br/>to efficiently process and use it. In some cases, these problems are caused by massive, monolithic<br/>datasets that are difficult to store, transfer, and/or analyze. In other cases, the first-order<br/>problem is discovering and then aggregating relevant data that are widely disseminated in<br/>many different locations and formats, such as in the tables, text, and figures of published<br/>papers, government agency reports, spreadsheets, and websites. Geosciences currently lacks<br/>a cyberinfrastructure that can efficiently, cheaply, and with high precision and accuracy<br/>find, extract, and organize many different types of data that are critical to advancing science<br/>and leveraging current and past investments in data acquisition. Instead, there are dozens<br/>of isolated, sometimes redundant, geosciences data mining efforts that use humans as the primary<br/>mechanism for finding data and then keystroking them into structured databases. This mode<br/>of operation is not only costly and slow, but it is also an inefficient use of human resources<br/>and scientific expertise. This project develops a geoscience-oriented trained computing<br/>system that can serve as a cross-disciplinary tool for rapidly finding, extracting, and organizing<br/>geosciences data. Unlike traditional data processing systems, trained systems use statistical, or machine learning, techniques to provide rich answers to complex queries of data that are much less structured. <br/><br/>The longer-term vision is to establish an EarthCube trained computing system that can aid in finding, extracting, and aggregating data, as well as in processing, summarizing, and synthesizing them in a way that helps geoscientists to tackle new problems<br/>and better understand and model Earth systems. This project brings together a unique interdisciplinary team that is committed to building, testing, and operating an EarthCube Building Block that will bring the power of trained computing systems<br/>technologies to the broader geoscience community. Trained computing systems offer an entirely new breed of tools for data processing."
"1344587","SCH: INT: Novel Techniques for Patient-centric Disease Management using Automatically Inferred Behavioral Biomarkers and Sensor-Supported Contextual Self-Report","IIS","Smart and Connected Health","12/01/2013","04/27/2017","Jessica Ancker","NY","Cornell University","Standard Grant","Sylvia Spengler","11/30/2018","$1,976,976.00","Geraldine Gay, Tanzeem Choudhury, Daniel Stein, Jessica Ancker, Jessica Ancker","jsa7002@med.cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","8018","8018, 8062","$0.00","The vision of patient-centric, personalized, precision medicine and wellness will be fully realized only when an individual?s self-care and clinical decision making are informed by a rich, predictive model of that individual?s health status. The evolution and dissemination of mobile technology has created unprecedented opportunities for highly detailed and personalized data collection in a far more granular, unobtrusive, and even affordable way; these data include activity levels, location patterns, sleep, consumption, and communication and social interaction. However, turning this potential into practice requires that we develop the algorithms and methodologies to transform these raw data into actionable information. The research will develop novel and generalizable techniques to derive robust measures relevant to individual health and clinical decision making.  The team will develop and evaluate tools that convert raw human-activity data into clinically actionable behavioral biomarkers. This demands creative uses of the underlying technical capabilities (i.e., passive data capture, data analysis and machine learning, data visualization, user experience), as well as rigorous understanding of the underlying health condition and management (i.e. functional health measures, achievable and optimal health outcomes, patient challenges in adherence, risks and benefits associated with medication and other aspects of treatment, and clinical decision making). The approach has broad applicability across disease management (e.g., auto-immune, gastrointestinal, depression, cognitive decline, and neurologic disorders), but also calls for tailoring to specific conditions and individuals. Therefore, we will conduct this initial work in a specific context, that of chronic pain management for three prominent conditions: rheumatoid arthritis, osteoarthritis, and lower back pain. The behavioral biomarkers associated with our initial target domain, pain management, center around: (i) decline in activity levels; (ii) increase in stress; (iii) decrease in sleep quality; (iv) drop in function, e.g., reduction in travel distance or inability to go to work. The effectiveness of passive sensing capabilities of the mobile phone to track sleep, changes in activity level, stress, social isolation, geographic location and several other indicators that are likely antecedents or symptoms of pain interference has been demonstrated previously. <br/><br/>While behavioral biomarkers rely extensively on passively captured data streams (such as activity, location, communication, application usage and audio), there remain important cases in which self-report data is required to augment or clarify passively collected data. However, the standardized patient survey instruments that assess relevant symptoms and behavior are not suitable for use on a daily basis because of length, question design, or both. Further, traditional forms of self report are often intrusive, burdensome, and suffer high rates of attrition. A new approach, contextual recall, aims to mitigate the issues related to self-report through three key mechanisms: optimizing the delivery of prompts, providing the user with key contextual cues to improve recall, and employing visual input techniques as an alternative to long-form measures that do not scale well to frequent mobile self-reports. The approach to personalizing disease management is intentionally scalable in terms of affordability and accessibility. Passive data collection requires no user attention, and contextual recall is a form of self-report designed for busy individuals with a range of demands and constraints on their time, as well as potential literacy and numeracy constraints. The clinician-facing components of this approach are also designed to work in resource-constrained clinical settings where clinicians are under particular time pressure. The team will recruit patients and clinicians from typically underserved communities to engage in the participatory design process.  The overall contributions of this work will include development and evaluation of: (1) software techniques to combine and transform passively monitored and self-reported data streams into clinically meaningful, actionable, and personalized indicators, which we call behavioral biomarkers; (2) contextual recall that allows the collection of highly granular and contextually specific self-report data to enhance passively captured data with information from the patient perspective, while balancing the tension faced in balancing recall bias and usability; and (3) a methodology that systematizes the collaboration with clinical domain experts to develop and integrate behavioral biomarkers into clinical decision making for specific diseases. We will create and evaluate a modular and extensible suite of analytics and user interaction techniques designed to facilitate iterative implementation and evaluation. These modules will themselves be a contribution, but equally important will be the evaluation of the overall approach of behavioral biomarkers as a driver of precision medicine."
"1254192","CAREER: A Data-Driven Uncertainty-Guided Architecture for Energy Management in Sensor Systems","CNS","Networking Technology and Syst","02/01/2013","07/21/2015","Alberto Cerpa","CA","University of California - Merced","Continuing Grant","Thyagarajan Nandagopal","01/31/2018","$309,437.00","","acerpa@ucmerced.edu","5200 North Lake Road","Merced","CA","953435001","2092012039","CSE","7363","1045","$0.00","The dramatic growth of wireless sensor networks (WSNs) in the last few years has seen the emergence of a variety of applications and pilot deployments that span scientific, engineering, medical and other<br/>disciplines. This project addresses an important research problem in sensor networks: energy management. By using hierarchical WSNs with different resources at different proxy and sensor tiers, as well as novel machine learning methods, we plan to develop DURACEM, a data-driven uncertainty-guided architecture that addresses energy efficiency across many sensor network services in a unified manner. Our architecture is built around (a) prediction models of phenomena, energy usage, and communication and the uncertainty in these models, (b) a slew of services that can exploit the uncertainty measures to save energy and<br/>(c) an overall energy optimization framework that combines the different models and services at both the proxy and sensor tiers.<br/><br/>DURACEM has the potential for broad social impact. Our proposed research will provide fundamental services and tools to build next generation hierarchical sensor networks. DURACEM will provide a suite of efficient<br/>prediction models, as well as a variety of energy-optimized services built over these models. Our source code for our tools, libraries and prototypes will be made available to other researchers via an open-source software license. The project also focuses on building an inter-disciplinary teaching lab that can spur adoption of sensor networks by educating students across engineering and sciences. The project includes the development of a comprehensive curriculum that provides graduate and undergraduate students with core systems and analytical skills in sensor networks. The DURACEM project will enhance the research experiences of minority groups at University of California, Merced (UCM), which is one of only few research universities in the nation with a Hispanic student population greater than 40% and it is considered a Hispanic Serving Institution. The project includes an outreach undergraduate component specifically targeted to the minority<br/>students.<br/>"
"1253345","CAREER: The Algorithmic Foundations of Data Privacy","CNS","Algorithmic Foundations, COMPLEXITY & CRYPTOGRAPHY, Secure &Trustworthy Cyberspace","06/01/2013","05/31/2017","AARON ROTH","PA","University of Pennsylvania","Continuing Grant","Nina Amla","05/31/2020","$484,175.00","","aaroth@cis.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","CSE","7796, 7927, 8060","1045, 7434, 7927","$0.00","The past decade has seen a growing reliance on data driven technologies, including recommendation systems, targeted advertising, and search personalization. This growth in big data has made data privacy into a central concern. The central question raised is: how can we continue to extract useful information from large datasets, while provably protecting some measure of privacy for the individuals contained in these datasets?<br/><br/>This research centers around advancing the state of the art in privacy preserving data analysis. It specifically has several themes: (1) Exploiting structure in the private data being analyzed, as well as the classes of queries used in the analysis to give computationally efficient algorithms for private data analysis. (2) Deepening the connections between private data analysis and machine learning theory. (3) Relaxing the adversarial collusion model implicit in most work on the foundations of data privacy, and (4) applying the tools of differential privacy to usefully exploit and analyze noise in other algorithmic settings. To ensure the broad impact of this research, this project includes substantial outreach activities, including workshop organization, course development, and the development of a textbook and other educational materials."
"1343548","Neuro-Marker Discovery for Accurate Localization of the Sub-Thalamic Nucleus for Deep Brain Stimulation","CBET","Engineering of Biomed Systems","01/15/2013","06/10/2013","Nuri Ince","TX","University of Houston","Standard Grant","Carol Lucas","06/30/2015","$210,674.00","","nfince@central.uh.edu","4800 Calhoun Boulevard","Houston","TX","772042015","7137435773","ENG","5345","004E, 137E, 7237","$0.00","1067488<br/>Ince<br/><br/>Deep brain stimulation (DBS) has become the most common surgical treatment in the U.S. for patients with Parkinson?s disease (PD), and involves the stereotactic implantation of a DBS electrode within the subthalamic nucleus (STN) of the brain. The clinical efficacy of DBS depends critically on accurate localization of the STN. Currently, the initial surgical trajectory to STN is determined by preoperative stereotactic magnetic resonance imaging (MRI) of the brain. The ultimate location of the DBS electrode is then modified during surgery by data obtained from electrophysiological recordings--in the form of single-unit neuronal activity (SUA), derived from multiple microelectrode recording tracks.<br/>However, optimal placement of DBS electrodes within STN remains challenging due to current limitations of stereotactic imaging, poor isolation of SUA during microelectrode recordings, and anatomical differences in the location of deep brain structures between human subjects. Consequently, new techniques are required which can automatically localize or provide additional evidence to the neurosurgeon about STN localization. Microelectrodes can also be used to record local field potentials (LFPs) which, in contrast to SUA recordings, represent aggregate activity from populations of neurons surrounding the electrode tip. However, to be clinically useful, patterns in LFP data need to be translated into another modality so that they can be interpreted by the clinician.<br/>The intellectual merit of this project resides in testing the hypothesis that LFP data recorded intraoperatively can be used to identify STN location. Specifically, this project will record LFPs from both micro- and macro-electrodes at consecutive depths, as these electrodes are advanced to STN. Recorded neural data will be processed offline, using state-of-the-art signal processing and machine learning methods to identify novel neuro-markers which will be used for the  optimization of electrode placement in STN. <br/>This research project will identify and validate the use of specific LFP-derived data that correspond to optimal electrode positioning. The results of this interdisciplinary project will enable the development of a new technology for fusing microelectrode recordings with computational intelligence to localize STN during DBS surgery. The proposed research tools will provide valuable data for designing a new DBS surgery system that could be implemented by surgeons around the world. Such a system is expected to reduce the duration of the surgical procedure by enhancing STN localization, reduce the procedural hemorrhage rate by decreasing the number of microelectrode recording passes needed, and significantly decrease the rate of sub-optimal DBS electrode positioning, hence improving efficacy of stimulation. Moreover, the proposed efforts aim to address the nation?s current talent shortage in science and engineering majors in the field of neurotechnology. The interdisciplinary nature of the project offers a great environment for the education of graduate students with a concentration in instrumentation for use in the healthcare industry and the burgeoning field of neuromodulation."
"1320924","CIF:Small: Design and Analysis of Spatially-Coupled Coding Systems","CCF","COMM & INFORMATION THEORY","08/01/2013","04/21/2015","Henry Pfister","TX","Texas A&M Engineering Experiment Station","Standard Grant","richard brown","07/31/2017","$494,230.00","Krishna Narayanan","henry.pfister@duke.edu","400 Harvey Mitchell Pkwy S","College Station","TX","778454645","9798626777","CSE","7935","7923, 7935","$0.00","The storage and transmission of digital information is now ubiquitous in our society and plays a valuable role in the modern information-technology infrastructure.  Error-correcting codes are a key part of infrastructure components that efficiently and reliably store and transmit digital information.  Spatial coupling is a new technique for designing error-correcting codes that has been shown in some cases to achieve near-optimal performance with low-complexity decoding.  This research focuses on the design and analysis of innovative techniques for the storage and transmission of digital information based on spatially-coupled codes.<br/><br/>The research is broken into three broad thrusts.  The first thrust focuses on generalizing recent proof techniques for spatially-coupled codes to include a wide class of inference problems on spatially-coupled graphical models.  The second thrust considers the design of universal spatially-coupled codes for multiuser communication problems including the noisy Slepian-Wolf problem and multiple-access communication.  The third thrust targets source and channel coding problems with side information and focuses on nested spatially-coupled codes with enhanced message-passing decoding.<br/><br/>Spatial coupling also appears to be a general principle that is broadly applicable to problems involving message passing and inference on factor graphs. Therefore, this research is expected to have an impact on science and engineering disciplines beyond communications such as signal processing and machine learning."
"1319356","Randomized Models for Nonlinear Optimization:  Theoretical Foundations and Practical Numerical Methods","DMS","COMPUTATIONAL MATHEMATICS, OPERATIONS RESEARCH","09/15/2013","09/06/2013","Katya Scheinberg","PA","Lehigh University","Standard Grant","Leland Jameson","08/31/2017","$200,000.00","Frank Curtis","katyas@cornell.edu","Alumni Building 27","Bethlehem","PA","180153005","6107583021","MPS","1271, 5514","073E, 9263","$0.00","This project involves the design, analysis, and implementation of numerical algorithms for the mathematical optimization of large-scale, complex systems.  In particular, the novel feature of the proposed algorithms is the use of random sampling of objective function information in the context of solving deterministic (i.e., non-random) problems.  Despite the success of randomization in, e.g., stochastic gradient techniques for machine learning, it has yet to be used actively in other settings as it has been deemed too expensive in sequential computing environments.  However, with parallel computing becoming increasingly common, and with new advancements and convergence theory for randomized algorithms, these methods have great promise.  The research in this project will focus on the use of ``accurate'' randomized models, broadening of convergence theory, and implementation of effective software.  The novelty of the approach lies in achieving a middle ground between deterministic models that have to be accurate at each algorithmic step, and stochastic models that are accurate only in expectation, by exploiting random models that need to be accurate only with sufficiently high probability. The proposed strategies will balance per-iteration cost of the optimization routine with convergence speed while utilizing parallel computation.  The priority in the project on developing practical, general-purpose numerical methods based on theoretically sound methodologies solidifies the merits of the proposed work.<br/><br/>This project focuses on the development of novel numerical algorithms, and their analysis, for solving problems in two related realms of engineering design.  In the first, the aim is to minimize a quantity---e.g., cost, energy, or the discrepancy between expected and observed data---that can only be determined via a computer simulation. These ""black-box"" optimization problems arise in important areas such as molecular geometry optimization, circuit design, and groundwater modeling.  The second area represents those applications in which a given design needs to be robust under various input conditions, which includes problems in, e.g., medical image registration and the optimization of control systems.  The project promises to advance the study of algorithms for solving all of these types of problems via the common thread of exploiting randomization and parallel computation.The impact of this work will clearly be cross-disciplinary, and will benefit users of optimization methods and software in academia, governmental research laboratories, and private industry.  It will also promote the use of rigorous, classical algorithms in combination with randomized models for solving cutting-edge scientific problems.Finally, the educational plan will expose undergraduate and graduate students to modern efforts and challenges in computational mathematics, improve the educational opportunities for students interested in scientific research, and encourage  faculty interaction in area schools."
"1320603","SHF: Small: Bench-testing Environment for Automated Software Tuning (BEAST)","CCF","CI REUSE, HIGH-PERFORMANCE COMPUTING","08/01/2013","07/26/2013","Jack Dongarra","TN","University of Tennessee Knoxville","Standard Grant","Almadena Chtchelkanova","07/31/2016","$499,995.00","","dongarra@icl.utk.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","CSE","6892, 7942","6892, 7433, 7923, 7942, 9150","$0.00","In the world of high-performance scientific computing, the rapid emergence of hybrid processors that  make heavy use of accelerator technologies, such as Graphics Processing Units (GPUs) or the Intel Xeon  Phi (a.k.a., Many Integrated Cores, MIC), raises critical new challenges for computational scientists. Their research applications typically depend on computational kernels (i.e., software implementations of one or more of the basic patterns of scientific computing) that are optimized for speed. Such programs spend most of their computing time executing one or more of these kernels, and long experience has taught developers that tuning their kernels for the architecture of a given processor is absolutely essential to achieving excellent performance at the level of the individual computing node. Since scientists want to run these applications on supercomputers with thousands of such nodes, high performance at the node  level is essential to high productivity for the application at large. Unfortunately, for the vast majority of computational kernels, the three classic approaches to performance tuning?compiler-driven code transformations, low-level manual programming, or empirical autotuning?have always been very difficult, often producing mixed results; and the emerging era of hybrid processors makes all three techniques less effective still. The Bench-testing Environment for Automated Software Tuning (BEAST) makes a substantial contribution to solving this important problem.   <br/>BEAST creates a framework for exploring and optimizing the performance of computational kernels on hybrid processors that 1) applies to a diverse range of computational kernels, 2) (semi)automatically generates better performing implementations on various hybrid processor architectures, and 3) increases developer insight into why given kernel/processor combinations have the performance profiles they do. To achieve this three-fold goal, it applies the model used for traditional application benchmarking in a completely novel way: it combines an abstract kernel specification and corresponding verification test, similar to standard benchmarking, with an automated testing engine and data analysis and machine learning tools, called the BEAST workbench. Using a new method for specifying language-neutral code stencils and a prototype BEAST workbench, the project explores alternative tuning methods and strategies for a diverse range of computational kernels. <br/>Experiments carried out under this project are expected to show that the BEAST framework can dramatically improve the performance of many computational kernels that are of fundamental importance to scientific computing. As this software and the techniques for using it are made widely available to the science and engineering community, they will help to ensure the timely delivery of performance- optimized kernels for many domains and many types of hybrid processors, making the impact of the BEAST bench-tuning software infrastructure very broad indeed. Scientists and engineers, across a vast array of intellectually, economically and socially important domains, will be able to rapidly tune the underlying kernels in their applications to the characteristics of the latest platform, and thereby quickly gain the productivity benefits of each successive generation of accelerator technology."
"1253837","CAREER: Enabling License Compliance Analysis and Verification for Evolving Software","CCF","Software & Hardware Foundation, SOFTWARE ENG & FORMAL METHODS","09/01/2013","12/13/2017","Denys Poshyvanyk","VA","College of William and Mary","Continuing Grant","Sol Greenspan","08/31/2018","$478,010.00","","dposhyvanyk@wm.edu","Office of Sponsored Programs","Williamsburg","VA","231878795","7572213966","CSE","7798, 7944","1045, 7924, 7944, 9251","$0.00","This project proposes a novel unified model to help software developers license software and (re)use components complying with legal requirements. The solution will investigate novel combinations of information retrieval, internet-scale source code search, repository mining, and static analysis approaches to detect origins of software components.  The research will also rely on a feedback-driven hybrid blending of information retrieval and machine learning techniques for identifying components' licenses with high accuracy.  In addition, the proposed model will unify these building blocks for license compliance analysis and verification to reason about the given software, components, dependencies, and licenses, as well as their trustworthiness, constraints, and existing or potential legal compliance issues. <br/> <br/>The proposed research will lead to both theoretical foundations and practical solutions for the comprehensive analysis of complex legal compliance concerns to enable lawful software development and evolution. Among the broader impacts, the project will develop educational course content, involve underrepresented student groups, and produce software tools under open source licenses, collaborating with industry to transfer technology and empirically evaluate proposed research, and conducting K-12 outreach activities."
"1261830","ABI Innovation: Gini-based methodologies to enhance network-scale transcriptome analysis in plants","DBI","ADVANCES IN BIO INFORMATICS","09/01/2013","08/21/2014","Xiangfeng Wang","AZ","University of Arizona","Standard Grant","Peter McCartney","08/31/2016","$399,168.00","Ramin Yadegari","xwang1@email.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","BIO","1165","","$0.00","In biology, network techniques have been applied to interpret the interactions between genes, including the physical interactions of proteins and regulatory relationships between transcription factors and targets. Although numerous methods have been developed to infer a network from expression data, several computational challenges remain unsolved, such as, how to derive non-linear relationships between transcription factors and targets, how to properly decompose a network into individual sub-network modules, how to predict biologically significant genes via network-scale comparisons, how to integrate and use the heterogeneous forms of biological interaction data to facilitate network analysis, and how to seamlessly visualize a large-scale network for interactive data mining. To solve these problems, the primary goal of this project is to develop a software package - the Gini Network Analysis Toolkit (GNAT) that utilizes the Gini-based methodologies: a family of mathematical solutions that have been widely used in economics, physics, informatic networks, and social networks in analyzing non-normally distributed data. The core functional modules and algorithms in the GNAT include the use of supervised machine learning methods to infer transcriptional networks, the Gini correlation coefficient to derive non-linear regulatory relationships, the Gini regression analysis to decompose a time-series network, the Gini index to measure and compare the distributions of the network properties of modules and genes under different biological conditions, and eventually the discovery of biologically important genes with system perturbation and decision tree analysis. The PI will also develop a network explorer, BioNetscape, to efficiently organize and visualize the tremendous amount of network data generated from the GNAT using the k-core decomposition algorithm, Ajax technology and GPU (graphical processing unit) computing techniques. The GNAT will be implemented in R and organized as a streamlined workflow to compensate the shortcomings of the traditional gene-scale transcriptome analysis methods.<br/><br/>The GNAT software will greatly facilitate the ongoing network development projects in plant research. The GNAT will be made available to be integrated into the iPlant Discovery Environment, The Arabidopsis Information Resource (TAIR), Plant Expression Database (PLEXdb) and other consortium databases to enhance the function of network analysis and gene discovery in plants. The GNAT will also be integrated into the Galaxy and GenePattern platforms to provide a user-friendly graphical interface. The source-code and R packages will be released into the public domain for broader use in plant, animal and microbial biology. To integrate research into education, the PI?s laboratory will develop a web-based Virtual Next Generation Sequencing Workshop for training biologists who are not specialists in bioinformatics to analyze genomic, epigenomic, transcriptomic and small RNA data. The workshop courseware is composed of teaching materials prepared in the PI's class, self-practice datasets and a virtual UNIX web-console for training biologists to analyze different types of next generation sequencing data with minimal requirements for programming skills. This project explicitly addresses cross-disciplinary research training at multiple levels that will encourage the participation of underrepresented groups in computer sciences and mathematics at the University of Arizona, who will work to answering biological questions. The students from the ASEMS (Arizona Science, Engineering, and Math Scholars) and IGERT programs at the University of Arizona will participate in the PI's team to develop the GNAT, BioNetscape and VNW, and use these tools in their research."
"1319585","III: Small: Wavelet-Based Representations for Hyperspectral Data Processing and Interpretation","IIS","Info Integration & Informatics","10/01/2013","06/09/2016","Marco Duarte","MA","University of Massachusetts Amherst","Standard Grant","Sylvia Spengler","09/30/2017","$540,651.00","Mario Parente","mduarte@ecs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","7364","7364, 7923, 9102, 9251","$0.00","Hyperspectral imaging systems play an important role in scientific research, especially in planetary and terrestrial geology, environmental monitoring, military and security surveillance, and mineralogy. With current advances in imaging systems technology, large datasets at higher resolutions are being produced and research on automated analysis of these datasets is becoming critical. The goal of this project is to formulate mathematical models for hyperspectral datasets that capture the types of structure commonly identified by practitioners as informative and leveraged semantically in existing ad-hoc methods. The focus is on models based on wavelet representations for spectral signatures in order to provide features that represent the desired multiscale structural information. The features will leverage the wavelet's multiscale time-frequency analysis properties, and will be tested on several labeling, classification, and retrieval problems from a broad range of application areas that are expected to attract underrepresented groups in engineering. This proposal is centered on the application of the proposed models to hyperspectral signal processing and machine learning.  The overall goal is to formulate and study new geometric signal models for high-dimensional data and new performance metrics for parameter estimation to be leveraged by new computationally feasible estimation algorithms that (i) are amenable to compressive signal processing due to the use of geometric structure to capture relevant signal information and (ii) overcome the performance shortcomings observed in existing approaches. The formulation of a semantic model for spectral signatures is expected to increase the acceptance of universal representations in applications where processes driven by ad-hoc rules are commonplace. The project broadens participation of underrepresented groups by considering applications areas that have the potential to appeal to diverse communities and constituencies in order to attract interest from a diverse class of populations underrepresented in engineering. Student researchers will be exposed to collaborators from a diverse set of scientific backgrounds and cultures. Opportunities for undergraduate research and high-school teacher research experience will be offered in this project with a particular emphasis on venues serving groups underrepresented in engineering."
"1302687","CIF: Medium: Collaborative Research: New Approaches to Robustness in High-Dimensions","CCF","Comm & Information Foundations, SIGNAL PROCESSING","07/01/2013","04/16/2015","Martin Wainwright","CA","University of California-Berkeley","Continuing grant","John Cozzens","06/30/2017","$400,000.00","","wainwrig@eecs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","7797, 7936","7924, 7936","$0.00","Rapid development of large-scale data collection technology has<br/>ignited research into high-dimensional machine learning.  For<br/>instance, the problem of designing recommender systems, such as those<br/>used by Amazon, Netflix and other on-line companies, involves<br/>analyzing large matrices that describe users' behavior in past<br/>situations.  In sociology, researchers are interested in fitting<br/>networks to large-scale data sets, involving hundreds or thousands of<br/>individuals.  In medical imaging, the goal is to reconstruct<br/>complicated phenomena (e.g., brain images; videos of a beating heart)<br/>based on on a minimal number of incomplete and possibly corrupted<br/>measurements.  Motivated by such applications, the goal of this<br/>research is to develop and analyze models and algorithms for<br/>extracting relevant structure from such high-dimensional data sets in<br/>a robust and scalable fashion.<br/><br/><br/>The research leverages tools from convex optimization, signal<br/>processing, and robust statistics.  It consists of three main thrusts:<br/>(1) Model restrictiveness: Successful methods for high-dimensional<br/>data exploit low-dimensional structure; however, many real-world<br/>problems fall outside the scope of existing models.  This proposal<br/>significantly extends the basic set-up by allowing for multiple<br/>structures, leading to computationally efficient algorithms while<br/>eliminating negative effects of model mismatch.  (2) Non-ideal data:<br/>Missing data are prevalent in real-world problems, and can cause major<br/>breakdowns in standard algorithms for high-dimensional data. The<br/>second thrust devises relaxations and greedy approaches for these<br/>non-convex problems.  (3) Arbitrary Outliers: Gross errors can arise<br/>for various reasons, including fault-prone sensors and manipulative<br/>agents.  The third thrust proposes efficient and randomized algorithms<br/>to address arbitrary outliers."
"1331807","Collaborative Research:  CyberSEES:  Type 2: A New Framework for Crowd-Sourced Green Infrastructure Design","CCF","Information Technology Researc, CyberSEES","10/01/2013","11/22/2016","Arthur Schmidt","IL","University of Illinois at Urbana-Champaign","Standard Grant","Rahul Shah","09/30/2018","$572,300.00","William Sullivan, Arthur Schmidt, Jong Lee, Kenton McHenry","aschmidt@uiuc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","1640, 8211","","$0.00","This project develops a novel computational Green Infrastructure (GI) design framework that integrates interactive, neighborhood-scale, collaborative design by multiple stakeholders (""crowd-sourced"" design) with multi-scale models of ecosystem and human impacts. The following research questions are being addressed: (1) How well does coupling of site-scale ecohydrology with catchment-scale hydraulic routing improve predictions? (2) How well can stakeholder preferences be predicted using design image feature extraction and machine learning? (3) What interactive optimization and visualization techniques lead to the most rapid and complete consensus among diverse stakeholders? (4) Do stakeholders using interactive cyberinfrastructure tools consider more options and explore more of the GI design space?<br/><br/>A ""crowd-sourced"" design framework is developed to enable stakeholders to interactively create and evaluate potential GI designs that reflect consideration of the full breadth of social, economic, and environmental criteria. The research questions are evaluated in diverse neighborhoods within three urban catchments in the Baltimore Ecosystem Study, working closely with environmental non-governmental organizations to ensure that the results will provide significant benefits to community stakeholders. The models developed in this project are the first to integrate criteria for human and ecosystem wellbeing with site- and watershed-scale hydrologic processes, a key advance for improving understanding and implementation of GI design. Map and image visualization identifies which visualization approaches best support improving stakeholder engagement for achieving consensus using interactive collaborative design. This makes technical advances in interactive optimization and model parameterization accessible to the broad range of stakeholders, from regulators and planners to contractors and homeowners."
"1262034","9th Conference on Bayesian Nonparametrics","DMS","STATISTICS","04/01/2013","03/18/2013","Subhashis Ghoshal","NC","North Carolina State University","Standard Grant","Gabor Szekely","03/31/2014","$20,000.00","","subhashis_ghoshal@ncsu.edu","2701 Sullivan DR STE 240","Raleigh","NC","276950001","9195152444","MPS","1269","7556","$0.00","The 9th Conference on Bayesian Nonparametrics is going to be held in Amsterdam, The Netherlands, from June 10 to 14, 2013.  Bayesian nonparametrics has evolved as one of the fastest growing areas of research in modern statistics. Its applications areas include genetics, finance, survival analysis, sociology, networks and machine learning. The conference is the most important meeting of researchers working in theory, methodology and all types of applications of Bayesian nonparametrics all over the world. This grant supports junior researchers currently working in U.S. institutions (graduate students, postdoctoral researchers and junior faculty generally within three years of completion of their terminal degree) to participate in the conference. Participation in this meeting is critical for junior researchers working in this area. The primary objective of this conference is to bring together experts and young researchers, as well as theoreticians and practitioners, who use Bayesian nonparametric techniques. The conference has a well-structured balanced program covering various areas of the subject. The scientific committee of the conference consists of renowned international experts on Bayesian nonparametrics and related topics. The meeting will include four overview plenary talks, forty-two invited talks, six contributed talks and a contributed poster session. Many of the invited speakers including a plenary speaker are women.<br/><br/>Providing support for junior researchers who do not have access to other sources of funding to attend the important international gathering of scientists working on one of the fastest growing areas of statistical sciences is key to maintaining the current leadership of American institutions in this field. These workshops in the past were always characterized by a congenial atmosphere particularly supportive of junior researchers. The conference will include a series of activities especially designed to maximize the active participation of young researchers and to provide them with many opportunities for interaction with other young researchers and with more senior colleagues. The conference will also provide American researchers opportunity to exchange ideas with leading researchers from elsewhere in the world such as Europe, Asia and Latin America. In addition, the conference will provide opportunities for young researchers to disseminate widely the results of their work, not only through contributed talks and posters, but also by facilitating the publication of peer-reviewed papers and a proposed conference volume to be published by a leading publisher. The extensive poster session and some slots for contributed talks are especially reserved for young researchers. Women and minorities are highly encouraged to take part in the conference with the help from the travel support."
"1319402","AF:III:Small:Collaborative Research: New Frontiers in Join Algorithms: Optimality, Noise, and Richer Languages","CCF","ALGORITHMS","09/01/2013","07/08/2016","Atri Rudra","NY","SUNY at Buffalo","Standard Grant","Rahul Shah","08/31/2017","$326,101.00","Atri Rudra","atri@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","7926","7923, 7926","$0.00","The relational join is central to relational database processing, which is the dominant way data is processed today. The join also models problems in biological and social networks, coding theory, compressed sensing, machine learning, and constraint satisfaction. Recently,the investigators described the first ever worst-case optimal algorithm (the NPRR algorithm) for join queries.These new results open a line of new tools to attack a diverse set of fundamental problems related to the join. This project aims to further exploit the new algorithmic techniques developed for NPRR to address the following three classes of problems:<br/><br/>(1) Optimal Join algorithms. Developing algorithms that are instance optimal when the data are stored in either traditional database indexes or new indexing structures is a goal of this project. (2) Coping with and Leveraging Noise. This project will extend the latest work to handle and leverage both worst-case and statistical noise models, bridging to coding theory and compressed sensing.(3) Expressive Query Languages. The project will explore a series of extensions to join queries that will pave the way to overcome challenges in motif finding, search, databases with functional dependencies, and more powerful classes of queries and join operations.<br/><br/>If successful, the results of this grant will apply to a variety of pattern extraction problems in modern massive, dynamic, and noisy data sets, which have a wide range of applications in complex network analysis, coding theory, and compressive sensing."
"1262561","Collaborative Research: ABI Innovation: Breaking through the taxonomic barrier of the fossil pollen record  using bioimage informatics","DBI","ADVANCES IN BIO INFORMATICS","08/01/2013","06/17/2015","Surangi Punyasena","IL","University of Illinois at Urbana-Champaign","Continuing grant","Peter McCartney","12/31/2017","$514,016.00","","punyasena@life.illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","BIO","1165","CL10","$0.00","The practice of identifying pollen has a large number of scientific applications and is used in fields as diverse as archaeology, biostratigraphy (the dating of rocks), and forensic science. Pollen and spores play a particularly important role in paleontology, because they form the most abundant and extensive record of plant diversity, dating back hundreds of millions of years. However, the most critical hypotheses in plant ecology and evolution (e.g. the assembly of plant communities, speciation and extinction) cannot be fully tested with pollen data due to the extreme difficulty of recognizing species from pollen and spore material. This project develops new methods to probe the shape and fine structural and textural properties of the grains using high-throughput, super-resolution structured illumination microscopy and automated image analysis in order to transform species identification from a subjective, by-eye procedure to a quantitative, computational practice. Since it is not known a priori which morphological features are phylogenetically meaningful, new machine learning techniques are being developed to model pollen images at multiple scales, identify aspects of shape and texture that are statistically informative, and infer their relation to the underlying phylogenetic structure. <br/><br/>The project has the ambitious long-term goal of creating a high-throughput system for analyzing pollen data that incorporates meaningful characterizations of pollen and spore morphology, provides testable hypotheses of biological affinity, and is open and available to the entire scientific community. This will allow researchers to break through the current taxonomic limitations of pollen identification and fundamentally change current practices in the discipline on many levels, from the basic task of identification and counting to the interpretation and use of these data in global climate-vegetation models. The project brings together a diverse, interdisciplinary team including international collaborators at the Smithsonian Tropical Research Institute in Panama and will train graduate and undergraduate students from multiple scientific disciplines and backgrounds in an emerging area of interdisciplinary research. A public outreach component is in development that will include a virtual microscopy web site using images generated by this research to introduce non-experts to the beauty, complexity, and relevance of pollen morphology. Additional information about this project can be found at: http://www.life.illinois.edu/punyasena"
"1330952","SBIR Phase II:  Automatic Scalable Architectural Validation for Microprocessors","IIP","STTR Phase II, SBIR Phase II","09/01/2013","08/26/2017","Zaher Andraus","MI","Reveal Design Automation, Inc.","Standard Grant","Peter Atherton","07/31/2018","$1,121,854.00","","zaher@revealda.com","3025 Boardwalk Dr Ste 209","Ann Arbor","MI","481083230","7342728231","ENG","1591, 5373","097E, 165E, 169E, 5373, 8033, 8240","$0.00","This Small Business Innovation Research (SBIR) Phase II project addresses the challenge of automating and scaling formal equivalence verification between architectural SystemC models and RTL Verilog models for microprocessors and ASIC microcontrollers. The complexity of industrial processors, together with the differences in semantics of SystemC and Verilog, create a significant modeling gap that makes it infeasible to verify RTL Verilog implementations against their SystemC specification models. This gap impedes the progression currently taking place in EDA, wherein designers are moving upwards in the abstraction level for modeling and verifying hardware designs. Our formal equivalence verification technology will allow automatically obtaining RTL from ESL models using high-level synthesis tools, and formally verifying the correctness of the resulting models against the specification models. It will also allow manually written RTL models to be verified against ESL models originally created for architectural simulation. Expected challenges include overcoming the spatial and temporal modeling gaps, and verifying equivalence for an unlimited depth using finite equivalence formulations. By end of project, we anticipate to prototype a software program that will represent a product for architectural validation of general purpose microcontrollers, capable of proving equivalence or finding bugs with reasonable computational resources.<br/><br/>The broader impact/commercial potential of this project is to make formal verification technologies scalable and directly usable by designers at higher abstraction levels, enabling exponential growth in design complexity without exponential growth in verification cost. The products resulting from this project will provide substantial benefit by ensuring design correctness for mission-critical components such as implantable medical devices, aviation hardware, and satellite/space systems. In addition to hardware verification, the work done in this project will contribute to firmware and software verification, which has utilized similar techniques in the past. It will additionally contribute to exploring industrial-oriented algorithms and heuristics in the domain of automated reasoning and constraint satisfaction problems, used in theorem proving, machine learning, scheduling optimization, gaming, and network security."
"1340226","""Succinct Data Representations and Applications","CCF","COMPUT GAME THEORY & ECON","05/01/2013","05/02/2013","Christos Papadimitriou","CA","University of California-Berkeley","Standard Grant","Balasubramanian Kalyanasundaram","04/30/2014","$25,000.00","","cp3007@columbia.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","7932","8083","$0.00","Research and advances in Big Data is a high priority for the society.  A week-long workshop on the applications of succinct data representations to Big Data analysis  will bring together about forty mathematicians and computer scientists to exchange ideas about, and share current work on, succinct representations of data enabling sophisticated analysis, as in compressed sensing and streaming, sparsification, distributed sensing and data fusion, machine learning, optimization and complex query processing. The workshop will be in cooperation with the Simons Institute on the Theory of Computing at UC Berkeley, and will take place on September 16-20 2013, during the Institute's semester-long program on algorithmic foundations of big data analysis, thus also engaging in its work the (more than thirty) participants of that program.  The workshop will be open to all potential participants and the findings and video tapes of presentations will be distributed to the public for comments and engagements."
"1239319","CPS: Breakthrough: Distributed Computing Under Uncertainty: A New Paradigm for Cooperative Cyber-Physical Systems","CNS","INFORMATION TECHNOLOGY RESEARC","01/01/2013","09/11/2012","Nicola Elia","IA","Iowa State University","Standard Grant","David Corman","12/31/2016","$595,011.00","","nelia@umn.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","1640","7918, 9102, 9150","$0.00","This project is to develop a dynamical systems model of distributed computation motivated from recent work on the distributed computation of averages. The key idea is that static optimization problems (particularly convex optimization problems) can be solved by designing a dynamic system that stabilizes around the optimal solution of the problem. Moreover, when the optimization problem is separable, then the designed dynamic system decomposes into a set of locally-interacting dynamic systems. This is expected to open a door to a host of new computational approaches that take advantage of recent developments in control engineering including robust control (providing a mechanism for errors introduced by discretization), Markovian Jump Linear Systems (providing a mechanism for random discretization time), event-driven control (providing a mechanism for assured asynchronous execution), control over networks (providing a mechanism for improved performance of distributed computational systems in general). The new approach is essential in emerging applications, where the optimization runs on physically separated agents, operating in a noisy environment and communicating over unreliable channels.  As a test bed, the project will make use of a two-vehicle robotic system developed by the PI designed to monitor a crop of corn plants, where the dynamic systems perspective of this grant will, for example, allow for distributed optimal estimation toward the goal of optimal station-keeping.<br/><br/>By studying how natural systems can collectively compute and optimize, this research has potential to impact many disciplines involving networked systems, from controlling the electric power grid, to modeling the behavior of social, biological or economic systems.  It is directly applicable to cooperative networked multi-agent systems like robotic search and rescue missions and disaster-relief operations, distributed machine learning problems, and intelligent systems. An intriguing mix of motivating applications and theoretical problems offer a unique multidisciplinary educational opportunity to students who will be involved in the project, and provide exciting innovative material for courses and labs. Software developed will be distributed as open source via the CPS Virtual Organization."
"1262265","ABI Innovation: Computational methods for macromolecular binding analysis","DBI","ADVANCES IN BIO INFORMATICS, EPSCoR Co-Funding","08/01/2013","08/13/2013","Changhui Yan","ND","North Dakota State University Fargo","Standard Grant","Jennifer Weller","07/31/2017","$301,702.00","","changhui.yan@ndsu.edu","Dept 4000 - PO Box 6050","FARGO","ND","581086050","7012318045","BIO","1165, 9150","9150","$0.00","In the field of molecular biology, there is an urgent need for tools that can help elucidate the structural basis for macromolecular binding. This project will develop computational methods for automated discovery of structural and physical-chemical elements contributing to the affinity and specificity of macromolecular binding. Graph models for the representation of protein structures and graph kernel-based machine-learning methods will add to the analysis and prediction of binding sites. The proposed graph models will provide a succinct data structure to encode a range of structural and physical properties germane to molecular interactions. Particularly, the models will reflect the flexibility of protein structures. The proposed innovative graph-kernel-based approach will investigate the modular organization of binding sites and discover characteristic patterns associated with the modules. The methods and procedures produced in this work will assist researchers in pinpointing the location of the binding sites and elucidating the binding mechanism.<br/><br/>Comprehensive interdisciplinary educational and outreach plans will target undergraduates, graduates, and researchers from industry. Interdisciplinary bioinformatics courses will bring together students from life sciences and computer science and spearhead the curriculum development for the bioinformatics program at North Dakota State University (NDSU)."
"1344990","CSR: EAGER: Design and Implementation of a Fine-Grained Appliance Energy Profiling System for Green Building","CNS","SPECIAL PROJECTS - CISE, Computer Systems Research (CSR","07/01/2013","02/06/2015","Nirmalya Roy","MD","University of Maryland Baltimore County","Standard Grant","M. Mimi McClure","12/31/2015","$290,040.00","","nroy@umbc.edu","1000 Hilltop Circle","Baltimore","MD","212500002","4104553140","CSE","1714, 7354","7916, 9178, 9251","$0.00","Green building applications need efficient and fine-grained determination of power consumption pattern of a wide variety of consumer-grade appliances through non-intrusive load monitoring (NILM) techniques for an effective adaptation and percolation of demand response model down to the consumer level appliances. A key inhibitor to the widespread adoption of such demand response policy at the consumer grade appliances for intelligent building energy management, is the inability of smart plug to efficiently determine, control or infer the power consumption pattern of multiple devices in tandem.  In practice, deploying smart plug based NILM and acquiring the low-level power measures of a large number of devices is often difficult or impossible due to the deployment complexity and varying characteristics of devices and thus must instead be employed at the circuit-level and inferred through the incorporation of novel usage-based measurement and probabilistic level-based disaggregation algorithm. But the challenges in deploying non-intrusive load monitoring algorithm involve disaggregating individual device?s consumption from the aggregate power measurement, as well as modeling and incorporating the usage based prediction. Thus in this project we will focus on advanced machine learning and data analytics algorithms that capture the measurement based approach and circuit level NILM with the autonomous profiling and prediction logic to enable the deployment of flexible and fungible smart plug and the evolvability of future DR model in green building applications."
"1302267","CGV: Medium: Collaborative Research: A Heterogeneous Inference Framework for 3D Modeling and Rendering of Sites","IIS","GRAPHICS & VISUALIZATION","07/01/2013","05/16/2013","Holly Rushmeier","CT","Yale University","Standard Grant","Ephraim P. Glinert","06/30/2017","$600,000.00","Julie Dorsey","rushmeier@cs.yale.edu","Office of Sponsored Projects","New Haven","CT","065208327","2037854689","CSE","7453","7453, 7924","$0.00","Organizing and using 3D data related to physical sites is important in many applications such as historical reconstruction, architectural design, and urban planning.  However, no method has been developed that exploits the full range of data types available for such sites.  Useful data often comes from historical sources, and requires substantial processing to be useful.  Some of this processing can be automated, but some of it must be done by humans.  An as-yet unsolved problem is how to coordinate human effort to efficiently carry out this process.  In the current project the PIs will address quantitative and qualitative accuracy issues in reconstructing 3D sites so as to allow for input and participation by different populations in building data sets, and will demonstrate a variety of applications using a heterogeneous 3D site representation. Specifically, the work will make the following contributions: new techniques for annotating heterogeneous input will be developed, balancing automated and human input; new techniques for coordinating digital computation, human computation, and machine learning will be devised; new tools for architectural analysis and design, and for material weathering analysis, will be developed based on the new 3D representation; and new ideas for storytelling from 3D data will be demonstrated.  Project outcomes will include a new organization of heterogeneous data for 3D sites, new insights into the relative contributions of automated techniques and human computation in the domain of 3D site data (which will be applicable to other challenging problems involving large complex data sets), new algorithms for reconstructing 3D models, and new techniques for conducting studies in architecture and in cultural heritage.<br/><br/>Broader Impacts:  This research will have a strong impact on architectural-design and cultural heritage documentation, interpretation and communication.  The various phases of the project will involve students at both the graduate and undergraduate levels, and in diverse disciplines including computer science, architecture, and art history.  The PIs will produce teaching modules based on this work targeted at computer science, architecture, and cultural heritage."
"1309665","Collaborative Research:   Renyi Divergence-based Robust Inference in Regression, Time Series and Association Studies.","DMS","STATISTICS","07/15/2013","07/24/2013","Tharuvai Sriram","GA","University of Georgia Research Foundation Inc","Standard Grant","Gabor J. Szekely","08/31/2017","$75,999.00","","tn@stat.uga.edu","310 East Campus Rd","ATHENS","GA","306021589","7065425939","MPS","1269","","$0.00","This collaborative research project focuses on developing a novel approach to dimension reduction in regression, time series, and multivariate association studies based on a family of Rnyi divergences, with a central theme of providing estimators that are inherently robust to data contamination, sustaining only a minimal loss in efficiency. This family not only characterizes the conditional independence underlying the concept of sufficient dimension reduction in regression and time series, but also characterizes independence between canonical variates in multivariate association studies. The novelty of the approach lies in exploiting a tuning parameter of the family, which balances the efficiency and the degree of robustness of the estimators. In each of the three areas, this project focuses on investigating a host of issues such as: (i) the computation of estimates, (ii) the detection of the true dimension, (iii) the selection of an optimal tuning parameter, and (iv) a formal justification of the method via theory. Furthermore, the project focuses on carrying out an in-depth study of robustness via influence functions and sample/empirical influence functions. Finally, the project focuses on finding an optimal Rnyi divergence measure that is both robust and efficient, without the need for prior outlier detection or removal. <br/><br/>Rapid advances in technology have led to an information overload in most sciences. A typical characteristic of many contemporary datasets is that they are relatively high-dimensional in nature. This has prompted a shift in the applied sciences toward a different relationship-study genre arising in regression, time series and multivariate association, popularly known as dimension reduction, whose goal is to reduce the dimensionality of the variables as a first phase in the data analysis. However, the presence of outliers in high-dimensional datasets adversely affects the performance of existing dimension reduction methodologies, resulting in conclusions that are not completely reliable. Given that outliers are commonly encountered in high-dimensional datasets and that their presence is hard to detect, there is an urgent need to identify dimension reduction methods that possess some degree of automatic robustness, or non-sensitivity, to outliers. The proposed project provides robust dimension reduction methods, which would contribute significantly to the analysis of high-dimensional data arising in fields such as the social sciences, machine learning, sports, economics, environmental studies, morphometrics and cancer studies, among others. In fact, this project will not only provide novel tools for scientists in various disciplines to obtain reliable conclusions on high-dimensional data analysis, but also significantly advance the statistical theory, thereby paving a new research path in dimension reduction."
"1254204","CAREER: Structured Nonlinear Estimation via Message Passing: Theory and Applications","CCF","COMM & INFORMATION FOUNDATIONS, SIGNAL PROCESSING","03/01/2013","04/30/2015","Alyson Fletcher","CA","University of California-Santa Cruz","Continuing grant","John Cozzens","05/31/2017","$399,826.00","","akfletcher@ucla.edu","1156 High Street","Santa Cruz","CA","950641077","8314595278","CSE","7797, 7936","1045, 7936","$0.00","A fundamental challenge in engineering and science today is that systems contain tremendous numbers of interconnected components with complex interactions.  Examples include communication and sensor networks, high-dimensional medical images, or biological systems such as vast sets of interconnected spiking neurons responding to a large array of stimuli.  Graphical models provide a probabilistic framework for modeling such systems, and contemporary message-passing algorithms lead to computationally feasible operations by decomposing problems on larger systems into smaller ones.  This research develops a broader methodology and new algorithms to address larger classes of more complex nonlinear interconnected systems with potential for great technological impact.  For wider dissemination, this is coupled with educational initiatives including developing courses combining perspectives in signal processing, machine learning, and statistics in the context of modern applications.  An open-source code base will foster cross-disciplinary research in students, educators, and industry.<br/><br/>This research combines the power of high-dimensional graphical models with recent advances in random systems theory to tackle a much wider scope of problems than traditional message-passing or linear methods allow.  The investigator addresses the key gaps in scalable estimation and model inference for structured nonlinear systems and develops powerful general algorithms for solving core problems.  Four main objectives address  aspects of this broader goal: (i) systematic general methods for representing systems characterized by arbitrary interconnections of linear and nonlinear components; (ii) computationally scalable message-passing algorithms for estimation; (iii) rigorous quantification of high-dimensional performance; and (iv) validation of the methods on real data, including neurological system identification.  These research thrusts greatly expand the scope of statistical estimation techniques and provide a rigorous approach to large-scale signal processing problems underlying the big data technology of today."
"1302172","CGV: Medium: Collaborative Research: A Heterogeneous Inference Framework for 3D Modeling and Rendering of Sites","IIS","GRAPHICS & VISUALIZATION","07/01/2013","05/16/2013","Daniel Aliaga","IN","Purdue University","Standard Grant","Ephraim P. Glinert","06/30/2017","$600,000.00","Jennifer Neville","aliaga@cs.purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7453","7453, 7924","$0.00","Organizing and using 3D data related to physical sites is important in many applications such as historical reconstruction, architectural design, and urban planning.  However, no method has been developed that exploits the full range of data types available for such sites.  Useful data often comes from historical sources, and requires substantial processing to be useful.  Some of this processing can be automated, but some of it must be done by humans.  An as-yet unsolved problem is how to coordinate human effort to efficiently carry out this process.  In the current project the PIs will address quantitative and qualitative accuracy issues in reconstructing 3D sites so as to allow for input and participation by different populations in building data sets, and will demonstrate a variety of applications using a heterogeneous 3D site representation. Specifically, the work will make the following contributions: new techniques for annotating heterogeneous input will be developed, balancing automated and human input; new techniques for coordinating digital computation, human computation, and machine learning will be devised; new tools for architectural analysis and design, and for material weathering analysis, will be developed based on the new 3D representation; and new ideas for storytelling from 3D data will be demonstrated.  Project outcomes will include a new organization of heterogeneous data for 3D sites, new insights into the relative contributions of automated techniques and human computation in the domain of 3D site data (which will be applicable to other challenging problems involving large complex data sets), new algorithms for reconstructing 3D models, and new techniques for conducting studies in architecture and in cultural heritage.<br/><br/>Broader Impacts:  This research will have a strong impact on architectural-design and cultural heritage documentation, interpretation and communication.  The various phases of the project will involve students at both the graduate and undergraduate levels, and in diverse disciplines including computer science, architecture, and art history.  The PIs will produce teaching modules based on this work targeted at computer science, architecture, and cultural heritage."
"1308376","Monte Carlo methods for complex multimodal distributions with applications in Bayesian inference","DMS","STATISTICS","08/15/2013","07/30/2013","Qing Zhou","CA","University of California-Los Angeles","Standard Grant","Nandini Kannan","09/30/2016","$120,000.00","","zhou@stat.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","MPS","1269","","$0.00","When a posterior distribution has multiple modes, unconditional expectations, such as the posterior mean, may not offer informative summaries of the distribution. Motivated by this problem, the investigator proposes to develop Markov chain Monte Carlo (MCMC) methods that may generate sufficient samples from the domain of attraction of every major mode and therefore construct estimates for the probability mass of and conditional expectations given a domain. Computational methods will be developed to build the landscape of a distribution based on an MCMC sample. This project will contribute novel methodologies on MCMC and Bayesian inference with multimodal posterior distributions, and generalize theory on adaptive Markov chains. A new algorithm, based on the framework of the multi-domain sampler, will be developed to group dynamically domains separated by low barriers and to construct the tree of sublevel sets for a distribution. The tree includes local modes as terminal nodes and barriers as internal nodes. This project also develops Bayesian inference methods via domain-based estimation and algorithms to quantify the stability of a posterior mode and its domain of attraction, with applications in Bayesian missing data problems and structure estimation. Convergence and ergodicity of the multi-domain sampler with global moves will be studied under the framework of doubly adaptive MCMC. A theoretical model, based on the tree of sublevel sets, will be developed to facilitate convergence and efficiency analysis of MCMC algorithms. <br/> <br/>Scientific problems in many disciplines may be solved by sampling from a given probability distribution. Monte Carlo methods, Markov chain Monte Carlo in particular, are a class of stochastic simulation algorithms that may draw samples from almost any distribution. However, these algorithms suffer from low efficiency when the distribution has multiple local modes. Therefore, the first significance of the proposed project comes from its applicability to many problems in various scientific fields, including statistical physics, chemical physics, and computational biology. On the other hand, there are almost no existing methods that can extract useful information about a multimodal distribution from Monte Carlo samples. The proposed project includes a systematic development of computational methods for constructing novel and comprehensive summaries about a multimodal distribution via a unified graphical representation for the landscape of a distribution. This can greatly enhance the current understanding of many problems in statistics and machine learning by, for example, quantifying the difficulty of a problem and providing visualization of a high-dimensional objective function."
"1255826","CIF21 DIBBs: Designing the Roadmap for Social Network Data Management","OAC","DATANET","05/01/2013","03/07/2013","Thomas Carsey","NC","University of North Carolina at Chapel Hill","Standard Grant","Robert Chadduck","12/31/2014","$109,738.00","","carsey@unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","7726","7433, 8048","$0.00","CIF21 DIBBs: Designing the Roadmap for Social Network Data Management <br/>Scholarly interest in network analysis has increased dramatically in the social sciences and beyond. The explosion of social media tools such as Facebook, Flickr, and Twitter, along with new developments in machine learning and data mining have produced new types of behavioral data for scholars to analyze (Mislove et al., 2007). Significant advances in mathematics, statistics, and computer science have also produced unprecedented opportunities to analyze ?Big? social network data. Social network analysis sits at the cutting edge of social science and also links the social, natural, and computational sciences. Understanding the multi-faceted nature of social networks and their effects on human behavior is one of the grand challenges faced as this project seeks to maximize our investments in scientific research (NSF-ACCI, 2011). However, the community has not seen comparable advances in the management, archiving, and sharing of social network data. This presents a fundamental obstacle to advancing network science across the social, natural, and computational sciences. This proposal seeks to begin to remedy this problem. The data management needs social network scholars are complex. Network data often come from unstructured environments that require researchers to define and describe a set of units or actors (called nodes) and the connections (called edges) between them. Networks might be static or dynamic, include one type of node or multiple node types, and include edges that are uni-directional or bi-directional and weighted or unweighted. In addition, as relationships spread within the network and/or a network grows, the associated data management, data storage, and analytical memory requirements can grow exponentially.  This proposal brings together the social network analysis, information science, computer science, and data archive communities to develop a data infrastructure to support advanced analysis and research on social networks as well as to facilitate data sharing and archiving within this community. The group will address key questions concerning data storage architecture and lifecycle requirements, develop design specifications for creating a sustainable data infrastructure that will be discoverable, searchable, accessible, and usable to the entire research and education community, and initialize a prototype solution based on that plan.  Intellectual Merit: The proposed project will bring together the social network analysis community to work with information technology professionals to design a robust data management infrastructure to promote the sharing and interoperability of social network data. Effective data management for social network data amplifies the impact of research by revealing data quality issues early in the data collection process, ensuring that required data is retained and usable throughout the life of a research project. It also facilitates data sharing and reuse. A key to responsible data stewardship is the application and auditing of quality data management policies ? something included in this proposal. Providing a robust infrastructure to store, analyze, curate, share, and manage important social network data will increase researchers? production and provide an unprecedented view of the social world only visible through social network data.  Broader Impacts: The project will facilitate data sharing and increase social network data availability while assuring researchers that data management policies are followed. It will help formalize a community of network data experts that will begin developing best practices for the community. Availability of data particularly benefits early-stage researchers, and researchers at diverse institutions. Widespread availability of data facilitates citizen science and the integration of science and teaching at all levels of education. Managed data sharing is critical to the multi disciplinary research to answer the critical challenges facing society today. It would be difficult to overstate the importance of social network analysis to better understand human networks and social behavior."
"1308899","Evolving Combinatorial Structures","DMS","PROBABILITY","08/01/2013","07/19/2013","Harry Crane","NJ","Rutgers University New Brunswick","Standard Grant","Tomek Bartoszynski","07/31/2016","$130,350.00","","hcrane@stat.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","MPS","1263","","$0.00","The project studies probability models for evolving combinatorial structures, particularly partition, tree and graph-valued stochastic processes. Specific topics to be studied include representation and characterization theorems of combinatorial Markov processes, continuum tree and interval graph scaling limits, consistent systems of partition, tree and graph-valued processes, and connections to random matrices and Levy processes. The dominant theme of the research will be the effect of probabilistic symmetries, especially exchangeability, on the structural properties of evolving large combinatorial objects, as these structural properties impact various practical aspects of these processes. <br/><br/>As a result of this project, we should gain further understanding of models for time-varying discrete structures, especially partitions, trees and networks. Such processes arise as natural models in various disciplines, including genetics, physics, biology, computer science and statistics. In particular, understanding graph-valued processes has potentially far-reaching applications in the diverse and burgeoning field of complex networks. Effective models for real-world networks are relevant to problems in national security, public health, sociology, computer science and physical sciences. Other areas in which combinatorial models can be useful include phylogenetics, machine learning, statistics and Bayesian inference."
"1253292","CAREER: Autotuning for multicore and manycore architectures: an enhanced feedback-driven approach","CNS","Special Projects - CNS, CSR-Computer Systems Research","02/01/2013","04/20/2017","Apan Qasem","TX","Texas State University - San Marcos","Continuing Grant","Matt Mutka","01/31/2020","$550,018.00","","apan@txstate.edu","601 University Drive","San Marcos","TX","786664616","5122452314","CSE","1714, 7354","1045, 7354, 9178, 9251","$0.00","Achieving a high fraction of peak performance on complex architectures has been a perennial challenge for application developers. The emergence of multicore processors and accelerators has greatly exacerbated this problem. With an increasing number of cores per socket, deep hierarchies of shared and distributed caches, and exascale computing on the horizon, multicore platforms pose unprecedented challenges for software development and application tuning. This research confronts the challenge of multicore and manycore software development by improving automatic performance tuning through improved feedback diagnostics. Autotuning efficiency is achieved through enhanced knowledge of the problem domain, program features and architectural characteristics. To this end, we are developing a set of tools that allow specification, collection and synthesis of tuning related information. This rich set of information is then exploited through novel machine learning models to deliver scalable, portable and sustainable performance on diverse architectures. <br/><br/>In line with this research, we have established, SISTEM, a structured interdisciplinary program for undergraduates that brings together faculty and students from different STEM disciplines to explore cross-cutting research problems in which computational thinking, modeling and simulation play a central role.  <br/><br/>The techniques developed as past of this research will have a direct economic impact through saved energy and computation cycles on high-end systems. The improved efficiency of applications will allow researchers to execute large-scale simulations that will enable modeling of complex phenomenon in a broad range of disciplines including medicine, high-energy physics, climate modeling and nanotechnology."
"1414452","CAREER: The Dynamics of Collective Intelligence","IIS","Robust Intelligence","08/10/2013","06/02/2014","Sanmay Das","MO","Washington University","Continuing Grant","Jie Yang","05/31/2016","$227,682.00","","sanmay@wustl.edu","CAMPUS BOX 1054","Saint Louis","MO","631304862","3147474134","CSE","7495","1045, 7495, 9251","$0.00","This project studies the design of information systems like wikis and information markets. Research in social science has established that often there is a ""wisdom of the crowd"" -- i.e., collectives can display more intelligence than the individuals they are composed of. When such collective information systems work, they serve as superb aggregators and disseminators of information. However, fundamental computational challenges remain in understanding how to design them optimally.<br/><br/>This research is advancing along several lines, including<br/><br/>(1) general theories of how information is aggregated in different social media, developed and validated using real data gathered from existing databases and generated from user experiments; <br/><br/>(2) algorithms for facilitation of user interactions so that the medium in question can deliver the promised results (for example, market-making algorithms for liquidity provision in information markets);<br/><br/>(3) theoretical and practical characterization of the possibilities for rogue users to manipulate collective wisdom systems;<br/><br/>(4) algorithms for detecting malicious users, and mechanisms that thwart miscreants. <br/><br/>The research is naturally interdisciplinary in nature, drawing from machine learning and probabilistic reasoning, data mining and social networks, as well as finance and economics. It contributes to our understanding of complex social phenomena like the growth of information in wikis and blogs, as well as to the development of intelligent reasoning algorithms for agents in complex, uncertain multi-agent environments like markets.<br/><br/>The design of agents that participate in markets and social systems improves the quality of online markets and improves information flow in virtual spaces. Further, insights gained from modeling market structures and social spaces can tell us how to design them better. For example, understanding the impact of different levels of central control on wiki articles or open source software projects yields guidelines for how much central control is optimal in different settings.<br/><br/>In a world where computation and social systems are increasingly intertwined, the PI's research and education program exposes students to multidisciplinary ideas through the introduction of a new class on collective intelligence, social networks and e-commerce, and the development and extensive use of the very objects of study -- information markets and wikis -- in classroom and lab settings. The PI is also developing an experimental project for putting freely accessible course wikis online, similar to online course materials at other universities, but open to editing by the community."
"1338960","I-Corps:  Standardized MRI Interpretation for Low Back Pain Diagnosis","IIP","I-Corps","05/01/2013","04/17/2013","Vipin Chaudhary","NY","SUNY at Buffalo","Standard Grant","Rathindra DasGupta","10/31/2013","$50,000.00","","vipin@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","ENG","8023","","$0.00","The proposed project incorporates novel machine learning based methods from a large clinical Magnetic Resonance Imaging (MRI) dataset. These methods incorporate high- and low-level imaging features including intervertebral disc location, shape, and intensity. Moreover, these methods model the disc structure as a Markov chain to enable neighborhood information utilization. In addition to the imaging features, these methods utilize patient meta-data including patient age, height, weight, pain and disability score, patient history, and physical exam findings. The availability of these features provides information for the diagnosis of low back pain. These methods generate unbiased and reproducible interpretation. This technology is capable of providing standardized, unbiased, and reproducible MRI interpretation. <br/><br/>Many people are affected by low back pain and it is the most common reason behind job-related disability. It is a prominent chronic disease that causes major disruption in people's lives. An annual estimate of at least $50 billion is spent in the United States on diagnosis and related rehabilitation of low back pain patients. Moreover, the associated individualized treatment and rehabilitation cost is significant and often requires special pre-approval to undergo treatment from the insurance providers responsible for paying for health care costs. The most common current clinically approved standard for low back pain diagnosis is MRI testing and the diagnostic interpretation of MRIs is highly subjective. All subsequent therapeutic recommendations are based on the subjective report. This technology may be able to provide the MRI interpretation based on a standardized protocol will significantly impact the treatment plan outcome and minimize overtreatment."
"1302641","SHF: Medium:  Programmability, Portability, Performance and Energy Efficiency for Heterogeneous Systems","CCF","PROGRAMMING LANGUAGES","09/01/2013","08/13/2013","Vikram Adve","IL","University of Illinois at Urbana-Champaign","Standard Grant","Anindya Banerjee","08/31/2017","$899,786.00","Sarita Adve, Robin Rutenbar","vadve@cs.uiuc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7943","7924, 7943","$0.00","To maximize energy efficiency, future mobile devices will include a diverse range of hardware, such as large and small general-purpose processor cores, vector units, graphics processing units (GPUs), digital signal processors (DSPs), and semi-custom and custom accelerator cores.  This ""heterogeneity"" could power a new wave of innovation in mobile computing but is blocked by several fundamental challenges. Some of the biggest challenges are that such heterogeneous systems are highly challenging to program; that it is very difficult for software applications that use the diverse hardware to be portable across different mobile devices; that the memory systems in these devices are inflexible and inefficient; and that the semi-custom and custom accelerators are poorly integrated with the rest of the memory system and the programming environments.<br/><br/>A key insight behind this project is that a carefully designed hardware abstraction layer --- a ""Virtual Instruction Set"" --- that abstracts away the differences in parallelism and memory subsystems across the different compute units can provide a framework in which all of the above interrelated problems can be solved extremely effectively. The project is developing a framework called Virtual Instruction Set Computing that uses this approach to address the above challenges. The framework uses just two or three models of parallelism and a uniform, rich model of communication to capture the full spectrum of heterogeneous hardware.  The hardware memory architecture supports specialized memory sub-systems and novel memory optimizations customized for those sub-systems, while compilers partition the memory used by applications to make use of these partitions; together, these specialization techniques will provide an order of magnitude improvement in memory efficiency.  Semi-custom accelerators for the key domain of Machine Learning are driving new programming and memory system design techniques to integrate and use semi-custom accelerators in such systems.  The overall research builds on the widely used LLVM virtual instruction set and compiler infrastructure (previously developed by members of this research team), which are already widely used in industry, enhancing the potential for technology transfer from this work. If this project is successful, it can enable far more powerful mobile phones, tablets, and other such devices, and far more advanced software applications that can make full use of the rich capabilities of these devices."
"1329751","CPS: Breakthrough: Secure Telerobotics","CNS","Information Technology Researc, Secure &Trustworthy Cyberspace","10/01/2013","09/09/2013","Howard Chizeck","WA","University of Washington","Standard Grant","David Corman","03/31/2017","$500,000.00","Tadayoshi Kohno","chizeck@u.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","1640, 8060","7918","$0.00","In telerobotic applications, human operators interact with robots through a computer network. This project is developing tools to prevent security threats in telerobotics, by monitoring and detecting malicious activities and correcting for them. To develop tools to prevent and mitigate security threats against telerobotic systems, this project adapts cybersecurity methods and extends them to cyber-physical systems. Knowledge about physical constraints and interactions between the cyber and physical components of the system are leveraged for security. A monitoring system is developed which collects operator commands and robot feedback information to perform real-time verification of the operator. Timely and reliable detection of any discrepancy between real and spoofed operator movements enables quick detection of adversarial activities. The results are evaluated on the UW-developed RAVEN surgical robot.<br/><br/>This project brings together research in robotics, computer and network security, control theory and machine learning, in order to gain better understanding of complex teleoperated robotic systems and to engineer telerobotic systems that provide strict safety, security and privacy guarantees. The results are relevant and applicable to a wide range of applications, including telerobotic surgery, search and rescue missions, military operations, underwater infrastructure and repair, cleanup and repair in hazardous environments, mining, as well as manipulation/inspections of objects in low earth orbit. The project algorithms, software and hardware are being made available to the non-profit cyber-physical research community. Graduate and undergraduate students are being trained in cyber-physical systems security topics, and K-12, community college students and under-represented minority students are being engaged."
"1247469","BIGDATA: Mid-Scale: DCM: A Formal Foundation for Big Data Management","IIS","Big Data Science &Engineering","01/01/2013","07/30/2014","Dan Suciu","WA","University of Washington","Continuing Grant","Maria Zemankova","12/31/2017","$2,966,667.00","Bill Howe, Magdalena Balazinska","suciu@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","8083","7433, 7924, 8083","$0.00","The ability to analyze massive-scale datasets has become an important tool both in industry and in the sciences and many systems have recently emerged to support it. However, effective methods for deep data analytics are currently high-touch processes: they require a highly specialized expert who thoroughly understands the application domain and pertinent disparate data sources and who needs to perform repeatedly a series of data exploration, manipulation and transformation steps to prepare the data for querying, machine learning or data mining algorithms. This project explores the foundations of big data management with the ultimate goal of significantly improving the productivity in big data analytics by accelerating the bottleneck step of data exploration. The project integrates two thrusts: a theoretical study, which leads to new fundamental results regarding the complexity of various new (ad hoc) data transformations in modern massive-scale systems, and a systems study, which leads to a multi-platform software middleware for expressing and optimizing ad hoc data analytics techniques. The middleware is designed to augment and integrate existing analytics solutions in order to facilitate and improve methods of interest to the community and compatible with many existing platforms.<br/><br/>The results of this project will make it easier for domain experts to conduct complex data analysis on big data and on large computer clusters. All research results will be released in a middleware package layered on top of existing big-data systems. The middleware includes all the new algorithms, optimization techniques, fault-tolerance and skew mitigation mechanisms, and generalized aggregates developed during the project. In addition, the project develops and deploys a Web-based query-as-a-service interface to the new middleware. The project Web site (http://myriadb.cs.washington.edu) provides access to the software, additional results and information. Project results will be included in educational and outreach activities in big data analytics, including new curricula at the undergraduate, graduate, and professional levels."
"1253418","CAREER: Privacy Analytics for Users in a Big Data World","CNS","Special Projects - CNS, Secure &Trustworthy Cyberspace","02/01/2013","03/23/2017","Rachel Greenstadt","PA","Drexel University","Continuing Grant","Dan Cosley","01/31/2019","$494,747.00","","greenstadt@nyu.edu","1505 Race St, 10th Floor","Philadelphia","PA","191021119","2158955849","CSE","1714, 8060","1045, 7434, 9102","$0.00","Increasing amounts of data are being collected about users, and increasingly sophisticated analytics are being applied to this data for various purposes. Privacy analytics are machine learning and data mining algorithms applied by end-users to their data for the purpose of helping them manage both private information and their self-presentation.  This research develops privacy analytics that help users answer three interconnected questions about their online persona (1) What data does the user consider sensitive, and in what contexts should one share it?; (2) What does the data say about the user; and (3) Who knows what? These privacy analytics introduce a novel, inverse data mining problem where users analyze their data to estimate the conclusions the data will produce when incorporated into larger data sets.  This project designs new algorithms for quantitative and automated methods to detect privacy-related phenomena that have been observed qualitatively. These algorithms support the development of usable privacy enhancing technologies and will give users tools to cope with and manage their data in a complicated data environment. These tools will provide awareness to users about how their data is being used. These analytics will also help answer questions critical to the development of privacy law and policy.<br/><br/>This work involves approximately twenty-five undergraduates in research activities, exposing them to research methods and privacy issues.  This project also develops novel educational materials including course offerings for an interdisciplinary master's program in security and educational tools for use by the general public to bridge the digital divide.<br/>"
"1302285","III: Medium: Geometric and topological approaches to biomolecular structure and dynamics","IIS","Cellular Dynamics and Function, Information Technology Researc, Cross-BIO Activities, Info Integration & Informatics","09/15/2013","08/04/2016","Guowei Wei","MI","Michigan State University","Standard Grant","Sylvia Spengler","08/31/2018","$1,016,489.00","Yang Wang, Yiying Tong","wei@math.msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","CSE","1114, 1640, 7275, 7364","7924, 8750","$0.00","Experimental exploration of self-organizing biomolecular systems, such as viruses, molecular motors and proteins in Alzheimer's disease, has been a dominating driving force in scientific discovery and innovation in the past few decades. Unfortunately, quantitative understanding of biomolecular structure, function, and dynamics severely lags behind the pace of the experimental progress. An average protein in human body has about 5500 atoms, which, together with its surrounding water molecules, involve about 100,000 degrees of freedom. The dimensionality increases dramatically for complex biological processes and biomolecular systems. The real time structure optimization, dynamic simulation, and data analysis of molecular motors and/or viruses in human cells are intractable with full-atom models at present. A crucial question is how to reduce the number of degrees of freedom, while retaining the fundamental physics in complex biological systems. The proposed research may be transformative. As the first differential geometry based multiscale/ multiresolution approach to biomolecular systems, it will open a new direction and foster similar approaches in multiscale modeling of other large data systems in future research. Additionally, new persistently stable manifold strategy can be applied to other fields, such as image processing, computer aided design, and fluid mechanics. Furthermore, the proposed new coupled equations will lead to new research topics in geometry, topology, PDE analysis and mathematical biology. Finally, our new theoretical framework is directly integrated into popular software packages to ensure extensive usage by the community of researchers throughout mathematics, computer science and biology. The proposed research has a solid educational component. The project will support the training of student and junior researchers in mathematical modeling, data analysis and algorithm development. The enhancement of curricula from the proposed research is planned as a continuation of PIs teaching-research practice. Special curriculum development, outreach program and annual workshops are designed to further broaden educational and societal impacts. <br/><br/><br/>The proposed research addresses grand challenges in the structure, function and dynamics of self-organizing biomolecular systems due to exceptionally massive data sets. These challenges are tackled through the introduction of a new differential geometry based multiscale model, together with a multiresolution coarse grained method based on persistently stable manifolds in molecular dynamics data. This proposal offers innovative new approaches to an important area in massive data management, dimensionality reduction, computational mathematics and mathematical modeling. This project uses a number of geometric and topological approaches to address the scaling issues..  First, the multidisciplinary team will use multiscale framework which reduces the dimensionality and number of degrees of freedom by a macroscopic continuum description of the aquatic environment, and a microscopic discrete description of biomolecules. To further reduce the dimensionality of excessively large biomolecular systems, they introduce a multiresolution coarse-grained approach based on persistently stable manifolds in molecular dynamics data. A total free energy functional is introduced to bring the macroscopic surface tension and microscopic potential interactions on an equal footing. The differential geometry theory of surfaces is utilized to describe the interface between macroscopic and microscopic domains. Potential driven geometric flows are constructed to minimize the total free energy functional. Euler characteristic and total curvature are employed to analyze the topology and corresponding function of biomolecules. Frenet frames are utilized to characterize the local geometry and associated stable manifolds in dynamical data of biomolecular systems. Machine learning algorithms are proposed to extract stable manifolds. In the last step, a strategy is introduced to explore the persistence of stable manifolds, which provides the assurance for the reliability of the coarse grained model. In addition to promising and extensive preliminary results illustrating the power of this approach, extensive validation and application have been proposed to ensure that this methodology yields robust and powerful tools for biomolecular structure optimization and dynamical simulation."
"1253553","CAREER: Cooperative Motion Planning for Human-Operated Robots","IIS","CAREER: FACULTY EARLY CAR DEV, Robust Intelligence","10/01/2013","09/09/2013","Kris Hauser","IN","Indiana University","Continuing Grant","Gregory Chirikjian","05/31/2015","$283,620.00","","kkhauser@illinois.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","CSE","1045, 7495","1045, 7495","$0.00","This proposal outlines a research and educational plan to advance decision-making techniques for robots that cooperate with human operators. Because humans far exceed the abilities of state-of-the-art robots in vision, creativity, and adaptability, interest is rapidly growing in a human-centered approach to robotics: combining the strengths of humans with the superior precision and repeatability of robots. And yet, our available motion planning tools, while powerful at computing motions for complex autonomous tasks, are poorly suited for human-centered applications that demand responsive and natural motions. This proposal hypothesizes that a new cooperative motion planning paradigm will support major advances in intuitiveness and task performance of human-operated robots such as intelligent vehicles, tele-surgery systems, search and-rescue robots, and household robots. This hypothesis is echoed in an educational plan that aims to train engineers with cross-disciplinary strengths that bridge both the technical and social dimensions of robotics. Initial human subjects studies on novice operators with the PI's cooperative motion planning algorithms suggest that the technique leads to dramatic reductions in task completion time and collision rate in cluttered environments. The proposed work will conduct further investigations along this line of research to 1) identify characteristics of cooperative planners - such as optimality, responsiveness, and completeness - that yield effective human-operator systems, both in terms of objective performance metrics and subjective preferences, 2) to design planners that optimize cooperativity metrics under computational resource and communication constraints, and 3) to enhance the capabilities of such planners to assist operators in complex manipulation tasks.<br/><br/>The planners developed in this research and the rich datasets acquired via user studies will serve as resources to help human-robot interaction (HRI) researchers design safe and socially acceptable robot behaviors. Moreover, advances in cooperative motion planning may have long-term social and economic impact by enabling new applications of robotics in driver assist systems, space exploration, medicine, household robotics, manufacturing, and construction. Research is integrated with education in a range of activities that include CS curriculum development, development of a new graduate course on optimization and machine learning, and in new software libraries for robotics education. New modules on motion planning, behavior recognition, and HRI will be incorporated in AI and robotics courses. An REU is requested for each summer of the grant and will be recruited from a minority-serving institution in cooperation with the Alliance for the Advancement of African-American Researchers in Computing (A4RC). One or more IU undergraduates will be involved in research and mentored according to the Undergraduate Research Opportunities in Computing (UROC) program, with preference given to minority and women students."
"1312644","Computional Analysis of Inverse problems","DMS","COMPUTATIONAL MATHEMATICS","05/15/2013","05/14/2013","Yuanwei Qi","FL","The University of Central Florida Board of Trustees","Standard Grant","Junping Wang","08/31/2014","$26,290.00","Qiyu Sun, Jiongmin Yong, Alexandru Tamasan","yuanwei.qi@ucf.edu","4000 CNTRL FLORIDA BLVD","Orlando","FL","328168005","4078230387","MPS","1271","7556, 9263","$0.00","This grant supports an international conference on ""Computational Analysis of Inverse Problem,"" to be held in May 16-18, 2013 at the University of Central Florida (UCF) in Orlando, Florida. The theme of the conference is on computation and analysis of inverse problems. Inverse problems have wide range of applications in medical imaging, remote sensing, tomography and nondestructive testing, machine learning, geophysics and statistical inference. In the past 20 years, there have been major advances in inverse problems and its foundational aspects, where methods of functional analysis, partial differential equations and applied harmonic analysis have played pivotal roles in guiding the computational methods. The aim of the conference is to bring together top international experts to facilitate collaboration and communication among researchers under the common theme of inverse problems. There will be 20 invited speakers from USA, Europe and Asia Pacific, 15 of them from USA. Funds from NSF shall provide support for graduate students and recent PhDs and cover partial expenses of some domestic senior researchers.<br/><br/>Inverse problems have wide range of applications including medical and other imaging techniques, location of oil and other mineral deposits underneath the earth's surface through remote sensing, creation of astrophysical images from telescope. In addition to the traditional application in medical imaging, which plays a big role in improving national health care, the theory and computational methods developed to study inverse problems have demonstrated a big impact on energy independence through more effective drilling of oil and natural gas. They also help us to deep our understanding of climate change through creation of astrophysical images. The conference will serve the purpose of encouraging communication and collaboration between researchers not only in different scientific disciplines and industries, but also across international boundaries. The conference is expected to attract many young researchers and graduate students to participate in conference activities, in particular women and other under-represented minorities. The participation of graduate students and young researchers from USA and other countries will give arise to a good opportunity for mentoring activities for these groups."
"1318386","III: Small: Algorithms for decoding complex patterns of genomic variation","IIS","Info Integration & Informatics","09/01/2013","06/19/2015","Vineet Bafna","CA","University of California-San Diego","Continuing grant","Sylvia Spengler","08/31/2017","$500,000.00","","vbafna@cs.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7364","7364, 7923","$0.00","Genomes evolve and diversify through different mechanisms, including small point mutations, and large structural variations (SV). As entire populations of individuals get sequenced, we observe a complex mosaic of patterns. Some of these are characteristic of a selective constraint such as tolerance to lack of oxygen (for highlander populations), or lactose tolerance. In one aim of the proposal, the investigators develop computational techniques for identifying characteristic genetic patterns to identify genes that are adapting to these selective constraints. The other aims to reconstruct regions with complex variation patterns such as the Killer cell Immunoglobulin-like Receptor (KIR) region. KIR diversity plays a significant role in mediating immune response, helping with an understanding of diseases including rheumatoid arthritis, control of HIV disease progression as well as the success rate of cell replacement therapy for certain leukemias (blood cancer). The investigators will use a mix of techniques from combinatorial algorithms, machine learning, and population genetics to decode the genetic patterns. The proposal has broader impact in the field as part of a larger effort to develop efficient computational tools for genetic analysis; a critical problem in the modern era of inexpensive sequencing. The tools and technologies described here will have a direct impact on understanding the genetic diversity of populations, and towards a personalized approach to healthcare.<br/><br/>The proposal seeks to decipher the observed genetic variation across populations using two thrusts. In one thrust, it looks to haplotype genomic structural variation, and discover the genomic architecture of complex immunological regions like KIR and HLA. In a second thrust, the investigators analyze patterns of variation that are indicative of selective constraints. For selection signatures, the investigators will provide a better understanding of currently available tests using the scaled site frequency spectrum, and use an algorithmic approach to identify a better discriminator. For the rearranged genomic regions, the investigators will use optimization algorithms to adjust read coverage in highly repetitive regions. The proposal has broader impact in the field as part of a larger effort to develop effcient computational tools for genetic analysis; a critical problem in the modern era of inexpensive sequencing. The tools and technologies described here as well will have a direct impact on understanding the genetic diversity of under-represented populations, and towards a personalized approach to healthcare. The proposed research is tightly connected to undergraduate and graduate education, as all research here will be directly incorporated in interdisciplinary classes. The PI has a strong track record mentoring womena and other under-represented students in Computer Science."
"1345052","Climate Informatics Workshop","IIS","Information Technology Researc, CSR-Computer Systems Research, Info Integration & Informatics, EarthCube","09/15/2013","09/04/2013","Claire Monteleoni","DC","George Washington University","Standard Grant","Sylvia Spengler","08/31/2018","$90,702.00","","cmontel@colorado.edu","1922 F Street NW","Washington","DC","200520086","2029940728","CSE","1640, 7354, 7364, 8074","1640, 7354, 7364, 7433, 7484, 7556","$0.00","Understanding and responding to climate change is a key scientific and societal challenge of the 21st century. Recent advances in satellites and environmental sensors have made it possible to gather vast quantities of data concerning temperature, sea ice, sea level, rainfall, vegetation, etc. There is an urgent need for scientific and technical expertise for developing and effectively applying computational tools that can make use of such data to build increasingly accurate predictive models that offer insights to inform our understanding of, and response to, climate change. <br/><br/>This award supports a series of three workshops on Climate Informatics, an emerging discipline at the intersection of climate sciences and data sciences. The workshops, through a combination of tutorials that introduce climate scientists and data scientists to each other's disciplines, invited talks by established researchers in climate informatics, breakout sessions and for identifying research challenges and opportunities for interdisciplinary collaborations, serve a critically important role in building a vibrant Climate Informatics research community. The award provides support for the participation of approximately 60 to graduate students in each of the three workshops. The workshops contribute to the education and interdisciplinary training of a diverse workforce in STEM areas that span Climate Sciences and Data Sciences (including Machine Learning, Data Mining, Inference, Decision Making) as well as efforts to broaden the participation of women and other underrepresented groups in STEM research and education. Additional information about the Climate Informatics Workshop Series can be found at: https://sites.google.com/site/1stclimateinformatics/"
"1320620","RI: Small: Expressiveness and Automated Bundling in Mechanism Design: Principles and Computational Methodologies","IIS","Robust Intelligence","09/01/2013","08/27/2013","Tuomas Sandholm","PA","Carnegie-Mellon University","Standard Grant","Weng-keen Wong","08/31/2016","$425,000.00","","sandholm@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7495","7495, 7923","$0.00","Mechanism design is the science of designing the rules of a game (e.g., auction or election) so that good outcomes ensue despite each participant (human or computational) acting based on self-interest.  Intuitively, allowing individuals or organizations to express richer preferences should yield better outcomes.  Billions of dollars of annual savings from outcome efficiency improvements due to increased expressiveness have indeed been observed, for example, in combinatorial sourcing auctions by the PI and others.  What is missing is a rigorous methodology for designing appropriately expressive mechanisms.  This project combines new theoretical results in mechanism design - including computational measures of expressiveness - with custom search algorithms and machine learning techniques.  The goal is to create knowledge about mechanism design with varying levels of expressiveness.  The work also involves developing an operational methodology to guide the design of appropriately expressive mechanisms across a broad class of combinatorial and multi-attribute domains. Furthermore, the work will yield new theory and computational methodologies for bundling.  The objects of study are rational agents and agents with forms of bounded rationality.<br/> <br/>While many of the results will be general, the work will be validated in qualitatively different applications such as combinatorial auctions, advertising markets, and catalog-offer-based (web) commerce.  In the US alone, combinatorial multi-attribute sourcing auctions give rise to tens of billions of dollars in annual trade.  Annual volumes associated with advertising markets, spectrum auctions, consumer-to-consumer auctions, and catalog-based commerce are all in hundreds of billions or trillions of dollars.  Improvements would thus offer substantial economic and societal benefits."
"1317602","Computation of Large-Scale, Multi-Dimensional Sparse Optimization Problems","DMS","CDS&E-MSS","09/01/2013","09/04/2015","Wotao Yin","CA","University of California-Los Angeles","Continuing grant","Yong Zeng","08/31/2016","$299,999.00","","wotaoyin@math.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","MPS","8069","9263","$0.00","The proposed research largely lies in sparse optimization, a new distinct area of research in optimization for discovering sparse or other simple-structured solutions from dense datasets. Its development draws algorithmic techniques from classical nonlinear programming and is nurtured by the development in many other areas of data science. Today, the size, complexity, and diversity of instances have grown significantly. The proposed research addresses these new challenges in the following directions: data and variable splitting for handling multiple regularizers and for parallel and distributed optimization, efficient model path computation and regularization parameter selection, stochastic approximation, and coordinate descent methods for non-convex optimization. These investigations are expected to significantly reduce the running times of the existing algorithms, giving rise to novel algorithms to enable the solutions of a wide ranges of problems that are currently not solvable in data sciences. In particular, the expected results will fit machine learning models to data previously inaccessible (e.g., distributed data), enable the mining of data in much higher dimensions and across different modalities, as well as handle multiple regularizers in a computationally tractable way.<br/><br/>Technological advances in data gathering have led to a rapid proliferation of big data in diverse areas such as the Internet, engineering, climate studies, cosmology, and medicine. In order for this massive amount of data to make sense, new computational approaches are being introduced to let scientists and engineers analyze their data. Among these approaches, sparse optimization and structured solutions have grown enormously important. Today, their scopes are quickly expanding. Beyond the sensing and processing of 1D signals and 2D images, high-dimensional quantities such as 3D video, 4D CT, and multi-way tensors have become the data or unknown variables in models. Beyond the sparsity structure, structures such as low-rankness, sparse graph, tree structure, linear representation of a few dictionary atoms, as well as their combinations, have debut as desired structures in various applications including genome mapping, protein structure study, social network analysis, stock price prediction, and text/speech mining. The proposed research will build on the recent successes and lead to new techniques for handling large-sized, diverse-typed data and variables, novel algorithms for pursuing a variety of structures in solutions, the extension of existing numerical methods to parallel and decentralized computing architectures, and the contributions to solving key problems in several aforementioned application areas."
"1338099","MRI: Acquisition of Big-Data Private-Cloud Research Cyberinfrastructure (BDPC)","CNS","Major Research Instrumentation","10/01/2013","08/30/2013","Moshe Vardi","TX","William Marsh Rice University","Standard Grant","Rita Rodriguez","09/30/2016","$400,000.00","Lydia Kavraki, Ashok Veeraraghavan, Genevera Allen, Stephen Bradshaw","vardi@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","CSE","1189","1189","$0.00","Proposal #: 13-38099<br/>PI(s):  Vardi, Moshe; Allen, Genevera; Bradshaw, Stephen; Karaki, Lydia; Veeraraghavan, Ashok<br/>Institution:      Rice University<br/>Title: MRI/Acq.: Big-Data Private Cloud Research Cyberinfrastructure (BDPC)            <br/>Project Proposed:<br/>This project, acquiring a novel cyber-infrastructure instrument for big data cloud computing designed as a loosely coupled computations system with large memory requirements, enables a significant range of application domains as well as research into infrastructures for cloud computing. The domain sciences addressed range from development of big-data enabling software technologies spanning fundamental computer science work, to the analysis of electronic medical records, twitter streams, and hurricane evacuation strategies. Additional benefits are expected in understanding disease and therapeutic treatments, and in the development and application of mathematical models in the areas of machine learning, optimization, compressed sensing, image processing, and statistical analysis and data mining. The instrument will also help bridge the gap between numerical models and observations in astrophysics.<br/>Broader Impacts:  <br/>The broader impacts on society, and especially in education and training (including for members of underrepresented groups) are all compelling. The instrument will directly impact the educational experience for all students taking classes in computing and computational problem solving. The targeted research communities are diverse and broad, including the underrepresented groups, with strong empirical and experimental components. The proposed instrument is highly suitable for training and education."
"1355279","Collaborative Research: Balancing the Portfolio: Efficiency and Productivity of Federal Biomedical R&D Funding","SMA","SciSIP-Sci of Sci Innov Policy, SciSIP Infrastructure","07/01/2013","08/14/2013","Margaret Blume-Kohout","NM","New Mexico Consortium","Standard Grant","Mark Fiegener","07/31/2017","$369,561.00","","mblumeko@gettysburg.edu","4200 West Jemez Road, Suite 301","Los Alamos","NM","875442587","5054124200","SBE","7626, 8075","7626, 9150, 9179","$0.00","This cross-disciplinary, cross-institution collaborative research project combines economic analysis with state-of-the-art methods from statistical machine learning, to assess the relative efficiency and efficacy of research and development expenditures across the U.S. National Institutes of Health (NIH) portfolio of extramural projects. The novel combination of econometrics, topic modeling, and document classification permits analysis of massive collections of grant abstracts and scientific publications, identification of latent research topics present in NIH-funded research, assessment of possible spillover effects across research topics, and evaluation of causal linkages between changes in NIH funding by research topic and scientific advances. Two specific research outcomes are considered: scientific publications, classified by topic; and pharmaceutical innovation, measured by drugs entering into clinical development to treat specific diseases.  Planned research also includes refinement of existing economic theory to produce normative evaluations of the allocation of public research spending.<br/><br/>Broader Impacts: This research will inform key policy questions related to federal funding of biomedical research. First, fiscal austerity requires careful attention to the nation's research portfolio and investments. This project will describe and evaluate the productivity of those investments, as a first step towards policy recommendations for rebalancing the portfolio, to maximize society's expected return on investment. Second, if NIH funding for basic research spurs increased pharmaceutical innovation, NIH possesses an important policy tool to promote pharmaceutical R&D in areas of high therapeutic importance or significant health disparity."
"1306630","Algorithms and Statistical Methods for Personalized Diagnosis and Therapy in Cancer","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","07/01/2013","06/28/2013","Mathukumalli Vidyasagar","TX","University of Texas at Dallas","Standard Grant","Radhakisan Baheti","06/30/2018","$369,605.00","","m.vidyasagar@utdallas.edu","800 W. Campbell Rd., AD15","Richardson","TX","750803021","9728832313","ENG","7607","092E","$0.00","At present, there are several public efforts under way to generate massive data sets<br/>derived from all available cancer tissues. Such data is already available for four forms of cancer: Ovarian,<br/>lung, breast and colon, and more are on the way. One of the characteristics of these data sets is that the<br/>number of features that are measured is in the tens of thousands, while the number of tissue samples for<br/>each form of cancer is in the hundreds. The main challenge therefore is to extract the most informative features<br/>that can be used to distinguish one set of cancer patients from another, for example, those that respond to a<br/>particular form of therapy from those who do not. Such features, referred to as biomarkers, can then be used<br/>to develop therapies that are customized to focused groups or even individual patients. However, almost<br/>all available algorithms for extracting relevant features from big data sets face a ""barrier"" in that the number<br/>of features extracted is bounded below by the number of training samples. This number, which might be<br/>in the hundreds, is far too large to be useful in biological applications. In this project, it is proposed to<br/>develop some novel algorithms for feature extraction that can break through this ""barrier"" and identify far<br/>fewer features than the number of training samples. These newly developed algorithms will be analyzed<br/>in terms of their statistical behavior and their optimality; in addition they will be validated on actual data<br/>sets from lung, ovarian and endometrial cancer.<br/><br/>Another important aspect of current cancer therapy is the widespread acceptance of the need to use<br/>multi-drug combinations. This is because when a patient is treated with a single drug, almost invariably<br/>the tumor will grow back even if it shrinks initially, and the relapsed tumor is often resistant to the drug.<br/>Due to combinatorial explosion, it is not feasible to try out all possible combinations of drugs in experimental<br/>settings. Moreover, due to the complexity of the behavior of cancer cells, it is also not possible<br/>to develop analytical models for the mechanisms of action of multiple drugs used in combination. It is<br/>therefore imperative to develop methodologies for predicting the efficacy of multi-drug combinations while<br/>making almost no assumptions about the mechanism of action of each drug. In this project, it is proposed to use<br/>the so-called ""maximum entropy method"" to develop such a prediction methodology. The maximum entropy<br/>method was developed about fifty years in the context of deriving equilibrium statistical mechanics<br/>from information theory, and is widely accepted as one of the best methods to be used when it is desired to<br/>minimize the number of a priori assumptions.<br/><br/>Intellectual Merit: Currently available algorithms for classification and regression such as LASSO, elastic<br/>net, and Dantzig have the feature that the number of key features extracted is roughly equal to the number<br/>of training samples. However, even this number is too large to be of practical use in biological situations.<br/>Preliminary investigations on a new algorithm invented by the PI show that it does not have this limitation.<br/>Moreover, this new algorithm has shown promising performance on two types of cancer data sets:<br/>endometrial and ovarian. If a sound theoretical foundation can be established for the observed behavior of<br/>this algorithm, as well as for another that is still in the conceptual stage, that would be a very significant<br/>contribution to statistics and to machine learning theory. On another front, if it can be established through<br/>theory and experiment that the maximum entropy method can be used to predict the efficacy of multi-drug<br/>combinations, that would greatly advance both the theory of the method and the practical applicability of<br/>multi-drug therapy.<br/><br/>Broader Impacts: Cancer is the second leading cause of death in the USA, in other industrialized countries,<br/>and also in newly industrializing countries. It is widely accepted that cancer is the most ""individual"" of diseases<br/>in that no two manifestations are alike. Therefore personalized therapy is the way forward. However,<br/>there are very few methodologies for developing personal therapy that are agnostic as to the type of cancer.<br/>The present project aims to develop precisely such methodologies. Given the large mindshare of cancer in<br/>the scientific community and in society at large, it can be safely assumed that if the project is successfully<br/>completed, then the research findings would be followed up by the cancer researcher community. To hasten<br/>the process, the PI will work with several cancer researchers in the UT Southwestern Medical Center in<br/>Dallas and in the M. D. Anderson Cancer Center in Houston.<br/><br/>The project will entail the training of two graduate students and at least one undergraduate summer<br/>intern per year. This would serve to increase the pool of trained manpower and also to disseminate the<br/>analytical approach to cancer therapy design to a broader audience."
"1309954","Collaborative Research:   Renyi Divergence-based Robust Inference in Regression, Time Services and Association Studies","DMS","STATISTICS","07/15/2013","07/24/2013","Ross Iaci","VA","College of William and Mary","Standard Grant","Gabor J. Szekely","06/30/2017","$54,001.00","","riaci@wm.edu","Office of Sponsored Programs","Williamsburg","VA","231878795","7572213966","MPS","1269","","$0.00","This collaborative research project focuses on developing a novel approach to dimension reduction in regression, time series, and multivariate association studies based on a family of Rnyi divergences, with a central theme of providing estimators that are inherently robust to data contamination, sustaining only a minimal loss in efficiency. This family not only characterizes the conditional independence underlying the concept of sufficient dimension reduction in regression and time series, but also characterizes independence between canonical variates in multivariate association studies. The novelty of the approach lies in exploiting a tuning parameter of the family, which balances the efficiency and the degree of robustness of the estimators. In each of the three areas, this project focuses on investigating a host of issues such as: (i) the computation of estimates, (ii) the detection of the true dimension, (iii) the selection of an optimal tuning parameter, and (iv) a formal justification of the method via theory. Furthermore, the project focuses on carrying out an in-depth study of robustness via influence functions and sample/empirical influence functions. Finally, the project focuses on finding an optimal Rnyi divergence measure that is both robust and efficient, without the need for prior outlier detection or removal. <br/><br/>Rapid advances in technology have led to an information overload in most sciences. A typical characteristic of many contemporary datasets is that they are relatively high-dimensional in nature. This has prompted a shift in the applied sciences toward a different relationship-study genre arising in regression, time series and multivariate association, popularly known as dimension reduction, whose goal is to reduce the dimensionality of the variables as a first phase in the data analysis. However, the presence of outliers in high-dimensional datasets adversely affects the performance of existing dimension reduction methodologies, resulting in conclusions that are not completely reliable. Given that outliers are commonly encountered in high-dimensional datasets and that their presence is hard to detect, there is an urgent need to identify dimension reduction methods that possess some degree of automatic robustness, or non-sensitivity, to outliers. The proposed project provides robust dimension reduction methods, which would contribute significantly to the analysis of high-dimensional data arising in fields such as the social sciences, machine learning, sports, economics, environmental studies, morphometrics and cancer studies, among others. In fact, this project will not only provide novel tools for scientists in various disciplines to obtain reliable conclusions on high-dimensional data analysis, but also significantly advance the statistical theory, thereby paving a new research path in dimension reduction."
"1333841","Network Optimization of Functional Connectivity in Neuroimaging for Differential Diagnoses of Brain Diseases","CMMI","SERVICE ENTERPRISE SYSTEMS","09/01/2013","06/05/2014","Wanpracha Chaovalitwongse","WA","University of Washington","Standard Grant","Georgia-Ann Klutke","05/31/2017","$350,000.00","Paul Borghesani, Natalia Kleinhans, Tara Madhyastha, Thomas Grabowski","artchao@uark.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","ENG","1787","076E, 078E, 116E, 8023, 9147, 9178, 9231, 9251, MANU","$0.00","The objective of this award is to develop a computational framework for identifying the critical network topology of brain connectivity in neuroimaging data, specifically functional magnetic resonance imaging (fMRI). In this framework, network optimization modeling and mathematical programming algorithms will be employed to characterize connectivity patterns in fMRI data from different brain regions. Machine learning techniques will be employed to construct a pattern recognition model used to detect biomarkers and predict the brain disease conditions (i.e., abnormals vs. controls). An information-theoretic approach will be used to select the most informative brain regions to improve the generalizability and to increase the accuracy of the diagnosis prediction model.<br/><br/>If successful, the results of this research will lead to improvements in efficiency and efficacy of brain functional connectivity modeling and new developments of optimization methods for handling large-scale spatio-temporal data. The developed computational framework will be extremely useful for neuroscientists and neurologists to identify abnormal functional connectivity in the brain and to gain a greater understanding of the brain function. The framework will be employed and tested as a novel biomarker for differential diagnoses of brain disorders. Alzheimer?s disease (AD), autism spectrum disorder (ASD), and Parkinson?s disease (PD) will be the case points in this project to test if our computational framework is a sensitive enough tool to detect alterations in brain connectivity associated with brain disorders. Accurate diagnosis can substantially extend a patient?s lifespan and some treatments have different outcomes at different disease stages. Additionally, the developed computational framework can be applied to other real-life large-scale spatio-temporal data that arise in other research areas such as manufacturing, medicine, bioinformatics, neuroscience, finance, and geosciences."
"1262351","Collaborative Research: ABI Innovation: Breaking through the taxonomic barrier of the fossil pollen record using bioimage informatics","DBI","ADVANCES IN BIO INFORMATICS","08/01/2013","06/05/2015","Washington Mio","FL","Florida State University","Continuing grant","Peter H. McCartney","07/31/2017","$300,572.00","","mio@math.fsu.edu","874 Traditions Way, 3rd Floor","TALLAHASSEE","FL","323064166","8506445260","BIO","1165","","$0.00","The practice of identifying pollen has a large number of scientific applications and is used in fields as diverse as archaeology, biostratigraphy (the dating of rocks), and forensic science. Pollen and spores play a particularly important role in paleontology, because they form the most abundant and extensive record of plant diversity, dating back hundreds of millions of years. However, the most critical hypotheses in plant ecology and evolution (e.g. the assembly of plant communities, speciation and extinction) cannot be fully tested with pollen data due to the extreme difficulty of recognizing species from pollen and spore material. This project develops new methods to probe the shape and fine structural and textural properties of the grains using high-throughput, super-resolution structured illumination microscopy and automated image analysis in order to transform species identification from a subjective, by-eye procedure to a quantitative, computational practice. Since it is not known a priori which morphological features are phylogenetically meaningful, new machine learning techniques are being developed to model pollen images at multiple scales, identify aspects of shape and texture that are statistically informative, and infer their relation to the underlying phylogenetic structure. <br/><br/>The project has the ambitious long-term goal of creating a high-throughput system for analyzing pollen data that incorporates meaningful characterizations of pollen and spore morphology, provides testable hypotheses of biological affinity, and is open and available to the entire scientific community. This will allow researchers to break through the current taxonomic limitations of pollen identification and fundamentally change current practices in the discipline on many levels, from the basic task of identification and counting to the interpretation and use of these data in global climate-vegetation models. The project brings together a diverse, interdisciplinary team including international collaborators at the Smithsonian Tropical Research Institute in Panama and will train graduate and undergraduate students from multiple scientific disciplines and backgrounds in an emerging area of interdisciplinary research. A public outreach component is in development that will include a virtual microscopy web site using images generated by this research to introduce non-experts to the beauty, complexity, and relevance of pollen morphology. Additional information about this project can be found at: http://www.life.illinois.edu/punyasena"
"1262107","Collaborative Research: ABI Innovation: Genome-Wide Inference of mRNA Isoforms and Abundance Estimation from Biased RNA-Seq Reads","DBI","ADVANCES IN BIO INFORMATICS","09/01/2013","08/31/2013","Tao Jiang","CA","University of California-Riverside","Standard Grant","Peter H. McCartney","08/31/2017","$569,932.00","","jiang@cs.ucr.edu","Research & Economic Development","RIVERSIDE","CA","925210217","9518275535","BIO","1165","","$0.00","The University of California, Riverside and University of California, Los Angeles are awarded collaborative grants to identify mRNA isoforms on a genome-wide basis. Due to alternative splicing events in eukaryotic cells, the identification of mRNA isoforms (or transcripts) is a difficult problem in molecular biology. Traditional experimental methods for this purpose are time-consuming and cost  ineffective. The emerging RNA-Seq technology provides a possible effective way to address this problem. This project aims to develop efficient and accurate methods for inferring isoforms and estimating their abundance levels from RNA-Seq data where the reads may be sampled non-uniformly due to the existence of various biases including positional, sequencing and mappability biases. In particular, a novel statistical framework based on quasi-multinomial distributions will be introduced and a companion expectation-maximization (EM) algorithm developed for estimating isoform abundance levels that can handle all above biases in RNA-Seq data. The algorithms will be implemented efficiently in C++, tested extensively on both simulated and real RNA-Seq data in human, mouse and drosophila, and made available to the public for free. The performance of the algorithms will be evaluated extensively using both simulated and real RNA-Seq data. In the latter case, perturbations to some important splicing factors will be introduced into selected cell lines to induce widespread alteration of splicing events. RNA-Seq data of these cells, combined with quantitative RT-PCR validation, will provide an enriched dataset to assess the performance of the algorithms in  predicting both isoform abundance and relative variation. In addition, the  validation results may provide insight on the regulatory functions of the splicing  factors and serve as a testbed for further improvement of the algorithms.<br/><br/>The broader impact of this project is twofold. First, RNA-Seq data analysis is a timely topic in bioinformatics due to the recent rapid advance in next generation sequencing (NGS) technologies and its potential impact in life sciences and medicine. Despite the success of many RNA-Seq applications, several challenges remain in the analysis of RNA-Seq data, one of which comes from the understanding and handling of biases in RNA-Seq reads. The approaches proposed in this project for treating RNA-Seq biases combine unique techniques from statistics, machine learning and combinatorial algorithms. Moreover, the experimental validation results may shed light on the regulatory functions of some important splicing factors. Second, the project will provide an excellent opportunity for the training of two computer science PhD students, a postdoc and two biology undergraduate students in the interdisciplinary field of computational biology and bioinformatics. Since many of the involved students are female, the research will also help improve the representation of women in science and engineering."
"1315336","SBIR Phase I:  Objective Assessment of Agitation and Sedation","IIP","SMALL BUSINESS PHASE I","07/01/2013","06/17/2013","Behnood Gholami","NJ","Autonomous Healthcare Inc","Standard Grant","Jesus Soriano Molla","06/30/2014","$150,000.00","","bgholami@autonomoushealthcare.com","132 Washington St","Hoboken","NJ","070304692","3477741617","ENG","5371","124E, 5371, 8018, 8032, 8038, 8042","$0.00","This Small Business Innovation Research Phase I project is will investigate the feasibility of developing an objective agitation and sedation assessment algorithm for patients in the intensive care unit (ICU). Use of machine learning to identify clinically relevant information to characterize the agitation and sedation state of the patients is investigated. Furthermore, a plan to ?calibrate? and validate the agitation and sedation score provided by the algorithm will be designed. Agitation and sedation assessment is a challenging problem for patients undergoing critical care. Agitation, which is primarily characterized by excessive gross motor movement, is experienced by 74% of adults during ICU stay. Agitated patients may do physical harm to themselves by dislodging vital life support and monitoring devices with excessive musculoskeletal activity. Oversedation increases risk to the patient since liberation from mechanical ventilation may not be possible due to a diminished level of consciousness and respiratory depression from sedative. Currently, the assessment is performed by the clinical staff and no technology exists for such assessment. It is anticipated that through this research, novel algorithms for reliable detection of a patient?s agitation and sedation state using their physiological signals will be developed.<br/><br/><br/>The broader impact/commercial potential of this project includes reduction in clinical staff workload and healthcare costs. Current clinical practice in patient critical care requires the nursing staff to assess the patient's agitation and sedation state and provide sedatives to ameliorate the patient's agitation. The process relies on subjective assessments and may result in oversedation, which in turn increases the number of interventions, length of mechanical ventilation, and duration of stay in the ICU, and hence, increases healthcare costs. Development of an objective agitation and sedation assessment system can have a great impact on the quality of care in a critical care setting. Such a system can enable continuous patient monitoring and increase quality of care. Currently, clinical staff need to attend to multiple patients and continuous monitoring of patients is not feasible. In addition, in the absence of an automated agitation and sedation assessment algorithm, early indications of undersedation or oversedation can be overlooked due to the complex nature of the patient critical care problem and clinical staff's workload."
"1251151","BIGDATA: Small: DA: Patient-level predictive modeling from massive longitudinal databases","IIS","INFORMATION TECHNOLOGY RESEARC, Big Data Science &Engineering","07/01/2013","06/28/2013","Marc Suchard","CA","University of California-Los Angeles","Standard Grant","Sylvia J. Spengler","06/30/2017","$688,969.00","David Madigan","msuchard@ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","1640, 8083","7433, 7923, 8083","$0.00","Massive longitudinal healthcare data, such as administrative claims and electronic health records, provide an opportunity to greatly enhance the accuracy and clinical impact of patient-level predictions across a wide range of outcomes. This research targets the national priority domain of healthcare IT and showcases the advances that Big Data afford in helping patients make informed healthcare decisions leading to improved outcomes. Other involved stakeholders include healthcare providers, insurers and governmental agencies, and the databases this proposed grant employs encompass diverse and vulnerable patient populations, including the young, the poor and the elderly. Within this context, this grant is seeking to predict patient-level health events based upon personal characteristics and conditions. Accurate and well-calibrated predictions could significantly improve the wellbeing of patients and populations. This grant proposes to derive predictive models from massive observational data and then, for example, predict that a particular patient has an 18% chance of experiencing a stroke in the next 12 months. With this prediction in hand, caregivers and patients can optimize medical interventions and implement behavioral changes to hopefully prevent the predicted event. Further, this grant integrates two graduate student researchers, whose mentored experiences begin to rectify the shortage of data scientists trained at the intersection of statistics and medicine, and provides general statistical software tools for building large-scale predictive models from massive data across scientific domains.<br/><br/><br/>From a technical perspective, the proposed grant aims to first evaluate performance and applicability of an existing predictive model across five administrative claims and electronic health record databases covering over 80 million lives, using CHADS2 stroke risk as a motivating example. Then the grant will develop an innovative data-driven process for building patient-level predictive models from longitudinal observational data, and initially apply the process to predicting stroke in patients with atrial fibrillation for comparison of performance against CHADS2, Finally, the grant aims to explore characteristics of the process<br/>and resulting models, such as: evaluation of out-of-sample predictive performance in different databases; consideration of how models change over time; and assessment of which clinical variables most substantially contribute to patient-level predictions. Together, this research will focus on identifying heuristics to extract clinically relevant predictors from longitudinal electronic healthcare data, developing algorithms to use this information in multivariate modeling through massive parallelization using graphics processing units, optimized for data sparsity, and evaluating performance based on accuracy in predicting outcomes at the patient level. As a proof-of-concept, the grant will develop an approach to predict stroke risk and apply this approach across five disparate data sources (80+ million patients, including drugs, lab values, procedures, emergency room visits, primary care visits, inpatient encounters, etc) that reflect diverse patient populations across the US, including the privately insured, Medicare-eligible, and Medicaid beneficiaries. The underlying goal of the grant is to apply innovative statistical and machine learning techniques using advancing computer technology to large-scale observational data to develop accurate and well-calibrated patient-level predictive models enabling the prediction of future medical events for individual patients."
