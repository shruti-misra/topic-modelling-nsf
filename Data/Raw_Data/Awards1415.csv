DocID,AwardNumber,Title,NSFOrganization,Program(s),StartDate,LastAmendmentDate,PrincipalInvestigator,State,Organization,AwardInstrument,ProgramManager,EndDate,AwardedAmountToDate,Co-PIName(s),PIEmailAddress,OrganizationStreet,OrganizationCity,OrganizationState,OrganizationZip,OrganizationPhone,NSFDirectorate,ProgramElementCode(s),ProgramReferenceCode(s),ARRAAmount,Abstract
0,1439052,XPS: FULL: DSD: Collaborative Research: Rapid Prototyping HPC Environment for Deep Learning,CCF,Exploiting Parallel&Scalabilty,8/1/14,8/5/14,Jack Dongarra,TN,University of Tennessee Knoxville,Standard Grant,Tevfik Kosar,7/31/17,"$382,500.00 ",Piotr Luszczek,dongarra@icl.utk.edu,1331 CIR PARK DR,Knoxville,TN,379163801,8659743466,CSE,8283,9150,$0.00 ,"The impact of Big Data is all around us and is enabling a plethora of commercial services. Further it is establishing the fourth paradigm of scientific investigation where discovery is based on mining data rather than from theories verified by observation. Big Data has established a new discipline (Data Science) with vibrant research activities across several areas of computer science. This ?Rapid Python Deep Learning Infrastructure? (RaPyDLI) project advances Deep Learning (DL) which is a novel exciting artificial intelligence approach to Big Data problems, which also involves a sophisticated model and a corresponding ?big compute? needing high end supercomputer architectures. DL has already seen success in areas like speech recognition, drug discovery and computer vision where self-driving cars are an early target. DL uses a very general unbiased way of analyzing large data sets inspired by the brain as a set of connected neurons. As with the brain, the artificial neurons learn from experience corresponding to a ?training dataset? and the ?trained network? can be used to make decisions. Trained on voices, the DL network can enhance voice recognition and trained on images, the DL network can recognize objects in the image. A recent study by the Stanford participants in this project trained 10 billion connections on 10 million images to recognize objects in an image. This study involved a dataset that was approximately 0.1% the size of data ?learnt? by an adult human in their lifetime and one billionth of the total digital data stored in the world today. Note the 1.5 billion images uploaded to social media sites every day emphasize the staggering size of big data. The project aims to enhance by DL by allowing it to use large supercomputers efficiently and by providing a convenient DL computing environment that enables rapid prototyping i.e. interactive experimentation with new algorithms. This will enable DL to be applied to much larger datasets such as those ?seen? by a human in their lifetime. The RaPyDLI partnership of Indiana University, University of Tennessee, and Stanford enables this with expertise in parallel computing algorithms and run times, big data, clouds, and DL itself.<br/>RaPyDLI will reach out to DL practitioners with workshops both to gather requirements for and feedback on its software. Further it will proactively reach out to under-represented communities with summer experiences and DL curriculum modules that include demonstrations built as ?Deep Learning as a Service?.<br/>RaPyDLI will be built as a set of open source modules that can be accessed from a Python user interface but executed interoperably in a C/C++ or Java environment on the largest supercomputers or clouds with interactive analysis and visualization. RaPyDLI will support GPU accelerators and Intel Phi coprocessors and a broad range of storage approaches including files, NoSQL, HDFS and databases. RaPyDLI will include benchmarks as well as software and will offer a repository so users can contribute the high level code for a range of neural networks with benefits to research and education."
1,1439007,XPS: FULL: DSD: Collaborative Research: Rapid Prototyping HPC Environment for Deep Learning,CCF,Exploiting Parallel&Scalabilty,8/1/14,8/5/14,Geoffrey Fox,IN,Indiana University,Standard Grant,Tevfik Kosar,7/31/17,"$315,000.00 ","Gregor von Laszewski, Judy Qiu",gcf@indiana.edu,509 E 3RD ST,Bloomington,IN,474013654,3172783473,CSE,8283,,$0.00 ,"The impact of Big Data is all around us and is enabling a plethora of commercial services. Further it is establishing the fourth paradigm of scientific investigation where discovery is based on mining data rather than from theories verified by observation. Big Data has established a new discipline (Data Science) with vibrant research activities across several areas of computer science. This ?Rapid Python Deep Learning Infrastructure? (RaPyDLI) project advances Deep Learning (DL) which is a novel exciting artificial intelligence approach to Big Data problems, which also involves a sophisticated model and a corresponding ?big compute? needing high end supercomputer architectures. DL has already seen success in areas like speech recognition, drug discovery and computer vision where self-driving cars are an early target. DL uses a very general unbiased way of analyzing large data sets inspired by the brain as a set of connected neurons. As with the brain, the artificial neurons learn from experience corresponding to a ?training dataset? and the ?trained network? can be used to make decisions. Trained on voices, the DL network can enhance voice recognition and trained on images, the DL network can recognize objects in the image. A recent study by the Stanford participants in this project trained 10 billion connections on 10 million images to recognize objects in an image. This study involved a dataset that was approximately 0.1% the size of data ?learnt? by an adult human in their lifetime and one billionth of the total digital data stored in the world today. Note the 1.5 billion images uploaded to social media sites every day emphasize the staggering size of big data. The project aims to enhance by DL by allowing it to use large supercomputers efficiently and by providing a convenient DL computing environment that enables rapid prototyping i.e. interactive experimentation with new algorithms. This will enable DL to be applied to much larger datasets such as those ?seen? by a human in their lifetime. The RaPyDLI partnership of Indiana University, University of Tennessee, and Stanford enables this with expertise in parallel computing algorithms and run times, big data, clouds, and DL itself.<br/>RaPyDLI will reach out to DL practitioners with workshops both to gather requirements for and feedback on its software. Further it will proactively reach out to under-represented communities with summer experiences and DL curriculum modules that include demonstrations built as ?Deep Learning as a Service?.<br/>RaPyDLI will be built as a set of open source modules that can be accessed from a Python user interface but executed interoperably in a C/C++ or Java environment on the largest supercomputers or clouds with interactive analysis and visualization. RaPyDLI will support GPU accelerators and Intel Phi coprocessors and a broad range of storage approaches including files, NoSQL, HDFS and databases. RaPyDLI will include benchmarks as well as software and will offer a repository so users can contribute the high level code for a range of neural networks with benefits to research and education."
2,1439005,XPS: FULL: DSD: Collaborative Research: Rapid Prototyping HPC Environment for Deep Learning,CCF,Exploiting Parallel&Scalabilty,8/1/14,8/5/14,Andrew Ng,CA,Stanford University,Standard Grant,Tevfik Kosar,7/31/17,"$202,500.00 ",,ang@cs.stanford.edu,450 Jane Stanford Way,Stanford,CA,943052004,6507232300,CSE,8283,,$0.00 ,"The impact of Big Data is all around us and is enabling a plethora of commercial services. Further it is establishing the fourth paradigm of scientific investigation where discovery is based on mining data rather than from theories verified by observation. Big Data has established a new discipline (Data Science) with vibrant research activities across several areas of computer science. This ?Rapid Python Deep Learning Infrastructure? (RaPyDLI) project advances Deep Learning (DL) which is a novel exciting artificial intelligence approach to Big Data problems, which also involves a sophisticated model and a corresponding ?big compute? needing high end supercomputer architectures. DL has already seen success in areas like speech recognition, drug discovery and computer vision where self-driving cars are an early target. DL uses a very general unbiased way of analyzing large data sets inspired by the brain as a set of connected neurons. As with the brain, the artificial neurons learn from experience corresponding to a ?training dataset? and the ?trained network? can be used to make decisions. Trained on voices, the DL network can enhance voice recognition and trained on images, the DL network can recognize objects in the image. A recent study by the Stanford participants in this project trained 10 billion connections on 10 million images to recognize objects in an image. This study involved a dataset that was approximately 0.1% the size of data ?learnt? by an adult human in their lifetime and one billionth of the total digital data stored in the world today. Note the 1.5 billion images uploaded to social media sites every day emphasize the staggering size of big data. The project aims to enhance by DL by allowing it to use large supercomputers efficiently and by providing a convenient DL computing environment that enables rapid prototyping i.e. interactive experimentation with new algorithms. This will enable DL to be applied to much larger datasets such as those ?seen? by a human in their lifetime. The RaPyDLI partnership of Indiana University, University of Tennessee, and Stanford enables this with expertise in parallel computing algorithms and run times, big data, clouds, and DL itself.<br/>RaPyDLI will reach out to DL practitioners with workshops both to gather requirements for and feedback on its software. Further it will proactively reach out to under-represented communities with summer experiences and DL curriculum modules that include demonstrations built as ?Deep Learning as a Service?.<br/>RaPyDLI will be built as a set of open source modules that can be accessed from a Python user interface but executed interoperably in a C/C++ or Java environment on the largest supercomputers or clouds with interactive analysis and visualization. RaPyDLI will support GPU accelerators and Intel Phi coprocessors and a broad range of storage approaches including files, NoSQL, HDFS and databases. RaPyDLI will include benchmarks as well as software and will offer a repository so users can contribute the high level code for a range of neural networks with benefits to research and education."
3,1420897,RI: Small: Dynamic Attractor Computing: A Novel Computational Approach Applied Towards Temporal Pattern and Speech Recognition,IIS,Robust Intelligence,10/1/14,8/18/14,Dean Buonomano,CA,University of California-Los Angeles,Standard Grant,Kenneth Whang,9/30/17,"$399,698.00 ",,dbuono@ucla.edu,10889 Wilshire Boulevard,LOS ANGELES,CA,900951406,3107940102,CSE,7495,"7495, 7923, 8089",$0.00 ,"Harnessing the brain's computational strategies has been a long sought objective of computational neuroscience and machine learning. One reason this goal has remained elusive is that most neurocomputational frameworks have not effectively captured a fundamental computational feature of the brain: the ability to seamlessly encode, represent, and processes temporal information. The current project seeks to address this shortcoming by using the neural dynamics inherent to recurrent neural networks to generate temporal patterns and process temporal information. <br/><br/>The ability to generate the fine motor patterns necessary to play the piano or parse the complex temporal structure of speech, are but two examples of the human brain's sophisticated ability to generate and process complex temporal patterns. Notably, both these examples also illustrate an additional feature of the brain's computational abilities: ""temporal warping."" We can play the same musical piece at different speeds, or understand speech spoken at slow or fast rates. The mechanisms underlying the brain's ability to process temporal information in a flexible and temporally invariant fashion are a key focus of the current proposal. Recent theoretical and experimental studies have favored the view that the brain does not have sampling rates, time bins or explicit delay lines; but rather encodes time and the temporal features of stimuli through the internal dynamics of recurrent neural networks. The computational potential of these recurrent neural network models, however, has been limited for two reasons: 1) while it is well established that the recurrent connections of neural circuits are plastic, it has proven challenging to incorporate plasticity into simulated recurrent neural network models; 2) the dynamic regimes with the most computational potential are precisely those that also exhibit chaos--voiding much of their computational potential because the dynamics is not reproducible across trials. Building on a novel framework, this project tunes the weights of the recurrent connections in a manner that ""tames"" the chaotic dynamics of recurrent networks. The approach creates locally stable trajectories (dynamic attractors) which provide a novel and potentially powerful computational approach that can elegantly encode temporal information, and retain internal memories of recent events. Of particular relevance to the current project is to demonstrate that these networks can produce families of similar neural trajectories that flow at different speeds, thus allowing the network to generate the same complex motor pattern at different speeds. This project will also determine if the principles of dynamic attractors and time warping can be applied in the domain of sensory processing, using speech recognition as a test bed for the brain's ability to discriminate complex spatiotemporal stimuli."
4,1451177,CAREER: Machine Learning Theory with Connections to Algorithmic Game Theory and Combinatorial Optimization,CCF,Algorithmic Foundations,6/1/14,9/12/14,Maria-Florina Balcan,PA,Carnegie-Mellon University,Continuing grant,jack snoeyink,11/30/15,"$289,279.00 ",,ninamf@cs.cmu.edu,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,CSE,7796,"1045, 9218",$0.00 ,"Over the years, Machine Learning has become a very broad discipline with important applications to many areas including computer vision, speech recognition, robotics, and bio-surveillance, to name just a few. Moreover, many of these application areas have faced a huge increase in the volume of available data of various kinds. In order to be able to use all this data a number of new learning approaches have been proposed. These approaches have been intensely explored in the machine learning community, with many heuristics and specific algorithms, as well as  experimental results reported. Unfortunately, however, the standard theoretical models do not capture the key issues involved in these learning techniques, and it has become clear that for developing robust, versatile, and general algorithms in these settings a more fundamental understanding is necessary. This project will develop theoretical foundations for such learning paradigms which are of significant practical importance but  are not explained by existing theoretical models. This project will also develop fundamental new connections between Machine Learning, Game Theory, and Combinatorial Optimization, that will aid in advancing and solving important problems in all three areas.<br/><br/>The key research directions of this project are:<br/><br/>1. Developing mathematical foundations and algorithms for important machine learning paradigms that are not captured well by standard models. This includes new analysis frameworks as well as new practical and theoretically justified algorithms both for semi-supervised learning and active learning, two important emerging approaches for incorporating unlabeled data and interaction in the learning process. This project will also explore a fundamentally new approach to analyzing clustering -- a classic central task in the analysis and exploration of data, for which the existing theory has been very brittle. This framework will enable practitioners to describe in a formal way the properties they believe to be true about their data, and then use these properties to choose or design the right algorithm for their needs.<br/><br/>2. Developing novel fundamental connections between Machine Learning and Algorithmic Game Theory in order to solve difficult problems in multi-agent systems that have resisted previous approaches. In particular, many multi-agent interactions have bad equilibria, and it is important to develop methods for helping agents in such bad states to move to better ones. This project will develop techniques for understanding and influencing the behavior of natural dynamics in games of this type, by using connections to important concepts  in Machine Learning, such as learning from untrusted experts' advice.<br/><br/>3. Developing fundamental connections between Machine Learning and Combinatorial Optimization in order to advance both areas. These include new connections between approximation algorithms and learning-based objectives for clustering, and new algorithms and computational theory for learning submodular functions. Submodular functions, which describe laws of diminishing returns, are ubiquitous in economic optimization problems, and methods for learning them from observed data can aid in designing improved decision procedures.<br/><br/>Altogether, this project will advance Machine Learning, Algorithmic Game Theory, and Combinatorial Optimization, by developing and exploiting novel connections between these areas. The theory developed by this work will enable the next generation of powerful algorithms for machine learning and multi-agent systems. It will additionally impact a wide range of application areas including computer vision, robotics, bio-surveillance, and online auction design. More broadly, the research results of this project will have impact across a number of scientific, medical, and industrial fields. The PI's education plan further contributes to the project's impact. In addition to advising a diverse set of students on projects directly related to this project, the progress in this research will be used to influence the curriculum via special courses presenting the theoretical advances along with their applications. The PI will also contribute to increasing the participation of women in computational sciences."
5,1409886,RI: Medium: Collaborative Research: Models of Handshape Articulatory Phonology for Recognition and Analysis of American Sign Language,IIS,Robust Intelligence,6/1/14,6/9/14,Diane Brentari,IL,University of Chicago,Standard Grant,Tatiana Korelsky,5/31/19,"$275,546.00 ",Jason Riggle,dbrentari@uchicago.edu,6054 South Drexel Avenue,Chicago,IL,606372612,7737028669,CSE,7495,"7495, 7924",$0.00 ,"Sign languages are the primary means of communication for millions of Deaf people in the world, including about 350,000-500,000 American Sign Language (ASL) users in the US.  While the hearing population has benefited from advances in speech technologies such as speech recognition and spoken web search, much less progress has been made for sign language interfaces.  Advances depend on improved technology for analyzing sign language from video.  In addition, the linguistics of sign language is less well-understood than that of spoken language.  This project addresses both of these needs, with an interdisciplinary approach that will contribute to research in linguistics, language processing, computer vision, and machine learning.  Applications of the work include better access to ASL social media video archives, interactive recognition and search applications for Deaf individuals, and ASL-English interpretation assistance.<br/><br/>This project focuses on handshape in ASL, in particular on one constrained but very practical component:  fingerspelling, or the spelling out of a word as a sequence of handshapes and trajectories between them.  Fingerspelling comprises up to 35% of ASL, depending on the context, and includes 72% of ASL handshapes, making it an excellent testing ground.  The project addresses gaps in existing work by focusing on handshape in various conditions, including fast, highly coarticulated signing.  The main project activities include development of (1) robust automatic detection and recognition of fingerspelled words using new handshape models, including segmental and ""multi-segmental"" graphical models of ASL phonological features; (2) techniques for generalizing across signers, styles, and recording conditions; (3) improved phonetics and phonology of handshape, in particular contributing to an articulatory phonology of sign; and (4) publicly released multi-speaker, multi-style fingerspelling data and associated semi-automatic annotation."
6,1409837,RI: Medium: Collaborative Research: Models of Handshape Articulatory Phonology for Recognition and Analysis of American Sign Language,IIS,ROBUST INTELLIGENCE,6/1/14,6/9/14,Karen Livescu,IL,Toyota Technological Institute at Chicago,Standard Grant,Tatiana Korelsky,5/31/18,"$854,131.00 ",Gregory Shakhnarovich,klivescu@ttic.edu,6045 S Kenwood Ave,Chicago,IL,606372803,7738340409,CSE,7495,"7495, 7924",$0.00 ,"Sign languages are the primary means of communication for millions of Deaf people in the world, including about 350,000-500,000 American Sign Language (ASL) users in the US.  While the hearing population has benefited from advances in speech technologies such as speech recognition and spoken web search, much less progress has been made for sign language interfaces.  Advances depend on improved technology for analyzing sign language from video.  In addition, the linguistics of sign language is less well-understood than that of spoken language.  This project addresses both of these needs, with an interdisciplinary approach that will contribute to research in linguistics, language processing, computer vision, and machine learning.  Applications of the work include better access to ASL social media video archives, interactive recognition and search applications for Deaf individuals, and ASL-English interpretation assistance.<br/><br/>This project focuses on handshape in ASL, in particular on one constrained but very practical component:  fingerspelling, or the spelling out of a word as a sequence of handshapes and trajectories between them.  Fingerspelling comprises up to 35% of ASL, depending on the context, and includes 72% of ASL handshapes, making it an excellent testing ground.  The project addresses gaps in existing work by focusing on handshape in various conditions, including fast, highly coarticulated signing.  The main project activities include development of (1) robust automatic detection and recognition of fingerspelled words using new handshape models, including segmental and ""multi-segmental"" graphical models of ASL phonological features; (2) techniques for generalizing across signers, styles, and recording conditions; (3) improved phonetics and phonology of handshape, in particular contributing to an articulatory phonology of sign; and (4) publicly released multi-speaker, multi-style fingerspelling data and associated semi-automatic annotation."
7,1423276,RI: Small: CompCog: Modeling Latent Discrete Knowledge Across Utterances,IIS,Robust Intelligence,8/1/14,6/10/16,Jason Eisner,MD,Johns Hopkins University,Continuing Grant,Tatiana Korelsky,7/31/18,"$457,999.00 ",,jason@cs.jhu.edu,1101 E 33rd St,Baltimore,MD,212182686,4439971898,CSE,7495,"7495, 7923, 9251",$0.00 ,"Each human language is a system of conventions for communicating information.  Yet how does everyone know this complex system?  Describing it is difficult even for linguists.  Yet young children somehow figure out the rules and vocabulary of their native language.  Adults continue to learn when confronted with unfamiliar words, with new conventions associated with social media, or with the layout conventions of a new website. <br/> <br/>This project develops new artificial intelligence methods for tasks of this kind.  These methods will enable computers to deal with a wider variety of human language data, thus improving information access and global communication.  They will also provide insight as to why human intelligence is able to succeed at these problems.<br/><br/>The methods will seek to discern the systematic structure that explains the patterns in naturally occurring linguistic data.  Specifically, our computers will analyze naturally occurring data in order to learn:<br/><br/>* How to break down words into meaningful parts and reassemble those parts into new words.  This is a subject that linguists call morphophonology.  It is practically important in automated analysis and translation of speech and text.<br/><br/>* How to break down sentences into meaningful phrases.  This requires determining the basic word order facts of the language -- the problem of grammar induction, considered to be a central mystery of human language learning.<br/><br/>* How to extract machine-readable data from large websites that present databases in human-readable form.  This involves automatically figuring out the database structure and layout conventions of a website.<br/><br/>* How to track names across large quantities of informal text.  By discovering the principles that govern how people use and modify names, a computer can recognize that the nickname ""Vlad P."" or the misspelled patronymic ""Vladimir Vladimirovich"" might be variant ways of referring to ""Vladimir Putin,"" especially in a political comment.  <br/><br/>The project will address each of these domains in a principled way.  Our strategy in each domain is to develop a novel Bayesian generative model along with efficient, principled machine learning algorithms for approximate inference.  We expect to expand the range of modeling and inference techniques that are available to the natural language processing community.  <br/><br/>Innovative technical directions include the automatic reconstruction of phonological underlying forms, a novel treatment of grammar induction as structured prediction, a nonparametric model of databases and database-backed websites, and a phylogenetic model of name variation."
8,1415757,SBIR Phase I:  Translational Information Management for Industry,IIP,SMALL BUSINESS PHASE I,7/1/14,5/20/14,Bruce Buchanan,TX,i2k Connect LLC,Standard Grant,Peter Atherton,12/31/14,"$143,800.00 ",,buchanan@cs.pitt.edu,10419 Ten Point Lane,Missouri City,TX,774592996,7134137880,ENG,5371,"5371, 8032",$0.00 ,"The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project will be improved effectiveness of document management systems in U.S. Businesses.  The project integrates novel approaches to unsupervised machine learning, concept identification, and ontology construction to create a sustainable content management system that will allow companies to find and associate information more accurately and efficiently. Corporations run on information, and routine operations depend on finding information efficiently. For example, corporate acquisitions require filing information quickly in the acquiring company's systems; employee turnover necessitates intelligent analysis to enable continuing operations; and regulatory compliance and legal retention requirements demand consistent categorization and correct retention of records. While creating electronic documents is easy, finding and analyzing them remain difficult tasks. The proposed project is intended to provide effective assistance to companies, within everyday business practices, without requiring major investments in change. Distribution of information between corporate data centers and the cloud further necessitates tools to help with classification consistency and searchability. If successful, this project will provide an encompassing framework within which company workflows are integrated and corporate workers can more easily and efficiently extract usable information from corporate IT systems. <br/><br/>This Small Business Innovation Research (SBIR) Phase I project provides new software tools for knowledge workers. Industrial information technology requires the integration of proven methods in a robust, sustainable framework. The investigators' prior work in artificial intelligence demonstrated that a well-designed framework, with open source packages and interstitial software, can provide an effective knowledge management system. In this project the company intends to mine and extend research ideas from knowledge management, artificial intelligence, natural language processing, machine learning, information retrieval and human-computer interfaces. Work in artificial intelligence has shown that domain knowledge is necessary for high performance problem solving. The company intends to leverage corporate knowledge to augment keyword search with semantics of the domain.  Concept identification methods developed for natural language processing will be used to augment the powerful statistical tools provided by unsupervised machine learning and information retrieval technology."
9,1406049,II-EN: Software Tools for Monte-Carlo Optimization,CNS,"CCRI-CISE Cmnty Rsrch Infrstrc, Robust Intelligence",10/1/14,6/20/16,Alan Fern,OR,Oregon State University,Standard Grant,Todd Leen,9/30/19,"$456,286.00 ","Thomas Dietterich, Prasad Tadepalli, Alex Groce, Sinisa Todorovic",afern@eecs.oregonstate.edu,OREGON STATE UNIVERSITY,Corvallis,OR,973318507,5417374933,CSE,"7359, 7495","7359, 7495, 9251",$0.00 ,"The Computing Research Infrastructure project supports the development of an open-source software library for Monte Carlo methods in artificial intelligence on a cloud-based platform.  Monte Carlo methods are randomized numerical algorithms used in AI, machine learning, data mining, and the physical sciences.  As data size and model complexity continue to grow, advanced large-scale Monte-Carlo techniques (including parallel implementation) has become ubiquitous.  However software tools to easily implement advanced techniques for large-scale Monte Carlo are not established or broadly available.<br/><br/>The software library developed in this project will help bridge this gap, and lower the barrier to adoption of advanced Monte Carlo techniques by a broad research community.  The library will include a variety of existing state-of-the-art algorithms, as well as novel software components. The algorithms and tools have many important applications, including: <br/>(a) optimization of ecological management problems, including endangered species conservation, forest fire management, and invasive species management (b) automated software testing, (c) optimization for experimental design in science and engineering, (d) tracking of multiple objects from noisy visual evidence, and (e) activity and object recognition in computer vision. These problems have significant societal and economic importance and the research has the potential to significantly extend current capabilities.<br/><br/>The algorithms and tools will be implemented using a common interface supporting a cloud-based platform, which will allow other researchers to extend and apply the library to important applications. The software library will be integrated into the<br/>undergraduate and graduate curriculum at Oregon State University.  In addition, an online course centered around the theory and application of the library components will be developed, facilitating use by a wide audience.<br/><br/>The developed library will include components based on the investigators' research that realize a number of technical<br/>innovations for Monte-Carlo Optimization (MCO), including: a) Exploiting multi-fidelity simulators in MCO for offline and online planning, (b) Developing MCO techniques for item discovery problems, (c) Developing MCO techniques for online policy improvement in sequential decision making, (d) Learning to reduce branching factors for more efficient online MCO, and (e) Integrating symbolic reasoning and MCO for scalable sequential decision making. These new capabilities will advance the state-of-the-art in artificial intelligence and enable new applications to be addressed that are beyond the scope of prior MCO methods."
10,1423515,RI: Small: Algorithms for accelerating optimization in deep learning,IIS,Robust Intelligence,8/1/14,4/14/15,Miguel Carreira-Perpinan,CA,University of California - Merced,Standard Grant,Rebecca Hwa,7/31/18,"$457,999.00 ",,mcarreira-perpinan@ucmerced.edu,5200 North Lake Road,Merced,CA,953435001,2092012039,CSE,7495,"7495, 7923, 9251",$0.00 ,"Intelligent processing of complex signals such as images or sound is often performed by a parameterized, nested hierarchy of simple nonlinear processing layers, as in a deep neural net, an object recognition cascade or a speech front-end. Joint estimation of the parameters of all the layers and selection of an optimal architecture is a difficult nonconvex optimization problem, difficult to parallelize, and requiring significant human expert effort, which leads to suboptimal systems in practice. This research will develop a new, general mathematical strategy to learn the parameters and, to some extent, the architecture of nested systems, called the method of auxiliary coordinates (MAC). MAC has provable convergence, is easy to implement reusing existing algorithms for single layers, applies even when parameter derivatives are not available, easily makes use of parallel architectures, and often provides reasonable models within a few iterations.  The PI's research and teaching will introduce undergraduate students to the design of nested machine learning systems, and provide computer science graduate students with skills in optimization, in support of specific areas (machine learning, computer vision/speech, etc.). The PI will broadly disseminate the results of the research by publishing papers in optimization, machine learning, computer vision and speech. The PI will make Matlab and C/C++ code available for the algorithms developed and use it as teaching aid in his courses on optimization and machine learning. The PI will strive to involve a diverse population of students in the research.<br/><br/>MAC could drastically facilitate, by reducing runtime and human effort, the practical design and estimation of complex models currently being developed in data-rich disciplines such as machine learning, computer vision and speech, but also in other areas of engineering and science. It could also obviate the construction of hand-crafted features in trees and other classifiers. It may bring a wide-ranging and timely benefit to society given that serial computation is reaching a plateau and cloud computing is becoming a commodity, and intelligent data processing is finding its way into mainstream devices (phones, cameras, etc.), thanks to increases in computational power and data availability.  The research will develop MAC along two aims. Optimization: The PI will explore different ways to define auxiliary coordinates; different algorithms to solve the MAC-constrained problem; and efficient W and Z steps. The PI will also investigate the convergence properties of these approaches and their ability to be parallelized. Machine learning: The PI will apply MAC-based optimization to several existing and new nested models: deep neural nets; best-subset feature selection; learning features for decision trees; dictionary and classifier learning; parametric embeddings; learning hash functions; distal learning; model adaptation; and others. The evaluation plan includes standard benchmarks and articulatory speech modeling tasks. The research is interdisciplinary and potentially transformative: it opens many opportunities for research in optimization and machine learning, promoting thinking in terms of MAC formulations when designing learning systems, and could replace or complement the backpropagation algorithm in learning nested systems both in the serial and parallel settings. The models developed for articulatory speech will also improve our understanding of speech production."
11,1409257,RI: Medium: Collaborative Research: Write A Classifier: Learning Fine-Grained Visual Classifiers from Text and Images,IIS,Robust Intelligence,6/15/14,6/10/16,Smaranda Muresan,NY,Columbia University,Continuing Grant,Jie Yang,5/31/19,"$463,208.00 ",,smara@columbia.edu,2960 Broadway,NEW YORK,NY,100276902,2128546851,CSE,7495,"7495, 7924",$0.00 ,"This project develops the learning strategy using textual narrative and images makes the learning effective without a huge number of images that a typical visual learning algorithm would need to learn the class boundaries.  The research team investigates computational models for joint learning of visual concepts from images and textual descriptions of fine-grained categories, for example, discriminating between bird species.  The research activities have broader impact in three fields: computer vision, natural language processing, and machine learning. There is a huge need to develop algorithms to automatically understand the content of images and videos, with numerous potential applications in web searches, image and video archival and retrieval, surveillance applications, robot navigation and others. There are various applications for developing an intelligent system that can use narrative to define and recognize categories.<br/><br/>This project addresses two research questions:  First, given a visual corpus and a textual corpus about a specific domain, how to jointly and effectively learn visual concepts? Second, given these two modalities how to facilitate learning novel visual concepts using only pure textual descriptions of novel categories in the domain? The research team approaches the problem on three integrated fronts: Learning, Natural Language Processing (NLP), and Computer Vision. On the learning front, the project investigates and develops algorithms suitable for learning and predicting visual classifiers with side textual information. On the NLP front, the project aims to develop novel methods for learning global and local discriminative category-level attributes and their values from text, with feedback from human computation and visual signal. The project investigates supervised and unsupervised methods for detecting visual text, and learning methods for deep language understanding to build such rich domain models from the noisy visual text. On the Vision front, the project addresses the tasks of detection and classification with side textual information. The project investigates models for the shape and appearance of a general category that can specialize to different subordinates, in a way that allows interpreting information from text within a proper geometric context, and handle variability in viewpoints and articulation."
12,1409683,RI: Medium: Collaborative Research: Write A Classifier: Learning Fine-Grained Visual Classifiers from Text and Images,IIS,Robust Intelligence,6/15/14,7/15/20,Ahmed Elgammal,NJ,Rutgers University New Brunswick,Continuing Grant,Jie Yang,5/31/21,"$509,588.00 ",,elgammal@cs.rutgers.edu,33 Knightsbridge Road,Piscataway,NJ,88543925,8489320150,CSE,7495,"7495, 7924, 9251",$0.00 ,"This project develops the learning strategy using textual narrative and images makes the learning effective without a huge number of images that a typical visual learning algorithm would need to learn the class boundaries.  The research team investigates computational models for joint learning of visual concepts from images and textual descriptions of fine-grained categories, for example, discriminating between bird species.  The research activities have broader impact in three fields: computer vision, natural language processing, and machine learning. There is a huge need to develop algorithms to automatically understand the content of images and videos, with numerous potential applications in web searches, image and video archival and retrieval, surveillance applications, robot navigation and others. There are various applications for developing an intelligent system that can use narrative to define and recognize categories.<br/><br/>This project addresses two research questions:  First, given a visual corpus and a textual corpus about a specific domain, how to jointly and effectively learn visual concepts? Second, given these two modalities how to facilitate learning novel visual concepts using only pure textual descriptions of novel categories in the domain? The research team approaches the problem on three integrated fronts: Learning, Natural Language Processing (NLP), and Computer Vision. On the learning front, the project investigates and develops algorithms suitable for learning and predicting visual classifiers with side textual information. On the NLP front, the project aims to develop novel methods for learning global and local discriminative category-level attributes and their values from text, with feedback from human computation and visual signal. The project investigates supervised and unsupervised methods for detecting visual text, and learning methods for deep language understanding to build such rich domain models from the noisy visual text. On the Vision front, the project addresses the tasks of detection and classification with side textual information. The project investigates models for the shape and appearance of a general category that can specialize to different subordinates, in a way that allows interpreting information from text within a proper geometric context, and handle variability in viewpoints and articulation."
13,1450916,EAGER: How does deep learning improve speech recognition accuracy?,IIS,"ROBUST INTELLIGENCE, SOFTWARE & HARDWARE FOUNDATION",9/1/14,8/25/14,Steven Wegmann,CA,International Computer Science Institute,Standard Grant,Tatiana D. Korelsky,8/31/16,"$150,000.00 ",,swegmann@icsi.berkeley.edu,1947 CENTER ST STE 600,Berkeley,CA,947044115,5106662900,CSE,"7495, 7798","7495, 7916, 7945",$0.00 ,"Pervasive and accurate automatic speech recognition has the potential to transform society in many positive ways, not the least of which is providing better access to information for those who find it difficult or even impossible to interact with computers using a keyboard: e.g. the elderly, the physically disabled, or the vision impaired. Every day millions of people use applications based on this technology to solve problems that are most naturally accomplished by interacting with machines via voice. However, the most successful of these applications have always been rather limited in scope, because, although useful, speech recognition can be frustratingly unreliable. For example, human beings are easily able to understand one another despite loud background noise in a crowded room, severe distortion over a telephone channel, or wide variation in accents within their common language, but even much milder examples of these problems will completely derail a speech recognition system. This EArly Grant for Exploratory Research (EAGER) supports a project whose short term goal is to understand in a deep, quantitative way why methodology used in nearly all speech recognizers is so brittle. The long term goal is to leverage this understanding by developing less brittle methodology that will enable more accurate speech recognition with a wider scope of applicability. <br/><br/>This exploratory study will, first, discover why multilayer perceptrons (MLPs) can sometimes improve speech recognition accuracy, second, use these diagnostic insights to select better MLP architectures, and, third, release software so that others can leverage the developed methods. The use of MLPs has staged a remarkable resurgence in the last decade, in particular the ""deep"" architectures developed recently. In the field of speech recognition, there are two applications of MLPs that have significantly improved large vocabulary speech recognition accuracy. Each of these applications work within the standard speech recognition machinery, which uses hidden Markov models (HMMs) to model the acoustics, mel-frequency cepstral coefficients (MFCCs) for the models' inputs (features), and multivariate normals for the hidden states' marginal distributions. The first application makes a relatively minor adjustment to the standard machinery by augmenting the standard model inputs with new features learned from data using a MLP. The second application makes a more substantial change to the standard machinery by replacing the collection of hidden states' marginal distributions with a single MLP that models the marginal state posteriors. The research in the exploratory study will discover the basic mechanisms that the MLP-based features use to substantially improve HMM-based speech recognition accuracy. This research builds upon the previous work in the ICSI lab that used simulation and a novel sampling process to quantify the impacts that the major HMM assumptions have on speech recognition accuracy.<br/><br/>Other recent research on MLPs for speech recognition has either concentrated on implementation, i.e., how to actually improve speech recognition accuracy, or on theoretical asymptotic results. While this research is obviously important, it has proceeded largely by trial and error and, in particular, it has not addressed the interesting scientific questions surrounding how these applications of MLPs actually improve speech recognition accuracy. A deeper understanding of this latter question should, in the short term, lead to further improvements in speech recognition accuracy and, in the long term, enable the development of more suitable and successful models for speech recognition than the HMM, which would be a transformative advance in the field."
14,1449747,I-Corps:  Commercialization feasibility research of animation-projection ExpressionBots,IIP,I-Corps,7/1/14,7/14/14,Mohammad Mahoor,CO,University of Denver,Standard Grant,Rathindra DasGupta,12/31/15,"$50,000.00 ",,mmahoor@du.edu,2199 S. University Blvd.,Denver,CO,802104711,3038712000,ENG,8023,,$0.00 ,"Existing robotic platforms in the market have serious limitations for natural face-to-face communication. An alternative approach that could overcome many of these problems is to use state-of-the-art character animation technologies to project lifelike 3-D models onto a robotic mask that can display the 3-D models? natural speech and facial expressions. Currently, there are no such animation-based human robotic agents available in the market that can be acquired, nor can such capabilities be assembled from plug-and-play mechatronic components without significant design work and customization.  In this proposal the team has developed a robotic head that projects an animation on a translucent custom designed facial mask to mimic emotive facial gestures and visual speech.<br/><br/>The proposed product (ExpressionBot) is an extremely low-cost and portable robotic head using animation-projection and a custom designed facial mask that can display rich emotive facial gestures and visual speech generated by an avatar agent. The agent can speak, produce and mirror facial expressions synchronized with agent?s visual and prosodic speech. The ExpressionBot can be used as science tutor in classrooms, companionbots in nursing homes, museums tour guide, hotels receptionists, and tourist information kiosks. The ExpressionBot1 comprises a rich Physical Display Portal complete with software libraries for robotic character instantiation including facial expression generation and visual speech generation, human facial recognition, human speech recognition and natural language processing, and world environment recognition."
15,1433485,EAGER: Discovery of Segmental Sub-Word Structure in Speech,IIS,ROBUST INTELLIGENCE,3/1/14,3/4/14,Karen Livescu,IL,Toyota Technological Institute at Chicago,Standard Grant,Tatiana Korelsky,2/28/15,"$99,911.00 ",,klivescu@ttic.edu,6045 S Kenwood Ave,Chicago,IL,606372803,7738340409,CSE,7495,"7495, 7916",$0.00 ,"This EArly Concept Grant for Exploratory Research (EAGER) investigates new machine learning techniques for discovering sub-word units in speech for use in automatic speech recognition (ASR).  The representation of this EArly Concept Grant for Exploratory Research investigates new machine learning techniques for discovering sub-word units in speech for use in automatic speech recognition (ASR).  The representation of words in terms of sub-word units is rarely learned from data, despite significant disagreement among linguists as to the sub-word unit inventory.  This project represents exploratory work toward a larger goal of making all aspects of ASR learnable, using scientific insights while being discriminatively trained.<br/><br/>In contrast with prior work, speech segments are clustered into units using discriminatively learned segmental similarities, rather than via dynamic time warping or hidden Markov models.  Rather than pre-supposing phoneme-like units, multiple heterogeneous unit types<br/>are learned.  The project also leverages multi-modal (video, articulatory, and so on) data to improve unit discovery by sharing<br/>information across modalities.  In this exploratory work, the learned units are used in a discriminative model that rescores initial outputs from a standard phone-based recognizer, and the experiments focus on small-/medium-vocabulary recognition.<br/><br/>This project explores new ways of discovering the basic units of speech.  Beyond improvements to speech recognition, this project has<br/>the potential for broad impact on other research areas involving sequences with segmental sub-structure (such as text, video,<br/>biological data, and financial data) or involving clustering.  The results may also include new representations for the study of speech<br/>in linguistics and speech science.  From a societal perspective, in the long term making speech recognition more learnable will enable<br/>improved porting of the technology to under-served linguistic communities, which do not have the benefit of large data sets or other resources."
16,1423865,"Doctoral Dissertation Research: The interaction between tone, prosody and accent",BCS,Linguistics,8/1/14,7/8/14,Alan Yu,IL,University of Chicago,Standard Grant,Joan Maling,7/31/16,"$15,339.00 ",Kathryn Franich,aclyu@uchicago.edu,6054 South Drexel Avenue,Chicago,IL,606372612,7737028669,SBE,1311,"1311, SMET",$0.00 ,"The world's languages have traditionally been divided into one of three categories (stress-accent, pitch-accent, or tone) based on how pitch is utilized to distinguish words from one another. However, this division of languages is based largely on the impressionistic judgments rather than on detailed quantitative measurements of linguistic properties. Recent work has demonstrated that this typology is deeply inadequate for characterizing both the precise role(s) of pitch in language, and the relevant sources of diversity among languages. For example, although a language may utilize pitch as the primary acoustic cue to differentiate between words, it may also use stress, suggesting that these two phenomena are not mutually exclusive. The prevailing typology, though arguably flawed, has formed the basis for many important claims about the nature of language and linguistic complexity, which in turn have shaped how linguistic pedagogies are developed for use in foreign language teaching, and the way that prosodic systems are modeled for developing tools such as speech recognition software. Given the importance of such resources in today's global society, it is important for linguists to form a clear picture of the ways in which the acoustic properties of speech are used by speakers both in production and in perception. The current project aims to take a closer look at pitch, accent and prosodic structure in an effort to better understand how these structural elements interact, with the end goal of developing a more accurate linguistic typology, and a more adequate set of tools for language teaching and natural language processing.<br/><br/>Research for this project will take place in Southwestern Cameroon, focusing on the Bamileke languages, a subgroup of the Mbam-Nkam branch of the Benue-Congo group (Niger-Congo family). These languages have some of the most complex tone systems in the world, and much remains to be understood about their structure. Recent work suggests that word- and phrase-level accentual properties reminiscent of those found in prototypical stress languages are responsible for previously unexplained tonal phenomena in these languages, and that accent may play a much more important role in the organization of sounds than was formerly thought. This research will endeavor to explore in detail the way that acoustic measures such as pitch, vowel quality, amplitude, syllable and vowel duration interact in language production, and the way that these cues are weighted by listeners when perceiving speech. Data will be collected in the form of controlled production and perception experiments, as well as in the form of natural conversation between native speakers of the language. Research findings will contribute to our understanding of the ways in which pitch can be used alongside other acoustic cues at different levels of grammatical structure in a language with lexically-based tonal contrasts. Since many of the languages deemed 'critical' for learning by the US government also fit this description (Mandarin Chinese, Punjabi, Hausa, etc.), this work will have important implications for a wide variety of national language training initiatives."
17,1450349,EAGER: Feasibility of Using Speech as Biomarker for Concussions,IIS,"ROBUST INTELLIGENCE, Smart and Connected Health",9/1/14,4/6/15,Christian Poellabauer,IN,University of Notre Dame,Standard Grant,Jie Yang,8/31/17,"$316,000.00 ",Patrick Flynn,cpoellab@cse.nd.edu,940 Grace Hall,NOTRE DAME,IN,465565708,5746317432,CSE,"7495, 8018","7495, 7916, 8018, 9251",$0.00 ,"This project studies the feasibility of using speech as a novel biomarker for concussions, based on insights obtained from prior research that have shown links between impaired brain functioning and speech. Mild traumatic brain injuries (mTBI) such as concussions are arguably one of the most pressing concerns in sports today, with an estimated 2-4 million cases every year in youth sports alone. The potential short- and long-term impacts on the health and well-being of individuals with brain injuries are extensive. Untreated concussions can lead to diseases such as chronic traumatic encephalopathy, an elevated incidence of Alzheimer's disease, and dementia developing at a higher rate and a younger age. It is therefore imperative to accurately detect and appropriately treat mTBI, especially among adolescents, whose brains are still developing and more prone to long-term damage. <br/><br/>This research advances the understanding of the relationship between brain injuries and acoustic features in speech and lays the foundation for novel and accurate concussion screening tools. Initial results based on voice collections from more than 300 boxers (25 of which were concussed) have been promising and indicate that speech may have the potential to serve as a new, easy-to-capture biomarker for mTBI. Using mobile technologies such as smartphones and tablets, brief speech recordings from youth athletes are being captured. With the help of speech recognition techniques, each word is isolated and acoustic features are extracted for each phoneme in each word. Features that are being analyzed include temporal characteristics of speech (e.g., speech duration per word or phoneme or articulation rate) and frequency spectrum characteristics (e.g., pitch, formant frequencies, and mel-frequency cepstrum coefficients). Using state-of-the-art statistical analysis and machine learning techniques, these features are then analyzed for their potential as marker for mild forms of brain injuries."
18,1345495,SBIR Phase I: Beating back bullying: A virtual role-playing simulation that teaches perspective taking using artificially intelligent characters and crowdsourcing,IIP,SMALL BUSINESS PHASE I,1/1/14,12/31/13,Geoff Marietta,MA,Marietta Geoff E,Standard Grant,Glenn H. Larsen,6/30/14,"$150,000.00 ",,gem160@mail.harvard.edu,1 Oxford St.,Cambridge,MA,21382901,6177773364,ENG,5371,"110E, 5371, 8031, 8032, 8042",$0.00 ,"This SBIR Phase I project proposes to create a pro-social simulation to reduce aggressive behaviors in children, using a novel platform for simulated role-playing from crowdsourced data. Through a browser or mobile app, participants enter a 3D virtual school environment where they play the roles of a bullying victim and bystander, and interact with characters controlled by artificial intelligence (AI). By walking in another's shoes, users take the perspective of others and empathize with them -- capacities proven to foster improved relationships. The effectiveness of the simulation lies in allowing users to interact in the virtual world as they would in their real-world school. Virtual characters respond authentically by drawing from a massive database of recorded human dialogue, providing individualized responses tailored to unique user input. The project makes three important intellectual merit contributions. In furthering computer science, it integrates advanced computing techniques -- AI, natural language processing, and crowdsourcing -- in a novel way. Second, it makes theoretical contributions to social science by enhancing understanding of how social psychology principles function in the virtual world. Third, in the field of analytics, software tools will mine patterns of communication strategies, generating knowledge about the impact of specific phrases on relationships. <br/><br/>The broader/commercial impact of reducing bullying and improving relationships between children through unscripted, psychologically vivid, safely-controlled interactions in virtual environments is enormous. An extensive and deep body of research shows that bullying is pervasive in schools and has significant acute and chronic effects on learning and health outcomes. Despite the tremendous societal cost of bullying -- estimated around $25 billion -- and legal mandates to address the problem, a majority of schools do not have anti-bullying interventions in place. Existing interventions are expensive, difficult to implement, and focus on building awareness, rather than changing behaviors. This project will create a cost-effective and rapidly-scalable technology that mimics the complexities of human interaction to increase social perspective taking and empathy in children and adolescents. Dozens of middle and high schools have already agreed to implement the technology once developed. Improvement in student relationships and learning in these schools will be immediate. The technology developed has tremendous commercial potential for any training that relies on social and communication skills, such as corporate management, doctor-patient communication, autism therapy, and English language learning. Applications in these and other areas will generate dozens of new jobs for educators, engineers, developers, and data analysts."
19,1343940,SCH: EXP: Intelligent Clinical Decision Support with Probabilistic and Temporal EHR Modeling,IIS,Smart and Connected Health,1/1/14,11/12/14,Sriraam Natarajan,IN,Indiana University,Standard Grant,Sylvia Spengler,12/31/17,"$686,411.00 ","Kris Hauser, Sriraam Natarajan, Shaun Grannis",sriraam.natarajan@utdallas.edu,509 E 3RD ST,Bloomington,IN,474013654,3172783473,CSE,8018,"8018, 8061",$0.00 ,"Clinical decision support has the potential to reduce healthcare costs and improve patient outcomes, while shedding light into policy questions surrounding healthcare costs and practices in the US.  This project aims to develop intelligent clinical decision support techniques for recommending optimal action plans - including both diagnostic tests and medical interventions - for treating chronic disease, performing multi-step and adaptive treatments, and modifying long-term health habits. In an effort to integrate evidence-driven decision-making with established clinical practices, the research will develop disease-agnostic artificial intelligence techniques that combine data from large electronic health records (EHRs) with recommendations from human experts. A prototype decision support system will be tested on three clinical settings - cardiology, clinical depression, and emergency room readmission - using existing EHR datasets and consultation with domain experts from clinical partners. Outcomes-driven and cost-driven optimized decisions will be compared to current clinical practice. This exploratory research will provide the groundwork for follow-up projects in decision support information presentation, integration with clinical workflow and IT systems, and making the transition from retrospective studies to clinical trials.  Other broader impacts include workshops for healthcare applications of AI, and women and minority students will be recruited and mentored in graduate and undergraduate computer science research.<br/><br/>The technical approach of this research builds on state-of-the-art machine learning and artificial intelligence methods to automatically learn, simulate, and reason about patient-specific treatment plans.  Such methods must be simultaneously probabilistic and temporal.  Probabilistic techniques are needed to handle significant uncertainties in clinical diagnoses and outcomes, much like a human clinician would.  Temporal techniques are needed to consider sequences of future decisions over the course of treatment, rather than decisions at single time points.  More specifically, this project will consider the use of statistical relational learning (SRL) techniques to mine for probabilistic, temporal patterns in large electronic health records, and these patterns will be used in partially-observable Markov decision processes (POMDPs) that exhaustively search for optimal treatment sequences. Recent results indicate that SRL achieves superior performance to other machine learning methods in predicting cardiac arrest from demographic and lifestyle observations, and POMDP treatment plans outperform existing fee-for-service practices by reducing costs by 50% and improving outcomes by 40% on a clinical depression dataset.  By combining SRL and POMDPs, specifically, using SRL to learn a disease progression model used by the POMDP, this project aims to achieve further improvements in recommendation quality and computational scalability for complex treatments.  Furthermore, because EHRs may suffer from limited or missing data, clinical decision support tools should follow established practices and expert knowledge when necessary.  To do so, new workflows for integrating expert knowledge into SRL and POMDPs will be explored.  Evaluation will be performed on a variety of disease scenarios in conjunction with clinical partners at Marshfield Clinic, Centerstone, Wake Forest School of Medicine, and South Bend Memorial Hospital."
20,1421925,RI: Small: Neuroevolution of Brain-Inspired Computational Models Over Vast Timescales,IIS,Robust Intelligence,7/1/14,6/24/14,Kenneth Stanley,FL,The University of Central Florida Board of Trustees,Standard Grant,James Donlon,8/31/18,"$472,542.00 ",,kstanley@cs.ucf.edu,4000 CNTRL FLORIDA BLVD,Orlando,FL,328168005,4078230387,CSE,7495,"7495, 7923",$0.00 ,"For many years researchers inspired by the idea of natural selection have experimented with computer programs called evolutionary algorithms.  These algorithms simulate a kind of artificial breeding process in which a set of candidates generated by the computer are evaluated for their ability to perform a desired task.  The best performers are then allowed to reproduce with slight variation to form a new and hopefully improved generation.  In recent years evolutionary algorithms have exhibited the ability to evolve brain-like structures called artificial neural networks in an approach called neuroevolution.  These evolved networks perform tasks often critical to technological progress and artificial intelligence like controlling robots or recognizing images.    However, unlike evolution in nature, which yields dramatic changes over hundreds of thousands of generations, evolutionary algorithms have rarely been run for more than a few thousand.   This project for the first time is applying new evolutionary techniques that reward continual novelty and diversification to experiments evolving over hundreds of thousands of generations, on the scale of nature.  The driving hypothesis is that modern evolutionary algorithms run on this scale can yield robotic behaviors, agent morphologies, and decision-making capabilities significantly beyond the current state of the art.<br/><br/>To investigate long-term evolution in practice, artificial neural networks are being evolved in a variety of domains through new kinds of novelty-driven neuroevolution algorithms designed to avoid the convergence seen in typical evolutionary experiments.  Because this new class of algorithms tends to avoid convergence, the long-term dynamics and ultimate potential for discovery of such algorithms over vast time scales (i.e. hundreds of thousands of generations) is almost entirely unknown.  The idea of running neuroevolution at unprecedented timescales mirrors recent results in related areas like deep learning where massive computation has proven capable of fundamentally altering the kinds of problems that can be solved.  Because evolutionary runs over hundreds of thousands of generations yield enormous troves of evolutionary data, an important component of the project is the development visualization techniques for exploring and characterizing the results of such runs.   The project overall is producing a new set of tools, a new set of evolved capabilities for autonomous control and decision-making, and an increased understanding of the implications of big data and big computation for simulated evolution in computers."
21,1350598,CAREER: A Broad Synthesis of Artificial Intelligence and Social Choice,IIS,"Robust Intelligence, Algorithmic Foundations",2/15/14,1/31/18,Ariel Procaccia,PA,Carnegie-Mellon University,Continuing Grant,James Donlon,1/31/20,"$548,308.00 ",,arielpro@seas.harvard.edu,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,CSE,"7495, 7796","1045, 7495, 7796, 7932",$0.00 ,"Social choice theory is the field that studies the aggregation of individual preferences toward a collective choice. While the artificial intelligence (AI) community has so far played a dominant role in the study of the computational aspects of social choice, the interaction between core AI paradigms and social choice theory has been surprisingly limited. <br/><br/>This project is enhancing the interaction between the two fields through a synthesis of social choice with the following AI areas: (i) decision making under uncertainty, by building on models studied in AI to create new ways to model, analyze, and make decisions in environments where preferences are dynamically changing; (ii) multiagent systems, by studying settings where agents randomly vote over multiple states, and investigating the connection between normative properties and system performance; and finally (iii) machine learning, by employing insights about strategic behavior under structured preferences, developed in the social choice literature, in order to design regression learning algorithms that discourage strategic manipulation.<br/><br/>An overarching goal of this project is to demonstrate the potential of social choice theory to AI researchers, and ultimately to establish social choice theory as a standard paradigm in AI. Equally importantly, this project is expected to increase the scope of social choice theory. Broader impacts include a new web-based voting system, which has the potential to serve and educate hundreds of thousands of users; dissemination through a new book on computational social choice; and a workshop on computational social choice, which will help set a new agenda for the field."
22,1433765,Computer-Assisted Imaging for Structural Damage,CMMI,Structural and Architectural E,8/1/14,6/24/14,David Lattanzi,VA,George Mason University,Standard Grant,Yick Hsuan,7/31/18,"$264,942.00 ",,dlattanz@gmu.edu,4400 UNIVERSITY DR,FAIRFAX,VA,220304422,7039932295,ENG,1637,"036E, 039E, 040E, 1057, CVIS",$0.00 ,"After a disaster, such as an earthquake, inspectors are tasked with assessing the integrity of affected buildings and structures. Depending on the scale of the disaster, the number of required inspections can range into the thousands. There are both public safety and economic pressures to consider, and so rapid and accurate assessments of buildings and structures are vital. The goal of this research project is to develop a method using digital image analysis and artificial intelligence to assess the integrity of civil structures after a natural disaster. Fully realized, this technology will enable post-disaster inspectors to rapidly and accurately estimate structural damage using only a digital camera and portable computer. <br/><br/>Research in a comprehensive method of computer vision-based structural assessment will be pursued, one that is flexible enough to operate in highly varied and challenging field environments. To be truly comprehensive, such a methodology must be hierarchical, first recognizing the system-level context of structural components observed in an image and then leveraging that information to augment localized descriptions of damage extracted from segmentation routines. In this research program, the visual and instrumentation records of NEES experiments, available through the NEESHub, will be mined and analyzed to validate the algorithm. The research program will advance understanding as to how visually observable damage correlates to structural performance, and will provide insights into the suitability of hierarchical learning techniques for use in the field of computer vision-based structural assessment. More broadly, the project will result in an extensible method of visual, non-contact structural assessment that will provide a foundation for future structural monitoring system."
23,1451571,EAGER: CortiCore - Exploring the Use of An Automata Processor as an MISD Accelerator,CCF,"Robust Intelligence, Software & Hardware Foundation",8/15/14,6/14/16,Kevin Skadron,VA,University of Virginia Main Campus,Standard Grant,Almadena Chtchelkanova,12/31/17,"$179,995.00 ","Mircea Stan, Kevin Skadron",skadron@cs.virginia.edu,P.O.  BOX 400195,CHARLOTTESVILLE,VA,229044195,4349244270,CSE,"7495, 7798","7916, 7941, 7942, 7943, 7944, 8091, 8206",$0.00 ,"A novel computational accelerator architecture - the Automata Processor -has recently been introduced by Micron, that extends the computational paradigm of non-deterministic finite automata with important new capabilities. This architecture is particularly well suited for tasks involving  pattern matching.  Preliminary results suggest speedups as high as 1000X are possible, especially applications that entail combinatorial search, i.e., searching among many possible patterns to find the best match.  This project evaluates the suitability of this novel architecture for accelerating combinatorial search, using cortical learning algorithms (i.e., algorithms for machine learning that are inspired by observations and/or theories of how the brain works) as a case study. Until now, cortical learning algorithms have primarily been implemented only in software, which leads to solutions that are slow, large, expensive and power hungry, and thus limits their applicability. In particular, this project initially focuses on accelerating hierarchical temporal memory, a cortical learning algorithm that has recently been shown to be highly effective for analysis and integration of high-data-rate, multi-modal sensor and video data. It embodies many characteristics of a variety of combinatorial search tasks, combining and extending techniques from Bayesian networks, clustering, and decision trees.  This project is the first to evaluate the ability of the ""enhanced automata"" paradigm to accelerate cortical learning algorithms, and one of the first to explore the capabilities of the Automata Processor. In the process of evaluating the best way to accelerate cortical learning algorithms, this project will yield insights into the suitability of the Automata Processor for other artificial intelligence algorithms. It will also lead to development of new algorithms, software libraries, programming guidelines, and a new programming interface, to help speed the mapping of other applications to the Automata Processor and future accelerators. It will also yield techniques to improve the performance, flexibility, and energy efficiency of future accelerators, and new insights into the design and programming of heterogeneous systems with diverse accelerator hardware units.<br/><br/> This project has potential to lay the foundations for a novel acceleration framework that enables efficient solutions to a large set of intractable problems, with orders-of-magnitude improvements in performance and energy efficiency, and to guide development of future accelerators. As a consequence of these acceleration capabilities, portable, low-power artificial intelligence solutions could become ubiquitous.  This project creates tools that facilitate research and product development involving accelerator-based computing.  This project contributes to education and outreach through new course materials and assignments, hands-on research and training opportunities in cutting-edge acceleration paradigms, and new academic-industry collaborations."
24,1350008,CAREER: Teaching Machines to Design Self-Assembling Materials,DMR,CONDENSED MATTER & MAT THEORY,6/1/14,3/23/17,Andrew Ferguson,IL,University of Illinois at Urbana-Champaign,Continuing grant,Daryl Hess,8/31/18,"$360,000.00 ",,andrewferguson@uchicago.edu,1901 South First Street,Champaign,IL,618207406,2173332187,MPS,1765,"1045, 8400, 9216",$0.00 ,"TECHNICAL SUMMARY<br/><br/>This CAREER award supports theoretical and computational research and education in the understanding and design of self-assembling biomaterials. Self-assembly of structured aggregates by the spontaneous organization of their constituent building blocks is prevalent in the natural world, and is an attractive route to fabricate artificial materials with desirable properties that cannot be easily produced by other means. The design of building blocks programmed to self-assemble custom materials is a grand challenge in materials science.<br/><br/>In this work, the PI will integrate statistical mechanics theory with nonlinear machine learning algorithms to establish a new theoretical and computational approach to understand and program the self-assembly of nanostructured biomaterials. Using these tools, the PI will extract from molecular simulations the pathways and mechanisms by which building blocks self-assemble into structured aggregates. This methodology overcomes a key scientific challenge by integrating thermodynamics and kinetics in a unified framework that identifies both what stable aggregates form (thermodynamics) and how they assemble (kinetics and mechanisms). <br/><br/>The collective order parameters unveiled by this approach are good descriptors of the slow dynamical motions driving assembly, and present a natural parameterization for kinetically meaningful free energy landscapes that link building block properties to collective assembly behavior. By ""sculpting"" the landscape topography through rational manipulation of building block structure and chemistry the PI's group will program the assembly of desired structures that are thermodynamically stable and kinetically accessible (design).<br/><br/>The PI will apply a new approach to three technologically important self-assembling biomaterials: 1) ""patchy colloid"" polyhedral clusters for small molecule encapsulation, 2) ultra-short peptide mineralization templates for silica nanotubes for controlled drug release, heavy metal ion adsorption, and catalysis, and 3) antimicrobial peptide amphiphile nanostructures for antibiotic resistant bacteria. This work will establish new basic understanding and control of materials assembly, and accelerate development of new structural and functional biomaterials. <br/><br/>The integrated education and outreach plan incorporates the scientific outcomes into education and outreach, and supports graduate training, undergraduate research, and mentoring of underrepresented minority groups. The PI will create a new materials science course to equip the next generation workforce with computational tools, support undergraduate students in performing portions of the work, and promote the recruitment, retention, and success of students of color through mentorship of minority students and high school outreach.<br/><br/>NONTECHNICAL SUMMARY<br/><br/>This CAREER award supports a theoretical and computational research program to design microscopic building blocks with the ability to spontaneously self-organize into materials with desirable properties. This way of making materials is known as ""bottom-up self-assembly"", as opposed to more familiar ""top-down"" manufacturing. Imagine if it will be possible one day to design molecules with just the right shape and properties so that shaking them in a flask spontaneously self-assembled a solar cell! In this work, the PI will combine ideas from the fields of thermodynamics and machine learning (sometimes known as artificial intelligence) to establish a new tool to allow computers to learn both what structures can be formed by a particular building block, and how they assemble. The PI will then flip this problem to use our tool to help reverse-engineer building blocks to assemble custom materials. <br/><br/>The PI's group will apply these tools to the design of three useful biological materials: 1) micron-sized particles possessing directional sticky patches that assemble polyhedral clusters to hold and deliver small molecules, 2) short peptides that assemble networks to template the synthesis of silica nanotubes for drug delivery, cleanup of heavy metal pollutants, and catalysis of chemical reactions, and 3) longer peptides that assemble into nanometer sized rods that can kill antibiotic resistant bacteria such as the MRSA ""superbug"".<br/><br/>This award also supports an integrated research and education program in which the scientific results from this work will enrich and enhance undergraduate and graduate classes, and high school outreach activities. Undergraduate students will directly participate in the scientific research by working with the PI during the summer months. The PI will also design and teach a new class providing hands-on experience in the computational materials modeling, analysis, and design, and maintain his commitment to promote the recruitment and success of students of color through mentorship of undergraduate and graduate minority students."
25,1410514,A Machine Learning Framework for Acceleration of Materials Prediction,DMR,"CONDENSED MATTER & MAT THEORY, CI REUSE",9/1/14,6/3/16,Alexey Kolmogorov,NY,SUNY at Binghamton,Continuing grant,Daryl Hess,8/31/18,"$372,000.00 ",,kolmogorov@binghamton.edu,4400 VESTAL PKWY E,BINGHAMTON,NY,139026000,6077776136,MPS,"1765, 6892","7237, 7433, 7569, 8084, 9216, 9263",$0.00 ,"NON-TECHNICAL SUMMARY<br/><br/>Computational research has been playing an increasingly important role in the development of new materials. The central aim of this project is to create a theoretical framework and computational tools capable of speeding up the prediction of new synthesizable materials by orders of magnitude. The efficiency and the reliability of the method will be achieved by combining two bio-inspired algorithms. A learning ""neural network"" scheme will be implemented in the ""Module for Ab Initio Structure Evolution"" package which presently has the capability to perform global structure optimization with an evolutionary algorithm. The introduced computational approach will be applicable to a broad range of material classes and expected to accelerate the exploration of complex systems. A particular focus will be placed on the development of metal oxide catalysts and metal-based battery electrodes for energy-related applications.<br/><br/>Artificial neural networks are a powerful tool used in many research areas for dealing with classification, control, and interpolation problems in multi-dimensional spaces. The application of the method to solid state problems requires a formalism specifically designed for building and training neural networks to map the potential energy profiles of given atomic configurations. Namely, an automated algorithm should be able to parse an arbitrary atomic configuration into a suitable set of input parameters, train the neural network on a relevant dataset, and monitor the ability to produce accurate total energy and atomic forces during simulations. The PI's preliminary work and the recent research in the field have shown that, compared to widely used quantum mechanical and classical models, neural networks have the potential to provide a good balance between accuracy and efficiency.<br/><br/>The project will include multiple educational activities to foster high-school and undergraduate students' interest in science and mathematics. Students will have the opportunity to learn about computational methods and take part in the research under PI's supervision. A part of the outreach work will be done through Binghamton University's Evolutionary Studies program which brings together students and researchers from biological, physics, and computer sciences.<br/><br/>TECHNICAL SUMMARY<br/><br/>This award supports computational and theoretical research and education directed at advancing analytical and computational techniques for materials discovery. Computational input can be particularly valuable at the initial stages of materials development providing libraries of synthesizable candidates and possible synthesis conditions. The success of determining new stable materials depends on the careful global optimization of crystal structures and the accurate evaluation of their thermodynamic stability. Traditionally, the two challenges have been addressed separately. <br/><br/>This project will combine a neural network method with an evolutionary algorithm to automate and accelerate compound prediction. The neural network formalism will be generalized to describe a wide range of interatomic interactions with near ab initio accuracy while scaling linearly with the system size. Preliminary tests have shown the ability of such models to capture subtle many-body effects. The developed computational approach is expected to accelerate the exploration of systems known to exhibit particularly complex structures, such as metal oxide catalysts or metal-based battery electrodes. Neural network models will be trained, in particular, on already generated databases containing thousands of entries.  Analysis of the constructed neutral-network-based interpolators will advance the fundamental understanding of the bonding mechanisms for future rational materials design. <br/><br/>The compound prediction tool offering a combination of two bio-inspired algorithms will be released as an open-source code. One of the objectives is to create a shared online resource with full access to all built interatomic models and all ab initio data used for training the neural networks. Such a platform will ensure reuse of generated density functional theory data and reduce the redundancy of expensive ab initio calculations. Validation of the developed predictive approach will be carried out in close collaborations with Chemistry and Engineering experimental groups at Binghamton University.  The project will also include multiple educational activities to foster high-school and undergraduate students' interest in science and mathematics. Students will have the opportunity to learn about computational methods and take part in the research under PI's supervision. A part of the outreach work will be done through Binghamton University's Evolutionary Studies program which brings together students and researchers from biological, physics, and computer sciences."
26,1441009,Project Engage: Training Secondary Teachers to Deliver Computer Science and Engineering Instruction,DRL,"STEM - Computing Partnerships, Computing Ed for 21st Century",9/1/14,7/22/15,David Allen,TX,University of Texas at Austin,Standard Grant,Julio E. Lopez-Ferrao,8/31/17,"$457,287.00 ","Pauline Dow, Calvin Lin",allen@che.utexas.edu,"3925 W Braker Lane, Ste 3.340",Austin,TX,787595316,5124716424,EHR,"1996, 7382","7578, 8244, 9177, SMET",$0.00 ,"Developing computer science teachers who can support high school students in being successful in rigorous, academic computer science courses is a national need.  Project Engage: Training Secondary Teachers to Deliver Computer Science and Engineering Instruction will support the implementation of an emerging Computer Science Principles course called Thriving in Our Digital World (TODW) in forty-five urban, suburban, and rural schools, including public, private, magnet, and charter schools, in Texas.   The TODW course will be offered in a unique dual enrollment mode (concurrent high school and college credit).  This project will test two innovative professional development techniques for expanding the reach to more schools: flipped classroom techniques, in which teachers receive recorded video-based professional development, and then use face-to-face time for more hands-on activities; and an automated system which will use artificial intelligence technology to support negotiation of common grades for student work between high school teachers and college faculty.  This project's research agenda will address the following core challenge: In a field such as computer science where there is limited expertise, how can we most efficiently scale professional development so as many schoolteachers as possible can provide high quality instruction?  <br/><br/>The STEM-C (Science, Technology, Engineering, and Mathematics, including Computing) Partnerships program supports research-driven partnerships between STEM experts and K-12 school systems to bring about institutional change for better STEM education at the K-12 level. This STEM-C Partnerships' Computer Science Education Expansion project builds on prior funding of the UTeachEngineering: Training Secondary Teachers to Deliver Design-Based Engineering Instruction Partnerships through the National Science Foundation's Math and Science Partnership program. This project will produce the scientific foundation and the concrete artifacts needed to deliver a highly scalable curriculum for TODW. The artifacts will include a differentiated curriculum, scalable professional development, and scalable assessment tools and processes. The differentiated curriculum will include formative assessments which can be used by teachers to architect novel pathways through the curriculum for individual students or whole classes. Assessments will be created that can be delivered through a double-blind collaborative review (DBCR) rubric scoring tool, which will include machine learning algorithms that help detect consistent discrepancies between scores of individual (novice) computer science teachers and (expert) college-level computer science professors. The DBCR tool will use machine learning techniques such as basic text features (Bag of Words), syntactic and semantic modeling (n-grams and Latent Dirichlet Allocation), and cluster analysis against existing artifact corpora. Since the assessments of student artifacts are explicitly tied to rubrics, human evaluators may also use the DBCR tool to specify specific features that support line item rubric scores. The available features will vary from project to project but will leverage a variety of tools available in the DBCR user interface: (1) selectable categorical classifications, (2) the highlighting of relevant text features, and (3) annotations to mark document structures, such as cross references and supporting evidence.  Project evaluation will focus on both implementation fidelity of the TODW course and student-level outcomes in classes that implement TODW. If successful, the professional development could serve as a model for more scalable teacher professional development in many domains."
27,1453460,Preliminary Study to Demonstrate the Performance and Power Advantages of FPGAs over GPUs for Deep Learning in Computer Vision,CCF,ALGORITHMIC FOUNDATIONS,8/1/14,8/6/14,Michael Ferdman,NY,SUNY at Stony Brook,Standard Grant,Tracy J. Kimbrel,7/31/16,"$95,000.00 ",Peter Milder,mferdman@cs.stonybrook.edu,WEST 5510 FRK MEL LIB,Stony Brook,NY,117940001,6316329949,CSE,7796,7916,$0.00 ,"We stand on the verge of dramatic advances in deep learning algorithms, which will soon enable widespread adoption of computer-vision-based object recognition in scientific inquiry, commercial applications, and everyday life. However, practical large-scale applications in this area are currently limited by the computational capabilities of conventional computer systems. In recent years, technological improvement in computer processors (CPUs) has considerably slowed. This has led to an increase in interest in using Graphics Processing Units (GPUs) to accelerate deep learning computer vision algorithms. Although GPUs can perform these tasks faster than CPUs, they suffer from inflexibility and very high power cost. An alternative technology called the Field-Programmable Gate Array (FPGA) is very attractive for problems in this domain thanks to its flexibility and power efficiency. However, FPGAs have been underutilized in this area, in large part due to unfamiliarity and misconceptions. The goal of this project is to demonstrate the power and performance advantages of FPGAs over GPUs for deep-learning-based computer vision problems via hard experimental evidence. The PIs will disseminate their findings to the research community at large with the goal of encouraging the use of FPGAs in ground-breaking work tackling the grand challenges of deep learning and computer vision.<br/><br/><br/>This project consists of a three-stage research plan. First, the PIs will prepare and validate a state-of-the-art image detection application based on convolutional neural networks. This will utilize the popular Caffe library, which allows convolutional networks to be evaluated on CPU and GPU. Second, the PIs will perform a detailed characterization and profiling of the performance of this application on GPU, seeking to understand the performance characteristics and their underlying causes. Third, the PIs will implement portions of the algorithm on an FPGA, and perform an in-depth analysis to find and explain the advantages and disadvantages offered by the platform. The PIs anticipate demonstrating that the slowest portion of the algorithm on the GPU will achieve significant speedup on the FPGA, arising from the efficient support of irregular fine-grain parallelism. Meanwhile, the fastest portion of the algorithm on the GPU is anticipated to run with comparable performance on the FPGA, but at dramatically lower power consumption.<br/><br/>This project will integrate research with graduate and undergraduate education. PhD students will be exposed to GPU optimization and application-specific high-performance FPGA design. Masters and undergraduate students will gain valuable skills assisting the project through the Masters Advanced Project in Computer Science and the Undergraduate Senior Design Project in Electrical and Computer Engineering. The results of the study will be published at prominent venues to ensure maximum exposure for the relevant research communities."
28,1446996,EAGER: Collaborative Research: Scaling Up Discriminative Learning for Natural Language Understanding and Translation,IIS,Robust Intelligence,8/15/14,8/13/14,Daniel Gildea,NY,University of Rochester,Standard Grant,Tatiana Korelsky,7/31/16,"$129,053.00 ",,gildea@cs.rochester.edu,"518 HYLAN, RC BOX 270140",Rochester,NY,146270140,5852754031,CSE,7495,"7495, 7916",$0.00 ,"This EArly Grant for Exploratory Research aims to improve automatic understanding of natural language by machines, and automatic translation between languages such as Chinese and English. In the realm of understanding, the project develops methods for syntactically and semantically analyzing, or parsing, sentences. Improved parsing can help in accessing the enormous amount of information available in unstructured text on the web and in databases of newspapers and scanned books. Improved translation between languages increases opportunities for trade as well as for dissemination of information generally between nations and cultures. Machine translation is widely used today despite its generally poor quality, and any improvement in quality will improve access to information for millions of people.  This project aims to exploit the power of machine learning algorithms that are designed to discriminate between correct and incorrect outputs by numerically optimizing mathematical functions that are defined in terms of the data available for training.  Discriminative structured prediction algorithms have witnessed great success in the field of natural language processing (NLP) over the past decade, generally surpassing their generative counterparts. However, there remain two major problems which prevent discriminative methods from scaling to very large datasets: first, they typically assume exact search (over a prohibitively large search space), which is rarely possible in practice for problems such as parsing and translation. Secondly, they normally assume the data is completely annotated, whereas many naturally occurring datasets are only partially annotated: for example a parallel text in machine translation includes the source and target sentence pairs but not the derivation between them. As a result of these two problems, the current methods are not taking full advantage of the enormous and ever increasing amount of text data available to us.<br/><br/>This EArly Grant ofr Exploratory Research (EAGER) aims to: <br/>- Develop a linear-time structured learning framework specifically tailored for inexact search, which hopefully retains theoretical properties of structured learning (e.g. convergence) under exact search.  <br/>- Extend this framework to handle latent variables, such as derivations in machine translation, syntactic structures in semantic parsing, and semantic representations in question answering.  <br/>If the exploratory extension to latent variable frameworks is sucessful, it will enable longer-term research to: <br/>- Apply these efficient learning algorithms to discriminative training of machine translation systems over the entire training dataset rather than only on a small development set.  <br/>- Apply these efficient learning algorithms to discriminative training for syntactic and semantic parsing, with the goal of scaling up semantic parsing to enable web-scale knowledge extraction."
29,1449278,EAGER: Collaborative Research: Scaling Up Discriminative Learning for Natural Language Understanding and Translation,IIS,ROBUST INTELLIGENCE,8/15/14,8/13/14,Liang Huang,NY,CUNY Queens College,Standard Grant,Tatiana D. Korelsky,9/30/16,"$135,372.00 ",,huanlian@oregonstate.edu,65 30 Kissena Blvd,Flushing,NY,113671575,7189975400,CSE,7495,"7495, 7916",$0.00 ,"This EArly Grant for Exploratory Research aims to improve automatic understanding of natural language by machines, and automatic translation between languages such as Chinese and English. In the realm of understanding, the project develops methods for syntactically and semantically analyzing, or parsing, sentences. Improved parsing can help in accessing the enormous amount of information available in unstructured text on the web and in databases of newspapers and scanned books. Improved translation between languages increases opportunities for trade as well as for dissemination of information generally between nations and cultures. Machine translation is widely used today despite its generally poor quality, and any improvement in quality will improve access to information for millions of people. This project aims to exploit the power of machine learning algorithms that are designed to discriminate between correct and incorrect outputs by numerically optimizing mathematical functions that are defined in terms of the data available for training. Discriminative structured prediction algorithms have witnessed great success in the field of natural language processing (NLP) over the past decade, generally surpassing their generative counterparts. However, there remain two major problems which prevent discriminative methods from scaling to very large datasets: first, they typically assume exact search (over a prohibitively large search space), which is rarely possible in practice for problems such as parsing and translation. Secondly, they normally assume the data is completely annotated, whereas many naturally occurring datasets are only partially annotated: for example a parallel text in machine translation includes the source and target sentence pairs but not the derivation between them. As a result of these two problems, the current methods are not taking full advantage of the enormous and ever increasing amount of text data available to us.<br/><br/>This EArly Grant ofr Exploratory Research (EAGER) aims to: <br/>- Develop a linear-time structured learning framework specifically tailored for inexact search, which hopefully retains theoretical properties of structured learning (e.g. convergence) under exact search. <br/>- Extend this framework to handle latent variables, such as derivations in machine translation, syntactic structures in semantic parsing, and semantic representations in question answering. <br/>If the exploratory extension to latent variable frameworks is sucessful, it will enable longer-term research to: <br/>- Apply these efficient learning algorithms to discriminative training of machine translation systems over the entire training dataset rather than only on a small development set. <br/>- Apply these efficient learning algorithms to discriminative training for syntactic and semantic parsing, with the goal of scaling up semantic parsing to enable web-scale knowledge extraction."
30,1451202,BRAIN EAGER: Cell-type-specific Optogenetics in Wild-type Animals,IOS,"Cross-BIO Activities, Organization",9/1/14,8/18/14,Ian Wickersham,MA,Massachusetts Institute of Technology,Standard Grant,Evan Balaban,8/31/17,"$300,000.00 ","Robert Desimone, Kay Tye, Li-Huei Tsai",wickersham@MIT.EDU,77 MASSACHUSETTS AVE,Cambridge,MA,21394301,6172531000,BIO,"7275, 7712","7916, 8091, 9178",$0.00 ,"This project consists of engineering a system for producing selective expression of light-inducible molecules in targeted neuron population in non-genetically modified animals of any species.  The result will be a set of reagents that will be made freely available to the scientific community through nonprofit repositories and service centers.  This new set of tools will enable the study of neural circuitry with greater resolution, power, and throughput than is currently possible, allowing major advances to be made in understanding the organization of the complex neural systems underlying perception, cognition, and behavior.  This increased understanding could also result in improved artificial intelligence and machine learning.  Finally, the future direct application of the technology in human patients holds promise for potentially treating conditions such as Parkinson's disease and epilepsy, by allowing the selective activation or inactivation of distinct components of the compromised neural circuitry that is associated with these disorders.<br/><br/>Over the last decade, sophisticated genetic tools have been developed that allow control and monitoring of neuron electrical activity using light alone.  ""Optogenetics"", as this area of technology has become known, is only useful if optogenetic molecules can be specifically expressed in functionally meaningful groups of neurons instead of broadly in all the diverse neuron types that are present in any brain region.  This requirement has confined their use almost entirely to genetically modified (transgenic) mice and rats.  The approach of using transgenic animals has three major disadvantages.  First, the production and maintenance of transgenic rodents is very expensive.  Second, even within transgenic rodents, it allows the optogenetic study and manipulation of only one or two cell types at a time, preventing powerful combinatorial experiments in which different neuron types are independently controlled within the same tissue.  These combinatorial experiments will be critical for deciphering the complex interactions between cell types. Third, it restricts the experiments to rodents, preventing studies in other important taxa including primates, in which optogenetic experimentation during complex cognitive tasks would almost certainly provide major insights into the neural circuitry underlying cognition. This project aims to create engineered binding proteins that recognize selected endogenous proteins that will then act as scaffolds for assembly of transcription factors that will activate gene expression in specific neurons."
31,1451017,BRAIN EAGER: Integrative Cross-Modal and Cross-Species Brain Models: Motivation and Reward,IIS,"Cross-BIO Activities, CRCNS-Computation Neuroscience",9/1/14,8/18/14,Katherine Heller,NC,Duke University,Standard Grant,Jie Yang,8/31/17,"$300,000.00 ",,kheller@gmail.com,"2200 W. Main St, Suite 710",Durham,NC,277054010,9196843030,CSE,"7275, 7327","7916, 8089, 8091",$0.00 ,"Motivation translates goals into action, and significantly impacts cognition along several dimensions. The motivation for reward biases attention, perception, and memory, and enhances learning, with effects evidenced behaviorally as well as in specific brain regions.  However, given such broad effects of reward motivation, the question remains: how does reward motivation propagate throughout the brain and how does it dynamically change the greater neural circuitry to prime us to behave appropriately? In order to answer these questions we develop statistical models for the effect of motivation on neural circuitry, which combine data recorded using different instruments across a variety of species. A fuller understanding of the neural effects of motivation would elucidate how it impacts important cognitive processes, yielding insights that are useful for better performance in various arenas, from education to therapy. <br/><br/>In order to gain a more complete understanding of the neural network dynamics underlying the behavioral and cognitive effects of motivation, it is necessary to integrate research in human subjects, and in animal models. While extensive and crucial research has been carried out on reward motivation separately in humans and animal models, there is a clear need for improved translation across species. An overarching analytical framework for translation is extremely important, due to the complexity of the problems being addressed, and to leverage the strengths offered by each species and available technology. The aim of this work is the development of dynamic, hierarchical Bayesian models to discover functional neural networks that can translate across species and data collection modalities. Bayesian models of human behavior, and Bayesian machine learning inspired methods for neural network modeling, have been extremely successful, making Bayesian methods fertile ground for explorations into translational neural network discovery."
32,1420785,Doctoral Dissertation: Investigating the role of grammatical representation in language learnability,BCS,Linguistics,7/15/14,7/8/14,Edward Gibson,MA,Massachusetts Institute of Technology,Standard Grant,William Badecker,12/31/15,"$11,710.00 ",Leon Bergen,egibson@mit.edu,77 MASSACHUSETTS AVE,Cambridge,MA,21394301,6172531000,SBE,1311,"1311, 9179, SMET",$0.00 ,"Technologies which process natural language have become ubiquitous in the last decade. Web search engines, for example, process billions of pages of text, in order to determine which of those pages best match a user's search query. Many interfaces for interacting with computers -- for example, Apple's Siri personal assistant -- take voice-issued commands from their users, and must process these commands in order to follow the users' instructions. Finally, machine translation technologies have become available for many of the world's most common languages, allowing users to automatically translate text that they find in foreign books or websites. These technologies mostly rely on simple models of language, known as n-gram models or context-free grammars, which were developed in the 1950's and 1960's, and refined in later decades. These simple models of language have many advantages, most notably that they can be used to process large amounts of data very quickly. Because of their simplicity, however, these models are not able to capture many aspects of meaning in natural language. This has resulted in limitations for the technologies discussed above; virtual personal assistants are only able to process very simple types of instructions, and machine translations is still far from being as accurate as human translation. In the current project, Leon Bergen and Dr. Edward Gibson will be investigating more sophisticated kinds of language models, with the goal of increasing the ability of computers to understand language.<br/><br/>Under the direction of Dr. Gibson, Mr. Berger will be studying language models known as mildly context-sensitive grammars. These grammars are able to express certain types of linguistic knowledge that humans have, but which cannot be expressed using simpler types of grammatical formalisms. For example, native speakers of English know that a declarative sentence like ""Mary kicked the ball"" is closely related in meaning to the question ""What did Mary kick?"" Although this fact seems obvious, it is difficult (or impossible) to express using simple types of grammars. However, mildly context-sensitive grammars can be used to express this knowledge in a very natural way. Mr. Bergen and Dr. Gibson will be studying whether mildly context-sensitive grammars can be automatically learned from examples of grammatical sentences. To do this, they will be using techniques from machine learning, a branch of computer science and statistics that develops algorithms that can automatically learn from data. The researchers will integrate these learning algorithms with their grammatical formalism, and will test whether their method learns an accurate grammar. The accuracy of the grammar will be evaluated using a corpus -- a collection of sentences -- in which every sentence has been manually annotated with its correct grammatical structure. If accurate mildly context-sensitive grammars can be learned in this manner, then this provides a potential method for improving the natural language processing technologies which were discussed above. In particular, because this method does not require an expert to write down the complete grammar for a language, it has the potential to be deployed without tremendous engineering effort, and may be deployed easily in foreign languages."
33,1443085,"C1F21 DIBBS: Porting Practical Natural Language Processing (NLP) and Machine Learning (ML) Semantics from Biomedicine to the Earth, Ice and Life Sciences",OAC,"ADVANCES IN BIO INFORMATICS, Polar Cyberinfrastructure, Data Cyberinfrastructure, EarthCube",11/1/14,7/17/15,Christopher Jenkins,CO,University of Colorado at Boulder,Standard Grant,Amy Walton,10/31/18,"$1,497,785.00 ","James Martin, Martha Palmer, Ruth Duerr",chris.jenkins@colorado.edu,"3100 Marine Street, Room 481",Boulder,CO,803031058,3034926221,CSE,"1165, 5407, 7726, 8074","7433, 8048",$0.00 ,"Semantics is the study of word-based information. The sciences are filled with word-based descriptive data: field observations, materials and habitat identifications, parameter names and units, events and processes. Semantics are also important in medicine, where the human body and illnesses have to be described.  To enhance interoperability among these word-based (semantic) systems, and to more readily explore the rapidly growing quantities of semantic data, there has been a movement towards organizing word-based data in ways that allow machine-assisted, automated analysis.  Biomedicine has made great progress in organizing and using semantic information because of substantial funding investments. This project builds upon extensive investments in the biomedical field, providing an opportunity to rapidly develop the organization of semantic concepts for other domain sciences. <br/><br/>A toolkit developed by the Center for Computational Language and Education Research (CLEAR TK) will be used to build semantic resources (taxonomies, ontologies, and semantic networks) for three science domains (geology, cryology, and biology).  CLEAR TK is a state-of-the-art natural language processing (NLP) and machine learning (ML) system that also has essential tools for machine-assisted annotation, validation, document tagging, and event extraction.  The CLEAR TK system has been used operationally for biomedical semantic applications, including in high-profile hospitals.  In this project, developments are focused upon the science fields of geology, ice and snow, and biology.  In these fields, accurate extraction of semantic information from the word-based data is required so users can quickly find the data they really need. This project provides a valuable opportunity to expand and evaluate semantic capabilities in conjunction with several scientific domain experts."
34,1444949,PFI:AIR - TT:  A Clinical Predictive Model Based Smart Decision Support System for Congestive Obstructive Pulmonary Disease (COPD) related Re-hospitalization,IIP,Accelerating Innovation Rsrch,9/15/14,9/3/14,Ankur Agarwal,FL,Florida Atlantic University,Standard Grant,Jesus Soriano Molla,8/31/17,"$199,594.00 ","Andrew Duffell, Ravi Behara, Xingquan Zhu",ankur@cse.fau.edu,777 GLADES RD,BOCA RATON,FL,334316424,5612970777,ENG,8019,8019,$0.00 ,"This PFI: AIR Technology Translation project focuses on translating smart predictive modeling technologies to develop a Clinical Decision Support System (CDSS) to fill the need for reducing re-hospitalization for patients with Congestive Obstructive Pulmonary Disease (COPD). COPD affects almost 24 million people in the U.S. and is the 3rd leading cause of death. The CDSS is important because it provides better quality of care for patients while providing significant costs savings related to hospital readmissions. The project will result in a prototype of a smart CDSS, which will enable practitioners to better understand if a COPD patient is expected to be a candidate for hospital readmission within 30 days of discharge. This cloud-based CDSS will have the following unique features: ability to utilize patient information available either in electronic or plain text format, novel algorithms derived from big data analytics, natural language processing and machine learning and ability to provide alerts to healthcare professionals about high readmission risk patients. These features provide the following advantages: improving the efficiency of the care process through system accessibility, effectively utilizing and integrating relevant patient information from various sources, reducing total cost of care through early and preventative intervention, and improving the quality of care and quality of life of the patients.  Further, this CDSS will help reduce the variability in care so that hospitals with historically lower performance can benefit from best practices.<br/><br/>This project addresses the following technology gaps as it translates from research discovery toward commercial application in the area of clinical decision support systems. The predominant technical gaps are a lack of fusion of structured and unstructured data for the development of predictive models, a limited use of natural language processing, and a significant lack of integration of local and global clinical information. This project provides a comprehensive platform to address these gaps by integration the structured local and global clinical data available in either CCD, CCR or CCDA format with the unstructured clinical data such as physician notes and laboratory reports. Further the system utilizes UIMA based cTAKES natural language processing sub-system that is developed for COPD to analyze this unstructured data.  The predictive capabilities of CDSS are based on multiple leading-edge technologies including data mining, machine learning, natural language processing and data visualization techniques. In addition, graduate students involved in this project will have opportunities to learn the interdisciplinary domain of medical information systems and their real-world applications. In addition, these students will be mentored in technology transfer and commercialization activities. <br/><br/>Larkin Community Hospital (largest D.O. program in the U.S.) and Humana (the largest health plan in Florida) will provide clinical data and expertise to the project. This project could be effectively deployed at hospitals, healthplans, accountable care organizations and managed care organizations for avoiding hospital readmissions related to COPD thereby, providing significant savings related to hospital readmissions while improving quality of care."
35,1451430,EAGER: Discrete Algorithms in NLP,IIS,"Robust Intelligence, Algorithmic Foundations",9/1/14,8/20/14,Hal Daume,MD,University of Maryland College Park,Standard Grant,Tatiana Korelsky,8/31/15,"$75,000.00 ",Samir Khuller,hal@umiacs.umd.edu,3112 LEE BLDG 7809 Regents Drive,COLLEGE PARK,MD,207425141,3014056269,CSE,"7495, 7796","7495, 7796, 7916",$0.00 ,"Algorithms that can understand human language must be able to recognize the underlying structure (e.g., subject-verb-object) of that language. Computational approaches developed in the natural language processing community typically have build ad hoc, one-off algorithms for solving the hard, combinatorial optimization problems that arise in such tasks. Most large-scale systems are built using complex combinations of heuristics applied to try to make approximate search techniques better. Concurrently, the algorithms community has developed scalable exact algorithms and approximation algorithms for solving many of these hard combinatorial optimization problems. This EArly Grant for Exploratory Research investigates the connection between these two extremes: the language processing community with the hard problems they need solved, and the algorithms community with the provably correct algorithms for solving such hard problems. The biggest technical challenge this exploration addresses is how to couple the statistical learning algorithms necessary to build effective language applications with the types of abstractions that make efficient algorithms possible. In particular, this project explores the application of ""inverse optimization"" to machine learning. For example, if one has access to an efficient algorithm for solving a particular discrete optimization problem, how can one learn parameters that make that particular algorithm as high accuracy as possible? Success in this project will give rise to theoretically principled, efficient algorithms for learning to solve complex linguistic tasks, which can transform to downstream applications like machine translation, automatic question answering and information retrieval.<br/><br/>This project's main technical innovation is the coupling of ""inverse optimization"" problems with online learning techniques. For instance, suppose that the end goal is to find some particular structure. The search for this structure can often be cast as a particular form of dynamic programming problem, which in turn often becomes a shortest path problem in a hypergraph. The machine learning challenge then is to learn a model under which the solution to this shortest path search is actually the desired structure. From an algorithmic perspective, this requires finding a set of inputs under which a given structure is optimal: inverse optimization. However, it is not enough for a given structure to be optimal: it must also beat all other (non-optimal) structures by some given margin. This project will develop a combination of online learning algorithms and inverse optimization formulations that enable such advances."
36,1345432,SBIR Phase I: A Semantic Data-Driven Human Capital Recommendation System,IIP,SMALL BUSINESS PHASE I,1/1/14,5/27/14,Monica Messer,NE,Green Schingle LLC,Standard Grant,Glenn H. Larsen,12/31/14,"$179,999.00 ",,monica.messer@schingle.com,11205 Wright Circle #140,Omaha,NE,681444719,4026894501,ENG,5371,"5371, 8032, 8033, 8039, 9150",$0.00 ,"This SBIR Phase I project proposes to apply the combination of semantic matchmaking and a knowledge acquisition loop to human capital selection processes to generate a rank-ordered short-list of qualified candidates. The problem with available job matching methods is they rely primarily on keyword searches and lack comprehensive learning that includes feedback on quality of recommendations. The opportunity is to build technology to create a candidate recommendation system incorporating feedback on relevance of its performance. The research objective is to determine if integrating semantic matching and machine learning provides a feasible basis for creating a commercially viable predictive candidate recommendation system. The natural language processing techniques, and the ontology knowledge base developed during this research, have the potential to advance knowledge and understanding of using natural text to express the availability of skills and experience. The anticipated result is the creation of a human capital ontology structure and rules that learn from feedback loops of rank-order successes to improve semantic matching. While this project assesses the significance of ontology and machine learning to improve the quality of candidate recommendations, the results also enhance technological understanding of combining ontology and machine learning within broader work-force science research.<br/><br/>The broader/commercial impact of this product is to significantly increase the success of selected applicants and improve employers' ability to manage human capital selection efficiently and at less cost by presenting them with a more accurate short-list. Identifying the most qualified candidates should increase the return on investment in over $6 billion spent annually by businesses in the United States for online recruiting efforts ultimately resulting in greater organizational competence and competitive advantage. By implementing technology that automates the process of rank-ordering job candidates based on skills, qualifications, and machine-learning from historical selections, businesses will be able to reduce costs associated with identifying quality job candidates while dealing with the growing challenge of talent shortages, especially in Science, Technology, Engineering, and Math (STEM) professions. The feedback loop provides for knowledge acquisition to be integrated; thus improving results over time and reducing effort needed to identify qualified candidates. Such insights into talent data will help businesses make more informed recruiting, hiring, and other resource management decisions. This innovation has the potential to extend to talent information in disparate sources offering Big Data analytics solutions for many talent management functions and providing unprecedented insights for human capital management."
37,1420667,RI: Small: Improving Crowd-Sourced Annotation by Autonomous Intelligent Agents,IIS,Robust Intelligence,8/1/14,7/22/14,Daniel Weld,WA,University of Washington,Standard Grant,Weng-keen Wong,7/31/18,"$460,000.00 ",,weld@cs.washington.edu,4333 Brooklyn Ave NE,Seattle,WA,981950001,2065434043,CSE,7495,"7495, 7923",$0.00 ,"Supervised machine learning methods are arguably the greatest success story for Artificial Intellitence with a deep underlying theory and applications ranging from medical diagnosis and scientific data analysis to ecommerce recommender systems and credit-card fraud detection. Unfortunately, all these methods require labeled training data, which has been annotated by a human --- a time consuming and extremely expensive process. This project will use automated decision theory to control the annotation process, saving significant amounts of human labor and extending the practical use of machine learning to a much broader array of societal problems.  <br/><br/>Specifically, the methods address the case where labeled data is crowd-sourced by a large number of human annotators whose skill and error rates are variable. The project develops new control algorithms that let the learner efficiently ask specific workers to label (or redundantly re-label) specific examples.  To test the practicality of their methods, the PIs build and conduct studies with the Information Omnivore, a fully autonomous agent that optimizes the annotation of natural language processing (NLP) training data. By continuously posing questions to paid workers and volunteer citizen-scientists, the Omnivore 1) will learn which problems are hard and which are easy, 2) will learn about the skills of the various workers, 3) and will decide questions to ask which workers in order to maximize the accuracy of the learned model given scare human help. Besides contributing to the science of automated control, the Omnivore will generate labeled training data for two important NLP problems: named entity linking (NEL) and information extraction (IE), greatly helping the community of NLP researchers. Furthermore, the researchers plan a number of outreach efforts, including curriculum development, participation in the K12 Paws on Science program at the Pacific Science Center and interaction with the diverse students comprising the Washington STate Academic RedShirt (STARS) in Engineering program. The specific algorithms proposed by the PIs are notable in several respects. Their decision-theoretic optimization framework operationalizes intuitions like (1) one should assign more or better workers to hard problems and (2) one should redirect effort away from easy questions or from tasks that are too hard to solve. Automating this reasoning is hard because problem difficulty and worker skill are latent variables and thus the agent must confront an exploration / exploitation tradeoff as it balances actions that enable it to learn about the capabilities of workers with the ultimate goal of producing quality annotations. The PIs consider two cases: Task Allocation for Annotation Accuracy tries to maximize the overall annotation accuracy of a fixed size data set through batch assignment of workers to tasks. Re-Active Learning seeks instead to directly construct an accurate ML classifier through a balanced mix of annotator requests to re-label old or label new examples. In both cases they propose a model based on decision-theoretic methods (e.g., partially-observable Markov decision processes (POMDPs) and multi-armed bandits). The PIs propose to integrate their methods in the Information Omnivore, a long-lived software agent that integrates planning and execution, acts in the real world, and learns a model of its environment. The Omnivore will allow large-scale latitudinal studies of their algorithms, and as a byproduct will generate NLP training data that will greatly assist a large community of other researchers."
38,1422910,"AF: Small: Foundations for Learning in the Age of Big Data---New Frameworks and Algorithms for Interactive, Distributed, and Multi-Task Machine Learning",CCF,"Robust Intelligence, Algorithmic Foundations",8/1/14,7/24/14,Maria-Florina Balcan,PA,Carnegie-Mellon University,Standard Grant,Tracy Kimbrel,7/31/17,"$400,000.00 ",,ninamf@cs.cmu.edu,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,CSE,"7495, 7796","7495, 7923, 7926",$0.00 ,"Machine learning is a broad discipline with important application domains including computer vision, robotics, sustainability, and bio-surveillance. Its past successful evolution was heavily influenced by mathematical foundations developed for core problems of generalizing from labeled data. However, with the variety of applications of machine learning across science, engineering, and computing in the age of Big Data, re-examining the underlying foundations of the field has become imperative. This project aims to substantially advance the field of machine learning by developing foundations and algorithms for a number of important modern learning paradigms. These include interactive learning, where the algorithm and the domain expert engage in a two-way dialogue to facilitate more accurate learning from less data compared to the classic approach of passively observing labeled data; distributed learning, where a large dataset is distributed across multiple servers and the challenge lies in learning with limited communication; and multi-task learning, where the goal is to solve multiple related learning problems from less data by taking advantage of relationship among the learning tasks. The project also aims to develop new connections between machine learning and property testing, a flourishing area of theoretical computer science. In addition to solving fundamental questions in each of these directions, the project will highlight and leverage synergies between these topics.<br/><br/>More specifically, the key research directions of this project are: (1) Developing mathematical foundations for interactive learning by analyzing new forms of interactions between the learning algorithm and the domain expert that could lead to fast and efficient learning of difficult tasks by wisely exploiting the capabilities of domain experts. (2) Developing new algorithms for distributed learning, an important modern scenario where data is distributed among several locations. This project will develop protocols that trade off the various types of resources involved in such settings (computation, communication, and domain expertise). (3) Developing new algorithms with provable guarantees for learning multiple related tasks from limited amounts of labeled data and massive amounts of unlabeled data by wisely exploiting explicitly known or latent relationships between the given tasks. (4) Developing mathematical foundations for property testing, where the question is to quickly determine whether there exists a low-error rule of a desired form by using significantly less data than needed to actually find the rule itself. This project will specifically focus on active and distributed scenarios, with the goal of using testing as a way to improve learning efficiency itself.<br/><br/>Broader impacts include mentoring women in CS and actively organizing workshops and seminars in the interdisciplinary area."
39,1405688,II-EN: Hadoop NextGen Infrastructure for Heterogeneous Approaches to Data-Intensive Computing,CNS,CCRI-CISE Cmnty Rsrch Infrstrc,8/1/14,7/25/14,Jimmy Lin,MD,University of Maryland College Park,Standard Grant,Aidong Zhang,7/31/17,"$499,852.00 ",,jimmylin@umd.edu,3112 LEE BLDG 7809 Regents Drive,COLLEGE PARK,MD,207425141,3014056269,CSE,7359,7359,$0.00 ,"The ability for organizations to process enormous quantities of data and to extract insights from those data has revolutionized commerce and science. This phenomenon, known as ""big data"", is shaping the very fabric of our society. Our insatiable appetite for more data, and knowledge from the data, requires significant computational infrastructure for storage and analytical capabilities.  Continued investments in infrastructure for academic researchers are vital from two perspectives: From the research perspective, the university's ability to help advance the state of the art in big data technologies is dependent on access to the right computational resources. From the educational perspective, the university's mission to train the next generation of scientists and engineers cannot be successfully accomplished without big data infrastructure that is becoming essential to their careers.  The goal of this project is to provide computational resources to researchers at the University of Maryland to continuing envisioning the future of big data.<br/><br/>The modern empirical approach to tackling many challenges in natural language processing, information retrieval, data mining, machine learning, and other related domains involves exploiting large amounts of data to learn statistical models that are able to capture characteristics of the problem. A necessary ingredient to this ""big data"" approach is scalable infrastructure that can distribute computations across a cluster of machines. Hadoop, the open-source implementation of MapReduce, has achieved widespread adoption as the de facto platform for data-intensive computing.<br/><br/>Broadly speaking, MapReduce excels at large-scale content analysis in an offline, batch setting. However, this is not enough: we need a data-intensive computing platform that supports heterogeneous models of computation. Hadoop NextGen (aka YARN), provides exactly this: it allows a physical cluster to support a wide range of computational models via a generic resource allocation framework.<br/><br/>This project supports the acquisition of a Hadoop NextGen cluster at the University of Maryland to support the following activities:<br/><br/>1. To explore computational models beyond MapReduce, including<br/>   batch/online tradeoffs in machine learning, real-time streaming<br/>   computations, and graph processing.<br/><br/>2. To sustain innovations in algorithms for content analysis as well<br/>   as modeling implicit and latent relationships between heterogeneous<br/>   content (text, images, graphs, etc.) at scale.<br/><br/>3. To exploit novel hardware architectures for data-intensive<br/>   computing (e.g., Graphics Processing Units and Solid State Drives).<br/><br/>These resources will help the Laboratory for Computational Linguistics and Information Processing (CLIP) and collaborators at the University of Maryland sustain and enhance its successful record of innovation and the integration of research and education."
40,1359275,"REU Site: Machine Learning, Theory and Applications",IIS,"RSCH EXPER FOR UNDERGRAD SITES, RES EXP FOR TEACHERS(RET)-SITE",6/1/14,4/14/16,Jugal Kalita,CO,University of Colorado at Colorado Springs,Continuing Grant,Wendy Nilsen,5/31/18,"$379,901.00 ","Terrance Boult, Qing Yi, Rory Lewis, Kristen Walcott-Justice",jkalita@uccs.edu,"1420, Austin Bluffs Parkway",Colorado Springs,CO,809183733,7192553153,CSE,"1139, 1359","1359, 7218, 9250",$0.00 ,"The REU Site program involves participants in research projects in machine learning techniques and emerging applications, including text analytics and health informatics. Student projects are expected to yield results that will contribute to ubiquitous software-based technologies in innovative ways. The students are introduced to the research topics and helped to obtain in-depth understanding through introductory and on-going presentations. The REU Site program is focusing on training future computer scientists from institutions with limited research opportunities. An emphasis is given to recruiting under-represented minority students into computer science research at an early age. The research experience will encourage these students to be productive researchers in academic and non-academic environments during their future careers.<br/><br/>The REU students will be involved in research in machine learning and its applications in diverse and emergent areas such as intelligent software testing, intelligent complier design, health informatics and natural language processing. Students have the opportunity to utilize a combination of theoretical reading, analysis, and research within laboratory environments. The students develop algorithms to solve interesting and timely problems, develop software, perform hands-on research on novel problems, conduct experiments, and gain experience writing scientific papers and in oral presentation of their work in a conference-like setting. More information on this project can be found at the University of Colorado at Colorado Springs REU Site website (http://www.cs.uccs.edu/~jkalita/reu.html)."
41,1344944,SBIR Phase I:  Representation and Deep Learning for Free Text Applications,IIP,SBIR Phase I,1/1/14,6/4/14,Stephen Gallant,MA,"Textician, LLC",Standard Grant,Peter Atherton,12/31/14,"$165,000.00 ",,sgallant@mmres.com,51 Fenno St,Cambridge,MA,21386737,6176424959,ENG,5371,"5371, 8032",$0.00 ,"This Small Business Innovation Research (SBIR) Phase I project seeks to validate a new way to process textual material, so that computers can better learn applications related to natural language.  A goal is to enable computer learning methods to take better account of parse structure in sentences, which is currently difficult.  Two experimental prototypes will be constructed.  One will focus upon automatically determining whether online posts about a company, brand or institution are positive or negative (sentiment analysis).  This is difficult because sentence structure is important, and also because online posts can be informal, contain slang, etc.  The second prototype finds the most important paragraph during a search, also taking sentence structure into account.  Such paragraph-level information retrieval helps when extracting factual information from running text or web pages.  The new method represents words simultaneously with parse structure using a single high-dimensional vector (for example, a list of 1,000 numbers).  A successful SBIR project will ultimately improve machine learning applications for a wide range of tasks, including document retrieval, summarization, and automated translation.  Moreover, the same techniques can be applied to represent any structured collection prior to machine learning, including images and genomic information. <br/><br/>The broader impact/commercial potential of this project is to enhance capabilities for automated processing of free text and other structured data.  Motivation for this approach comes from neural networks and, in turn, it has applications to neural modeling and our understanding of how the brain processes information.  In the software industry, commercial innovation continues to revolve around automated processing of web pages, which plays a key role in creating many new companies.  Therefore, the ability to handle free text is increasing in importance. A better way to represent text for use with machine learning will open new capabilities wherever the structure of sentences must be taken into account.  This will lead to new startups, and provide consumers with new products and services. A successful project will validate new technology that can give a huge competitive edge to companies that take advantage of it, provide new and better capabilities for consumers, and advance those countries, such as our own, whose economies benefit from new technological innovations."
42,1405829,CI-EN-Collaborative Research: Supporting Research and Teaching for Next-Generation Search Engines in Lemur,CNS,CCRI-CISE Cmnty Rsrch Infrstrc,9/1/14,8/6/14,W. Bruce Croft,MA,University of Massachusetts Amherst,Standard Grant,Maria Zemankova,8/31/18,"$499,636.00 ",,croft@cs.umass.edu,Research Administration Building,Hadley,MA,10359450,4135450698,CSE,7359,7359,$0.00 ,"Many previous successes in information retrieval (IR) research have been based on open source software and test collections supported by university groups.  This project will extend open-source search engine software developed by the Lemur Project to support major emerging research areas that are important for the development of the next generation of search engines that will run on mobile, speech-based platforms and be capable of learning and adapting to individual users.  We will also add new capabilities that support large-scale collection of material from social media and common text mining tasks.  These extensions will also be an important part of training graduate students in computer science in these critical new research areas.<br/><br/>The impact of machine learning, natural language processing (NLP), and social media on IR research has been significant and continues to grow.  Machine learning techniques, in particular, have been instrumental in helping to develop ranking functions that involve linear or non-linear combinations of many representation features of queries and documents.  The enhancements we will provide in Lemur to support ongoing research in these areas include tools for training parameters, defining and incorporating new features, learning-to-rank methods that are relatively easy to use and well-integrated with the other components of the search engine, and integration with NLP and data mining toolkits.  These enhancements, and the architectural changes to support multi-pass searching and flexible query processing, are not available currently in any open source search engine or IR toolkit.<br/><br/>For further information, see the project web site at the URL:  http://www.lemurproject.org/."
43,1405045,CI-EN-Collaborative Research: Supporting Research and Teaching for Next-Generation Search Engines in Lemur,CNS,CCRI-CISE Cmnty Rsrch Infrstrc,9/1/14,8/6/14,Jamie Callan,PA,Carnegie-Mellon University,Standard Grant,Maria Zemankova,8/31/18,"$499,998.00 ",,callan@cs.cmu.edu,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,CSE,7359,7359,$0.00 ,"Many previous successes in information retrieval (IR) research have been based on open source software and test collections supported by university groups.  This project will extend open-source search engine software developed by the Lemur Project to support major emerging research areas that are important for the development of the next generation of search engines that will run on mobile, speech-based platforms and be capable of learning and adapting to individual users.  We will also add new capabilities that support large-scale collection of material from social media and common text mining tasks.  These extensions will also be an important part of training graduate students in computer science in these critical new research areas.<br/><br/>The impact of machine learning, natural language processing (NLP), and social media on IR research has been significant and continues to grow.  Machine learning techniques, in particular, have been instrumental in helping to develop ranking functions that involve linear or non-linear combinations of many representation features of queries and documents.  The enhancements we will provide in Lemur to support ongoing research in these areas include tools for training parameters, defining and incorporating new features, learning-to-rank methods that are relatively easy to use and well-integrated with the other components of the search engine, and integration with NLP and data mining toolkits.  These enhancements, and the architectural changes to support multi-pass searching and flexible query processing, are not available currently in any open source search engine or IR toolkit.<br/><br/>For further information, see the project web site at the URL:  http://www.lemurproject.org/."
44,1423784,Constructing and Validating an Automated Coding System for Electronic News Sources,SES,"SOCIOLOGY, METHOD, MEASURE & STATS",8/15/14,7/1/15,Pamela Oliver,WI,University of Wisconsin-Madison,Continuing grant,Joseph Whitmeyer,7/31/17,"$341,831.00 ",Chaeyoon Lim,oliver@ssc.wisc.edu,21 North Park Street,MADISON,WI,537151218,6082623822,SBE,"1331, 1333","9179, 9251",$0.00 ,"This project builds, tests, and validates an open-source automated system for coding social movement data from electronically available news sources. Although no source is perfect, scholars agree that the best general source of such data, specifically that on social protest, is a compilation of a large number of news sources. This project draws on the advances in machine learning developed in computer science and statistics and combining them with our deep substantive knowledge as sociologists of the problems of identifying and coding collective action events in news sources. An existing highly-regarded hand-coded data set based on the New York Times is used as the reference for ""training"" machine learning algorithms -- called ""classifiers"" -- to recognize elements of an action event in a news article and extract relevant information. We also collect and hand-code new data drawn from other regional, national, and international news sources to provide additional training sets to increase the range and variety of protests we are able to detect. A supplement to the project provides research experience for undergraduates who will be involved in collecting and coding these new data.<br/><br/>This project builds, tests, and validates an open-source automated system for coding data from electronically available news sources. It advances the state of data collection in social science and employs the latest developments in natural language processing and supervised machine learning within computer science and statistics. The result will be an open-source, publicly available system that may be used by other researchers and further improved and expanded.<br/><br/>This project promises to provide an important new methodological tool of broad interdisciplinary value to social scientists and to open the door to more efficiently compiling collective action-data from news sources that can improve both academic scholarship and public policy. All of the code for this project will be released under an open-source license and publicly accessible through a public source code repository. This work will be accessible and useful to scholars in social movements, international relations, and foreign policy. The work will also be of use to a large number of non-academics, such as foreign policy analysts and decision-makers, journalists, and those interested in computational methods of textual analysis and classification. The ability to code collective action data more efficiently and accurately from news sources has broad policy applicability."
45,1443176,EAGER:  Development of a Multivariate Biomarker Analysis Technique for Paleoclimate Reconstruction,AGS,PALEOCLIMATE PROGRAM,6/15/14,6/4/14,Jessica Tierney,MA,Woods Hole Oceanographic Institution,Standard Grant,David J. Verardo,5/31/16,"$75,894.00 ",,jesst@email.arizona.edu,183 OYSTER POND ROAD,WOODS HOLE,MA,25431041,5082893542,GEO,1530,"0000, 7916, OTHR",$0.00 ,"Reconstructions of past environments based on chemical ""proxies"" have provided invaluable insights into how the Earth system functions on a variety of timescales, from decades to millions of years. Organic molecular, or ""biomarker"", proxies are being applied increasingly in paleoclimate studies, and these compounds hold a wealth of information about an array of environmental conditions includin temperature, aridity, vegetation biome type, and water salinity. However, their current interpretation is mainly in terms of a single variable of interest, most often temperature. <br/><br/>This study, led by an early-career researcher from the Woods Hole Oceanographic Institution, is a first attempt to leverage the multidimensional quality of biomarker data for paleoclimate reconstruction. A statistical model will be developed using artificial neural network (ANN) and support vector regression (SVR) algorithms (""machine learning"" techniques). This will allow quantitative evaluation of how assemblages of biomarkers in sedimentary archives relate to an array of modern environmental conditions. The outcomes of the proposed project will include development of a new toolbox to quantitatively reconstruct past climates over land and in the ocean in many types of paleoclimate archives. Such a tool could have broad application across a range of fields including environmental studies, petroleum geology, and computer science."
46,1505223,HCC: Small: Modeling and Supporting Creativity During Collaborative STEM Activities,IIS,HCC-Human-Centered Computing,9/1/14,7/8/15,Winslow Burleson,NY,New York University,Standard Grant,William Bainbridge,4/30/18,"$439,946.00 ",,win@arizona.edu,70 WASHINGTON SQUARE S,NEW YORK,NY,100121019,2129982121,CSE,7367,"7367, 7923, 9251",$0.00 ,"This research will advance a novel technological approach that relies on machine learning techniques in general and Natural Language Processing (NLP) in particular to develop models and support for creativity during collaborative science, technology, engineering, and mathematics (STEM) educational activities. We will extend existing educational software with NLP capabilities to automatically assess and subsequently support creativity during collaborative tasks. The research questions are: (1) Which factors influence moment-by-moment creativity during collaborative problem solving activities? (2) How can NLP be used to build student models that detect those factors? (3) How can an ITS use this information to create personalized interventions to support creativity?<br/> <br/>The first phase in this research will collect data from students solving problems in pairs with an educational application to identify factors that are relevant to creativity processes and outcomes.  These data will be used to derive computational student models for automatically assessing student creativity in terms of both moment-to-moment processes and outcomes through machine learning methodologies focusing on an NLP approach. In addition to providing automatic assessment, the models will also inform factors that influence creativity during collaboration through educational data mining techniques. The final phase of the work will design and test a set of interventions to foster creativity during collaborative activities.<br/> <br/>Using data corresponding to pairs of students solving open-ended STEM-based problems, this research will develop a rich and nuanced understanding of creativity processes and outcomes in collaborative contexts, and how these relate to knowledge, affect and creative thinking styles. Relying on that understanding, it will develop and evaluate novel student models that recognize salient, creativity-related events through NLP techniques, as well as personalized support for creativity during collaborative activities and evaluating that support through an experiment with university students. This project will pave the way for a new class of collaborative cyberlearning technologies to both assess and foster creativity, through just-in-time personalized support based on easily deployed NLP-based student models."
47,1418269,Collaborative Research: Modeling Social Interaction and Performance in STEM Learning,DRL,REAL,9/1/14,8/17/14,Tiffany Barnes,NC,North Carolina State University,Standard Grant,Gregg Solomon,8/31/18,"$200,003.00 ",,tiffany.barnes@gmail.com,2701 Sullivan DR STE 240,Raleigh,NC,276950001,9195152444,EHR,7625,"7625, 8244, 8500",$0.00 ,"This Research on Education and Learning (REAL) project arises from an October 2014 Ideas Lab on Data-intensive Research to Improve Teaching and Learning. The intentions of that effort were to: (1) bring together researchers from across disciplines to foster novel, transformative, multidisciplinary approaches to using the data in large education-related data sets to create actionable knowledge for improving STEM teaching and learning environments in the medium term; and (2) revolutionize learning in the longer term. In this project, researchers from the Educational Testing Service, Columbia University Teachers' College, Arizona State University, and North Carolina State University will conduct data-driven, exploratory analyses to identify key places where social interactions impact learning outcomes in specific learning environments, with the goal of improving teaching and learning in large-scale STEM courses.<br/> <br/>This research takes advantage of data traces left in large-scale blended and online learning environments (including massively open online courses, or MOOCs).  The researchers will develop a comprehensive model for social learning in the context of such courses that will enable assessment of both the collaborative needs of individuals within the context of a class, and the quality of collaborations they are carrying out. Such diagnoses will allow both instructors and automated systems to provide advice to learners about the peers they might work with to enhance their learning (e.g., regarding the kinds of social interactions that will foster better understanding and development of important disciplinary capabilities). An interdisciplinary team of investigators with expertise in theory-driven educational data mining, natural-language processing, psychometrics, social-network analysis, and computer support for collaborative learning will collaborate to explore when learners in blended and online classes benefit from social interactions, and to understand how to identify more and less productive collaborative interactions.  The researchers will use data from three blended and online classes (e.g., log files capturing collaborative discussions, individual and collaborative interactions around well-instrumented examples, peer tutoring sessions, pair programming labs, paired projects) and a variety of data analysis approaches (e.g., text analysis, machine learning) to determine: (1) which cognitive, social, and affective dimensions of need and interaction can be identified from available data; (2) which analyses are useful in providing action-oriented collaboration advice; and (3) what additional types of data may be needed for making such recommendations. This exploration will be grounded in theories of social interactions for learning (e.g., self-explanation, dialectic with oneself and others, zone of proximal development, social learning theory of Bandura, peripheral and centripetal participation)."
48,1418219,Collaborative Research: Modeling Social Interaction and Performance in STEM Learning,DRL,REAL,9/1/14,8/17/14,Yoav Bergner,NJ,Educational Testing Service,Standard Grant,Gregg Solomon,7/31/17,"$362,857.00 ",Ryan Baker,ybb2@nyu.edu,Center for External Research,Princeton,NJ,85402218,6096832734,EHR,7625,"7625, 8244, 8500",$0.00 ,"This Research on Education and Learning (REAL) project arises from an October 2014 Ideas Lab on Data-intensive Research to Improve Teaching and Learning. The intentions of that effort were to: (1) bring together researchers from across disciplines to foster novel, transformative, multidisciplinary approaches to using the data in large education-related data sets to create actionable knowledge for improving STEM teaching and learning environments in the medium term; and (2) revolutionize learning in the longer term. In this project, researchers from the Educational Testing Service, Columbia University Teachers' College, Arizona State University, and North Carolina State University will conduct data-driven, exploratory analyses to identify key places where social interactions impact learning outcomes in specific learning environments, with the goal of improving teaching and learning in large-scale STEM courses.<br/> <br/>This research takes advantage of data traces left in large-scale blended and online learning environments (including massively open online courses, or MOOCs).  The researchers will develop a comprehensive model for social learning in the context of such courses that will enable assessment of both the collaborative needs of individuals within the context of a class, and the quality of collaborations they are carrying out. Such diagnoses will allow both instructors and automated systems to provide advice to learners about the peers they might work with to enhance their learning (e.g., regarding the kinds of social interactions that will foster better understanding and development of important disciplinary capabilities). An interdisciplinary team of investigators with expertise in theory-driven educational data mining, natural-language processing, psychometrics, social-network analysis, and computer support for collaborative learning will collaborate to explore when learners in blended and online classes benefit from social interactions, and to understand how to identify more and less productive collaborative interactions.  The researchers will use data from three blended and online classes (e.g., log files capturing collaborative discussions, individual and collaborative interactions around well-instrumented examples, peer tutoring sessions, pair programming labs, paired projects) and a variety of data analysis approaches (e.g., text analysis, machine learning) to determine: (1) which cognitive, social, and affective dimensions of need and interaction can be identified from available data; (2) which analyses are useful in providing action-oriented collaboration advice; and (3) what additional types of data may be needed for making such recommendations. This exploration will be grounded in theories of social interactions for learning (e.g., self-explanation, dialectic with oneself and others, zone of proximal development, social learning theory of Bandura, peripheral and centripetal participation)."
49,1418378,Collaborative Research: Modeling Social Interaction and Performance in STEM Learning,DRL,"REAL, ECR-EHR Core Research",9/1/14,8/7/15,Danielle McNamara,AZ,Arizona State University,Standard Grant,Gregg Solomon,8/31/17,"$226,587.00 ",,dsmcnamara1@gmail.com,ORSPA,TEMPE,AZ,852816011,4809655479,EHR,"7625, 7980","7625, 8244, 8500, 9177, 9251, SMET",$0.00 ,"This Research on Education and Learning (REAL) project arises from an October 2014 Ideas Lab on Data-intensive Research to Improve Teaching and Learning. The intentions of that effort were to: (1) bring together researchers from across disciplines to foster novel, transformative, multidisciplinary approaches to using the data in large education-related data sets to create actionable knowledge for improving STEM teaching and learning environments in the medium term; and (2) revolutionize learning in the longer term. In this project, researchers from the Educational Testing Service, Columbia University Teachers' College, Arizona State University, and North Carolina State University will conduct data-driven, exploratory analyses to identify key places where social interactions impact learning outcomes in specific learning environments, with the goal of improving teaching and learning in large-scale STEM courses.<br/> <br/>This research takes advantage of data traces left in large-scale blended and online learning environments (including massively open online courses, or MOOCs).  The researchers will develop a comprehensive model for social learning in the context of such courses that will enable assessment of both the collaborative needs of individuals within the context of a class, and the quality of collaborations they are carrying out. Such diagnoses will allow both instructors and automated systems to provide advice to learners about the peers they might work with to enhance their learning (e.g., regarding the kinds of social interactions that will foster better understanding and development of important disciplinary capabilities). An interdisciplinary team of investigators with expertise in theory-driven educational data mining, natural-language processing, psychometrics, social-network analysis, and computer support for collaborative learning will collaborate to explore when learners in blended and online classes benefit from social interactions, and to understand how to identify more and less productive collaborative interactions.  The researchers will use data from three blended and online classes (e.g., log files capturing collaborative discussions, individual and collaborative interactions around well-instrumented examples, peer tutoring sessions, pair programming labs, paired projects) and a variety of data analysis approaches (e.g., text analysis, machine learning) to determine: (1) which cognitive, social, and affective dimensions of need and interaction can be identified from available data; (2) which analyses are useful in providing action-oriented collaboration advice; and (3) what additional types of data may be needed for making such recommendations. This exploration will be grounded in theories of social interactions for learning (e.g., self-explanation, dialectic with oneself and others, zone of proximal development, social learning theory of Bandura, peripheral and centripetal participation)."
50,1418026,SCH: INT: Collaborative Research: Learning and Sensory-based Modeling for Adaptive Web-Empowerment Trauma Treatment,IIS,Smart and Connected Health,8/1/14,8/13/14,Jeffrey Cohn,PA,University of Pittsburgh,Standard Grant,Jie Yang,7/31/19,"$238,795.00 ",,jeffcohn@pitt.edu,300 Murdoch Building,Pittsburgh,PA,152603203,4126247400,CSE,8018,"8018, 8062",$0.00 ,"Mental trauma following disasters, military service, accidents, domestic violence and other traumatic events is a health issue costing multiple billion of dollars per year. Beyond its direct costs, there are indirect costs including a 45-150% greater use of medical and psychiatric care. While web-based support systems have been developed these are effectively a ""one-size-fits all"" approach lacking the personalization of regular treatment and the engagement and effectiveness associated with a tailored regimen. This project brings together a multi-disciplinary team of leading researchers in trauma treatment, facial analysis, computer vision and machine learning to develop a scalable, adaptive person-centered approach that uses vision and sensing to improve web-based trauma treatment.  In particular, the effort measures specific personalized variables during treatment and then uses a model to adapt treatment to individuals in need. <br/><br/>The core treatment design builds on well-established social-cognitive theory, where self-efficacy and physiological response are critical elements of recovery. The project measures these as well as engagement that is critical in self-directed web-based treatment. The modeling requires advances in computer vision and facial analysis to develop individualized models that can be computed with just a standard laptop. This project is the first effort to approximate changes in self-efficacy from sensory data. The effort uses and advances machine learning and domain adaption to support this approximation as well as to support the rapid personalization of models.  Building a smart system that empowers individuals by combining sensing and learning to improve web-based treatment offers a transformative approach to this national health need for cost-effective evidence-based treatment of trauma."
51,1418728,Algebraic Structures in Optimization,DMS,COMPUTATIONAL MATHEMATICS,7/1/14,6/29/14,Rekha Thomas,WA,University of Washington,Standard Grant,Leland Jameson,6/30/18,"$210,001.00 ",,thomas@math.washington.edu,4333 Brooklyn Ave NE,Seattle,WA,981950001,2065434043,MPS,1271,9263,$0.00 ,"The reconstruction of 3D-scenes from noisy camera images is a fundamental problem in computer vision. In this project, these reconstructions are approached as problems in polynomial optimization, which is not the usual practice in computer vision. By understanding the mathematical structure of these problems, we aim to advance theoretical understanding of these problems and to develop practical and efficient algorithms for 3D reconstruction.  This project creates an unusual opportunity to apply methods from optimization, algebraic geometry and computational algebra to applied problems that are both mathematically rich and practically important. A second component of this project seeks to develop the theory of cone factorizations of nonnegative matrices with a special emphasis on positive semi-definite factorizations. This new concept has many potential applications to diverse areas such as optimization, data compression, machine learning and statistics. Both projects involve graduate students and their training in a number of inter-disciplinary mathematical areas.  <br/><br/>The main project outlined in this proposal is a multi-year effort to understand the algebraic geometric foundations of 3D-reconstruction problems in computer vision.  This process involves two steps -- the first is to identify the constraints and the second to solve these problems efficiently using methods from convex optimization. The first step requires methods from computational algebra and classical algebraic geometry and the second step relies on techniques from optimization, real algebraic geometry and convex geometry. These problems also provide numerous fascinating applications of (computational) representation theory, multi-linear algebra, and geometry, and the two-way exchange will enrich both mathematics and computer vision. Students will be trained in this cross-disciplinary work and several researchers from mathematics, optimization, and computer vision will be involved in this project. The project on cone factorizations builds on a currently booming area at the interface of mathematics and computer science. Developed originally by the PI and collaborators for the sake of understanding extended formulations of convex sets, these factorizations have much further potential and applications. The plan is to develop the structural and mathematical theory of cone factorizations with the aim of better understanding extended formulations and other potential applications."
52,1418520,SCH: INT: Collaborative Research: Learning and Sensory-based Modeling for Adaptive Web-Empowerment Trauma Treatment,IIS,Smart and Connected Health,8/1/14,8/13/14,Terrance Boult,CO,University of Colorado at Colorado Springs,Standard Grant,Jie Yang,7/31/20,"$1,493,590.00 ","Charles Benight, Rory Lewis",tboult@vast.uccs.edu,"1420, Austin Bluffs Parkway",Colorado Springs,CO,809183733,7192553153,CSE,8018,"8018, 8062",$0.00 ,"Mental trauma following disasters, military service, accidents, domestic violence and other traumatic events is a health issue costing multiple billion of dollars per year. Beyond its direct costs, there are indirect costs including a 45-150% greater use of medical and psychiatric care. While web-based support systems have been developed these are effectively a ""one-size-fits all"" approach lacking the personalization of regular treatment and the engagement and effectiveness associated with a tailored regimen. This project brings together a multi-disciplinary team of leading researchers in trauma treatment, facial analysis, computer vision and machine learning to develop a scalable, adaptive person-centered approach that uses vision and sensing to improve web-based trauma treatment.  In particular, the effort measures specific personalized variables during treatment and then uses a model to adapt treatment to individuals in need. <br/><br/>The core treatment design builds on well-established social-cognitive theory, where self-efficacy and physiological response are critical elements of recovery. The project measures these as well as engagement that is critical in self-directed web-based treatment. The modeling requires advances in computer vision and facial analysis to develop individualized models that can be computed with just a standard laptop. This project is the first effort to approximate changes in self-efficacy from sensory data. The effort uses and advances machine learning and domain adaption to support this approximation as well as to support the rapid personalization of models.  Building a smart system that empowers individuals by combining sensing and learning to improve web-based treatment offers a transformative approach to this national health need for cost-effective evidence-based treatment of trauma."
53,1418523,SCH: INT: Collaborative Research: Learning and Sensory-based Modeling for Adaptive Web-Empowerment Trauma Treatment,IIS,Smart and Connected Health,8/1/14,3/3/17,Fernando De la Torre,PA,Carnegie-Mellon University,Standard Grant,Jie Yang,7/31/18,"$251,757.00 ",,ftorre@cs.cmu.edu,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,CSE,8018,"8018, 8062",$0.00 ,"Mental trauma following disasters, military service, accidents, domestic violence and other traumatic events is a health issue costing multiple billion of dollars per year. Beyond its direct costs, there are indirect costs including a 45-150% greater use of medical and psychiatric care. While web-based support systems have been developed these are effectively a ""one-size-fits all"" approach lacking the personalization of regular treatment and the engagement and effectiveness associated with a tailored regimen. This project brings together a multi-disciplinary team of leading researchers in trauma treatment, facial analysis, computer vision and machine learning to develop a scalable, adaptive person-centered approach that uses vision and sensing to improve web-based trauma treatment.  In particular, the effort measures specific personalized variables during treatment and then uses a model to adapt treatment to individuals in need. <br/><br/>The core treatment design builds on well-established social-cognitive theory, where self-efficacy and physiological response are critical elements of recovery. The project measures these as well as engagement that is critical in self-directed web-based treatment. The modeling requires advances in computer vision and facial analysis to develop individualized models that can be computed with just a standard laptop. This project is the first effort to approximate changes in self-efficacy from sensory data. The effort uses and advances machine learning and domain adaption to support this approximation as well as to support the rapid personalization of models.  Building a smart system that empowers individuals by combining sensing and learning to improve web-based treatment offers a transformative approach to this national health need for cost-effective evidence-based treatment of trauma."
54,1416136,SBIR Phase I: ACCORDION HEALTH ASSISTANT: PREDICTIVE ENGINE FOR PERSONALIZATION OF HEALTHCARE COSTS,IIP,SMALL BUSINESS PHASE I,7/1/14,9/29/14,Sriram Vishwanath,TX,Saltare Systems LLC,Standard Grant,Jesus Soriano Molla,12/31/14,"$150,000.00 ",Sriram Vishwanath,sriramuniverse@gmail.com,2607 Euclid Avenue,Austin,TX,787042607,6508234072,ENG,5371,"5371, 8018, 8023, 8032, 8042",$0.00 ,"The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is to empower individuals to make financially sound healthcare decisions; helping them navigate the otherwise convoluted and confusing healthcare cost landscape by enabling them to manage, plan, and better understand healthcare expenditures and budget for upcoming and future costs. This project takes a data-driven approach, developing sophisticated, innovative, and large-scale data mining and machine learning algorithms to automatically learn a plethora of cost patterns from over a 100 million healthcare records.<br/><br/>The proposed project will provide a user-friendly, engaging interface for individuals to manage and understand healthcare expenses for themselves and their families. By bringing together ideas in data mining, machine learning and natural language processing to enable our technology, we make fundamental progress in research and development in the field of healthcare informatics. The anticipated results are the development of an algorithmic suite that can be used to model and predict the nature of healthcare costs across regional boundaries and demographic groups in the United States."
55,1415308,Collaborative Research: New Statistical Learning and Scalable Computing for Large Unstructured Data,DMS,"OFFICE OF MULTIDISCIPLINARY AC, CDS&E-MSS, CDS&E",8/1/14,7/29/14,Annie Qu,IL,University of Illinois at Urbana-Champaign,Standard Grant,Yong Zeng,7/31/17,"$229,042.00 ",,aqu2@uci.edu,1901 South First Street,Champaign,IL,618207406,2173332187,MPS,"1253, 8069, 8084","7433, 8084, 9263",$0.00 ,"This proposal focuses on some fundamental issues concerning unstructured data that arise from text-heavy documents,  where the underlying data exhibit unique characteristics such as large volume, large variety and large velocity of change. Automating the process of information extraction is extremely critical in the information age, and has high-utility in online surveys,  and threat detection and prevention. The integrated program of research and education will have significant impacts in many fields such as machine learning and data mining,  natural language processing, opinion survey, business forecasting and service, health research,  and social and political science, among others. This will stimulate  interdisciplinary research and collaboration with scientists from disparate fields. The proposed project requires extensive algorithm and software development for target applications. In  particular, advanced computational tools will be developed through mapReduce over distributed computational platforms such as OpenMP, MPI and hadoop, and documentation of the software will be disseminated  along with the  technology transfer.<br/><br/>Unstructured data impose great challenges in that text documents need to be embedded and integrated with numerical input for statistical modeling, which  requires  overparameterized modeling to achieve accurate prediction and unbiased inference for high-dimensional data. The proposed research aims to develop new statistical methods and tools for sentiment analysis and text summarization utilizing word relations through graphs and personalized prediction for recommender systems.  It borrows information across all available information for document summarization, including tagged and untagged documents, leading to higher accuracy of tagging.  This will enhance information storage, sorting and processing as well as filtering. Moreover,  the project develops a novel approach for accurate personalized prediction utilizing the heterogeneity variation among all users, which impacts everyday life in terms of personalization, such  as in service, recommendation and advertising. More importantly,  the proposed statistical methodology and scalable computational algorithms will be valuable and useful for  other types of unstructured data. Finally, many of the advanced optimization techniques and computing procedures to be developed will also be applicable to other types of ``BIG"" data problems."
56,1415482,Collaborative Research: New statistical learning and scalable computation for large unstructured data,DMS,"OFFICE OF MULTIDISCIPLINARY AC, CDS&E-MSS, CDS&E",8/1/14,7/29/14,Junhui Wang,IL,University of Illinois at Chicago,Standard Grant,Yong Zeng,7/31/17,"$49,362.00 ",,junhui@uic.edu,809 S. Marshfield Avenue,Chicago,IL,606124305,3129962862,MPS,"1253, 8069, 8084","7433, 8084, 9263",$0.00 ,"This proposal focuses on some fundamental issues concerning unstructured data that arise from text-heavy documents,  where the underlying data exhibit unique characteristics such as large volume, large variety and large velocity of change. Automating the process of information extraction is extremely critical in the information age, and has high-utility in online surveys,  and threat detection and prevention. The integrated program of research and education will have significant impacts in many fields such as machine learning and data mining,  natural language processing, opinion survey, business forecasting and service, health research,  and social and political science, among others. This will stimulate  interdisciplinary research and collaboration with scientists from disparate fields. The proposed project requires extensive algorithm and software development for target applications. In  particular, advanced computational tools will be developed through mapReduce over distributed computational platforms such as OpenMP, MPI and hadoop, and documentation of the software will be disseminated  along with the  technology transfer.<br/><br/>Unstructured data impose great challenges in that text documents need to be embedded and integrated with numerical input for statistical modeling, which  requires  overparameterized modeling to achieve accurate prediction and unbiased inference for high-dimensional data. The proposed research aims to develop new statistical methods and tools for sentiment analysis and text summarization utilizing word relations through graphs and personalized prediction for recommender systems.  It borrows information across all available information for document summarization, including tagged and untagged documents, leading to higher accuracy of tagging.  This will enhance information storage, sorting and processing as well as filtering. Moreover,  the project develops a novel approach for accurate personalized prediction utilizing the heterogeneity variation among all users, which impacts everyday life in terms of personalization, such  as in service, recommendation and advertising. More importantly,  the proposed statistical methodology and scalable computational algorithms will be valuable and useful for  other types of unstructured data. Finally, many of the advanced optimization techniques and computing procedures to be developed will also be applicable to other types of ``BIG"" data problems."
57,1415500,Collaborative Research: New statistical learning and scalable computation for large unstructured data,DMS,"OFFICE OF MULTIDISCIPLINARY AC, CDS&E-MSS, CDS&E",8/1/14,7/29/14,Xiaotong Shen,MN,University of Minnesota-Twin Cities,Standard Grant,Yong Zeng,7/31/17,"$255,597.00 ",,xshen@umn.edu,200 OAK ST SE,Minneapolis,MN,554552070,6126245599,MPS,"1253, 8069, 8084","7433, 8084, 9263",$0.00 ,"This proposal focuses on some fundamental issues concerning unstructured data that arise from text-heavy documents,  where the underlying data exhibit unique characteristics such as large volume, large variety and large velocity of change. Automating the process of information extraction is extremely critical in the information age, and has high-utility in online surveys,  and threat detection and prevention. The integrated program of research and education will have significant impacts in many fields such as machine learning and data mining,  natural language processing, opinion survey, business forecasting and service, health research,  and social and political science, among others. This will stimulate  interdisciplinary research and collaboration with scientists from disparate fields. The proposed project requires extensive algorithm and software development for target applications. In  particular, advanced computational tools will be developed through mapReduce over distributed computational platforms such as OpenMP, MPI and hadoop, and documentation of the software will be disseminated  along with the  technology transfer.<br/><br/>Unstructured data impose great challenges in that text documents need to be embedded and integrated with numerical input for statistical modeling, which  requires  overparameterized modeling to achieve accurate prediction and unbiased inference for high-dimensional data. The proposed research aims to develop new statistical methods and tools for sentiment analysis and text summarization utilizing word relations through graphs and personalized prediction for recommender systems.  It borrows information across all available information for document summarization, including tagged and untagged documents, leading to higher accuracy of tagging.  This will enhance information storage, sorting and processing as well as filtering. Moreover,  the project develops a novel approach for accurate personalized prediction utilizing the heterogeneity variation among all users, which impacts everyday life in terms of personalization, such  as in service, recommendation and advertising. More importantly,  the proposed statistical methodology and scalable computational algorithms will be valuable and useful for  other types of unstructured data. Finally, many of the advanced optimization techniques and computing procedures to be developed will also be applicable to other types of ``BIG"" data problems."
58,1430912,SBIR Phase II: A Cloud-Based Service for Audio Access to News and Blogs,IIP,SMALL BUSINESS PHASE II,10/1/14,7/18/16,Radhika Thekkath,CA,"AgiVox, Inc.",Standard Grant,Glenn H. Larsen,9/30/16,"$761,756.00 ",,rthekkath@agivox.com,440 N. Wolfe Road,Sunnyvale,CA,940853869,6509960224,ENG,5373,"116E, 5373, 8032, 8039, 9231, 9251",$0.00 ,"This SBIR Phase II project will research the algorithms for automatic discovery of topics of interest for a user based on existing written content such as news and blogs. These topics can then be used to dynamically create a personalized content reader with high quality synthesized audio. Applications created from this cloud-based project will enable a car driver to get instant and relevant information without taking her eyes off the road, a must for today's lengthy commutes and in-car safety. Other applications will allow the same audio access to Internet news and blogs via a smartphone for those who are vision-impaired or vision-busy with exercising, gardening, etc. This project has a societal impact among the blind, aging eyes that have difficulty reading small print and small screens, and the car driver, since it provides the ability to find contextual news and blogs in an ""eyes-free"" manner and with an easy listening experience. The fundamental research components from this project can be re-applied to other content and similar fields of research. This project's applications have the potential to generate a revenue stream which will in turn create jobs and have an overall impact on the economy.<br/><br/>The goal of the research is to determine whether topic information extracted from a large corpus of unrelated documents using unsupervised machine-learning can be used for content discovery, improving the quality of synthesized speech, and discovering user preferences for a recommendation system. The research uses topic-modeling, a machine learning algorithm, to uncover topics across thousands of RSS feeds, and natural language processing to improve the quality of synthesized speech. Retrieval of specific RSS channels per user's topic preferences is then possible by using the probability of mappings between topics and RSS channels. Since content scanning by listening is a slower process than visually scanning for relevant responses, the current research proposal will improve this process by combining user preference with information retrieval for a better user experience. The topic discovery research will include three key components: discovering multiple levels of subtopics to create topic hierarchies for easier browsing, a method for identifying trending topics, and determining current and relevant topics for automation of audio content. Once this project is shown to produce effective results, the same techniques can be applied across other document collections."
59,1421165,III: Small: Multi-modal Neuroimaging Data Fusion and Analysis with Harmonic Maps Under Designed Riemannian Metric,IIS,CRCNS-Computation Neuroscience,8/1/14,7/31/14,Yalin Wang,AZ,Arizona State University,Standard Grant,Sylvia Spengler,7/31/19,"$418,114.00 ",,ylwang@asu.edu,ORSPA,TEMPE,AZ,852816011,4809655479,CSE,7327,"7364, 7923, 8089, 8091",$0.00 ,"The rapid development in acquiring multi-modal neuroimaging data provides exciting new opportunities to systematically characterize human brain structure, its relationship to cognition and behavior, and the contributions of genetic and environmental factors to individual differences in brain circuitry. To optimally use such rich multi-modal data, there is an urgent need for powerful computational frameworks to integrate and analyze multi-source data. The current practice usually combines available data features from different sources without considering the intrinsic geometry and biology structure relationship between data sources. Shape analysis based approach may serve as a bridge for a general and integrative approach to multi-model data fusion and analysis. Although numerous studies have been devoted to imaging data registration research, limited progress has been made to integrate different modality data with some physically nature and geometrically intrinsic structures. This proposal focuses on investigating and developing computational algorithms on harmonic map with prescribed Riemannian metric, and on producing theoretically sound and practically efficient solutions for general multi-modal data  fusion and analysis problems.  The work outlined in this proposal will have applications in a number of research fields, including (1) Shape Analysis, neuroimaging and medical imaging in general. The proposed research unifies and connects a variety of computational geometry techniques and tackles a few open problems making it an ideal framework for teaching concepts in shape analysis as well as providing students a broader context in which various components may fit together. The algorithms and tools developed in this project will have a direct impact on neuroimaging research. It may enable discovery of multi-modal imaging biomarkers for some neurodegenerative disease, such as Alzheimer's disease.  Harmonic maps and their related methods have applications in many other fields, including medical imaging, computer vision, machine learning, computer graphics, and geometric modeling. The PI will make the software tools accessible to the society. This project will facilitate the development of new courses and laboratory infrastructure for neuroimaging research. It also provides a unique opportunity for students from computer science to learn neuroscience more efficiently. The funding will allow continuation of ongoing efforts to actively recruit and advise students from under-represented groups.<br/><br/>An integrated research and education plan is outlined in this project to investigate and develop computational theorems and algorithms.  The first goal is to develop a method to compute the harmonic map under a designed Riemannian metric between general surfaces. One key novelty is that the new method formulates multi-source information with a Riemannian metric and thus the multi-source fusion problem is converted to compute a surface harmonic map which is adapted to any designed Riemannian metric on the target surface.  Next, a variational formulation that optimizes the diffeomorphic harmonic map via adjusting the Riemannian metric will be developed. The harmonic map guarantees that it has the global minimum deformation. It will be a practical way to optimize diffeomorphisms between surfaces and provide the flexibility to introduce general objective functions defined by other data sources.  In addition, an algorithm for volumetric harmonic maps under a designed Riemannian metric and a set of novel multivariate geometry statistics for multi-source data analysis will be implemented. The framework explores multi-source data fusion with intrinsic geometry structures and the multivariate statistics may provide more sensitive, reliable and accessible brain imaging biomarkers for neuroimaging analysis. The anticipated outcomes of this research project are: (1) new computational algorithms on harmonic maps with significant applications in various fields, such as medical imaging, computer vision, machine learning, computer graphics and geometric modeling; (2) a practical software package with a rigorous mathematical foundation to analyze multi-modal data and produce sensitive and comprehensive multivariate imaging statistics. It will be tested on a large public neuroimaging dataset and evaluated by various classification tasks."
60,1358839,Neurophysiological circuits underlying episodic memory formation in the human brain,BCS,COGNEURO,6/1/14,8/26/15,Gabriel Kreiman,MA,Children's Hospital Corporation,Continuing grant,Uri Hasson,5/31/18,"$695,939.00 ",,gabriel.kreiman@tch.harvard.edu,300 LONGWOOD AVENUE,Boston,MA,21155737,6179192729,SBE,1699,"1699, 8050",$0.00 ,"Learning and memory formation play a central role in our everyday lives. The critical nature of our ability to acquire new information and to remember previous events is manifested in a variety of conditions that affect memory ranging from learning deficits in children all the way to Alzheimer's disease and related conditions in the elderly. Previous studies have shown that the hippocampus and related structures in the medial temporal lobe (MTL) play a critical role in memory formation for places, images or words. Less is known about how spatiotemporal sequences and events are encoded and recalled. By using movies as a proxy to real-life memory formation for episodic events. Dr. Gabriel Kreiman at the Children's Hospital, Harvard Medical School, will investigate how neuronal activity in the human MTL support the formation of episodic memories during movie events and how the hippocampus interacts with neocortical areas and the amygdala during learning. Computer vision and machine learning techniques will be used to develop detailed annotations of movie events to characterize each sequence of frames in terms of its low-level properties, high-level content and emotional aspects and to predict memory formation for episodic events from audiovisual and emotional content. Furthermore, the researchers will combine this machine learning approach with neurophysiological recordings in the MTL of patients with epilepsy to understand how episodic memories for short movie events are encoded and recalled. <br/><br/>Advancing our understanding of the neural circuits and interactions that orchestrate memory formation will pave the way towards developing innovative technologies to directly interact with the human MTL to enhance learning of everyday episodic information.  The machine learning approach used here to characterize recall of complex audio-visual events will provide systematic metrics to make predictions about memory formation, therefore holding the promise of predicting recall in educational settings. Studying how hippocampal neurons participate in the encoding and recall of episodic events is likely to have major clinical implications for understanding and developing interventions for neurological conditions including age-related dementias and learning disabilities in children.  This project will also offer valuable training opportunities for undergraduate students, graduate students and postdoctoral fellows to gain first hand experience in a cutting edge interdisciplinary study involving computer science, cognitive science, neurophysiology and neurology."
61,1451412,EAGER: Leveraging Structure to Realize the Promise of Transfer Learning,IIS,Robust Intelligence,9/1/14,8/20/14,Fei Sha,CA,University of Southern California,Standard Grant,Weng-keen Wong,8/31/17,"$97,000.00 ",,feisha@usc.edu,University Park,Los Angeles,CA,900890001,2137407762,CSE,7495,"7495, 7916",$0.00 ,"The success of statistical machine learning relies critically on having access to a large amount of data for training. Learning algorithms become much less effective in data-poor situations. Examples of such challenges are recognizing uncommon visual categories from their images, understanding rare languages where both text and audio corpora are expensive to collect, adapting and personalizing assistive robots to new environments and owners, and identifying rare forms of diseases.  Transfer learning has been emerging as an appealing framework to address the challenge of being poor in data. The essential idea behind transfer learning is to leverage a cohort of related tasks, whose training data are abundant, to help to learn target tasks. The research project has several broader impacts. The most recent advances in transfer learning will be incorporated and integrated with the PI's teaching and research activities for graduate and undergraduate students from diverse scientific backgrounds. The project will actively engage undergraduate students in research. The results of the planned research will be rapidly and broadly disseminated to scientific communities via tutorials, review articles/surveys, invited talks and open-source software.<br/><br/>Despite progress, transfer learning methods are largely limited to classification tasks where the goal is to learn  a labeling function for data samples represented as points in the Euclidean space.  In contrast, data in many application problems are complex and rich in structure.  Examples include complex visual scenes where there are strong contextual dependency among object categories, and multimodal data where each modality is complementary to the others. Effectively exploiting the dependency and structures in such data will likely improve the effectiveness of transfer learning relative to methods that ignore them. This project develops statistical methods for structured transfer learning, with applications to problems in computer vision and robotics.  The project focuses on two directions: (1) transfer learning for structured prediction problems, and (2) cross-modal transfer learning.  The research develops new statistical learning methods that deepen understanding and invents practical statistical algorithm to tackle transfer learning problems for data with complex types.  Secondly, the invented methods are applied to practical applications problems in computer vision and robotic perceptions. The project will show that proper of structure in data advances the state-of-the-art of intelligent and autonomous systems in perceiving complex and challenging real-world environments."
62,1430984,SBIR Phase II:  Automatic Extraction of Financial Data from Text,IIP,SMALL BUSINESS PHASE II,10/1/14,11/2/16,Hassan Alam,CA,BCL Technologies,Standard Grant,Peter Atherton,6/30/17,"$965,999.00 ",,hassana@bcltechnologies.com,3031 Tisch Way,San Jose,CA,951282533,4085572080,ENG,5373,"116E, 169E, 5373, 7744, 8032, 8039, 8240, 9231, 9251",$0.00 ,"The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project will result from the availability of relevant financial information in structured computer-usable format with high accuracy in near real time. Currently, data embedded in financial text are extracted manually by hundreds of people working for data warehouses. This manual effort takes on the order of weeks making the bulk of the data unavailable in easily computer-usable form in real time.  The benefits of this project will be focused in three areas: (i) algorithmic trading programs will be able to use all data published worldwide immediately after the data is published; (ii) financial data warehouses will be able to provide an order of magnitude larger set of data concepts (from < 200 to > 3000); (iii) there will be increased transparency in the financial markets as financial information embedded in text becomes computer-readable. In 2012 algorithmic trading was estimated to exceed $5 Trillion in value with 750 Billion shares traded, generating a profit of over $600 Million. Financial transparency is an intangible benefit that will improve financial market efficiency.<br/><br/>This Small Business Innovative Research (SBIR) Phase II project will develop automated methods to extract and tag relevant financial concepts from the free text of financial documents such as an annual reports, press releases and analyst reports. The extracted financial concepts will be semantically mapped (tagged) to a financial taxonomy such as the US-GAAP or IFRS for standardized analysis. There is a growing need in current financial markets for accurate and timely access to relevant financial information for supporting trading and analysis decisions.  At the end of this project, the company's technology is expected to have the capability to provide such information to analysts and decision makers in a timely fashion. The primary goals of this project will be to: (i) build an end-to-end prototype system for automatically extracting financial data from text; (ii) extend the scope of the technology to reach a broader range of real-world applications; (iii) increase accuracy; (iv) reduce the processing time for financial data extraction. The technology will employ machine learning and natural language processing techniques toward financial concept annotation, extraction and semantic tagging to achieve these goals."
63,1445411,I-Corps:  eExplorer - Unstructured Data Analyzer,IIP,I-Corps,7/1/14,6/15/14,Manoj Pooleery,NY,Columbia University,Standard Grant,Rathindra DasGupta,12/31/14,"$50,000.00 ",,manoj@columbia.edu,2960 Broadway,NEW YORK,NY,100276902,2128546851,ENG,8023,,$0.00 ,"Unstructured data spread across a gamut of mediums - electronic communications such as emails, chat transcripts, and telephone transcripts.  The gamut of unstructured documents makes it a time consuming, labor-intensive, and at times an impossible task to derive intelligence from unstructured text. The proposed technology solutions in this proposal will have a significant impact in many different areas. For example, since unstructured data represent up to 80 percent of all data in the Banking and Finance Industry domain, the proposed solutions will provide a means of sifting through this data much faster than the existing ones, resulting in identifying frauds and compliance breaches, and cost savings for taxpayers and financial firms. Similarly, in the Healthcare domain, more efficient representations of digitized medical data can help physicians better tailor treatments by patient history, or identify health trends to benefit public health.<br/><br/>This team is leveraging their research in natural language processing (NLP), machine learning (ML), and social network analysis (specifically for extracting social networks from narrative text using the notion of a 'social event') to build a tool called eExplorer product suite. Rather than trying to convert unstructured text into traditional structured database design, eExplorer creates an adaptable, flexible and dynamic soft-structure on the unstructured text. eExplorer significantly reduces the time between data collection and analysis by supporting an interaction between an analyst and the data. While exploring the data, an analyst may provide examples of the kind of structure he/she wants to impose on the data. Using NLP and ML techniques, eExplorer learns the type of structure an analyst is trying to impose and adds a flexible and soft-structure on the entire data. This has an additional advantage that each analyst may build a different and their own view (or soft-structure) of the data."
64,1426452,NRI: Collaborative Research: Jointly Learning Language and Affordances,IIS,NRI-National Robotics Initiati,8/1/14,8/10/16,Stefanie Tellex,RI,Brown University,Standard Grant,Reid Simmons,7/31/17,"$347,622.00 ",,stefie10@cs.brown.edu,BOX 1929,Providence,RI,29129002,4018632777,CSE,8013,"7218, 8086, 9150",$0.00 ,"The investigators of this project envision a world where robots surround us, in our homes, in our hospitals, and in our factories, helping people by delivering medicine, preparing food, and assembling objects. Achieving this vision requires robots to communicate with people about their needs, and then plan their activities to help meet those needs.  Previous research has addressed these two problems separately, leading to technical solutions that do not work reliably in real-world situations, and to difficulties in human-robot communication.  To solve these problems, we are developing the Physically-Grounded Language with Affordances (PGLA) framework and concentrate our research into two thrusts: 1) enable a robot to observe a patient, then answer a nurse's questions about the patient's activity, and 2) enable a robot to respond to natural language requests in a collaborative cooking task and in a manufacturing setting. We will release our open-source data sets and code, which will have impact in other technical areas beyond robotics, such as computer vision and machine learning.  The results of our proposed research will find direct applications in industries such as manufacturing and assistive robotics.<br/><br/>This project takes a probabilistic approach to jointly learn to recognize affordances in the environment and predict associated natural language requests and descriptions.  Since the affordance map is grounded to perceptual data, our robots will learn to robustly manipulate objects in the physical world, respond to natural language commands, and describe their experiences using words.  Our learning approach enables the robot to infer cross-model knowledge from large data sets of people carrying out activities paired with natural language descriptions of the activities, leveraging the strength of each modality to inform the others. Our novel learning algorithms will integrate and learn from multi-domain databases such as the semantic web, visual scenes, and a novel activity database paired with natural language descriptions."
65,1426744,NRI: Collaborative Research: Jointly Learning Language and Affordances,IIS,National Robotics Initiative,8/1/14,7/9/15,Ashutosh Saxena,NY,Cornell University,Standard Grant,Reid Simmons,7/31/17,"$341,548.00 ",Bart Selman,asaxena@cs.cornell.edu,373 Pine Tree Road,Ithaca,NY,148502820,6072555014,CSE,8013,8086,$0.00 ,"The investigators of this project envision a world where robots surround us, in our homes, in our hospitals, and in our factories, helping people by delivering medicine, preparing food, and assembling objects. Achieving this vision requires robots to communicate with people about their needs, and then plan their activities to help meet those needs.  Previous research has addressed these two problems separately, leading to technical solutions that do not work reliably in real-world situations, and to difficulties in human-robot communication.  To solve these problems, we are developing the Physically-Grounded Language with Affordances (PGLA) framework and concentrate our research into two thrusts: 1) enable a robot to observe a patient, then answer a nurse's questions about the patient's activity, and 2) enable a robot to respond to natural language requests in a collaborative cooking task and in a manufacturing setting. We will release our open-source data sets and code, which will have impact in other technical areas beyond robotics, such as computer vision and machine learning.  The results of our proposed research will find direct applications in industries such as manufacturing and assistive robotics.<br/><br/>This project takes a probabilistic approach to jointly learn to recognize affordances in the environment and predict associated natural language requests and descriptions.  Since the affordance map is grounded to perceptual data, our robots will learn to robustly manipulate objects in the physical world, respond to natural language commands, and describe their experiences using words.  Our learning approach enables the robot to infer cross-model knowledge from large data sets of people carrying out activities paired with natural language descriptions of the activities, leveraging the strength of each modality to inform the others. Our novel learning algorithms will integrate and learn from multi-domain databases such as the semantic web, visual scenes, and a novel activity database paired with natural language descriptions."
66,1409279,TWC SBE: TTP Option: Medium: Collaborative: EPICA: Empowering People to Overcome Information Controls and Attacks,CNS,Secure &Trustworthy Cyberspace,8/1/14,6/27/14,Marshini Chetty,MD,University of Maryland College Park,Standard Grant,Shannon Beck,3/31/17,"$275,000.00 ",,marshini@uchicago.edu,3112 LEE BLDG 7809 Regents Drive,COLLEGE PARK,MD,207425141,3014056269,CSE,8060,"7434, 7924, 9102",$0.00 ,"This project studies the security of representative personalized services, such as search engines, news aggregators, and on-line targeted advertising, and identifies vulnerabilities in service components that can be exploited by pollution attacks to deliver contents intended by attackers.<br/><br/>This project also develops defense-in-depth countermeasures against pollution attacks. These include new server-side mechanisms to prevent the various cross-site-request-forgery schemes that allow an attacker to insert actions. The defense mechanisms also include a distributed data collection, measurement and analysis framework to detect anomalies in browsing behaviors and information contents that are indicative of pollution of user profiles or population preferences. The new information analysis techniques use machine learning and natural language processing to identify differences (e.g., missing information) that are significant or important to a user. The project also develops tools to alert users and guide them to understand and repair profiles, and studies regulatory models to incentivize the industry to adopt a more transparent practice.<br/><br/>This project develops an evaluation framework to facilitate the development and adoption of technologies. The evaluation plan includes user studies involving real, diverse user groups on the Internet.<br/><br/>To transition technologies to practice, this project makes the tools freely available, and deploys data collection and measurement systems on the Internet. This project also educates users about pollution attacks and engages with users to improve the usability of the tools."
67,1433817,Scholarships for Service: Increasing Talented Trus,DGE,CYBERCORPS: SCHLAR FOR SER,8/1/14,7/2/19,Rakesh Verma,TX,University of Houston,Continuing Grant,Victor Piotrowski,7/31/21,"$1,683,274.00 ","Shou-Hsuan Huang, Ernst Leiss, Weidong Shi",rverma@uh.edu,4800 Calhoun Boulevard,Houston,TX,772042015,7137435773,EHR,1668,"7254, 7434, 9178, 9179, SMET",$0.00 ,"The University of Houston (UH) seeks to establish a CyberCorps(R): Scholarship for Service (SFS) program to prepare graduate students for government service in order to help secure, protect and defend our Nation's information systems.  UH proposes to support 3 PhD students and 9 MS students.   UH is classified as a Very High Research University, is classified as a Hispanic-Serving institution, and has been designated as a Center of Academic Excellence in Information Assurance Education since 2009.  This project seeks to address the needs of the Gulf-coast region and beyond, the need of the nation for cybersecurity professionals, and the need for more women and underrepresented minorities in cybersecurity.<br/><br/>The focus of the project is on enriching cybersecurity education with related fields such as data mining, machine learning, statistics and natural language processing.   UH proposes to build a community of well-qualified scholars in trusted computing, provide a rich set of theoretical and applied cybersecurity experiences for the students, foster diversity in the student pool, and disseminate successful models of cybersecurity education to the nation.   The education plan includes the use of cyberwar and laboratory exercises for community building and peer mentoring.  Professional development activities planned include workshops focused on professional skill development, seminars, and external visits to government agencies and research institutes."
68,1409758,TWC SBE: TTP Option: Medium: Collaborative: EPICA: Empowering People to Overcome Information Controls and Attacks,CNS,Secure &Trustworthy Cyberspace,8/1/14,6/27/14,Michael Bailey,MI,Regents of the University of Michigan - Ann Arbor,Standard Grant,Jeremy Epstein,11/30/14,"$225,000.00 ",,mdbailey@illinois.edu,3003 South State St. Room 1062,Ann Arbor,MI,481091274,7347636438,CSE,8060,"7434, 7924",$0.00 ,"This project studies the security of representative personalized services, such as search engines, news aggregators, and on-line targeted advertising, and identifies vulnerabilities in service components that can be exploited by pollution attacks to deliver contents intended by attackers.<br/><br/>This project also develops defense-in-depth countermeasures against pollution attacks. These include new server-side mechanisms to prevent the various cross-site-request-forgery schemes that allow an attacker to insert actions. The defense mechanisms also include a distributed data collection, measurement and analysis framework to detect anomalies in browsing behaviors and information contents that are indicative of pollution of user profiles or population preferences. The new information analysis techniques use machine learning and natural language processing to identify differences (e.g., missing information) that are significant or important to a user. The project also develops tools to alert users and guide them to understand and repair profiles, and studies regulatory models to incentivize the industry to adopt a more transparent practice.<br/><br/>This project develops an evaluation framework to facilitate the development and adoption of technologies. The evaluation plan includes user studies involving real, diverse user groups on the Internet.<br/><br/>To transition technologies to practice, this project makes the tools freely available, and deploys data collection and measurement systems on the Internet. This project also educates users about pollution attacks and engages with users to improve the usability of the tools."
69,1505790,TWC SBE: TTP Option: Medium: Collaborative: EPICA: Empowering People to Overcome Information Controls and Attacks,CNS,Secure &Trustworthy Cyberspace,8/1/14,10/30/14,Michael Bailey,IL,University of Illinois at Urbana-Champaign,Standard Grant,Shannon Beck,7/31/18,"$225,000.00 ",,mdbailey@illinois.edu,1901 South First Street,Champaign,IL,618207406,2173332187,CSE,8060,"7434, 7924",$0.00 ,"This project studies the security of representative personalized services, such as search engines, news aggregators, and on-line targeted advertising, and identifies vulnerabilities in service components that can be exploited by pollution attacks to deliver contents intended by attackers.<br/><br/>This project also develops defense-in-depth countermeasures against pollution attacks. These include new server-side mechanisms to prevent the various cross-site-request-forgery schemes that allow an attacker to insert actions. The defense mechanisms also include a distributed data collection, measurement and analysis framework to detect anomalies in browsing behaviors and information contents that are indicative of pollution of user profiles or population preferences. The new information analysis techniques use machine learning and natural language processing to identify differences (e.g., missing information) that are significant or important to a user. The project also develops tools to alert users and guide them to understand and repair profiles, and studies regulatory models to incentivize the industry to adopt a more transparent practice.<br/><br/>This project develops an evaluation framework to facilitate the development and adoption of technologies. The evaluation plan includes user studies involving real, diverse user groups on the Internet.<br/><br/>To transition technologies to practice, this project makes the tools freely available, and deploys data collection and measurement systems on the Internet. This project also educates users about pollution attacks and engages with users to improve the usability of the tools."
70,1409635,TWC SBE: TTP Option: Medium: Collaborative: EPICA: Empowering People to Overcome Information Controls and Attacks,CNS,Secure &Trustworthy Cyberspace,8/1/14,3/16/15,Wenke Lee,GA,Georgia Tech Research Corporation,Standard Grant,Nina Amla,7/31/18,"$1,100,000.00 ","Hongyuan Zha, Hans Klein, Nicholas Feamster",wenke@cc.gatech.edu,Office of Sponsored Programs,Atlanta,GA,303320420,4048944819,CSE,8060,"7434, 7924",$0.00 ,"This project studies the security of representative personalized services, such as search engines, news aggregators, and on-line targeted advertising, and identifies vulnerabilities in service components that can be exploited by pollution attacks to deliver contents intended by attackers.<br/><br/>This project also develops defense-in-depth countermeasures against pollution attacks. These include new server-side mechanisms to prevent the various cross-site-request-forgery schemes that allow an attacker to insert actions. The defense mechanisms also include a distributed data collection, measurement and analysis framework to detect anomalies in browsing behaviors and information contents that are indicative of pollution of user profiles or population preferences. The new information analysis techniques use machine learning and natural language processing to identify differences (e.g., missing information) that are significant or important to a user. The project also develops tools to alert users and guide them to understand and repair profiles, and studies regulatory models to incentivize the industry to adopt a more transparent practice.<br/><br/>This project develops an evaluation framework to facilitate the development and adoption of technologies. The evaluation plan includes user studies involving real, diverse user groups on the Internet.<br/><br/>To transition technologies to practice, this project makes the tools freely available, and deploys data collection and measurement systems on the Internet. This project also educates users about pollution attacks and engages with users to improve the usability of the tools."
71,1423915,Collaborative Project: Enriching Security Curricula and Enhancing Awareness of Security in Computer Science and Beyond,DGE,CYBERCORPS: SCHLAR FOR SER,3/1/14,7/6/17,Ping Chen,MA,University of Massachusetts Boston,Standard Grant,Victor Piotrowski,8/31/18,"$146,078.00 ",,ping.chen@umb.edu,100 Morrissey Boulevard,Dorchester,MA,21253300,6172875370,EHR,1668,"7254, 9178",$0.00 ,"This project is building our nation's capacity to develop a workforce well trained in computer security.  Faculty from the University of Houston, University of Houston-Downtown and Texas Southern University, are developing interactive courseware and conducting workshops on the latest machine learning, data mining, natural language processing and statistics techniques as applied to computer security.  Courseware on security-critical domains such as wireless sensor networks, and on key societal privacy and security concerns is also being developed. Workshops are being organized to cross-fertilize security curricula with related fields and topics and to develop a broad-based consensus on courseware contents.  In addition to interactive courseware for security courses, materials are being developed to infuse security topics and increase awareness of information assurance in the broader computer science curriculum and, beyond computer science, in other STEM disciplines.  A hypertextbook integrating the courseware is being implemented.<br/><br/>Through faculty workshops, a community of information assurance education and research scholars is being created.  This community will build bridges between information assurance professionals and professionals in related fields. The workshops are training diverse groups of faculty from different types of institutions.  <br/><br/>Courseware and other results are available in the National Science Digital Library as well as at a project-specific web site which also links to exemplary courseware developed elsewhere.  Courseware and workshops are being rigorously evaluated."
72,1421943,RI: Small: Using Humans in the Loop to Collect High-quality Annotations from Images and Time-lapse Videos of Cells,IIS,"ADVANCES IN BIO INFORMATICS, Information Technology Researc, Cross-BIO Activities, Robust Intelligence",8/1/14,6/26/15,Margrit Betke,MA,Trustees of Boston University,Continuing Grant,Jie Yang,10/31/18,"$370,151.00 ",,betke@cs.bu.edu,881 COMMONWEALTH AVE,BOSTON,MA,22151300,6173534365,CSE,"1165, 1640, 7275, 7495","7495, 7923, 8750",$0.00 ,"Sequences of microscopy images of live cells are analyzed by cell biologists to understand cellular processes, for example, to prevent cancer or design bio-materials for wound healing.  Research progress is slowed or compromised when scientists find the image analysis efforts too labor-intensive to do themselves and the automation methods too numerous, unreliable, or difficult to use.  The project develops image-analysis software to leverage human and computer resources together, in particular on the internet, to create high-quality image interpretations.  Live-cell imaging studies support basic research to understand cellular processes and design biomaterials. The work on statistically significant performance evaluation can have broad impact on the research methodology in computer vision.<br/><br/>The research explores how human and computer resources can be leveraged together, in particular on the internet, to interpret images and videos of cells.  Initially, an expansive benchmark study of detection, segmentation, and tracking algorithms for analyzing images of live cells is conducted. Computer-vision approaches to address the major challenges for existing algorithms are then developed, for example, to interpret the emergence of new cells due to mitosis in time-lapse microscopy videos. Methods are designed for quantifying the quality of automatically and manually obtained annotations and the variability between multiple annotations.  A tool is built to effectively and efficiently use the expertise of domain specialists, in particular, cell biologists, to judge and select automated methods that analyze cell images.  Crowd-sourcing experiments in which internet workers analyze images are designed and conducted.  The quality of these lay workers' annotations is compared to the quality of annotations by domain experts and automated methods.  Finally, a machine learning system is developed that automatically determines which types of cell images or videos can be analyzed accurately with or without human involvement."
73,1422591,III: Small: Algorithmic Techniques for Determining Alterations in the Patterns of Chromosome Spatial Organization inside the Cell Nucleus,IIS,Info Integration & Informatics,8/1/14,7/14/15,Jinhui Xu,NY,SUNY at Buffalo,Continuing grant,Sylvia Spengler,7/31/18,"$499,968.00 ",Ronald Berezney,jinhui@buffalo.edu,520 Lee Entrance,Buffalo,NY,142282567,7166452634,CSE,7364,"7364, 7923",$0.00 ,"Recent research in cell biology has led to the emerging view that the 3-D arrangement of chromosome territories (CT) and the spatial positioning of genes within these territories are linked to genomic function and regulation. Despite this progress, it is not clear what is the spatial organization patterns of CTs inside the cell nucleus and how such patterns dynamically change during the cell cycle and at different stages of cell differentiation and cancer progression. To facilitate more in-depth studies of these important biological problems, in this project, the Pi will develop a set of efficient algorithmic techniques for determining the patterns and their alterations of three chromosome organization problems.  First, how are chromosomes spatially positioned inside the nucleus?  Second, how are chromosomes neighboring or associating with each other? Third, what is the internal structure of each individual chromosome? The core of this project is to develop efficient algorithms for solving a set of challenging computational problems which are essential for the chromosome organization problems, thus addressing the emerging science at the interface of computing and biology.  This project will bring research and educational opportunities to both graduate and undergraduate students. It will involve several Ph.D. students (including  2 female students), and one or two undergraduate students, from both the Computer Science and Engineering Department and Biological Sciences Department. As an integral part of this project, a new course in biomedical imaging will be developed that will enable the solving of biomedical problem with a knowledge of computer science.<br/><br/>This project will yield a set of efficient algorithmic techniques for the proposed problems, and will be used as automatic (or semi- automatic) tools to accurately determine the patterns of chromosome spatial organization inside the cell nucleus and how such patterns dynamically change during the normal cell cycle, during differentiation of keratinocytes into skin cells and following progression of normal breast cells to malignant cancer. This could provide new insight into how the global arrangement of chromosomes in the cell nucleus is related to the malignant state.  The set of algorithms will be implemented and tested using randomly generated data and real biological data. They will be integrated into an Algorithmic Toolbox developed in our previous research on the spatial positioning of the cell nucleus, and will be made available to the research community. (3) Results from this projects are likely to be used in other areas, and have a positive impact on them. For example, the problems of k-prototype learning, chromatic median, chromatic k-median clustering, median point-sets, and projective clustering are all fundamental problems in computer science and have applications in many other areas, such as machine learning, computer vision, and data mining. Some of the solutions can be used as information integration tools."
74,1409431,RI: Medium: Deep Neural Networks for Robust Speech Recognition through Integrated Acoustic Modeling and Separation,IIS,Robust Intelligence,6/1/14,4/19/16,Eric Fosler-Lussier,OH,Ohio State University,Continuing Grant,Tatiana Korelsky,5/31/19,"$798,082.00 ","DeLiang Wang, Michael Mandel",fosler@cse.ohio-state.edu,Office of Sponsored Programs,Columbus,OH,432101016,6146888735,CSE,7495,"7495, 7924",$0.00 ,"Over the last decade, speech recognition technology has become steadily more present in everyday life, as seen by the proliferation of applications including mobile personal agents and transcription of voicemail messages. Performance of these systems, however, degrades significantly in the presence of background noise; for example, using speech recognition technology in a noisy restaurant or on a windy street can be difficult because speech recognizers confuse the background noise with linguistic content. Compensation for noise typically involves preprocessing the acoustic signal to emphasize the speech signal (i.e. speech separation), and then feeding this processed input into the recognizer.  The innovative approach in this project is to train the recognition and separation systems in an integrated manner so that the linguistic content of the signal can inform the separation, and vice versa. <br/><br/>Given the impact of the recent resurgence of Deep Neural Networks (DNNs) in speech processing, this project seeks to make DNNs more resistant to noise by integrating speech separation and speech recognition, exploring three related areas.  The first research area seeks to stabilize input to DNNs by combining DNN-based suppression and acoustic modeling, integrating masking estimates across time and frequency, and using this information to improve reconstruction of speech from noisy input.  The second area seeks to examine a richer DNN structure, using multi-task learning techniques to guide the construction of DNNs better at performing all tasks and where layers have meaningful structure.  The final research area examines ways to adapt the spurious output of DNN acoustic models given acoustic noise.  With the focus of integrating speech separation and recognition, the project will be evaluated both by measuring speech recognition performance, as well as metrics that are more closely related to human speech perception.  This will ensure a broader impact of this research by providing insights not only to speech technology but also facilitating the design of next-generation hearing technology in the long run."
75,1422324,AF: Small: Algorithmic Techniques for Several Geometric Problems Arising in Biomedical Imaging Applications,CCF,Algorithmic Foundations,9/1/14,7/21/15,Jinhui Xu,NY,SUNY at Buffalo,Standard Grant,Rahul Shah,8/31/18,"$480,349.00 ",,jinhui@buffalo.edu,520 Lee Entrance,Buffalo,NY,142282567,7166452634,CSE,7796,"7923, 7929",$0.00 ,"Recent progress in biomedicine has heavily relied on computer science technology. As the territory of biomedicine is rapidly enlarging, more powerful computational techniques are needed to foster its continuous growth. This project is for developing efficient computer algorithms for a set of geometric problems arising in several biomedical imaging applications. Particularly, it will design algorithms for three challenging problems (as well as their related problems): (1) generalizations of Voronoi diagram (which is a fundamental structure in geometry) called Clustering Induced Voronoi Diagram (CIVD), (2) general forms of matching in geometric settings based on Earth Mover's Distance (EMD), and (3) uniform framework for various types of constrained clustering problems. Problem (1) is for developing a mathematical model for understanding the interaction of dendritic cells and T cells in the immune system. Problem (2) is motivated by a medical imaging application of combining anatomic and functional images of moving organs for better imaging quality. Problem (3) aims to achieve better  accuracy in image classifications by using a priori knowledge.<br/><br/>This project will use computational geometry techniques to develop novel algorithms for the proposed problems. It will introduce several general algorithmic techniques to the area of computational geometry, enriching and prodding its further development. These algorithmic techniques are also likely to be used in other areas, such as machine learning, computer vision, information security, data mining, and pattern recognition, and bring new ideas to these areas. This project could lead to several long term impacts. It could potentially help understanding the immune function and dysregulation in cancer, improving our ability to recovering fast organ motion (e.g., cardiac and lung motion), and achieving more accurate image classification in various applications."
76,1359170,REU Site in Computer Systems,CNS,RSCH EXPER FOR UNDERGRAD SITES,3/1/14,2/24/14,Zachary Dodds,CA,Harvey Mudd College,Standard Grant,Anindya Banerjee,2/28/18,"$358,068.00 ",Benjamin Wiedermann,dodds@cs.hmc.edu,301 Platt Boulevard,CLAREMONT,CA,917115901,9096218121,CSE,1139,9250,$0.00 ,"This funding renews a Research Experience for Undergraduate (REU) site at Harvey Mudd College. The site brings 10 undergraduates in order to engage them in research and encourage graduate study in computer science. The intellectual focus of the site is research in computer systems. The program provides a microcosm of the graduate experience through four broadly-scoped systems projects.  Students have a chance to gain experience in all aspects of the research process, including its social aspects. The site focuses on recruiting a sufficiently diverse pool of students, including women and underrepresented minorities.<br/><br/>The intellectual merit of the proposal rests with the leadership of an experienced team of mentors with excellent expertise and experience in the research areas. The site focuses on four main projects. The first project develops algorithms for estimating cophylogenetic trees based on biological host/parasite data. Students' work is encapsulated in a software package named Jane, currently in use by dozens of biology research groups. The second effort investigates approaches for automatic synthesis of music scores, furthering our understanding of computational music research. It leverages the Impro-Visor software, which has many thousands of users. The third project explores program analyses to support new, accessible domain-specific languages, secure information flow in industry-standard languages such as Javascript, and visualization tools for both sides of that programming spectrum. The fourth project tests machine-learning and computer vision-based algorithms that improve the state-of-the-art of performance of low-cost robots, such as sub-$300 quadcopters and laser-guided wheeled platforms.<br/><br/>The broader impacts include the site's emphasis on the broad human and research impacts of pursuing computer science at the graduate level -- especially for participants who had not previously considered graduate work. Challenging research problems prompt students' guided development of research skills: investigation, presentation, and publication. One-on-one introductions by advisors transition to student-led talks and culminate with publications and conference experiences during or after the summer. The program's most important impact is its cultivation of the next generation of creative and enthusiastic computer science researchers. The evaluation plan and the student research artifact archiving plan should contribute to educational research on effective structures for involving undergraduate students in research."
77,1353532,SBIR Phase II:  Computer Aided Prognosis of Debilitating Disease,IIP,SBIR Phase II,4/1/14,3/8/18,Andrew Buckler,MA,Elucid Bioimaging Inc.,Standard Grant,Ruth Shuman,3/31/18,"$1,353,339.00 ",,andrew.buckler@vascuvis.com,"225 Main Street, Suite 15",Wenham,MA,19841619,9784680508,ENG,5373,"116E, 165E, 169E, 5373, 8038, 8240, 9251",$0.00 ,"This Small Business Innovation Research (SBIR) Phase II project proposes to develop robust and effective imaging techniques for assessment of atherosclerotic disease severity for prognostic and longitudinal use. In the United States alone, approximately 5 million patients suffer pre-stroke symptoms of which 795,000 go on to a stroke annually. About 610,000 of these are first or new strokes, while the remainder are recurrent strokes. Despite these statistics, there is no effective test to tell who will or will not suffer acute events or to measure whether medical therapies are effective at reducing the risk. In this project, multivariate quantitative descriptors are developed using data from controlled outcome studies on a specialized model to discover and validate prognostic signatures that, in composite, perform well in both cross-sectional prognostic and longitudinal applications with high predictive value.  Phase I results are extended to support histologically verifiable tissue types, expanding the functional and performance attributes of the product with a tie to localized ground truth maps made possible with co-registration of histology with MRI. The plan is to iteratively validate the developed capability under commercially accepted design controls.  <br/><br/>The broader impact/commercial potential of this project will be the development of effective means for computer-aided prognostics using quantitative imaging phenotypes. Physicians face a complex and heterogeneous series of clinical manifestations of disease. Because disease arises through a complex interaction of multiple molecular signals and pathways often confounding the eventual effect, tools and approaches are needed to identify key pathways that reflect the underlying pathological processes. Functional imaging modalities have recently emerged for characterization of these disease processes and to obtain a better mechanistic understanding of the underlying biologic processes to distinguish more aggressive from less aggressive disease phenotypes. Computer-aided prognosis (CAP) of disease is a new and exciting complement to the field of computer-aided diagnosis (CAD). Since CAP approaches distinguish between different subtypes of a particular disease (as opposed to CAD schemes trying to distinguish diseased from benign processes), there is a need for more sophisticated image analysis, computer vision, and machine learning methods to identify subtle disease signatures that can separate unstable from stable disease.  The chosen application in this project is a use case that has the potential to radically increase the power of applications to support clinicians in pursuit of personalized medicine."
78,1420864,SHF: Small: Automatic High-Level Synthesis of Approximate Computing Circuits,CCF,Software & Hardware Foundation,7/1/14,5/25/16,Sherief Reda,RI,Brown University,Standard Grant,Sankar Basu,6/30/18,"$457,991.00 ",Ruth Bahar,Sherief_Reda@brown.edu,BOX 1929,Providence,RI,29129002,4018632777,CSE,7798,"7923, 7945, 9251",$0.00 ,"Reducing the power consumption of computing devices is a highly desirable goal for modern digital circuits. This project seeks new methods to lower power consumption of digital circuits by exploiting the inherent error resiliency of emerging application domains such as signal and image processing, computer vision, and machine learning. By giving up some arithmetic accuracy, it is possible to design approximate circuits with dramatically lower power consumption and smaller silicon footprint. Low-power and reliable operation of computing systems is key for the continued push for smaller, faster and increased functionality for these systems. Other than the obvious technical impact to industry, broader impacts of this project would include new and existing course development and development of a technical workforce, as it will involve a number of graduate and undergraduate students.<br/><br/>This project investigates new methods for the synthesis of approximate circuits that will be generated directly from their high-level behavioral descriptions. Working directly from high-level behavioral descriptions gives larger leverage for approximate circuit generation and easier incorporation within standard design flows.  The novel synthesis approach is inspired by program analysis techniques used in software engineering. The PIs propose: (1) techniques for intelligent analysis of hardware designs to produce approximate circuits where designers can trade-off power and accuracy in a controlled fashion; (2) techniques that enable fast and efficient design space exploration of approximate computing designs; and (3) support circuitry that leads to low-area overhead for either low-power or error-resilient deployments. This project will result in a software tool that will fit within standard integrated circuit design flows, and will take as input a system accurately described in a behavioral hardware description language and will produce as an output approximate computing circuits along with their support circuitry.  The tool will enable designers to make appropriate tradeoff decisions between arithmetic accuracy, power consumption, error resiliency and hardware overhead."
79,1439718,I/UCRC Phase I: iPerform - I/UCRC for Assistive Technologies to Enhance Human Performance,CNS,IUCRC-Indust-Univ Coop Res Ctr,9/15/14,1/29/19,Ovidiu Daescu,TX,University of Texas at Dallas,Continuing Grant,Behrooz Shirazi,8/31/20,"$325,000.00 ","Dinesh Bhatia, Gopal Gupta, Balakrishnan Prabhakaran",daescu@utdallas.edu,"800 W. Campbell Rd., AD15",Richardson,TX,750803021,9728832313,CSE,5761,"5761, 8039",$0.00 ,"The I/UCRC for Assistive Technologies (AT) will attract support for the advancement of AT research and promote innovation in academia that is driven by industrial needs. In work environments, ATs can improve work efficiency, identify safety risks, shorten the learning curve or worker training through simulations, and improve resource allocation, creativity and communication. In healthcare environments, AT tools can enhance sensory and cognitive capabilities, improve training & delivery, enable remote monitoring, delay physical and cognitive decline in chronic conditions, personalize rehabilitation, predict risks for the elderly who live alone, monitor sleep disorders, design better prosthetics or drugs, design better robotic assistants, smart wheelchairs, therapy games and tools to monitor mental/physiological conditions, such as depression, epilepsy, or heart problems. The seed projects conducted within the center will help advance basic research in computer vision, machine learning, user interfaces, brain imaging, human robot interaction, human computer interaction, virtual reality, simulation, and many other related research areas.<br/><br/>The projects conducted at I/UCRC for Assistive Technologies will drive a broad spectrum of advances in the areas such as worker productivity and safety, transportation, health, company operations and intelligence, and promote the development of AT research infrastructure. The center addresses real-world problems and thus can generate new jobs, products, services, and impact all areas where a human has the potential to improve. The center will play role in enhancing the quality and diversity of AT professionals and prepare a future generation of competitive employees-scientists who can solve problems due to unmet human needs. Through compelling projects, the center will also attract students to CSE fields. UTA and UTD have a strong record in training students and have ongoing NSF projects, e.g., to identify software errors, analysis of facial expressions to identify arthritic pain, or efficient multimodal database searches."
80,1439645,I/UCRC Phase I: iPerform - I/UCRC for Assistive Technologies to Enhance Human Performance,CNS,"Special Projects - CNS, IUCRC-Indust-Univ Coop Res Ctr",9/15/14,9/13/19,Fillia Makedon,TX,University of Texas at Arlington,Continuing Grant,Behrooz Shirazi,5/31/21,"$638,290.00 ","David Kung, Gautam Das, Hong Jiang, Vassilis Athitsos, Christoph Csallner, Gian Luca Mariottini, Shouyi Wang, Vangelis Metsis",makedon@cse.uta.edu,"701 S Nedderman Dr, Box 19145",Arlington,TX,760190145,8172722105,CSE,"1714, 5761","116E, 5761, 8039, 9231, 9251",$0.00 ,"The I/UCRC for Assistive Technologies (AT) will attract support for the advancement of AT research and promote innovation in academia that is driven by industrial needs. In work environments, ATs can improve work efficiency, identify safety risks, shorten the learning curve or worker training through simulations, and improve resource allocation, creativity and communication. In healthcare environments, AT tools can enhance sensory and cognitive capabilities, improve training & delivery, enable remote monitoring, delay physical and cognitive decline in chronic conditions, personalize rehabilitation, predict risks for the elderly who live alone, monitor sleep disorders, design better prosthetics or drugs, design better robotic assistants, smart wheelchairs, therapy games and tools to monitor mental/physiological conditions, such as depression, epilepsy, or heart problems. The seed projects conducted within the center will help advance basic research in computer vision, machine learning, user interfaces, brain imaging, human robot interaction, human computer interaction, virtual reality, simulation, and many other related research areas.<br/><br/>The projects conducted at I/UCRC for Assistive Technologies will drive a broad spectrum of advances in the areas such as worker productivity and safety, transportation, health, company operations and intelligence, and promote the development of AT research infrastructure. The center addresses real-world problems and thus can generate new jobs, products, services, and impact all areas where a human has the potential to improve. The center will play role in enhancing the quality and diversity of AT professionals and prepare a future generation of competitive employees-scientists who can solve problems due to unmet human needs. Through compelling projects, the center will also attract students to CSE fields. UTA and UTD have a strong record in training students and have ongoing NSF projects, e.g., to identify software errors, analysis of facial expressions to identify arthritic pain, or efficient multimodal database searches."
81,1405985,CI-P: Planning for SMART-MOVE: A Spatiotemporal Annotated Human Activity Repository for Advanced Motion Recognition and Analysis Research,CNS,"Special Projects - CNS, CCRI-CISE Cmnty Rsrch Infrstrc",9/1/14,4/10/15,Fillia Makedon,TX,University of Texas at Arlington,Standard Grant,Jie Yang,8/31/17,"$116,000.00 ","Heng Huang, Vassilis Athitsos, Junzhou Huang, Vangelis Metsis",makedon@cse.uta.edu,"701 S Nedderman Dr, Box 19145",Arlington,TX,760190145,8172722105,CSE,"1714, 7359","7359, 7495, 9251",$0.00 ,"Accurate human motion tracking and activity recognition are important in supporting numerous areas of computer science and engineering research, ranging from user modeling and human robot interaction to graphics and animation. A good resource of annotated datasets and annotation tools is particularly important in research involving physical human limitations, persons with chronic disabilities, such as post-stroke, ALS, Rheumatoid Arthritis, Cerebral Palsy, and others. Currently, a comprehensive community CRI with human activity data and analysis tools is not in place. The development of new methods and algorithms that will improve and enable real-world motion tracking applications is hampered by an inherent difficulty: the lack of large sets of training and testing data. This planning activity brings together experts in computer vision, machine learning, data mining, user interface design, assistive environments, human robot interaction, databases research, big data involving real time human activity, therapists, clinicians, device makers and sensor developers in order to identify the specific human activity data that should be collected and processed when building a repository of automatically and accurately annotated video data of human motion.<br/><br/>Specifically, a repository of automatically and accurately annotated video data of human motion has the potential to impact the research of several disciplines using human motion tracking and recognition by significantly improving the ability to recognize behavioral markers for assessing the confluence of environment, drugs, human psychology and thus extend clinical and psychological areas. However, due to big data issues (volume, velocity, variety, veracity, arising from multisensory visual input of human behavior), the tasks of automating data collection and especially annotation are non-trivial. Traditionally, video data of human motion are collected and annotated manually. Due to the large amount of data generated by video sensors, this task is very tedious and error prone. For this reason, most existing datasets are very small and specialized to the problem for which they were collected. This project will gather input from the research user communities on how to best develop such a CRI repository of human activity.  A workshop will be organized with invited experts who will help identify the specific human activity data that should be collected and processed. The input solicited during the workshop will help define the features and facilities of the proposed Smart-Move repository, a spatiotemporal annotated human motion repository by focusing on the following: (1) determining the specific activities to be captured and the methods of multisensory data collection; (2) tools for accurate real-time annotation and storage of the collected data;  (3) search facilities to enable researchers to contribute and provide feedback;  (4) event recognition and data summarization tools to enable researchers to analyze their own new big data automatically; (5) feasibility of the proposed data collection and analysis using a strategically chosen set of multi-stream data to keep the overall project cost low; (6) limitations when modeling heterogeneous types of human activity data collected by different sensors or of different sample frequencies."
82,1407156,SCH: EXP: Cost Efficient Osteoporosis Analysis using Dental Data,IIS,Smart and Connected Health,8/1/14,8/4/14,Haibin Ling,PA,Temple University,Standard Grant,Jie Yang,7/31/19,"$595,797.00 ","Jie Yang, Vasileios Megalooikonomou",hling@cs.stonybrook.edu,1801 N. Broad Street,Philadelphia,PA,191226003,2157077547,CSE,8018,"8018, 8061",$0.00 ,"This project investigates low-cost osteoporosis prescreening methods using dental data, which are collected during routine dental examination and thus at no additional cost. In particular, when a senior citizen attends the dental office for routine treatment, the proposed methods assess the evidence of osteoporosis based on collected data such as dental radiographs. The senior citizen is referred to a formal osteoporotic examination if high risk is found. Towards this goal, the project conducts three major research activities including systematical validation of the relation between dental data and bone quality measurement, dental image-based osteoporosis analysis, and integration of longitudinal and categorical information for osteoporosis prescreening. Decrease in bone quality causes major health problems in the United States. In particular, it has been estimated that osteoporosis afflicts 55% of Americans aged 50 and above. Early diagnosis of osteoporosis requires routine examination since no obvious symptom is associated with diagnosis before serious consequences, e.g., bone fracture, happen. Such routine examination can cause a big economic burden, since the data used in the current gold standard (i.e., dual energy X-ray absorptiometry) is not cost efficient to collect. <br/><br/>This project develops image analysis and machine learning methods for low-cost osteoporosis prescreening methods using dental data. The research advances science in both computational and clinical fields. In particular, it serves as an exemplary model of using routinely collected dental data for low-cost smart health assessment. Moreover, the specific techniques exploited or invented in this project can be easily generalized to other related clinical and non-clinical domains. In addition, the data analytics algorithms can be of general interest in many areas of science and engineering such as computer vision, medical image analysis, data mining, climate evolution, etc. The education activities of the project are tightly integrated with the research activities, by training and teaching students of different levels, disseminating research results to general audience, and involving under-represented students in research."
83,1431105,Making words disappear or appear: A neurocognitive and behavioral investigation of effects of speech rate on spoken word recognition,BCS,"Perception, Action & Cognition",8/15/14,8/14/14,Navin Viswanathan,NY,SUNY College at New Paltz,Standard Grant,Betty Tuller,10/31/15,"$50,000.00 ",,mailnavin@gmail.com,"Sponsored Programs, HAB 805",New Paltz,NY,125612443,8452573282,SBE,7252,"7252, 7298, 9251",$0.00 ,"Understanding how humans comprehend speech is an unsolved and challenging problem, in part because factors such as different speakers, dialects, and speaking rates introduce a great deal of temporal and spectral variability into the speech signal. The focus of this research is on the influence of temporal context on perception of segments, syllables, and words. Results of the research may offer insights into treatment of disorders that involve disruption of speech rate (e.g., dysarthria, stuttering, Parkinson's disease, and aphasia), inform approaches to improve speech technology applications (e.g., enhanced automatic speech recognition, more natural sounding computer-generated speech), and lead to new discoveries related to brain mechanisms involved in understanding spoken language. The investigators will also involve students in the research, including those from a primarily undergraduate institution collaborating on the project.<br/><br/>The investigators will test different accounts of temporal phenomena in the perception of speech. They propose two interacting cognitive mechanisms controlling phenomena at lexical and phonetic levels, each driven by a different neural timing mechanism. The hypothesis is that effects of lexical rate primarily stem from top-down, speech-specific temporal expectancies, while phonetic rate effects originate in bottom-up, transient rhythmic expectations that are not specific to speech. This hypothesis will be assessed using psychoacoustic studies, non-invasive measures of brain activity, and theoretical modeling in order to identify the processing characteristics revealed by neural representations of temporal properties of speech."
84,1435831,Collaborative Research: Effects of production variability on the acoustic consequences of coordinated articulatory gestures,BCS,Linguistics,9/1/14,8/14/14,Vikramjit Mitra,CA,SRI International,Standard Grant,William Badecker,8/31/16,"$32,973.00 ",,vmitra@speech.sri.com,333 RAVENSWOOD AVE,Menlo Park,CA,940253493,7032478529,SBE,1311,1311,$0.00 ,"The human voice, our oldest and most reliable communication tool, is now rapidly becoming the input interface of choice that we use everyday to interact with technologies such as car navigation systems, medical  and legal dictation systems, personal assistants like ""Siri,"" automated financial systems, etc.  Thousands of 'apps' have been developed to help consumers use voice to get the information they are looking for.   Speech recognition is the backbone of all of these technologies.  As a result, the performance of speech recognizers is key for customer satisfaction.  Currently, many systems still need to be tuned for a particular speaker to perform well, and the recognition task has be limited in other ways such as requiring (1) usage of a specific vocabulary,  (2) clear pronunciation of most of the words, especially the content words and (3) limited background noise. In this research, speech variability will be studied, and methods and models will be developed that will enable recognizers to be more speaker independent and capable of handling the full range of speech styles from clear articulation to very casually spoken speech. The results will also bear on linguistic models of speech planning and organization, providing evidence for how speakers trade off efficiencies in the production of speech against the need to be intelligible.<br/><br/>In this project, point source tracking of the speech articulators will be collected concurrently with the corresponding acoustics.  Speakers will record speech at both a normal and rapid pace (the purpose of the latter is to increase significantly the degree of variability in the signal).  This data will allow for the investigation of whether speakers always move their speech articulators in the direction of a desired target (e.g. tongue tip to teeth in producing /t/) even when a rapid production pace occludes the relevant acoustic information (as in ""perfect"").  If confirmed, this finding will point the way towards making recognition systems more robust through the incorporation of articulatory information.  In addition, such data will support the development of a speech inversion system capable of 'uncovering' hidden articulatory movements potentially masked from the acoustics."
85,1436600,Collaborative Research: Effects of production variability on the acoustic consequences of coordinated articulatory gestures,BCS,"LINGUISTICS, ROBUST INTELLIGENCE",9/1/14,8/14/14,Carol Espy-Wilson,MD,University of Maryland College Park,Standard Grant,William J. Badecker,8/31/16,"$132,362.00 ",,espy@eng.umd.edu,3112 LEE BLDG 7809 Regents Drive,COLLEGE PARK,MD,207425141,3014056269,SBE,"1311, 7495","1311, 7495, 9179",$0.00 ,"The human voice, our oldest and most reliable communication tool, is now rapidly becoming the input interface of choice that we use everyday to interact with technologies such as car navigation systems, medical  and legal dictation systems, personal assistants like ""Siri,"" automated financial systems, etc.  Thousands of 'apps' have been developed to help consumers use voice to get the information they are looking for.   Speech recognition is the backbone of all of these technologies.  As a result, the performance of speech recognizers is key for customer satisfaction.  Currently, many systems still need to be tuned for a particular speaker to perform well, and the recognition task has to be limited in other ways such as requiring (1) usage of a specific vocabulary,  (2) clear pronunciation of most of the words, especially the content words and (3) limited background noise. In this research, speech variability will be studied, and methods and models will be developed that will enable recognizers to be more speaker independent and capable of handling the full range of speech styles from clear articulation to very casually spoken speech. The results will also bear on linguistic models of speech planning and organization, providing evidence for how speakers trade off efficiencies in the production of speech against the need to be intelligible.<br/><br/>In this project, point source tracking of the speech articulators will be collected concurrently with the corresponding acoustics.  Speakers will record speech at both a normal and rapid pace (the purpose of the latter is to increase significantly the degree of variability in the signal).  This data will allow for the investigation of whether speakers always move their speech articulators in the direction of a desired target (e.g. tongue tip to teeth in producing /t/) even when a rapid production pace occludes the relevant acoustic information (as in ""perfect"").  If confirmed, this finding will point the way towards making recognition systems more robust through the incorporation of articulatory information.  In addition, such data will support the development of a speech inversion system capable of 'uncovering' hidden articulatory movements potentially masked from the acoustics."
86,1350550,CAREER: More than Words: Advancing Prosodic Analysis,IIS,ROBUST INTELLIGENCE,6/1/14,2/6/17,Andrew Rosenberg,NY,CUNY Queens College,Continuing grant,Tatiana D. Korelsky,2/28/17,"$37,741.00 ",,Andrew@cs.qc.cuny.edu,65 30 Kissena Blvd,Flushing,NY,113671575,7189975400,CSE,7495,"1045, 7495",$0.00 ,"Prosody is an essential component of human speech. Whereas the words are ""what is said"", prosody is ""how it is said"".  A wealth of information is communicated via prosody including information about a speaker's intent and state (speaking-style and emotion).  To advance the capabilities of machines to understand human speech, this CAREER project develops new representations of prosody and applies them to a variety of spoken language processing tasks:  word recognition, speaking-style recognition, dialog-act classification and speaker identification.  This project employs and advances semi-supervised and unsupervised representation learning techniques to characterize prosody.  This project also investigates prosody across multiple languages.  Speakers of multiple languages contribute speech and annotate some basic prosodic phenomena (phrasing and prominence).  The overarching goal is to identify a compact and universal representation of prosody that will be employed effectively in spoken language processing tasks across languages.  Scientific results, representations and tools for extraction will be made open-source as will the collected, annotated multi-lingual data.<br/><br/>Speech recognition is being integrated into our lives through mobile devices and spoken dialog systems.  The next great hurdle in the ability to communicate with machines via speech is understanding prosody.  Taking prosody into account will result in machines understanding humans better; conversely, automatically generating adequate prosody to convey intent will allow machines to sound more human.  Both types of improvement are sorely needed as automated conversation agents and robots are starting to become a part of our everyday lives.  Finally, this project implements an innovative and challenging education plan that is well-integrated with its research.  It includes curricula modules on prosodic analysis and representation learning to be widely disseminated.  Moreover, undergraduate students who provide and annotate speech samples for the project will get a hands-on introduction to computer science research, and will be compensated in part with tuition waivers for introductory courses in computer science."
87,1435592,Collaborative Proposal:  Effects of production variability on the acoustic consequences of coordinated articulatory gestures,BCS,LINGUISTICS,9/1/14,8/14/14,Mark Tiede,CT,"Haskins Laboratories, Inc.",Standard Grant,William J. Badecker,8/31/17,"$105,308.00 ",,tiede@haskins.yale.edu,300 George Street,New Haven,CT,65116610,2038656163,SBE,1311,"1311, 9179",$0.00 ,"The human voice, our oldest and most reliable communication tool, is now rapidly becoming the input interface of choice that we use everyday to interact with technologies such as car navigation systems, medical  and legal dictation systems, personal assistants like ""Siri,"" automated financial systems, etc.  Thousands of 'apps' have been developed to help consumers use voice to get the information they are looking for.   Speech recognition is the backbone of all of these technologies.  As a result, the performance of speech recognizers is key for customer satisfaction.  Currently, many systems still need to be tuned for a particular speaker to perform well, and the recognition task has be limited in other ways such as requiring (1) usage of a specific vocabulary,  (2) clear pronunciation of most of the words, especially the content words and (3) limited background noise. In this research, speech variability will be studied, and methods and models will be developed that will enable recognizers to be more speaker independent and capable of handling the full range of speech styles from clear articulation to very casually spoken speech. The results will also bear on linguistic models of speech planning and organization, providing evidence for how speakers trade off efficiencies in the production of speech against the need to be intelligible.<br/><br/>In this project, point source tracking of the speech articulators will be collected concurrently with the corresponding acoustics.  Speakers will record speech at both a normal and rapid pace (the purpose of the latter is to increase significantly the degree of variability in the signal).  This data will allow for the investigation of whether speakers always move their speech articulators in the direction of a desired target (e.g. tongue tip to teeth in producing /t/) even when a rapid production pace occludes the relevant acoustic information (as in ""perfect"").  If confirmed, this finding will point the way towards making recognition systems more robust through the incorporation of articulatory information.  In addition, such data will support the development of a speech inversion system capable of 'uncovering' hidden articulatory movements potentially masked from the acoustics."
88,1358096,IRES: US-German Research on Human-Computer Interaction in Ubiquitous Computing,OISE,"IRES Track I: IRES Sites (IS), EPSCoR Co-Funding",6/1/14,6/5/14,Andrew Kun,NH,University of New Hampshire,Standard Grant,Cassandra Dudka,5/31/18,"$249,994.00 ",W. Thomas Miller,andrew.kun@unh.edu,51 COLLEGE RD SERVICE BLDG 107,Durham,NH,38243585,6038622172,O/D,"7727, 9150","5936, 9150",$0.00 ,"Technical description.<br/> The UNH IRES program is a cooperative effort between the University of New Hampshire and the University of Stuttgart in Germany. The focus of the program is on ubiquitous computing, or ubicomp. Ubicomp is a multidisciplinary field of study that explores networked computing devices that are embedded in everyday objects. Each summer 3 undergraduate and 3 graduate students will conduct research for just over 8 weeks at the Human Computer Interaction (HCI) Lab of Professor Albrecht Schmidt at the University of Stuttgart. The HCI Lab is one of the leading ubicomp laboratories in the world.  <br/>Student research within the UNH Ubicomp IRES program will focus on two areas: in-vehicle speech interaction and speech interaction with public displays. Students will initially investigate in-vehicle speech interactions to establish how different speech recognition error rates and different response times affect the driver. Students will subsequently contrast interacting with a computer to interacting with a passenger. Finally, students will use the results of these experiments to propose new, safer approaches to driver-computer spoken interactions. In the context of speech interactions with public displays, students will explore the use of pupil diameter measurements from video camera based eye trackers to estimate the cognitive load associated with spoken interactions with electronic bulletin boards. Students will develop techniques for separating pupil diameter changes that are due to cognitive load from those that are due to visual target luminance on the display. Students will subsequently use pupil diameter to evaluate the cognitive load associated with different types of interactions with public displays, providing a baseline for proposing novel interactions that mix spoken and manual interactions. <br/><br/>Non-technical description.<br/>The UNH IRES program is a cooperative effort between the University of New Hampshire and the University of Stuttgart in Germany. The focus of the program is on ubiquitous computing, or ubicomp. Ubicomp is a multidisciplinary field of study that explores networked computing devices that are embedded in everyday objects. Each summer 3 undergraduate and 3 graduate students will conduct research for just over 8 weeks at the Human Computer Interaction (HCI) Lab of Professor Albrecht Schmidt at the University of Stuttgart. The HCI Lab is one of the leading ubicomp laboratories in the world. <br/>Student research within the UNH IRES program will focus on two areas: in-vehicle speech interaction and speech interaction with public displays. In the automotive domain, speech interaction with in-vehicle devices is an important topic. In-vehicle devices, such as built-in entertainment systems, and brought-in mobile phones, are proliferating, and research has shown that it can be dangerous to interact with these devices by touching and/or looking. Thus, much research has been done to explore speech as an alternative way of interaction with in-vehicle devices. Students in the UNH IRES program will relate the benefits and limitations of speech interaction with in-vehicle devices with real-world parameters, such as how well speech recognition works at any given moment. They will also work to identify why it is that talking to a passenger appears to reduce the probability of a crash, and how we might be able to use this new information to create safer in-vehicle speech interactions.<br/>Speech interaction is also an exciting topic in the budding exploration of electronic bulletin boards. Today, people use bulletin boards to display and read printed (physical) notices. But tomorrow?s electronic bulletin boards will act more like large, public webpages, edited by users from their mobile phones. Students in the UNH IRES program will compare speech interaction with other types of interactions with such electronic bulletin boards (such as typing directly on the bulletin board, or typing on a mobile phone). They will pay special attention to assessing the mental effort of each type of interaction. Their long-term goal will be to assess if speech interaction can reduce this mental effort."
89,1432606,Speech-Based Learning Analytics for Collaboration,DRL,"ECR-EHR Core Research, Cyberlearn & Future Learn Tech",9/1/14,8/18/14,Cynthia D'Angelo,CA,SRI International,Standard Grant,Karen King,8/31/17,"$1,499,944.00 ","Elizabeth Shriberg, Harry Bratt",cdangelo@illinois.edu,333 RAVENSWOOD AVE,Menlo Park,CA,940253493,7032478529,EHR,"7980, 8020",8244,$0.00 ,"The Education Core Research (ECR) program funds foundational research in STEM learning, STEM learning environments, workforce development, and broadening participation in STEM. One of the pressing current challenges in schools as curriculum focuses increasingly on inquiry, application, and synthesis (rather than factual recall) is how to efficiently manage, support, and assess collaboration in the classroom. While teaching with collaborative learning may be more productive, it also creates greater burdens on educators, including the need to have real-time diagnosis of problems in group work. Technology has been used in a variety of ways to provide real-time data, but this project takes a novel approach: using cutting edge computer science, specifically automatic speech processing technology, and studies how it might be used to support real-time diagnosis of collaboration among learners. <br/><br/>Using speech processing to recognize the words spoken, and then analyze whether collaboration is taking place, is an area of research that is far from applicability. This project will instead use speech processing technology to look for other aspects of speech that may serve as useful gross indicators of collaboration, such as turn taking, which is much easier to apply using current technological infrastructure. The proposal is a collaboration between learning scientists, computer scientists, linguists, and teachers, and will address three research questions: first, they will study (using human observation) how well indicators (such as turn-taking, or vocal tone associated with frustration) predict overall collaboration quality (i.e., collaborative learning behavior and outcomes); second, they will examine how well automated speech processing technology can identify these indicators; and finally, they will attempt to validate whether the automated system can usefully gauge the student collaboration. This research will be undertaken in the context of a well-studied Mathematics curriculum (Cornerstone Mathematics). Prior research on Cornerstone Math shows that while it is highly successful on average, student outcomes vary significantly with student discourse quality. The project proposes to study the automated system both in a controlled laboratory setting in which acoustics can be carefully controlled, and in a realistic field setting using diverse students in 5 middle school classrooms. The intellectual merit includes improving the state of the art in automated speech recognition technology to examine paralinguistic and prosodic features useful in gauging collaboration; novel research on the ways successful and unsuccessful collaboration shows certain acoustic markers; and design of new computer-supported collaborative learning technology to support teacher diagnosis of learner collaboration in real time. Broader impacts of the work relate to the eventual possibility that teachers could more easily implement collaborative learning in their classrooms, while paying attention to the teams or groups that need help the most."
90,1431118,Collaborative Research: Making words disappear or appear: A neurocognitive and behavioral investigation of effects of speech rate on spoken word perception,BCS,"PERCEPTION, ACTION & COGNITION",8/15/14,8/14/14,Lisa Sanders,MA,University of Massachusetts Amherst,Standard Grant,Betty H. Tuller,7/31/17,"$102,214.00 ",,lsanders@psych.umass.edu,Research Administration Building,Hadley,MA,10359450,4135450698,SBE,7252,"7252, 7298",$0.00 ,"Understanding how humans comprehend speech is an unsolved and challenging problem, in part because factors such as different speakers, dialects, and speaking rates introduce a great deal of temporal and spectral variability into the speech signal. The focus of this research is on the influence of temporal context on perception of segments, syllables, and words. Results of the research may offer insights into treatment of disorders that involve disruption of speech rate (e.g., dysarthria, stuttering, Parkinson's disease, and aphasia), inform approaches to improve speech technology applications (e.g., enhanced automatic speech recognition, more natural sounding computer-generated speech), and lead to new discoveries related to brain mechanisms involved in understanding spoken language. The investigators will also involve students in the research, including those from a primarily undergraduate institution collaborating on the project.<br/><br/>The investigators will test different accounts of temporal phenomena in the perception of speech. They propose two interacting cognitive mechanisms controlling phenomena at lexical and phonetic levels, each driven by a different neural timing mechanism. The hypothesis is that effects of lexical rate primarily stem from top-down, speech-specific temporal expectancies, while phonetic rate effects originate in bottom-up, transient rhythmic expectations that are not specific to speech. This hypothesis will be assessed using psychoacoustic studies, non-invasive measures of brain activity, and theoretical modeling in order to identify the processing characteristics revealed by neural representations of temporal properties of speech."
91,1423905,Effects of Native Language and Linguistic Exposure on Non-Native Listeners' Use of Prosodic Cues in Speech Segmentation,BCS,Linguistics,8/15/14,8/1/14,Annie Tremblay,KS,University of Kansas Center for Research Inc,Standard Grant,William Badecker,1/31/19,"$258,964.00 ",,atrembla@ku.edu,2385 IRVING HILL RD,Lawrence,KS,660457568,7858643441,SBE,1311,"1311, 9150, 9178, 9179, 9251, SMET",$0.00 ,"Speech is a continuous flow of sounds where no single device explicitly marks word boundaries. A crucial challenge for language learners is that the cues that signal word boundaries differ across languages; thus, a speaker's experience with her native language may be misleading when attempting to segment a second language into words. Specifying how adult language learners recognize words in continuous speech is very important, first because it can help resolve key theoretical debates about how language learning and language comprehension take place in adults. More precisely, it can shed light on whether (and if so, the extent to which) the adult brain is sufficiently plastic to develop sensitivity to new segmentation cues and the factors that modulate whether or not this learning is possible. This research can therefore have important implications for cognitive and linguistic sciences. Furthermore, speech segmentation is essential to successful communication among multilingual speakers. As our society is becoming increasingly multilingual, languages have become intrinsic components of K-12 and college curricula, and computational linguists and computer scientists have striven to develop technologies that could afford efficient communication among multilingual speakers. Investigating the factors that influence how adult language learners segment a second language into words can thus have important implications for the teaching of languages and for the development of communication technologies (e.g., automatic speech recognition).<br/><br/>This research focuses on the influence of the native language and of recent linguistic exposure on adult language learners' use of prosodic cues, specifically pitch, in speech segmentation. Its primary aim is to determine how the similarities and differences between the native language and second language affect adult language learners' ability to use prosodic cues in speech segmentation. Two hypotheses are tested: (i) the Prosodic Assimilation Hypothesis: Second-language prosodic systems that are similar to, yet different from, native-language prosodic systems are assimilated and thus more difficult to learn in speech segmentation than second-language prosodic systems that are very different from native-language prosodic systems; and (ii) the Cue-Weighting Transfer Hypothesis: The functional load of segmentation cues in the native language is carried over to the second language. A secondary aim of this research is to assess the effect of recent linguistic exposure on speech segmentation. A third hypothesis is tested: (iii) Probabilistic Speech Segmentation Hypothesis: The speech processing system uses a single set of speech segmentation strategies that reflect the probabilities of cues to word boundaries across the native language and second language. To test these hypotheses, this research focuses on native French, Korean, English, and Dutch listeners with or without knowledge of French or Korean as second languages. Listeners will complete visual-world eye-tracking and artificial-language segmentation experiments."
92,1431063,Making words disappear or appear: A neurocognitive and behavioral investigation of effects of speech rate on spoken word perception,BCS,"PERCEPTION, ACTION & COGNITION",8/15/14,2/26/15,Laura Dilley,MI,Michigan State University,Standard Grant,Betty Tuller,7/31/18,"$246,997.00 ",J. Devin McAuley,dilleyl@msu.edu,Office of Sponsored Programs,East Lansing,MI,488242600,5173555040,SBE,7252,"7252, 7298, 9251",$0.00 ,"Understanding how humans comprehend speech is an unsolved and challenging problem, in part because factors such as different speakers, dialects, and speaking rates introduce a great deal of temporal and spectral variability into the speech signal. The focus of this research is on the influence of temporal context on perception of segments, syllables, and words. Results of the research may offer insights into treatment of disorders that involve disruption of speech rate (e.g., dysarthria, stuttering, Parkinson's disease, and aphasia), inform approaches to improve speech technology applications (e.g., enhanced automatic speech recognition, more natural sounding computer-generated speech), and lead to new discoveries related to brain mechanisms involved in understanding spoken language. The investigators will also involve students in the research, including those from a primarily undergraduate institution collaborating on the project.<br/><br/>The investigators will test different accounts of temporal phenomena in the perception of speech. They propose two interacting cognitive mechanisms controlling phenomena at lexical and phonetic levels, each driven by a different neural timing mechanism. The hypothesis is that effects of lexical rate primarily stem from top-down, speech-specific temporal expectancies, while phonetic rate effects originate in bottom-up, transient rhythmic expectations that are not specific to speech. This hypothesis will be assessed using psychoacoustic studies, non-invasive measures of brain activity, and theoretical modeling in order to identify the processing characteristics revealed by neural representations of temporal properties of speech."
93,1421695,RI: Small: Collaborative Research: Cognitive models of the acquisition of vowels in context,IIS,"Perception, Action & Cognition, Robust Intelligence",9/1/14,6/15/15,Naomi Feldman,MD,University of Maryland College Park,Continuing Grant,Tatiana Korelsky,8/31/19,"$240,000.00 ",,nhf@umd.edu,3112 LEE BLDG 7809 Regents Drive,COLLEGE PARK,MD,207425141,3014056269,CSE,"7252, 7495","7252, 7495, 7923",$0.00 ,"Models of infant language acquisition allow researchers to test hypotheses about how infants learn their native language from speech.  This project focuses on how infants determine which sounds are meaningful in their language and concentrates on co-articulation, a process in which sounds are produced differently depending on qualities of neighboring sounds. Co-articulation is closely tied to the historical creation, change, and loss of sound patterns in language.  As a result, the models developed in this project further our understanding of both infant language acquisition and historical language change. Knowing how different aspects of the learning process interact can also help pinpoint deficits that might underlie developmental language disorders. Finally, building models of language acquisition that can work on speech data can potentially help create speech recognition technology that adapts robustly to novel languages, accents, and domains of discourse in the same way that human learners do.<br/> <br/>A series of unsupervised phonetic learning models are created to examine how learners take into account co-articulation.  Whereas previous models of phonetic learning have categorized each sound independently, these models begin with a set of constraints that capture possible dependences between sounds, formalized using a Markov random field. They then learn from the data which constraints characterize a particular language.  Recordings of child-directed speech from the CHILDES database are annotated through forced alignment to serve as test corpora for comparing the newly developed models with previous models. The project yields a rigorous evaluation of where existing models fall short, a new framework for accounting for phonetic variability, and corpora to support the development and evaluation of future phonetic learning models."
94,1446129,HCC: Small: Collaborative Research: Real-Time Captioning by Groups of Non-Experts for Deaf and Hard of Hearing Students,IIS,HCC-Human-Centered Computing,1/31/14,8/4/14,Jeffrey Bigham,PA,Carnegie-Mellon University,Continuing Grant,Ephraim Glinert,7/31/16,"$317,194.00 ",,jbigham@cmu.edu,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,CSE,7367,"7367, 7923",$0.00 ,"Many deaf and hard of hearing students use real-time captioning to participate in education.  Generally, real-time captions are provided by skilled professional captionists (stenographers) who use specialized keyboards or software to keep up with natural speaking rates of up to 225 words per minute.   But professional captionists are expensive and must be arranged in advance in blocks of at least an hour.  Automatic speech recognition (ASR) is improving, but still experiences high error rates in real classrooms.  In this collaborative effort involving the University of Rochester and Rochester Institute of Technology, the PIs will address these issues by blending human- and machine-powered captioning to produce captions on demand, in real time, for low cost.  The PIs' approach is for multiple non-experts and ASR to collectively caption speech in under 5 seconds, with the help of interfaces which encourage quick, incomplete captioning of live audio.  Because non-experts cannot keep up with natural speaking rates, new algorithms will merge incomplete captions in real time. (While the sequence alignment problem can be solved exactly with dynamic programming, existing approaches are too slow, are not robust to input error, and do not incorporate natural language semantics.)  Systematically varying audio saliency will encourage complete coverage of speech.  Non-expert captions will train ASR engines in real time, so that ASR may improve during a lecture. (Traditional approaches for ASR training assume that training occurs offline.)  The quikCaption mobile application will embody these ideas and will be iteratively designed with deaf and hard of hearing students at the National Technical Institute of the Deaf (NTID) via design sessions, lab studies and in-class deployments.  Non-expert captionists can be drawn from broad sources: volunteers willing to donate their time, classmates with relevant domain knowledge, or always-available paid workers.  They may be local (in the classroom) or remote.  Captionists may have experience from prior quikCaption sessions, or novice crowd workers recruited on demand from existing marketplaces (e.g., Mechanical Turk).  A flexible worker pool will allow real-time captions to be available on demand at low cost and for only as long as needed.<br/><br/>Broader Impacts:  This research will dramatically improve education for deaf and hard of hearing students by enabling access to serendipitous opportunities, such as conversations after class or last-minute guest lectures for which no interpreter or captionist was arranged.  Real-time captioning will also be useful in other settings such as school programs, artistic performances, and political events.  Older hard of hearing adults usually prefer captioning, and represent a sizable and growing population; hearing people may benefit because captioning is a first step in automatic translation of aural speech.  The algorithms developed as part of this project for real-time merging of incomplete natural language will likely be adaptable for other applications such as collaborative translation or communication over noisy mediums."
95,1422987,RI: Small: Collaborative Research: Cognitive models of the acquisition of vowels in context,IIS,Robust Intelligence,9/1/14,6/15/15,Micha Elsner,OH,Ohio State University,Continuing Grant,Tatiana Korelsky,8/31/19,"$240,000.00 ",,elsner.14@osu.edu,Office of Sponsored Programs,Columbus,OH,432101016,6146888735,CSE,7495,"7495, 7923",$0.00 ,"Models of infant language acquisition allow researchers to test hypotheses about how infants learn their native language from speech.  This project focuses on how infants determine which sounds are meaningful in their language and concentrates on co-articulation, a process in which sounds are produced differently depending on qualities of neighboring sounds. Co-articulation is closely tied to the historical creation, change, and loss of sound patterns in language.  As a result, the models developed in this project further our understanding of both infant language acquisition and historical language change. Knowing how different aspects of the learning process interact can also help pinpoint deficits that might underlie developmental language disorders. Finally, building models of language acquisition that can work on speech data can potentially help create speech recognition technology that adapts robustly to novel languages, accents, and domains of discourse in the same way that human learners do.<br/> <br/>A series of unsupervised phonetic learning models are created to examine how learners take into account co-articulation.  Whereas previous models of phonetic learning have categorized each sound independently, these models begin with a set of constraints that capture possible dependences between sounds, formalized using a Markov random field. They then learn from the data which constraints characterize a particular language.  Recordings of child-directed speech from the CHILDES database are annotated through forced alignment to serve as test corpora for comparing the newly developed models with previous models. The project yields a rigorous evaluation of where existing models fall short, a new framework for accounting for phonetic variability, and corpora to support the development and evaluation of future phonetic learning models."
96,1464553,RI: Small: Language Induction meets Language Documentation: Leveraging bilingual aligned audio for learning and preserving languages,IIS,"ROBUST INTELLIGENCE, DEL",9/1/14,4/30/15,David Chiang,IN,University of Notre Dame,Continuing grant,D.  Langendoen,9/30/18,"$470,000.00 ",,dchiang@nd.edu,940 Grace Hall,NOTRE DAME,IN,465565708,5746317432,CSE,"7495, 7719","7495, 7923",$0.00 ,"Thousands of the world's languages are in danger of dying out before they have been systematically documented. Many other languages have millions of speakers, yet they exist only in spoken form, and minimal documentary records are available. As a consequence, important sources of knowledge about human language and culture are inaccessible, and at risk of being lost forever. Moreover, it is difficult to develop technologies for processing these languages, leaving their speech communities on the far side of a widening digital divide. The first step to solving these problems is language documentation, and so the goal of this project is to develop computational methods based on automatic speech recognition and machine translation for documenting endangered and unwritten languages on an unprecedented scale.<br/><br/>To be successful, any approach must guarantee both the sufficiency and interpretability of the documentation it produces. This project ensures sufficiency by using a combination of community outreach, crowdsourcing techniques, and mobile/web technologies to collect hundreds of hours (millions of words) of speech. The interpretability is enabled by augmenting original speech recordings with careful verbatim repetitions along with translations into a well-resourced language. Finally, computational models are developed to automate transcription of recordings and alignment with translations, resulting in bilingual aligned text. The result is a kind of digital Rosetta Stone: a large-scale key for interpreting the world's languages even if they are not written, or no longer even spoken."
97,1423406,RI: Small: Language Induction meets Language Documentation: Leveraging bilingual aligned audio for learning and preserving languages,IIS,"ROBUST INTELLIGENCE, DEL",9/1/14,9/5/14,David Chiang,CA,University of Southern California,Continuing grant,Tatiana D. Korelsky,11/30/14,"$470,000.00 ",Steven Bird,dchiang@nd.edu,University Park,Los Angeles,CA,900890001,2137407762,CSE,"7495, 7719","7495, 7923",$0.00 ,"Thousands of the world's languages are in danger of dying out before they have been systematically documented. Many other languages have millions of speakers, yet they exist only in spoken form, and minimal documentary records are available. As a consequence, important sources of knowledge about human language and culture are inaccessible, and at risk of being lost forever. Moreover, it is difficult to develop technologies for processing these languages, leaving their speech communities on the far side of a widening digital divide. The first step to solving these problems is language documentation, and so the goal of this project is to develop computational methods based on automatic speech recognition and machine translation for documenting endangered and unwritten languages on an unprecedented scale.<br/><br/>To be successful, any approach must guarantee both the sufficiency and interpretability of the documentation it produces. This project ensures sufficiency by using a combination of community outreach, crowdsourcing techniques, and mobile/web technologies to collect hundreds of hours (millions of words) of speech. The interpretability is enabled by augmenting original speech recordings with careful verbatim repetitions along with translations into a well-resourced language. Finally, computational models are developed to automate transcription of recordings and alignment with translations, resulting in bilingual aligned text. The result is a kind of digital Rosetta Stone: a large-scale key for interpreting the world's languages even if they are not written, or no longer even spoken."
98,1360670,Understanding Prosody and Tone Interactions through Documentation of Two Endangered Languages,BCS,"DEL, IIS SPECIAL PROJECTS",7/15/14,8/3/15,Christian DiCanio,CT,"Haskins Laboratories, Inc.",Continuing grant,Colleen Fitzgerald,12/31/15,"$340,456.00 ",,dicanio@haskins.yale.edu,300 George Street,New Haven,CT,65116610,2038656163,SBE,"7719, 7484","1311, 7484, 7719, SMET",$0.00 ,"For accurate automated translation of text-to-spoken-word or spoken-word-to-text, it is necessary to understand the prosodic patterning of speech since prosody, i.e. stress, rhythm, and pauses between phrases, is central in conveying meaning and structuring discourse. In addition, many languages use tone to change the meaning of words but little is known about how prosodic features interact with tone. Since tone and prosody are conveyed by using some of the same acoustic dimensions (pitch, duration, voice quality), one must know how these two systems interact for automatic speech recognition and translation.<br/><br/>To investigate and test hypotheses about the relationship of tone and prosody, Christian Di Canio, along with an interdisciplinary team, will create a database of 30 hours of transcribed narratives from Itunyoso Trique and expand an existing similar database in Yoloxchitl Mixtec, both both Mixtecan languages from eastern central Mexico. These transcribed narratives will then be parsed into smaller units applying a 'forced alignment' tool used in automatic speech recognition. Important results from the project will include the testing and improvement of the forced alignment tool; new corpora and expanded dictionaries for and Yoloxchitl Mixtec; and an analysis of tone and prosody interactions in these two languages. The resulting prosodically segmented and tagged corpora will be a first of its kind for an endangered language that will be of use to the speaker-community and the broader scientific community.<br/><br/>This project is partially supported by funds from the Robust Intelligence program."
99,1349080,Doctoral Dissertation Research: Bias in Novel Category Learning,BCS,LINGUISTICS,4/15/14,4/5/14,Lisa Davidson,NY,New York University,Standard Grant,William J. Badecker,3/31/16,"$4,946.00 ",Sean Martin,lisa.davidson@nyu.edu,70 WASHINGTON SQUARE S,NEW YORK,NY,100121019,2129982121,SBE,1311,"1311, 9179",$0.00 ,"Understanding the role that prior linguistic experience plays in the acquisition of novel categories is critical to our development of theories of how individuals learn a first or second language.<br/><br/>This dissertation examines learning bias in novel phonetic category learning, as in artificial language learning experiments or second language acquisition. During category learning, learners discover categories largely based on the distribution of their input along phonetic cue dimensions. Learners have been shown to make approximately optimal use of distributional information when discovering categories with single cues, whether or not their native language uses a given cue. In contrast, when learning categories requiring multiple cues, learners frequently attend to only some of the available cues, resulting in poorer category learning. For example, Spanish speakers learning English often show difficulty with the vowel distinction in 'sheep' vs 'ship'. Many learners initially attend primarily to statistically unreliable vowel duration differences although stronger cues are available in the form of spectral differences.<br/><br/>Under the direction of Dr. Lisa Davidson, Sean Christopher Martin will carry out a series of experiments to test the degree to which native language experience gives rise to this type of learning bias in category learning. Native speakers of English will be trained to recognize new vowel categories which are distinguished by pairs of familiar and unfamiliar cues. The stimuli are then systematically manipulated to control which of the cues is more reliable. In this way, the respective contributions of preference for statistically robust cues and bias to attend to cues which the learner's prior experience suggests are reliable can be distinguished.<br/><br/>A computational model of learner behavior accounts for expected results in terms of a hierarchical account of category learning. In order to learn from high-dimensional input like the speech signal, the model develops generalizaitons about cue reliability while learning category structure, giving rise to more efficient first-language learning but later generating learning bias because it initially predicts unfamiliar cues to be low-reliability.<br/><br/>The results of this research will aid understanding of second language learning, highlighting issues which might contribute to poor second language learning outcomes. In addition, the proposed computational model demonstrates an approach which allows for more flexible and adaptable category representations which might allow future speech recognition systems to more easily cope with the natural variability of the speech signal."
100,1417090,"STTR Phase I:  Self-Inflating bubble provides comfortable, effective hearing protection with directionality",IIP,STTR Phase I,7/1/14,6/29/17,Stephen Ambrose,CO,"Asius Technologies, LLC",Standard Grant,Jesus Soriano Molla,12/31/15,"$194,967.00 ",Todd Ricketts,Stephen.Ambrose@AsiusTechnologies.com,1257 Whitehall,Longmont,CO,805042668,7202042676,ENG,1505,"1505, 163E, 6840, 8035, 9139, HPCC",$0.00 ,"The broader impact/commercial potential of this project will be to improve HPD use-rates among workers in loud noise environments, reduce permanent hearing loss rates and usher in the field of sealed-ear bio-acoustical studies.  This HPD is expected to provide greater comfort and fit than custom earplugs yet have a cost similar to the annual expense of the foam type.  The use-rate with this HPD is expected to compete with and likely exceed that of the custom earplugs and be the new hearing protection device of choice for both markets.  The anticipated hearing protection device use-rate increase in loud workplaces will have the effect of improving workers? health and stabilizing their productivity rates, positions and wages.  The presence of this HPD with its novel approach to hearing protection on the market will enhance appreciation for hearing health and further popularize biophysics solutions to everyday issues.  The underlying research will open up interest in sealed-ear bio-acoustical phenomena.  Once the performance results of this HPD and the associated tympanic membrane response study reaches the mainstream audiological and acoustic research communities, the desire to evaluate the health effects of a range of in-ear audio devices and associated ear canal acoustics will become widespread.<br/><br/>This Small Business Technology Transfer Research (STTR) Phase I project addresses the need for a hearing protection device (HPD) that is comfortable to wear, effectively blocks harmful sounds and yet enables communication.  HPDs go unused or misused in loud workplace environments due to discomfort and insufficient utility with regards to situational awareness leading to the single most extensive non-fatal labor-based sickness of permanent hearing loss in the United States and in particular, among members of the armed forces.  A prototype HPD is under development with a soft, ?disappears from perception? inflatable ear-coupler that fully seals and isolates the ear from exterior sound and with variability to adjust for differing loudness situations.  Tests will measure sound isolation, speech recognition, occlusion effect, and perceived comfort, occlusion and stress on the tympanic membrane compared to the most common earplug style hearing protection devices on the market today.  Standard audiometric methods such as Real Ear Attenuation at Threshold, Microphone In Real Ear, tympanometry, and reflectance will be used to perform tests.  Expected results include improved sound isolation, clearer speech recognition, significantly less to no occlusion effect, greatly preferred comfort levels and reduced stress on the tympanic membrane compared to the common hearing protection devices."
101,1414172,SHF: Large: Collaborative Research: Exploiting the Naturalness of Software,CCF,Software & Hardware Foundation,7/1/14,6/3/20,Premkumar Devanbu,CA,University of California-Davis,Continuing Grant,Sol Greenspan,6/30/21,"$1,240,108.00 ","Zhendong Su, Vladimir Filkov",devanbu@cs.ucdavis.edu,OR/Sponsored Programs,Davis,CA,956186134,5307547700,CSE,7798,"7798, 7925, 7944, 8091, 8624, 9251",$0.00 ,"This inter-disciplinary project has its roots in Natural Language (NL) processing. Languages such as English allow intricate, lovely and complex constructions; yet, everyday, ``natural? speech and writing is simple, prosaic, and repetitive, and thus amenable to statistical modeling. Once large NL corpora became available, computational muscle and algorithmic insight led to rapid advances in the statistical modeling of natural utterances, and revolutionized tasks such as translation, speech recognition, text summarization, etc.  While programming languages, like NL, are flexible and powerful, in theory allowing a great variety of complex programs to be written, we find that ``natural? programs that people actually write are regular, repetitive and predictable. This project will use statistical models to capture and exploit this regularity to create a new generation of software engineering tools to achieve transformative improvements in software quality and productivity. <br/> <br/>The project will exploit language modeling techniques to capture the regularity in natural programs at the lexical, syntactic, and semantic levels. Statistical modeling will also be used to capture alignment regularities in ``bilingual? corpora such as code with comments, or explanatory text (e.g., Stackoverflow) and in systems developed on two platforms such as Java and C#.  These statistical models will help drive novel, data-driven approaches for applications such as code suggestion and completion, and assistive devices for programmers with movement or visual challenges. These models will also be exploited to correct simple errors in programs. Models of bilingual data will used to build code summarization and code retrieval tools, as well as tools for porting across platforms. Finally, this project will create a large, curated corpus of  software, and code analysis products, as well as a corpus of alignments within software bilingual corpora, to help create and nurture a research community in this area."
102,1413927,SHF: Large: Collaborative Research: Exploiting the Naturalness of Software,CCF,"Information Technology Researc, Software & Hardware Foundation",7/1/14,9/16/16,Tien Nguyen,IA,Iowa State University,Continuing Grant,Sol Greenspan,2/28/17,"$333,224.00 ",,nguyen.n.tien@gmail.com,1138 Pearson,AMES,IA,500112207,5152945225,CSE,"1640, 7798","7925, 7944",$0.00 ,"This inter-disciplinary project has its roots in Natural Language (NL) processing. Languages such as English allow intricate, lovely and complex constructions; yet, everyday, ``natural? speech and writing is simple, prosaic, and repetitive, and thus amenable to statistical modeling. Once large NL corpora became available, computational muscle and algorithmic insight led to rapid advances in the statistical modeling of natural utterances, and revolutionized tasks such as translation, speech recognition, text summarization, etc.  While programming languages, like NL, are flexible and powerful, in theory allowing a great variety of complex programs to be written, we find that ``natural? programs that people actually write are regular, repetitive and predictable. This project will use statistical models to capture and exploit this regularity to create a new generation of software engineering tools to achieve transformative improvements in software quality and productivity. <br/> <br/>The project will exploit language modeling techniques to capture the regularity in natural programs at the lexical, syntactic, and semantic levels. Statistical modeling will also be used to capture alignment regularities in ``bilingual? corpora such as code with comments, or explanatory text (e.g., Stackoverflow) and in systems developed on two platforms such as Java and C#.  These statistical models will help drive novel, data-driven approaches for applications such as code suggestion and completion, and assistive devices for programmers with movement or visual challenges. These models will also be exploited to correct simple errors in programs. Models of bilingual data will used to build code summarization and code retrieval tools, as well as tools for porting across platforms. Finally, this project will create a large, curated corpus of  software, and code analysis products, as well as a corpus of alignments within software bilingual corpora, to help create and nurture a research community in this area."
103,1414030,SHF: Large: Collaborative Research: Exploiting the Naturalness of Software,CCF,"Information Technology Researc, Software & Hardware Foundation",7/1/14,9/16/16,William Cohen,PA,Carnegie-Mellon University,Continuing Grant,Sol Greenspan,6/30/18,"$666,667.00 ",,wcohen@cs.cmu.edu,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,CSE,"1640, 7798","7925, 7944",$0.00 ,"This inter-disciplinary project has its roots in Natural Language (NL) processing. Languages such as English allow intricate, lovely and complex constructions; yet, everyday, ``natural? speech and writing is simple, prosaic, and repetitive, and thus amenable to statistical modeling. Once large NL corpora became available, computational muscle and algorithmic insight led to rapid advances in the statistical modeling of natural utterances, and revolutionized tasks such as translation, speech recognition, text summarization, etc.  While programming languages, like NL, are flexible and powerful, in theory allowing a great variety of complex programs to be written, we find that ``natural? programs that people actually write are regular, repetitive and predictable. This project will use statistical models to capture and exploit this regularity to create a new generation of software engineering tools to achieve transformative improvements in software quality and productivity. <br/> <br/>The project will exploit language modeling techniques to capture the regularity in natural programs at the lexical, syntactic, and semantic levels. Statistical modeling will also be used to capture alignment regularities in ``bilingual? corpora such as code with comments, or explanatory text (e.g., Stackoverflow) and in systems developed on two platforms such as Java and C#.  These statistical models will help drive novel, data-driven approaches for applications such as code suggestion and completion, and assistive devices for programmers with movement or visual challenges. These models will also be exploited to correct simple errors in programs. Models of bilingual data will used to build code summarization and code retrieval tools, as well as tools for porting across platforms. Finally, this project will create a large, curated corpus of  software, and code analysis products, as well as a corpus of alignments within software bilingual corpora, to help create and nurture a research community in this area."
104,1427680,"Workshop: Non-volatile Memories Workshop 2014. To Be Held March 9-11, 2014, on the UCSD campus in La Jolla, California.",ECCS,EPMD-ElectrnPhoton&MagnDevices,3/1/14,3/4/14,Paul Siegel,CA,University of California-San Diego,Standard Grant,Usha Varshney,2/28/15,"$15,000.00 ",,psiegel@ucsd.edu,Office of Contract & Grant Admin,La Jolla,CA,920930621,8585344896,ENG,1517,107E,$0.00 ,"Advances in data storage technology have been crucial to the evolution of the modern information age, enabling and accelerating the invention of new information-related applications in consumer entertainment, personal and business computing, enterprise data management, and scientific research. High-capacity, non-volatile, solid-state drives (SSDs) are in the process of revolutionizing this world of data storage.  SSDs have a number of advantages compared to conventional disk and tape drives, notably in shock resistance, reduced power consumption, and faster data access.  Although currently less competitive in some storage applications with respect to cost per bit, write latency,  and product lifetime, continuing advances in SSDs based upon several non-volatile memory (NVM)  technologies are setting the stage for a revolution in how computer systems and applications access and manipulate persistent data.  Improved flash memories - along with emerging technologies such as magnetic RAM (MRAM), phase-change memories (PCM), spin-torque transfer memories (STTM), resistive RAM (RRAM), and the memristor - are driving designers to rethink how they integrate storage devices into computing systems, how operating systems manage data, and how applications create and process information.  Realizing the full potential of NVM technologies is an exciting and important challenge with enormous societal consequences.  The proposed workshop is the fifth in the annual series of Non-volatile Memories Workshops (NVMWs) that have been held on the campus of the University of California, San Diego.  The primary objective continues to be the development of a ""vertical"" vision for research on the role of NVM technologies in an ever-increasing number of application scenarios, ranging from data-intensive computing systems to super high-resolution video games.  As the capabilities of NVM-based storage rapidly evolve, it is more critical than ever that researchers at each level of the system ""stack"" be aware of the needs, challenges, and opportunities associated with the other levels.  The workshop provides researchers and practitioners the opportunity to gain a broader understanding of what is needed to accelerate the development and adoption of NVM-based storage technologies, and to establish relationships that will provide the basis for further advances.  The workshop program includes a half-day tutorial session, two keynote addresses, approximately 32 technical presentations selected by an expert Program Committee, and a poster session.  Topics addressed range from new NVM device technologies, data handling techniques, system architectures, and future NVM applications in areas such as distributed storage networks, neuromorphic computing, high-speed data caching, and speech recognition.  Open registration for the entire workshop is intended to encourage broad participation from academia, industry, and government.  The technical program is structured to provide a unique educational opportunity, particularly for students and researchers who are new to the area.<br/><br/>Intellectual Merit:<br/>This workshop will address fundamental problems in the science, engineering, and application of high-performance data storage systems based upon non-volatile memories.  This includes the study of nanoscale physical phenomena that permit storage of information; the development of coding algorithms for reliable, persistent, and secure data storage; the analysis of system architectures for data-intensive computing; and the conception of new paradigms for non-volatile storage of data in a variety of applications.<br/><br/>Broader Impacts:<br/>The exchange of knowledge and the generation of novel ideas that result from the workshop will have significant impact on the computing and data storage industries, both vitally important to the national economy.  The educational component of the workshop will benefit students and postdoctoral researchers, as well as more senior participants.  An archival website will provide a lasting record of the workshop proceedings and a resource for the scientific community and general public."
105,1420971,CHS: Small: Robust Interactive Audio Source Separation,IIS,HCC-Human-Centered Computing,10/1/14,6/4/15,Bryan Pardo,IL,Northwestern University,Standard Grant,Ephraim Glinert,9/30/18,"$514,261.00 ",,pardo@northwestern.edu,750 N. Lake Shore Drive,Chicago,IL,606114579,3125037955,CSE,7367,"7367, 7923, 9251",$0.00 ,"Algorithms to separate audio sources have many potential uses such as to extract important audio data from historic recordings or to help people with hearing impairments select what to amplify and what to suppress in their hearing aids. Computer processing of audio content can potentially be used to isolate the sound sources of interest and to improve the audio clarity any time that the content exhibits interference from multiple sound sources, such as to extract a single voice of interest from a room full of voices. However, current sound source identification and separation methods are only reliable when there is a single predominant sound. This project will develop the science and technology that is needed to more easily isolate a single sound source from audio content with multiple competing sources, and that is needed to build interactive computer systems that will guide users though an interactive source separation process, to permit the separation and recombining of sound sources in a manner that is beyond the reach of existing audio software. The outcomes of the project will improve the possibility of speech recognition in environments with multiple talkers, will be useful for many scientific inquiries such as in biodiversity monitoring through the automated analysis of field recordings, and will be broadly useful any time that manual tagging of audio data is not practical.<br/><br/>While many computational auditory scene analysis algorithms have been proposed to separate audio scenes into individual sources, current methods are brittle and difficult to use and as a result have not been broadly adopted by potential users. The methods are brittle in that each algorithm relies on a single cue to separate sources and if the cue is not reliable then the method fails. The methods are difficult to use because the algorithms cannot predict which audio scenes any specific algorithm is likely to work on, and so the user does not know which method to apply in any given case. They are also difficult to use because their control parameters are hard to understand for users who lack expertise in signal processing. This project will research how to integrate multiple source separation algorithms into a single framework, and how to improve the ease of use by exploring interfaces that permit users to interactively define what they wish to isolate in audio scenes, and that permit systems to provide users with guidance on selecting a tool and setting the necessary parameters. The project will produce an open-source audio source separation tool that embodies these scientific research outcomes."
106,1444754,19th Annual SIGART/AAAI Doctoral Consortium,IIS,ROBUST INTELLIGENCE,5/15/14,5/9/14,Matthew Taylor,CA,Association for the Advancement of Artificial Intelligence,Standard Grant,Hector Munoz-Avila,4/30/15,"$17,610.00 ",,taylorm@eecs.wsu.edu,2275 E BAYSHORE RD STE 160,East Palo Alto,CA,943033224,6503283123,CSE,7495,"7495, 7556",$0.00 ,"Support to student travel for select students participating in the 19th Annual SIGART/AAAI Doctoral Consortium.  AAAI (Association for the Advancement of Artificial Intelligence) is the major professional association for AI.  This year it holds its yearly conference in Quebec City, Canada, where this DC will also take place.  At the Doctoral Consortium (DC), PhD students who are pursuing work on AI-related topics present their proposed research and receive feedback from a panel of established researchers, as well as from other student participants.  This provides the students with invaluable exposure to outside perspectives on their work at a critical time in their research and also enables them to explore their career objectives.<br/><br/>The DC program includes interactive sessions for feedback on dissertation topics from authorities in the field, collaboration-building sessions, early career advice, and well-placed networking opportunities.  It is held this year in the venue of several important, co-located conferences: 28th AAAI Conference on Artificial Intelligence (AAAI-14), the 5th Symposium on Educational Advances in Artificial Intelligence (EAAI-14), and the 36th meeting of the Cognitive Science Society (CogSCI-14).  Participation in a doctoral mentoring opportunity such as this one raises the potential to bring in participants who might not have attended an AI conference out of lack of habit or resources. This is especially true for those at smaller institutions and those which have less developed AI programs. Engaging such participants has the potential to draw more talent into AI research, improve research ideas in their formative stage, and engender collaborations across the breadth of disciplines associated with intelligent systems.<br/><br/><br/>"
107,1359008,REU Site: Undergraduate Research on Artificial Intelligence and Text Analysis,IIS,RSCH EXPER FOR UNDERGRAD SITES,5/1/14,4/23/14,Sharon Small,NY,Siena College,Standard Grant,Wendy Nilsen,4/30/17,"$359,923.00 ",Larry Medsker,ssmall@siena.edu,515 LOUDON ROAD,LOUDONVILLE,NY,122111462,5187822322,CSE,1139,9250,$0.00 ,"The Siena College Research Experiences for Undergraduates (REU) Site provides unique opportunities for undergraduates to develop research expertise in the field of Artificial Intelligence (AI). The Siena College REU Site environment fosters learning through research by means of faculty advisors and peer mentors. The students work in teams on real problems in information extraction in areas such as information gathering for military and security needs to targeted marketing of products. Especially due to the emerging area of Big Data, this REU program is having an impact on applications that address national and global societal problems, such as medical information retrieval systems that identify people for clinical trials and systems to predict the focus of a groups with adverse intentions from automated text analysis. Teams working in these areas need to have diversity in skills, knowledge, and cultural perspectives, and the Siena REU program is recruiting a wide range of students, including a focus on women and other under-represented groups, so that the students can gain the broad experience needed to address the big problems in today's society. <br/><br/>The REU students are involved in research teams with Siena College faculty and student mentors working on projects that are conducted at the Siena College Institute for Artificial Intelligence (SCIAI) in computational linguistics (CL) for information extraction. The projects are improving the state of the art in CL, as well as investigating interesting real-world applications. Students are learning about the nature of scientific research and how to communicate their work in student-authored publications and through presentations at research conferences. The results of the students' work, including transcripts of presentations and copies of posters, are disseminated via the Siena College REU Site website (http://www.siena.edu/reu)."
108,1452078,The 20th SIGART/AAAI Doctoral Consortium,IIS,ROBUST INTELLIGENCE,11/15/14,11/17/14,David Roberts,CA,Association for the Advancement of Artificial Intelligence,Standard Grant,Hector Munoz-Avila,10/31/15,"$17,610.00 ",,robertsd@csc.ncsu.edu,2275 E BAYSHORE RD STE 160,East Palo Alto,CA,943033224,6503283123,CSE,7495,"7495, 7556",$0.00 ,"The AAAI Doctoral Consortium (DC) brings together established researchers and mid-career Ph.D. students for a two-day workshop where students and faculty interact in a range of settings to provide both formal and informal career and research mentoring. As a means of encouraging young and upcoming researchers in Artificial Intelligence, the AAAI Doctoral Consortium has been proven to be a relatively inexpensive and extremely effective model. Many of the current reviewers, mentors, and panelists were once student participants.<br/><br/>The Intellectual Merit of the AAAI Doctoral Consortium lies in the unique opportunity for junior researchers to gain high quality feedback on academic and social issues relevant to their career, from senior researchers within their community. As part of the DC program, mentors and students interact in meetings, panels, working lunches, and during oral presentations and discussions. For many students, this is the only opportunity to receive focused input from researchers other than members of their dissertation committee. Students not only gain the experience of giving a talk on their thesis research and receiving specific feedback on their work, they also are given the opportunity to discuss more personal issues, such as balancing work and family. <br/><br/>The Broader Impacts of the AAAI DC are centered on bringing together a wide range of students at different stages of research, from different types of labs and universities and different regions across the US and around the world, alongside an equally diverse group of mentors and panelists. By interacting with established members of the Artificial Intelligence community, students also gain a better perspective of the AI community as a whole. The 2015 AAAI DC plans to reach out directly to more students within the AAAI community?not only to the few who are accepted to present their work, but also to the many early graduate students and undergraduate students who attend AAAI, by inviting these students to attend the panels. In doing so, the impact of the DC will extend beyond the directly-support participants by encouraging other students to interact with mentors, and hopefully apply to the DC in future years."
109,1346484,STTR Phase I: Serious Game for Energy Science,IIP,STTR PHASE I,1/1/14,11/7/13,Monica Trevathan,TX,"Tietronix Software, Incorporated",Standard Grant,Glenn H. Larsen,12/31/14,"$224,497.00 ",Jana Willis,Monica.Trevathan@tietronix.com,1331 Gemini Av.,Houston,TX,770582794,2814619300,ENG,1505,"1505, 8031, 8032, 8043, 9177",$0.00 ,"This STTR Phase I project, Serious Game for Energy Science (SGES), proposes to address the question 'Can a serious game in Science, Technology, Engineering and Math (STEM), with artificial intelligence and teacher controls, engage the learner, impact student problem-solving abilities, increase interest in STEM-related activities/fields, and positively impact student learning outcomes in science, math, and reading comprehension?' Educational institutions have seen increased demand for technologies impacting learning outcomes and promoting STEM interest. As gaming becomes more broadly accepted, there is greater demand for Serious Games - games built specifically to enhance learning. This research innovation will develop two components to be modeled in STEM serious game development: (1) artificial intelligence, and (2) instructional management tool for teachers. SGES will accomplish this innovation as well as contribute to, and advance, the body of knowledge in Serious Game development. SGES research objectives are to determine the effects of a serious game in Energy Science on students: engagement, problem-solving abilities, interest in STEM-related activities and fields, and learning outcomes in science, math, and reading comprehension. Students will be inspired to learn about energy science, energy production, and environmental impacts by being immersed in an interactive world with interesting characters and engaging story objectives.<br/><br/>The broader/commercial impact of Serious Game for Energy Science (SGES) is an expandable model adaptable to learning objectives across grades levels and content areas. This research will establish models for artificial intelligence and teacher controls in serious games, and support teaching science, math, and reading comprehension with gaming. This will impact the commercialization of serious games in any science, technology, engineering and mathematics (STEM) topic. With the increase use of serious games in learning, development of this projects' research-based model will increase the economic competitiveness of the United States in the serious games market sector. Innovative serious games like SGES can potentially increase knowledge/interest in STEM related skills/careers addressing national needs for STEM graduates in the workforce. SGES will enhance math, science, environmental literacy, and reading comprehension through student exploration of energy science, energy production, and environmental impact using problem-solving, critical thinking, communication, and collaboration skills. The project research results will impact the number of students entering STEM related careers or graduating with STEM degrees by engaging them through game play. The SGES project aims to improve STEM education for all grade levels and improve educator development by providing educators with an effective use of technology in the classroom."
110,1423419,RI: Small: Design and Implementation of Goal-directed Solvers for Answer Set Programming,IIS,ROBUST INTELLIGENCE,7/1/14,6/24/14,Gopal Gupta,TX,University of Texas at Dallas,Standard Grant,James Donlon,6/30/18,"$495,109.00 ",,gupta@utdallas.edu,"800 W. Campbell Rd., AD15",Richardson,TX,750803021,9728832313,CSE,7495,"7495, 7923",$0.00 ,"This project is focused on the development of an efficient Answer Set Programming (ASP) solver, advancing the state-of-the-art in logic based knowledge representation, logic programming and artificial intelligence.  ASP is an elegant way to represent knowledge and perform advanced reasoning (common sense reasoning, non-monotonic reasoning, planning, constraint satisfaction, etc.). ASP is based on the stable model semantics proposed by Gelfond and Lifschitz. It has gained wide acceptance in the last fifteen years in the knowledge representation (KR) and artificial intelligence (AI) research communities due to its incorporation of negation, its expressiveness and simple, intuitive syntax. Considerable past research has been done in developing the ASP paradigm as well as its implementations and applications.  Implementation techniques for realizing answer set solvers range from simple guess-and-check based methods to those based on SAT solvers and complex heuristics. Applicability of current ASP systems is limited due to (i) the current implementation methods not being goal-directed (i.e., not being query-driven), (ii) need for grounding the answer set program if predicates are present, (iii) being forced to find the model of the entire program (even though to answer a given query only a small subset of the model needs to be computed), and (iv) no answer being produced even if a minor inconsistency (unrelated to the query) is present in the knowledge base. This project addresses these problems by developing a query-driven implementation of answer set programs containing predicates.  Current systems have to process the entire knowledge base (expressed as an answer set program) to compute an answer. In contrast, the query-driven method developed in this project only accesses and processes parts of the knowledge base that are involved in answering the query. Query-driven execution allows predicates to be directly included in answer set programs. It also leads to efficiency in execution.  <br/><br/>The query-driven method is based on PI's group's recent discovery of coinductive logic programming. Coinductive logic programming imparts operational semantics to greatest fixed point-based computations. Given a query and an answer set program, this coinduction-based operational semantics is used to compute (partial) answer sets that contain the query goal(s). With query-driven execution, predicates can be supported directly, i.e., answer set programs containing predicates no longer have to be grounded first. The main tasks of this project are the following: (i) develop an efficient query-driven, top-down execution strategy for propositional answer set programs; (ii) extend this query-driven execution strategy to handle Datalog ASP (without grounding the program first); (iii) further extend this query-driven execution strategy to handle Predicate ASP (without grounding the program first); (iv) develop coinductive extension of ASP and its implementation; (v) develop a query-driven abductive reasoning engine based on ASP; and, (vi) further extend the engine to incorporate constraints over reals.  The key intellectual contributions of the research is the investigation of techniques for query-driven execution of answer set programs and advanced reasoning systems that employ negation.  The research tests the claim that a query-driven implementation can more elegantly (and efficiently) support constraints and abduction in ASP. The broader impacts of this work include the availability of more powerful applications of knowledge representation; mechanisms for common sense reasoning; integration of advanced ASP systems into education and research venues; and the development of the research careers of graduate and undergraduate students, including those from under-represented groups."
111,1441892,Support for Young Researchers to attend the 2014 Intelligent Tutoring Systems Conference,IIS,"REAL, Cyberlearn & Future Learn Tech",6/1/14,5/5/14,Beverly Woolf,MA,University of Massachusetts Amherst,Standard Grant,christopher hoadley,5/31/15,"$14,000.00 ",,bev@cs.umass.edu,Research Administration Building,Hadley,MA,10359450,4135450698,CSE,"7625, 8020","7556, 8045, 8055",$0.00 ,"The United States has historically been the global leader in the field of Intelligent Tutoring Systems, or ways to use computerized artificial intelligence to enhance teaching and learning in contexts ranging from children learning math in school, to soldiers learning highly technical jobs in the US military. The preeminent conference in this field is the ITS conference; at this conference the latest research is presented and practitioners learn the state of the art techniques that allow creation of these important educational technologies. <br/><br/>This proposal would support seven Ph.D. students, selected through a competitive process, to attend the conference, present their work, and receive additional mentoring outside of their dissertation committees. The intellectual merit of the work rests on the studies the graduate students submit to be considered for participation in the early career track of the conference; this work is then enhanced by guidance from world-class mentors who meet with the students in a structured format to improve their research. The broader impact includes the career impact on the seven selected students, especially since promising graduate students whose advisors may not have funding to send them to the conference can still be included, and their work can be showcased and improved. Possible long-term broader impacts include building the field of ITS researchers and improving the quality of tutoring systems, and thus eventually, improving the quality of education."
112,1350339,"CAREER: Combining Crowdsourcing and Computational Creativity to Enable Narrative Generation for Education, Training, and Healthcare",IIS,HCC-Human-Centered Computing,2/15/14,12/29/17,Mark Riedl,GA,Georgia Tech Research Corporation,Continuing Grant,William Bainbridge,1/31/20,"$549,998.00 ",,riedl@cc.gatech.edu,Office of Sponsored Programs,Atlanta,GA,303320420,4048944819,CSE,7367,"1045, 7367",$0.00 ,"The proposed project explores the problem of automated narrative generation, the creation of narratives by computer systems. The project introduces a transformative new approach to narrative generation that blends human and computational creativity with crowdsourcing. The system addresses fundamental limitations of computer reliance on pre-coded domain knowledge in order to generate a virtually unlimited variety of narratives and make it possible for non-experts and non-programmers to create interactive narratives.  The research has four major components: (1) Develop artificial intelligence algorithms that emulate human ability to create narratives. (2) Design and implement novel models of human-computer creative collaboration. (3) Study fundamental questions pertaining to human narrative learning and cognition. (4) Explore the role of narrative generation in real-world domains: virtual agents that create rapport with humans and intelligent creativity augmentation tools for creating and sharing interactive experiences. The work will be piloted in two healthcare systems: a virtual agent that creates rapport and fosters longitudinal engagement with patients through autobiographical narratives; and intelligent tools that allow caregivers to create social skill scenarios for young adults with autism to practice. <br/><br/>Narratives are important because they are a fundamental means by which humans organize, understand, and explain the world. If computer systems could create effective narratives, they would be better able to interact with people. The research will result in novel algorithms, software, and a body of experimental knowledge that will enable the building of interactive narrative systems that are practical, scalable, usable by non-programmers, and can address societally important problems in education, training, and healthcare interventions. The proposed approach to creativity support will significantly lower the technical, artistic, and skill barriers to creating interactive narrative systems, opening avenues for educators, trainers, caregivers, and hobbyists to create and share interactive experiences.<br/><br/>The project includes an educational plan to develop a sustainable, annual summer hack-a-thon camp wherein high school students work alongside K-12 teachers to create interactive narratives that motivate and guide classroom inquiry based learning. The hack-a-thon aims to provide minority and low socioeconomic high school students with hands-on computing science experience and to produce a library of interactive inquiry-based learning software systems for K-12 teachers."
113,1420316,RI: Small: A Systematic Approach to Robot Task and Motion Planning in Belief Space,IIS,ROBUST INTELLIGENCE,8/15/14,8/13/14,Tomas Lozano-Perez,MA,Massachusetts Institute of Technology,Standard Grant,Reid Simmons,7/31/18,"$450,000.00 ",Leslie Kaelbling,tlp@csail.mit.edu,77 MASSACHUSETTS AVE,Cambridge,MA,21394301,6172531000,CSE,7495,"7495, 7923",$0.00 ,"Non-technical Abstract:<br/>Robots have the potential for wide-ranging positive impacts on society in complex or dangerous applications such as disaster relief, elder care and the reshoring of manufacturing jobs.  However, existing robots have had limited success is these domains, mainly because the planning and control algorithms are not robust to misconceptions in the robot's ""understanding"" of its environment nor to small imperfections in the robot's ability to execute the required actions.  The overall goal of this project is to develop the sensing, planning, and control algorithms necessary to overcome these problems, and hence necessary to allow robots to work productively in complex domains shared with humans.<br/><br/>The key activities of this project are the development of new ways of representing uncertainty in the state of the world that support efficient planning for robots. These new representations and algorithms provide principled and practical methods of integrating perception and action in complex domains. The resulting algorithms are tested in the context of a real robot performing household tasks in a kitchen environment.<br/><br/>The project also involves a thorough integration of research and education. Graduate and undergraduate students are involved in all aspects of the research. Furthermore, the research in this project forms the basis of an undergraduate subject on robot planning algorithms under development at MIT.<br/><br/>Technical Abstract:<br/>The overall goal of this project is to develop the estimation, planning, and control techniques necessary to enable robots to perform robustly and intelligently in complex uncertain domains. Robots operating in complex, unknown environments have to deal explicitly with uncertainty. Sensing is increasingly reliable, but still inescapably local: robots cannot see, immediately, inside cupboards, under collapsed walls, or into nuclear containment vessels. Planning, whether in household and disaster-relief domains, requires explicit consideration of uncertainty and the selection of actions at both the task and motion levels to support gathering information.<br/><br/>In order to explicitly consider the effects of uncertainty and to generate actions that gain information, it is necessary to plan in belief space: that is, the space of the robot's beliefs about the state of its environment, which we will represent as probability distributions over states of the environment. For planning purposes, the initial state is a belief state and the goal is a set of belief states: for example, a goal might be for the robot to believe with probability greater than 0.99 that all of the groceries are put away in acceptable locations, or that there are no survivors remaining in the rubble.<br/><br/>This project is developing a systematic, integrated approach to finding plans efficiently in high-dimensional uncertain domains. By factoring the belief space and exploiting a decoupling between geometric and probabilistic reasoning, this approach can employ constraint satisfaction methods to generate good solutions relatively efficiently. This program of basic research provides conceptual, formal, algorithmic, and software results that are of use in mobile manipulation robotics, as well as artificial intelligence more generally, including applications from medical diagnosis and treatment to electronic commerce to managing energy production and distribution systems."
114,1416980,An Intelligent Ecosystem for Science Writing Instruction,DRL,Discovery Research K-12,9/1/14,3/29/16,Christian Schunn,PA,University of Pittsburgh,Standard Grant,David B. Campbell,3/31/16,"$44,009.00 ","Diane Litman, Amanda Godley",schunn@pitt.edu,300 Murdoch Building,Pittsburgh,PA,152603203,4126247400,EHR,7645,,$0.00 ,"The ability to express scientific ideas in both written and oral form is an important 21st century skill.   Teachers, employers, and college faculty lament the inability of many high school graduates to write clearly.  This deficit in writing is due in part because teachers do not have the time to provide appropriate, timely feedback to students on their writing. This project would help teachers help students achieve these skills through automating an effective feedback process, in ways that are customized to particular disciplines and local classroom needs, particularly in high needs districts.  The project will contribute to knowledge about how students learn to write and how computer assisted systems can support this learning.<br/><br/>This project will develop and test three tools:  1) Teaching resources organized as developmental trajectories for teachers to use (e.g. from more simple to more complex; with diagnostics and strategies for addressing particular challenges); 2) A teacher dashboard that uses Artificial Intelligence tools to provide timely formative assessment to teachers by highlighting problem areas in their students' writing and peer reviews; and 3) An online teacher resource exchange to rapidly grow the set of appropriate assignments that can be used with this approach, critically filtered by student performance metrics.  The project builds on a current system called SWoRD, which supports student peer reviewing in many disciplines within and beyond science.  Working with six lead teachers and larger set of pilot teachers, the project  will develop a trajectory of effective writing assignments in Biology, Chemistry, and Physics.  In year three, there will be a summative evaluation with 90 teachers."
115,1409549,RI: Medium: Collaborative Research: Experience-Based Planning: A Framework for Lifelong Planning,IIS,"Robust Intelligence, NRI-National Robotics Initiati",8/1/14,6/20/16,Maxim Likhachev,PA,Carnegie-Mellon University,Standard Grant,James Donlon,7/31/18,"$363,995.00 ",,maxim@cs.cmu.edu,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,CSE,"7495, 8013","7495, 7924, 9251",$0.00 ,"Robots need to improve their behavior over time, yet produce consistent behavior in order to allow humans to predict their actions, which is necessary to develop trust in their behavior or even cooperate with them. Furthermore, many tasks repeat, such as opening drawers. This project develops technology that addresses these issues by viewing planning as a lifelong process and exploiting the structure of human environments for efficiency, for example that drawers typically open in similar ways.<br/><br/>This research collaboration is developing a framework for lifelong planning based on experience graphs that aims to improve performance of planning over time by exploiting past experiences when solving similar planning tasks. The concept is novel because experiences are used to guide the heuristic search as opposed to be used for mere replay or adaptation. The idea that makes this possible is a novel heuristic search-based framework that can take advantage of prior experiences and still provide rigorous guarantees on completeness and path quality. The team studies how experiences can be utilized effectively during planning, how planning should gather experiences, how it should prune redundant experiences and how it can obtain experiences from demonstrations. Applications include everyday household tasks and low-volume manufacturing tasks. The software developed in this collaborative research is being integrated into the SBPL library, one of the core libraries in ROS. The project also incorporates educational activities as well as activities that help to bridge the research communities in robotics and artificial intelligence, two separate communities despite their common interest in autonomous systems."
116,1350671,"CAREER: Problem Solving in Dynamic, Distributed Environments",IIS,"Robust Intelligence, EPSCoR Co-Funding",1/15/14,1/23/14,Roger Mailler,OK,University of Tulsa,Standard Grant,James Donlon,12/31/19,"$450,793.00 ",,roger-mailler@utulsa.edu,800 S. Tucker Drive,Tulsa,OK,741049700,9186312192,CSE,"7495, 9150","1045, 7495, 9150",$0.00 ,"Computers are increasingly used to monitor and manage many aspects of our daily lives.  These systems are often required to work together to solve complex problems that are rapidly changing.  Current approaches to addressing these situations develop tailored distributed protocols that are verified through empirical testing.  This project increases the practical applicability of distributed problem solving techniques by developing a theoretical model of these problems based on thermodynamic theory.  Using this model, a protocol's performance can, for the first time ever, be predicted under previously untested conditions.<br/><br/>This theoretical model is validated through extensive empirical evaluation and this project develops a new protocol that alters its problem solving strategy to maximize the trade-off between deliberate and reactive decision making based on environmental dynamics.  This protocol is applied to address a pressing practical problem: allocating telescopes for tracking objects in Low Earth Orbit (LEO). With nearly all of our manned space missions and satellites in LEO, effectively monitoring space debris has broad implications to society at large and scientific progress along numerous directions.<br/><br/>This transformative research combines cross-disciplinary ideas from artificial intelligence, distributed systems, and statistical physics.   The educational initiatives in this project directly address the recruitment and retention of students, especially focusing on women and minorities, into Computer Science by generating excitement through the Heartland Gaming Expo and by utilizing a new peer outreach program, called Engineering Ambassadors."
117,1423260,CHS: Small: Advanced Design Principles for Computer Simulated Agents,IIS,HCC-Human-Centered Computing,9/1/14,12/17/19,Christine Lisetti,FL,Florida International University,Continuing Grant,William Bainbridge,8/31/20,"$545,618.00 ","Mark Williams, Maya Boustani",lisetti@cis.fiu.edu,11200 SW 8TH ST,Miami,FL,331990001,3053482494,CSE,7367,"7367, 7923, 9251",$0.00 ,"This project will investigate human interaction with simulated agents in situations where humans expect empathic communication, such as healthcare.  In domains such as this, the person interacts with computer-based interventions (CBI) as an education process evolves.  Almost all psychosocial interventions currently available on the web are delivered merely via text.  Research indicates that many users will lose interest and drop out, although completion is critical to achieving desired goals.  Human-computer interaction literature suggests that interacting with simulated agents can increase the user's engagement, but that the user will expect social competence when interacting with them.  This project will answer a set of research questions for the design of simulated characters with some empathic intelligence, to create a new modality for the delivery of CBIs.  The specific health-related application area will facilitate development and evaluation of new techniques and design principles, that will have much wider applicability, potentially whenever people interact with computer-based systems through simulated agents. <br/><br/>This research will advance the ability of computer scientists to create competent simulated agents that can adapt their verbal and non-verbal behavior to the user's affective states, and over time tailor their interaction to the specific user to produce the maximum positive impact in terms of users' engagement and achievement of their goals.  The model of empathy and social competence developed will enable simulated characters to adapt in real time to a user's short-lived emotions over a single interactive session.  Longer-term affective states will be modeled over long-term interaction via follow-up sessions with the same individual.  Thus, development of rapport between a human and an artificial intelligence is a dynamic process over time, and the research is expected to discover new principles that might not be seen in exclusively short-term interactions. The project will provide: (1) a scheme to design tailored interaction by constructing a dynamic user-model with user's demographic information and fluctuating personal characteristics; (2) a model of empathic verbal communication built by combining motivational interviewing techniques with an ontology-based dialog system; (3) a computational model for the integration of verbal and non-verbal communication cues by adapting the character's facial expressions, vocal modulation, and kinesics to its verbal utterances in the context of the session.  In addition, the research will engage scientific questions about diversity in communication style across human groups, facilitated by the fact that the project is housed at the region's principal minority-serving research university.  The fact that the research will involve students in the development of systems that integrate multiple technologies will give them an excellent educational experience, gaining competence that will be valuable for a range of future careers."
118,1409987,RI: Medium: Collaborative Research: Experience-Based Planning: A Framework for Lifelong Planning,IIS,Robust Intelligence,8/1/14,7/13/20,Sven Koenig,CA,University of Southern California,Standard Grant,James Donlon,7/31/21,"$348,000.00 ",,skoenig@usc.edu,University Park,Los Angeles,CA,900890001,2137407762,CSE,7495,"7495, 7924, 9251",$0.00 ,"Robots need to improve their behavior over time, yet produce consistent behavior in order to allow humans to predict their actions, which is necessary to develop trust in their behavior or even cooperate with them. Furthermore, many tasks repeat, such as opening drawers. This project develops technology that addresses these issues by viewing planning as a lifelong process and exploiting the structure of human environments for efficiency, for example that drawers typically open in similar ways.<br/><br/>This research collaboration is developing a framework for lifelong planning based on experience graphs that aims to improve performance of planning over time by exploiting past experiences when solving similar planning tasks. The concept is novel because experiences are used to guide the heuristic search as opposed to be used for mere replay or adaptation. The idea that makes this possible is a novel heuristic search-based framework that can take advantage of prior experiences and still provide rigorous guarantees on completeness and path quality. The team studies how experiences can be utilized effectively during planning, how planning should gather experiences, how it should prune redundant experiences and how it can obtain experiences from demonstrations. Applications include everyday household tasks and low-volume manufacturing tasks. The software developed in this collaborative research is being integrated into the SBPL library, one of the core libraries in ROS. The project also incorporates educational activities as well as activities that help to bridge the research communities in robotics and artificial intelligence, two separate communities despite their common interest in autonomous systems."
119,1402004,Environmentally Responsive Supramolecular Constructs and Models for Chemical Communication,CHE,Macromolec/Supramolec/Nano,8/1/14,6/2/17,Jonathan Sessler,TX,University of Texas at Austin,Standard Grant,George Janini,7/31/18,"$450,000.00 ",,sessler@mail.utexas.edu,"3925 W Braker Lane, Ste 3.340",Austin,TX,787595316,5124716424,MPS,6885,7234,$0.00 ,"With this award from the Macromolecular, Supramolecular and Nanochemistry Program, Jonathan L. Sessler of The University of Texas is carrying out research aimed at creating new systems that can communicate chemically. Living systems are unique in their ability to communicate by purely chemical means and a wide array of new technologies could become possible if we were able to mimic this behavior. Chemical communication in living systems can occur over short distances, which is what happens when brain cells communicate by the release and capture of neurotransmitter molecules. Or, the communication can take place over large distances, such as when insects, or even some mammals, communicate by releasing pheromones. Although these processes are well known in the world of biology, it has not yet been possible to reproduce this type of communication in an artificial system. In this work, the investigators are doing just that, using sophisticated laboratory methods they have developed to accomplish molecular design and construction in a controlled fashion. This work will likely impact the development of possible new information systems. New approaches to artificial intelligence and wireless communication may become possible as a result of this fundamental research. The work is having a further broad impact through the training of the next generation of scientists in cutting-edge laboratory techniques that will prepare the students involved in this research for work in a wide variety of technologically challenging areas.<br/><br/>This project focuses on the use of electron rich and electron poor receptors whose features can be modulated via changes in pH, redox potential, and substrate binding. Particular emphasis is being placed on tetrathiafulvalene calixpyrrole receptors and various electron deficient substrates, including fullerene derivatives. Conformational switching, as engendered by anion binding to a calixpyrrole, for instance, is used to induce changes in equilibrium. These perturbations, in turn, serve to release small chemical species that can act as triggers for the modulation of other receptor systems, such as those built from oligopyrroles. A range of techniques, including standard spectroscopies, photochemistry, color-based reactions, and solid state crystallography is being used to characterize the systems involved in this project and the changes in their features due to induced perturbations in equilibrium behavior."
120,1400784,A Grammar-Based Approach to Visual-Haptic Object Perception,BCS,"Perception, Action & Cognition, HCC-Human-Centered Computing, Robust Intelligence",9/1/14,8/14/14,Robert Jacobs,NY,University of Rochester,Standard Grant,Betty Tuller,8/31/19,"$399,049.00 ",,robbie@bcs.rochester.edu,"518 HYLAN, RC BOX 270140",Rochester,NY,146270140,5852754031,SBE,"7252, 7367, 7495","7252, 7367, 7495, 9251",$0.00 ,"People can perceive the shape of objects accurately and reliably but how this occurs is not yet understood. This ability may stem, at least in part, from our use of both visual information and haptic information (information obtained when an object is touched or grasped). Moreover, if we learn to recognize an object based on visual information, we can often recognize the same object when our eyes are closed but we are allowed to grasp it. Similarly, if we learn to recognize an object based on haptic information, we can often recognize the object when we see it but cannot touch it. In other words, we exhibit cross-modal transfer of object shape information. How does information from the eyes and hands link up in the brain to yield a coherent representation of object shape? Insights obtained from this research can contribute both to our understanding of how humans perceive object shape using vision and/or touch and to development of improved robotic and other artificial intelligence systems operating in multi-modal settings in industrial, medical, military, and other applications.<br/><br/>The present project develops a theory of visual-haptic object shape perception in which people's notions of object similarity are not based on sensory features but rather on latent or hidden variables that represent object parts and their spatial relations in an abstract, modality-independent format. Object representations are formalized using a probabilistic ""shape grammar"" with Bayesian inference used to infer grammar-based object representations when an object is viewed, when it is grasped, or both. The model is tested using data obtained from behavioral studies of visual, haptic, and visual-haptic object shape perception by humans. The investigators will explore the types of representational change that underlie the transition from perceptual novice to expert (e.g, radiologists) and will assess whether perceptual expertise is well characterized as category learning, grammar learning, both, or neither. The research program will also develop a large public database of code for re-creating both visual and haptic features of complex objects. This will allow other researchers to fabricate the objects using a 3D printer, enhancing complementarity and comparison across research sites. Finally, training undergraduate and graduate students in the emerging field of computational cognitive science will contribute to a new generation of multidisciplinary scientists working across traditional boundaries between cognitive science and computer science."
121,1418922,WORKSHOP: HRI 2014 Pioneers,IIS,"Information Technology Researc, HCC-Human-Centered Computing",1/15/14,1/9/14,Robin Murphy,TX,Texas A&M Engineering Experiment Station,Standard Grant,Ephraim Glinert,12/31/14,"$35,016.00 ",,robin.r.murphy@tamu.edu,400 Harvey Mitchell Pkwy S,College Station,TX,778454645,9798626777,CSE,"1640, 7367","7367, 7556",$0.00 ,"This is funding to support a Pioneers Workshop (doctoral consortium) of approximately 18 graduate students and post-docs (including about 12 U.S. participants) from diverse research communities (e.g., computer science and engineering, psychology, cognitive science, robotics, human factors, human-computer interaction design, and communications), along with distinguished research faculty.  NSF funding will be used solely to cover travel, housing, and subsistence for eligible U.S. attendees.  The event will take place on Monday, March 3, 2014, immediately preceding the Ninth International Conference on Human Robot Interaction (HRI 2014), to be held March 4-6 in Bielefeld, Germany, and which is jointly sponsored by ACM and IEEE.  HRI is a single-track, highly selective annual international conference that seeks to showcase the very best inter- and multi-disciplinary research in human-robot interaction with roots in social psychology, cognitive science, HCI, human factors, artificial intelligence, robotics, organizational behavior, anthropology and many more, and invites broad participation.  The theme of HRI 2014 is ""(E)Merging Perspectives"" which seeks to combine both user and system perspectives to advance new and possibly unorthodox methodologies.  To extend the current singular approaches, this year's conference emphasizes papers that demonstrate the usage of novel empirical methods, the integration of empirical findings into complex robot systems, and holistic approaches in system evaluation.  More information about the conference is available online at http://humanrobotinteraction.org/2014. <br/><br/>The Pioneers Workshop is designed to complement the conference, by providing a forum for students and recent graduates in the field of HRI to share their current research with their peers and a panel of senior researchers in a setting that is less formal and more interactive than the main conference.  During the workshop, participants will talk about the important upcoming research themes in the field, encouraging the formation of collaborative relationships across disciplines and geographic boundaries.  To these ends, the workshop format will include oral presentations from 2 student attendees, poster presentations from all attendees, a hands-on meet-and-greet session, two alumni speakers, a keynote, and a panel presentation by senior researchers.  The oral presentations and the interactive poster sessions will provide a forum for participants to share their research, enabling them to receive feedback on their work and to gain perspective on the field.  The hands-on meet-and-greet session will involve networking and the cultivation of cross-disciplinary ideas.  The alumni presentations will provide advice for short-term goals and a recent perspective on looking for jobs within the community.  The keynote lecture will provide a global and future vision for HRI.  The panel presentation will feature five senior HRI researchers from both academia and industry who will share insights about their own careers, answer career path questions, and provide insight into the interdisciplinary nature of the HRI community.  The conversations between the panel and participants will continue over lunch and during dinner.<br/><br/>Broader Impacts:  This workshop will afford a unique opportunity for the best of the next generation of researchers in human-robot interaction to be exposed to and discuss current and relevant topics as they are being studied in several different research communities (including but not limited to computer science and engineering, psychology, robotics, human factors and ergonomics, and HCI).  This is important for the field, because it has been recognized that transformative advances in research in this fledgling area can only come through the melding of cross-disciplinary knowledge and multinational perspectives.  Participants will be encouraged to create a social network both among themselves and with senior researchers at a critical stage in their professional development, to form collaborative relationships, and to generate new research questions to be addressed during the coming years.  Participants will also gain leadership and service experience, as the workshop is largely student organized and student led.  The PI has expressed her strong commitment to recruiting women and members from under-represented groups.  To further ensure diversity the event organizers will consider an applicant's potential to offer a fresh perspective and point of view with respect to HRI, and will limit the number of participants accepted from a particular institution to two, with one being a woman if two are accepted"
122,1447570,Doctoral Consortium Support for the 2014 International Conference on Automated Planning and Scheduling,IIS,ROBUST INTELLIGENCE,8/15/14,8/3/14,Julie Shah,CA,Association for the Advancement of Artificial Intelligence,Standard Grant,Jie Yang,7/31/15,"$16,000.00 ",,arnoldj@mit.edu,2275 E BAYSHORE RD STE 160,East Palo Alto,CA,943033224,6503283123,CSE,7495,"7495, 7556",$0.00 ,"This student travel grant supports graduate/undergraduate and young researchers to attend the 2014 International Conference on Automated Planning and Scheduling and to participate in the Doctoral Consortium (ICAPS-DC). The ICAPS DC is a primary method for broadening participation and improving retention of doctoral researchers in the field of automated planning and scheduling. The DC at ICAPS 2014 is the 12th occurrence of the event, and takes place in June 2014 in Portsmouth, NH, USA. This is the first ICAPS held in the US since 2007. Those receiving awards through the Support Fund are able to participate in events, including but not limited to the DC, where they can receive career and research advice from peers and mentors, present preliminary results and plans for their dissertations, and build professional relationships. To support career development, invited speakers present emerging research opportunities, discuss research skills and the process of transitioning to the workplace after graduation. The participation of Ph.D. students at ICAPS 2014 and the DC is be beneficial to both the students and the international planning community, and to ensure cross-fertilization among researchers in areas of planning, scheduling, robotics, operations research, combinatorial search, knowledge representation and reasoning, and applications."
123,1414379,PFI:AIR - TT:  Toward Commercialization: Development of Neural Network Control and Power Converter Prototype for Renewables and Smart Grid Integration,IIP,Accelerating Innovation Rsrch,9/15/14,6/13/16,Shuhui Li,AL,University of Alabama Tuscaloosa,Standard Grant,Prakash Balan,8/31/17,"$217,780.00 ","Donald Wunsch, Paula Cordero, Rachel Frazier",sli@eng.ua.edu,801 University Blvd.,Tuscaloosa,AL,354870001,2053485152,ENG,8019,"116E, 8019, 9150, 9231, 9251",$0.00 ,"This PFI: AIR Technology Translation project focuses on translating a neural network vector control technology to fill the need for renewable and smart grid control and integration. The technology is important because it will improve the power quality and facilitate an uninterrupted energy supply, increase incentives for consumers to use energy from renewable resources and electric vehicles, and accelerate progress towards America's target of deriving 20% of its electrical energy from renewable resources by 2030. The project will result in a prototype of a grid-connected power converter using the neural network vector control technology. This technology has the following unique features: fast response time, low overshoot and close to ideal control performance. These features provide the advantages of improved efficiency, reliability, stability and power quality of an electric utility system with integrated renewables as compared to the leading competing conventional standard vector control technology in this market space.  <br/><br/>This project addresses the following technology gaps as it translates from research discovery toward commercial application: (1) proving the concept of the neural network control technology for grid-connected converters that meets the needs of electric power systems, (2) demonstrating a functional prototype power converter board that uses neural network control to integrate renewables into smart grids, (3) evaluating and benchmarking a commercially valuable solution of neural network control against conventional technology, and (4) developing a strategy for commercialization beyond this project. In addition, personnel involved in this project, PIs, undergraduates and graduates, will receive innovation translation experiences through activities from technical and business perspectives. The team of students, along with the PI, will participate in the Crimson Startup Canvas, UA's customer discovery program to understand the formal process through which to identify sustainable business models, increase the chance of attracting funding and investments, create new jobs and benefit society.<br/><br/>The project engages Southern Company Services to participate in an advisor role, offer guidance for the project, and guide commercialization aspects in this technology translation effort from research discovery toward commercial reality."
124,1441331,"EXP: Collaborative Research: PerSketchTivity- Empowering and Inspiring Creative, Competent, Communicative, and Effective Engineers through Perspective Sketching",IIS,"S-STEM-Schlr Sci Tech Eng&Math, Cyberlearn & Future Learn Tech",9/1/14,7/2/19,Tracy Hammond,TX,Texas A&M Engineering Experiment Station,Standard Grant,Edward Berger,9/30/19,"$433,056.00 ","Erin McTigue, Jeffery Liew",hammond@tamu.edu,400 Harvey Mitchell Pkwy S,College Station,TX,778454645,9798626777,CSE,"1536, 8020","7218, 8045, 8244, 8841, 9251",$0.00 ,"The Cyberlearning and Future Learning Technologies Program funds efforts that will help envision the next generation of learning technologies and advance what we know about how people learn in technology-rich environments. Cyberlearning Exploration (EXP) Projects explore the viability of new kinds of learning technologies by building examples and studying their possibilities for fostering learning as well as challenges to using them well. This project examines whether technology can support learning to freehand sketch. Sketching has been demonstrated to play an important role in a number of domains, including engineering, and the ability to quickly sketch has been shown to improve creativity by making it easier for engineers to generate ideas and communicate them. This project will modify artificial intelligence tools that support recognizing sketches to directly help teach undergraduate engineers how to sketch well. Research studies will examine whether the tool helps students learn sketching skills, and importantly how it influences their spatial reasoning ability. Thus, if successful this research will not only create tools to allow people to learn to sketch better, but also will advance our understanding of how spatial reasoning and sketching are linked, and could eventually lead to more effective engineering education.<br/><br/>The project proposes two interconnected strands of work: developing the software tool and conducting research studies in the context of undergraduate engineering courses. The software tool will use a heterogenous set of classifiers to help provide feedback to learners as they perform a sequence of sketching exercises on tablets. The design process will iterate on the tool to explore what types of feedback are most helpful and how different classifiers can be used to detect different levels of sketching skill. The program of research will include studying whether sketching training leads to advances in spatial reasoning skills, whether it affects design self-efficacy and attitudes towards sketching, transfer of spatial skillsets to design activities in other courses, and how sketching skills correlate to success on spatial reasoning tasks.  In addition, through iterative development including user-centered design processes, design principles for sketching based tools will be derived. Data sources will include both qualitative and quantitative data such as pre- and post-test spatial reasoning tasks, structured interviews, surveys, and artifact analysis. Additionally, students (N=approximately 30-40) using the new tool in class will be compared to control cohorts of approximately 30 students who either use traditional engineering curricula (little free-hand sketching and some isometric drawing) and a sketching curriculum without the AI tool."
125,1441291,"EXP: Collaborative Research: PerSketchTivity- Empowering and Inspiring Creative, Competent, Communicative, and Effective Engineers through Perspective Sketching",IIS,Cyberlearn & Future Learn Tech,9/1/14,8/22/14,Julie Linsey,GA,Georgia Tech Research Corporation,Standard Grant,Edward Berger,8/31/18,"$179,999.00 ",Wayne Li,julie.linsey@me.gatech.edu,Office of Sponsored Programs,Atlanta,GA,303320420,4048944819,CSE,8020,"8045, 8244, 8841",$0.00 ,"The Cyberlearning and Future Learning Technologies Program funds efforts that will help envision the next generation of learning technologies and advance what we know about how people learn in technology-rich environments. Cyberlearning Exploration (EXP) Projects explore the viability of new kinds of learning technologies by building examples and studying their possibilities for fostering learning as well as challenges to using them well. This project examines whether technology can support learning to freehand sketch. Sketching has been demonstrated to play an important role in a number of domains, including engineering, and the ability to quickly sketch has been shown to improve creativity by making it easier for engineers to generate ideas and communicate them. This project will modify artificial intelligence tools that support recognizing sketches to directly help teach undergraduate engineers how to sketch well. Research studies will examine whether the tool helps students learn sketching skills, and importantly how it influences their spatial reasoning ability. Thus, if successful this research will not only create tools to allow people to learn to sketch better, but also will advance our understanding of how spatial reasoning and sketching are linked, and could eventually lead to more effective engineering education.<br/><br/>The project proposes two interconnected strands of work: developing the software tool and conducting research studies in the context of undergraduate engineering courses. The software tool will use a heterogenous set of classifiers to help provide feedback to learners as they perform a sequence of sketching exercises on tablets. The design process will iterate on the tool to explore what types of feedback are most helpful and how different classifiers can be used to detect different levels of sketching skill. The program of research will include studying whether sketching training leads to advances in spatial reasoning skills, whether it affects design self-efficacy and attitudes towards sketching, transfer of spatial skillsets to design activities in other courses, and how sketching skills correlate to success on spatial reasoning tasks.  In addition, through iterative development including user-centered design processes, design principles for sketching based tools will be derived. Data sources will include both qualitative and quantitative data such as pre- and post-test spatial reasoning tasks, structured interviews, surveys, and artifact analysis. Additionally, students (N=approximately 30-40) using the new tool in class will be compared to control cohorts of approximately 30 students who either use traditional engineering curricula (little free-hand sketching and some isometric drawing) and a sketching curriculum without the AI tool."
126,1353757,IDBR Type A: Miniaturized Two-photon Microscopy for Deep Brain Imaging: An Integrated Circuit Design Using Electrowetting Optics,DBI,"INSTRUMENTAT & INSTRUMENT DEVP, BioP-Biophotonics, Cross-BIO Activities",4/1/14,3/1/16,Juliet Gopinath,CO,University of Colorado at Boulder,Continuing Grant,Robert Fleischmann,3/31/19,"$945,874.00 ","Diego Restrepo, Victor Bright, Emily Gibson",juliet.gopinath@colorado.edu,"3100 Marine Street, Room 481",Boulder,CO,803031058,3034926221,BIO,"1108, 7236, 7275",8007,$0.00 ,"An award is made to the University of Colorado to do deep brain imaging using a novel miniature nonlinear microscope. Optical imaging methods combined with fluorescent markers offer the unprecedented ability to study functioning of the complex neural networks in the brain down to the resolution of individual neurons. However, due to light scattering in tissue, over 75% of the brain cannot be studied. Technology that offers the path for high resolution deep brain functional imaging is urgently needed in order to further advance the fundamental understanding of how the brain works. This project will investigate a fiber-optic imaging instrument incorporating adaptable optics. The miniature fiber-optic imaging system implanted minimally-invasively will enable visualization of thousands of neurons deep in the brain. The large volume of imaging is important for understanding the complex interconnections involved in neural networks while access to new regions of the brain will open up study in important areas of the brain that are currently not accessible with other techniques.<br/><br/>The work is interdisciplinary in nature, combining aspects of biology, materials science, physics, and engineering and will provide excellent opportunities for students to broaden their scientific knowledge outside of a specific discipline. The PI's will disseminate the results of their work through teaching and education outreach that includes student groups, undergraduate research opportunity programs and summer programs for under-represented undergraduates. Beyond basic research, results from this project will be used to further the understanding of brain function, advance artificial intelligence, and treat neurological disorders."
127,1354297,Innovations in Electroacoustics and Computing: Print Disablity and as a Model for Technology Innovation and Transfer,SES,"HCC-Human-Centered Computing, STS-Sci, Tech & Society",5/15/14,5/5/14,Mara Mills,NY,New York University,Standard Grant,Frederick Kronz,4/30/17,"$239,138.00 ",,mmills@nyu.edu,70 WASHINGTON SQUARE S,NEW YORK,NY,100121019,2129982121,SBE,"7367, 7603","1353, 7603",$0.00 ,"Through archival research and interviews with innovators, the PI will produce a history of electronic reading technologies for blind and print-disabled people, and their co-evolution with mainstream reading practices. Beyond the introduction of new formats such as audiobooks and electronic books, print access efforts in the twentieth century gave rise to numerous technical innovations that transferred to other branches of electroacoustics and computing. Innovations in long-playing records,pitch-shifting with magnetic tape, scanning, optical character recognition (OCR), and synthetic speech ultimately retooled reading for both humans and machines. The project will contribute to the history of computing through attention to the overlooked topics of optical character recognition (OCR) as a mode of data input, and pattern-matching as a technique for artificial intelligence.<br/><br/><br/>Based on these historical examples, this communications scholar develops new tools for understanding and stimulating innovative technology design and transfer. The work will contribute to the subfield of disability and STS, will train two disabled students, and will help destigmatize the category of assistive technology by tracing the ways these devices intervene into media policy, are repurposed for broad use, or in fact are marketed to multiple audiences. In addition to a monograph, the project will result in a website that will preserve and make publicly-available examples from several historical reading formats (e.g. Talking Books, text-to-tone, and text-to-speech systems). The website will model best practices of electronic accessibility."
128,1335137,"GOALI: Improving Blood Collection, Production, and Inventory Operations",CMMI,"GOALI-Grnt Opp Acad Lia wIndus, MANFG ENTERPRISE SYSTEMS, OPERATIONS RESEARCH",5/15/14,8/16/13,Turgay Ayer,GA,Georgia Tech Research Corporation,Standard Grant,Georgia-Ann Klutke,4/30/17,"$360,000.00 ","Chelsea White, John DeShane, Zeynep Ozkaynak",turgay.ayer@isye.gatech.edu,Office of Sponsored Programs,Atlanta,GA,303320420,4048944819,ENG,"1504, 1786, 5514","071E, 1504, 1786, 5514, 9147, MANU",$0.00 ,"The objectives of this Grant Opportunity for Academic Liaison with Industry (GOALI) award are: (1) to develop and analyze mathematical and computational models of key elements of the blood products collection, production, and inventory system at a large regional level in order to improve the overall system performance and (2) to advance the state of knowledge in the analysis of these models. The methods that will be used and extended include: completely observed and partially observed Markov decision processes (MDPs) and Markov games, chance constrained MDPs, artificial intelligence-based heuristic search algorithms, mathematical programming, exact and approximate dynamic programming, and stochastic programming.  In collaboration with the American Red Cross (ARC), a comprehensive decision support system (DSS) based on proposed models and algorithms will be built. The outcomes of this DSS will be tested for validation in real collection, production, and inventory environments at the ARC production facility, and at hospitals and blood banks in the region.<br/><br/>This research is intended to directly impact the quality of delivery in 120 regional hospitals and health facilities and hence can potentially affect millions of people in the Southeast United States. If successful, this research will lead to a more cost-effective blood products collection, production, and inventory system with fewer blood product unit stock outs and with a reduced number of blood product units exceeding their lifetime. Furthermore, the models and decision support tools developed as part of these projects may ultimately be adapted by other service regions in the United States."
129,1450502,A Workshop on Extensible Distributed Systems,CNS,Computer Systems Research (CSR,9/1/14,8/25/14,Robbert VanRenesse,NY,Cornell University,Standard Grant,M. Mimi McClure,8/31/15,"$60,772.00 ",Lorenzo Alvisi,rvr@cs.cornell.edu,373 Pine Tree Road,Ithaca,NY,148502820,6072555014,CSE,7354,7556,$0.00 ,"A Distributed System is a system consisting of multiple computers communicating through message passing.  In the past 50 years, distributed systems have evolved from being a novelty to a fact of life---a very large fraction of computers today are part of a distributed system.  With computers, sensors, and actuators becoming embedded in virtually every device, users are themselves becoming, as it were, embedded in a ubiquitous computing fabric.  An Extensible Distributed System is a distributed system that is pervasively integrated into people's environments.  The distributed systems research community has developed models and techniques that have supported this remarkable success story and enabled technologies, such as cloud computing, that have the potential to fundamentally change the way in which individuals, businesses, and government access computing resources.  The pervasive or ubiquitous systems community in the meantime has leveraged distributed systems and networking technology, as well as artificial intelligence, to integrate devices even closer into our experience.  And in the post Dennard-Scaling era, computer architecture is undergoing a profound transformation, and, in an effort to address the conflicting goals of delivering performance and saving energy, it is exploring designs that are exporting weaker abstractions to software developers.<br/><br/>There are, however, gaps between the foundations built by the various research communities and the opportunities offered by today's and tomorrow's applications and infrastructure.  To respond to these challenges, the field not only needs to profoundly retool today's techniques and abstractions, but also to develop a new vision, designed to address the opportunities offered by new hardware, new networking, and new applications. This project supports a workshop that will serve as a catalyst for triggering the process of reflection and innovation necessary to develop this new vision and the research agenda that will bring it about.<br/><br/>Extensible Distributed Systems are the cornerstone of, to name a few, cloud computing, sensor/actuator networks, and industrial control systems, and their impact and ubiquity is likely to only increase in the next decade. The in-depth discussions about the state of field and research agenda that this workshop aims to develop are thus likely to have tremendous impact on society. In addition to helping define a research agenda for the extensible distributed systems community, this workshop will help crystallize the shape of future graduate and undergraduate education in distributed computing.  The workshop will balance academic and industrial participation, will engage leaders in the field, and will actively seek participation from underrepresented groups."
130,1442208,Workshop: Doctoral Consortium for HCOMP 2014,IIS,HCC-Human-Centered Computing,5/1/14,4/28/14,Matthew Lease,TX,University of Texas at Austin,Standard Grant,William Bainbridge,4/30/15,"$24,966.00 ",,ml@ischool.utexas.edu,"3925 W Braker Lane, Ste 3.340",Austin,TX,787595316,5124716424,CSE,7367,"7367, 7556",$0.00 ,"This is funding to support a Doctoral Consortium (workshop) of promising doctoral students and distinguished research faculty to be held in conjunction with the HCOMP 2014 Conference on Human Computation & Crowdsourcing, sponsored by the Association for the Advancement of Artificial Intelligence (AAAI). The Consortium will enhance the scientific workforce in this emerging research area by developing a group of promising young researchers interested in human computation and crowdsourcing. The award will also enable these young researchers to attend the HCOMP 2014 conference, thus allowing them to interact with other researchers and conference events; to learn of potential career paths within academia and industry; to access an international network of researchers who can support their professional development; and to observe the interdisciplinary nature, diversity and interrelationships of research in human computation.<br/><br/>The HCOMP 2014 Doctoral Consortium will be a research-focused meeting of an international group of selected Ph.D. candidates with a panel of distinguished research faculty. The Doctoral Consortium Chairs will select six additional distinguished researchers to serve as faculty mentors; this group also will serve as the review committee for student applications. Students will be selected based on a paper giving an overview of the student's dissertation research, an explanation of why the students wants to participate in the Doctoral Consortium, a CV, and an advisor's letter of support. The PIs will give preference to students who are most in need of mentoring and joining a peer group. <br/><br/>The full-day workshop event will include activities to guide the research of these promising young researchers. The Consortium will allow participants to interact with established researchers and with other students, through presentations, question-answer sessions, panel discussions, and invited presentations. Each participant will give a short presentation on their research and will receive feedback from at least one faculty mentor and from fellow students. The Consortium will include activities led by the faculty, such as a panel discussion, to give students more information about the process and lessons of research and life in academia and industry. To further integrate the Doctoral Consortium participants into the conference itself, students will have a chance to present their work as posters in an interactive poster session and their papers will be posted online on the workshop webpage. These activities will benefit the participants by offering each fresh perspectives and comments on their work from researchers outside their own institution, both from faculty and other students; providing a supportive setting for mutual feedback on participants' current research and guidance on future research directions; and enabling participants to form a cohort of new researchers."
131,1433021,REU Site: The Rice University Summer Institute of Statistics (RUSIS),DMS,WORKFORCE IN THE MATHEMAT SCI,3/1/14,4/21/14,Javier Rojo,NV,"Board of Regents, NSHE, obo University of Nevada, Reno",Continuing grant,Jennifer Slimowitz Pearl,12/31/15,"$134,611.00 ",,javier.rojo@oregonstate.edu,1664 North Virginia Street,Reno,NV,895570001,7757844040,MPS,7335,"9150, 9250",$0.00 ,"This award provides continued support for a successful 10-week summer REU site within the Statistics Department at Rice University for the study of Statistics and its applications. As the number of domestic graduate students in the Mathematical Sciences continues to decline, there is a critical need to develop human resources to continue supporting the United States' advantage in the world of science and technology. The Rice University Summer Institute of Statistics (RUSIS), now in its 10th year and fourth funding cycle, has been successful in encouraging students to pursue graduate degrees in Mathematics and Statistics. Roughly 85% of the students who have attended RUSIS and have graduated, are now doctoral students in Ph.D. programs around the country, and roughly 61% of them are members of underrepresented populations in Mathematics. RUSIS has accomplished this through intensive courses, close supervision of research projects, and visits to various research institutes and agencies in Houston. <br/><br/><br/>The program will train and mentor 12 (7 NSF- and 5 NSA-supported) selected underrepresented minority students and students with no easy access to a research experience at their institution, including community college students, through intensive core courses in probability, stochastic processes, and statistical inference, with special emphasis on areas of current interest; e.g. multiple comparisons, extreme value theory, multivariate survival analysis, risk-reliability-sustainability of complex infrastructure systems, artificial intelligence, statistical learning, statistical genetics, and general biostatistics. The RUSIS engages the students in research projects under close collaboration with faculty mentors, and with the objective of producing joint publications, when the summer work merits it. Students present their results at national meetings and they are mentored in the preparation and presentation of their talks. In addition, students meet with an advisory committee composed of top scientists and present their work to them. The RUSIS also teaches short courses on the use of Unix platforms, LaTeX, and software to be utilized for research purposes such as Mathematica, Splus and/or R, and Matlab. In addition, the program organizes student and faculty visits to scientific facilities (e.g., Biomathematics and Biostatistics at MD Anderson Cancer Center, NASA). Through informal meetings, the program discusses a variety of topics ranging from applying to Graduate School to career experiences by outstanding scientists, and discussion of cutting-edge topics in Statistics. The program evaluates and monitors the progress of students for seven years (expected time for them to finish graduate school) after their participation, and an annual evaluation of the program by the participating students and an external advisory committee is an integral and valuable part of the program. The investment is starting to produce concrete results. The first RUSIS alumnus received his Ph.D. during the summer of 2011 and is now an assistant professor of statistical sciences in a Ph.D. program. Several more are due to obtain their PhDs in the next few years."
132,1345724,"SBIR Phase I: Trains of Thought, a novel game-based learning curriculum module to teach STEM and entrepreneurship skills",IIP,SMALL BUSINESS PHASE I,1/1/14,11/6/13,James Niehaus,MA,Charles River Analytics Inc,Standard Grant,Glenn H. Larsen,6/30/14,"$149,966.00 ",,jniehaus@cra.com,625 Mount Auburn Street,Cambridge,MA,21384545,6174913474,ENG,5371,"5371, 8031, 8032, 8043, 9117",$0.00 ,"This SBIR Phase I project proposes to design and demonstrate the feasibility of Trains of Thought, a novel game-based learning curriculum module to teach STEM (science, technology, engineering, and mathematics) and entrepreneurship skills to high school students. This project has significant intellectual merit. US kindergarten through twelfth grade (K-12) STEM education is falling behind global standards (National Science Board, 2007; US Department of Labor, 2007), and current educational practice is not doing enough to prepare our students to compete in the global economy. Trains of Thought explores new applications of game-based learning to this problem. The approach combines (1) proven educational research in preparation for future learning (Schwartz & Martin, 2004) and guided experiential learning with scaffolding (Hmelo-Silver, Duncan, & Chinn, 2007); (2) an existing, popular, open source business simulation video game; and (3) artificial intelligence experience management (Weyhrauch, 1996), (Roberts, 2011), (Mott & Lester, 2006) to balance educational objectives with engaging gameplay, maintaining a sense of flow. The project builds upon emerging educational research results to develop a fundamentally new methodology for game-based learning. If successful, this effort should set new standards for the use of games for teaching STEM and entrepreneurship in K-12 classrooms.<br/><br/>The broader/commercial impact of the proposed research, curriculum module, and accompanying game software can help to reverse the trend of declining US STEM and entrepreneurship education. This effect directly supports our future business leaders in driving American innovation and our nation's long-term economic prosperity. The transformative concept of effective game-based learning has the potential to reach out to under-represented demographics in STEM-based entrepreneurial businesses - including women, African-American, Hispanic, and Latino populations - by providing students new ways to engage, interact, and learn from core educational content. The knowledge gained from the proposed effort also has significant implications to the scientific and educational communities for the design and application of game-based learning. Our principles, methods of development, and evaluation outcomes may form a model for developing additional curriculum modules on a variety of subjects, therefore increasing effectiveness of US STEM and entrepreneurship education in a wider context. These results may also spur new research efforts to refine and extend our findings in game-based learning, creating more effective STEM and entrepreneurship education in future efforts."
133,1416907,IBSS: New Methods for Investigating the Formation of Individual and Shared Concepts and Their Dynamic Dispersion Across Related Societies,SMA,Interdiscp Behav&SocSci IBSS,9/1/14,8/29/14,Kimberly Jameson,CA,University of California-Irvine,Standard Grant,Brian Humes,2/28/19,"$980,923.00 ","Louis Narens, Natalia Komarova, Dominik Wodarz",kjameson@uci.edu,"141 Innovation Drive, Ste 250",Irvine,CA,926173213,9498247295,SBE,8213,"8213, 8605",$0.00 ,"This interdisciplinary research project will develop and test new mathematical models that explore the ways through which conceptual meaning is represented in languages as those languages change in complexity over time.  The project investigators also will examine the ways such meaning is shared among groups of individuals in societies.  The project's models will describe the dynamic development of concepts as a geometric system and will provide methods for understanding the linguistic representation of concepts and the ways semantic meaning from one community can be influenced by that of neighboring communities.  Although this project will focus on the ways that color terms have evolved within languages and societies, the insights and information from this project will apply beyond the domain of color representation to any set of concepts in which objects have a similarity structure that can be assessed and described mathematically.  Examples of the kinds of situations where the approach and methods to be developed during this project will have utility are the following:  (1) the development of unambiguous and formally scalable artificial intelligence and robotic analogs of human classification and categorization systems; (2) the development of a global communication methodology that could be used to enhance  rapid global information messaging capabilities; and (3) the construction of standardized systems for information representation in critical systems, such as medical diagnostic systems and transportation systems. <br/><br/>The formation and communication of concepts permeate a diverse range of human activities.  They play roles in education systems; in the organization and design of transportation systems; in the physical and virtual design of retail markets and consumer goods; in classifications of quality and risk in medical diagnoses; in business performance; and in social values.  Psychologists, linguists, anthropologists, computer scientists, and other scholars have studied concepts by focusing on specific examples of concept formation while trying to understand how such conceptual systems are formed.  One specific concept that has received attention is how color terms ""evolve"" and how their conceptual meaning is understood and shared by members of a society.  Conceptualization of color is an important special case because color stimuli can be precisely measured and easily duplicated, and the human perceptual space of a million colors can be described with mathematical precision.  This project will focus on the development and testing of mathematical models that capture the ways color term concepts are categorized and shared.  The models to be designed and tested will use geometric formalisms for characterizing meaning in general and will specifically demonstrate their use by investigating color terms and concepts.  Testing will use data from a wide variety of societies, including the Mesoamerican Color Survey (MCS), a database of systematically collected categorization behaviors of 900 individuals who have communicated with one or more of 116 endangered or developing languages that are at various stages of color lexicon development.  The MCS is largely in hand-written form, and one product of this project will be its full digitalization using modern computer science crowd-sourcing methods.  Full digitalization of the MCS database will make it available for use by the global scientific community for the first time.  This project is supported through the NSF Interdisciplinary Behavioral and Social Sciences Research (IBSS) competition."
134,1443861,Enhancement of Spectrum Decision through Probabilistic Graphical Models,AST,EARS,9/15/14,9/6/14,Naima Kaabouch,ND,University of North Dakota Main Campus,Standard Grant,Jon Williams,8/31/18,"$291,599.00 ",,naima.kaabouch@engr.und.edu,264 Centennial Dr Stop 7306,Grand Forks,ND,582027306,7017774151,MPS,7976,"7976, 9150",$0.00 ,"The goal of this research is increase the consumer's quality of service in environments where more and more services are competing for use of the radio spectrum.  It is widely believed that future regulation of the spectrum will change from the current static environment, where opening new channels for communications takes years, to highly dynamic environment where smart radios will jump from frequency channel to frequency channel.   In this dynamic environment, the consumer will share the spectrum with many different types of devices, including radars, other phones, Wi-Fi servers and a host of new smart devices. <br/><br/>The new smart devices will constantly change their frequencies, data rates and modulation schemes, thereby creating a dynamic, more uncertain, environment. The scientists will explore how uncertainty influences the characterization of radio spectrum usage. The focus is on the sensing and decision-making aspects of the problem rather than management issues.   The scientists will apply their experience with Artificial Intelligence and transfer techniques of dynamic problem solving from other domains. The proposed study has the potential to develop decision support models that will inform new policies for spectrum management in the future. <br/><br/>This research project's primary aim is to advance the knowledge and understanding of wireless communication scenarios to enrich the spectrum decision process in cognitive radio by evaluating the impact of uncertainty on the different cognitive cycle changes of adaptive radios. To achieve this goal, they propose to identify, classify, and characterize the random and deterministic variables present in a typical wireless scenario and model their causal relations, using probabilistic graphical models, such as Bayesian networks and influence diagrams."
135,1415912,SBIR Phase I: Automatic Design and Optimization of Novel Drug Candidates,IIP,SMALL BUSINESS PHASE I,7/1/14,12/9/14,Shahar Keinan,NC,"Teraffinity, Inc.",Standard Grant,Ruth M. Shuman,6/30/15,"$174,950.00 ",,skeinan@cloudpharmaceuticals.com,"108 Fawn Dr., Suite 201",Wake Forest,NC,275879717,9103981200,ENG,5371,"163E, 5371, 8038",$0.00 ,"The broader impact/commercial potential of this Small Business Innovation Research (SBIR) project is to provide a tool for designing new drugs. Designing new drugs that bind to a specified protein target required finding the best molecule in a vast chemical space. The proposed design tool can search this large chemical space much more efficiently and cheaply compared to current methods. It is based on a ""reverse engineering"" method to solve the problem of going from a set of desired properties back to chemical structures that may have these properties. This design tool allows the efficient search of the database to find the best molecule in advance of laboratory work. The envisioned technology is expected to shorten the screening and drug discovery phase from 3 years to 1 year, while saving ~$20 M per target overall, by shifting current practices and enabling a rapid, lower cost, and more novel targeted drug discovery and lead identification. <br/><br/>This project proposes to develop a computational drug discovery platform. This platform uses advanced QM/MM (quantum chemistry and molecular chemistry combined) calculations for binding accuracy combined with an artificial intelligence heuristic search algorithm to find the most appropriate molecule in a vast molecular space. The advances to be accomplished with this SBIR project include adding two novel toolsl to improve the performance of the current ""in silico"" drug design algorithm: 1) a multi-object optimization algorithm to further improve high accuracy, and 2) an automated scaffold design algorithm. These will result in highly selective designs with novel scaffolds that not only bind well to the target, but that also have excellent drug-like properties such as low probability of toxicity and off target effects, as well as greater stability and synthesizability. These advancements should provide higher accuracy of binding affinity prediction results with fewer false positives and better molecule selection, and reduced labor, which allows for greater automation."
136,1351892,CAREER: Characterizing Object Recognition Machinery in a Newborn Visual System,BCS,DS -Developmental Sciences,4/15/14,8/14/17,Justin Wood,CA,University of Southern California,Continuing Grant,Peter Vishton,3/31/19,"$559,086.00 ",,justin.wood@usc.edu,University Park,Los Angeles,CA,900890001,2137407762,SBE,1698,"1045, 1698",$0.00 ,"How does early experience shape how we process and interpret visual information? Two major limitations have made this question difficult to answer. First, researchers can typically collect only a few data points from newborns, which prevents precise measurement of the infants' visual cognitive abilities. Second, human infants cannot ethically be raised in controlled environments from birth, which prevents researchers from studying how specific experiences shape the newborn mind. <br/><br/>To overcome these limitations, Dr. Wood has developed a new controlled-rearing method using a non-human animal model. This method can be used to measure all of a newborn's behavior (24 hours/day, 7 days/week) with high precision (9 samples/second) within strictly controlled environments. With support from this NSF CAREER award, Dr. Wood will use the new controlled-rearing method to characterize how newborns recognize objects at the onset of visual object experience. <br/><br/>Dr. Wood's laboratory will use a two-pronged approach. First, the lab will perform a series of controlled-rearing experiments with newborn chickens. Studies of chickens can inform human cognitive development because chickens and humans have similar neural processing systems for sensory information. These controlled-rearing experiments will reveal how specific visual experiences shape newborns' object recognition abilities. The findings will provide the foundation for a new, publicly-accessible database that describes how specific sensory experiences relate to specific behaviors in a newborn organism. <br/><br/>Second, the lab will build biologically-inspired computational models of newborns' object recognition behavior, using state-of-the-art techniques from artificial intelligence. These models will make predictions that can be compared to the data from the controlled-rearing experiments. This will help identify how the visual system processes objects. This approach integrates ideas from developmental psychology, vision science, and computational neuroscience, providing a unified framework for studying the origins of object recognition and other visual cognitive abilities."
137,1415653,SBIR Phase I:  Assistive Digital Vision for the Blind,IIP,SMALL BUSINESS PHASE I,7/1/14,12/17/14,Arman Ghodousi,VA,Ghodousi LLC,Standard Grant,Muralidharan S. Nair,6/30/15,"$172,500.00 ",,arman@g-technologygroup.com,5702 General Washington DR.,Alexandria,VA,223120000,4805443192,ENG,5371,"163E, 5371, 6840, 8035, 9139, HPCC",$0.00 ,"This Small Business Innovation Research (SBIR) Phase I project seeks to implement state-of-the art image processing algorithms designed to identify objects amidst a complex background, which currently require high power computer processing resources, onto an embedded device, which would allow the capability to be used in small, portable, and affordable devices. The project further aims to include an obstacle avoidance algorithm to identify objects within a specific range. This high-risk research promises wide-ranging benefits, with the focus to be on creation of an assistive digital vision technology. The main objectives of the proposed effort include: 1) integrating a camera and sonar onto a commercially available embedded device; 2) development of an obstacle detection/avoidance algorithm for the sonar sensor and successfully implementing it into the embedded device; 3) customization of object identification / recognition algorithms for the camera sensor to implement them into the embedded device. Success will be evaluated by measuring performance of the resultant breadboard device in its ability to detect objects at various distances, and to recognize three common objects against a cluttered background. The goal and expected result is to exceed a probability of detection of 80%.<br/><br/>The broader impact/commercial potential of this project is enabling blind people to develop a more comprehensive mental picture of their surrounding and improving their situational awareness. There are 39 million visually impaired people living around the world. Guide dogs and white canes are the preferred mobility aids, but there is little else available to them that is both user-friendly and affordable. The envisaged technology will be compatible with the white cane and will alert the user to the presence of above ground obstacles, such as traffic and sign poles and overhanging objects. In addition, the object identification will further allow the user to develop a more comprehensive mental picture of his surroundings and improve his mobility. While other navigation support products are in the high hundreds to thousands of dollars, the technology proposed herein is expected to be commercialized in a product available for less than $200, thus allowing it to make a broad impact to the wide population of the visually impaired. The resultant technology that contains embedded camera and sonar technologies in a portable device may also contribute to the field of robotics and artificial intelligence, wherein a better understanding of the environment will support new decision-making algorithms."
138,1449029,AAAI-15 Support for Robotic Activities,IIS,National Robotics Initiative,9/1/14,8/28/14,Sandip Sen,CA,Association for the Advancement of Artificial Intelligence,Standard Grant,Jeffrey Trinkle,8/31/15,"$26,000.00 ",,sandip@utulsa.edu,2275 E BAYSHORE RD STE 160,East Palo Alto,CA,943033224,6503283123,CSE,8013,"7556, 8086",$0.00 ,"The AI and Robotics research communities have benefitted from collaborations over the past several decades. However, the recent acceleration in the development of new algorithmic techniques in both fields and new hardware in robotics provide many opportunities for high-impact collaborations. The PIs are requesting funds to take advantage of this opportunity by holding the first of what is expected to be a series of meetings to encourage collaboration between these two communities. The first meeting will be held at the most important AI conference, AAAI-2015, in Austin TX. There will be events that bring some of the most accomplished researchers from Robotics to AAAI and also events that will be exciting school children and undergraduate students."
139,1428999,MRI:  Acquisition of a Dual Acquisition Station Dense Sensory Array EEG/ERP System,BCS,Major Research Instrumentation,8/1/14,8/8/14,Geoffrey Potts,FL,University of South Florida,Standard Grant,John Yellen,7/31/17,"$104,107.00 ","Cynthia Cimino, Emanuel Donchin, Chad Dube",gfpotts@usf.edu,4019 E. Fowler Avenue,Tampa,FL,336172008,8139742897,SBE,1189,1189,$0.00 ,"The executive cognitive functions are humans' most advanced component mental operations. Examples of executive functions include manipulating the contents of working memory, inhibiting automatic but inappropriate responses, and flexibly switching between strategies. These functions allow people to make effective decisions, regulate social behavior, and plan ahead. The executive functions are the last to mature in development and are often the first to deteriorate in normal aging as well as mental disorders and neurological injury and disease. Research has yet to fully describe the neural bases of these higher-order mental operations, and understanding the neural bases of the executive functions might provide insight into why some people are better at planning and decision-making than others and allow for more effective intervention and treatment of executive function impairment. The neural bases of the executive functions likely involve networks of neural structures distributed throughout the brain working together in specific temporal sequences, with individual nodes active for only tens of milliseconds. A large literature exists suggesting the prefrontal cortex as a core structure within this network. The research enabled by the dense-sensor array electroencephalography (EEG) system, with extended frontal coverage, investigates the coordinated neural activity underlying the executive functions.<br/><br/>Event-related potentials (ERPs) are scalp recorded neural electrical responses to stimuli and actions embedded in the ongoing EEG. ERPs are an excellent means of assessing the timecourse of functionally relevant neural activity during cognitive processing, and hence a powerful tool for investigating the neural network dynamics that make executive functioning possible. Dense-sensor array EEG, like the 128 channel system here, improves ERP's spatial resolution, allowing better estimation of the neural sources of the scalp-recorded signal. Dense sensor array ERPs allows researchers to examine the neural network activity related to specific cognitive operations. Researchers will use this system to study learning and memory, potentially improving learning strategies, individual differences in risky decision-making (why some individuals make riskier choices than others), attention selection (how the brain may use economic principles, like expected value, to allocate its limited capacity processing resources), and other executive functions, their variability across individuals, and their disruption in neurological injuries (e.g. mild traumatic brain injury) and disease (e.g. Huntington's disease)."
140,1442997,CIF21 DIBBs: An Infrastructure for Computer Aided Discovery in Geoscience,OAC,"AERONOMY, Data Cyberinfrastructure, EarthCube",11/1/14,7/29/19,Victor Pankratius,MA,Massachusetts Institute of Technology,Standard Grant,Amy Walton,10/31/19,"$1,424,765.00 ","Philip Erickson, Frank Lind",pankrat@mit.edu,77 MASSACHUSETTS AVE,Cambridge,MA,21394301,6172531000,CSE,"1521, 7726, 8074","075Z, 7433, 8048",$0.00 ,"Next-generation Geoscience needs to handle rapidly growing data volumes from ground-based and space-based sensor networks. As real-world phenomena are mapped to data, the scientific discovery process essentially becomes a search process across multidimensional data sets. The extraction of meaningful discoveries from this sea of data therefore requires highly efficient and scalable machine assistance to enhance human contextual understanding. This is necessary both for testing new hypotheses as well as for the detection of novel events and monitoring for natural hazards.<br/><br/>This project develops a computer-aided discovery approach that provides scientists with better support to answer questions such as: What inferences can be drawn from an identified feature?  What does a finding mean and how does it fit into the big theoretical picture? Does it contradict or confirm previously established models and findings? How can  concepts and ideas be tested effectively? To achieve this, scientists can programmatically express hypothesized Geoscience scenarios, constraints, and model variations. This approach helps delegate the automatic exploration of the combinatorial search space of possible explanations in parallel on a variety of data sets. Furthermore, programmable crawlers can scale the search and discovery of interesting phenomena on cloud-based infrastructures. The computer-aided discovery prototype is evaluated in case studies from Geospace science, including the exploration of structures in space and time using combined GPS, optics, and Geospace radar data."
141,1537228,"Memory encoding in spatially structured networks: dynamics, discrete geometry & topology",DMS,MATHEMATICAL BIOLOGY,11/3/14,5/12/16,Carina Curto,PA,Pennsylvania State Univ University Park,Standard Grant,Junping Wang,8/31/16,"$94,071.00 ",,ccurto@psu.edu,110 Technology Center Building,UNIVERSITY PARK,PA,168027000,8148651372,MPS,7334,9150,$0.00 ,"Hippocampal networks are believed to be a major center of associative learning due to the central role of the hippocampus in learning and memory, as well as the relatively high levels of recurrent connectivity and synaptic plasticity.  The lack of topographic structure in hippocampus has made it a natural inspiration for associative memory models, such as the Hopfield model, for encoding memories in unstructured recurrent networks. At the same time, studies in rodents have uncovered the critical role of hippocampus in spatial navigation and, more recently, time-tracking. In contrast to associative memory encoding, these functions have been successfully modeled using spatially structured networks. How can these viewpoints be reconciled? The central goal of this research is to develop a mathematical theory of memory encoding in spatially structured networks, and to study the neural codes that arise from such networks.  Specifically, the research will develop mathematical theory to answer the following questions: (1) How can overlapping memory patterns be encoded precisely as attractors of an unstructured neural network, without introducing unwanted ""spurious states""? (2) How can memories be encoded in a spatially structured network, such as a bump attractor network, while maintaining functions that depend on the network's spatial organization?  (3) Aside from error correction, what are the advantages of redundancy in a neural code, such as the hippocampal place field code, that is characterized by heavily overlapping receptive fields? This last question will also be explored via the analysis of cortical and hippocampal data sets provided by collaborating labs.<br/><br/>The hippocampus is often thought of as a ""Swiss knife"" in the brain.  Decades of experimental work have uncovered its essential role in learning and memory, as well as in spatial navigation.  From a theoretical standpoint, it is puzzling how the same neural network can achieve such disparate functions.  In particular, mathematical models of memory encoding are fundamentally quite different from models of spatial navigation.  This work will integrate these two major types of neural network models, with the goal of understanding how the hippocampus can support multiple important functions.  At its core, the research will advance the mathematical theory behind our understanding of network-level computation in the brain."
142,1451007,"BRAIN EAGER: A Massively Parallel Electrocorticographic Recording, Stimulating and Chemical Detection Device to Understand Neural-Network Functioning in Behaving Animals",DBI,"INSTRUMENTAT & INSTRUMENT DEVP, Activation",9/1/14,8/27/18,Karen Mesce,MN,University of Minnesota-Twin Cities,Standard Grant,Robert Fleischmann,8/31/19,"$300,000.00 ","Christy Haynes, Matthew Chafee",mesce001@umn.edu,200 OAK ST SE,Minneapolis,MN,554552070,6126245599,BIO,"1108, 7713","7916, 8091",$0.00 ,"------------------------------------<br/><br/>Abstract <br/>Proposal #1451007<br/>This award is being made jointly by the Neural Systems Cluster in the Division of Integrative and Organismal Systems and the Instrument Development for Biological Research program (IDBR) in the Division of Biological Infrastructure.<br/><br/>How an animal renders a correct decision to select an appropriate behavior to express over another is not well understood at the level of individual brain neurons. Such decision making, however, is not always easy to study or understand because a number of factors can bias behavioral choice in dynamic ways (for example, fluctuating neurohormones or environmental conditions). Even in simpler invertebrate animals, with a reduced number of brain neurons, the operational state of their neural networks is neither easy to follow nor predictable. Thus to solve some of the most pressing questions in the field of neuroscience, technological advances must be made so that the functioning of brains can be studied under more naturalistic conditions. To this end, a team of scientists in engineering, nanoscience, chemistry, computer science, and biology will work together to design, fabricate and test a novel brain recording and stimulation device that, in parallel, will detect fluctuations in neuroactive substances. The team will begin by making prototypes of the device and testing it on leech and insect brains that have fewer neurons, but have well defined correlations between nerve cell activity and behaviors. The team is committed to the interdisciplinary cross-training of graduate and undergraduate students, especially females and underrepresented minorities. The goal of team mentoring is such that students will be well versed in both the biological and engineering aspects of the device. School visits are also planned to engage K-12 students in neuroscience, chemistry and engineering-related demonstrations, encouraging them to participate in STEM fields. <br/><br/>The cross-disciplinary team will fabricate and test a novel multi-electrode integrated ElectroCorticoGraphy (ECoG) device and chemical sensing system having high temporal resolution. Patterned brain activity will be collected in parallel with neuromodulatory substances such as dopamine (DA), serotonin (5-HT) and octopamine (OA). The team's aim is to fabricate a device that will be 2 x 2 mm square, micron-level thin, flexible and biocompatible for extended use, with a minimum of output wires; our future goal will be to develop a completely remote sensing/monitoring capability. Such post-fabrication modification will be conducted at the University of Minnesota's Nano Center. Furthermore, the team aims to identify conserved neural algorithms or rules for context-dependent decision making that span the invertebrates (leech and honey bee) to non-human primates. Fabricated devices will be placed: 1) around dorsal and ventral aspects of the brain of the leech while it makes a decision to crawl or swim (DA and 5-HT-dependent switching); 2) over the Kenyon cells of the honey bee brain during a modified PER (proboscis-extension) learning-and-memory task (potential DA, 5-HT, and OA involvement); and 3) over the Prefrontal Cortex of monkeys during a spatial-cognitive task that will mimic one used for the honey bee (measuring DA changes)."
143,1534687,CAREER: Simultaneous imaging of photoreceptor and post-photoreceptor responses in the retina,CBET,BioP-Biophotonics,11/1/14,3/4/15,Xincheng Yao,IL,University of Illinois at Chicago,Standard Grant,Leon Esterowitz,7/31/16,"$161,443.00 ",,xcy@uic.edu,809 S. Marshfield Avenue,Chicago,IL,606124305,3129962862,ENG,7236,"005E, 014E, 1045, 1187, 9150",$0.00 ,"1055889<br/>Yao<br/><br/>The research objective of this proposal is to investigate stimulus-evoked intrinsic optical signals (IOSs) associated with photoreceptor and post-photoreceptor neural responses in the complex retinal neural network. This project will start with investigating fast IOSs in frog retinal slices. A retinal slice preparation exposes a cross-section of the retinal layers, and thus allow simultaneous recording of fast IOSs from the photoreceptors and post-photoreceptor neurons, with feasibility of concurrent electrophysiological recording of targeted retinal cells. This phase will test two hypotheses: 1) Fast IOSs correlated with early photoreceptor activities to light stimulation occurs immediately after the stimulus delivery; while fast IOSs associated with second- and third-order post-photoreceptor neurons may have a time delay relative to the stimulus. 2) Fast IOSs can be used to image oscillatory and spike neural activities generated by post-photoreceptor neurons. The second phase of this project is to explore simultaneous optical coherence tomography (OCT) imaging of photoreceptors and post-photoreceptor responses in the intact eye. In order to achieve this objective, an acousto-optic deflector (AOD) based optical coherence tomography (OCT) is proposed for pursuing vibration- and inertia-free optical dissection of retinal neural activities."
144,1412066,Dynamics of Large Networks,DMS,APPLIED MATHEMATICS,8/1/14,7/3/14,Georgi Medvedev,PA,Drexel University,Standard Grant,Victor Roytburd,7/31/17,"$150,277.00 ",,medvedev@drexel.edu,"1505 Race St, 10th Floor",Philadelphia,PA,191021119,2158955849,MPS,1266,"8396, 8609",$0.00 ,"A number of very important problems in science and technology lead to the analysis of large networks of interacting dynamical systems. Our ability to predict epileptic seizures, to effectively control a power grid, or to coordinate a group of robots rely on our understanding of the principles underlying collective behavior in coupled dynamical systems. Many natural and man-made networks around us feature extraordinary richness and complexity of interconnections. Mathematical modeling of such networks poses new challenges for nonlinear science and requires new approaches incorporating combinatorial and probabilistic methods into dynamical analysis of complex systems. Graph Theory holds an extraordinary potential to inspire new powerful techniques for extended dynamical systems and applications to technological, social, economic, and biological networks. In this research, the Principal Investigator (PI) combines state-of-the-art techniques of Graph Theory with analytical methods for Dynamical Systems to develop an effective set of tools for studying coupled dynamical systems and their applications in neuroscience. The results of this research will be integrated into graduate courses in Dynamical Systems and Mathematical Neuroscience. <br/><br/>In this project, the PI develops a unified approach for studying dynamical networks as evolution equations on three types of spatial domains: Caley graphs, quasirandom graphs, and graph limits. For problems in each class, analytical and algebraic techniques, which mesh well with the underlying spatial structures, are identified. These techniques are used to study stability of spatial patterns in systems of coupled phase oscillators on Caley graphs, synchronization in systems of coupled chaotic maps, and differential equations on different random graphs including small-world and graphs that exhibit power law behavior, as well as those that exhibit temporally structured stable patterns in a neural network model of learning. The PI seeks systematic ways for describing the role of network connectivity in shaping the dynamics in coupled systems. This work is aimed toward development of a theory for interacting dynamical systems on regular and random graphs."
145,1427547,NRI: Collaborative Research: Modeling and Verification of Language-based Interaction,IIS,NRI-National Robotics Initiati,8/15/14,3/10/15,Nicholas Roy,MA,Massachusetts Institute of Technology,Standard Grant,David Miller,7/31/19,"$525,000.00 ",,nickroy@mit.edu,77 MASSACHUSETTS AVE,Cambridge,MA,21394301,6172531000,CSE,8013,8086,$0.00 ,"Many autonomous systems today, such as personal or service robots, are designed primarily to perform tasks independently and in isolation. Integrating these robots with human partners can often result in poor performance, as the robot does not know how to interpret human interaction, and cannot merge information from this  interaction with a model that guarantees robot performance.  This research brings together key elements that are just now reaching a sufficient level of maturity for integration: firstly, natural language processing and probabilistic modeling to capture human input, and secondly probabilistic synthesis and verification of the combined human-robot systems to ensure correct performance. The outcome will be theory and software to enable correct, effective and natural interactions between robots and humans to be realized. This research will impact most future autonomous systems which require interactions with humans, including service, personal and planetary robots. <br/><br/>The goal of this research is to develop models and algorithms for synthesizing and verifying an integrated human-plus-robot system based on natural language interaction. Algorithms are being developed for probabilistic modeling and inference of natural language, including the grounding of the constituents of the language into the physical world and the human's expectations. These models will enable the development of a distribution over specifications for control synthesis, which will in turn enable the development and verification of correct-by-construction controllers to a particular level of probability. The out years will consider interactive human-robot dialogue to resolve conflicts, and ""open world"" scenarios to enable on-line learning of new models over time. It is expected that this research will enable high reliability and performance in many autonomous systems because of the inherent interaction with humans.  Outcomes include open source data and software; community workshops; and undergraduate and graduate student education in the unique area of language, modeling and verification for robotics."
146,1427030,NRI: Collaborative Research: Modeling and Verification of Language-based Interaction,IIS,NRI-National Robotics Initiati,8/15/14,8/18/14,Mark Campbell,NY,Cornell University,Standard Grant,David Miller,7/31/19,"$700,000.00 ",Hadas Kress Gazit,mc288@cornell.edu,373 Pine Tree Road,Ithaca,NY,148502820,6072555014,CSE,8013,8086,$0.00 ,"Many autonomous systems today, such as personal or service robots, are designed primarily to perform tasks independently and in isolation. Integrating these robots with human partners can often result in poor performance, as the robot does not know how to interpret human interaction, and cannot merge information from this  interaction with a model that guarantees robot performance.  This research brings together key elements that are just now reaching a sufficient level of maturity for integration: firstly, natural language processing and probabilistic modeling to capture human input, and secondly probabilistic synthesis and verification of the combined human-robot systems to ensure correct performance. The outcome will be theory and software to enable correct, effective and natural interactions between robots and humans to be realized. This research will impact most future autonomous systems which require interactions with humans, including service, personal and planetary robots. <br/><br/>The goal of this research is to develop models and algorithms for synthesizing and verifying an integrated human-plus-robot system based on natural language interaction. Algorithms are being developed for probabilistic modeling and inference of natural language, including the grounding of the constituents of the language into the physical world and the human's expectations. These models will enable the development of a distribution over specifications for control synthesis, which will in turn enable the development and verification of correct-by-construction controllers to a particular level of probability. The out years will consider interactive human-robot dialogue to resolve conflicts, and ""open world"" scenarios to enable on-line learning of new models over time. It is expected that this research will enable high reliability and performance in many autonomous systems because of the inherent interaction with humans.  Outcomes include open source data and software; community workshops; and undergraduate and graduate student education in the unique area of language, modeling and verification for robotics."
147,1433108,Workshop: Support for a workshop on scientific research applications of natural language technologies,IIS,"Linguistics, HCC-Human-Centered Computing, Robust Intelligence",5/1/14,4/29/14,Noah Smith,PA,Carnegie-Mellon University,Standard Grant,William Bainbridge,4/30/15,"$25,000.00 ",,nasmith@cs.cmu.edu,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,CSE,"1311, 7367, 7495","1311, 7367, 7495, 7556",$0.00 ,"This is funding for a one-day workshop on scientific research applications of natural language technologies to be held at the annual meeting of the Association for Computational Linguistics (ACL), in Baltimore on June 26, 2014. A major growth area in applied computer science has been the application of automated techniques to massive datasets to answer scientific questions about people and society.  Although much work in this area focuses on structured data or network data, linguistic data is also a key source of evidence for these phenomena. While some existing natural language processing (NLP) techniques have found use in this growing community, new techniques for discovering and analyzing social meanings and structures in text are in high demand.<br/><br/>Intellectual merit.  Engagement between NLP researchers and domain scientists will introduce new problem formulations and new theoretical frameworks that will broaden and deepen applications of language technology to social science. Potential topics for presentations and discussion include (but are by no means limited to): inferring social relations from conversation and other linguistic behavior; automatic extraction of event data from text; inference of author and speaker properties from text and speech; relating text datasets to author social networks; tracking language change over space, time, and communities; measuring linguistic influence; computational analysis of literary and historical corpora; and tracking the flow of information, ideas, and sentiment through social networks. Much of the data to be studied comes from online communities, which are a focus of the Cyber-Human Systems program.<br/><br/>Broader Impact.  This workshop will increase the visibility of the computational social science application area for ACL researchers and will help build connections between language technologists and social scientists. The workshops format aims at fostering interactions among participants and invited speakers, contributing towards building a community interested in language technologies and domain scientists. This format is especially beneficial to student participants, who will leave with new ideas about guiding applications. As is typical for ACL workshops, an archival proceedings will be published openly through the ACL Anthology. The workshop organizers will report on the workshop, synthesizing the discussion at the workshop with an emphasis on research topics with greatest potential in the near future.  The report will be published openly (e.g., posted to arXiv and possibly submitted for publication in a relevant journal) within a few months after the workshop."
148,1408141,Collaborative Research: An Intelligent Restoration System for a Self-healing Smart Grid (IRS-SG),ECCS,"EPMD-ElectrnPhoton&MagnDevices, EPCN-Energy-Power-Ctrl-Netwrks",8/15/14,7/23/18,Ganesh Venayagamoorthy,SC,Clemson University,Standard Grant,Usha Varshney,8/31/19,"$176,000.00 ",,gkumar@ieee.org,230 Kappa Street,CLEMSON,SC,296345701,8646562424,ENG,"1517, 7607","155E, 9150, 9251",$0.00 ,"How can we restore power more effectively after a major outage, such as a hurricane, a cascade blackout or a more local outage? Can we use modern computer-based methods to get better performance than what we have today, in a system which is mainly informal and based on guesswork under conditions of great stress and limited information? This new collaborative project will bring together an expert on power restoration with a pioneer of new intelligent computation methods for the power grid, in hopes of finding a more powerful and modern way to restore power more effectively. The goals are; (1) to develop an online adaptive restoration tool using advanced scalable computational intelligence techniques; (2) investigate a novel scheme to use renewable resources in system restoration; (3) explore a blackstart unit investment strategy to improve the self-healing capability; and (4) real-time implementation and demonstration of the new system on benchmark and utility power systems. The grant will include travel to New Zealand, to discuss use of the new system to help provide better response to events like earthquakes. It will also include outreach to Native Americans in the Dakotas. <br/><br/>The problem of efficient restoration is very difficult from a technical point of view. A small number of researchers, like the lead PI, have developed a few tools of practical use in this problem, but it is still largely an unsolved problem. The main justification for NSF involvement in this area, and for significant hope of success, is the use of intelligent systems methods far beyond what anyone has applied in the past to this problem, methods pioneered in the intelligent systems part of the EPCN program at NSF<br/>(described for example in the book Handbook of RLADP edited by Frank Lewis and Derong Liu). Data from new sources such as synchrophasors will be part of this work. The underlying algorithms are designed from the start to run on massively parallel<br/>distributed systems, such as Cellular Neural Network hardware, which allow much faster real-time computation than traditional sequential computers and algorithms."
149,1354015,IDBR TYPE A: Development of a Line Confocal Bessel Beam Platform for High-speed High-Volume 3D Imaging In Vivo,DBI,"INSTRUMENTAT & INSTRUMENT DEVP, Engineering of Biomed Systems, Cross-BIO Activities",5/1/14,6/17/15,Katsushi Arisaka,CA,University of California-Los Angeles,Continuing Grant,Christopher Sanford,4/30/16,"$642,364.00 ","Dolores Bozovic, Laurent Bentolila, Thomas Otis, Elissa Hallem",arisaka@physics.ucla.edu,10889 Wilshire Boulevard,LOS ANGELES,CA,900951406,3107940102,BIO,"1108, 5345, 7275","017E, 137E, 138E, 8007",$0.00 ,"This NSF IDBR award is made to Prof. Katsushi Arisaka and collaborators at the University of California, Los Angeles, to develop a Bessel beam line confocal microscope. The goal of this project is to develop a system enabling the recording of three-dimensional cell structure in vivo, in real-time, with exceptional penetrating depth, and with minimal damage to the targeted sample. The proposed system will advance scientific understanding by facilitating cellular observation and systems level biologic analysis. Significantly, this system will enable the unprecedented high-speed recording of cell dynamics at super-resolution, in a temporal range permitting observation of developmental phenomena.<br/><br/>The broader impact is a cost-effective, easily configurable and user-friendly 3D imaging system for use by scientists towards the in vivo structural characterization of dynamic biological samples over a timespan of days. This collaboration will generate an available and reproducible microscope that significantly advances obtainable information concerning embryo development, dynamic neural network function, cellular differentiation and regulation, while enabling high-volume 3D cell imaging. This project directly integrates educational and research goals through incorporation of microscope development into a novel, interdisciplinary laboratory course developed by Dr. Arisaka at UCLA. Thus, development and construction of the system will serve as an educational platform directly fostering student learning. Moreover, the system will be housed in the Advanced Light Microscope Facility at the California NanoSystems Institute (CNSI), resulting in widespread availability to the extended scientific community.<br/><br/>This award is being made jointly by two Programs- (1) Instrument Development for Biological Research, in the Division of Biological Infrastructure (Biological Sciences Directorate), and (2) Biomedical Engineering, in the Division of Chemical, Bioengineering, Environmental and Transport Systems (Engineering Directorate)."
150,1362098,19th Lexical-Functional Grammar Conference,BCS,"LINGUISTICS, DEL",3/1/14,1/24/14,Damir Cavar,MI,Eastern Michigan University,Standard Grant,Joan Maling,2/28/15,"$12,199.00 ",Malgorzata Cavar,dcavar@indiana.edu,Office of Research Development,YPSILANTI,MI,481972212,7344873090,SBE,"1311, 7719","1311, 7719, SMET",$0.00 ,"This award provides support for the 19th Lexical-Functional Grammar (LFG) Conference, to be held July 17-19, 2014 in Ann Arbor, Michigan. A central goal of the LFG framework is to create a model of grammar which is sufficiently sophisticated for theoretical linguists to formulate a model of natural language, but which can also be used in theoretical and applied areas of computational linguistics and natural language processing. Because of its formal rigor, LFG has been used as the theoretical basis of various machine translation tools, for example, AppTek's TranSphere, and the Julietta Research Group's Lekta. The annual conference is the largest annual LFG meeting, offering a venue for the exchange of ideas, and fostering cooperation for theoretical and computational linguists. It has a truly international outreach, with participants coming from Europe, Asia, Australia, and America. This will be the first LFG conference held in the USA since 2007. <br/><br/>The leitmotif of the 2014 conference is ""Language Documentation and Linguistic Theory."" It will provide a platform for the discussion of the role of linguistic theories (LFG and other generative models, e.g. Head-Driven Phrase Structure Grammar (HPSG) and the Minimalist Program) in language documentation and descriptive linguistic research. The possibility of using theoretically motivated computational environments (e.g. Finite State Morphologies or syntactic parsers) for ongoing efforts in the domain of under-resourced and endangered languages research is highly relevant in the context of many projects seeking to document and maintain endangered languages. The conference will be followed by a one-day ParGram workshop, and a one-day workshop on unbounded dependencies in LFG."
151,1447786,BIGDATA: IA: DKA: Collaborative Research: High-Thoughput Connectomics,IIS,Big Data Science &Engineering,9/1/14,8/25/14,Nir Shavit,MA,Massachusetts Institute of Technology,Standard Grant,Wendy Nilsen,8/31/19,"$765,000.00 ",,shanir@csail.mit.edu,77 MASSACHUSETTS AVE,Cambridge,MA,21394301,6172531000,CSE,8083,"7433, 8083",$0.00 ,"High-Throughput Connectomics <br/><br/>Connectomics is the science of mapping the connectivity between neuronal structures to help us understand how brains work. Using the analogy of astronomy, connectomics researchers wish to build 'telescopes' that will allow scientists to accurately view the brain. However, as in astronomy, the raw data collected by microtomes and electron microscopes, the instruments of connectomics, is too large to store effectively, and must be analyzed at very high computation rates. Our goal is to research, develop, and deploy a software architecture that enables high-throughput analysis of connectomics data at the speed at which it is being acquired. We will develop the first computational infrastructure to support high-throughput connectomics without human intervention. If successful, this system will allow for the first time the mapping of a cortical column of a small mammalian brain (1 cubic millimeter), and hopefully within a few years the mapping of significant sections of a mammalian cortex. <br/><br/>The solution to the big data problem of connectomics is a new high-throughput connectomics software architecture that we call MapRecurse. MapRecurse, named so because it bears some resemblance to the widely used MapReduce framework, will provide a unified way of specifying sequences of computational steps and validation tests to be applied to the collected data. Key to MapRecurse will be the ability to layout data and computation in a structured way that preserves locality. Using it, programmers will be able to apply fast, less accurate segmentation algorithms to low resolutions of the data in order to quickly compute a first version of the output neural network graph. Domain-specific graph theoretical methods will then check for correctness of the graph and identify areas of inconsistencies that are in need of further refinement. MapRecurse will then apply bottom-up, local processing with slower, more accurate segmentation and reconstruction algorithms to higher resolutions of the data, verifying and correcting any errors. The iterations progress recursively and in parallel across multiple cores, giving the approach its name. We believe that MapRecurse and the data structures and algorithms developed here will find applications in other high-throughput applications, such as, in astronomy, biology, social media applications, or economics."
152,1447344,BIGDATA: IA: DKA: Collaborative Research: High-Throughput Connectomics,IIS,"Smart and Connected Health, Big Data Science &Engineering",9/1/14,8/25/14,Hanspeter Pfister,MA,Harvard University,Standard Grant,Wendy Nilsen,8/31/18,"$935,000.00 ",Jeff Lichtman,pfister@seas.harvard.edu,1033 MASSACHUSETTS AVE,Cambridge,MA,21385369,6174955501,CSE,"8018, 8083","7433, 8018, 8083",$0.00 ,"High-Throughput Connectomics <br/><br/>Connectomics is the science of mapping the connectivity between neuronal structures to help us understand how brains work. Using the analogy of astronomy, connectomics researchers wish to build 'telescopes' that will allow scientists to accurately view the brain. However, as in astronomy, the raw data collected by microtomes and electron microscopes, the instruments of connectomics, is too large to store effectively, and must be analyzed at very high computation rates. Our goal is to research, develop, and deploy a software architecture that enables high-throughput analysis of connectomics data at the speed at which it is being acquired. We will develop the first computational infrastructure to support high-throughput connectomics without human intervention. If successful, this system will allow for the first time the mapping of a cortical column of a small mammalian brain (1 cubic millimeter), and hopefully within a few years the mapping of significant sections of a mammalian cortex. <br/><br/>The solution to the big data problem of connectomics is a new high-throughput connectomics software architecture that we call MapRecurse. MapRecurse, named so because it bears some resemblance to the widely used MapReduce framework, will provide a unified way of specifying sequences of computational steps and validation tests to be applied to the collected data. Key to MapRecurse will be the ability to layout data and computation in a structured way that preserves locality. Using it, programmers will be able to apply fast, less accurate segmentation algorithms to low resolutions of the data in order to quickly compute a first version of the output neural network graph. Domain-specific graph theoretical methods will then check for correctness of the graph and identify areas of inconsistencies that are in need of further refinement. MapRecurse will then apply bottom-up, local processing with slower, more accurate segmentation and reconstruction algorithms to higher resolutions of the data, verifying and correcting any errors. The iterations progress recursively and in parallel across multiple cores, giving the approach its name. We believe that MapRecurse and the data structures and algorithms developed here will find applications in other high-throughput applications, such as, in astronomy, biology, social media applications, or economics."
153,1422021,RI: Small: Geometry- and Symmetry-Driven Computer Vision Methods for High-Throughput Automated Microscopic Imaging,IIS,"ADVANCES IN BIO INFORMATICS, Information Technology Researc, Cross-BIO Activities, Robust Intelligence",8/1/14,6/3/15,Davi Geiger,NY,New York University,Continuing Grant,Jie Yang,7/31/17,"$427,208.00 ",,geiger@cs.nyu.edu,70 WASHINGTON SQUARE S,NEW YORK,NY,100121019,2129982121,CSE,"1165, 1640, 7275, 7495","7495, 7923, 8750",$0.00 ,"Humans not only outperform current Computer Vision methods in complex general problems such as object detection/recognition, but also in domain-specific tasks such as counting cells and detecting cell divisions in time-lapse videos of mammalian embryos. This project develops a key computer vision component to reach human-like recognition performance in shape recognition for biology applications. The project provides automated methods and software tools for biology research. The project also develops a framework for addressing fundamental issues in geometry to contribute to computer vision research. <br/><br/>This research is rooted on pair wise symmetry and the construction of six dimensional histograms from symmetry measures. It hypothesizes that shape properties of objects can be robustly represented by marginalizing this histogram. The research team develops a method to build shape properties by performing products of marginalized histograms as to create higher level shape descriptions.  The short-term goal of this project is to apply this concept to count and track overlapping cells in early mouse and human embryos. The outcome of the work includes a database of hierarchical trees of cell divisions in a mouse-embryo up to the 8-cell is essential for the analysis of particular genes in the early phases of life.  The mid-term goal is to develop a shape database available to all researchers, where other computer vision methods can be tested. The long-term goal is to crack the challenging problem of understanding shapes in images. This project impacts the development of detection and recognition of objects in images and the mathematical description of shapes. It also impact on the development of a theory of leaning from big data, as it investigates shape-structures of high dimensional data."
154,1420446,Reducing Racial and Gender Achievement Gaps in STEM:  Use of Natural Language Processing to Understand Why Affirmation Interventions Improve Performance,HRD,"REAL, ECR-EHR Core Research",9/1/14,8/18/15,Valerie Purdie-Vaughns,NY,Columbia University,Continuing Grant,Celestine Pea,8/31/17,"$1,035,994.00 ","Smaranda Muresan, Geoffrey Cohen, Jonathan Cook",vpvaughns@psych.columbia.edu,2960 Broadway,NEW YORK,NY,100276902,2128546851,EHR,"7625, 7980",,$0.00 ,"Addressing issues related to reducing the size of the achievement gaps in STEM disciplines among subpopulations of students is important to helping the Nation meet its 21st Century science and technology needs. Research shows that causes of achievement gaps in STEM arise from reciprocal interactions between societal, social, and environmental factors that might suppress students' true academic potential in challenging academic STEM domains. This project focuses on environmental factors (identified as social identity threats) that devalue, marginalize, or discriminate against students based on a social identity like race, gender, disability status, or socioeconomic status; such factors can eventually lead students to withdraw and disengage in STEM learning and careers. The objectives of this research are to: (1) synthesize and systematically analyze data from interventions (affirmation writing essays) shown to help reduce the impact of social identity threats on student participation in STEM; and (2) apply results of the synthesis and analyses to enhance existing interventions (e.g., maximize impact on subpopulations of students whose  achiement in STEM fields is below their potential).<br/><br/>The research project will proceed in two phases.  First, the investigators will create an encrypted online repository of data from  more than 2,500 affirmation writing essays, previously collected through randomized double-blind experiments involving approximately 1,400 students who vary by race, ethnicity, age, gender, and social class. The researchers will link this online repository of information to academic and psychological outcomes for middle school and college students. Using natural language processing (NLP), topic modeling, and other methods the investigators will identify   sematic content and essay structure processes that mediate affirmation effects and highlight meaning of the effectiveness of the essay writing interventions. <br/><br/>Results of these analyses will be used to develop and  test a more robust intervention for reducing social identity threats involving African Americans, White, and female students.  One hundred eighty (180) students (90 females and 90 males) will participate in two separate laboratory studies.  One, conducted at Columbia University, will focus on race as a social factor; the second, conducted at Penn State University, will focus on gender.   The ultimate goal of this work is  to uncover and address psychological factors that might otherwise hinder students' participation in STEM careers."
155,1506786,"HCC: Medium: Collaborative Research: Generating Accurate, Understandable Sign Language Animations Based on Analysis of Human Signing",IIS,HCC-Human-Centered Computing,8/20/14,11/13/14,Matt Huenerfauth,NY,Rochester Institute of Tech,Continuing Grant,Ephraim Glinert,6/30/16,"$59,964.00 ",,matt.huenerfauth@rit.edu,1 LOMB MEMORIAL DR,ROCHESTER,NY,146235603,5854757987,CSE,7367,"7367, 7924",$0.00 ,"American Sign Language (ASL) animations have the potential to make information accessible to many deaf adults in the United States who possess only limited English literacy. In this research, which involves collaboration across three institutions, the PIs' goal is to gain a better understanding of ASL linguistics through computational techniques while advancing the state of the art in the generation of ASL animations for accessibility applications for people who are deaf.  To these ends, the PIs will develop linguistically based models of two aspects of ASL production: movements required for head gestures and facial expressions that carry essential grammatical information and frequently extend over domains larger than a single sign, and the timing and coordination of manual and non-manual elements of ASL signing.  Preliminary work has shown that these issues significantly affect how well signers understand ASL animations, and that these aspects of current ASL animation technologies require improvement.  How should the face of a human or animated character be articulated to perform, with accuracy, the linguistically meaningful facial expressions that are part of ASL grammar?  How should the onsets, offsets, and transitions of these movements be produced?  How should the facial expressions and hand movements be temporally coordinated so that the ASL production is as grammatically correct and understandable as possible?  To answer open questions such as these, the PIs' novel approach will apply techniques from computer vision to linguistically annotated video data collected from human signers, in order to produce models for use in animation-production. The PIs will expand their existing annotated video ASL corpora through new data collection and annotation, and will analyze these data to study the use, timing, and synchronization of manual and non-manual components of ASL production.  The annotated videos will be used to train high quality computer vision models for recognition of linguistically significant facial expressions and timing subtleties.  Parameters of these computer vision models will be used to hypothesize computational models of ASL timing and facial movements, to be incorporated into ASL-animation generation software and evaluated by native signers.   The models will be iteratively refined in cycles of user-based studies and incorporated into ASL animation technologies to more accurately mimic human signing.  Project outcomes will include high quality models of the movement of virtual human characters for animations of ASL performance.  The analysis of video corpora of ASL will produce new linguistic insights into the micro-facial expressions and the temporal coordination of the face and hands in ASL production, while advances in the analysis of ASL prosody will contribute to an understanding of the fundamental commonalities and modality-specific differences between signed and spoken languages that is essential to a full understanding of the human language faculty. The creation of new modeling approaches and recognition techniques will advance the field of computer vision, by benefiting the identification and tracking of the human face and body in video during the rapid and complex movements of ASL (and other forms of human movement).<br/><br/>Broader Impacts:  This research will lead to significant improvements to technology for generating linguistically accurate ASL animations, which will make information, applications, websites, and services more accessible to the large number of deaf individuals with relatively low English literacy.  Advances in computer vision techniques for recognizing ASL in videos of humans will have general applicability in human-computer interaction, recognition and animation of facial expressions, and computer vision.  The corpora created in this project will enable students and researchers in both linguistics and computer science (including those without access to the requisite technological and human resources to carry out their own data collection from native signers and time-intensive linguistic annotations) to engage in research on ASL.  The techniques to be developed will also enable partial automation of the time-consuming creation of annotated ASL video corpora.  As in the PIs' earlier work, the proposed research will create opportunities for people who are deaf and members of other underrepresented groups to participate in scientific research."
156,1430803,Doctoral Dissertation Research: Focus Association in Superlative Expressions,BCS,LINGUISTICS,7/1/14,6/17/14,Roumyana Pancheva,CA,University of Southern California,Standard Grant,Joan Maling,6/30/15,"$5,442.00 ",Barbara Tomaszewicz,pancheva@usc.edu,University Park,Los Angeles,CA,900890001,2137407762,SBE,1311,"1311, SMET",$0.00 ,"Behind the seemingly effortless use of language for conveying information lie extremely complex processes of encoding and extracting meaning. Sentences mark information as old, shared by speaker and listener, or as new, and thus in focus, through word order and prosody. Some of the real-time processes underlying the comprehension of words and sentence structure are fairly well understood by now, but processes involving the interaction between the structure, prosody and meaning of a sentence are just beginning to be studied. The researchers will obtain experimental evidence concerning the role of focus, marked through prosody and word order, in the interpretation of superlative expressions (e.g. 'the most expensive cake'). In English, prosodic focus expressed through the placement of accent (indicated by capitals) results in the different interpretation of sentences with identical word order: compare 'John gave MARY the most expensive cake' and 'JOHN gave Mary the most expensive cake'. In other languages, changes in word order have similar effect on the meaning of sentences with superlative expressions. The project aims to obtain evidence that focus, expressed through prosody or word order, guides processing during real-time comprehension of text. Understanding the role of focus in superlative expressions is important for linguistic theories of sentence structure, meaning, prosody and the interactions among them. Understanding the role of prosodic focus may have applications in natural language processing and speech technology. The findings will also have implications for other disciplines that study cognitive function such as psychology and neuroscience.  <br/><br/>The researchers have previously identified cross-linguistic differences in the range of interpretations available to sentences with superlative expressions. They have explained these differences through the generalization that certain grammatical environments preclude focus-affected interpretation. The novel proposal that in a certain configuration the meaning contributed by the superlative is determined by focus can be tested experimentally. The researchers will investigate whether the hypothesized ""focus association"" effect, which results in a particular reading of a superlative sentence, is computed during real-time sentence processing in Polish. Polish, unlike English, shows both a wider range of interpretations in sentences with superlatives, and a syntactic way to manipulate the available interpretations, allowing manipulation of word order and silent prosody. (It has been shown that implicit accent placement is obligatory in silent reading.) The researchers will test whether the hypothesized focus association effect obtains in a parallel way to well-known cases of focus association involving the adverb 'only' (e.g. 'John only gave Mary a CAKE', 'John only gave MARY a cake'), in a self-paced reading experiment (Experiment 1) and in an ERP experiment (Experiment 2). The self-paced reading and ERP methodologies complement each other, and address the issue of the time-course of the cognitive operations that underlie native speakers' comprehension of sentences with superlatives."
157,1439237,CompCog: Human Scene Processing Characterized by Computationally-derived Scene Primitives,BCS,"Cognitive Neuroscience, Perception, Action & Cognition",9/1/14,8/15/16,Michael Tarr,PA,Carnegie-Mellon University,Standard Grant,Betty Tuller,2/28/19,"$463,158.00 ",Elissa Aminoff,michaeltarr@cmu.edu,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,SBE,"1699, 7252","1699, 7252, 7956, 8605",$0.00 ,"How do our brains take the light entering our eyes and turn it into our experience of the world around us? Critically, this experience seems to involve a visual ""vocabulary"" that allows us to understand new scenes based on our prior knowledge. The investigators explore the nature of this visual language, exploring the specific computations that are realized in the brain mechanisms used for scene perception. The work combines data from state-of-the-art computer vision systems with human neuroimaging to both predict brain responses when viewing complex, real-world scenes, and to analyze and understand the hidden structure embedded in real-world images. This effort is essential for building a theory of how we are able to see and for improving machine vision systems. More broadly, biologically-inspired models of vision are essential for the effective deployment of intelligent technology in navigation systems, assistive devices, security verification, and visual information retrieval.<br/><br/>The artificial vision system adopted in this research is highly data-driven in that it is learning about the visual world by continuously ""looking at"" real-world images on the World Wide Web. The model, known as ""NEIL"" (Never Ending Image Learner, http://www.neil-kb.com/), leverages cutting-edge big-data methods to extract a vocabulary of scene parts and relationships from hundreds of thousands of images. The relevance of this vocabulary to human vision will then be tested using both functional magnetic resonance imaging (fMRI) and magnetoencephalography (MEG) neuroimaging. The hypothesis is that the application of prior knowledge about scenes expresses itself through learned associations between the specific parts and relations forming the vocabulary for scene perception. Moreover, different kinds of associations may be instantiated within distinct components of the functional brain network responsible for scene perception. Overall, this research will build on a recent, highly-successful artificial vision system in order to provide a more well-specified theory of the parts and relations underlying human scene perception. At the same time, the research will provide information about the human functional relevance of computationally-derived scene parts and relations, thereby helping to refine and improve artificial vision systems."
158,1536043,CI-P: Toward Unified Tool Support for Linguistic Corpus Annotation,CNS,CCRI-CISE Cmnty Rsrch Infrstrc,9/1/14,4/21/15,Mark Finlayson,FL,Florida International University,Standard Grant,Tatiana Korelsky,8/31/15,"$31,286.00 ",,markaf@fiu.edu,11200 SW 8TH ST,Miami,FL,331990001,3053482494,CSE,7359,7359,$0.00 ,"Development of the computer processing of language is a key scientific and technological capability that is funded by the NSF. In support of these efforts, thousands of texts, comprising hundreds of thousands of words, are hand-processed every year to provide data to train computer algorithms.  This annotation is time-consuming, expensive, and difficult, and it is hampered by a long-standing problem: the lack of unified, specialized software tools to assist in annotation and annotation management. Researchers at MIT envision creating a new software infrastructure, called a Unified Annotation Workbench (UAW), that is an off-the-shelf solution to this problem. A UAW will significantly the effectiveness of every dollar spent on annotation.  Importantly, a UAW will be useful not only to linguistic annotation community: it will also benefit many scientific and engineering fields that depend on people to annotation-like work.  As a small selection, this includes human-computer interaction, cognitive science, cognitive psychology, sociology, psychiatry, and any field related to the digital humanities.<br/><br/>Computational linguistics and statistical natural language processing (NLP) are important areas of study, both scientifically and technologically.  Advances in these fields are fed by a universal hunger for the analysis of language data for information processing tasks.  Large annotated corpora are a key resource that enables these advances. But despite the widely-recognized importance of annotated corpora, the field has a major lack: there is no off-the-shelf, general, unified tool for performing text annotation.  Faced with this lack, many language researchers create their own tools from scratch, at significant cost. These tools are usually hastily designed, not released for general use, not maintained, and often redundant with capabilities implemented by others. This leads to lost opportunities, as researchers forego projects that present too many difficulties in tool design; it reduces the ability of researchers to build upon and replicate other?s work, as a critical part of the infrastructure is not available; and this duplication of effort represents a significant waste of resources. In this infrastructure planning project, the MIT team will take three steps toward a Unified Annotation Workbench (UAW): a general, unified, off-the-shelf infrastructure to support corpus annotation.  First, they will comprehensively review the state-of-the-art of annotation tools. Second, they will identify potential implementation technologies for a UAW and create software mockups.  Third, they will organize a workshop to engage the annotation community as to the best form of a UAW."
159,1442027,EAGER: Enabling Discovery and Scientific Collaboration on Human Memory via the Web-Based Atlas and Tissue Bank for Patient H.M.'s Brain,BCS,COGNEURO,9/1/14,8/22/16,Jacopo Annese,CA,University of California-San Diego,Standard Grant,alumit ishai,11/30/16,"$300,000.00 ",,drjannese@gmail.com,Office of Contract & Grant Admin,La Jolla,CA,920930621,8585344896,SBE,1699,"1699, 7916, 8089, 8091",$0.00 ,"Knowledge of a specific neural network supporting memory function in the human brain stems from the case of patient H.M. who, in 1953 underwent an experimental medial temporal lobectomy in the hope of reducing the frequency and severity of his epileptic seizures. The operation was successful in that respect, but it unexpectedly left him incapable of creating new memories. For more than five decades, H.M. participated in hundreds of experiments and his case was discussed in thousands of scientific publications. His brain contained the clues to understand how memory works; however, determining with precision which structures were damaged was not possible because even the latest neuroimaging could not clearly resolve the anatomy of the temporal lobes. With support from the National Science Foundation, Dr. Jacopo Annese will complete an 'open source', web-based microscopic atlas of H.M.'s brain which was donated to science post-mortem. The tools-embedded atlas will support the creation of teaching curricula that will expose students to raw neuroimaging data from multiple modalities, cutting edge brain mapping algorithms, web-based exploration tools, all within the clinical and biographical context of H.M. as an individual. The cyber infrastructure created through this project is expected to enable discovery neuroscience by participants world wide.  <br/><br/>Specifically, Dr. Annese and his research team will (1) provide a dedicated support infrastructure to maintain and manage the web atlas for H.M.'s brain; (2) significantly increase the accuracy of the atlas by increasing the number of digitized histological slices to achieve 1 mm per slice interval (from 3mm interval); (3) acquire and deliver image stacks to enable remote quantitative studies; (4) implement new web tools to enable the handling of remote request and curation of results from different laboratories; (5) convert the images into formats that can be 3-D printed using consumer products.   Such cyber infrastructure will make the valuable H.M. data available for new retrospective studies that may further change our current view of how memory is established in the human brain and enable quantitative analyses at the cellular level using a 'virtual microscope'.  The resulting atlas will be used by researchers worldwide to re-interpret, based on clear anatomical evidence, the results from hundreds of neuropsychological exams conducted when H.M. was alive."
160,1405863,CI-P: Toward Unified Tool Support for Linguistic Corpus Annotation,CNS,CCRI-CISE Cmnty Rsrch Infrstrc,5/1/14,8/14/14,Mark Finlayson,MA,Massachusetts Institute of Technology,Standard Grant,Tatiana Korelsky,5/31/15,"$100,000.00 ",,markaf@fiu.edu,77 MASSACHUSETTS AVE,Cambridge,MA,21394301,6172531000,CSE,7359,7359,$0.00 ,"Development of the computer processing of language is a key scientific and technological capability that is funded by the NSF. In support of these efforts, thousands of texts, comprising hundreds of thousands of words, are hand-processed every year to provide data to train computer algorithms.  This annotation is time-consuming, expensive, and difficult, and it is hampered by a long-standing problem: the lack of unified, specialized software tools to assist in annotation and annotation management. Researchers at MIT envision creating a new software infrastructure, called a Unified Annotation Workbench (UAW), that is an off-the-shelf solution to this problem. A UAW will significantly the effectiveness of every dollar spent on annotation.  Importantly, a UAW will be useful not only to linguistic annotation community: it will also benefit many scientific and engineering fields that depend on people to annotation-like work.  As a small selection, this includes human-computer interaction, cognitive science, cognitive psychology, sociology, psychiatry, and any field related to the digital humanities.<br/><br/>Computational linguistics and statistical natural language processing (NLP) are important areas of study, both scientifically and technologically.  Advances in these fields are fed by a universal hunger for the analysis of language data for information processing tasks.  Large annotated corpora are a key resource that enables these advances. But despite the widely-recognized importance of annotated corpora, the field has a major lack: there is no off-the-shelf, general, unified tool for performing text annotation.  Faced with this lack, many language researchers create their own tools from scratch, at significant cost. These tools are usually hastily designed, not released for general use, not maintained, and often redundant with capabilities implemented by others. This leads to lost opportunities, as researchers forego projects that present too many difficulties in tool design; it reduces the ability of researchers to build upon and replicate other?s work, as a critical part of the infrastructure is not available; and this duplication of effort represents a significant waste of resources. In this infrastructure planning project, the MIT team will take three steps toward a Unified Annotation Workbench (UAW): a general, unified, off-the-shelf infrastructure to support corpus annotation.  First, they will comprehensively review the state-of-the-art of annotation tools. Second, they will identify potential implementation technologies for a UAW and create software mockups.  Third, they will organize a workshop to engage the annotation community as to the best form of a UAW."
161,1462143,HCC: Small: Collaborative Research:  Analysis of Language Samples for Detecting Language Impairment in Monolingual and Bilingual Children,IIS,"HCC-Human-Centered Computing, EPSCoR Co-Funding",8/31/14,9/12/14,Thamar Solorio,TX,University of Houston,Standard Grant,Ephraim Glinert,9/30/16,"$130,500.00 ",,thamar.solorio@gmail.com,4800 Calhoun Boulevard,Houston,TX,772042015,7137435773,CSE,"7367, 9150","7367, 7923, 9102, 9150",$0.00 ,"It is widely recognized that language impairment can have a negative effect on literacy skills, and that children suffering language impairment are at a higher risk of academic under-achievement and lower overall social development.  Hence, early and accurate language assessment for children is critical, especially for those with non-mainstream linguistic backgrounds.  Spontaneous language samples are commonly used in communication disorders to measure the speaker's competence across a range of complementary language skills.  These elicitation tasks allow clinicians and clinical researchers to analyze speech fluency by looking at the patterns of disfluencies and other speech disruptions.  Language productivity can be gauged by computing mean length of utterance, along with measures of vocabulary and total utterances produced.  Morpho-syntactic skills can also be analyzed from these data, by manually coding for specific grammatical constructions that are known to signal developmental milestones.  At present, use of the information contained in these language samples is restricted to the capacity of human experts to manually analyze the data, since little has been done to use computational models for this task   In this collaborative effort by PIs in the University of Alabama at Birmingham and the University of Texas at Dallas, the objective is to address this problem by developing computational approaches for scoring samples from children along different language dimensions, including speech fluency, syntactic structure, content, and coherence, with the long term goal of building robust computational linguistic approaches for identifying language impairments in children.   With these ends in mind, the PIs will investigate a number of core research questions, including measuring syntactic complexity in children's language, evaluating content in story retelling and play sessions, and detecting disfluencies in children's transcripts.  Moreover, this research will focus on analyzing samples from children with three different language backgrounds: English monolinguals, Spanish monolinguals, and Spanish-English bilinguals of Mexican descent (the latter representing the fastest growing minority in this country).  Since their models will be data driven, the PIs expect to be able to evaluate empirically the differences in developmental patterns of speech in children across these linguistic diversities.   Addressing the bilingual population involves modeling code-switching behavior; thus, additional core research questions include measuring syntactic complexity in code-switched data, and identification and categorization of code-switching patterns in bilingual children.  <br/><br/>Broader Impacts:  This research will contribute to developing more accurate and practical tools for assessing language development in children, a field to which little attention has been paid to date.  Addressing the challenges involved in the automated analysis of children's speech will also advance the field of Natural Language Processing (NLP) in general.  Moreover, since the project involves children with three different linguistic backgrounds, the new technology will have low language dependency and so should be easily portable to other languages and domains.  In the field of communication disorders, applying corpus-based approaches to language assessment is still in its infancy; project outcomes will have a direct impact on this field, by providing new metrics for scoring spontaneous language samples of children that can complement the battery of assessment tools currently used."
162,1438913,Group Travel Grant for the Doctoral Consortium of the IEEE Conference on Computer Vision and Pattern Recognition 2014,IIS,"INFORMATION TECHNOLOGY RESEARC, ROBUST INTELLIGENCE",6/1/14,5/1/14,Philippos Mordohai,NJ,Stevens Institute of Technology,Standard Grant,Jie Yang,5/31/15,"$15,050.00 ",,Philippos.Mordohai@stevens.edu,CASTLE POINT ON HUDSON,HOBOKEN,NJ,70305991,2012168762,CSE,"1640, 7495","7495, 7556",$0.00 ,"This grant partially supports 20 students at institutions across the U.S. to participate in the Doctoral Consortium at the IEEE Conference on Computer Vision and Pattern 2014 Recognition (CVPR). CVPR is the premier annual conference with about 2000 senior and student participants in computer vision, held in North America and attended by members of the international research community. The Doctoral Consortium is to highlight the work of senior Ph.D. students, who are close to finishing their degrees, or recent graduates, and to give these students the opportunity to discuss their research with senior researchers matched with their expertise.<br/><br/>This project enables top-quality Ph.D. students to present, discuss, and receive feedback on original research within the broader community are all critical components of graduate student development. The opportunity to receive advice on their research and career plans from experts from different institutions, and with potentially different perspectives, in many cases is not available internally within one's own institution. This project supports the career development of some of the brightest junior researchers in computer vision, contributes to the computer research community and related fields in general by drawing attention to an important aspect of graduate student development, and potentially increasing the number of active researchers and teachers in STEM."
163,1463102,CI-New: Collaborative Research: Federated Data Set Infrastructure for Recognition Problems in Computer Vision,CNS,CCRI-CISE Cmnty Rsrch Infrstrc,10/1/14,9/17/14,Jason Corso,MI,Regents of the University of Michigan - Ann Arbor,Standard Grant,Jie Yang,9/30/17,"$150,000.00 ",,jjcorso@eecs.umich.edu,3003 South State St. Room 1062,Ann Arbor,MI,481091274,7347636438,CSE,7359,7359,$0.00 ,"Broad access to image and video datasets has been responsible for much of the progress in computer vision recognition problems over the last decade.  These common benchmarks have played a leading role in transforming recognition research from a black art into an experimental science.  Progress, however, has stagnated; although datasets continue to grow, they are developed and annotated in isolation: e.g., a collection of sporting activities, a set of objects in images, etc. These isolated datasets suffer from task and domain-specific bias, and knowledge transfer across them is extremely limited.  This project is investigating and establishing a prototype architecture that federates across various recognition problems and modalities, by establishing a common namespace for entities, events and annotations across the datasets.  The project is also establishing a web-portal for the prototype federated dataset architecture and linking two existing recognition datasets into the prototype architecture. The resulting federated structure is truly greater than the sum of its parts, and can support new research that was not previously possible for the computer vision community and other related fields.<br/><br/>As a first test scenario for this federated architecture, this project is investigating and constructing a new federated dataset of images and video annotated with various forms of associated text.  Image and video content annotations span both the spatial and temporal dimensions while textual annotations reflecting depicted content range from complete free-form natural language descriptions, to more targeted phrases and referring expressions, to individual keyword lists.  This dataset is being constructed to promote and enhance collaboration efforts between the vision and language communities by providing a new multi-modal annotated dataset with associated research competitions."
164,1405612,CI-New: Collaborative Research: Federated Data Set Infrastructure for Recognition Problems in Computer Vision,CNS,CCRI-CISE Cmnty Rsrch Infrstrc,10/1/14,8/26/14,Jason Corso,NY,SUNY at Buffalo,Standard Grant,Jie Yang,10/31/14,"$150,000.00 ",,jjcorso@eecs.umich.edu,520 Lee Entrance,Buffalo,NY,142282567,7166452634,CSE,7359,7359,$0.00 ,"Broad access to image and video datasets has been responsible for much of the progress in computer vision recognition problems over the last decade.  These common benchmarks have played a leading role in transforming recognition research from a black art into an experimental science.  Progress, however, has stagnated; although datasets continue to grow, they are developed and annotated in isolation: e.g., a collection of sporting activities, a set of objects in images, etc. These isolated datasets suffer from task and domain-specific bias, and knowledge transfer across them is extremely limited.  This project is investigating and establishing a prototype architecture that federates across various recognition problems and modalities, by establishing a common namespace for entities, events and annotations across the datasets.  The project is also establishing a web-portal for the prototype federated dataset architecture and linking two existing recognition datasets into the prototype architecture. The resulting federated structure is truly greater than the sum of its parts, and can support new research that was not previously possible for the computer vision community and other related fields.<br/><br/>As a first test scenario for this federated architecture, this project is investigating and constructing a new federated dataset of images and video annotated with various forms of associated text.  Image and video content annotations span both the spatial and temporal dimensions while textual annotations reflecting depicted content range from complete free-form natural language descriptions, to more targeted phrases and referring expressions, to individual keyword lists.  This dataset is being constructed to promote and enhance collaboration efforts between the vision and language communities by providing a new multi-modal annotated dataset with associated research competitions."
165,1405883,CI-New: Collaborative Research: Federated Data Set Infrastructure for Recognition Problems in Computer Vision,CNS,CCRI-CISE Cmnty Rsrch Infrstrc,10/1/14,8/26/14,Julia Hockenmaier,IL,University of Illinois at Urbana-Champaign,Standard Grant,Jie Yang,9/30/19,"$300,000.00 ",,juliahmr@illinois.edu,1901 South First Street,Champaign,IL,618207406,2173332187,CSE,7359,7359,$0.00 ,"Broad access to image and video datasets has been responsible for much of the progress in computer vision recognition problems over the last decade.  These common benchmarks have played a leading role in transforming recognition research from a black art into an experimental science.  Progress, however, has stagnated; although datasets continue to grow, they are developed and annotated in isolation: e.g., a collection of sporting activities, a set of objects in images, etc. These isolated datasets suffer from task and domain-specific bias, and knowledge transfer across them is extremely limited.  This project is investigating and establishing a prototype architecture that federates across various recognition problems and modalities, by establishing a common namespace for entities, events and annotations across the datasets.  The project is also establishing a web-portal for the prototype federated dataset architecture and linking two existing recognition datasets into the prototype architecture. The resulting federated structure is truly greater than the sum of its parts, and can support new research that was not previously possible for the computer vision community and other related fields.<br/><br/>As a first test scenario for this federated architecture, this project is investigating and constructing a new federated dataset of images and video annotated with various forms of associated text.  Image and video content annotations span both the spatial and temporal dimensions while textual annotations reflecting depicted content range from complete free-form natural language descriptions, to more targeted phrases and referring expressions, to individual keyword lists.  This dataset is being constructed to promote and enhance collaboration efforts between the vision and language communities by providing a new multi-modal annotated dataset with associated research competitions."
166,1447639,BIGDATA: F: DKA: Collaborative Research: Clustering Algorithms for Data Streams,IIS,Big Data Science &Engineering,9/1/14,8/26/14,Vladimir Braverman,MD,Johns Hopkins University,Standard Grant,Aidong Zhang,8/31/18,"$1,000,000.00 ","Alexander Szalay, Randal Burns, Tamas Budavari, Benjamin Van Durme",vova@cs.jhu.edu,1101 E 33rd St,Baltimore,MD,212182686,4439971898,CSE,8083,"7433, 8083",$0.00 ,"This project will develop novel theoretical methods and algorithms for clustering massive datasets with applications to astronomy, neuroscience and natural language processing. Clustering is the process of creating groups of data based on similarities between individual data points. The developed theoretical methods will be used in applications where clustering algorithms are critical and the input data is extremely large. First, new clustering algorithms will be designed to scale and will allow for better cosmological simulations. The simulations involve billions of particles in each snapshot, and existing clustering algorithms based upon a simple friends-of-friends approach do not scale to these cardinalities. Second, this project will advance the computational capabilities in statistical neuroscience by employing clustering algorithms to discover both regular patterns and anomalies in normal and abnormal brain graphs. Finally, this research will explore the important topic of finding anomalies in massive text streams, such as Twitter. In this setting, one is concerned with detecting anomalous bursts in traffic content that share a similar pattern. These bursts might signal an important political event or a natural disaster. This project will support undergraduate and graduate research aimed at developing skills needed for algorithmic work on massive data sets.<br/><br/>There exist numerous heuristics and approximation algorithms for many variants of the clustering problem. However, these methods are often slow or infeasible for applications with massive datasets. This research will improve space and time upper bounds for clustering algorithms in the streaming model. This project will address the k-mean and k-median problems in the dynamic streaming model, extend the results on separable data when the input comes from Euclidian space, improve the bounds in the sliding window model, combine the coresets technique with novel sampling approaches and the method of smooth histograms. The PIs' previous work has already been applied to natural language processing and this project will expand this direction further and explore the important topic of ""First Story Detection."" Furthermore, this research will explore the similarities and differences between various sampling and sketching techniques, and how they could be used in large multidimensional astronomical databases, like SDSS (Sloan Digital Sky Survey) SkyServer. These novel approaches will provide major speedups for the execution of large statistical aggregate queries. The new streaming algorithms will be used to find substructure in very large cosmological N-body simulations. <br/><br/>For further information see the project web site at: http://www.cs.jhu.edu/~vova"
167,1405822,CI-New: Collaborative Research: Federated Data Set Infrastructure for Recognition Problems in Computer Vision,CNS,CCRI-CISE Cmnty Rsrch Infrstrc,10/1/14,8/26/14,Tamara Berg,NC,University of North Carolina at Chapel Hill,Standard Grant,Jie Yang,9/30/19,"$299,751.00 ",,tlberg@cs.unc.edu,104 AIRPORT DR STE 2200,CHAPEL HILL,NC,275991350,9199663411,CSE,7359,7359,$0.00 ,"Broad access to image and video datasets has been responsible for much of the progress in computer vision recognition problems over the last decade.  These common benchmarks have played a leading role in transforming recognition research from a black art into an experimental science.  Progress, however, has stagnated; although datasets continue to grow, they are developed and annotated in isolation: e.g., a collection of sporting activities, a set of objects in images, etc. These isolated datasets suffer from task and domain-specific bias, and knowledge transfer across them is extremely limited.  This project is investigating and establishing a prototype architecture that federates across various recognition problems and modalities, by establishing a common namespace for entities, events and annotations across the datasets.  The project is also establishing a web-portal for the prototype federated dataset architecture and linking two existing recognition datasets into the prototype architecture. The resulting federated structure is truly greater than the sum of its parts, and can support new research that was not previously possible for the computer vision community and other related fields.<br/><br/>As a first test scenario for this federated architecture, this project is investigating and constructing a new federated dataset of images and video annotated with various forms of associated text.  Image and video content annotations span both the spatial and temporal dimensions while textual annotations reflecting depicted content range from complete free-form natural language descriptions, to more targeted phrases and referring expressions, to individual keyword lists.  This dataset is being constructed to promote and enhance collaboration efforts between the vision and language communities by providing a new multi-modal annotated dataset with associated research competitions."
168,1421655,III: Small: Collaborative Research: Automatically Generating Contextually-Relevant Visualizations,IIS,Info Integration & Informatics,9/1/14,8/13/14,Brent Hecht,MN,University of Minnesota-Twin Cities,Standard Grant,Maria Zemankova,2/28/17,"$120,832.00 ",,bhecht@northwestern.edu,200 OAK ST SE,Minneapolis,MN,554552070,6126245599,CSE,7364,"7364, 7453, 7923",$0.00 ,"Information visualizations, such as charts and maps, can greatly enhance news articles by adding context, helping the reader understand complex facts, aiding in decision making, and making information more memorable. Unfortunately, creating good news visualizations is a difficult and labor-intensive task that involves numerous complex decisions. A designer must identify data relevant to the article, clean the data, generate the visualization (a complex process on its own), and provide annotations to connect the article and visualization. While some design guidelines have been developed, many decisions are based on designer intuition, a process that is not scalable to the thousands of news articles that are published every day. This project seeks to build intelligent tools to help designers more quickly create good news visualizations and to develop systems that generate news visualizations autonomously. This research project will enhance citizen understanding of complex information in the news and improve numerical, graphical, and geography literacy. Additionally, the research will provide support for new job categories (e.g., data scientists, computational journalists, data analysts, etc.) and existing companies (e.g., online media, search engines, etc.) in their evolution to new interactive platforms. The research results will be integrated into a broad set of widely accessible educational materials for a variety of courses (visualization, spatial computing, and text analysis) and will serve as research and practical training for undergraduates, graduates, and professionals. <br/><br/>Providing a scalable solution to automatically generating contextually-relevant visualizations requires the understanding and encoding of the design process. Specifically, the goals of this project are (a) identifying the decision process of visualization designers, (b) creating automated components that operationalize these decisions including text processing, searching through a wide range of heterogeneous data sources and datasets (e.g. census data, stock market data, government macroeconomic data), and automatic visualization construction and annotation, and (c) ranking of the visualizations based on well-known quantitative metrics from information retrieval and information visualization such as relevance, expressiveness, and effectiveness. By extracting key comparisons from an article's text through the use of natural language processing and using existing visualization-article pairs as an evaluation corpus, the system will ensure that relevant datasets are found and that the selected visual forms preserve and enhance the information conveyed in the article. For example, the system will automatically create thematic maps for geospatial comparisons of population change in the U.S. and time series for longitudinal comparisons of company financial results. Although the focus of this work in on the news domain, the research can be extended to other application areas including textbooks, internal company reports, and more generally, to any texts that implicitly or explicitly correspond to quantitative data. Further information, including source code, demos, papers, and datasets, are available at the project homepage (http://txt2vis.cond.org)."
169,1421734,RUI: Intermediate-level Vision:  Grouping of generic features for image and video processing,CCF,Comm & Information Foundations,8/1/14,8/7/14,Toshiro Kubota,PA,Susquehanna University,Standard Grant,Phillip Regalia,7/31/18,"$187,449.00 ",,kubota@susqu.edu,514 University Ave,Selinsgrove,PA,178701164,5703724571,CSE,7797,"7923, 7936, 9229",$0.00 ,"The human vision system is able to recognize objects and understand the scene from the boundary of the objects alone. It is astonishingly robust so that it can sustain this capability under distraction by non-boundary points and sparse sampling of the boundary points. How the system achieves this feat is largely unknown. This project investigates how a computer can replicate it algorithmically. The problem is fundamental and closely related to perceptual organization and intermediate level vision problems. Thus, this research has the potential to impact a wide range of computer vision applications. Since the input (a small set of isolated points) is small compared to the whole image and has no color information, the algorithm is efficient, robust against changes in illumination and contrast, and applicable to any imaging modalities.<br/><br/>The main problem is to interpolate boundary points into a perceptually salient set of surfaces without being distracted by spurious non-boundary points. Interpolation by straight skeletons brings a time-reversible, multi-scale representation of a point set where salient boundary points tend to form a polygonal surface persistently while spurious non-boundary points tend to disappear quickly as the scale increases. Because of the time-reversible nature, a surface at each scale can be traced back to the original scale or the original point set. Thus, this technique can be used to form a set of salient surfaces from the original point set. This research develops a general purpose feature grouping algorithm using the straight skeleton interpolation and applies it to a number of intermediate vision problems in 2D and 3D domains. The investigator actively involves undergraduate students into the research and promotes STEM education in the northeastern part of the U.S.A through presentations, demonstrations, and outreach activities. The source code and toolboxes will be made publicly available and used to promote STEM education."
170,1421438,III: Small: Collaborative Research: Automatically Generating Contextually-Relevant Visualizations,IIS,Info Integration & Informatics,9/1/14,8/13/14,Eytan Adar,MI,Regents of the University of Michigan - Ann Arbor,Standard Grant,Maria Zemankova,8/31/17,"$379,126.00 ",,eadar@umich.edu,3003 South State St. Room 1062,Ann Arbor,MI,481091274,7347636438,CSE,7364,"7364, 7453, 7923",$0.00 ,"Information visualizations, such as charts and maps, can greatly enhance news articles by adding context, helping the reader understand complex facts, aiding in decision making, and making information more memorable. Unfortunately, creating good news visualizations is a difficult and labor-intensive task that involves numerous complex decisions. A designer must identify data relevant to the article, clean the data, generate the visualization (a complex process on its own), and provide annotations to connect the article and visualization. While some design guidelines have been developed, many decisions are based on designer intuition, a process that is not scalable to the thousands of news articles that are published every day. This project seeks to build intelligent tools to help designers more quickly create good news visualizations and to develop systems that generate news visualizations autonomously. This research project will enhance citizen understanding of complex information in the news and improve numerical, graphical, and geography literacy. Additionally, the research will provide support for new job categories (e.g., data scientists, computational journalists, data analysts, etc.) and existing companies (e.g., online media, search engines, etc.) in their evolution to new interactive platforms. The research results will be integrated into a broad set of widely accessible educational materials for a variety of courses (visualization, spatial computing, and text analysis) and will serve as research and practical training for undergraduates, graduates, and professionals. <br/><br/>Providing a scalable solution to automatically generating contextually-relevant visualizations requires the understanding and encoding of the design process. Specifically, the goals of this project are (a) identifying the decision process of visualization designers, (b) creating automated components that operationalize these decisions including text processing, searching through a wide range of heterogeneous data sources and datasets (e.g. census data, stock market data, government macroeconomic data), and automatic visualization construction and annotation, and (c) ranking of the visualizations based on well-known quantitative metrics from information retrieval and information visualization such as relevance, expressiveness, and effectiveness. By extracting key comparisons from an article's text through the use of natural language processing and using existing visualization-article pairs as an evaluation corpus, the system will ensure that relevant datasets are found and that the selected visual forms preserve and enhance the information conveyed in the article. For example, the system will automatically create thematic maps for geospatial comparisons of population change in the U.S. and time series for longitudinal comparisons of company financial results. Although the focus of this work in on the news domain, the research can be extended to other application areas including textbooks, internal company reports, and more generally, to any texts that implicitly or explicitly correspond to quantitative data. Further information, including source code, demos, papers, and datasets, are available at the project homepage (http://txt2vis.cond.org)."
171,1514460,CAREER:  Management of Unstructured Information During Software Evolution,CCF,Software & Hardware Foundation,8/10/14,4/21/15,Andrian Marcus,TX,University of Texas at Dallas,Continuing Grant,Sol Greenspan,7/31/16,"$140,451.00 ",,amarcus@utdallas.edu,"800 W. Campbell Rd., AD15",Richardson,TX,750803021,9728832313,CSE,7798,"1045, 1187, 7944, 9218, HPCC",$0.00 ,"Software is comprised of a multitude of artifacts; some of them are intended to be read by the compiler, while many others are intended to be read by developers.  The user centric information is often expressed in natural language and it is usually much larger in size than the source code.  Given the amount of unstructured data present in existing software systems, tools are necessary for its storage, retrieval, and analysis, before it is delivered to the users.  This type of information is essential during software evolution, when developers need it to understand the software.<br/><br/>The research will define and evaluate an infrastructure for the management of the textual information present in software systems.  The infrastructure will make use of Information Retrieval techniques in combination with statistical and rule-based Natural Language Processing methods and other text analysis techniques.  The infrastructure will be used do define a new type of conceptual model of a software system that will complement the structural, behavioral, and architectural models, which can be extracted and built with traditional analysis and modeling methods.  The new conceptual model and infrastructure will be used to define novel methodologies and build tools to support a variety of software evolution tasks, such as: change propagation in software, traceability link recovery between software artifacts, error and change prediction, quality measurement, concept location, refactoring, and program comprehension in general.  The planned infrastructure will offer a platform for researchers from different areas of computer science (such as, software engineering and computational linguistics) to use state of the art results from each field.  The empirical work will result in a repository of software artifacts and analysis data of textual nature from software to be used to support rigorously controlled experimentation and benchmarking in the research community.<br/>"
172,1422020,ACL 2014 Student Research Workshop,IIS,ROBUST INTELLIGENCE,3/1/14,2/19/14,Jordan Boyd-Graber,MD,University of Maryland College Park,Standard Grant,Tatiana D. Korelsky,2/28/15,"$15,000.00 ",,jbg@umiacs.umd.edu,3112 LEE BLDG 7809 Regents Drive,COLLEGE PARK,MD,207425141,3014056269,CSE,7495,"7495, 7556",$0.00 ,"The Association for Computational Linguistics (ACL) is the primary international organization in the field of natural language processing, and its annual conference is the primary international conference in this research field. This project supports the travel, conference, and housing expenses of students selected to participate in the Association for Computational Linguistics (ACL) Student Research Workshop, held in conjunction with the ACL conference on June 23-25, 2014 in Baltimore. The workshop consists of two tracks: full paper presentations and poster presentations. All selected work has only student authors. The full paper sessions are composed of paper presentations followed by a discussion panel led by respected researchers in the field. The workshop is organized and run by students.<br/><br/>The Student Research Workshop provides a valuable opportunity for the next generation of researchers to enter the computational linguistics community. It allows the best students in the field to receive critical feedback on their work from external experts and to forge contacts with other students and senior researchers. The organizers also pair each participant with a senior researcher to provide specific feedback on the students' research.  The students who select papers for the workshop  and plan the sessions also gain valuable opportunities for professional growth and interaction with the researchers on the organizing committee of the main conference. The workshop contributes to the maintenance and development of a skilled and diverse computational linguistics and natural language processing research community."
173,1350553,CAREER: Holistic Scene Understanding with Multiple Hypotheses from Vision Modules,IIS,Robust Intelligence,9/1/14,6/17/16,Dhruv Batra,VA,Virginia Polytechnic Institute and State University,Continuing grant,Jie Yang,5/31/17,"$289,196.00 ",,dbatra@vt.edu,Sponsored Programs 0170,BLACKSBURG,VA,240610001,5402315281,CSE,7495,"1045, 7495",$0.00 ,"This project develops algorithms and techniques for holistic scene understanding from images. The key barrier to building the next generation of vision systems is ambiguity. For example, a patch from an image may look like a face but may simply be an incidental arrangement of tree branches and shadows. Thus, a vision module operating in isolation often produces nonsensical results, such as hallucinating faces floating in thin air. This project develops a visual system that jointly reasons about multiple plausible hypotheses from different vision modules such as 3D scene layout, object layout, and pose estimation. The developed technologies have the potential to improve vision systems and make fundamental impact - from self-driving cars bringing mobility to the physically impaired, to unmanned aircrafts helping law enforcement with search and rescue in disasters. The project involves research tightly integrated with education and outreach to train the next generation of young scientists and researchers.  <br/><br/>This research addresses the fundamental challenge in joint reasoning by extracting and leveraging a small set of diverse plausible hypotheses or guesses from computer vision modules (e.g. a patch may be a {sky or a vertical surface} x {face or tree branches}). This project generates new knowledge and techniques for (1) generating a small set of diverse plausible hypotheses from different vision modules, (2) joint reasoning over all modules to pick a single hypothesis from each module, and (3) reducing human annotation effort by actively soliciting user feedback only on the small set of plausible hypotheses. <br/><br/>Project Webpage: http://computing.ece.vt.edu/~dbatra"
174,1423305,"RI: Small: Inferring the ""Dark Matter"" and ""Dark Energy"" from Image and Video",IIS,ROBUST INTELLIGENCE,7/15/14,7/15/14,Song-Chun Zhu,CA,University of California-Los Angeles,Standard Grant,Jie Yang,6/30/18,"$454,400.00 ",,sczhu@stat.ucla.edu,10889 Wilshire Boulevard,LOS ANGELES,CA,900951406,3107940102,CSE,7495,"7495, 7923",$0.00 ,"This project develops core techniques for improving the performance of key tasks in computer vision, such as recognizing objects, understanding scenes and events. Improving the performance of these tasks is able to generate broader impacts to the following applications: (1) video surveillance for security and timely intelligence; (2) intelligent robots for rescue in disaster areas; and (3) aerial scene and activity understanding from videos taken by unmanned aerial vehicles. In these applications, a significant portion of the contents in images, including i) entities such as objects, stuff like liquid, human actions, and scenes; and ii) relations, such as intents of humans, causal effects of actions, physical fields and attractions in a scene, cannot be recognized by the geometry and appearance features that are commonly used in current computer vision research. These entities and relations are referred as the ""dark matter"" and ""dark energy,"" by analogy to cosmology models in physics, and plans to develop a unified representation that integrate the ""visible"" and the ""dark"" in a common model where the visible can be used to infer the dark, and the dark pose constraints for the inference of the visible in return. The research team is collaborating with industrial partner for technology transfer.<br/><br/>More specifically, the project studies the following topics: i) Representing causal knowledge to go beyond associational knowledge in computer vision. Casual models are a large part of human knowledge and crucial for answering deeper questions on why, why not, what if (counterfactual). This research is the first formal study of causality (learning, modeling, and reasoning) in the vision literature.  ii) Reasoning the dark entities and relations to go beyond the current geometry and appearance-based paradigm. Perceptual causality, human intents and physics are generally applicable to all categories of object, scene, action and events, i.e., transportable across datasets. These entities and relations are deeper, and more invariant, than geometry and appearance - the dominating features used in visual recognition.   iii) Developing joint representation and joint inference algorithm. The rich contextual and causal links in this joint representation are essential for building robust vision systems where each visual entity can be inferred through multi-routes, but are not systematically studied and integrated in the existing paradigm."
175,1450527,EAGER: Identifying Affective Events and Situations in Text,IIS,ROBUST INTELLIGENCE,9/1/14,8/18/14,Ellen Riloff,UT,University of Utah,Standard Grant,Tatiana D. Korelsky,8/31/16,"$149,991.00 ",,riloff@cs.utah.edu,75 S 2000 E,SALT LAKE CITY,UT,841128930,8015816903,CSE,7495,"7495, 7916, 9150",$0.00 ,"Most natural language processing (NLP) systems still have only a literal understanding of words and phrases. In particular, current NLP technology is oblivious to the psychological impact that events and situations have on people. For example, NLP systems can recognize phrases associated with being hired or fired, but they do not understand that being hired is usually desirable while being fired is generally undesirable. This exploratory research aims to endow NLP technology with the ability to recognize events and situations that will have a positive or negative impact on people. The proposed research could be transformative because it would enable a fundamentally deeper understanding of language beyond the literal meaning of words and phrases. New methods resulting from this research would benefit a wide spectrum of applications that require understanding of written texts and human conversation, document summarization, question answering, sarcasm recognition, and identification of threatening statements. Society could also benefit from this technology through new opportunities to automatically harvest knowledge from the Web and social media about the events and personal situations that most frequently affect different types of people. Recognizing affective states could help to identify at-risk individuals who may be a danger to themselves or others, such as teenagers who are being bullied, veterans who suffer from post-traumatic stress disorder, young adults who feel angry or disenfranchised, and elderly people who may be lonely and depressed. <br/><br/>This EArly Grant for Exploratory Research investigates novel methods for learning to recognize ""affective"" events and situations in text. Affect state recognition is essential to understand people's motivations, plans and goals, causal chains, and the narrative structure of text and conversation. The goal of this research is to create weakly supervised learning methods to automatically identify phrases corresponding to affective events and situations that have a positive or negative impact on people. This research aims to develop a set of independent, weakly supervised learners, where each component will exploit a specific type of linguistic structure and discourse context to identify events and situations that are associated with positive or negative affect states. The set of weakly supervised learners will then be embedded in an ensemble architecture, and multi-view learning methods will be explored to enable large-scale bootstrapped knowledge acquisition across the set of independent learners. Key aspects of this work explore different types of linguistic and discourse structures for bootstrapped learning, different approaches for learning multi-word expressions for events and situations, and different methods for handling contextual ambiguity during both learning and recognition. Lexical affect stateknowledge would improve the ability of NLP systems to recognize sarcasm, identify threats, recognize causal relationships, and achieve a deeper understanding of narrative text and conversational dialogue."
176,1359459,REU Site: MedIX: Medical Informatics Experiences in Undergraduate Research,IIS,RSCH EXPER FOR UNDERGRAD SITES,5/1/14,4/23/14,Daniela Raicu,IL,DePaul University,Standard Grant,Wendy Nilsen,4/30/18,"$306,000.00 ",Jacob Furst,draicu@cti.depaul.edu,1 East Jackson Boulevard,Chicago,IL,606042287,3123627595,CSE,1139,9250,$0.00 ,"The main objectives of the Medical Informatics (MedIX) REU Site program are to engage undergraduate students in challenging research projects, encourage them to pursue graduate education and to expose students to interdisciplinary research, especially at the border of information technology and medicine. The MedIX REU Site runs for three summers, with eight student participants doing research ten weeks each summer under the supervision of three mentors dedicated to undergraduate research. The REU Site is hosted by two interdisciplinary laboratories: the Medical Informatics Laboratory at DePaul University and the Imaging Research Institute at the University of Chicago.  The research environment offers the students the opportunity to interact with computer scientists, medical physicists, and medical doctors.  <br/><br/>All of the projects are inspired by state-of-the-art research questions in imaging informatics. Students work as part of faculty-undergraduate teams on new problems ranging from traditional image processing (e.g., computer-aided diagnosis, breast density assessment, cancer detection) to structured reporting and natural language processing of radiology reports, to workflow and process re-engineering to the application of data mining and ontology-based means for image annotation and markup (e.g., lung nodule detection and interpretation). Ultimately, each project has the long-term potential to increase the quality of healthcare available to people everywhere. More information on the program can be found at the MedIX REU Site web site (http://facweb.cs.depaul.edu/research/vc/medix)."
177,1453555,Collaborative Research: EAGER: Prototype of an Image-Based Ecological Information System (IBEIS),CNS,CSR-Computer Systems Research,9/1/14,9/2/16,Tanya Berger-Wolf,IL,University of Illinois at Chicago,Standard Grant,Marilyn McClure,8/31/18,"$191,186.00 ",,berger-wolf.1@osu.edu,809 S. Marshfield Avenue,Chicago,IL,606124305,3129962862,CSE,7354,"7916, 9102",$0.00 ,"Images are rapidly becoming the most abundant, widely available, and cheapest source of information about the natural world. Images taken by field scientists, tourists, and incidental photographers, and gathered from camera traps and autonomous vehicles provide rich data with the promise of addressing big ecological questions at high resolution and at fine-grained scale. Realizing this potential requires building a large autonomous computational system that starts from image collections and progresses all the way to answering ecological queries, such as population sizes, species distributions and interactions, and movement patterns. The system must have methods of extracting the relevant ecological information from the images and of integrating with other ecological data sources, with minimal human interaction, using state-of-the art information management, computer vision, and data analytics technologies. Such a system will advance computer systems and simultaneously enable ecology to develop as a science of connections across spatial, temporal, and biological scales, as well as provide data- and scientifically-grounded support for ecological decisions. <br/><br/>This work aims to build a prototype of an Image-Based Ecological Information Software System (IBEIS) that relies on a proliferation of images collected daily on a single facility from many different sources, both human and automatic, to determine both the species as well as recognition of distinct individuals. The system will allow for tracking location and movement while providing a data management system that will allow scientists to better understand, and at finer granularity, behaviors and motivations. The system will include: (1) an infrastructure and a mechanism for collecting images from tourists and other sources; (2) a (cloud) infrastructure and a data management system for storing, accessing, and manipulating the images and the derived data; (3) computer vision techniques for extracting information from the images about the identity of individual units, as well as techniques for combining that information with other relevant data to derive information about meaningful ecological units; and (4) statistical techniques and query structures to support ecological queries of the data, such as population sizes and dynamics, movement history and home ranges, and species interactions. <br/><br/>This work will advance computer systems including information management, computer vision, and data analytics technologies, all the while increasing public engagement in science and ecology."
178,1353628,SBIR Phase II:  Quantifying Consumer Rationale Expressed in Free Text Online Discussions,IIP,SMALL BUSINESS PHASE II,4/15/14,5/5/17,Paul Nemirovsky,NY,dMetrics Inc.,Standard Grant,Peter Atherton,6/30/17,"$899,999.00 ",,paul.nemirovsky@gmail.com,181 North 11th St,Brooklyn,NY,112111175,6176427163,ENG,5373,"169E, 4080, 5373, 8032, 9139, HPCC",$0.00 ,"This Small Business Innovation Research Phase II project aims at creating novel methods for analyzing consumer behavior and product acceptance. This project will develop efficient, automated natural language processing (NLP) methods that perform at Web scale to analyze a broad spectrum of consumer experiences with products, as reported in conversations on social media, in consumer reviews, etc. Currently available representations of consumer commentary are relatively shallow. In this project more detailed representations will be built, advancing NLP technology and delivering state-of-the-art analysis of conversations that detail consumer decisions, motivations, and questions about products. In addition, the representations of consumer commentary will be integrated with third-party data, resulting in the development of predictive models of consumer behavior that exceed the accuracy of current models. This will allow product vendors to quantify the influencing factors behind product adoption and attrition, which is crucial to proving marketing hypotheses, predicting business outcomes, and improving customer awareness of product issues.<br/><br/>The broader impact/commercial potential of this project lies in the ability to improve the analysis and use of consumers' perception and experience of products. Deeper analysis of millions of consumer experiences reported online will help the public, manufacturers, marketers, and regulators alike make better decisions about products. Currently, gaining representative insights about a product requires manual processing of an overwhelming range of information sources. Automatic, in-depth analysis of these disparate sources at Web scale will deliver an unbiased, authoritative stream of quantified product information and consumer opinion, improving consumer experience, product performance, and industry transparency. The richness of the resulting representation will enable an understanding of finer-grained aspects of product performance and consumer behavior, which cannot be extracted or analyzed using existing techniques such as sentiment analysis or topic-modeling. The technology developed in this project will translate to fundamental improvements in the conduct of market research, commercial campaigns and regulatory investigations, as well as enhancing the public's ability to make informed decisions about products."
179,1453428,Collaborative Research: EAGER: Prototype of an Image-Based Ecological Information System (IBEIS),CNS,"CSR-Computer Systems Research, Info Integration & Informatics, Software Institutes",9/1/14,2/26/16,Daniel Rubenstein,NJ,Princeton University,Standard Grant,Marilyn McClure,8/31/18,"$88,928.00 ",,dir@princeton.edu,Off. of Research & Proj. Admin.,Princeton,NJ,85442020,6092583090,CSE,"7354, 7364, 8004","7433, 7916",$0.00 ,"Images are rapidly becoming the most abundant, widely available, and cheapest source of information about the natural world. Images taken by field scientists, tourists, and incidental photographers, and gathered from camera traps and autonomous vehicles provide rich data with the promise of addressing big ecological questions at high resolution and at fine-grained scale. Realizing this potential requires building a large autonomous computational system that starts from image collections and progresses all the way to answering ecological queries, such as population sizes, species distributions and interactions, and movement patterns. The system must have methods of extracting the relevant ecological information from the images and of integrating with other ecological data sources, with minimal human interaction, using state-of-the art information management, computer vision, and data analytics technologies. Such a system will advance computer systems and simultaneously enable ecology to develop as a science of connections across spatial, temporal, and biological scales, as well as provide data- and scientifically-grounded support for ecological decisions. <br/><br/>This work aims to build a prototype of an Image-Based Ecological Information Software System (IBEIS) that relies on a proliferation of images collected daily on a single facility from many different sources, both human and automatic, to determine both the species as well as recognition of distinct individuals. The system will allow for tracking location and movement while providing a data management system that will allow scientists to better understand, and at finer granularity, behaviors and motivations. The system will include: (1) an infrastructure and a mechanism for collecting images from tourists and other sources; (2) a (cloud) infrastructure and a data management system for storing, accessing, and manipulating the images and the derived data; (3) computer vision techniques for extracting information from the images about the identity of individual units, as well as techniques for combining that information with other relevant data to derive information about meaningful ecological units; and (4) statistical techniques and query structures to support ecological queries of the data, such as population sizes and dynamics, movement history and home ranges, and species interactions. <br/><br/>This work will advance computer systems including information management, computer vision, and data analytics technologies, all the while increasing public engagement in science and ecology."
180,1350521,"CAREER: High-order Tensor Analysis for Groupwise Correspondence: Theory, Algorithms, and Applications",IIS,"GRAPHICS & VISUALIZATION, Robust Intelligence",2/1/14,8/6/14,Haibin Ling,PA,Temple University,Standard Grant,Jie Yang,11/30/19,"$479,691.00 ",,hling@cs.stonybrook.edu,1801 N. Broad Street,Philadelphia,PA,191226003,2157077547,CSE,"7453, 7495","1045, 7453, 7495",$0.00 ,"Visual matching is a fundamental problem in computer vision (CV) and intensive research efforts have been devoted to building correspondence between a pair of visual objects. By contrast, finding correspondence among an ensemble of objects remains challenging. This project develops a unified framework for this problem and to apply the framework to different applications. The research establishes a close correlation between the classical multi-dimensional assignment (MDA) problem and low-rank tensor approximation. Such correlation paves a way of using high-order tensor analysis for groupwise visual matching that assumes an MDA formulation. Along the way, a series of algorithms are developed to address challenging issues such as computational efficiency and context modeling. These algorithms are then deployed to different tasks including simultaneous tracking of multiple targets, tracking of deformable structures, and batch alignment of visual ensembles. <br/><br/>This project can generate broad impact on areas of computer vision, computer graphics, combinatorial optimization, oral and maxillofacial radiology, image-guided intervention, physical therapy, security and defense, education research, etc. On the one hand, the fundamental importance of visual matching makes the project transformative to many other CV problems. On the other hand, the project benefits a wide range of fields outside the CV community through the use of interdisciplinary applications as test beds. This project also integrates tightly research and education with highlights on supervising students from underrepresented groups, combining computer vision and education research, and involving undergraduates in research."
181,1453503,Collaborative Research: EAGER: Prototype of an Image-Based Ecological Information System (IBEIS),CNS,"Special Projects - CNS, CSR-Computer Systems Research",9/1/14,2/22/16,Charles Stewart,NY,Rensselaer Polytechnic Institute,Standard Grant,Marilyn McClure,8/31/17,"$164,092.00 ",,stewart@cs.rpi.edu,110 8TH ST,Troy,NY,121803522,5182766000,CSE,"1714, 7354","1714, 7916, 9178, 9251",$0.00 ,"Images are rapidly becoming the most abundant, widely available, and cheapest source of information about the natural world. Images taken by field scientists, tourists, and incidental photographers, and gathered from camera traps and autonomous vehicles provide rich data with the promise of addressing big ecological questions at high resolution and at fine-grained scale. Realizing this potential requires building a large autonomous computational system that starts from image collections and progresses all the way to answering ecological queries, such as population sizes, species distributions and interactions, and movement patterns. The system must have methods of extracting the relevant ecological information from the images and of integrating with other ecological data sources, with minimal human interaction, using state-of-the art information management, computer vision, and data analytics technologies. Such a system will advance computer systems and simultaneously enable ecology to develop as a science of connections across spatial, temporal, and biological scales, as well as provide data- and scientifically-grounded support for ecological decisions. <br/><br/>This work aims to build a prototype of an Image-Based Ecological Information Software System (IBEIS) that relies on a proliferation of images collected daily on a single facility from many different sources, both human and automatic, to determine both the species as well as recognition of distinct individuals. The system will allow for tracking location and movement while providing a data management system that will allow scientists to better understand, and at finer granularity, behaviors and motivations. The system will include: (1) an infrastructure and a mechanism for collecting images from tourists and other sources; (2) a (cloud) infrastructure and a data management system for storing, accessing, and manipulating the images and the derived data; (3) computer vision techniques for extracting information from the images about the identity of individual units, as well as techniques for combining that information with other relevant data to derive information about meaningful ecological units; and (4) statistical techniques and query structures to support ecological queries of the data, such as population sizes and dynamics, movement history and home ranges, and species interactions. <br/><br/>This work will advance computer systems including information management, computer vision, and data analytics technologies, all the while increasing public engagement in science and ecology."
182,1445079,EAGER: Effective Detection of Vulnerabilities and Linguistic Stratification in Open Source Software,CNS,"SPECIAL PROJECTS - CISE, Secure &Trustworthy Cyberspace",10/1/14,12/18/15,Raul Aranovich,CA,University of California-Davis,Standard Grant,Nina Amla,9/30/17,"$314,999.00 ","Vladimir Filkov, Premkumar Devanbu",raranovich@ucdavis.edu,OR/Sponsored Programs,Davis,CA,956186134,5307547700,CSE,"1714, 8060","7434, 7916, 8225, 9102, 9178, 9251",$0.00 ,"Software vulnerabilities are weaknesses in the code that may be exploited by cybercriminals to harm a system. They often do not hinder a program's functionality, and are thus difficult to detect. This project focuses on developing methods to identify such ""weak spots"" in a program, where vulnerabilities are more likely to occur. <br/><br/>The approach used for detecting weak spots is based on the novel idea of examining linguistic patterns employed by code developers in Open-Source Software (OSS) online communities. Using a combination of natural language processing methods and sociolinguistic analyses, the PIs research the links between a programmer's role within a social hierarchy of trust and influence and his or her skills in producing code that avoids vulnerabilities and adheres to the communal cybersecurity standards. The research results in a faster way to identify vulnerabilities, therefore contributing to make programs safer. It also contributes to understanding of the natural properties of code and the social dynamics of communication in online groups, laying the foundation for further research into linguistic aspects of software engineering."
183,1422381,CHS: Small: Making sense of information in online discussion boards with novel social computing platforms,IIS,HCC-Human-Centered Computing,9/1/14,7/16/14,Olena Mamykina,NY,Columbia University,Standard Grant,William Bainbridge,8/31/17,"$499,931.00 ",,om2196@cumc.columbia.edu,2960 Broadway,NEW YORK,NY,100276902,2128546851,CSE,7367,"7367, 7923",$0.00 ,"This project will analyze how individuals make sense of information collected within online health forums and how existing computing platforms facilitate or inhibit this process.  Increasingly, individuals of all walks of life go online to collect and share information, collectively make sense of it and negotiate its meaning, reach a consensus, or agree to disagree, and, thus, generate a repository of collective wisdom that can be shared and reused.  These social behaviors, often referred to as ""sensemaking,"" are particularly consequential in health and wellness management, as increasing numbers of individuals join online health support communities and rely on their peers for help and advice.  Despite the ongoing research on social computing platforms and on attitudes, perceptions and behaviors of their users, the dynamics of computer-mediated sensemaking remain poorly understood.  Moreover, most of the information collected within these forums continues to exist in the form of discussion threads that provide little guidance as to the main topics discussed, the relationships between posts within the thread, different attitudes towards the topics of interest, and the overall dynamics of the discussions.  The overarching goal of this research is to develop novel summarization, visualization, and interaction mechanisms to help individuals make sense of information and opinions collected in online forums. <br/><br/>Specifically, the research will seek to understand how sensemaking unfolds within discussion threads, describe it through a set of formalizations, or semantic discussion typologies, and identify ways to study it computationally.  Participatory design methods will then be used to develop novel visualization and interaction mechanisms for facilitating individual and collective sensemaking within online forums.  A combination of computational data analysis and visualization techniques will automatically generate visual summaries of discussions threads that can help users to understand the evolution of meaning in discussions. This research will build upon previous work in natural language processing and data mining, to represent salient properties of the discussion threads, such as discussion topics and their transformation overtime, and the various attitudes towards the topics among the users. Finally, the discussion visualization tools will be evaluated on their impact on individual and collective sensemaking in controlled experiments and in a deployment study within an existing online health community oriented toward the difficult health issues associated with diabetes."
184,1418044,Bio-Sphere: Fostering Deep Learning of Complex Biology for Building Our Next Generation's Scientists,DRL,Discovery Research K-12,9/1/14,6/20/18,Sadhana Puntambekar,WI,University of Wisconsin-Madison,Continuing Grant,Catherine L. Eberbach,8/31/20,"$2,798,985.00 ","N Narayanan, Janice Anderson, Sharon Derry, Jee-Seon Kim",puntambekar@education.wisc.edu,21 North Park Street,MADISON,WI,537151218,6082623822,EHR,7645,,$0.00 ,"Today's citizens face profound questions in science. Preparing future generations of scientists is crucial if the United States is to remain competitive in a technology-focused economy. The biological sciences are of particular importance for addressing some of today's complex problems, such as sustainability and food production, biofuels, and carbon dioxide and its effect on our environment. Although knowledge in the life sciences is of critical importance, this is an area in which there are significantly fewer studies examining students' conceptions than in physics and chemistry. The goal of this project is to help middle school students, particularly in rural and underserved areas, develop deep scientific knowledge and knowledge of the practices and routines of science.  A major strength of Bio-Sphere is the inclusion of hands-on design and engineering in biology, a field in which there are fewer instances of curricula that integrate engineering design at the middle school level. The units will enable an in-depth, cohesive understanding of science content, and Bio-Sphere will be disseminated nationally and internationally through proactive outreach to teachers as well as scholarly publications.<br/><br/>This project addresses the need to inculcate deep learning of complex science by bringing complex socio-scientific issues into middle school classrooms, and providing students with instructional materials that allow them to practice science as scientists do.  Research teams will develop, iteratively refine and evaluate an innovative learning environment called Bio-Sphere.  Bio-Sphere combines the strengths of hands-on design and engineering, engages students in the practices of science, and fosters learning of complex science issues, especially among underserved populations.  Each Bio-Sphere unit presents a complex science issue in the form of a design challenge that students solve by conducting experiments, using visualizations in an electronic textbook, and connecting with the community. The units, aligned with the Next Generation Science Standards, provide greater coherence, continuity, and sustained instruction focused on uncovering and integrating key ideas over long periods of time. The project will follow a design-based research methodology. In Phase 1, the Bio-Sphere materials will be developed. Phase 2 will consist of studies in Wisconsin schools to generate existence proofs, i.e., examining enactments with respect to the designed objectives to understand how a design works. Phase 3 studies will focus on practical implementation: how to bring this innovative design to life in very different classroom contexts and without the everyday support of the design team, and will be conducted in rural schools in Alabama and North Carolina."
185,1423651,RI: Small: Visual Situation Recognition: An Integration of Deep Networks and Analogy-Making,IIS,Robust Intelligence,9/1/14,5/15/18,Melanie Mitchell,OR,Portland State University,Standard Grant,Kenneth Whang,8/31/19,"$475,754.00 ",,mm@pdx.edu,1600 SW 4th Ave,Portland,OR,972070751,5037259900,CSE,7495,"7495, 7923, 8089, 9251",$0.00 ,"This project investigates a novel approach to building computer systems that can recognize visual situations.  While much effort in computer vision has focused on identifying isolated objects in images, what people actually do is recognize coherent situations--collections of objects and their interrelations that, taken together, correspond to a known concept, such as ""a child's birthday party,"" or ""a man walking a dog on the beach,"" or ""two people about to fight,"" or ""a blind person crossing the street.""  Situation recognition by humans may appear on the surface to be effortless, but it relies on a complex dynamic interplay among human abilities to perceive objects, systems of relationships among objects, and analogies with stored knowledge and memories.  No computer vision system yet comes close to capturing these human abilities.  Enabling computers to flexibly recognize visual situations would create a flood of important applications in fields as diverse as medical diagnosis, interpretation of scientific imagery, enhanced human-computer interaction, and personal information organization.<br/><br/>The approach explored in this project integrates two previously studied approaches:  brain-inspired neural networks for lower-level vision and cognitive-level models of concepts and analogy-making.  In this integrated architecture, recognizing situations--via analogies with stored conceptual structures--will be a dynamic process in which bottom-up (perceptual) and top-down (conceptual) influences affect one another as perception unfolds.  If successful, this system will be able to recognize visual situations in a way that scales well with the complexity of the scene and the abstract concept being recognized.  As part of this project, a number of benchmark image datasets--reflecting different abstract visual situations--will be collected to evaluate the recognition system.  In addition, the PI will design and run a public competition on automated recognition of visual situations, using the collected datasets.  This competition will spur research on this topic, and help researchers working in this area evaluate the success of various methods and gauge the current state of the art on abstract visual recognition.  All source code and benchmarking databases developed in this project will be made publicly available via the web."
186,1356347,"ABI Development: Global Names Discovery, Indexing and Reconciliation Services",DBI,ADVANCES IN BIO INFORMATICS,8/1/14,10/19/15,Dmitry Mozzherin,MA,Marine Biological Laboratory,Continuing Grant,Peter McCartney,7/31/16,"$410,293.00 ","Richard Pyle, David Patterson, Dmitry Mozzherin",mozzheri@illinois.edu,7 M B L ST,WOODS HOLE,MA,25431015,5082897243,BIO,1165,9178,$0.00 ,"In the mid-18th century biology as a science was revolutionized by Carl Linneaus and the use of binomial scientific names. Such names consist of two words in Latin - genus name and specific epithet - for example, Homo sapiens for humans. For higher classifications Linneaus adopted names consisting of a single Latin word. This nomenclatural system dramatically simplified handling species information, and gave scientists from different countries a standard way of communication. This system has been so successful it has persisted for over 250 years and is still in very active use today. An enormous amount of scientific and popular science literature uses scientific names. In the electronic age scientific names enable the effective organization of biodiversity information from various sources, as demonstrated by the Encyclopedia of Life (http://eol.org) and Biodiversity Heritage Library (http://bhl.org). However the naming system introduced by Linnaeus is far from perfect. For example, if scholars decide to move a particular species to different genus - the name changes, and finding information about it becomes much more difficult. For example, a very prominent model organism of modern science Drosophila melanogaster (fruit fly) is now recommended to be placed in the genus Sophophora, changing its name to Sophophora melanogaster. Such changes are inevitable in Linnaean nomenclature, and they create a significant amount of confusion. The project aims to create a system which would allow a user to enter a name, and find out if it is a known scientific name, if another name should be used instead, if there are other historical synonyms, and if this name is a misspelling of a known scientific name. The system will also display which data sources contain information related to a name, and will provide a list of relevant web-pages (URLs). This project aims to make significant strides in removing ambiguity and confusion from biodiversity data. Teaching activities are planned with college students at Arizona State and there are opportunities for participation in Google Summer of Code and Biodiversity informatics training courses developed at MBL.<br/><br/>The first stage of the research was also funded by NSF and it allowed the development of a prototype and 'proof of concept' algorithms for finding and verifying scientific names in texts, images, species lists. This second phase aims to make production version of the algorithms, make them precise, fast, and scale them up to serve the global biological community.  The task of scientific name disambiguation is not a simple one. It requires cooperation of many researchers. Discovery of scientific names in texts uses natural language processing algorithms which we plan to improve. The project aims to collect all known spellings and renderings of scientific names (20 million are currently collected) and organizes them into groups belonging to the same sets of organisms. It also catalogues where each spelling was used and stores information as a global name index. The database is powered by a search engine which uses fuzzy matching algorithms, and name parsing algorithms to find that names like 'Carek scirpoidea Michx. var. convoluta Kk.' and 'Carex scirpoidea subsp. convoluta (Kukenth) D. A. Dunlop' refer to the same<br/>subspecies. The project's URL is: http://globalnames.org/"
187,1462280,CHS: Medium: Collaborative Research: Immediate Feedback to Support Learning American Sign Language through Multisensory Recognition,IIS,HCC-Human-Centered Computing,8/20/14,9/15/14,Matt Huenerfauth,NY,Rochester Institute of Tech,Standard Grant,Ephraim Glinert,8/31/20,"$537,997.00 ",,matt.huenerfauth@rit.edu,1 LOMB MEMORIAL DR,ROCHESTER,NY,146235603,5854757987,CSE,7367,"7367, 7924",$0.00 ,"American Sign Language (ASL) is a primary means of communication for 500,000 people in the United States and a distinct language from English, conveyed through hands, facial expressions, and body movements.  Studies indicate that deaf children of deaf parents read better than deaf children of hearing parents, mainly due to better communication when both children and parents are deaf.  However, more than 80% of children who are deaf or hard of hearing are born to hearing parents.  It is challenging for parents, teachers, and other people in the life of a deaf child to learn ASL rapidly enough to support the visual language acquisition of the child.  Technology that can automatically recognize aspects of ASL signing and provide instant feedback to these students of ASL would give them a time-flexible way to practice and improve their signing skills.  The goal of this project, which involves an interdisciplinary team of researchers at three colleges within the City University of New York (CUNY) with expertise in computer vision, human-computer interaction, and Deaf and Hard of Hearing education, is to discover the most effective underlying technologies, user-interface design, and pedagogical use for an interactive tool to provide such immediate, automatic feedback for students of ASL.<br/><br/>Most prior work on ASL recognition has focused on identifying a small set of simple signs performed, but current technology is not sufficiently accurate on continuous signing of sentences with an unrestricted vocabulary.  The PIs will develop technologies to fundamentally advance ASL partial recognition, that is to identify linguistic/performance attributes of ASL without necessarily identifying the entire sequence of signs, and automatically determine if a performance is fluent or contains errors.  The research  will include five thrusts: (1) based on ASL linguistics and pedagogy, to identify a set of observable attributes indicating ASL fluency; (2) to discover new technologies for automatic detection of the ASL fluency attributes through fusion of multimodality (facial expression, hand gesture, and body pose) and multisensory information (RGB and Depth videos); (3) to collect and annotate a dataset of RGBD videos of ASL, performed at varied levels of fluency, by students and native signers; (4) to develop an interactive ASL learning tool that provides ASL students immediate feedback about whether their signing is fluent or not; and (5) to evaluate the robustness of the new algorithms and the effectiveness of the ASL learning tool, including its educational benefits.  The work will lead to advances in computer vision technologies for human behavior perception, to new understanding of user-interface design with ASL video, and to a revolutionary and cost-effective educational tool to assist ASL learners achieve fluency, using recognition technologies that are robust and accurate in the near-term.  Project outcomes will include a dataset of videos at varied fluency levels, which will be valuable for future ASL linguists or instructors, students learning ASL, and computer vision researchers."
188,1400906,CHS: Medium: Collaborative Research: Immediate Feedback to Support Learning American Sign Language through Multisensory Recognition,IIS,HCC-Human-Centered Computing,9/1/14,6/26/14,Matt Huenerfauth,NY,CUNY Queens College,Standard Grant,Ephraim Glinert,10/31/14,"$537,997.00 ",,matt.huenerfauth@rit.edu,65 30 Kissena Blvd,Flushing,NY,113671575,7189975400,CSE,7367,"7367, 7924",$0.00 ,"American Sign Language (ASL) is a primary means of communication for 500,000 people in the United States and a distinct language from English, conveyed through hands, facial expressions, and body movements.  Studies indicate that deaf children of deaf parents read better than deaf children of hearing parents, mainly due to better communication when both children and parents are deaf.  However, more than 80% of children who are deaf or hard of hearing are born to hearing parents.  It is challenging for parents, teachers, and other people in the life of a deaf child to learn ASL rapidly enough to support the visual language acquisition of the child.  Technology that can automatically recognize aspects of ASL signing and provide instant feedback to these students of ASL would give them a time-flexible way to practice and improve their signing skills.  The goal of this project, which involves an interdisciplinary team of researchers at three colleges within the City University of New York (CUNY) with expertise in computer vision, human-computer interaction, and Deaf and Hard of Hearing education, is to discover the most effective underlying technologies, user-interface design, and pedagogical use for an interactive tool to provide such immediate, automatic feedback for students of ASL.<br/><br/>Most prior work on ASL recognition has focused on identifying a small set of simple signs performed, but current technology is not sufficiently accurate on continuous signing of sentences with an unrestricted vocabulary.  The PIs will develop technologies to fundamentally advance ASL partial recognition, that is to identify linguistic/performance attributes of ASL without necessarily identifying the entire sequence of signs, and automatically determine if a performance is fluent or contains errors.  The research  will include five thrusts: (1) based on ASL linguistics and pedagogy, to identify a set of observable attributes indicating ASL fluency; (2) to discover new technologies for automatic detection of the ASL fluency attributes through fusion of multimodality (facial expression, hand gesture, and body pose) and multisensory information (RGB and Depth videos); (3) to collect and annotate a dataset of RGBD videos of ASL, performed at varied levels of fluency, by students and native signers; (4) to develop an interactive ASL learning tool that provides ASL students immediate feedback about whether their signing is fluent or not; and (5) to evaluate the robustness of the new algorithms and the effectiveness of the ASL learning tool, including its educational benefits.  The work will lead to advances in computer vision technologies for human behavior perception, to new understanding of user-interface design with ASL video, and to a revolutionary and cost-effective educational tool to assist ASL learners achieve fluency, using recognition technologies that are robust and accurate in the near-term.  Project outcomes will include a dataset of videos at varied fluency levels, which will be valuable for future ASL linguists or instructors, students learning ASL, and computer vision researchers."
189,1400810,CHS: Medium: Collaborative Research: Immediate Feedback to Support Learning American Sign Language through Multisensory Recognition,IIS,HCC-Human-Centered Computing,9/1/14,6/26/14,Elaine Gale,NY,CUNY Hunter College,Standard Grant,Ephraim Glinert,8/31/20,"$120,000.00 ",,egale@hunter.cuny.edu,695 Park Avenue,New York,NY,100655024,2127724020,CSE,7367,"7367, 7924",$0.00 ,"American Sign Language (ASL) is a primary means of communication for 500,000 people in the United States and a distinct language from English, conveyed through hands, facial expressions, and body movements.  Studies indicate that deaf children of deaf parents read better than deaf children of hearing parents, mainly due to better communication when both children and parents are deaf.  However, more than 80% of children who are deaf or hard of hearing are born to hearing parents.  It is challenging for parents, teachers, and other people in the life of a deaf child to learn ASL rapidly enough to support the visual language acquisition of the child.  Technology that can automatically recognize aspects of ASL signing and provide instant feedback to these students of ASL would give them a time-flexible way to practice and improve their signing skills.  The goal of this project, which involves an interdisciplinary team of researchers at three colleges within the City University of New York (CUNY) with expertise in computer vision, human-computer interaction, and Deaf and Hard of Hearing education, is to discover the most effective underlying technologies, user-interface design, and pedagogical use for an interactive tool to provide such immediate, automatic feedback for students of ASL.<br/><br/>Most prior work on ASL recognition has focused on identifying a small set of simple signs performed, but current technology is not sufficiently accurate on continuous signing of sentences with an unrestricted vocabulary.  The PIs will develop technologies to fundamentally advance ASL partial recognition, that is to identify linguistic/performance attributes of ASL without necessarily identifying the entire sequence of signs, and automatically determine if a performance is fluent or contains errors.  The research  will include five thrusts: (1) based on ASL linguistics and pedagogy, to identify a set of observable attributes indicating ASL fluency; (2) to discover new technologies for automatic detection of the ASL fluency attributes through fusion of multimodality (facial expression, hand gesture, and body pose) and multisensory information (RGB and Depth videos); (3) to collect and annotate a dataset of RGBD videos of ASL, performed at varied levels of fluency, by students and native signers; (4) to develop an interactive ASL learning tool that provides ASL students immediate feedback about whether their signing is fluent or not; and (5) to evaluate the robustness of the new algorithms and the effectiveness of the ASL learning tool, including its educational benefits.  The work will lead to advances in computer vision technologies for human behavior perception, to new understanding of user-interface design with ASL video, and to a revolutionary and cost-effective educational tool to assist ASL learners achieve fluency, using recognition technologies that are robust and accurate in the near-term.  Project outcomes will include a dataset of videos at varied fluency levels, which will be valuable for future ASL linguists or instructors, students learning ASL, and computer vision researchers."
190,1353200,SBIR Phase II:  Applying Semantic Paradata to Outcomes-aligned Assessment,IIP,SBIR Phase II,4/15/14,7/23/15,Robert Robson,OR,Eduworks Corporation,Standard Grant,Glenn H. Larsen,9/30/16,"$768,000.00 ",,robby.robson@eduworks.com,400 SW 4th Street,Corvallis,OR,973334899,5417530844,ENG,5373,"5373, 8031, 8240",$0.00 ,"This SBIR Phase II project will result in a cloud-based web application that uses natural language processing to automatically generate online assessment questions for any digital textual material and to align questions with topics and learning outcomes. These questions will be available for instructors to review, edit and use in online quizzes that students can take on desktops, laptops, tablets or smartphones. The application will grade the quizzes and analyze both individual and aggregate results. Instructors can enhance the analysis by tying errors to common student misconceptions. The application will provide instructors with near real-time updates on student learning and provide students with personalized and immediate feedback. The application will maintain a bank of questions that can be customized to an academic unit or institution and that will grow with use. The question bank and data collected by the system will be mined to improve performance over time.  A beta release will be evaluated at multiple universities during Phase II. <br/><br/>The broader/commercial impact of this work includes its potential to improve instruction for over 21 million American college and university students. Assessments play critical roles in higher education, but it currently takes considerable instructor time and effort to produce, grade and maintain them.  Using the application developed in this Phase II SBIR, instructors will be able to generate and deliver quizzes quickly enough for ad-hoc in-class use and easily enough to integrate frequent practice and diagnostic tests into daily instruction. This will enable instructors to better evaluate students, keep students engaged, and tailor their instruction to student needs. Students will be able to take more practice tests, which evidence shows helps retain and recall class content, and administrators will obtain more data tied directly to student outcomes.  Expected benefits include increased student success rates and richer data that universities and accreditation agencies can analyze to evaluate and improve programs. In addition, this Phase II SBIR will implement, test and improve techniques for automated question generation and outcomes alignment that can be used in other applications."
191,1423337,III: Small: Collaborative Research: Keyphrase Extraction in Document Networks,IIS,Info Integration & Informatics,9/1/14,7/12/16,Cornelia Caragea,TX,University of North Texas,Continuing grant,Maria Zemankova,1/31/18,"$324,866.00 ",Paul Tarau,cornelia@uic.edu,1155 Union Circle #305250,Denton,TX,762035017,9405653940,CSE,7364,"7364, 7923",$0.00 ,"Keyphrases for a document concisely describe the document using a small set of phrases (i.e., sequences of contiguous words in a document). For example, the keyphrases ""social networks"" and ""interest targeting"" quickly provide us with a high-level topic description (i.e., a summary) of a document focused on targeting interest for recommending services such as products and news to users, in the context of social networks. Given today's very large collections of documents, these keyphrases are extremely important not only for summarizing a document, but also for the search and retrieval of relevant information. However, keyphrases are not always available directly. Instead, they need to be gleaned from the many details in documents. This project addresses the problem of automatic keyphrase extraction from research papers, which are enablers of the sharing and dissemination of scientific discoveries. The goal of the project is to explore accurate approaches that automatically discover and extract keyphrases in documents, using document networks, which will help users handle and digest more information in less time during these ""big data"" times. Educationally, this research will involve training of both graduate and undergraduate students in the active area of research of keyphrase extraction, which has high impact in many real-world applications such as online advertising, document categorization, recommendation, and summarization, Web search and discovery, and topic tracking in newswire.<br/><br/>Although much research to date has been done on automatic keyphrase extraction, no previous approaches have captured the impact of documents on one another via the citation relation that connects documents in a network. This project will investigate models that take into consideration the linkage between citing and cited documents in a document network and will explore various qualitative and quantitative aspects of the question: ""What are the key phrases or concepts in a document?"" Scalable iterative algorithms will be designed and developed that capture different aspects of documents (e.g., topics or concepts), as well as the impact of one document on another (e.g., influence or topic evolution) in a document network. The results of this research will have a direct pipeline to the CiteSeerX digital library (http://citeseerx.ist.psu.edu). The software, tools, and benchmark datasets developed during the course of this project will be broadly disseminated via the project website (http://people.cs.ksu.edu/~ccaragea/keyphrases.html).  All findings will be shared to the research community through publications in academic journals and presented in Information Retrieval, Text Mining and Natural Language Processing conferences."
192,1422951,III: Small: Collaborative Research: Keyphrase Extraction in Document Networks,IIS,Info Integration & Informatics,9/1/14,7/26/16,C. Lee Giles,PA,Pennsylvania State Univ University Park,Continuing grant,Maria Zemankova,8/31/18,"$175,000.00 ",,giles@ist.psu.edu,110 Technology Center Building,UNIVERSITY PARK,PA,168027000,8148651372,CSE,7364,"7364, 7923",$0.00 ,"Keyphrases for a document concisely describe the document using a small set of phrases (i.e., sequences of contiguous words in a document). For example, the keyphrases ""social networks"" and ""interest targeting"" quickly provide us with a high-level topic description (i.e., a summary) of a document focused on targeting interest for recommending services such as products and news to users, in the context of social networks. Given today's very large collections of documents, these keyphrases are extremely important not only for summarizing a document, but also for the search and retrieval of relevant information. However, keyphrases are not always available directly. Instead, they need to be gleaned from the many details in documents. This project addresses the problem of automatic keyphrase extraction from research papers, which are enablers of the sharing and dissemination of scientific discoveries. The goal of the project is to explore accurate approaches that automatically discover and extract keyphrases in documents, using document networks, which will help users handle and digest more information in less time during these ""big data"" times. Educationally, this research will involve training of both graduate and undergraduate students in the active area of research of keyphrase extraction, which has high impact in many real-world applications such as online advertising, document categorization, recommendation, and summarization, Web search and discovery, and topic tracking in newswire.<br/><br/>Although much research to date has been done on automatic keyphrase extraction, no previous approaches have captured the impact of documents on one another via the citation relation that connects documents in a network. This project will investigate models that take into consideration the linkage between citing and cited documents in a document network and will explore various qualitative and quantitative aspects of the question: ""What are the key phrases or concepts in a document?"" Scalable iterative algorithms will be designed and developed that capture different aspects of documents (e.g., topics or concepts), as well as the impact of one document on another (e.g., influence or topic evolution) in a document network. The results of this research will have a direct pipeline to the CiteSeerX digital library (http://citeseerx.ist.psu.edu). The software, tools, and benchmark datasets developed during the course of this project will be broadly disseminated via the project website (http://www.cse.unt.edu/~ccaragea/keyphrases.html). All findings will be shared to the research community through publications in academic journals and presented in Information Retrieval, Text Mining and Natural Language Processing conferences."
193,1400802,CHS: Medium: Collaborative Research: Immediate Feedback to Support Learning American Sign Language through Multisensory Recognition,IIS,HCC-Human-Centered Computing,9/1/14,6/24/19,YingLi Tian,NY,CUNY City College,Standard Grant,Ephraim Glinert,8/31/20,"$565,918.00 ",,ytian@ccny.cuny.edu,Convent Ave at 138th St,New York,NY,100319101,2126505418,CSE,7367,"7367, 7924, 9251",$0.00 ,"American Sign Language (ASL) is a primary means of communication for 500,000 people in the United States and a distinct language from English, conveyed through hands, facial expressions, and body movements.  Studies indicate that deaf children of deaf parents read better than deaf children of hearing parents, mainly due to better communication when both children and parents are deaf.  However, more than 80% of children who are deaf or hard of hearing are born to hearing parents.  It is challenging for parents, teachers, and other people in the life of a deaf child to learn ASL rapidly enough to support the visual language acquisition of the child.  Technology that can automatically recognize aspects of ASL signing and provide instant feedback to these students of ASL would give them a time-flexible way to practice and improve their signing skills.  The goal of this project, which involves an interdisciplinary team of researchers at three colleges within the City University of New York (CUNY) with expertise in computer vision, human-computer interaction, and Deaf and Hard of Hearing education, is to discover the most effective underlying technologies, user-interface design, and pedagogical use for an interactive tool to provide such immediate, automatic feedback for students of ASL.<br/><br/>Most prior work on ASL recognition has focused on identifying a small set of simple signs performed, but current technology is not sufficiently accurate on continuous signing of sentences with an unrestricted vocabulary.  The PIs will develop technologies to fundamentally advance ASL partial recognition, that is to identify linguistic/performance attributes of ASL without necessarily identifying the entire sequence of signs, and automatically determine if a performance is fluent or contains errors.  The research  will include five thrusts: (1) based on ASL linguistics and pedagogy, to identify a set of observable attributes indicating ASL fluency; (2) to discover new technologies for automatic detection of the ASL fluency attributes through fusion of multimodality (facial expression, hand gesture, and body pose) and multisensory information (RGB and Depth videos); (3) to collect and annotate a dataset of RGBD videos of ASL, performed at varied levels of fluency, by students and native signers; (4) to develop an interactive ASL learning tool that provides ASL students immediate feedback about whether their signing is fluent or not; and (5) to evaluate the robustness of the new algorithms and the effectiveness of the ASL learning tool, including its educational benefits.  The work will lead to advances in computer vision technologies for human behavior perception, to new understanding of user-interface design with ASL video, and to a revolutionary and cost-effective educational tool to assist ASL learners achieve fluency, using recognition technologies that are robust and accurate in the near-term.  Project outcomes will include a dataset of videos at varied fluency levels, which will be valuable for future ASL linguists or instructors, students learning ASL, and computer vision researchers."
194,1451500,EAGER: New Optimization Methods for Machine Learning,IIS,Robust Intelligence,9/1/14,8/29/17,Tony Jebara,NY,Columbia University,Standard Grant,Weng-keen Wong,5/31/18,"$100,000.00 ",,jebara@cs.columbia.edu,2960 Broadway,NEW YORK,NY,100276902,2128546851,CSE,7495,"7495, 7916",$0.00 ,"This proposal explores the optimization of complicated nonlinear equations that underlie machine learning problems by reducing them to simpler easy-to-solve update rules. The learning problems include classification, regression, unsupervised learning and more. Through a method known as majorization, complicated optimization problems are handled by iteratively solving simpler problems like least-squares and traditional linear algebra operations. The proposal focuses on how to parallelize this method so that it can efficiently leverage many CPUs/GPUs simultaneously and in a distributed manner. Furthermore, by making the method stochastic, faster convergence on large or streaming data-sets becomes possible. Other variations are explored such as sparse learning where the recovered solution is forced to be compact which also leads to further efficiency.  <br/><br/>Increasingly, the vast majority of machine learning problems in the literature are optimized by using generic first- and second-order methods.  The approach in this proposal is designed specifically for machine learning optimization problems and uses majorization and bounding to guarantee monotonic convergence.  In preliminary work, majorization has produced faster convergence in practice as well as novel theoretical guarantees. To make the method truly viable in practice, this proposal puts forward distributed, parallel, stochastic and sparse extensions. Since such extensions may violate monotonic convergence guarantees, the proposal explores additional algorithmic and theoretical efforts to preserve guarantees while also obtaining fast algorithms. In particular, parallelization and distributed computation is performed by wrapping current state-of-the-art least squares solvers with bound majorization steps. Stochastic computation is explored using singleton, small-batch and variable-sized batch methods. Sparsity is achieved by iterating current large-scale sparse solvers like FISTA and QUIC within the bound majorization technique.  In terms of broader impact, one graduate student will be supported and will help produce downloadable tools for machine learning experts as well as practitioners.  Modules will be developed to add to the PI's existing courses in machine learning. The PI will organize a one-day workshop on majorization methods.  The proposal also provides a public project website with access to research publications, software/data downloads and schedules of upcoming events."
195,1350522,CAREER: Machine Learning for Complex Health Data Analytics,IIS,Info Integration & Informatics,9/1/14,6/27/18,Benjamin Marlin,MA,University of Massachusetts Amherst,Continuing Grant,Sylvia Spengler,8/31/19,"$536,527.00 ",,marlin@cs.umass.edu,Research Administration Building,Hadley,MA,10359450,4135450698,CSE,7364,"1045, 7364",$0.00 ,"The fields of health and behavioral science are currently undergoing a data revolution. The Health Information Technology for Economic and Clinical Health act of 2009 has resulted in the wide adoption of electronic health records and the emergence of increasingly vast stores of heterogeneous clinical data. Simultaneously, emerging mobile health (mHealth) technologies are enabling the collection of ever-larger volumes of continuous physiological measurements and behavioral self-report data in non-clinical settings. Such data sources have the potential to yield transformative advances in the fundamental understanding of human behavior and health. They also have the potential to significantly enhance numerous applications including data-driven clinical decision support and continuous health monitoring, which will lead to increased efficiency within the healthcare system and facilitate a transition to patient-centered, personalized care. The proposed work will address several fundamental sources of complexity in the analysis of both clinical and mHealth data, enabling researchers in health and behavioral science to extract more useful knowledge from these data sources. The software toolboxes that will be developed will have immediate applications in research conducted by a network of research partners, and will also be broadly disseminated. The integrated education plan includes the development of an innovative applied machine learning course that will provide training in topics like cloud-scale computing that are of direct relevance to massive health data analytics. The outreach plan involves developing and running a health data-themed outreach workshop for underrepresented groups to foster computational thinking and broaden participation in computing. <br/><br/>The ability to learn models from complex data and apply those models to extract useful knowledge is at the core of machine learning research. This proposal seeks to significantly expand the frontiers of machine learning by developing new models and algorithms designed to meet the challenges posed by complex health data analysis. Key sources of complexity in clinical and mHealth data include sparse and irregular sampling, incompleteness, noise, non-stationary temporal dynamics, between-subjects variability, high volume, high velocity and heterogeneity. The presence of one or more of these factors in a given data source is often sufficient to render current machine learning methods ineffective or completely inapplicable. The long-term goal of this research is the development and validation of customized machine learning models and algorithms that can respond to all of these challenges. The objective of this proposal is to develop models and algorithms that address the following specific problems: (1) How can we extract useful knowledge from sparse and irregularly sampled clinical time series data?  (2) How can we automate feature discovery from wearable physiological sensor data in the presence of high levels of noise, significant between subjects variability, and heterogeneous sensing modalities?  (3) How can we make the learning of physiological time series event detection algorithms robust to event labels that are obtained through self-report mechanisms with limited reliability and temporal fidelity?"
196,1409097,RI: Medium: Deep Annotation: Measuring Human Vision to Improve Machine Vision,IIS,Robust Intelligence,9/1/14,8/7/14,David Cox,MA,Harvard University,Standard Grant,Kenneth Whang,8/31/19,"$674,099.00 ",Ken Nakayama,davidcox@fas.harvard.edu,1033 MASSACHUSETTS AVE,Cambridge,MA,21385369,6174955501,CSE,7495,"7495, 7924",$0.00 ,"Machine learning is the science of designing computational systems that can learn from data, much as humans do.  However, while many machine learning approaches rely on humans to provide labels for training examples that are used for learning, human-provided labels represent just a tiny fraction of the information that can be gleaned from humans.  This project brings together a multidisciplinary team with expertise spanning computer science, neuroscience and psychology to pioneer a new paradigm in machine learning that seeks to better mimic human performance by incorporating new kinds of information about human behavior.  <br/><br/>Specifically, this project brings the disciplines of psychophysics and psychometrics, which seek to quantitatively describe human performance -- patterns of errors, reaction times, and variations across populations of humans -- together with machine learning to develop systems that learn both from human successes and failures, to generate artificial systems that perform better and generalize better to data outside of their training sets.  The project team has already shown initial proof of concept in applying these ideas to the problem of face detection in difficult, cluttered real-world images.  During the project period, the team will greatly expand these ideas, developing new applications (including face and object recognition tasks), a broader range of machine learning settings (including regression and feature selection), and methods for incorporating new kinds of data (such as fMRI brain scans) for guiding machine learning algorithms.  This research represents a new direction in machine learning research, which increasingly has important and broad impact in our modern, data-driven world.  In addition, it is anticipated that the theoretical gains in machine learning derived from this work will feed back into psychology, enabling rapid screening of candidate hypotheses about how the brain works by artificial systems which can then be tested on humans using an advanced crowdsourcing platform for quantifying human behavior."
197,1350078,CAREER: Supervised Learning for Incomplete and Uncertain Data,IIS,Info Integration & Informatics,5/1/14,5/11/16,Alina Zare,MO,University of Missouri-Columbia,Continuing grant,Sylvia Spengler,4/30/17,"$297,011.00 ",,azare@ece.ufl.edu,115 Business Loop 70 W,COLUMBIA,MO,652110001,5738827560,CSE,7364,"1045, 7364, 9150, 9251",$0.00 ,"This CAREER project will advance the state of the art in supervised machine learning to allow for incomplete, uncertain and unspecific label information.  Supervised machine learning algorithms produce desired outputs for given input data by learning from example training data.  The methods generally rely on completely and accurately labeled training data to drive the learning algorithm. However, many applications are plagued with labels that are incomplete, uncertain, and unspecific (lack precision).  Current techniques do not adequately handle such data.<br/><br/>For example, analysis of satellite imagery to identify the content of each pixel is often conducted by coupling unsupervised learning methods (that do not rely on labeled training data) with manual exploration.  This is time-consuming, error-prone, and expensive.  Imagine, instead, easy-to-use tools that could understand the content of each pixel in satellite imagery.  Extremely large amounts of road map data (for example from Google Maps or OpenStreetMap) and social media information (for example geo-tagged photographs, video clips, and social networking posts) are continually collected and stored.  These data could be used as sparsely-labeled training data (with varying degrees of specificity and uncertainty) to guide understanding of satellite imagery.<br/><br/>Although the data is available, algorithms have yet to be developed to combine these data sources and identify the content of pixels in satellite images.  This work will advance this and other potential applications of machine learning where incomplete, uncertain and unspecific labels in training data challenge the development of effective machine learning algorithms.  <br/><br/>This CAREER project will achieve these advances through the following research objectives:<br/>(1) Investigate and develop a mathematical framework and associated algorithms for Multiple Instance Function Learning that addresses linear and non-linear classification and regression problems with varying levels and types of sparsity, uncertainty, and specificity in training labels.<br/><br/>(2) Study and apply the proposed framework and algorithms towards the fusion of satellite imagery, road map data and social media for global scene understanding. <br/><br/>This research will be conducted in conjunction with integrated education and outreach activities.  In particular, an interactive web application will be developed to provide an avenue for introducing concepts from machine learning and remote sensing to the public for dissemination and outreach. This interactive web application will also be used, along with additional hands-on activities, to introduce high school students to machine learning and remote sensing concepts during an annual summer engineering camp held at the University of Missouri in Columbia, MO.  Paired with the web application will be a research website in which data, code, publications and presentations will be shared with the research community. Furthermore, undergraduate and graduate research assistants will be trained in the areas of machine learning and remote sensing. Finally, relevant research topics will be introduced in the PI's undergraduate and graduate courses."
198,1409802,CSR:  Medium:  Distributed Inference Algorithms for Machine Learning and Optimization,IIS,"CSR-Computer Systems Research, Info Integration & Informatics",9/1/14,8/25/17,Alexander Smola,PA,Carnegie-Mellon University,Continuing grant,Aidong Zhang,8/31/18,"$1,200,000.00 ","David Andersen, Suvrit Sra",smola@cmu.edu,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,CSE,"7354, 7364","7354, 7364, 7924",$0.00 ,"Machine learning systems are operating at increasing scale in ways that benefit nearly all areas of human activity, from improved voice recognition, search and advertising, automatic language translation, and on the horizon, to activities such as self-driving cars. It is already extremely hard to implement on large, scalable clusters of computers the ""inference algorithms"" that enable these systems, and future trends of computer data centers will further exacerbate this difficulty: increasingly large numbers of nodes, heterogeneous clusters that mix conventional microprocessors, graphics processors, larger numbers of small and power-efficient microprocessors, and hardware changes such as the introduction of flash-based solid-state disks. The goal of this proposal is to design, analyse, and implement novel inference algorithms that not only take advantage of these trends for high performance , but that also enable future, even-larger-scale systems to be implemented.<br/><br/>    The proposal specifically aims to achieve the following:<br/><br/>1. Develop a broad family of novel optimization algorithms for machine learning;<br/>2. Analyse their convergence properties theoretically, as well as empirically;<br/>3. Release open-source code implementing them.<br/><br/>    The research proposed is based upon four likely shifts in the design of data centers of the future:<br/><br/>1. Small and power efficient microprocessors with a much improved CPU power to energy consumption ratio will become common in the data centers of the future.<br/>2. Architectures mixing different types of hardware, ranging from computer graphics processors to general purpose multi-core microprocessors are becoming the norm among all major semiconductor manufacturers. These changes will propagate to the data center.<br/>3. Hard disks are increasingly being supplemented and replaced by solid state memory which requires 10,000 to 100,000 times less time to access.<br/>4. Modern network architectures that replace traditional hierarchical tree structures (with inherent bottlenecks) by more balanced layouts are being enabled by software-defined networking and specialized network chips.<br/><br/>    All four of these aspects offer considerable potential to design faster machine learning algorithms. Doing so requires tightly coupled algorithmic and systems design that successfully creates algorithms that work well on the kinds of systems that can be built, and systems to be built that provide the right support for machine learning algorithms.<br/><br/>    The software developed for this project will be distributed as open source.<br/><br/>For further information see the project web site at:  http://www.parameterserver.org"
199,1444285,31st Annual Conference on Machine Learning (ICML 2014),IIS,"Information Technology Researc, Info Integration & Informatics, Robust Intelligence",5/15/14,5/14/14,Artur Dubrawski,PA,Carnegie-Mellon University,Standard Grant,Todd Leen,4/30/15,"$35,000.00 ",,awd@cs.cmu.edu,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,CSE,"1640, 7364, 7495",7556,$0.00 ,"This grant enables student participation at the International Conference on Machine Learning (ICML).  ICML is one of the premier conferences in Machine Learning.  In addition to rigorously refereed conference papers, it features poster sessions, tutorials, workshops and invited talks. This award provides funds for travel scholarships for students from US institutions to help cover their travel and costs of attending the conference. The selection of students to be funded is based on a review by the selection committee of their authorship and financial needs. The student scholarships are very important for encouraging student participation in this premier conference and for shaping the future of the field as a whole. Special attention will be paid to broadening the participation of students from groups that are traditionally underrepresented in Computer Science in general and Machine Learning research in particular, and students from institutions with limited Machine Learning expertise who would benefit from the opportunity to interact with researchers from around the world. <br/><br/>Attendance at the ICML represents a unique opportunity for students interested in Machine Learning research. The poster sessions at the conference are designed enable students to recieve feedback from leading Machine Learning researchers from around the world and to help them become part of the larger Machine Learning research community. The award contributes to the education and training of the next generation of researchers and educators in an increasingly important area. It also helps broaden the participation of underrepresented groups and women in Machine Learning research."
200,1451453,EAGER: Machine Learning to Combat Adversarial Attacks,IIS,Robust Intelligence,9/1/14,8/20/14,Daniel Lowd,OR,University of Oregon Eugene,Standard Grant,Weng-keen Wong,8/31/16,"$104,997.00 ",,lowd@cs.uoregon.edu,5219 UNIVERSITY OF OREGON,Eugene,OR,974035219,5413465131,CSE,7495,"7495, 7916",$0.00 ,"In domains such as spam and fraud, adversaries actively modify their behavior to avoid being detected by a system constructed to identify attacks. For example, spammers add and remove words from their email messages in order to bypass filters, and web spammers try to deceive search engines by creating ""link farms"" to make a web site seem important. This project analyzes how adversaries evade classifiers and develops new algorithms, based on a novel combination of machine learning and the theory of designing actions in the face of adversaries, that are more robust. In particular, it will focus on classifiers that use data compression statistics and graph structure to make predictions. These classifiers are popular and effective in a growing number of domains, but also challenging to analyze. The analyses and algorithms developed in this project will be useful for building machine learning systems more accurate in the face of evasive adversaries. This will improve our ability to fight web spam, social network spam, online auction fraud, credit card fraud, and more. Given the high worldwide cost of cybercrime, even a small increase in accuracy could save millions of dollars per year.<br/><br/>Most previous work on modeling adversaries in machine learning has been limited to linear classifiers with features that can be manipulated independently. These results tell us little about the vulnerabilities of widely used non-linear classifiers. Such models also ignore the relational structure necessary to identify social network spam, comment spam, fake reviews, and more. This project takes the first steps towards a much broader understanding of the weaknesses present in machine learning methods, and how best to eliminate them. Specifically, the project explores how an intelligent adversary can evade non-linear compression-based classifiers in the face of features that are highly interdependent. System designers can use these results to understand possible system vulnerabilities and intelligently choose classifiers that are less vulnerable. To learn more robust models, this project integrates a model of the adversary into learning algorithms, optimizing its performance against the adversary's best response. The work focuses on structured prediction, which can model relationships among objects, such as users in a social network, and complex adversarial actions, such as creating new accounts or buying followers. These new methods are evaluated on synthetic and real-world adversarial domains, including Twitter spam."
201,1447449,BIGDATA: F: DKA: CSD: Human and Machine Co-Processing,IIS,Big Data Science &Engineering,9/1/14,11/9/18,Robert Nowak,WI,University of Wisconsin-Madison,Standard Grant,Maria Zemankova,8/31/19,"$1,396,830.00 ","Stephen Wright, Rebecca Willett",rdnowak@wisc.edu,21 North Park Street,MADISON,WI,537151218,6082623822,CSE,8083,"7433, 8083",$0.00 ,"Human experts are crucial to data analysis. Their roles include sifting through large datasets to facilitate search, retrieval, and machine learning. Humans often perform much better than machines at such tasks, but the speed and capacity of human experts is a limiting factor in the human-machine co-processing.  This project is addressing two aspects of human-machine co-processing:  winnowing Big Data to produce manageable subsets for human expert analysis, and machine learning algorithms that learn efficiently from human experts with a minimal amount of human interaction.  This has a wide range of applications; to ensure broad applicability of the results the project is evaluating the techniques in multiple domains: cognitive science, large-scale astronomical data analysis, and experimental design in materials science.<br/><br/>The approach used for data winnowing is based on developing predictive models and identifying data that does not fit the models.  A key research challenge is non-stationary environments:  the underlying model changes over time.  Preliminary work shows promise on selection from a finite set of models, and new work investigates more flexible parametric models. The active learning task uses the multi-armed bandit problem to model identify which features have the greatest impact on human decisions.  This task also investigates learning from comparisons/rankings rather than predictions; conjecturing that there may exist low-dimensional structure governing human reasoning and decision-making that enables learning with significantly fewer comparisons than might otherwise be required.  A common theme in both tasks is ensuring computational complexity is low enough to facilitate real-time interactions with human experts in spite of the volume of data. This is achieved using bounded approximations and convex relaxations of the optimization programs used to guide the interaction."
202,1350965,CAREER: Exact Algorithms for Learning Latent Structure,IIS,Robust Intelligence,7/1/14,6/20/14,David Sontag,NY,New York University,Standard Grant,Weng-keen Wong,8/31/17,"$499,981.00 ",,dsontag@mit.edu,70 WASHINGTON SQUARE S,NEW YORK,NY,100121019,2129982121,CSE,7495,"1045, 7495",$0.00 ,"One of the fundamental tasks in science is to infer the causal relationships between variables from data, and to discover hidden phenomena that may affect their outcome. We can attempt to automate this scientific process by searching over probabilistic models of how the observed data might be influenced by unobserved (latent) factors or variables. Machine learning of such models provides insight into the underlying domain and a means of predicting the latent factors. However, it is challenging to search over the exponentially many models, and existing algorithms are unable to scale to large amounts of data.<br/><br/>The goal of this CAREER award will provide novel algorithms to circumvent this computational intractability. Based on a classical idea in statistics called the method-of-moments, the new algorithms will be applied in bioinformatics to discover regulatory modules from disease expression profiles, and in health care to predict a patient's clinical state using data from their electronic medical record. A key component of the project is to involve high school students from disadvantaged backgrounds in the research to inspire them to pursue STEM careers.<br/><br/>The project advances machine learning by introducing several new techniques for unsupervised and semi-supervised learning of Bayesian networks. The project overcomes the computational challenges associated with maximum-likelihood estimation by developing new method-of-moment based algorithms for learning latent variable models, focusing on settings where inference itself may be intractable. This includes Bayesian networks of discrete variables where a top layer consists of latent factors and a bottom layer consists of the observed data, a form of discrete factor analysis. The proposed algorithms run in polynomial time and are guaranteed to learn a close approximation to the true model.<br/><br/>The techniques developed as part of this project have the potential to be transformative in the social and natural sciences by enabling the efficient and accurate discovery of latent variables from discrete data. Furthermore, in collaboration with emergency department clinicians, the new algorithms will be applied to learn models relating diseases to symptoms from noisy and incomplete data that is routinely collected as part of electronic medical records. This will advance the field of machine learning in health care by providing algorithms that generalize between institutions without the need for a large amount of labeled training data.<br/><br/>The insights about exploratory data analysis developed as part of this project will be integrated into innovative curriculum in data science, both as part of an undergraduate class and new Master's classes. The project will bring students from nearby high schools to NYU throughout the academic year and during the summer to learn about machine learning through participation in the proposed research, having them use the unsupervised learning algorithms to discover new medical insights. The PI will also develop and deliver tutorials on machine learning to clinicians and the health care industry."
203,1421094,RI: Small: Collaborative Research: MatCam: A Camera that Sees Materials,IIS,Robust Intelligence,9/1/14,3/9/15,Ko Nishino,PA,Drexel University,Standard Grant,Jie Yang,8/31/18,"$257,886.00 ",,kon@drexel.edu,"1505 Race St, 10th Floor",Philadelphia,PA,191021119,2158955849,CSE,7495,"7495, 7923, 9251",$0.00 ,"This project develops the first material camera, or MatCam, that outputs a per-pixel label of object material and its properties that can be used in visual computing tasks. In the everyday real world there are a vast number of materials that are useful to discern including concrete, metal, plastic, velvet, satin, water layer on asphalt, carpet, tile, wood, and marble.  A device for identifying materials has important implications in developing new technologies. For example, a mobile robot may use a MatCam to determine whether the terrain is grass, gravel, pavement, or snow in order to optimize mechanical control.  In e-commerce, the material composition of objects can be tagged by a MatCam for advertising and inventory.  The potential applications are limitless in areas such as robotics, digital architecture, human-computer interaction, intelligent vehicles and advanced manufacturing. Furthermore, material maps have foundational importance in nearly all vision algorithms including segmentation, feature matching, scene recognition, image-based rendering, context-based search, and object recognition and motion estimation. The camera brings material recognition to the broader scientific and engineering communities, in a similar way that depth cameras are currently used in many fields outside of computer vision. <br/><br/>This research brings high accuracy material estimation out of the lab and into the real-world for fast high-accuracy per-pixel material estimates.  The program has three technical aims.  First, a material appearance database is captured and stored with an exploration robot viewing surfaces from multiple angles.   This large, structured and actionable visual dataset is then used to develop computational appearance models.  A novel methodology using angular reflectance gradients is integrated for characterizing features of surface appearance.  Using the training data and statistical inference methods, these models are designed for hardware implementation. The final aim is the material camera implementation as a near real-time prototype of point-and-shoot material acquisition that extends RGB-D cameras to RGB-DM cameras that provide color, depth, and material.  The hardware implementation of the material appearance models utilizes FPGA and SoC (system-on-chip) technology."
204,1421134,RI: Small: Collaborative Research: MatCam: A Camera that Sees Materials,IIS,Robust Intelligence,9/1/14,8/29/14,Kristin Dana,NJ,Rutgers University New Brunswick,Standard Grant,Jie Yang,8/31/18,"$250,000.00 ",,kristin.dana@rutgers.edu,33 Knightsbridge Road,Piscataway,NJ,88543925,8489320150,CSE,7495,"7495, 7923",$0.00 ,"This project develops the first material camera, or MatCam, that outputs a per-pixel label of object material and its properties that can be used in visual computing tasks. In the everyday real world there are a vast number of materials that are useful to discern including concrete, metal, plastic, velvet, satin, water layer on asphalt, carpet, tile, wood, and marble.  A device for identifying materials has important implications in developing new technologies. For example, a mobile robot may use a MatCam to determine whether the terrain is grass, gravel, pavement, or snow in order to optimize mechanical control.  In e-commerce, the material composition of objects can be tagged by a MatCam for advertising and inventory.  The potential applications are limitless in areas such as robotics, digital architecture, human-computer interaction, intelligent vehicles and advanced manufacturing. Furthermore, material maps have foundational importance in nearly all vision algorithms including segmentation, feature matching, scene recognition, image-based rendering, context-based search, and object recognition and motion estimation. The camera brings material recognition to the broader scientific and engineering communities, in a similar way that depth cameras are currently used in many fields outside of computer vision. <br/><br/>This research brings high accuracy material estimation out of the lab and into the real-world for fast high-accuracy per-pixel material estimates.  The program has three technical aims.  First, a material appearance database is captured and stored with an exploration robot viewing surfaces from multiple angles.   This large, structured and actionable visual dataset is then used to develop computational appearance models.  A novel methodology using angular reflectance gradients is integrated for characterizing features of surface appearance.  Using the training data and statistical inference methods, these models are designed for hardware implementation. The final aim is the material camera implementation as a near real-time prototype of point-and-shoot material acquisition that extends RGB-D cameras to RGB-DM cameras that provide color, depth, and material.  The hardware implementation of the material appearance models utilizes FPGA and SoC (system-on-chip) technology."
205,1509625,RAPID: Automatic and Non-intrusive Screening for Potential Viral Disease Carriers,IIS,"INFORMATION TECHNOLOGY RESEARC, ROBUST INTELLIGENCE",12/1/14,12/5/14,Guodong Guo,WV,West Virginia University Research Corporation,Standard Grant,Jie Yang,11/30/16,"$87,847.00 ",,Guodong.Guo@mail.wvu.edu,P.O. Box 6845,Morgantown,WV,265066845,3042933998,CSE,"1640, 7495","001Z, 1640, 7495, 7914, 9150",$0.00 ,"Viral disease outbreaks such as Ebola epidemic can be a serious concern for the general public. Many these viral diseases cause a temperature increase in human body and can be visually detected from infrared images. This project develops an intelligent system to automatically detect human body temperature for non-intrusively screening potential viral disease carriers from a crowd.  The system is built on recent advances in computer vision technologies and thermal infrared sensors. <br/><br/>This project addresses an important and interesting application of computer vision. The research leads to development of a prototype system that can be deployed in a public pace to automatically measure human body temperatures from facial images. The research work includes discovering how to localize the facial features accurately and precisely from both visible and infrared images for the purpose of accurate measure of body temperatures and reducing false alarms in screening of potential viral disease carriers. The key approach utilizes both visible light and infrared spectra to achieve a robust solution. It can also process multiple people in a crowd non-intrusively and speed up the screening process. The research in this project can address the critical need for screening of viral disease carriers, and advance the study on machine vision algorithms applicable to an important and practical problem related to public health."
206,1351049,"CAREER: Microscopy Image Analysis to Aid Biological Discovery: Optics, Algorithms, and Community",IIS,"Robust Intelligence, EPSCoR Co-Funding",5/1/14,1/29/20,Zhaozheng Yin,MO,Missouri University of Science and Technology,Standard Grant,Jie Yang,3/31/20,"$369,867.00 ",,zhaozheng.yin@stonybrook.edu,300 W 12th Street,Rolla,MO,654096506,5733414134,CSE,"7495, 9150","1045, 7495, 9150, 9251",$0.00 ,"This project develops image analysis algorithms and systems to process microscopy images that record the proliferation history of biological specimens and evaluate their behaviors that respond to different culturing conditions, therefore, deciphering complex biological processes and accelerating the advance of biological discovery. The research combines techniques of physical optics, computer vision and crowdsourcing to bring a breakthrough to microscopy imaging and microscopy image analysis. The developments of such technologies transform the image-based biology research from subjective to a rigorous, quantitative, and efficient manner. The research team also seeks to promote interdisciplinary collaboration between biological imaging and computer vision, integrate the research outcomes into education activities, and disseminate the project to a wide audience via web, K-12 group, conferences and industry collaborations.<br/><br/>Previous microscopy image analysis methods do not consider the particular image formation process and treat them in the same manner as general natural images, causing many difficulties or failures in the image analysis. This project addresses the challenges in a principally different way by investigating the theoretical foundation of microscopy optics. The computational imaging models of microscopes are derived and used to restore artifact-free images and extract optics-oriented image features, which makes the automated image analysis fundamentally correct and easy. The models are further used to enhance the microscope's functionalities including calibration and virtual microscopy. A cyber-enabled research community is being established within which active learning and crowd-computing are leveraged to improve the algorithm performance and biological discovery.<br/><br/>Updates are available from http://web.mst.edu/~yinz/."
207,1405847,II-New: Seeing the Future: Ubiquitous Computing in EyeGlasses,CNS,CCRI-CISE Cmnty Rsrch Infrstrc,8/1/14,12/12/19,Jan-Michael Frahm,NC,University of North Carolina at Chapel Hill,Standard Grant,Jie Yang,7/31/20,"$613,623.00 ","Henry Fuchs, Tamara Berg, Jay Aikat, Cynthia Sturton",jmf@cs.unc.edu,104 AIRPORT DR STE 2200,CHAPEL HILL,NC,275991350,9199663411,CSE,7359,7359,$0.00 ,"This project is building a prototype ecosystem in which a collection of people can interact in an environment with the envisioned personal augmented reality (AR) device of the future research. The provided hardware serves as a platform for understanding and building of the ecosystem for personal AR devices that a user can ask ""Where did I put my keys?"" and get an instantaneous response from his/her eyeglasses. The envisioned system can also remind people to take medicine, create a collaborative reconstruction of an environment incorporating the perspectives of multiple people, or an app that enables one to shop for any item seen while walking down the street. All of these applications and more could be enabled by personal ubiquitous hands-free visual devices that have total awareness of the environment. <br/><br/>This project provides hardware to enable research in developing technologies for personal AR systems that are unencumbered, ubiquitous, and can be used hands-free, with a wide field-of-view display overlaid to the user's view of the surroundings. Such devices would enable the development of societally and commercially beneficial applications such as assistance tools for physicians. The prototyped hardware and software components are supporting the research to reach these goals. The collaborative projects enabled by the prototypes are also studying unique research questions, including enhancing everyday life activities, health care management and training, as well as computer science research questions such as distributed interactive computer vision, distributed computing, energy/power management, security, and privacy. Broader impacts include developing applications for the disabled, tools for healthcare management, and elderly assistance."
208,1418737,Collaborative Research: Wavelet Frames for Variational Models in Imaging: Bridging Discrete and Continuum,DMS,COMPUTATIONAL MATHEMATICS,8/1/14,8/2/16,Weiyu Xu,IA,University of Iowa,Continuing Grant,Leland Jameson,7/31/19,"$129,167.00 ",,weiyu-xu@uiowa.edu,2 GILMORE HALL,IOWA CITY,IA,522421320,3193352123,MPS,1271,"9150, 9263",$0.00 ,"From the beginning of science, visual observations have been playing important roles. Advances in computer technology have made it possible to apply some of the most sophisticated developments in mathematics and the sciences to the design and implementation of fast algorithms running on a large number of processors to process image data. As a result, image processing and analysis techniques are now applied to virtually all natural sciences and technical disciplines ranging from computer sciences and electronic engineering to biology and medical sciences; and digital images have come into everyone's life. Mathematics has been playing an important role in image and signal processing from the very beginning. There are two major mathematical approaches for image restoration, namely, wavelet tight frame approaches and differential/variational approaches. The main research objective of this project is to investigate geometric aspects of the former approach by connecting it with the latter. It will give rise to new mathematical models and numerical algorithms that benefit researchers in academia, national research laboratories, as well as in industry. The understandings of the geometric aspects of the wavelet frames and the connections with differential operators will contribute to both the community of computational harmonic analysis and the community of variational techniques and numerical PDEs. The education plan will bring undergraduate and graduate students to the frontiers of research in computational mathematics, computer vision and medical imaging; and strengthen the collaborations among mathematicians, engineers, computer scientists and medical doctors.<br/><br/>Wavelet frames are systems of functions that provide linear representations of functions living in certain function spaces such as L2(Rn). In contrast to the classic (bi)orthogonal wavelet bases, such representations are generally redundant which is desirable in many applications. Although most theoretical aspects of wavelet frames have already been well understood in the literature, geometric meanings of the wavelet frame transform are still generally unknown. In fact, the lack of geometric interpretations is one of the major flaws of wavelet frames that prohibits the applications of wavelet frames in some important problems of data analysis that require geometric regularization of the objects-of-interest reside in the data. The main research objective of this proposal is to develop a generic geometric interpretation to the wavelet frame transform, by studying its relations with differential operators within various variational frameworks. Based on the geometric interpretation, we propose new models and algorithms for several important applications such image restoration (deblurring, inpainting, CT/MR imaging, etc.). Through both theoretical analysis and numerical experiments, we will explore the advantages of the proposed wavelet frame based models over the existing variational and differential models for different applications. The proposed research will focus on: (1) the approximation of the differential operators by the wavelet frame transform within general variational frameworks; (2) solving large-scaled ill-posed inverse problems (e.g., image restoration, blind deconvolution) through convex/nonconvex optimizations using wavelet frames; (3) designing and solving wavelet frame based models in real-world applications in imaging such as low-dose CT image reconstruction, removing blurs caused by camera shaking, etc. The study of the geometric meanings of the wavelet frame transform will interpret wavelet frames and their associated optimization models from a whole new angle. Such fundamental study enables us, for the very first time, to fully utilize the unique properties of wavelet frames in geometry-involved data analysis tasks and finding numerical solutions of PDEs. The practical advantages (such as the quality of restoration for inverse problems) of wavelet frame transform over standard finite difference approximations in various applications will become more evident after the proposed studies. Furthermore, this project will also bring new understandings to numerical methods solving variational models; and answers some fundamental and important questions of variational models that are unclear from the literature."
209,1463960,NRI-Small: Context-Driven Haptic Inquiry of Objects Based on Task Requirements for Artificial Grasp and Manipulation,CBET,National Robotics Initiative,7/1/14,10/14/14,Veronica Santos,CA,University of California-Los Angeles,Standard Grant,Christina Payne,9/30/18,"$454,632.00 ",,vjsantos@ucla.edu,10889 Wilshire Boulevard,LOS ANGELES,CA,900951406,3107940102,ENG,8013,"7923, 8086",$0.00 ,"PI: Santos, Veronica<br/>Proposal Number: 1208519<br/><br/>Intellectual Merit: Human-like dexterous manipulation is featured prominently as a grand challenge in the 2009 Roadmap for U.S. Robotics' report. Human dexterity relies heavily on tactile sensation and is influenced by proprioceptive and visual feedback. The proposed work aims to advance artificial manipulators by integrating a new class of multimodal tactile sensors with anthropomorphic artificial hands and developing generalizable routines for context-driven haptic inquiry of objects based on task requirements for artificial grasp and manipulation. A primary goal is the development of capabilities for a robot hand to efficiently learn about objects in its unstructured environment through touch, specifically for cases where computer vision would fail to provide critical information about the physical hand-object interactions. While computer vision provides preliminary information about an object and its environment, vision alone cannot provide all essential information necessary for successful physical hand-object interactions. This is especially true when digits are occluded by the grasped object, and when the hand-object interaction is completely out of view. Inspiration for the haptic inquiry framework will be drawn from a suite of human haptic exploration procedures. In contrast to haptic exploration, haptic inquiry will require that the order and time spent on each exploratory procedure depend on task goals. The order and type of questions to be asked haptically will be context-dependent and designed to yield high-level, task-directed information at a low cost of inquiry. The weight given to each mode of tactile sensing (force, vibration, temperature) will also be tuned according to the context of the task.<br/>This proposal aims to strengthen the robustness of co-robot systems by developing a framework for context-driven, task-directed haptic inquiry that integrates multi-digit tactile and proprioception data in a task-appropriate manner. The framework will be developed and deployed on an anthropomorphic robot hand outfitted with a new class of commercially-available multimodal tactile sensors. The work is transformative because it will enable co-robot systems to remain functional even in the absence of visual feedback, which is typically the primary form of feedback for robotic systems. The long-term research objective of this proposal is to reduce the cognitive burden on the user of an artificial manipulator. <br/><br/>Broader Impacts: The proposed translational research could enhance the functional capabilities of co-robot systems in which humans use artificial manipulators to work in unstructured, unsafe, or limited access environments (prosthetic, rehabilitative, assistive, space, underwater, military, rescue, surgery).  The proposed work could benefit the human user of a co-robot system by empowering the robot with the ability to control low-level perception-action loops autonomously without burdening the human. The ROS operating system may be used to simulate and control an anthropomorphic robot hand outfitted with commercially-available tactile sensors using commercially-available actuators. Custom source code (C, MATLAB, ROS) and an open source haptic library for a commercially-available tactile sensor (suitable for data mining) will be made publicly available for the benefit and advancement of the robotics community."
210,1358939,REU SITE: Multidisciplinary Research Experiences for Undergraduates in the Internet of Things,CNS,"RSCH EXPER FOR UNDERGRAD SITES, RES EXP FOR TEACHERS(RET)-SITE, CPS-Cyber-Physical Systems",3/1/14,3/28/16,Anne Ngu,TX,Texas State University - San Marcos,Continuing Grant,Harriet Taylor,2/28/19,"$343,481.00 ",Byron Gao,hn12@txstate.edu,601 University Drive,San Marcos,TX,786664616,5122452314,CSE,"1139, 1359, 7918","1359, 9250",$0.00 ,"This funding renews a Research Experience for Undergraduates (REU) Site at Texas State University - San Marcos. Undergraduate students will engage in summer research involving a variety of interesting and challenging problems associated with the Internet of Things.   Internet of Things represents a vision for the next generation of the Internet where everyday physical objects can be attached with sensors and seamlessly integrated with the Internet.  This REU Site provides opportunities for a cohort of undergraduate students to work during the summer on research and development of innovative Internet of Things technologies that could have significant societal impacts.   The site emphasizes the participation of a diverse group of students, in particular women, minorities, and non-traditional students. This project is co-funded by the Cyber-Physical Systems program.<br/><br/>The intellectual merit of the project rests with the project leadership, an experienced research group with excellent expertise and experience in the research area.  All of the student projects have a strong research basis derived from the synergistic research expertise of the five core faculty members from the Computer Science and Engineering departments.  The projects will advance the state-of-the-art in big data analysis and management, information retrieval, computer vision, and software engineering in the context of the Internet of Things.<br/><br/>The broader impacts include providing a unique opportunity for undergraduate students with limited research opportunities to experience research in the interdisciplinary theme of Internet of Things.   The participating faculty members are committed to including under-represented and non-traditional students in their research and have an established recruitment network to reach these students.  Thus this project has the potential to produce new computer science graduate students and faculty members and to advance discovery and understanding while promoting learning."
211,1422441,CHS: Small: Generative models of shapes,IIS,HCC-Human-Centered Computing,7/1/14,6/26/14,Evangelos Kalogerakis,MA,University of Massachusetts Amherst,Standard Grant,Ephraim Glinert,6/30/18,"$499,997.00 ",,kalo@cs.umass.edu,Research Administration Building,Hadley,MA,10359450,4135450698,CSE,7367,"7367, 7453, 7923",$0.00 ,"This research will advance the state of the art in three-dimensional (3D) shape synthesis by developing generative probabilistic models that will enable the automatic understanding of semantics from shape geometry, and which will lead to the development of new computational modeling algorithms that allow anybody to easily create compelling and highly detailed 3D content.  Users will be able to create shapes by simply providing high-level specifications, shape types, parts, semantic shape attributes, landmark points, or sketches based on simple and intuitive user interfaces.  These models will also enable the computer to infer complete geometry from partial geometric data acquired by range cameras and to fill in any missing shape parts, or to robustly recognize objects in a scene acquired by 3D sensors.  Project outcomes will advance the state of the art in 3D modeling, providing users with intuitive tools that significantly lower the barrier of rapid and easy creation of detailed shapes.  Such tools are becoming increasingly important, since there is a growing interest in 3D models in scientific and engineering fields such as collaborative virtual environments, augmented reality, simulation, computer-aided design, and architecture.  In particular, this work will significantly benefit 3D printing; where despite hardware advances, the main bottleneck remains the creation of shapes to be supplied to the printer.  The research will also advance the state of the art in shape understanding and object recognition, which are important for computer vision and robotics applications.<br/><br/>The key idea behind these generative models is that they represent complex hierarchical compositions, correlations and variations of detailed geometric shape features, as well as their relationships with high-level semantic shape attributes.  The models will be automatically learned from large shape repositories available on the Web, after the input shapes are pre-processed by new algorithms the Principal Investigator will develop for simultaneous shape segmentation and landmark localization so that their parts and points are consistently labeled.  Existing shape synthesis algorithms are limited to re-use and re-combine shape parts from a repository, or synthesize shapes in specific classes (such as human bodies or faces), with limited geometric variability and no structural or semantic variability.  The Principal Investigator's generative models, on the other hand, will instead learn how to densely place points and patches to create new plausible shapes in complex domains, such as furniture, vehicles, tools, creatures, etc.  Inference algorithms built upon the generative models will be able to synthesize shapes given linguistic terms or sparse geometric input.  As a result, the research will lead to the development of new 3D content creation tools that will transform the field of computational modeling: instead of executing a series of painstaking low-level geometric editing and manipulation commands, users will perform simple, easy, and intuitive interactions to achieve their design goals."
212,1443054,CIF21 DIBBs: Middleware and High Performance Analytics Libraries for Scalable Data Science,OAC,"Tribal College & Univers Prog, EDUCATION AND WORKFORCE, Data Cyberinfrastructure",10/1/14,6/2/20,Geoffrey Fox,IN,Indiana University,Standard Grant,Amy Walton,9/30/20,"$5,283,170.00 ","Madhav Marathe, Shantenu Jha, Judy Qiu, Fusheng Wang",gcf@indiana.edu,509 E 3RD ST,Bloomington,IN,474013654,3172783473,CSE,"1744, 7361, 7726","7433, 8048, 9251",$0.00 ,"Many scientific problems depend on the ability to analyze and compute on large amounts of data.  This analysis often does not scale well; its effectiveness is hampered by the increasing volume, variety and rate of change (velocity) of big data.  This project will design, develop and implement building blocks that enable a fundamental improvement in the ability to support data intensive analysis on a broad range of cyberinfrastructure, including that supported by NSF for the scientific community. The project will integrate features of traditional high-performance computing, such as scientific libraries, communication and resource management middleware, with the rich set of capabilities found in the commercial Big Data ecosystem. The latter includes many important software systems such as Hadoop, available from the Apache open source community.  A collaboration between university teams at Arizona, Emory, Indiana (lead), Kansas, Rutgers, Virginia Tech, and Utah provides the broad expertise needed to design and successfully execute the project.  The project will engage scientists and educators with annual workshops and activities at discipline-specific meetings, both to gather requirements for and feedback on its software.  It will include under-represented communities with summer experiences, and will develop curriculum modules that include demonstrations built as 'Data Analytics as a Service.'<br/><br/>The project will design and implement a software Middleware for Data-Intensive Analytics and Science (MIDAS) that will enable scalable applications with the performance of HPC (High Performance Computing) and the rich functionality of the commodity Apache Big Data Stack.  Further, this project will design and implement a set of cross-cutting high-performance data-analysis libraries; SPIDAL (Scalable Parallel Interoperable Data Analytics Library) will support new programming and execution models for data-intensive analysis in a wide range of science and engineering applications.   The project addresses major data challenges in seven different communities: Biomolecular Simulations, Network and Computational Social Science, Epidemiology, Computer Vision, Spatial Geographical Information Systems, Remote Sensing for Polar Science, and Pathology Informatics.  The project libraries will have the same beneficial impact on data analytics that scientific libraries such as PETSc, MPI and ScaLAPACK have had for supercomputer simulations.  These libraries will be implemented to be scalable and interoperable across a range of computing systems including clouds, clusters and supercomputers."
213,1519890,CAREER:  New Directions in Spatial Statistics,DMS,"STATISTICS, Division Co-Funding: CAREER",9/1/14,7/3/17,Debashis Mondal,OR,Oregon State University,Continuing Grant,Gabor Szekely,6/30/19,"$351,845.00 ",,debashis.mondal@oregonstate.edu,OREGON STATE UNIVERSITY,Corvallis,OR,973318507,5417374933,MPS,"1269, 8048","1045, 1187",$0.00 ,"The  de Wijs process (also known as the  Gaussian free field in statistical physics) is a fundamental spatial process that arises as the scaling limit of lattice based Gaussian Markov random fields and generalizes Brownian motion in two-dimensions. However, at present,  there is a wide gap between the theory of Gaussian free field (including the subsequent theory of random fields) in statistical physics and modern probability, and the current practice of spatial statistics via lattice based Gaussian Markov random fields. Thus, there is great need to bridge this gap to develop a principled framework for statistics and inference of spatial models  and to pursue novel computations that make such inferences feasible.  This  project will consider formulating appropriate functionals of the de Wijs process to construct useful random fields and novel matrix-free computations via conjugate gradient and other methods, and will focus on developing new areas of scientific applications. The proposed research will also shed new light on and allow deeper understanding of theoretical and computational issues discussed by many researchers in spatial statistics in the past decades.  Novel matrix-free computations  will provide further impetus to study parametric bootstrap methods and multi-scale modeling, and to construct a new class of non-Gaussian random fields.  The project will contribute  to obtaining enhanced scientific understanding in studies of environmental bioassays, arsenic contamination of groundwater and distributions of galaxies. <br/><br/>Advances in the field of spatial statistics are important because new statistical methods can be applied to a wide range of scientific questions in fields such as astronomy, agriculture, biomedical imaging, computer vision, climate and environmental studies, epidemiology and geology. The de Wijs process is one fundamental spatial process that generalizes Brownian motion from time to space.  Using the de Wijs process as a fundamental building block, this project will develop novel mathematics and derive fast, efficient and large-scale statistical computations so that various scientific questions can be answered in a practical way. This will lead to new developments  for the analysis of continuum spatial data  and spatial point patterns, and will allow us to obtain enhanced scientific understanding in studies of environmental bioassays, arsenic contamination of groundwater and distributions of galaxies.  The statistics and the computations that will be developed in this project will also be particularly relevant for various research problems that arise in environmental or global change, and in health studies. Finally, this project will integrate research and educational activities through the development of new graduate and undergraduate courses and will also provide valuable training and learning opportunities for students at graduate and undergraduate levels."
214,1418772,Collaborative Research: Wavelet Frames for Variational Models in Imaging: Bridging Discrete and Continuum,DMS,COMPUTATIONAL MATHEMATICS,8/1/14,8/12/16,Bin Dong,AZ,University of Arizona,Continuing Grant,Leland Jameson,7/31/17,"$170,832.00 ",Leonid Kunyansky,dongbin@math.arizona.edu,888 N Euclid Ave,Tucson,AZ,857194824,5206266000,MPS,1271,9263,$0.00 ,"From the beginning of science, visual observations have been playing important roles. Advances in computer technology have made it possible to apply some of the most sophisticated developments in mathematics and the sciences to the design and implementation of fast algorithms running on a large number of processors to process image data. As a result, image processing and analysis techniques are now applied to virtually all natural sciences and technical disciplines ranging from computer sciences and electronic engineering to biology and medical sciences; and digital images have come into everyone's life. Mathematics has been playing an important role in image and signal processing from the very beginning. There are two major mathematical approaches for image restoration, namely, wavelet tight frame approaches and differential/variational approaches. The main research objective of this project is to investigate geometric aspects of the former approach by connecting it with the latter. It will give rise to new mathematical models and numerical algorithms that benefit researchers in academia, national research laboratories, as well as in industry. The understandings of the geometric aspects of the wavelet frames and the connections with differential operators will contribute to both the community of computational harmonic analysis and the community of variational techniques and numerical PDEs. The education plan will bring undergraduate and graduate students to the frontiers of research in computational mathematics, computer vision and medical imaging; and strengthen the collaborations among mathematicians, engineers, computer scientists and medical doctors.<br/><br/>Wavelet frames are systems of functions that provide linear representations of functions living in certain function spaces such as L2(Rn). In contrast to the classic (bi)orthogonal wavelet bases, such representations are generally redundant which is desirable in many applications. Although most theoretical aspects of wavelet frames have already been well understood in the literature, geometric meanings of the wavelet frame transform are still generally unknown. In fact, the lack of geometric interpretations is one of the major flaws of wavelet frames that prohibits the applications of wavelet frames in some important problems of data analysis that require geometric regularization of the objects-of-interest reside in the data. The main research objective of this proposal is to develop a generic geometric interpretation to the wavelet frame transform, by studying its relations with differential operators within various variational frameworks. Based on the geometric interpretation, we propose new models and algorithms for several important applications such image restoration (deblurring, inpainting, CT/MR imaging, etc.). Through both theoretical analysis and numerical experiments, we will explore the advantages of the proposed wavelet frame based models over the existing variational and differential models for different applications. The proposed research will focus on: (1) the approximation of the differential operators by the wavelet frame transform within general variational frameworks; (2) solving large-scaled ill-posed inverse problems (e.g., image restoration, blind deconvolution) through convex/nonconvex optimizations using wavelet frames; (3) designing and solving wavelet frame based models in real-world applications in imaging such as low-dose CT image reconstruction, removing blurs caused by camera shaking, etc. The study of the geometric meanings of the wavelet frame transform will interpret wavelet frames and their associated optimization models from a whole new angle. Such fundamental study enables us, for the very first time, to fully utilize the unique properties of wavelet frames in geometry-involved data analysis tasks and finding numerical solutions of PDEs. The practical advantages (such as the quality of restoration for inverse problems) of wavelet frame transform over standard finite difference approximations in various applications will become more evident after the proposed studies. Furthermore, this project will also bring new understandings to numerical methods solving variational models; and answers some fundamental and important questions of variational models that are unclear from the literature."
215,1447822,BIGDATA: F: DKA: Learning a Union of Subspaces from Big and Corrupted Data,IIS,"OFFICE OF MULTIDISCIPLINARY AC, Robust Intelligence, Big Data Science &Engineering",9/1/14,5/1/17,Rene Vidal,MD,Johns Hopkins University,Standard Grant,Jie Yang,8/31/18,"$608,530.00 ",Daniel Robinson,rvidal@cis.jhu.edu,1101 E 33rd St,Baltimore,MD,212182686,4439971898,CSE,"1253, 7495, 8083","7433, 8083, 8251, 9251",$0.00 ,"This project develops theory and algorithms for automatically discovering multiple low-dimensional structures in high-dimensional data, and evaluates these algorithms in image clustering applications. The developed techniques enhance our ability to handle big data problems from multiple sources and modalities, and advance the knowledge on how to interpret massive amounts of complex high-dimensional data. The techniques developed in this project can significantly broaden the applicability of existing results in sparse representation theory to subspace clustering problems, which have found widespread applications in image processing (e.g., image denoising, compression, representation, and segmentation), computer vision (e.g., motion segmentation and face clustering) and dynamical systems (e.g., hybrid system identification).<br/> <br/>This research develops provably correct and scalable algorithms for learning a union of low-dimensional subspaces from big and corrupted data. The algorithms are based on the so-called self-expressiveness property of the data, which states that an uncorrupted data point can be well approximated by an affine combination of other uncorrupted data points. This research shows that by imposing a structured sparse and low-rank prior on the coefficients, one can discover multiple structures in the data. In the case of uncorrupted data, the research team studies conditions on the data under which a perfect clustering is possible. In the case of data corrupted by outliers, the research team studies conditions under which perfect clustering and outlier rejection are possible. In the case of data with missing entries, the research team studies conditions under which perfect clustering and data completion are possible. The project also develops efficient and scalable algorithms that benefit from distributed and high-performance computing for solving the various subspace clustering problems. These algorithms enable solving large-scale problems in computer vision, including image clustering."
216,1408763,AF: Medium: Collaborative Research: Fast and accurate optimization in planar graphs and beyond,CCF,Algorithmic Foundations,8/1/14,7/6/17,Jeff Erickson,IL,University of Illinois at Urbana-Champaign,Continuing Grant,Joseph Maurice Rojas,7/31/19,"$550,000.00 ",,jeffe@cs.uiuc.edu,1901 South First Street,Champaign,IL,618207406,2173332187,CSE,7796,"7924, 7926, 7929",$0.00 ,"A planar graph is a network drawn on the plane so that no edges cross. Planar and near-planar graphs have been fertile ground for algorithms research since the mid-1950s, both because they naturally model many classes of networks that arise in practice, and because they admit simpler and faster algorithms than more general graphs.  Algorithms for such graphs find numerous applications in geographic information systems, computer graphics, solid modeling, computer vision, VLSI design, information visualization, finite-element mesh generation, computational geometry, and other areas.  New techniques introduced in the last ten years have led to an explosion of new optimization algorithms for planar graphs and their generalizations, as well as the emergence of a small but growing research community that draws on and contributes to this pool of techniques; however, we are coming up against limitations of these techniques. <br/><br/>The goal of this project is to develop efficient algorithms for several fundamental optimization problems in planar graphs and related graph families.  Working on problems just outside the range of existing tools will lead the PIs toward developing the next batch of techniques that will maintain momentum in the field.  Specific targets include faster and simpler exact algorithms for variants of shortest paths, maximum flows, minimum cuts, and minimum-cost flows, as well as new polynomial-time approximation schemes for several network design, clustering, and facility location problems for which no such schemes are currently known.  The PIs will also implement and experimentally evaluate some algorithms that appear particularly promising, and make the resulting software publicly available, to support the efforts of students, researchers, and professionals in many different areas of computing."
217,1359199,REU Site: Undergraduate Research Experience in Multimedia Data Analytics,IIS,RSCH EXPER FOR UNDERGRAD SITES,5/1/14,4/25/14,Shou-Hsuan Huang,TX,University of Houston,Standard Grant,Wendy Nilsen,4/30/18,"$360,001.00 ",Ernst Leiss,shuang@cs.uh.edu,4800 Calhoun Boulevard,Houston,TX,772042015,7137435773,CSE,1139,9250,$0.00 ,"The REU site in the Department of Computer Science at the University of Houston (UH) introduces undergraduate students to Computational Data Analytics, an area of increasing national importance. Student research projects include Bioinformatics, Face Recognition, Computer Vision, Computational Physiology, Computational Cytometry, and Information Extraction for Financial Data. Ten students are selected in each of the three years of the program. The student recruitment plan actively encourages students from colleges and universities that offer limited research opportunities, such as historically black colleges and universities. Emphasis is on attracting female students and underrepresented minorities. The University of Houston is a comprehensive research university, a Hispanic Serving Institution (HSI), and one of the most ethnically diverse in the nation. The REU Site program strives to attract non-traditional students into research, reinforce their interest in computer science, retain these students in their academic programs, and prepare them for graduate school.  Results of the research conducted at the REU Site program will be disseminated through discipline-appropriate publications. <br/><br/>The REU Site program has four main objectives:<br/><br/>(1) Students will develop competency in scientific research, presentation, and writing skills.<br/>(2) Participants will produce meaningful research results during their summer program. These results will be written up in a format that is acceptable for publication in an appropriate conference or journal.<br/>(3) Participants will sustain and increase their interest in research careers after their on-campus summer through continuing contacts and research participation, and delivery of a presentation on their research.<br/>(4) Participants will learn to present and disseminate their research results.<br/><br/>The activities aimed at achieving the main objectives are integrated with student cohort activities, including site visits to cutting-edge research laboratories, seminars on career development, ethics in computer science, methods of literature search, and practical advice on questions related to graduate school. The goal is to make a lasting impact beyond the 10 weeks the students spend at the REU Site. The success of the program will be continuously evaluated through student and faculty questionnaires, and by an education professor, and this feedback will be used to improve the program in operation. The program maintains a website<br/>(http://www.cs.uh.edu/reu/) that supports the application and evaluation process and informs future, current, and past participants about deadlines and decisions, events, and achievements of the people involved in it."
218,1440772,CC*IIE Networking Infrastructure: Cyberinfrastructure - Creation of Science DMZ at UL Lafayette,OAC,Campus Cyberinfrastructure,10/1/14,8/20/14,Ben Blundell,LA,University of Louisiana at Lafayette,Standard Grant,Kevin Thompson,9/30/16,"$491,513.00 ","Joseph Neigel, Arun Lakhotia, Ryan Benton, Raju Gottumukkala",ben@louisiana.edu,104 E University Ave,Lafayette,LA,705032014,3374825811,CSE,8080,9150,$0.00 ,"The University of Louisiana at Lafayette is creating a dedicated high-speed science DMZ network infrastructure to support science and engineering faculty on campus. Various current and future research projects within the university have computational and data transfer needs that dramatically exceed beyond what existing infrastructure can support. The science DMZ delivers a next-generation packet transport service with better traffic identification, security, and application controls, and offers a 40Gbps data transfer capacity that connects all major research laboratories on campus. The infrastructure also links this campus wide connectivity to the statewide Louisiana Optical Network Initiative (LONI), and Internet2. This infrastructure addresses major bottlenecks to scientific discovery, and provides researchers with better access to visualization venues at Louisiana Immersive Technology Enterprise (LITE), and High Performance Computing (HPC) systems available on both LONI and XSEDE. <br/><br/>This upgraded next-generation network infrastructure also provides increased capacity for remote visualization, telepresence, and bandwidth-intensive data transfers between computational clusters. This will be of great benefit to multiple science and engineering programs across campus in the areas of computational biology, bioinformatics, computer vision, cybersecurity, and data science. The infrastructure also supports research areas such as coastal wetland loss, epidemiology, alternative energy, and medicine that have high economic and societal impact. The project also creates exposure for students at the University of Louisiana at Lafayette to leverage this network infrastructure."
219,1447566,BIGDATA: F: DKA: Collaborative Research: High-Dimensional Statistical Machine Learning for Spatio-Temporal Climate Data,IIS,Big Data Science &Engineering,9/1/14,8/25/14,Arindam Banerjee,MN,University of Minnesota-Twin Cities,Standard Grant,Maria Zemankova,8/31/19,"$357,000.00 ",,banerjee@cs.umn.edu,200 OAK ST SE,Minneapolis,MN,554552070,6126245599,CSE,8083,"7433, 8083",$0.00 ,"While statistical machine learning has seen major advances over the past two decades, rigorous approaches for high-dimensional spatio-temporal scientific data analysis have not received as much attention.  On the other hand, several core scientific areas, including climate science, ecology, environmental sciences, and neuroscience, are generating increasing amounts of high-resolution spatio-temporal data.  It is vital to develop rigorous machine learning approaches for such complex high-dimensional spatio-temporal data in order for key scientific breakthroughs in these areas in the next few decades.  The project contributes to these endeavors by focusing on two key technical and scientific areas: spatio-temporal big data analysis and climate science.  The project systematically develops the statistical machine learning foundations for the analysis of large scale complex high-dimensional spatio-temporal data, and applies such advances to problems arising in climate science, where the total amount of data is set to cross an Exabyte (1 Exabyte = 1000 Petabytes) soon. <br/><br/>The technical work in the project has three broad and interacting components: structured probabilistic graphical models for spatio-temporal data analysis, generalized graphical models for multivariate heavy tailed distributions, and physics-guided models with a richer class of structural constraints and capturing multi-scale phenomena.  The project applies these technical advances to climate science, by generating climate projections at high-resolutions.  Currently, the lack of requisite spatial resolution of current climate models makes automatic assessments of impacts, adaptation and vulnerability (IAV) difficult for a variety of sectors, including urban planning, freshwater resources, food security, energy, transportation systems, human health, and coastal systems."
220,1407732,Support Vector Machines for Censored Data,DMS,STATISTICS,7/1/14,6/17/16,Michael Kosorok,NC,University of North Carolina at Chapel Hill,Continuing grant,Gabor J. Szekely,6/30/18,"$410,000.00 ",,kosorok@unc.edu,104 AIRPORT DR STE 2200,CHAPEL HILL,NC,275991350,9199663411,MPS,1269,,$0.00 ,"Recent advances in medical research, including those related to the study of the human genome, have led to the development of personalized medicine. Personalized medicine describes medical treatment that is tailored to a patient based on the patient's genetic profile and other personal biomedical information. Developing personalized medical treatment regimens is challenging since it involves learning from high-dimensional patient data. Moreover, data from personalized-medicine clinical studies are typically subject to censoring, i.e., the data are not fully observed, due, for example, to patients dropping out during the course of a study. While statistical analysis of censored data is a well-developed area, most of the existing statistical tools were developed under restrictive assumptions that often do not hold for high-dimensional data settings. It is therefore important to develop an approach for analysis of censored data that can be applied to today's high-dimensional data sets. In this project we will develop novel machine learning techniques that can handle both high-dimensional and censored data. The algorithms that we will develop will be applicable not only in the field of personalized medicine, but also in other disciplines in which high-dimensional censored data are common, such as engineering, economics, and sociology.<br/><br/>In this research, we will extend the framework of support vector machines (SVMs) to censored data. First, we will develop support vector learning techniques for different types of censored data, including right censored data, interval censoring, current status data, and multistage decision problems with censored data. We will then study the theoretical properties of these estimators, including their finite-sample properties and their asymptotic behavior. For this goal we will develop novel methodology, including new finite-sample tools for censored data. Finally, we will apply the tools that we develop to real-world data. We will compare the proposed learning methods with existing methods using theoretical tools, simulation, and analysis of real-world data. Finally, we will develop software for each of the different algorithms that we study. This software will be developed such that it can be integrated into existing machine learning software."
221,1350337,CAREER: Active Learning through Rich and Transparent Interactions,IIS,Info Integration & Informatics,5/1/14,4/30/18,Mustafa Bilgic,IL,Illinois Institute of Technology,Continuing Grant,Sylvia Spengler,12/31/20,"$549,863.00 ",,mbilgic@iit.edu,10 West 35th Street,Chicago,IL,606163717,3125673035,CSE,7364,"1045, 7364",$0.00 ,"Machine learning models are trained on data that are annotated (labeled) by humans.  The accuracy of the trained models generally improves with the number of annotated data examples. Yet, annotating takes time, money, and effort.  Active learning aims to minimize the costs by determining which exemples are most informative and directing the human labeler to them.  Improvements in active learning will lower the costs associated with data annotation and lead to faster implementations of intelligent systems for a range of applications including robotics, speech technology, error and anomaly detection (for example in medicine, financial fraud, and condition-based maintenance of infrastructure), targeted advertising, human-computer interfaces, and bioinformatics.<br/><br/>In traditional active learning approaches, algorithms are limited in the types of information they can acquire, and they often do not provide any rationale to the user as to why a particular exemplar is chosen for annotation.  This CAREER project develops a new paradigm dubbed ""rich and transparent active learning.""  This new paradigm opens a communication channel between algorithms and users whereby they can exchange a rich set of queries, answers, and explanations.  By using rich feedback from users the algorithms will be able to learn the target concept more economically, reducing the resources required to build an accurate predictive model.  By explaining their reasoning, these algorithms will achieve transparency, build trust, and open themselves to scrutiny. <br/>  <br/>Towards that end, the project develops methods that allow algorithms to use a rich set of queries for resource-efficient model training, and generate explanations that are informative but not overwhelming for the users.  The methods developed build on expected loss minimization, information theory, and principles from human-computer interaction.  Approaches are evaluated using publicly available datasets and user studies carried out as part of the project.  The project develops case studies on two high-impact real-world problems: detecting fraudulent health-care claims, and identifying patients at risk of disease.<br/><br/>The rich and transparent active learning paradigm provides unique educational opportunities.  In contrast to standard machine learning algorithms, operated as black boxes, interactive and transparent machine learning is expected to raise students' interest and motivation for data science.  Two PhD and several undergraduate and high school students are being trained under this award.  A new graduate course on interactive machine learning is being developed.  Finally the PI ensures effective outreach to under-represented groups by partnering with a Chicago public high school whose student population includes 90% minorities."
222,1447587,BIGDATA: F: DKA: Collaborative Research: High-Dimensional Statistical Machine Learning for Spatio-Temporal Climate Data,IIS,Big Data Science &Engineering,9/1/14,8/25/14,Auroop Ganguly,MA,Northeastern University,Standard Grant,Maria Zemankova,8/31/19,"$356,998.00 ",,a.ganguly@northeastern.edu,360 HUNTINGTON AVE,BOSTON,MA,21155005,6173733004,CSE,8083,"7433, 8083",$0.00 ,"While statistical machine learning has seen major advances over the past two decades, rigorous approaches for high-dimensional spatio-temporal scientific data analysis have not received as much attention.  On the other hand, several core scientific areas, including climate science, ecology, environmental sciences, and neuroscience, are generating increasing amounts of high-resolution spatio-temporal data.  It is vital to develop rigorous machine learning approaches for such complex high-dimensional spatio-temporal data in order for key scientific breakthroughs in these areas in the next few decades.  The project contributes to these endeavors by focusing on two key technical and scientific areas: spatio-temporal big data analysis and climate science.  The project systematically develops the statistical machine learning foundations for the analysis of large scale complex high-dimensional spatio-temporal data, and applies such advances to problems arising in climate science, where the total amount of data is set to cross an Exabyte (1 Exabyte = 1000 Petabytes) soon. <br/><br/>The technical work in the project has three broad and interacting components: structured probabilistic graphical models for spatio-temporal data analysis, generalized graphical models for multivariate heavy tailed distributions, and physics-guided models with a richer class of structural constraints and capturing multi-scale phenomena.  The project applies these technical advances to climate science, by generating climate projections at high-resolutions.  Currently, the lack of requisite spatial resolution of current climate models makes automatic assessments of impacts, adaptation and vulnerability (IAV) difficult for a variety of sectors, including urban planning, freshwater resources, food security, energy, transportation systems, human health, and coastal systems."
223,1447574,BIGDATA: F: DKA: Collaborative Research: High-Dimensional Statistical Machine Learning for Spatio-Temporal Climate Data,IIS,Big Data Science &Engineering,9/1/14,8/25/14,Pradeep Ravikumar,TX,University of Texas at Austin,Standard Grant,Maria Zemankova,11/30/16,"$357,267.00 ",,pradeepr@cs.cmu.edu,"3925 W Braker Lane, Ste 3.340",Austin,TX,787595316,5124716424,CSE,8083,"7433, 8083",$0.00 ,"While statistical machine learning has seen major advances over the past two decades, rigorous approaches for high-dimensional spatio-temporal scientific data analysis have not received as much attention.  On the other hand, several core scientific areas, including climate science, ecology, environmental sciences, and neuroscience, are generating increasing amounts of high-resolution spatio-temporal data.  It is vital to develop rigorous machine learning approaches for such complex high-dimensional spatio-temporal data in order for key scientific breakthroughs in these areas in the next few decades.  The project contributes to these endeavors by focusing on two key technical and scientific areas: spatio-temporal big data analysis and climate science.  The project systematically develops the statistical machine learning foundations for the analysis of large scale complex high-dimensional spatio-temporal data, and applies such advances to problems arising in climate science, where the total amount of data is set to cross an Exabyte (1 Exabyte = 1000 Petabytes) soon. <br/><br/>The technical work in the project has three broad and interacting components: structured probabilistic graphical models for spatio-temporal data analysis, generalized graphical models for multivariate heavy tailed distributions, and physics-guided models with a richer class of structural constraints and capturing multi-scale phenomena.  The project applies these technical advances to climate science, by generating climate projections at high-resolutions.  Currently, the lack of requisite spatial resolution of current climate models makes automatic assessments of impacts, adaptation and vulnerability (IAV) difficult for a variety of sectors, including urban planning, freshwater resources, food security, energy, transportation systems, human health, and coastal systems."
224,1359280,REU Site: The Lehigh Smart Spaces Project,IIS,RSCH EXPER FOR UNDERGRAD SITES,5/1/14,5/9/16,John Spletzer,PA,Lehigh University,Continuing Grant,Wendy Nilsen,12/31/17,"$340,000.00 ",Mooi-Choo Chuah,john@boxrobotics.ai,Alumni Building 27,Bethlehem,PA,180153005,6107583021,CSE,1139,9250,$0.00 ,"The REU Site at Lehigh University focuses on engaging students in Smart Spaces research. Student projects include the development of technologies for saving energy in both residential and commercial spaces.  Research will also support home-centered approaches to assisted living and healthcare.  This has the potential to enable elderly and persons with disabilities to maintain their independence longer.  Independence brings not only improved quality of life, but also significant positive financial implications due to the changing age demographic in the United States.  For the participants, students will be recruited nationally from institutions where undergraduate STEM research opportunities are limited, with an emphasis on regional Pennsylvania State System of Higher Education (PASSHE) universities. Emphasis is made to recruit from groups that are traditionally underrepresented in computer science, to include low-income and first-generation college students, underrepresented minorities, and women. Lastly, the project will reinforce Lehigh University's own diversity initiatives - specifically, the Carl Greer Scholars program and articulation agreements with local community colleges - to increase the number of women and underrepresented minorities in engineering.  <br/><br/>Each year, ten undergraduate students participate in a research experience investigating problems in the area of smart environments.  Integrating ubiquitous computing, sensing, and actuators in a networked environment, students work directly with computer science faculty to improve the efficiency and effectiveness of the space. The objectives are to foster both undergraduate and intra-departmental research, to improve students' post-graduation outcomes with a complementary professional development seminar series, and to increase the number of students pursuing graduate education. The intellectual merit of the project lies in both the interdisciplinary nature of the research and the physical site itself.  Students will directly interact with eleven faculty members (including 6 NSF CAREER Award recipients) across a range of research topics, to include: 3-D augmented reality, ambient intelligence, data analytics for energy-aware spaces, computer vision, embedded devices, mobile computing, networking and security, robotics, and user interface design. The sheer size of the physical site at Lehigh's Mountaintop Campus also contributes to its research potential. At 120,000 square feet, it supports large-scale development in a real-world environment, rather than constraining such problems to simulation. Another unique aspect of the physical site is the potential for projects to be integrated into the building's infrastructure, creating one of the largest smart spaces of its kind. More information can be found at the project web site:  http://www.cse.lehigh.edu/smartspaces."
225,1409520,AF: Medium: Collaborative Research: Fast and accurate optimization in planar graphs and beyond,CCF,Algorithmic Foundations,8/1/14,8/8/17,Philip Klein,RI,Brown University,Continuing Grant,Joseph Maurice Rojas,1/31/19,"$665,987.00 ",,klein@brown.edu,BOX 1929,Providence,RI,29129002,4018632777,CSE,7796,"7924, 7926, 7929, 9150, 9251",$0.00 ,"A planar graph is a network drawn on the plane so that no edges cross. Planar and near-planar graphs have been fertile ground for algorithms research since the mid-1950s, both because they naturally model many classes of networks that arise in practice, and because they admit simpler and faster algorithms than more general graphs.Algorithms for such graphs find numerous applications in geographic information systems, computer graphics, solid modeling, computer vision, VLSI design, information visualization, finite-element mesh generation, computational geometry, and other areas.New techniques introduced in the last ten years have led to an explosion of new optimization algorithms for planar graphs and their generalizations, as well as the emergence of a small but growing research community that draws on and contributes to this pool of techniques; however, we are coming up against limitations of these techniques.<br/><br/>The goal of this project is to develop efficient algorithms for several fundamental optimization problems in planar graphs and related graph families.Working on problems just outside the range of existing tools will lead the PIs toward developing the next batch of techniques that will maintain momentum in the field.Specific targets include faster and simpler exact algorithms for variants of shortest paths, maximum flows, minimum cuts, and minimum-cost flows, as well as new polynomial-time approximation schemes for several network design, clustering, and facility location problems for which no such schemes are currently known.The PIs will also implement and experimentally evaluate some algorithms that appear particularly promising, and make the resulting software publicly available, to support the efforts of students, researchers, and professionals in many different areas of computing."
226,1447836,RI: Small: Organizing recognition: the uses of perceptual organization,IIS,Robust Intelligence,1/15/14,8/4/14,George Kamberov,AK,University of Alaska Anchorage Campus,Standard Grant,Jie Yang,8/31/15,"$52,573.00 ",,gkamberov@uaa.alaska.edu,3211 PROVIDENCE DRIVE,ANCHORAGE,AK,995084614,9077861777,CSE,7495,"7495, 7923, 9215, HPCC",$0.00 ,"Recognizing objects in images is the central problem of computer vision.  One approach (``bag of words'') compiles simple statistics on the image brightness patterns and recognizes by correlating these statistics, via learning, with the imaged objects. It cannot exploit important information on the image's spatial layout. Another approach, perceptual organization (PO), computes a distinctive, structural description of the image contents and recognizes based on this. Researchers agree that PO is a crucial early stage of recognition--and that its results are unreliable. (Images compress the 3D world and are ambiguous; PO cannot eliminate the ambiguity since it has no high-level knowledge of what the image is ``about.'') This leads to a fundamental dilemma: How can a recognition system use the result of PO if it cannot be trusted?<br/><br/>To exploit perceptual organizations without succumbing to their unreliability, this project uses a strategy that averages over all possible organizations weighted by their probability, instead of computing a single, ``most likely'' image description. This strategy is applied to diverse tasks such as matching images by the shapes of the objects within; recognizing articulated objects such as people and animals; tracking objects through video; and computing stable perceptual organizations. The project also studies the integration of this approach with older ones into a flexible and capable recognition system. The result will be new techniques for the analysis, manipulation, and search of images.  The methods developed will be integrated in the curriculum and disseminated to researchers, and the software will be made publicly available.<br/><br/>"
227,1431904,Science Inquiry Using Physical and Virtual Experiments: Systematic Investigation of Issues and Conditions for Learning,DRL,ECR-EHR Core Research,9/1/14,2/23/16,Sadhana Puntambekar,WI,University of Wisconsin-Madison,Standard Grant,Elizabeth VanderPutten,8/31/20,"$1,464,256.00 ","N Narayanan, N. Sanjay Rebello, Jee-Seon Kim",puntambekar@education.wisc.edu,21 North Park Street,MADISON,WI,537151218,6082623822,EHR,7980,"8244, 8818",$0.00 ,"Both physical labs and virtual labs are often used in science teaching, and both have their advantages and disadvantages. With each providing different affordances for learning, it is often feasible to combine the two in a multitude of ways. This project will conduct a series of studies to: (1) Uncover the differential benefits of physical versus virtual labs, for learners with a range of abilities and prior knowledge, and for content of different difficulty levels; (2)  Understand the differential effects of sequencing physical and virtual labs for different learners and content; and (3) Examine how best to combine physical and virtual labs in mixed reality environments. In addressing an important question in educational research, the project will help clarify the currently conflicting findings about the learning benefits of physical and virtual experimentation in fostering deep learning of science concepts. Publications resulting from the project will significantly extend current knowledge about learning from hands-on experimentation and learning from simulated experiments.<br/><br/>The research team will systematically examine factors that affect learning from physical and virtual labs in studies conducted across three grade levels in three states.  Rigorous analytical tools will assess the effect of several factors on students' learning processes and outcomes. Each year's studies will build upon results from the prior years. The first set of studies will address the comparison of virtual and physical experimentation. A second and third set of studies will investigate the sequencing of physical and virtual experimentation under different conditions. The fourth set of studies will explore an integration of the physical and the virtual for different learners and materials. In each study, a range of data will be collected to understand how students learn, their learning outcomes, and the strategies that teachers use to facilitate learning from physical and virtual labs.  Outcomes will include (1) comparisons of students' learning in physical, virtual, sequenced physical and virtual, and integrated physical and virtual labs; 2) knowledge about conditions that promote or hinder learning from physical and virtual labs individually and in various combinations; (3) an understanding of how learning is affected in physical, virtual, sequenced, and integrated labs by differing contexts, topics, student prior knowledge and ability; and (4) a clear and detailed qualitative picture of the differences and similarities in teacher strategies and student learning dynamics in physical, virtual, sequenced, and integrated labs."
228,1409992,CHS: Medium: Collaborative Research: Responsive Generation of Intrinsically Motivating Scenarios,IIS,HCC-Human-Centered Computing,9/1/14,7/30/14,Michael Mateas,CA,University of California-Santa Cruz,Standard Grant,William Bainbridge,8/31/18,"$857,000.00 ","Noah Wardrip-Fruin, Arnav Jhala",michaelm@soe.ucsc.edu,1156 High Street,Santa Cruz,CA,950641077,8314595278,CSE,7367,"7367, 7924",$0.00 ,"This project will develop advanced methods for automatically generating scenarios that are intrinsically motivating, responsive to the user's behavior, and potentially beneficial in many spheres of the economy and culture.  As numerous existing communication media are merging, computer-based narratives have become increasingly complex, significant, and influential.  However, at present they are inflexible and ignore the characteristics and goals of the individual user.  For many of our most pressing societal needs - from more effective education to addressing climatology challenges - a key component of any approach is motivation.  Intrinsic motivation, which involves performing an activity because it is inherently interesting, is often associated with deep learning and creativity.  This project employs an approach that combines types of reasoning drawn from computational creativity and logic programming to meet this challenge.<br/><br/>The research will demonstrate the first successful scenario generator, dynamically combining narrative and simulated action, and showing that generation can be guided by domain models, laying the foundation for projects to leverage generated scenarios for intrinsically motivating learning and other activities.  In so doing, it will execute a novel evaluation plan that will produce a more refined understanding of the relationship between intrinsic motivation and learning, including the interrelated categories of engagement, agency, and valuation of outcomes by comparing player experience of integrated scenario systems with and without allowing the user's choices to influence the challenges and narrative.  It is hoped that this research will demonstrate the utility of heterogeneous architectures for enabling previously-impossible experiences for users in a wide range of interactive experiences.  A specific focus is creation of a computer-based educational experience focused on meteorological shifts, made possible by this project's technical advances and informed by its findings related to user motivation and learning.<br/><br/>While the research community has had some success in generating both narrative and simulated action, generating both together presents novel research questions.  Further, to meet social needs, this generation must be capable of being guided by pedagogical and other explicitly-represented goals.  This research seeks to achieve two fundamental advances.  First, the project will demonstrate a successful scenario generator, showing that generation can be guided by domain models, which will enable future projects to leverage generated scenarios for intrinsically motivating learning and other activities. Second, user studies will support the first empirically grounded understanding of the effects of narrative and action responsiveness and variation on user experience, particularly engagement, motivation, and learning.  By demonstrating successful scenario generation, this project will turn attention to the scenario as a potential fundamental unit for educational software, with the potential to reach underrepresented groups and produce significant economic benefit."
229,1410004,CHS: Medium: Collaborative Research: Responsive Generation of Intrinsically Motivating Scenarios,IIS,HCC-Human-Centered Computing,9/1/14,7/30/14,Jill Denner,CA,ETR Associates,Standard Grant,William Bainbridge,8/31/18,"$321,100.00 ",,jilld@etr.org,5619 Scotts Valley Drive,Scotts Valley,CA,950663248,8314402235,CSE,7367,"7367, 7924",$0.00 ,"This project will develop advanced methods for automatically generating scenarios that are intrinsically motivating, responsive to the user's behavior, and potentially beneficial in many spheres of the economy and culture.  As numerous existing communication media are merging, computer-based narratives have become increasingly complex, significant, and influential.  However, at present they are inflexible and ignore the characteristics and goals of the individual user.  For many of our most pressing societal needs - from more effective education to addressing climatology challenges - a key component of any approach is motivation.  Intrinsic motivation, which involves performing an activity because it is inherently interesting, is often associated with deep learning and creativity.  This project employs an approach that combines types of reasoning drawn from computational creativity and logic programming to meet this challenge.<br/><br/>The research will demonstrate the first successful scenario generator, dynamically combining narrative and simulated action, and showing that generation can be guided by domain models, laying the foundation for projects to leverage generated scenarios for intrinsically motivating learning and other activities.  In so doing, it will execute a novel evaluation plan that will produce a more refined understanding of the relationship between intrinsic motivation and learning, including the interrelated categories of engagement, agency, and valuation of outcomes by comparing player experience of integrated scenario systems with and without allowing the user's choices to influence the challenges and narrative.  It is hoped that this research will demonstrate the utility of heterogeneous architectures for enabling previously-impossible experiences for users in a wide range of interactive experiences.  A specific focus is creation of a computer-based educational experience focused on meteorological shifts, made possible by this project's technical advances and informed by its findings related to user motivation and learning.<br/><br/>While the research community has had some success in generating both narrative and simulated action, generating both together presents novel research questions.  Further, to meet social needs, this generation must be capable of being guided by pedagogical and other explicitly-represented goals.  This research seeks to achieve two fundamental advances.  First, the project will demonstrate a successful scenario generator, showing that generation can be guided by domain models, which will enable future projects to leverage generated scenarios for intrinsically motivating learning and other activities. Second, user studies will support the first empirically grounded understanding of the effects of narrative and action responsiveness and variation on user experience, particularly engagement, motivation, and learning.  By demonstrating successful scenario generation, this project will turn attention to the scenario as a potential fundamental unit for educational software, with the potential to reach underrepresented groups and produce significant economic benefit."
230,1402723,Workshop on Frontiers in Image and Video Analysis,IIS,ROBUST INTELLIGENCE,1/15/14,1/10/14,Rama Chellappa,MD,University of Maryland College Park,Standard Grant,Todd Leen,12/31/14,"$55,850.00 ",Larry Davis,rama@cfar.umd.edu,3112 LEE BLDG 7809 Regents Drive,COLLEGE PARK,MD,207425141,3014056269,CSE,7495,"7495, 7556",$0.00 ,"The bombing attacks at the Boston Marathon in April 2013 presented the law enforcement community with significant challenges in terms of the volume and variety of video and still images acquired in the course of the investigation. Tens of thousands of individual media files in multiple formats were submitted from a variety of sources. These sources included broadcast television feeds, private Close-Circuit Television (CCTV) systems, mobile device photographs and videos recovered from the scene, as well as photographs and videos submitted by the public. Teams of analysts reviewed this evidence using mostly manual processes to determine the sequence of events before and after the bombing, ultimately leading to a quick resolution of the case. In the aftermath, it has become evident that the proliferation of video and image recording devices in fixed and mobile devices make it inevitable that a similar situation will occur in future events. As a result, it is incumbent upon the law enforcement community and the U.S. Government at large to further explore the use of automated approaches, available today or in the coming years, to better organize and analyze such large volumes of multimedia data. The findings of this workshop will help define the future research agenda. <br/><br/>The problem of searching for actionable intelligence information from unconstrained images and videos is an unsolved problem. Solving this involves addressing many sub-problems such as video summarization, shot detection/scene change detection, geo-tagging, robust face recognition, human action recognition, semantic description, image recognition and designing human in the loop systems. In addition, issues such as data collection and performance evaluation have to be addressed. Given that several hundreds of videos and a large collection of still images may be available for analysis, there is a great need to develop robust computer vision techniques. While many existing computer vision algorithms perform reasonable well in constrained acquisition conditions, their performance when unconstrained images and videos are given, is less than satisfactory. This workshop precisely addresses the challenges that arise in analyzing a large collection of unstructured image/video collection.  This workshop explores the state of the art in algorithms being developed in academia that can support forensic analysis and identification in large volumes of images and videos (e.g., multimedia). The workshop informs long- and near-term research and development efforts aimed at optimally addressing this situation in the future. The workshop identifies those video and image analysis problems which are: (1) Considered solved (i.e., ready to deploy in specific operational scenarios); (2) Nearly solved  (i.e., could lead to solutions with one to three years of development); and (3) Over-the-Horizon problems (i.e., those challenges requiring concerted effort over the next 3-5 years and beyond)."
231,1455886,"CAREER: Effective Analysis, Exploration and Visualization of Big Flow Data to Understand Dynamic Flows",IIS,Info Integration & Informatics,8/25/14,7/28/20,Chaoli Wang,IN,University of Notre Dame,Continuing Grant,Sylvia Spengler,4/30/21,"$521,245.00 ",,chaoli.wang@nd.edu,940 Grace Hall,NOTRE DAME,IN,465565708,5746317432,CSE,7364,"1045, 7364, 9251",$0.00 ,"The ever-growing size and complexity of flow data produced from many scientific, engineering and medical simulations pose significant challenges which are not thoroughly addressed by existing visualization techniques. These challenges include computation, interaction, visualization and user challenges. Addressing the computation challenge is a central research focus and remains a prominent direction in the field, while the other challenges are often overlooked. The goal of this CAREER project is to address these less investigated challenges by pioneering a comprehensive framework toward effective visual understanding of flow fields. It contributes to the state of the art flow visualization by promoting an innovative database approach to shape-based field line modeling and classification, investigating new string-, sketch- and graph-based interfaces and interactions for flow field exploration, and exploring occlusion and clutter reduction through unconventional streamline repositioning and automatic tour generation. The general approach developed in this research is expected to substantially improve our ability to visually understand a wide spectrum of flow fields, ranging from the traditional application of fluid flows to new applications such as traffic flows, cash flows and message flows. This project will provide training for graduate and undergraduate students in the area of data visualization and scientific computing via capstone class projects. A pedagogical toolbox will be designed along with web-based resources to support teaching visualization classes through expressive demos, potentially benefiting universities nationwide with a similar teaching need. The PI will continue to attract underrepresented students through university and department outreach programs and engage local middle and high school students through summer youth programs. <br/><br/>This research tackles the fundamental challenges in visualizing large, complex three-dimensional steady and unsteady flow fields. Underlying the proposed work is a novel database approach to field line shape encoding, classification and interrogation. The PI will integrate and unify a variety of concepts from geometric modeling, computer vision and data mining to create robust visual characters and words from field lines for shape analysis and organization. Novel interfaces and interactions will be introduced to enable intuitive retrieval of partial field lines via textual and visual forms, and examination of hierarchical field lines and their spatiotemporal relationships in the transformed graph space. Innovative streamline repositioning for focus+context viewing and automatic tour for examining hidden or occluded flow features will be devised to move from clutter to clarity in the visualization. The success of this research will benefit a wide variety of applications within and beyond graphics and visualization, such as shape analysis, visual perception, database organization, game development, and visualization in education.<br/><br/>The PI will collaborate with scientists and researchers at university, industry and national labs, applying the proposed solutions to solve real-world problems. Research results will be evaluated through both domain expert reviews and formal user studies. Selected research outcomes will be integrated into user-engaging educational applications that will be run on tablet devices and delivered to the general public for wide dissemination. This CAREER project will build a solid foundation for addressing key challenges in flow visualization, and lead to multidisciplinary collaborations spanning atmospheric cloud, combustion chemistry and cardiovascular research. It will also produce fruitful deliverables, featuring the first-ever benchmark field line shape database, tutorials and workshops at premier visualization conferences, and pedagogical tools and game apps."
232,1413417,Collaborative Research: Quantifying Human Retinotopic Mapping by Conformal Geometry,DMS,"Cross-BIO Activities, MATHEMATICAL BIOLOGY, MSPA-INTERDISCIPLINARY, Activation",7/1/14,6/20/14,Yalin Wang,AZ,Arizona State University,Standard Grant,Junping Wang,6/30/18,"$208,000.00 ",,ylwang@asu.edu,ORSPA,TEMPE,AZ,852816011,4809655479,MPS,"7275, 7334, 7454, 7713",8007,$0.00 ,"Retinotopy is the mapping of visual inputs from the retina to neurons in the brain. A retinotopic map is a visual of a particular occurrence of neuron activity taking place on a specific location in the brain. By analyzing the stimulus-referred functional magnetic resonance imaging (fMRI) response, retinotopic maps of the human visual cortex are generated. It has been hypothesized that human retinotopic maps are conformal mappings, but to date no theoretical models have been developed to quantify these maps. This project uses conformal geometry and fMRI data to study retinotopic maps in an attempt to model visual cortical organizations in the brain. This project will: (i) compute the intrinsic geometrical features that will determine and/or validate the conformality in the human retinotopy; (ii) model the relationships between the retinotopic maps in extrastriate visual areas and those in the primary visual cortex; (iii) develop methods to quantify retinotopic maps of individual subjects. The mathematical models applied in this project combine tools from topology, conformal geometry, complex analysis, optimization, and quasiconformal Teichmller theory. <br/><br/>Visual processing areas have been estimated to occupy more than half of the total surface of the primate neocortex. However, an understanding of visual cortical organizations still remains elusive due to their biological complexity. Retinotopic mapping of the human visual cortex, therefore, can be an important tool for studying the brain's circuitry. This project will produce theoretically sound and practically efficient methods for quantifying retinotopic maps. These methods will lead to non-invasive biomarkers of visual functions and may lead to cures for visual deficits. The computational theories and algorithms developed in this project will have applications in other fields, including computer vision, computer graphics, sensor networks, and geometric modeling. This project contributes to the BRAIN Initiative by increasing our knowledge of visual cortical organizations in the human brain and by developing laboratory infrastructure for knowledge discovery from brain imaging data. This project will create an interdisciplinary environment for graduate and undergraduate students and will develop interdisciplinary courses at the interface of mathematics and neuroscience."
233,1349462,"CAREER: Effective Analysis, Exploration and Visualization of Big Flow Data to Understand Dynamic Flows",IIS,Info Integration & Informatics,5/1/14,4/30/14,Chaoli Wang,MI,Michigan Technological University,Continuing grant,Sylvia Spengler,9/30/14,"$143,882.00 ",,chaoli.wang@nd.edu,1400 Townsend Drive,Houghton,MI,499311295,9064871885,CSE,7364,"1045, 7364",$0.00 ,"The ever-growing size and complexity of flow data produced from many scientific, engineering and medical simulations pose significant challenges which are not thoroughly addressed by existing visualization techniques. These challenges include computation, interaction, visualization and user challenges. Addressing the computation challenge is a central research focus and remains a prominent direction in the field, while the other challenges are often overlooked. The goal of this CAREER project is to address these less investigated challenges by pioneering a comprehensive framework toward effective visual understanding of flow fields. It contributes to the state of the art flow visualization by promoting an innovative database approach to shape-based field line modeling and classification, investigating new string-, sketch- and graph-based interfaces and interactions for flow field exploration, and exploring occlusion and clutter reduction through unconventional streamline repositioning and automatic tour generation. The general approach developed in this research is expected to substantially improve our ability to visually understand a wide spectrum of flow fields, ranging from the traditional application of fluid flows to new applications such as traffic flows, cash flows and message flows. This project will provide training for graduate and undergraduate students in the area of data visualization and scientific computing via capstone class projects. A pedagogical toolbox will be designed along with web-based resources to support teaching visualization classes through expressive demos, potentially benefiting universities nationwide with a similar teaching need. The PI will continue to attract underrepresented students through university and department outreach programs and engage local middle and high school students through summer youth programs. <br/><br/>This research tackles the fundamental challenges in visualizing large, complex three-dimensional steady and unsteady flow fields. Underlying the proposed work is a novel database approach to field line shape encoding, classification and interrogation. The PI will integrate and unify a variety of concepts from geometric modeling, computer vision and data mining to create robust visual characters and words from field lines for shape analysis and organization. Novel interfaces and interactions will be introduced to enable intuitive retrieval of partial field lines via textual and visual forms, and examination of hierarchical field lines and their spatiotemporal relationships in the transformed graph space. Innovative streamline repositioning for focus+context viewing and automatic tour for examining hidden or occluded flow features will be devised to move from clutter to clarity in the visualization. The success of this research will benefit a wide variety of applications within and beyond graphics and visualization, such as shape analysis, visual perception, database organization, game development, and visualization in education.<br/><br/>The PI will collaborate with scientists and researchers at university, industry and national labs, applying the proposed solutions to solve real-world problems. Research results will be evaluated through both domain expert reviews and formal user studies. Selected research outcomes will be integrated into user-engaging educational applications that will be run on tablet devices and delivered to the general public for wide dissemination. This CAREER project will build a solid foundation for addressing key challenges in flow visualization, and lead to multidisciplinary collaborations spanning atmospheric cloud, combustion chemistry and cardiovascular research. It will also produce fruitful deliverables, featuring the first-ever benchmark field line shape database, tutorials and workshops at premier visualization conferences, and pedagogical tools and game apps."
234,1414735,EAPSI: Construction progress: Monitoring and quality control using smartphones,OISE,EAPSI,6/1/14,5/28/14,Kook (Kevin) Han,IL,Han Kook I,Fellowship,Anne L. Emig,5/31/15,"$5,070.00 ",,,,Savoy,IL,618749624,,O/D,7316,"5942, 5978, 7316",$0.00 ,"Construction monitoring provides construction practitioners -owners, contractors, subcontractors, and tradesmen- with the information need for project control decisions. These decisions directly impact the overall efficiency of a construction project. Given that global construction spending is trillions of dollars, even small gains in efficiency could lead to enormous cost savings to the world. The goal of this project is to reduce both construction cost and delivery time with new progress monitoring and quality control. This information will be gathered from a large set of images and video streams that will be provided by consumer-level smart devices, such as smartphones and tablets. This research will be conducted at Seoul National University in South Korea in collaboration with Dr. Moonseo Park, an expert researcher in large-scale construction. The goal of the proposed project is to automate the process of large-scale building construction monitoring through advances in computer vision and construction management.<br/><br/>This research will enhance and simplify current progress monitoring and quality control tasks on construction sites by providing automated 3D as-built models that can be used in conjunction with Building Information Models for analyzing progress and quality. Using inexpensive consumer-level smart devices, construction managers can easily collect data and quickly submit inputs to the server and visualize outcomes with interfaces that enable effective project control decisions. This will be achieved by addressing fundamental underlying research challenges in computer vision and construction management including improving the efficiency and reliability of image-based 3D reconstruction method through GPU-based architecture for structure-from-motion; recognizing material properties as well as geometry from 2D images; and establishing a grammar in augmented reality visualization for field reporting. This NSF EAPSI award is funded in collaboration with National Research Foundation of Korea."
235,1439311,I-Corps:  Cyber Enabling Laparoscopy Devices,IIP,I-Corps,5/1/14,4/23/14,Marc Garbey,TX,University of Houston,Standard Grant,Rathindra DasGupta,4/30/15,"$50,000.00 ","Barbara Bass, Brian Dunkin",mgarbey2@houstonmethodist.org,4800 Calhoun Boulevard,Houston,TX,772042015,7137435773,ENG,8023,,$0.00 ,"Laparoscopic surgery is a popular alternative to open surgery due to the considerable reduction of recovery time, pain and complications. However, limited access to the operating field, indirect vision, and operating rooms (OR) originally built for open surgery conspire to make the surgeon's work more difficult and inefficient. The goal of the proposed work is to provide a smart OR system to improve safety and procedural success while reducing cost. To achieve this goal, the team will focus on: a smart trocar prototype that automatically recognizes the laparoscopic instruments and allows the accurate reconstruction of the time line in any minimally invasive surgery; and, a smart trocar that provides an accurate 3D localization of the tip of the laparoscopic instruments. <br/><br/>The proposed technology will provide essential information to improve the management of any minimally invasive procedure, reduce cost, and improve performance. In addition, the proposed technology will dramatically advance the technology of the OR by setting up a new REal Time Interactive Navigation Assistance (RETINA) for Surgery that provides robust directions to improve safety. Real time parallel computing, robust computer vision and finite element simulation are the main three complementary building blocks of this OR system. Accurate assessment of the surgeon & smart OR interaction by surgical training experts and close collaboration with industry will provide guidance to the agile development of the OR service."
236,1350323,CAREER: Large-scale Appearance Modeling,IIS,HCC-Human-Centered Computing,1/15/14,3/6/18,Pieter Peers,VA,College of William and Mary,Continuing Grant,Ephraim Glinert,12/31/19,"$473,479.00 ",,ppeers@cs.wm.edu,Office of Sponsored Programs,Williamsburg,VA,231878795,7572213966,CSE,7367,"1045, 7367, 7453",$0.00 ,"The visual appearance of the world around us is the result of complex light interactions between different surfaces and material properties that comprise a scene.  Despite staggering advances in data-driven appearance modeling, the creation of accurate models of large environments remains an open problem.  The reliance of most current appearance modeling methods on active lighting to probe different slices of a scene's appearance precludes their use in environments where there is limited or no control over the incident ambient lighting.  Furthermore, to facilitate calibration, many appearance modeling techniques estimate the appearance of a scene from a fixed vantage point, excluding scenes too large to fit in a single view with sufficient detail.  In this research, the PI will investigate two novel appearance modeling paradigms designed expressly for large-scale environments under uncontrolled ambient lighting: appearance-from-motion and appearance-by-similarity.  The former exploits relations between observations from different viewpoints to infer the full reflectance behavior, while the latter seeks to identify the best match from a library of pre-existing appearance instances to a possibly under-constrained set of observations.  To support these two paradigms, a novel appearance model will be developed that builds upon our intuitions regarding scene appearance.  The work will focus on two common types of input: community photo-collections and targeted video sequences.<br/><br/>Broader Impacts:  This research will pave the way towards practical techniques for in-situ appearance modeling of large-scale environments, while stimulating new research in computer vision and in data-driven appearance modeling in computer graphics by answering fundamental questions as to whether we can model appearance from motion and/or by exploiting similarity.  The project will have far-reaching impact not only on computer science but also on diverse fields ranging from metropolitan planning to cultural heritage to entertainment.  The ability to model existing environments will be beneficial to various security and safety training programs (for example, virtual fire drill simulations of existing buildings and sites could help train and prepare firefighters and first responders).  The emerging field of virtual reality therapy will also benefit from this research, by making it easier to create digital models of large-scale environments (so that, for example, patients who have suffered a stroke can practice motor rehabilitation skills in virtual reproductions of environments they encounter in their daily lives, while autistic children can train to improve their social interactions in virtual reproductions of places such as classrooms which they encounter in their daily lives)."
237,1461847,Topological methods for Azumaya algebras,DMS,"ALGEBRA,NUMBER THEORY,AND COM, TOPOLOGY",9/1/14,10/24/14,David Antieau,IL,University of Illinois at Chicago,Standard Grant,Joanna Kania-Bartoszynsk,7/31/16,"$45,347.00 ",,benjamin.antieau@gmail.com,809 S. Marshfield Avenue,Chicago,IL,606124305,3129962862,MPS,"1264, 1267",,$0.00 ,"The PI will engage in several projects at the border of algebraic geometry and algebraic topology. Three projects aim to use topological methods to understand the Brauer group, Azumaya algebras, and more generally torsors on schemes. (1) The PI will study the extent to which the foundational results of Jackowski, McClure, and Oliver on maps between classifying spaces of complex algebraic groups can be extended to finite approximations to these classifying spaces. Progress on this problem will enable the solution of a host of problems about when torsors for complex algebraic groups extend from the generic point of a scheme to the entire scheme. In low dimensions, early progress on this problem has been used by the PI and Ben Williams to settle an old question of Auslander and Goldman on the existence of Azumaya maximal orders in unramified division algebras, where it transpires that there are purely topological obstructions to the existence of these Azumaya maximal orders. (2) The PI will work toward computing the Chow groups and singular cohomology of the classifying spaces of special linear groups by various central subgroups. This has been done in special cases by Vezzosi and Vistoli. However, greater generality is needed for most applications. These Chow groups are fundamental objects in algebraic geometry, controlling the characteristic classes associated to certain torsors of fundamental importance in the study of the Brauer group. The computations will be directly useful to the first project, and to the following project. (3) The PI and Ben Williams previously formulated the topological period-index problem and established first results. They will continue this study, especially as it relates to the algebraic period-index conjecture. In particular, their results in low dimensions suggest a method for disproving the period-index conjecture, which would be a fundamental advance. Following this idea to its conclusion is the major aspiration of the first set of projects. A fourth project aims to continue to build a bridge between higher category theory and classical algebraic geometry, bringing the formidable techniques of the former to bear on various questions in the arithmetic of derived categories. For example, the PI is developing a toolbox using higher category theory that will allow a purely derived-category proof of Panin's computations of the K-theory of projective homogeneous spaces, once the existence of certain exceptional objects on the split forms of these spaces is known.<br/><br/>The PI proposes work in algebraic geometry and algebraic topology, two areas of modern mathematics. Algebraic geometry is an ancient subject with many connections to real-world problems. Its goal is to understand the geometry of solutions sets of polynomial equations, equations of central importance in various disciplines, such as theoretical physics, cryptography, and the modeling of dynamical systems like weather. Algebraic topology on the other hand developed more recently, in the 19th century, and aims to study a general notion of shape, less rigid than the idea of shape studied in geometry. It has found striking applications in the last decade, for instance to the analysis of large data sets that occur in computer vision and cancer research, frequently finding patterns that more traditional methods of data analysis fail to find. The proposal of the PI will bring the considerable machinery and insight of algebraic topology to bear on several questions in algebraic geometry which have been identified by the community as among the most important."
238,1412722,Collaborative Research: Quantifying Human Retinotopic Maps by Conformal Geometry,DMS,"MATHEMATICAL BIOLOGY, MSPA-INTERDISCIPLINARY, Activation",7/1/14,6/20/14,Zhong-Lin Lu,OH,Ohio State University,Standard Grant,Junping Wang,6/30/19,"$60,000.00 ",,zhonglin@nyu.edu,Office of Sponsored Programs,Columbus,OH,432101016,6146888735,MPS,"7334, 7454, 7713",8007,$0.00 ,"Retinotopy is the mapping of visual inputs from the retina to neurons in the brain. A retinotopic map is a visual of a particular occurrence of neuron activity taking place on a specific location in the brain. By analyzing the stimulus-referred functional magnetic resonance imaging (fMRI) response, retinotopic maps of the human visual cortex are generated. It has been hypothesized that human retinotopic maps are conformal mappings, but to date no theoretical models have been developed to quantify these maps. This project uses conformal geometry and fMRI data to study retinotopic maps in an attempt to model visual cortical organizations in the brain. This project will: (i) compute the intrinsic geometrical features that will determine and/or validate the conformality in the human retinotopy; (ii) model the relationships between the retinotopic maps in extrastriate visual areas and those in the primary visual cortex; (iii) develop methods to quantify retinotopic maps of individual subjects. The mathematical models applied in this project combine tools from topology, conformal geometry, complex analysis, optimization, and quasiconformal Teichmller theory. <br/><br/>Visual processing areas have been estimated to occupy more than half of the total surface of the primate neocortex. However, an understanding of visual cortical organizations still remains elusive due to their biological complexity. Retinotopic mapping of the human visual cortex, therefore, can be an important tool for studying the brain's circuitry. This project will produce theoretically sound and practically efficient methods for quantifying retinotopic maps. These methods will lead to non-invasive biomarkers of visual functions and may lead to cures for visual deficits. The computational theories and algorithms developed in this project will have applications in other fields, including computer vision, computer graphics, sensor networks, and geometric modeling. This project contributes to the BRAIN Initiative by increasing our knowledge of visual cortical organizations in the human brain and by developing laboratory infrastructure for knowledge discovery from brain imaging data. This project will create an interdisciplinary environment for graduate and undergraduate students and will develop interdisciplinary courses at the interface of mathematics and neuroscience."
239,1450543,EAGER: Machine learning of discourse structure for personalized online tutoring,IIS,Robust Intelligence,9/1/14,1/27/16,Geoffrey Gordon,PA,Carnegie-Mellon University,Standard Grant,Weng-keen Wong,8/31/17,"$200,000.00 ",Reid Simmons,ggordon@cs.cmu.edu,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,CSE,7495,"7495, 7916",$0.00 ,"Technology in education provides both tremendous opportunities and tremendous challenges. By delivering instruction online, we may be able to reach a much wider range of students than previously possible. But, while technology allows instruction to reach more students, it also risks losing the rich interaction and feedback that in-person instruction provides. To be truly effective, courses need both rich content and personalized interaction and feedback. So, this project seeks to provide a technical foundation for restoring richer interaction to the online learning experience, particularly through natural language content such as discussion boards and peer grading.<br/><br/>The project will develop machine learning techniques for discovering high-level structure in natural language text, including discourse structure (relationships among pieces of text), topic structure, and semantic structure. To discover this structure, the project investigates the use of spectral learning methods. These methods, which rely on factoring a matrix or tensor of observed moments, are able to learn latent structures efficiently and without local optima. Of particular interest and challenge is to develop tractable methods that can learn latent structure from unlabeled or weakly-labeled data. Many current NLP techniques have difficulty with informal language and do not extract higher level structure; our goal is to develop spectral methods to address these weaknesses. The project will attempt to learn this high-level latent structure in data taken from discussion boards of large online classes. The idea is that future work could use this understanding to help personalize the way students access this sort of rich natural language content: e.g., by helping to focus searches, find relevant and cohesive content, synthesize answers to student questions, promote productive behaviors in online discussions, and support self and peer grading. The eventual goal is to provide personalized interfaces to students that facilitate their learning of novel material."
240,1427872,"MRI Development: Enabling Research in Natural Communication with Virtual Tutors, Therapists, and Robotic Companions",CNS,"Major Research Instrumentation, Information Technology Researc, Special Projects - CNS, HCC-Human-Centered Computing, Robust Intelligence, NRI-National Robotics Initiati",9/1/14,5/11/17,Mohammad Mahoor,CO,University of Denver,Standard Grant,Rita Rodriguez,8/31/19,"$1,348,000.00 ","Ronald Cole, Wayne Ward, Juan Wachs",mmahoor@du.edu,2199 S. University Blvd.,Denver,CO,802104711,3038712000,CSE,"1189, 1640, 1714, 7367, 7495, 8013","1189, 8086, 9251",$0.00 ,"This project, developing SocioBot-SDS (SocioBot-Spoken Dialog System), an instrument in the form of a robotic character with an emotional response, is expected to advance research involving next generation human-machine interactions. The robotic instrument will be used for therapeutic and educational purposes. Its development will specifically contribute to the research area of perceived speech and visual behaviors. The components of the instrument integrate a unique level of programmability and robustness to the display of human- and non-human-like emotive gestures with conventional orientation control through an articulated neck. The work is expected to accelerate research and development of social robots that can accurately model the dynamics of face-to-face communication with a sensitive and effective human tutor, clinician, or caregiver to a degree unachievable with current instrumentation. The robotic agent builds on advances in computer vision, spoken dialogue systems, character animation and effective computing to conduct dialogues that establish rapport with users producing rich, emotive facial gestures synchronized with prosodic speech generations in response to users' speech and emotions. The instrument represents a new level of integration of emotive capabilities that enable researchers to study socially emotive/robots/agents that can understand spoken language and show emotions and interact, speak, and communicate effectively with people in a natural way (as humans do).<br/><br/>The instrument provides an exciting platform for research and training supporting cross-discipline technology. Since the research community would have access to the instrument, research in how to optimize communication among people and avatars might be accelerated through the perception of speech patterns and visual behaviors of those interacting. The instrument represents a new level of integration of emotive capabilities and serves as a platform for designing a new generation of more immersive and effective intelligent tutoring and therapy systems, and robot-assisted therapeutic treatments for human disabilities that include infants at risk for sensory, attention and language delays, as well as adults with mental disabilities.  From an educational perspective, the proposed activities will enhance inter-disciplinary education by involving students at all levels, during and beyond the development of the instrument. The resulting instrument will be freely distributed for researchers to investigate robotic behaviors that lead to immersive and effective applications across a variety of task domains such as teaching students to read, tutoring students in science, conducting speech and language therapy sessions, or providing companionship to elderly individuals in their homes. Moreover, the activities initiated by this development enhance interdisciplinary education, involving students at the undergraduate and graduate levels, during and beyond the development of the instrument."
241,1421643,CSR: Small: Collaborative Research:  Adaptive Memory Resource Management in a Data Center - A Transfer Learning Approach,CNS,CSR-Computer Systems Research,10/1/14,6/29/15,Steven Carr,MI,Western Michigan University,Standard Grant,Marilyn McClure,9/30/19,"$112,000.00 ",,steve.carr@wmich.edu,1903 West Michigan Avenue,Kalamazoo,MI,490085200,2693878298,CSE,7354,"7923, 9178, 9251",$0.00 ,"Cloud computing has become a dominant scalable computing platform for both online services and conventional data-intensive computing (examples include Amazon's EC2, Microsoft's Azure, IBM's SmartCloud, etc.).  Cloud computing data centers share computing resources among a large set of users, providing a cost effective means to allow users access to computational power and data storage not practical for an individual. A data center often has to over-commit its resources to meet Quality of Service contracts. The data center software needs to effectively manage its resources to meet the demands of users submitting a variety of applications, without any prior knowledge of these applications. <br/><br/>This work is focused on the issue of management of memory resources in a data center. Recent progress in transfer learning methods inspires this work in the creation of dynamic models to predict the cache and memory requirements of an application. The project has four main tasks: (i) an investigation into how recent advancements in transfer learning can help solve data center resource management problems, (ii) development of a dynamic cache predictor using on-the-fly virtual machine measurements, (iii) creation of a dynamic memory predictor using runtime characteristics of a virtual machine, and (iv) development of a unified resource management scheme creating a set of heuristics that dynamically adjust cache and memory allocation to fulfill Quality of Service goals.  In tasks (i)-(iii), transfer learning methods are employed and explored to facilitate the transfer of knowledge and models to new system environments and applications based on extensive training on existing systems and benchmark applications.  The prediction models and management scheme will be evaluated on common benchmarks including SPEC WEB and CloudSuite 2.0.  The results of this research will have broad impact on the design and implementation of cloud computing data centers. The results will help improve resource utilization, boost system throughput, and improve predication performance in a cloud computing virtualization system. Additionally, the methods designed and knowledge they impart will advance understanding in both systems research and machine learning."
242,1422342,CSR: Small: Collaborative Research: Adaptive Memory Resource Management in a Data Center - A Transfer Learning Approach,CNS,CSR-Computer Systems Research,10/1/14,9/4/14,Laura Brown,MI,Michigan Technological University,Standard Grant,Marilyn McClure,5/31/18,"$299,993.00 ",Zhenlin Wang,lebrown@mtu.edu,1400 Townsend Drive,Houghton,MI,499311295,9064871885,CSE,7354,7923,$0.00 ,"Cloud computing has become a dominant scalable computing platform for both online services and conventional data-intensive computing (examples include Amazon's EC2, Microsoft's Azure, IBM's SmartCloud, etc.).  Cloud computing data centers share computing resources among a large set of users, providing a cost effective means to allow users access to computational power and data storage not practical for an individual. A data center often has to over-commit its resources to meet Quality of Service contracts. The data center software needs to effectively manage its resources to meet the demands of users submitting a variety of applications, without any prior knowledge of these applications. <br/><br/>This work is focused on the issue of management of memory resources in a data center. Recent progress in transfer learning methods inspires this work in the creation of dynamic models to predict the cache and memory requirements of an application. The project has four main tasks: (i) an investigation into how recent advancements in transfer learning can help solve data center resource management problems, (ii) development of a dynamic cache predictor using on-the-fly virtual machine measurements, (iii) creation of a dynamic memory predictor using runtime characteristics of a virtual machine, and (iv) development of a unified resource management scheme creating a set of heuristics that dynamically adjust cache and memory allocation to fulfill Quality of Service goals.  In tasks (i)-(iii), transfer learning methods are employed and explored to facilitate the transfer of knowledge and models to new system environments and applications based on extensive training on existing systems and benchmark applications.  The prediction models and management scheme will be evaluated on common benchmarks including SPEC WEB and CloudSuite 2.0.  The results of this research will have broad impact on the design and implementation of cloud computing data centers. The results will help improve resource utilization, boost system throughput, and improve predication performance in a cloud computing virtualization system. Additionally, the methods designed and knowledge they impart will advance understanding in both systems research and machine learning."
243,1428204,MRI: Acquisition of a 3D object and motion capture system,CNS,MAJOR RESEARCH INSTRUMENTATION,9/1/14,8/27/14,Marc Olano,MD,University of Maryland Baltimore County,Standard Grant,Rita Rodriguez,8/31/17,"$175,195.00 ","Erle Ellis, Daniel Bailey, Amy Hurst, Shaun Kane",olano@csee.umbc.edu,1000 Hilltop Circle,Baltimore,MD,212500002,4104553140,CSE,1189,1189,$0.00 ,"This project, acquiring a photogrammetry-based 3D acquisition system (multi-camera computer vision 3D capture system) as a research instrument, aims to enable easy and repeatable collection of data across a wide range of research areas (Computer Science, Information Science, Geography, Environmental Science, Engineering, American Studies, Visual Arts, History, Library Science, etc.). The system consists of 96 synchronized cameras and associated flash and projection lights to capture a single object ranging from about 5 centimeters to several meters instantly. The flexibility of a photo-based system allows the same system to capture high resolution models over a wide range of scales, and to be equally useful for a single rigid object, an object in motion, or a person using a subject, object, or software, all with no object or subject preparation and minimal setup or reconfiguration.<br/><br/>The scanning device is likely to have significant impacts: the projects on assistive technologies promise a lasting impact on disabled individuals; the brain imaging and genomics projects promise improvements on diagnostics health; the project will also have impact in the humanities, in understanding, and in documenting artifacts; and the digital library project will directly address how data generated by an instrument like this one can be preserved and referenced.<br/><br/>Additionally, the project will have impacts on education and training of students. The center that will house the instrument has significant undergraduate staff who will be exposed to a state-of-the-art instrument and the research it enables. Undergraduate assistants, under the guidance of the center staff, will support the staffing for the instrument. Moreover, the instrument will impact several classes directly which will be able to use object and motion scanning equipment that is seldom available to undergraduates."
244,1422669,RI: Small: Engineering and Learning Visual Representations,IIS,ROBUST INTELLIGENCE,7/15/14,7/14/14,Stefano Soatto,CA,University of California-Los Angeles,Standard Grant,Jie Yang,6/30/18,"$456,617.00 ",,soatto@ucla.edu,10889 Wilshire Boulevard,LOS ANGELES,CA,900951406,3107940102,CSE,7495,"7495, 7923",$0.00 ,"Visual data, including video imagery, conveys ""information"" about objects of interest within the scene: Shape, material, identity, relations, etc. However, it is also highly redundant, and subject to variability that has little to do with the properties of the scene of interest, but instead depend on the sensor, the vantage point, and the nature of the illuminant, etc. This project addresses the question of determining what function of imaging data should be inferred and stored, that is, as ""informative"" as possible for a class of tasks such as object or scene detection, localization, recognition and categorization, and at the same time as ""compressed"" as possible, and insensitive to nuisance variability. Such a function is called a Representation. This research has pedagogical value, by framing seemingly unrelated methods as different approximations of an ideal Representation, thus facilitating the educational process in Computer Vision. This is further expected to facilitate the design of better Representations, and therefore improved algorithms for visual recognition (detection, localization, recognition, and categorization) systems, with impact in a range of applications from autonomy (e.g., robotic navigation and surveillance) to interaction (e.g., assisted surgery and augmented reality).<br/><br/>The project frames the problem of inferring optimal task-specific Representations in terms of the Information Bottleneck Principle, and addresses issues of computability, approximation, and dimensionality reduction within this framework. It also addresses questions of ""learnability,"" to determine whether a generic learning architecture can approximate an optimal representation. The Information Bottleneck is a generalization and relaxation of the notion of minimal sufficient statistic, where complexity constraints and task relevance are explicitly taken into account. The challenge is that modeling the generative process for visual data entails complex geometry (surface shape), topology (occlusions), photometry (material reflection, illumination), and dynamics (motion) with the object of interest living in infinite-dimensional spaces. Thus, the Information Bottleneck is difficult to even formalize, let alone instantiate, compute, and optimize. The project focuses on developing approximations of the Information Bottleneck that are tractable and yet enjoy performance guarantees."
245,1431009,SBIR Phase II:  Autonomous 3D Scanner for Building Interiors and Exteriors,IIP,SMALL BUSINESS PHASE II,10/1/14,9/10/14,Oskar Skrinjar,GA,"Scientific Imaging and Visualization, LLC",Standard Grant,Ben Schrag,4/30/17,"$669,885.00 ",,oskar@scientificiv.com,1996 Wellesley Trace,Atlanta,GA,303383088,4048632371,ENG,5373,"5373, 8034, 8039",$0.00 ,"This SBIR Phase II project will result in a system that can scan building interiors and exteriors and construct corresponding detailed, photo-realistic 3D models. Having such models is useful in a number of applications. However, the existing approaches for the construction of such models are labor intensive and require highly trained people and expensive equipment, which make the entire process prohibitively expensive for most applications. The proposed system will significantly reduce the cost of construction of such models by using only video input and automating most of the process. The proposed system could be used to relatively inexpensively construct detailed, photo-realistic 3D models of interiors and exteriors of: real estate properties, restaurants, sport clubs, museums, historic buildings, architectural sites, touristic sites, university campuses, amusement parks, water parks, zoos, apartment complexes, resorts, hotels, hospitals, shopping malls, etc. The constructed 3D models can then be explored online using computers, smartphones and tablets. Furthermore, the system can be used for mapping or exploring buildings in situations where it is dangerous or impractical for people to enter, which is often the case in rescue and law-enforcement applications. Given the wide range of potential customers, the proposed system has the potential to generate both one-time earnings as well as recurring income streams.<br/><br/>The objective of the project is to develop a system for inexpensive generation of photo-realistic 3D models of building interiors and exteriors and provide an on-line interactive visualization interface for the viewing of the 3D models. The proposed system is based on a small unmanned aerial vehicle (UAV) equipped with a wireless camera that can semi-autonomously fly inside and around the building, video record all the interior and exterior structures, and send the video and other recorded information to the base station where the 3D model is constructed using computer vision techniques. The proposed system requires the development of two innovative technologies: (1) semi-automated construction of texture-mapped 3D models of structures video recorded from a flying vehicle, and (2) semi-autonomous flying of an UAV inside and around a building based on the video stream from the UAV's camera. While 3D scanning is an existing technology and small UAVs have already been developed, this proposal will combine these technologies and enhance their capabilities to deliver new value. In addition, computer, smartphone and tablet software for interactive visualization and exploration of the constructed 3D models will be developed along with a user-friendly interface."
246,1351047,CAREER: Towards a Big Data Application Server Stack,CNS,"CAREER: FACULTY EARLY CAR DEV, CSR-Computer Systems Research",2/1/14,3/12/18,Tyson Condie,CA,University of California-Los Angeles,Continuing Grant,Marilyn McClure,1/31/19,"$464,715.00 ",,tconde@cs.ucla.edu,10889 Wilshire Boulevard,LOS ANGELES,CA,900951406,3107940102,CSE,"1045, 7354",1045,$0.00 ,"Google's MapReduce inspired much of the Big Data Analytics work and has served as a template for open source systems like Apache Hadoop. The MapReduce programming model has wide applicability, but widespread adoption has exposed some limitations, such as the lack of support for iteration (which is common in machine learning algorithms), stream processing, graph analytics, real-time and interactive queries. Beyond the programming framework, the underlying implementation offers a template for how to scale-out massively distributed computations: break them up into small tasks that can be carried out in parallel by partitioning the underlying data, and save intermediate state to mitigate the impact of partial failures (which must be planned for when running on large clusters). The challenge then, is to build implementations of other programming frameworks (e.g., SQL and machine learning) that share the same scale-out and fault-tolerance runtime characteristics of MapReduce without imposing its limitations. <br/><br/>Resource managers such as Apache Hadoop YARN, Google Omega and Berkeley Mesos take a first step in this direction by separating resource allocation from the details of higher-level programming models and languages. Resource managers multiplex several jobs on the same underlying machine cluster, thereby increasing utilization and fostering clean-slate software stacks. When the task executing in a container a slice of a single machine's resources (CPU/GPU, memory, disk) is finished, the container is returned to the resource manager, where it is made available to other jobs. Unlike in higher-level stacks, a container is a blank-slate process, designed to host arbitrary computations. This project prescribes further reusable software layers that capture issues like how many resources should I dedicate to a job?; what are the redundant code-pathways and can I provide them in a reusable library?; what are the right language and runtime abstractions? Exploring these questions in the context of systems like MapReduce and related SQL implementations, ML toolkits, storage systems, and messaging systems, on next generation resource managers, is the primary focus of our work.<br/><br/>The goal is to unify a suite of large-scale data processing tasks on a single runtime layer, built on modern resource managers (the cloud operating systems). Our results will factor out commonalities in specialized systems and provide them in a single underlying runtime system, shortening the time to ?market? for the next ready-to-use Big Data toolkit, which in turn would increase the availability of such tools to the broader community. Experience gained by implementing and deploying applications at scale, over next generation resource managers, could help inform critical design choices in the development of future cloud computing platforms, and hence impact a broad range of scientific, engineering, national security, healthcare and business applications. The project offers enhanced opportunities for research-based advanced training of graduate and undergraduate students, including members of groups that are currently under-represented in computer science, in databases, machine learning, and cloud computing."
247,1359361,REU Site: Imaging in the Physical Sciences,PHY,"EWFD-Eng Workforce Development, Integrative Activities in Phys",2/15/14,2/18/16,Roger Dube,NY,Rochester Institute of Tech,Continuing Grant,Kathleen McCloud,1/31/18,"$273,044.00 ","Jacob Noel-Storr, Carl Salvaggio",dube@cis.rit.edu,1 LOMB MEMORIAL DR,ROCHESTER,NY,146235603,5854757987,MPS,"1360, 9134","116E, 9250, SMET",$0.00 ,"This Research Experiences for Undergraduates (REU) site at the Rochester Institute of Technology provides eight undergraduate students each summer the opportunity to participate in research on topics in physics, mathematics, astrophysics, remote sensing, biomedical imaging, environmental science, vision science, nano-technology, materials science, color science, computer vision and graphics, and microelectronic engineering. In addition to participating in research with an experienced mentor, the REU students will engage in weekly activities to ensure not only that their research stays on track but that they have an engaging collegial and scholarly experience. By the end of the program, every student will have practiced a one-minute ""elevator pitch""; given a formal presentation about their work; and prepared a conference poster and a 4-page letter style paper. The REU site will further support five students to present their work at national conferences after the summer.<br/><br/>This REU site will have a number of broader impacts. The site has a strong plan to recruit undergraduate students from groups underrepresented in physics, particularly deaf or hard of hearing students and American Indian students. Steps have been taken to provide a culturally safe environment for all students. The proposed evaluation of the project seeks to answer a number of questions that will inform this project and others like it."
248,1431147,Neural habituation: A unified account of visual identification dynamics across tasks,BCS,"PERCEPTION, ACTION & COGNITION",7/15/14,8/1/16,David Huber,MA,University of Massachusetts Amherst,Standard Grant,Lawrence Gottlob,6/30/18,"$390,281.00 ",,dehuber@psych.umass.edu,Research Administration Building,Hadley,MA,10359450,4135450698,SBE,7252,"7252, 8819, 9251",$0.00 ,"People often fail to see things that appear right before their eyes. This can occur in many different situations and for many different types of information, ranging from failing to identify where or when something appeared to not noticing that it appeared it all. This project aims to explain failures of detection across a wide range of situations using a unified theoretical framework. Understanding the mechanisms of visual detection failure and its role in normal visual processing is important for many societal problems that involve continuously monitoring visual information, including forensic applications. For example, accurate screening of baggage at security checkpoints or monitoring security video requires an understanding of when human detection is likely to fail. Also, designing successful artificial vision systems requires an understanding of why some types of detection failure are actually necessary for the processing of continuous visual information. The brain's mechanism for separating what is different from what is the same in the constant stream of visual information may come at a cost in some situations. This is relevant to designing real-time computer vision systems that can process ongoing visual information accurately.<br/> <br/>The theory under investigation in this project is that visual detection failures are often due to neural habituation, which occurs when a neural representation within a network becomes temporarily over-activated after prolonged visual exposure. During the habituation period, the neural representation is temporarily less accessible, which can lead to a failure to consciously notice or detect that particular piece of information if it appears again during the habituation period. The hypothesis tested is that the brain uses transient neural habituation to track information that is constant versus changing. For example, by becoming habituated to recently viewed faces, the brain avoids confusing the attributes of those faces with a currently viewed face, helping to tell people apart. The cost is that people often fail to detect recently seen attributes if they re-appear during the habituation period. To probe visual detection and detection failures, scientists often use rapid serial visual presentation tasks, in which multiple visual images are presented in quick succession. This project aims to explain detection failure across a wide range of such tasks with a unified theoretical model of neural habituation. This effort will increase current understanding of how processes occurring at the level of neural networks can explain processes occurring at the level of visual detection and awareness."
249,1418386,Novel Paradigms in Geometric Modeling of Large and High-Dimensional Data Sets,DMS,"COMPUTATIONAL MATHEMATICS, CDS&E-MSS",7/15/14,7/11/14,Gilad Lerman,MN,University of Minnesota-Twin Cities,Standard Grant,Christopher Stark,6/30/18,"$250,000.00 ",,lerman@umn.edu,200 OAK ST SE,Minneapolis,MN,554552070,6126245599,MPS,"1271, 8069","7433, 8084, 8396, 8609, 9263",$0.00 ,"The principal investigator and his collaborators aim to develop effective data modeling paradigms that are sufficiently simple for statistical inference. Current scientific investigations, as well as industrial applications, produce and rely on massive, high-dimensional and possibly corrupted data sets. A major focus of applied mathematicians and statisticians in this area has been on quantitative geometric data modeling. In order to effectively analyze large data and obtain meaningful statistical inference, the underlying geometric models need to be sufficiently simple. The proposal suggests mathematical paradigms for such effective geometric models. It plans to develop rigorous mathematical theory for these paradigms combined with carefully designed numerical strategies addressing specific and important applications. Despite the recent progress in this area, there are many open directions, several of which this research project addresses.<br/><br/>More specifically, the proposal focuses on several important directions of geometric data modeling. One direction aims to address modern issues in single robust subspace modeling with respect to new paradigms of learning and computation that have hardly been addressed so far in this setting. Another direction will explore important issues in modeling data by multiple subspaces or manifolds with new paradigms and perspectives. The proposal will also emphasize specific paradigms of low-rank and sparse modeling, which are induced by important applications, such as approximate nearest subspace for object recognition, improved feature tracking, structure from motion in computer vision, and sparse modeling in the atmospheric sciences."
250,1447491,BIGDATA: F: DKA: CSD: Topological Data Analysis and Machine-Learning with Community-Accepted Features,IIS,"OFFICE OF MULTIDISCIPLINARY AC, Big Data Science &Engineering",9/1/14,8/26/14,John Harer,NC,Duke University,Standard Grant,Jie Yang,8/31/19,"$599,508.00 ",Paul Bendich,john.harer@duke.edu,"2200 W. Main St, Suite 710",Durham,NC,277054010,9196843030,CSE,"1253, 8083","7433, 8083, 8251",$0.00 ,"This project develops a new set of techniques from topological data analysis (TDA) to enrich the analytical toolkit for big data problems. In particular, TDA is well-suited at picking up repetitive (even quasi-repetitive) patterns that exist at many different scale levels within a dataset. The developed techniques can be applied to many multimedia applications. For example, a system that can correctly recognize certain motion patterns from a large set of surveillance data can help to identify threatening situations as they arise. Or a methodology that takes short song snippets and extract indicators of genre similarity may eventually be used to suggest new sound patterns. <br/><br/>This project constructs an analytical pipeline for using TDA-ML (machine-learning) methods on features already in use in the multimedia research communities. Topological Data Analysis (TDA) is almost fifteen years old. One of its key tools is the persistence diagram (PD), a compact and robust summary of the low-dimensional multi-scale topological and geometric information in a high-dimensional point cloud. Crucially, this information is extracted without need for dimension reduction. Over the last few years, two exciting developments have enriched TDA. First, theoretical and practical work on algorithms and implementations has enabled the fast computation of large numbers of PDs. Second, a coherent methodology has developed to do machine-learning (ML) with PDs as features, with several examples showing that one can augment more-standard feature sets with PD features, and find interesting signal that was not apparent before. This research discovers compelling signals in these features which are not previously apparent, but are immediately understandable because of the choice of features. The research team investigates both video and audio features. The ML methods are used to further measure importance of the features in a given context."
251,1423210,III: Small: Automatic Database Management System Tuning Through Large-scale Machine Learning,IIS,Info Integration & Informatics,8/1/14,8/18/14,Andrew Pavlo,PA,Carnegie-Mellon University,Standard Grant,Aidong Zhang,7/31/18,"$499,674.00 ",Geoffrey Gordon,pavlo@cs.cmu.edu,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,CSE,7364,"7364, 7923",$0.00 ,"The ability to collect, process, and analyze large amounts of data is paramount for being able to extrapolate new knowledge in business, scientific, and medical applications. Database management systems (DBMSs) are the critical component of modern ""Big Data"" applications  because they are the central repository for all of this information.  But tuning a DBMS to perform well is historically a difficult task because they have hundreds of configuration ""knobs"" that control everything in the system, such as the amount of memory to use and how often data is written. Getting these settings wrong will prevent the system from answering questions about data in a reasonable amount of time or even cause it to lose data. Many organizations resort to hiring experts to configure these knobs, but this is prohibitively expensive. Personnel cost is estimated to be almost 50% of the total ownership cost of a DBMS, and many administrators spend nearly a quarter of their time on these tuning activities. Furthermore, as databases grow in both size and complexity, optimizing a DBMS to meet the needs of new applications has surpassed the abilities of even the best human experts. Thus, the goal of this proposal is to develop the foundation and corresponding practical techniques for the automatic configuration of DBMSs by using machine learning on large-scale collections of historical performance data. Our approach will differ from previous work in that we seek to reduce the amount of time that is needed to train the algorithms that tune the DBMS for each  application by relying on knowledge gained from previous tuning  efforts. The results from this work will allow anyone to deploy a DBMS that is able to handle large amounts of data and more complex workloads without any expertise in database administration.<br/><br/>Achieving good performance in a database management system (DBMS) is non-trivial because they are complex systems with many tunable options that control nearly all aspects of their runtime operation. Getting this tuning right is critical for modern high-volume and high-throughput workloads, as the performance gains can be significant. As such, many organizations resort to hiring an expensive database administrator to manually tune their DBMS. But the size and complexity of databases have now surpassed the abilities of even the best human experts. Hence, we plan to develop automatic techniques for tuning and optimizing DBMS configurations for a broad class of application workloads. We will explore the foundations of using machine learning to scale DBMSs for larger data sets, thereby removing a major impediment in deriving the full benefits of data-driven decision making applications. The crux of our approach is to map an arbitrary application's workload to features of one or more canonical benchmarks that best represents the workload's properties, and then to collect performance data from the DBMS using that benchmark. This data is then used to train models that will allow us to identify the dependencies between knobs and their effects on the DBMS. From this, the models will select a near-optimal knob setting for the application. This differs from earlier work that focused on optimizing a single DBMS installation in isolation and are unable to leverage knowledge gained from previous tuning efforts. Our approach will not require the user to generate a large sample data set of (potentially  expensive) experiments to derive the proper configuration.<br/><br/>For further information see project web site at:  http://oltpbenchmark.com"
252,1350954,"CAREER: Exploiting low-dimensional structure in data for more effective, efficient and interactive machine intelligence",CCF,Comm & Information Foundations,7/1/14,7/13/18,Christopher Rozell,GA,Georgia Tech Research Corporation,Continuing Grant,Phillip Regalia,6/30/21,"$474,839.00 ",,crozell@gatech.edu,Office of Sponsored Programs,Atlanta,GA,303320420,4048944819,CSE,7797,"1045, 7495, 7936",$0.00 ,"The rapid increase in sensor data is revolutionizing many areas of technology, defense, and scientific discovery.  Fortunately, despite data being high-dimensional, various aspects of the data can frequently be characterized as having low-dimensional geometric structure. This research project dramatically improves machine intelligence by exploiting this  geometric structure for more effective, efficient and interactive data analysis systems.  Complementary to these technical objectives, this project also aims to engage, recruit, and educate a diverse collection of students to STEM careers by  developing novel curricular and outreach materials that illustrate how mathematics can be used in information systems.  The potential benefits of this project are wide ranging in areas where data plays a fundamental role.<br/><br/>Improving machine intelligence requires understanding how to best exploit the underlying low-dimensional structure in data for a given type of task, and this project is guided by three research objectives toward this goal.  The first objective enhances machine effectiveness by exploiting the fact that multiple observations of the same phenomenon are often related by movement along a manifold, with a particular focus on the canonical computer vision problem of invariant object recognition.  The second objective seeks to improve computational efficiency by developing dimensionality reduction techniques for manifold-modeled data that preserves information about nonlinear feature-space mappings.  The third objective seeks to leverage interactivity to fully ""close the loop"" between between humans and machines while learning low-dimensional information from a human expert (an extension of the active learning paradigm).  The project also pursues two educational objectives, including developing curriculum modules for pre-college outreach and integrating neural systems content into the ECE curriculum to illustrate the connections between quantitative methods and intelligent systems."
253,1450933,EAGER: Studying Emotional Responses of Children with Autism in Interaction with Facially Expressive Social Robots,IIS,Robust Intelligence,9/1/14,8/22/14,Mohammad Mahoor,CO,University of Denver,Standard Grant,Reid Simmons,8/31/18,"$80,000.00 ",,mmahoor@du.edu,2199 S. University Blvd.,Denver,CO,802104711,3038712000,CSE,7495,"7495, 7916",$0.00 ,"Recent research suggests that children with autism exhibit certain positive social behaviors when interacting with robots when compared to their peers that do not interact with robots. These investigations suggest that interaction with robots may be a promising approach for rehabilitation of children with autism. Despite the positive signs and observations reported in the literature, research on using social robots for autism therapy is in its infancy and more fundamental and exploratory studies should be carried out before one can study the effectiveness and clinical impact of robots for autism therapy.<br/><br/>This project explores several research questions on emotional and behavioral response of children with autism when interacting with a facially expressive robot.  Representative questions include: (1) Do children with autism recognize facial expressions shown by an expressive robot similarly to typically developed (TD) children? (2) Should the robot use body gesture and movement in conjunction with facial expression to better convey emotion to children with autism? (3) How do physiological responses of Autistic and TD children vary in interaction with a human versus a robot? To address these questions, the research team plans to recruit a group of children diagnosed with High Functioning Autism and TD children to interact with a facially expressive robot. The sessions are video recorded and analyzed using state-of-the-art automatic computer vision algorithms for facial expression recognition. Children's physiological and emotional responses are also acquired using Electro Dermal Activity (EDA) sensors. The EDA signals are used to create a computational affective model to describe and compare the arousal (excitement) level of the two groups of children in interaction with the robot.  Results from this study can be used in designing robot-assisted therapy for human mental and social disabilities such as designing interventions for children suffering from autism, depression, or attention disorder. In addition, the affective model can impact the measurement of emotional conditions in experimental non-interactive protocols. Data generated in this project are a rich source for further analysis by other researchers."
254,1346087,SBIR Phase I: Mobile Indoor Localization and Navigation System Using Sensory Data with Data Mining and Machine Learning Techniques,IIP,SMALL BUSINESS PHASE I,1/1/14,12/6/13,Benjamin Balaguer,CA,Intelligent Computer Programming Labs Inc.,Standard Grant,Peter Atherton,6/30/14,"$128,004.00 ",,bbalaguer@icplabs.com,413 Saddle Rock Lane,Rio Vista,CA,945710000,4152460889,ENG,5371,"5371, 8033",$0.00 ,"This Small Business Innovation Research (SBIR) Phase I project will explore the feasibility of an indoor localization technology that works efficiently and accurately when GPS does not. GPS receivers, although universally adopted by the general public, do not work indoors and suffer from inaccuracies of up to 25 meters in outdoor urban environments. By analyzing and processing data generated from WiFi, bluetooth, cellphone signals, magnetometers, accelerometers, compasses, and gyroscopes, a building's sensory blueprint can be created. The building's sensory blueprint can then be exploited to localize people holding smart mobile devices. The research will consist in investigating, designing, implementing, and validating the following: (i) a motion model capable of detecting a user's movement with accelerometers, compasses, and gyroscopes, (ii) a diverse set of different machine learning algorithms to be compared in terms of speed and localization accuracy, (iii) a probabilistic measurement model built from the best machine learning algorithm of phase (ii), and (iv) a Monte Carlo Localization (MCL) algorithm that combines the motion and measurement models. The final indoor localization algorithm will be implemented, tested in real-world conditions, and refined to prove the technology's superiority in terms of accuracy, speed, and applicability.<br/><br/>The broader impact/commercial potential of this project is that it could revolutionize the way buildings are used. The technology offers benefits to both end-users and companies. On one hand, users inside large buildings (e.g., supermarkets, shopping malls, hospitals, museums) will have access to floor plans, location-based information, and turn-by-turn directions directly on their mobile devices. On the other hand, companies will be able to analyze their customers' movements and provide them with targeted information or advertising when and where they need it. Other applications of the technology will provide societal benefits: (i) first responders will be able to accurately localize victims thus reducing response times and saving lives, (ii) building managers will be able to save up to 30% energy and money by conditioning each room in real-time based on the room's occupancy, (iii) people with disabilities will be able to use the technology for assistance such as finding wheelchair-accessible routes, and (iv) warehouse managers will be able to reduce order fulfillment time. Indoor localization will be, in the near future, as pervasive as GPS is today."
255,1407241,Graph-based Learning and Inference for Sparse Regularized Techniques,DMS,STATISTICS,8/15/14,8/24/16,Yufeng Liu,NC,University of North Carolina at Chapel Hill,Continuing grant,Gabor Szekely,7/31/18,"$120,000.00 ",Shu Lu,yfliu@email.unc.edu,104 AIRPORT DR STE 2200,CHAPEL HILL,NC,275991350,9199663411,MPS,1269,,$0.00 ,"Machine learning is a very active area of interdisciplinary research, closely related to statistics, optimization, and computer science. The goal of this project is to develop several cutting-edge machine learning techniques for solving high dimensional problems. The team plans to develop new techniques for estimating complex graphs and to establish inference procedures for sparse regression methods, using some recently developed tools in optimization. Techniques to be developed in this project have a wide range of applications in many disciplines. Such applications help to promote interdisciplinary research among statistics, operations research, and bioinformatics. Several students will be involved in the research activities.<br/><br/>Many machine learning techniques fit in the regularization framework. This project will develop several new regularized methods. In particular, the team will use sparse regularized tools for complex graphical model estimation. Furthermore, the team will build a new inference tool for sparse regularized regression methods such as the LASSO, by reformulating the LASSO problem as a stochastic variational inequality in optimization. State-of-the-art techniques in optimization will be introduced to the statistical community.  The researchers are committed to establishing both theoretical properties and efficient computational tools for the designed methods. Applications in various disciplines will help to generate new knowledge and inspirations from those disciplines."
256,1422549,CIF: Small: Sparsity in Quadratic Optimization through Low-Rank Approximations,CCF,Comm & Information Foundations,9/1/14,7/16/14,Georgios-Alex Dimakis,TX,University of Texas at Austin,Standard Grant,Phillip Regalia,8/31/18,"$425,239.00 ",,dimakis@austin.utexas.edu,"3925 W Braker Lane, Ste 3.340",Austin,TX,787595316,5124716424,CSE,7797,"7923, 7936",$0.00 ,"Nonnegativity and sparsity are highly desirable properties in data and signal decomposition algorithms. Nonnegativity is particularly relevant when the involved variables have a physical interpretation and ensures a separation of properties that interact in an additive manner. Nonnegativity and sparsity have been used in principal component analysis for numerous applications including bioinformatics, hyperspectral imaging and computer vision.<br/><br/>This research involves the study of sparsity and nonnegativity in quadratic optimization problems. This project develops novel algorithms for solving such problems under sparsity and nonnegativity constraints using a low-rank projection framework. This framework allows the development of novel algorithms for nonnegative sparse principal component analysis and matrix factorization. The research program relies on a fruitful synthesis of tools from information theory, combinatorics and linear algebra. The developed algorithms are both empirically outperforming the previous state of the art and have provable approximation guarantees."
257,1350870,CAREER: Transforming data analysis via new algorithms for feature extraction,CCF,ALGORITHMIC FOUNDATIONS,7/1/14,6/24/16,Luis Rademacher,OH,Ohio State University,Continuing grant,Jack Snoeyink,9/30/16,"$270,471.00 ",,lrademac@ucdavis.edu,Office of Sponsored Programs,Columbus,OH,432101016,6146888735,CSE,7796,"1045, 7926",$0.00 ,"Analysis and exploration of data, including classification, inference, and retrieval, are ubiquitous tasks in science and applied fields. Given any such task, a fundamental paradigm is the extraction of features that are relevant. In the design of algorithms for the analysis and exploration of data, feature extraction techniques act as basic building blocks or primitives that can be combined to model complex behavior. Some of the fundamental feature extraction tools include Principal Component Analysis (PCA), Independent Component Analysis (ICA), and half-space-based learning and classification. Data rarely satisfy the precise assumptions of these models and feature extraction tools, and combining these tools amplifies errors. This motivates the challenging task of designing new algorithms that are robust against noise and that can be combined as building blocks while keeping the error propagation under control.<br/><br/>The proposed work will:<br/>(1) Raise ICA from a very successful practical tool to an algorithmic primitive with strong theoretical guarantees and applicability to a rich family of problems beyond independence.<br/>(2) Find reasonable assumptions and algorithms that allow efficient learning of intersections of half-spaces.<br/>(3) Systematically study the following well-motivated refinement of PCA known as the subset selection problem. This refinement aims to select relevant features among the given features of the input data, unlike PCA, which creates new and possibly artificial features.<br/><br/>New feature extraction algorithms enhance the toolbox available to researchers in data-intensive fields such as biology, signal processing and computer vision. They also enable improved data analysis by practitioners in security, marketing, business and government processes, and essentially any field that involves the analysis of feature-rich data. The proposed work includes the implementation of the more practical algorithms.<br/><br/>Education and outreach aspects of this project include the mentoring of young researchers, the design of a new course for graduate and undergraduate students incorporating some of the PI's research, and the involvement of pre-college students and local communities into science and research."
258,1423591,III: Small: Collaborative Research: Robust Materials Genome Data Mining Framework for Prediction and Guidance of Nanoparticle Synthesis,IIS,"Info Integration & Informatics, CDS&E",8/15/14,6/5/17,Hua Wang,CO,Colorado School of Mines,Standard Grant,Sylvia Spengler,7/31/18,"$258,000.00 ","Andrew Herring, Christopher Maupin",huawang@mines.edu,1500 Illinois,Golden,CO,804011887,3032733000,CSE,"7364, 8084","7364, 7433, 7923, 8084, 9251",$0.00 ,"This proposal supports novel computer science research to provide foundations for new technologies.  The research objective of this proposal is to design new robust data mining and machine learning algorithms for solving the computational challenges in complex materials genome data mining. The Materials Genome Initiative research has been launched by U.S. government to discover, manufacture, and deploy advanced materials fast and low-cost, which holds great opportunities to address the challenges in clean energy, national security, and human welfare. However, the major computational challenges are the bottlenecks for comprehensive materials genome data analysis due to unprecedented scale and complexity. There is a critical need for new data mining and machine learning strategies to bridge the gap and facilitate the new materials discovery. To solve the key and challenging problems in mining such comprehensive heterogeneous materials genome data, the PIs propose to develop a novel robust data mining  and explore ways to integrate features from multiple data sources. The PIs will make the developed computational methods and tools online, available to the public. These methods and tools are expected to impact other material genome and biochemistry research and enable investigators working on new material design to effectively test performance prediction hypothesis. The proposed algorithms and tools are expected to help knowledge extraction for applications in broader scientific domains with massive high-dimensional and heterogonous data sets. This project will facilitate the development of novel educational tools to enhance several current courses.<br/><br/>The PIs propose to develop a novel robust data mining framework targeting to explore the following research tasks. First, the PIs will develop new computational tools to automate the material genome data processing, including missing values imputation by a new robust rank-k matrix completion method, robust tensor factorization based feature extraction approach, and informative nanoparticles selection using robust active learning model. Second, the PIs will investigate the new sparse multi-task multi-view learning model to integrate heterogeneous material characterizations for predicting the catalytic capabilities and associations to theoretical modeling measurements. Third, to predict the catalytic capabilities of the new synthesized nanoparticles, the PIs will design novel robust semi-supervised learning models by investigating elastic embedding, adaptive loss, L1-norm graph, and directed graph models.  The proposed sparse multi-view feature learning and robust semi-supervised learning models meet the critical needs of large-scale data analysis and integration. Such unique capabilities will enable new computational applications in a large number of research areas. It advances and thus extends the relationship between engineering innovation and computational analysis."
259,1451037,EAGER: Data-Driven Learning and Decision Making in Healthcare,CMMI,SERVICE ENTERPRISE SYSTEMS,8/1/14,7/31/14,Mohsen Bayati,CA,Stanford University,Standard Grant,Georgia-Ann Klutke,7/31/17,"$300,000.00 ",,bayati@stanford.edu,450 Jane Stanford Way,Stanford,CA,943052004,6507232300,ENG,1787,"076E, 078E, 7916, 8023",$0.00 ,"The goal of this EArly-Grant for Exploratory Research (EAGER) award is to create a new generation of learning and decision making tools in healthcare systems enabled by growing availability of data. Making data-driven learning and decision making an integral part of healthcare has profound potential to both improve the quality of medical care and to reduce healthcare costs. Despite this, state-of-the art methods are still insufficient for achieving broad acceptance, and significant impact of evidence-based decision making in medical settings is lacking. The fundamental methodological and scientific challenges that this project aims to investigate lie at the intersection of multiple disciplines: decision processes, machine learning, and statistics. This project helps advance this multidisciplinary research area and also develops teaching units for training a new cohort of researchers in this high impact space. On the other hand, this project has the potential to impact other domains beyond healthcare. In particular, the emphasis on dynamic learning and optimal decisions from large amount of data make its findings relevant to a number of domains such as finance, marketing, and electronic commerce.<br/> <br/>Due to advances in technology and government incentives a large amount of data on patients' conditions is becoming available electronically. On the other hand advances in machine learning and statistics allow the design of ""predictive systems"": algorithms that can learn generalizable patterns by sifting through a large number of patient profiles and provide accurate future forecasts about clinical adverse events, treatment outcomes, or demand for healthcare services. These predictions can be produced in real-time as new data is captured and can help guide decisions in healthcare systems. However, when statistical learning models are used to guide decisions such as medical treatments, clinicians make decisions based on their predictions that can change subsequent patient data that are collected and used to ""re-train"" the predictive system, thereby updating the forecast probabilities at the presence of new evidence. Current predictive systems in practice fall in one of the following two categories: (1) they are never re-trained post first installation; or (2) they are periodically re-trained with the arrival of new data. However, the first approach leads to poor forecasts when the new data arrives and circumstances around the decisions change. On the other hand, recent advances in the theory of multi-armed bandits, reinforcement learning, and their applications to digital advertising and recommendation systems indicate that the second approach can also lead to low quality predictions due to the endogeneity of the decision making process. The new dynamic-learning and decision making tools developed in this project will be robust against these challenges by proper modeling of the interactions between the data elements, the decisions, and the consequences of the decisions on new data."
260,1423056,III: Small: Collaborative Research: Robust Materials Genome Data Mining Framework for Prediction and Guidance of Nanoparticle Synthesis,IIS,Info Integration & Informatics,8/15/14,8/14/14,Junzhou Huang,TX,University of Texas at Arlington,Standard Grant,Sylvia Spengler,7/31/18,"$250,000.00 ",Feiping Nie,jzhuang@uta.edu,"701 S Nedderman Dr, Box 19145",Arlington,TX,760190145,8172722105,CSE,7364,"7364, 7923",$0.00 ,"This proposal supports novel computer science research to provide foundations for new technologies.  The research objective of this proposal is to design new robust data mining and machine learning algorithms for solving the computational challenges in complex materials genome data mining. The Materials Genome Initiative research has been launched by U.S. government to discover, manufacture, and deploy advanced materials fast and low-cost, which holds great opportunities to address the challenges in clean energy, national security, and human welfare. However, the major computational challenges are the bottlenecks for comprehensive materials genome data analysis due to unprecedented scale and complexity. There is a critical need for new data mining and machine learning strategies to bridge the gap and facilitate the new materials discovery. To solve the key and challenging problems in mining such comprehensive heterogeneous materials genome data, the PIs propose to develop a novel robust data mining  and explore ways to integrate features from multiple data sources. The PIs will make the developed computational methods and tools online, available to the public. These methods and tools are expected to impact other material genome and biochemistry research and enable investigators working on new material design to effectively test performance prediction hypothesis. The proposed algorithms and tools are expected to help knowledge extraction for applications in broader scientific domains with massive high-dimensional and heterogonous data sets. This project will facilitate the development of novel educational tools to enhance several current courses.<br/><br/>The PIs propose to develop a novel robust data mining framework targeting to explore the following research tasks. First, the PIs will develop new computational tools to automate the material genome data processing, including missing values imputation by a new robust rank-k matrix completion method, robust tensor factorization based feature extraction approach, and informative nanoparticles selection using robust active learning model. Second, the PIs will investigate the new sparse multi-task multi-view learning model to integrate heterogeneous material characterizations for predicting the catalytic capabilities and associations to theoretical modeling measurements. Third, to predict the catalytic capabilities of the new synthesized nanoparticles, the PIs will design novel robust semi-supervised learning models by investigating elastic embedding, adaptive loss, L1-norm graph, and directed graph models.  The proposed sparse multi-view feature learning and robust semi-supervised learning models meet the critical needs of large-scale data analysis and integration. Such unique capabilities will enable new computational applications in a large number of research areas. It advances and thus extends the relationship between engineering innovation and computational analysis."
261,1346448,STTR Phase I:  A Student Centered Adaptive Learning Engine,IIP,STTR Phase I,1/1/14,12/23/14,Mary Blink,PA,"TutorGen, Inc.",Standard Grant,Glenn H. Larsen,6/30/15,"$254,999.00 ",John Stamper,mjblink@tutorgen.com,505 Linden Ct,Mars,PA,160467167,7046992541,ENG,1505,"1505, 163E, 1654, 8031, 8032, 8039",$0.00 ,"This STTR Phase I project proposes to develop and validate a student centered adaptive learning engine that is focused on improving learning outcomes using data collected from new and existing educational technology projects combined with advanced technology to automatically generate adaptive capabilities, thus creating ready-to-go intelligent tutoring systems.  Providing adaptive instruction to students has been shown to be an effective way to improve student performance, yet very little educational software takes advantage of adaptive instruction due to high cost of creating adaptive content. This data-driven engine will significantly reduce the cost of adaptive learning by creating new methods of deriving intelligent tutoring capabilities from collected student data. Unlike pure machine learning solutions, this engine will allow for human input to maximize improvements through refinement over time. By using large datasets previously collected from existing tutors, these objectives can be tested and validated. The combination of human input with machine learning has the potential to make important gains in understanding student modeling. Finally, the engine will include new visualizations providing researchers, developers, and educators the tools to explore student data in ways that will allow for new insights into how students learn.<br/><br/>The broader/commercial impact of an adaptive learning engine includes the ability to connect to educational software providing a service to software companies thereby, improving and extending their new and/or existing software to adapt to individual students and maximize learning. Adding adaptive instruction to existing software has traditionally been difficult due to the high costs of creating adaptive instruction. This engine reduces the cost of offering adaptive instruction capabilities by providing connections to existing and new software. Existing software can add capabilities without complete redevelopment, creating whole new markets for existing educational software companies, while bringing intelligent tutors mainstream. The Engine will provide educators new tools to understand how students learn with software systems. Key software providers in the K-12, Higher Ed, and Corporate/Government educational markets will enhance the learning of their students while maintaining existing training and tutoring tools. Companies and organizations are looking for effective online teaching and training solutions that are flexible to meet varying learning needs and preferences of users to maximize learning efficiency. This engine will connect existing expertise and research with the innovative vision to expand the capabilities of intelligent tutoring systems to reach to a variety of markets using a human-centered, data-driven approach."
262,1426998,NRI: Large-Scale Collaborative Semantic Mapping using 3D Structure from Motion,IIS,"Information Technology Researc, NRI-National Robotics Initiati",9/1/14,6/24/16,Zsolt Kira,GA,Georgia Tech Applied Research Corporation,Continuing Grant,Jie Yang,8/31/18,"$600,000.00 ",Frank Dellaert,zkira@gatech.edu,Office of Sponsored Programs,Atlanta,GA,303320420,4048944819,CSE,"1640, 8013",8086,$0.00 ,"The project develops techniques to advance the state of the art in tackling the challenges associated with creating such representations using robots, namely issues related to the scalability and semantic interpretability of such maps. The research activities include advancement of knowledge in multiple fields, such as computer vision, structure from motion, robotics, and semantic mapping. The results have the potential for many societal applications including city planning, asset management, creation of historical records, and support for autonomous driving. The demonstration of the developed theoretical techniques for real-time interaction between humans and robots facilitated by a semantic map enables even greater societal benefit, for example for emergency management, crime prevention, and traffic management. Direct educational impact is anticipated for graduate students and the results are disseminated through both publications and software, allowing the community to leverage the results.<br/><br/>This research program advances real-time large-scale distributed semantic mapping of outdoor environments. Specifically, the research team is enabling real-time large-scale semantic mapping by using unsupervised object discovery, obviating the need for large sets of annotated videos for each object category which becomes prohibitive when dealing with hundreds of object categories. The research team frames this process within the structure from motion optimization framework, thereby leveraging geometric and multi-view constraints and features to increase reliability of object track association as well as category clustering. In addition to address scalability, the project develops a distributed, multi-robot system, allowing large teams of air and ground vehicles to cooperatively build a map of large geographic areas in reasonable time frames. Furthermore, the project develops techniques to make the maps more semantically-meaningful and hence interpretable by humans. To accomplish this objective, the research team uses automatic techniques to attach semantic labels to objects discovered in an unsupervised manner. Moreover, humans can interact with the system at multiple levels. Human users can refine both the object categories and semantic labels to increase their accuracy, as well as designate dynamic targets of interest and task robots to track them."
263,1451244,EAGER: Quantifying and Reducing Data Bias in Object Detection Using Physics-based Image Synthesis,IIS,Robust Intelligence,9/1/14,8/22/14,Kate Saenko,MA,University of Massachusetts Lowell,Standard Grant,Jie Yang,4/30/17,"$185,875.00 ",,saenko@bu.edu,Office of Research Admin.,Lowell,MA,18543692,9789344170,CSE,7495,"7495, 7916",$0.00 ,"This project develops improved computer vision methods for automatic recognition of arbitrary objects in images from realistic environments. Object recognition is typically performed by fitting a function that maps an image to likely object locations and labels. Such a function is fitted (trained) on a database of example images along with their human-assigned object locations and labels. This research can result in more accurate visual perception for socially relevant applications, such as robots performing household tasks, assisting the elderly, responding to disasters and quickly learning new manufacturing and service skills. It can also provide a common codebase for the wider community, new dataset challenges for domain adaptation problems, the dissemination of scientific and technical results and associated courseware, and specific outreach to ensure broad participation of underrepresented groups.<br/><br/>The specific research agenda is structured around two aims. The first aim is to establish bounds on the coverage of latent physical factors in datasets needed for human-level performance on arbitrary domains. The study involves both existing datasets and new datasets generated using graphics rendering techniques at various degrees of photorealism. The goal is to develop a theory of the physical complexity of a given dataset and how it affects generalization to real world object recognition tasks, with respect to a given image representation and learning framework. Physical parameters include but are not limited to: 3D shape, surface color, texture, background/scene, camera viewpoint, sensor noise, lighting, specularities and cast shadows. The second research aim is to learn image representations invariant to some of the physical causes of data bias. The goal is to develop model and representation learning methods that are able to learn from a combination of real and non-photorealistic synthetic data, and are resistant to common sources of data bias. The representations include simple edge-based descriptors, and more generally hierarchical representations based on layers of convolution and pooling operations."
264,1346457,STTR Phase I:  Rapid and Efficient Scene Modeling for Law Enforcement and Disaster Response,IIP,STTR Phase I,1/1/14,12/17/14,Sanjiv Singh,PA,"Near Earth Autonomy, Inc.",Standard Grant,Muralidharan Nair,6/30/15,"$254,800.00 ",Luis Ernesto Navarro-Serment,ssingh@realearth.us,150 N Lexington St.,Pittsburgh,PA,152082517,4125136110,ENG,1505,"1505, 163E, 6840, 8035, 9139, HPCC",$0.00 ,"This Small Business Technology Transfer Research (STTR) Phase I project will produce technology capable of constructing three-dimensional (3D) models of scenes accurately, rapidly, and at low cost. The technology leverages recent developments in computer vision and enhances them for operation under varying ambient conditions while increasing speed of model construction, thus enabling practical applications. Key system design requirements identified include: 1) ability to produce complete and accurate high fidelity models quickly; 2) robustness to varying ambient conditions; 3) ease of use by a non-specialist without extensive training; and 4) low cost. The proposed research objectives include: improvements to state-of-the-art algorithms for robust and efficient 3D modeling from images; the development of an approach for dense 3D reconstruction using different sensor modalities; the development of a sensor view point planning algorithm; the creation of techniques for the identification of missing model data; and the development of a system for merging data from different sources into a coherent 3D model of the environment. The work plan includes a thorough assessment of the system?s ability to reconstruct a scene accurately and completely, as well as the benefit of reconstructing scenes using sensors deployed by low-cost vehicles. <br/><br/>The broader impact/commercial potential of this project will be the widespread use of 3D modeling for scene documentation. Systems currently available for this application are expensive, slow, bulky, and their use requires special training. The technology resulting from this work will enable the production and commercialization of systems that are faster, more affordable, and easier to use by non-experts, thereby simplifying their adoption by a larger number of law enforcers, prosecutors, insurers, and other government agencies. In the transportation domain, for example, a speedy scene reconstruction is essential to restore the flow of traffic when accidents happen. On busy highways, traffic backup can grow at a rate of up to a mile per minute of delay in clearing the accident site. This currently limits the complete documentation of accidents to only those cases where fatalities occur. The technology proposed will allow the documentation of larger number of incidents. Additionally, a great reduction in the time and complexity involved in determining the cause of accidents is expected. The potential long-term benefits of this reduction include a more rapid evolution of regulation and policy to deal with chronic causes of traffic accidents, which will expedite the implementation of road safety mechanisms."
265,1416539,"SBIR Phase I: Improving Children's Motivation, Self-Efficacy, Science Learning and Reading Proficiency",IIP,SBIR Phase I,7/1/14,5/23/14,Ronald Cole,CO,Boulder Language Technologies,Standard Grant,Glenn H. Larsen,6/30/15,"$150,000.00 ",,rcole@boulderlearning.com,2690 Center Green Ct S Ste 200,Boulder,CO,803015406,3035799605,ENG,5371,"110E, 5371, 8031, 8032, 8039",$0.00 ,"This SBIR Phase I project's broader/commercial impact is the potential of the intelligent tutoring system to increase young learners' science motivation and achievement and to improve their fluent reading and comprehension skills.  The innovation thus addresses a critical national need to improve students' science learning and reading proficiency in our nation's schools, since national assessments of educational progress indicate that the majority of fourth, eighth and twelfth grade students are not proficient in either science or reading on standardized tests.  The innovation is thus designed to provide teachers with an accessible, affordable and highly effective learning tool that will improve their students' engagement, motivation and learning in both science and reading.  The projects' commercialization plan is designed to support widespread distribution of the innovation to schools in the U.S. and globally through existing channels, thus increasing the potential for broad societal impact.<br/><br/>The project aims to develop and demonstrate the feasibility of an intelligent tutoring system that will help children learn to reason about science and comprehend science texts through conversational interactions with a virtual tutor.  Successful outcomes of the project will lead to new insights and contribute to scientific knowledge about how advanced human language technologies, character animation technologies and computer vision technologies can be combined and integrated into intelligent tutoring systems to optimize young learners' engagement, motivation, self-efficacy and learning.  The project is based on theory and scientific research on how children learn through social interaction in multimedia environments, and prior research by the project team that improved children's motivation to learn science and their science achievement through conversations with a virtual science tutor.  Successful outcomes of the SBIR Phase I project will extend this prior research and advance scientific knowledge by demonstrating the feasibility and promise of an intelligent tutoring system that tracks and interprets students? eye movements to improve science learning, oral reading fluency and reading comprehension."
266,1454855,EAGER: Collaborative Research: Advanced Machine Learning for Prediction of Preterm Birth,IIS,"Info Integration & Informatics, Smart and Connected Health",9/1/14,8/26/14,Ansaf Salleb-Aouissi,NY,Columbia University,Standard Grant,Maria Zemankova,8/31/17,"$175,457.00 ","Bob Carpenter, Ronald Wapner",as2933@columbia.edu,2960 Broadway,NEW YORK,NY,100276902,2128546851,CSE,"7364, 8018","7364, 7916, 8018",$0.00 ,"The United States spends over 26 billion dollars per annum on the delivery and care of the 12-13% of infants who are born preterm. As preterm birth (PTB) is a major public health problem with profound implications on society, there would be extreme value in being able to identify women at risk of preterm birth during the course of their pregnancy. Previous predictive approaches have been largely unsuccessful since they have focused on a limited number of well described risk factors known to be correlated with preterm birth (e.g. prior preterm birth, race, and infection) and less on combining multiple factors. The latter approach is necessary to understand the complex etiologies of preterm birth. While identifying individual PTB risk factors has brought insight into the problem and has led in some cases to successful treatments such as progesterone for women with a previous preterm birth, this has only a limited impact on the overall frequency since many at risk patients, such as first time mothers (nulliparous), go untreated. Today, there is still no widely tested prediction system that combines well-known PTB factors and is clinically useful. There is, however, a global awareness of the need to discover and integrate the complex etiologies of prematurity in order to predict women at risk. Significant efforts have been made in the last couple of decades to collect large curated datasets of pregnant women. Previous studies on these datasets used relatively straightforward biostatitistical methodologies such as relative risk assessments to measure associations between factors and PTB. However, risk factors are studied independently of each other, which does not account for the multifactorial complexity of PTB. This exploratory project aims to investigate the value of more advanced machine learning methods by simultaneously considering all the factors, to develop better predictive methods. <br/><br/>The PTB data acquired in the context of this project brings together Electronic Health Records (EHRs) for mothers and their babies along with well-curated NIH data.  The data is rich with structured clinical data and unstructured free text that require manual feature extraction. This project, largely motivated by the PTB problem, has two main goals: <br/>(1) Improving the quality and aggregation the annotations for heterogeneous data. The researchers aim to capture socioeconomic, psychological and behavioral risk factors documented in the text of clinical notes via studying the process of manual feature extraction by human annotators. State-of-the-art methods either rely on the expertise of the annotator and/or the difficulty of the instance but ignore the variability in the quality of labeling over time due to fatigue, boredom, or knowledge. To improve the annotations, the project will develop a novel Bayesian framework for human labeling of unstructured data.  The Bayesian model will embed a complete set of parameters including the prevalence of each class, difficulty of the instance and variability in the quality of annotation during the process. If the model construction is successful, then the developed framework will replace ad-hoc heuristics into a well-designed process for producing high quality annotations. This framework would allow extracting reliable features from the clinical text for subsequent analyses in devising PTB prediction models.<br/>(2) Developing predictive models for multiple data spaces. To leverage all of the existing data, the project will investigate the value of using Vapnik's paradigm of Learning Using Privileged Information (LUPI) in the context of preterm birth. Privileged information is a data that is available for training models but is not available for test examples. Data in this project come with two potential privileged information spaces namely the clinical notes and the space of future events.   NICU data is an example of future event privileged information, which is only available for a subset of the examples (only premature babies requiring intensive care stay in the NICU). It has been shown that LUPI not only induces a better decision rule, it also increases the rate of convergence of the algorithm, hence requiring fewer training examples. This is a compelling property in the case of PTB prediction because of the rate of PTB. The project will extend LUPI into a powerful and applicable framework to handle the two spaces of privileged information, while developing spline-generating kernels, to manage LUPI's high computational cost. If successful, this proof-of-concept is expected to yield efficient and widely applicable LUPI algorithms in domains where privileged information is available, such as the financial domain and many other medical applications. <br/>The developed software, publications and datasets resulting from this project will be made publicly available to the research community through the project website (http://www1.ccls.columbia.edu/~ansaf/CING/PTB/)."
267,1403502,COLLABORATIVE RESEARCH: ARWED - AUGMENTED PERCEPTION FOR UPPER-LIMB REHABILITATION,CBET,Disability & Rehab Engineering,8/15/14,8/6/14,Amarnath Banerjee,TX,Texas A&M Engineering Experiment Station,Standard Grant,Aleksandr Simonian,7/31/18,"$199,976.00 ",John Buchanan,banerjee@tamu.edu,400 Harvey Mitchell Pkwy S,College Station,TX,778454645,9798626777,ENG,5342,010E,$0.00 ,"PI: Banerjee, Amarnath/Perez Gracia, Alba/Robson, Nina<br/>Proposal Number: 1403502/1403688/1404011<br/>Title: Collaborative Research: ARWED - Augmented Perception for Upper-Limb Rehabilitation<br/><br/>Broader Significance & Importance<br/>   Training and retraining the movement of individuals suffering loss of motor ability due to stroke is a challenging task. In many cases, only partial success is accomplished after long training sessions. Given the limitations of recovery for post-stroke patients, it is imperative that better tools and methods for retraining be developed. Even though several training hypotheses exist, such as need for an exact repetition of the training pattern, the solutions are still to be found. Overcoming some of these obstacles is the goal of this proposal. Recent studies show a direct link between human action perception and action execution. There is some evidence that the visual observation of human actions has an effect on the movement of the observer. This priming effect is reduced, or not present, when the motion is executed by a non-human device, such as a robot. This proposal aims to develop and test a novel wearable system for the training of the human arm. The system will allow the user to perceive the device as part of their upper limb (hence closing the perception/action loop). Training protocols based on observational learning findings will be implemented with the ARWED in order to develop it as rehabilitation training device for motor recovery in post-stroke patients.<br/><br/>Technical description<br/>   The intellectual merit centers on exploiting the link between action-observation and action execution in order to develop training protocols to facilitate rehabilitation following stroke. Currently, there are limited systems that utilize virtual reality in the relearning of biological movements. The proposed development of augmented wearable system (ARWED) requires solving several challenges in computer vision/modeling, and robot kinematic mapping. Testing the effectiveness of the device on priming the perception-action effect will require combined expertise from the areas of human kinematics, signal analysis, virtual reality, robotic fault recovery theory and rehabilitation. Therefore, the theoretical contributions emerging from this multidisciplinary collaborative research team will advance knowledge and understanding not only within the medical field, but also across the above-mentioned research areas.  The broader impact is based on the characteristics of the device to be developed, with an expanded ability in the training and re-training of patients with motor disabilities. Efficient design and manipulation will make the proposed ARWED system a reliable solution, which will be broadly utilized by medical professionals working in rehabilitation, sports therapy and convalescence. The proposed research will provide effective tools for the training and physical rehabilitation of patients with limb limitations at any scale, ranging from individuals suffering partial loss of motor ability to those with severe limitations in mobility due to strokes, birth defects or accidents. Motor learning theory shows that reducing feedback during practice benefits long term retention of motor skill training. Observational learning may offer greater benefits regarding transfer to ADLs, in comparison to robotic-based stroke training. Thus, a success indicator for these patients would be the beneficial transfer of training from the un-affected to affected limb or vice-versa. The ARWED system is expected to advance significantly the fundamentals of engineering and scientific knowledge, by implementing the device in experimental and educational work on cognition, telemanipulation and virtual reality. The medical community would also benefit from the development of the ARWED by furthering the understanding of how training using technology may enhance the recovery of motor control in diverse populations while providing a novel intervention that may prove more effective than what is currently available. The collaborative nature of the research team will lead to the education of undergraduate and graduate students in the areas of Engineering and Kinesiology. In the long run, the outcomes of the proposal will facilitate the communication between students that want to enter Physical and Occupational Therapy professions and those that want to design and develop mechanical devices that can aid in human recovery following neurological injury."
268,1409543,III: Medium: SimSQL: A Database System Supporting Implementation and Execution of Distributed Machine Learning Codes,IIS,Info Integration & Informatics,9/1/14,7/23/19,Christopher Jermaine,TX,William Marsh Rice University,Continuing Grant,Sylvia Spengler,3/31/20,"$1,200,000.00 ",,Christopher.m.jermaine@rice.edu,6100 MAIN ST,Houston,TX,770051827,7133484820,CSE,7364,"7364, 7924",$0.00 ,"Statistical machine learning (ML) is a commonly-applied framework for analyzing very large data sets. In statistical ML, the goal is to learn a statistical model that can be used to understand the data, find patterns, or make predictions.  Thus, many new software systems have been designed to support easy implementation and fast execution of parallel/distributed ML computer codes over large data sets.  Almost all of those systems are ""non-relational"" in the sense that they utilize data and programming models that are very different from today's relational database management systems.  Still, the attractiveness of the relational or database-oriented approach to data processing persists. For example, codes running on top of a database are declarative, so a programmer need only be concerned with what he or she wants, and not how to obtain it. This makes it easier to write codes and get them to run in a distributed environment, enabling a strong separation between the code and the database data processing algorithms, storage, hardware, and indexing, and even from the database schema.  Further, much of the world's structured data sits in relational databases, and extracting anything more than a small subsample of a large data set for external use is typically a non-starter. Being able to execute a ML inference code within the database, using the database engine, would greatly increase applicability of statistical ML.  <br/><br/>This project will perform the fundamental research necessary to make ML-in-the-database a mature technology. All of the ideas developed by the project will be prototyped, evaluated, and distributed within the context of SimSQL, which is a parallel, relational database system, augmented with the ability to perform ""stochastic analytics"". This means that SimSQL has special facilities that allow a user to define special database tables that have simulated data---these are data that are not actually stored in the database, but are produced by calls to statistical distributions.  Since tables of simulated data in SimSQL can have such recursive dependencies, it is easy to use SimSQL to run stochastic ML inference algorithms (such as MCMC) over ""Big Data"". Research tasks include increasing the level of performance of SimSQL by exploiting the optimization opportunities presented by large-scale, iterative, ML computations. They also include expanding the types of ML inference algorithms that can easily be specified in SimSQL's SQL dialect, making SimSQL applicable for various stochastic inference algorithms such as MCMC (Markov Chain Monte Carlo) and Monte Carlo EM (Expectation Maximization). Further, the project will investigate automatically compiling R and BUGS-like ML algorithm specifications into SimSQL SQL. All of the software developed by the project will be available open source under the Apache license.  The code can be downloaded from (and more information can be found at) http://cmj4.web.rice.edu/SimSQL/SimSQL.html"
269,1447676,"BIGDATA: F: DKA: Collaborative Research: Theory and Algorithms for Parallel Probabilistic Inference with Big Data, via Big Model, in Realistic Distributed Computing Environments",IIS,Big Data Science &Engineering,9/1/14,8/25/14,Eric Xing,PA,Carnegie-Mellon University,Standard Grant,Jie Yang,8/31/18,"$500,000.00 ",,epxing@cs.cmu.edu,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,CSE,8083,"7433, 8083",$0.00 ,"This project develops a new framework that enables machine learning (ML) systems to automatically comprehend and mine massive and complex data via parallel Bayesian inference on large computer clusters. The research has a profound impact on the practice and direction of Big Learning. The developed technologies have a catalytic effect on both ML research and applications: ML scientists are able to rapidly experiment on novel, cutting-edge ML models with minimal programming effort, unhindered by the limitations of single machines. Researchers from other fields, like biology and social sciences, are able to run contemporary advanced ML methods that transcend the capabilities of simple models, yielding new scientific insights on data whose size would otherwise be daunting. Data scientists at small start-ups are able to conduct ML analytics with complex models, putting their capabilities on par with huge companies possessing dedicated engineering and infrastructure teams. Students and beginners are able to witness distributed ML in action with just a few lines of code, driving ML education to new heights. <br/><br/>Technically, this research focuses on scaling up and parallelizing Bayesian machine learning, which provides a powerful, elegant and theoretically justified framework for modeling a wide variety of datasets.  The research team develops a suite of complementary distributed inference algorithms for hierarchical Bayesian models, which cover most commonly used Bayesian ML methods. The project focuses on combining speed and scalability with theoretical guarantees that allow us to assess the accuracy of the resulting methods, and allow practitioners to make trade-offs between speed and accuracy. Rather than focus on a few disconnected models, the project develops techniques applicable to a broad spectrum of hierarchical Bayesian models, resulting in a toolkit of building blocks that can be combined as needed for arbitrary probabilistic models - be they parametric or nonparametric, discriminative or generative. This is in contrast to much existing work on parallel inference, which tends to focus on parallelization in a specific model and cannot be easily extended. The project provides a solid algorithmic foundation for learning on Big Data with powerful models. The research contributes to democratizing advanced and large-scale ML methods for broad applications, by offering the user and developer community a library of general-purpose parallelizable algorithms for working on diverse problems using computer clusters and the cloud, bridging the gap between practical needs from data and basic research in ML."
270,1447721,"BIGDATA: F: DKA: Collaborative Research: Theory and Algorithms for Parallel Probabilistic Inference with Big Data, via Big Model, in Realistic Distributed Computing Environments",IIS,Big Data Science &Engineering,9/1/14,8/25/14,Sinead Williamson,TX,University of Texas at Austin,Standard Grant,Jie Yang,8/31/18,"$300,000.00 ",,sinead.williamson@mccombs.utexas.edu,"3925 W Braker Lane, Ste 3.340",Austin,TX,787595316,5124716424,CSE,8083,"7433, 8083",$0.00 ,"This project develops a new framework that enables machine learning (ML) systems to automatically comprehend and mine massive and complex data via parallel Bayesian inference on large computer clusters. The research has a profound impact on the practice and direction of Big Learning. The developed technologies have a catalytic effect on both ML research and applications: ML scientists are able to rapidly experiment on novel, cutting-edge ML models with minimal programming effort, unhindered by the limitations of single machines. Researchers from other fields, like biology and social sciences, are able to run contemporary advanced ML methods that transcend the capabilities of simple models, yielding new scientific insights on data whose size would otherwise be daunting. Data scientists at small start-ups are able to conduct ML analytics with complex models, putting their capabilities on par with huge companies possessing dedicated engineering and infrastructure teams. Students and beginners are able to witness distributed ML in action with just a few lines of code, driving ML education to new heights. <br/><br/>Technically, this research focuses on scaling up and parallelizing Bayesian machine learning, which provides a powerful, elegant and theoretically justified framework for modeling a wide variety of datasets.  The research team develops a suite of complementary distributed inference algorithms for hierarchical Bayesian models, which cover most commonly used Bayesian ML methods. The project focuses on combining speed and scalability with theoretical guarantees that allow us to assess the accuracy of the resulting methods, and allow practitioners to make trade-offs between speed and accuracy. Rather than focus on a few disconnected models, the project develops techniques applicable to a broad spectrum of hierarchical Bayesian models, resulting in a toolkit of building blocks that can be combined as needed for arbitrary probabilistic models - be they parametric or nonparametric, discriminative or generative. This is in contrast to much existing work on parallel inference, which tends to focus on parallelization in a specific model and cannot be easily extended. The project provides a solid algorithmic foundation for learning on Big Data with powerful models. The research contributes to democratizing advanced and large-scale ML methods for broad applications, by offering the user and developer community a library of general-purpose parallelizable algorithms for working on diverse problems using computer clusters and the cloud, bridging the gap between practical needs from data and basic research in ML."
271,1407205,SCH: EXP: LifeRhythm: A Framework for Automatic and Pervasive Depression Screening Using Smartphones,IIS,Smart and Connected Health,8/1/14,5/5/17,Bing Wang,CT,University of Connecticut,Standard Grant,Wendy Nilsen,7/31/18,"$745,215.00 ","Alexander Russell, Jinbo Bi, Alok Banga, Jayesh Kamath",bing@uconn.edu,438 Whitney Road Ext.,Storrs,CT,62691133,8604863622,CSE,8018,"8018, 8061, 9251",$0.00 ,"Because of its high prevalence and significant health and economic impacts, depression is a profound public health problem. Currently, screening for depression is based on physician-administered interview tools or patient self-report. While physician-administered tools are more authoritative, availability is constrained both by cost and lack of access to trained mental health professionals. Patient self-reporting, on the other hand, suffers from recall bias and inconsistent patient participation. In particular, neither approach satisfactorily addresses the chronic and recurring nature of depression that requires frequent assessment for monitoring onset and progress. To address depression as a public health problem, there is urgent need for an objective, accurate, easily accessible and scalable depression screening tool. The ubiquitous adoption of smartphones around the world creates new opportunities in automatic and pervasive screening of depression across large populations. The education plan of this proposal includes developing and enhancing various undergraduate and graduate-level courses, as well as disseminating the results to medical students through clinical supervision and increasing the participation from under-represented groups in research and outreach activities. <br/><br/>The goal of this project is to develop LifeRhythm, an automated system for automatic and pervasive depression screening using smartphone data. LifeRhythm continuously monitors the behavioral rhythms of individuals through their smartphones, extracts normalized features from the raw data, and applies multiple machine-learning models for real-time diagnosis. The project applies LifeRhythm to two settings that have complementary strengths. The first setting uses ""high-resolution"" sensing data collected from smartphones, which provides extremely rich and descriptive behavioral data, allowing the best leverage for machine learning models. The second setting uses ""low-resolution"" wireless association meta-data collected passively from large-scale WiFi networks, which eliminates the need of data collection on smartphones and can be especially valuable for a large organization, where it could automatically provide depression screening of tens of thousands of people simultaneously at very little cost. Development of LifeRhythm will be coupled with several tightly related machine-learning research efforts, including novel techniques for collaborative prediction, integrative learning, modeling of temporal dynamics, and model refinement using multiplicative-weights-based techniques. Though this proposal is primarily focused on development of screening tools, future work could naturally develop an associated intervention program. In addition, this research may lead to methodologies that are applicable to other mood disorders such as bipolar illness. The broader impacts will include dissemination of research results (and the annotated dataset) to the technical communities. The project web site (http://nlab.engr.uconn.edu/sch.html) provides access to additional information on research and results."
272,1454814,EAGER: Collaborative Research: Advanced Machine Learning for Prediction of Preterm Birth,IIS,"Info Integration & Informatics, Smart and Connected Health",9/1/14,8/26/14,Anita Raja,NY,Cooper Union,Standard Grant,Maria Zemankova,8/31/17,"$30,643.00 ",,anita.raja@hunter.cuny.edu,"30 Cooper Square, 8th Floor",New York,NY,100030000,2123534138,CSE,"7364, 8018","7364, 7916, 8018",$0.00 ,"The United States spends over 26 billion dollars per annum on the delivery and care of the 12-13% of infants who are born preterm. As preterm birth (PTB) is a major public health problem with profound implications on society, there would be extreme value in being able to identify women at risk of preterm birth during the course of their pregnancy. Previous predictive approaches have been largely unsuccessful since they have focused on a limited number of well described risk factors known to be correlated with preterm birth (e.g. prior preterm birth, race, and infection) and less on combining multiple factors. The latter approach is necessary to understand the complex etiologies of preterm birth. While identifying individual PTB risk factors has brought insight into the problem and has led in some cases to successful treatments such as progesterone for women with a previous preterm birth, this has only a limited impact on the overall frequency since many at risk patients, such as first time mothers (nulliparous), go untreated. Today, there is still no widely tested prediction system that combines well-known PTB factors and is clinically useful. There is, however, a global awareness of the need to discover and integrate the complex etiologies of prematurity in order to predict women at risk. Significant efforts have been made in the last couple of decades to collect large curated datasets of pregnant women. Previous studies on these datasets used relatively straightforward biostatitistical methodologies such as relative risk assessments to measure associations between factors and PTB. However, risk factors are studied independently of each other, which does not account for the multifactorial complexity of PTB. This exploratory project aims to investigate the value of more advanced machine learning methods by simultaneously considering all the factors, to develop better predictive methods. <br/><br/>The PTB data acquired in the context of this project brings together Electronic Health Records (EHRs) for mothers and their babies along with well-curated NIH data.  The data is rich with structured clinical data and unstructured free text that require manual feature extraction. This project, largely motivated by the PTB problem, has two main goals: <br/>(1) Improving the quality and aggregation the annotations for heterogeneous data. The researchers aim to capture socioeconomic, psychological and behavioral risk factors documented in the text of clinical notes via studying the process of manual feature extraction by human annotators. State-of-the-art methods either rely on the expertise of the annotator and/or the difficulty of the instance but ignore the variability in the quality of labeling over time due to fatigue, boredom, or knowledge. To improve the annotations, the project will develop a novel Bayesian framework for human labeling of unstructured data.  The Bayesian model will embed a complete set of parameters including the prevalence of each class, difficulty of the instance and variability in the quality of annotation during the process. If the model construction is successful, then the developed framework will replace ad-hoc heuristics into a well-designed process for producing high quality annotations. This framework would allow extracting reliable features from the clinical text for subsequent analyses in devising PTB prediction models.<br/>(2) Developing predictive models for multiple data spaces. To leverage all of the existing data, the project will investigate the value of using Vapnik's paradigm of Learning Using Privileged Information (LUPI) in the context of preterm birth. Privileged information is a data that is available for training models but is not available for test examples. Data in this project come with two potential privileged information spaces namely the clinical notes and the space of future events.   NICU data is an example of future event privileged information, which is only available for a subset of the examples (only premature babies requiring intensive care stay in the NICU). It has been shown that LUPI not only induces a better decision rule, it also increases the rate of convergence of the algorithm, hence requiring fewer training examples. This is a compelling property in the case of PTB prediction because of the rate of PTB. The project will extend LUPI into a powerful and applicable framework to handle the two spaces of privileged information, while developing spline-generating kernels, to manage LUPI's high computational cost. If successful, this proof-of-concept is expected to yield efficient and widely applicable LUPI algorithms in domains where privileged information is available, such as the financial domain and many other medical applications. <br/>The developed software, publications and datasets resulting from this project will be made publicly available to the research community through the project website (http://www1.ccls.columbia.edu/~ansaf/CING/PTB/)."
273,1421391,"RI: Small: When Algorithms Trade: Dynamics, Limits, and Economic Implications",IIS,Robust Intelligence,7/1/14,1/23/15,Michael Wellman,MI,Regents of the University of Michigan - Ann Arbor,Standard Grant,James Donlon,6/30/18,"$516,018.00 ",Jacob Abernethy,wellman@umich.edu,3003 South State St. Room 1062,Ann Arbor,MI,481091274,7347636438,CSE,7495,"7495, 7923, 9251",$0.00 ,"Recent years have seen a dramatic increase in algorithmic trading, to the point that the majority of orders in major equity exchanges today are generated by machines without direct human control.  Experience has shown that this automation--particularly at the extremes of high-frequency trading (HFT)--makes a qualitative difference, due to the unprecedented speed and lack of direct human control.  The emergence of HFT raises fundamental issues for the efficiency, fairness, and stability of financial markets.  The practice is highly controversial, yet the lack of scientific understanding of HFT's implications impedes informed public debate bearing on the question. <br/><br/>Algorithmic trading can also be viewed as herald of a wave of automated behavior with far-reaching effects in a plethora of domains.  Methodological improvements from this project advance our ability to anticipate effects of autonomous agents in other areas of major economic and societal impact.<br/><br/>This project conducts a systematic computational study of algorithmic trading.  The investigation combines online learning and optimization techniques from the point of view of theoretical machine learning and agent-based modeling (ABM) approaches to develop models of financial trading substantially more comprehensive and robust than heretofore possible.  Modeling financial markets as multiagent systems affords heterogeneity: traders differing in objectives, information (access to data and observability of the environment), and response capability (processing and execution speed).  Learning and decision-theoretic methods provide a principled basis for defining adaptive strategies that are effective across a broad range of operating conditions and possess guarantees in adversarial environments.  Evidence on algorithmic trading implications is derived through systematic computational experimentation.<br/><br/>The project contributes both to scientific knowledge about algorithmic trading, and to agent-based methodology for analyzing complex strategic domains.  One particularly novel feature of this study is its emphasis on the effect of temporal structure (e.g., communication latency patterns, adaptive strategies) on the dynamics of algorithm interaction.  The agent-based methodology developed here provides a unifying framework for selecting among candidate behaviors based on specified solution concepts, such as game-theoretic or evolutionary equilibria.  It exploits ideas from several fields, including simulation modeling, stochastic search, statistical analysis, and machine learning."
274,1411646,Variational Methods for Materials and Imaging Sciences,DMS,"OFFICE OF MULTIDISCIPLINARY AC, APPLIED MATHEMATICS",9/1/14,8/1/18,Irene Fonseca,PA,Carnegie-Mellon University,Continuing Grant,Victor Roytburd,8/31/20,"$1,222,302.00 ",,fonseca@andrew.cmu.edu,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,MPS,"1253, 1266","8396, 8609",$0.00 ,"The objectives of this project are the identification and pursuit of emerging areas of applied analysis, motivated by contemporary issues in imaging and materials science at the core of advances in high-end technology and of national scientific importance.  The two main topics of the project are<br/>(1) the mathematical study of modern semiconductors and nano structures, of pivotal importance in microelectric and optoelectronic technologies, such as reflective or anti-reflective coatings for optics, the fabrication of layers of insulators and semiconductors for integrated circuits, quantum well lasers, and<br/>(2) the analytical investigation of image segmentation and inpainting and recolorization for color images, fundamental to the advance of computer vision, medical imaging, film restoration, and scanning probe microscopy. <br/>Postdocs and graduate students are trained in the course of the project. <br/><br/>     Common features of the projects include the treatment of energies that involve terms of different dimensionality.  These often exhibit a large range of length and time scales, higher order derivatives, and discontinuous underlying fields.  Such features prevent the use of well understood functional analytic frameworks, they escape traditional mathematical theories, and they require state-of-the-art techniques, creative ideas, and the introduction of innovative mathematical tools.  The investigator and her collaborators use new and recently developed methods and a deep articulation of ideas in the calculus of variations, geometric measure theory, and nonlinear partial differential equations, to address problems that include in topic (1) epitaxy and the formation of quantum dots, the onset and propagation of dislocations, homogenization of composite materials, and in topic (2) signal denoising and detexturing, dejittering, inpainting, and recolorization.  These topics offer new opportunities for the integration of applied analysis in research and in the education of advanced graduate students and postdoctoral fellows, thus allowing for the training of a new generation of applied analysts at the forefront of contemporary mathematics as it interfaces with materials and imaging sciences."
275,1404011,COLLABORATIVE RESEARCH: ARWED - AUGMENTED PERCEPTION FOR UPPER-LIMB REHABILITATION,CBET,Disability & Rehab Engineering,8/15/14,7/30/18,Nina Robson,CA,California State University-Fullerton Foundation,Standard Grant,Aleksandr Simonian,7/31/19,"$83,212.00 ",,nrobson@fullerton.edu,1121 North State College Blvd.,Fullerton,CA,928313014,6572782106,ENG,5342,"010E, 9102, 9251",$0.00 ,"PI: Banerjee, Amarnath/Perez Gracia, Alba/Robson, Nina<br/>Proposal Number: 1403502/1403688/1404011<br/>Title: Collaborative Research: ARWED - Augmented Perception for Upper-Limb Rehabilitation<br/><br/>Broader Significance & Importance<br/>   Training and retraining the movement of individuals suffering loss of motor ability due to stroke is a challenging task. In many cases, only partial success is accomplished after long training sessions. Given the limitations of recovery for post-stroke patients, it is imperative that better tools and methods for retraining be developed. Even though several training hypotheses exist, such as need for an exact repetition of the training pattern, the solutions are still to be found. Overcoming some of these obstacles is the goal of this proposal. Recent studies show a direct link between human action perception and action execution. There is some evidence that the visual observation of human actions has an effect on the movement of the observer. This priming effect is reduced, or not present, when the motion is executed by a non-human device, such as a robot. This proposal aims to develop and test a novel wearable system for the training of the human arm. The system will allow the user to perceive the device as part of their upper limb (hence closing the perception/action loop). Training protocols based on observational learning findings will be implemented with the ARWED in order to develop it as rehabilitation training device for motor recovery in post-stroke patients.<br/><br/>Technical description<br/>   The intellectual merit centers on exploiting the link between action-observation and action execution in order to develop training protocols to facilitate rehabilitation following stroke. Currently, there are limited systems that utilize virtual reality in the relearning of biological movements. The proposed development of augmented wearable system (ARWED) requires solving several challenges in computer vision/modeling, and robot kinematic mapping. Testing the effectiveness of the device on priming the perception-action effect will require combined expertise from the areas of human kinematics, signal analysis, virtual reality, robotic fault recovery theory and rehabilitation. Therefore, the theoretical contributions emerging from this multidisciplinary collaborative research team will advance knowledge and understanding not only within the medical field, but also across the above-mentioned research areas.  The broader impact is based on the characteristics of the device to be developed, with an expanded ability in the training and re-training of patients with motor disabilities. Efficient design and manipulation will make the proposed ARWED system a reliable solution, which will be broadly utilized by medical professionals working in rehabilitation, sports therapy and convalescence. The proposed research will provide effective tools for the training and physical rehabilitation of patients with limb limitations at any scale, ranging from individuals suffering partial loss of motor ability to those with severe limitations in mobility due to strokes, birth defects or accidents. Motor learning theory shows that reducing feedback during practice benefits long term retention of motor skill training. Observational learning may offer greater benefits regarding transfer to ADLs, in comparison to robotic-based stroke training. Thus, a success indicator for these patients would be the beneficial transfer of training from the un-affected to affected limb or vice-versa. The ARWED system is expected to advance significantly the fundamentals of engineering and scientific knowledge, by implementing the device in experimental and educational work on cognition, telemanipulation and virtual reality. The medical community would also benefit from the development of the ARWED by furthering the understanding of how training using technology may enhance the recovery of motor control in diverse populations while providing a novel intervention that may prove more effective than what is currently available. The collaborative nature of the research team will lead to the education of undergraduate and graduate students in the areas of Engineering and Kinesiology. In the long run, the outcomes of the proposal will facilitate the communication between students that want to enter Physical and Occupational Therapy professions and those that want to design and develop mechanical devices that can aid in human recovery following neurological injury."
276,1403688,COLLABORATIVE RESEARCH: ARWED - AUGMENTED PERCEPTION FOR UPPER-LIMB REHABILITATION,CBET,Disability & Rehab Engineering,8/15/14,8/6/14,Alba Perez Gracia,ID,Idaho State University,Standard Grant,Aleksandr Simonian,7/31/18,"$152,757.00 ","Marco Schoen, Nancy Devine",perealba@isu.edu,"921 South 8th Avenue, Stop 8046",Pocatello,ID,832090002,2082822592,ENG,5342,"010E, 9150",$0.00 ,"PI: Banerjee, Amarnath/Perez Gracia, Alba/Robson, Nina<br/>Proposal Number: 1403502/1403688/1404011<br/>Title: Collaborative Research: ARWED - Augmented Perception for Upper-Limb Rehabilitation<br/><br/>Broader Significance & Importance<br/>   Training and retraining the movement of individuals suffering loss of motor ability due to stroke is a challenging task. In many cases, only partial success is accomplished after long training sessions. Given the limitations of recovery for post-stroke patients, it is imperative that better tools and methods for retraining be developed. Even though several training hypotheses exist, such as need for an exact repetition of the training pattern, the solutions are still to be found. Overcoming some of these obstacles is the goal of this proposal. Recent studies show a direct link between human action perception and action execution. There is some evidence that the visual observation of human actions has an effect on the movement of the observer. This priming effect is reduced, or not present, when the motion is executed by a non-human device, such as a robot. This proposal aims to develop and test a novel wearable system for the training of the human arm. The system will allow the user to perceive the device as part of their upper limb (hence closing the perception/action loop). Training protocols based on observational learning findings will be implemented with the ARWED in order to develop it as rehabilitation training device for motor recovery in post-stroke patients.<br/><br/>Technical description<br/>   The intellectual merit centers on exploiting the link between action-observation and action execution in order to develop training protocols to facilitate rehabilitation following stroke. Currently, there are limited systems that utilize virtual reality in the relearning of biological movements. The proposed development of augmented wearable system (ARWED) requires solving several challenges in computer vision/modeling, and robot kinematic mapping. Testing the effectiveness of the device on priming the perception-action effect will require combined expertise from the areas of human kinematics, signal analysis, virtual reality, robotic fault recovery theory and rehabilitation. Therefore, the theoretical contributions emerging from this multidisciplinary collaborative research team will advance knowledge and understanding not only within the medical field, but also across the above-mentioned research areas.  The broader impact is based on the characteristics of the device to be developed, with an expanded ability in the training and re-training of patients with motor disabilities. Efficient design and manipulation will make the proposed ARWED system a reliable solution, which will be broadly utilized by medical professionals working in rehabilitation, sports therapy and convalescence. The proposed research will provide effective tools for the training and physical rehabilitation of patients with limb limitations at any scale, ranging from individuals suffering partial loss of motor ability to those with severe limitations in mobility due to strokes, birth defects or accidents. Motor learning theory shows that reducing feedback during practice benefits long term retention of motor skill training. Observational learning may offer greater benefits regarding transfer to ADLs, in comparison to robotic-based stroke training. Thus, a success indicator for these patients would be the beneficial transfer of training from the un-affected to affected limb or vice-versa. The ARWED system is expected to advance significantly the fundamentals of engineering and scientific knowledge, by implementing the device in experimental and educational work on cognition, telemanipulation and virtual reality. The medical community would also benefit from the development of the ARWED by furthering the understanding of how training using technology may enhance the recovery of motor control in diverse populations while providing a novel intervention that may prove more effective than what is currently available. The collaborative nature of the research team will lead to the education of undergraduate and graduate students in the areas of Engineering and Kinesiology. In the long run, the outcomes of the proposal will facilitate the communication between students that want to enter Physical and Occupational Therapy professions and those that want to design and develop mechanical devices that can aid in human recovery following neurological injury."
277,1414741,EAPSI: Validation of Three-Dimensional Endoscopy System with Synthetic Vocal Fold Models,OISE,"EAPSI, EPSCoR Co-Funding",6/1/14,5/30/14,Kimberly Stevens,UT,Stevens                 Kimberly,Fellowship,Anne L. Emig,5/31/15,"$5,070.00 ",,,,Provo,UT,846063368,,O/D,"7316, 9150","5921, 5978, 7316, 9150",$0.00 ,"Voice disorders affect many people in the United States but effective clinical treatment is limited by knowledge of the physics of voice production proper diagnosis  and by challenges in diagnosis. Laryngoscopy, use of a specially designed endoscope to examine the vocal folds, is an important technique for vocal fold visualization in both clinical and research settings. However, the technique is limited to providing information regarding movement in two dimensions when the motion of vocal folds has been shown to be three-dimensional. Dr. Isao Tokuda at the Ritsumeikan University in Japan has developed a method for reconstructing three-dimensional vocal fold motion using a laryngoscope with two cameras, but the technique has not yet been extensively validated. In collaboration with Dr. Tokuda, this project will use synthetic vocal fold models, recently used in voice biomechanics research, to validate the stereo-endoscopy system. <br/><br/>Synthetic vocal fold models have recently been used in voice biomechanics research and will be used in the proposed project because they offer more control, are more robust, and are easier to acquire than excised animal models. Multiple cameras and common computer vision techniques will be used to validate the stereo-endoscopy system by recording and reconstructing the vibrating surface of synthetic, self-oscillating vocal fold models with both the stereo-endoscopy and high speed systems and comparing the results. The validation of the stereo-endoscopy system will allow its use in clinical settings as well as for quantitative analysis of vocal fold motion, leading to improved understanding of the dynamics involved in phonation. This NSF EAPSI award is funded in collaboration with the Japan Society for the Promotion of Science."
278,1414826,EAPSI: Understanding attention during the early stages of language acquisition using a new tracking eye gaze method,OISE,EAPSI,6/1/14,6/4/14,Joseph Burling,TX,Burling Joseph M,Fellowship,Anne L. Emig,5/31/15,"$5,070.00 ",,,,Houston,TX,770434232,,O/D,7316,"5921, 5978, 7316",$0.00 ,"When it comes to knowing how children learn their first words, understanding how visual attention is guided is especially important. Objects being learned can vary in many ways when taking a child's view of the world. When exposed to human speech, children have the option to gaze upon a wide range of items, some of which are potentially informative for grounding words (auditory information) to physical things they see (visual information). This project will apply cutting edge techniques used for study where children look when they learn. This research aims to develop a solution for the problems we encounter in child-directed eye tracking research. Dr. Nishida at Kyoto University, Japan, has the knowledge and expertise to help with achieving this goal by implementing a promising technique for point of gaze estimation.<br/><br/>Current methods for observing how attention is focused during learning typically involve the use of eye gaze tracking under experimental conditions. Currently, multiple hardware options exist for accomplishing this task, but have their own unique set of advantages and disadvantages. Mobile, wearable eye tracking solutions provide more flexibility in less constrained settings but require longer setup times, adjustments, and calibration. This increases the overall intrusiveness of the experimental procedure and often leads to the child being aware of such devices. Eye-tracking hardware not attached to the child (e.g., a specialized video monitor) typically have more precise measurements of gaze orientation but lack the ability to measure gaze in three-dimensional environments. The new method that will be used in this project, analyzes corneal reflections using computer vision algorithms on super resolution eye images to better accommodate child behavioral tasks. It has the potential to significantly reduce the complexities involved in obtaining accurate and precise data under depth-varying conditions for understanding the role of attention during the early stages of the language learning process. This NSF EAPSI award is funded in collaboration with the Japan Society for the Promotion of Science."
279,1409911,CHS: Medium: Collaborative Research: Social Learning in Mixed Human-Robot Groups for People with Disabilities,IIS,HCC-Human-Centered Computing,9/1/14,8/22/16,John Bricout,TX,University of Texas at Arlington,Continuing Grant,Ephraim Glinert,12/31/18,"$119,664.00 ",,jbricout@umn.edu,"701 S Nedderman Dr, Box 19145",Arlington,TX,760190145,8172722105,CSE,7367,"7367, 7924",$0.00 ,"Assistive robots promise to improve the lives of many people with disabilities in the near future.  But whether due to traumatic spinal cord injury, early onset multiple sclerosis, or the common effects of advancing age, the variety of physical and mental disabilities, and the different psychological reaction of each individual to them, make it impossible to program one-size-fits-all behaviors for assistive robots.  To achieve its full potential, the assistive robot must learn to match the type and degree of assistance offered to the disability level and preferences of the user, as well as to the user's environment and the level of trust between the user and the robot.  Thus, training the robot to fit the individual user is essential - but requiring all users to train all aspects of robot behavior is unrealistic. In this collaborative project involving faculty at two institutions, the PIs argue that a possible solution may derive from the observation that whenever a user needs to train a robot for a new behavior, it is likely that there are other users with similar disabilities, preferences and environments who might also benefit from this behavior.  The PIs will develop techniques which enable the learning of behaviors in human+robot pairs, the identification of possible beneficiaries of the new behaviors, and the transfer of these behaviors to these beneficiaries (where transferring a behavior from one human+robot pair to another might involve the transfer of code and data for the robot and/or the transfer of skills to the human user).  This research will demonstrate how mixed human+robot interaction can alter the relationship between users and their environment, while also rendering physical interaction between robot and human safer and more efficient.  The work will have broad national impact because of the expected rapid growth in coming years of the elderly segment of the population.<br/><br/>The PIs will pursue four thrusts to achieve their vision.  They will design adaptive algorithms and controllers (e.g., for sliding-scale robot autonomy) which allow a robot to be an effective facilitator of user interaction with novel environments during activities of daily living (ADLs).  They will develop models of human+robot trust in the context of assistive robot technology, and examine the effect of trust on the user experience.  They will implement social agents through which the community of users with a specific disability via their social networks can help in the creation and adoption of new solutions for ADL tasks.  And they will validate the ability of human+robot exchanges to increase functionality and performance of ADLs for disabled individuals.  The research will build on recent advances in robot control, psychological models of social learning, and models of social networks, as well as machine learning techniques of collaborative filtering and recommendation.  Project outcomes will include the creation of social agents that can interact on behalf of the user, discover learning opportunities, and actively participate in the transfer of learning.  The work will contribute to our understanding of how users can partner, both individually and collectively, with assistive robots, and will answer open questions relating to the interoperability and intelligibility of knowledge developed in one learning system to another."
280,1420732,Diagnosing misconceptions about algebra using Bayesian inverse reinforcement learning,DRL,"REAL, ECR-EHR Core Research, Cyberlearn & Future Learn Tech",9/1/14,7/14/16,Thomas Griffiths,CA,University of California-Berkeley,Continuing Grant,Gregg Solomon,8/31/19,"$443,248.00 ",Anna Rafferty,tomg@princeton.edu,Sponsored Projects Office,BERKELEY,CA,947101749,5106433891,EHR,"7625, 7980, 8020",8045,$0.00 ,"This project, to be conducted by researchers at the University of California, Berkeley, and Carleton College, will develop an online math tutor to help high school and college students learn algebra.  The tutor will diagnose students' misconceptions about algebra by asking them to solve a series of math problems.  The website will be made available to students anywhere, making it possible to collect large amounts of data on algebra problem solving that will be used to refine the technological approach, develop computational models of student learning, optimize the design of tests, and identify effective strategies for online learning and teaching.  This project will advance the work of the REAL (Research on Education and Learning) program in studying the cognitive basis of STEM (science, technology, engineering, and mathematics) learning, as well as the Cyberlearning program in discovering how to design and effectively use learning technologies of the future.  <br/><br/>The approach used for knowledge diagnosis in the online tutor will be Bayesian inverse reinforcement learning. This approach combines ideas from machine learning and cognitive science. Students' responses will be modeled using Markov decision processes, a decision-theoretic formalism that indicates how a rational agent should take a series of actions to achieve a goal. This will make it possible to calculate the probability that a student would take a particular series of actions, given his or her knowledge of the domain. Bayesian inference will be used to invert this model, providing a probability distribution over the knowledge state of the student given his or her actions. Applying this approach to algebra problem solving will make it possible to identify students' misconceptions from freeform solutions to algebra problems. The proposed research will require addressing challenges such as working with the infinite, structured state spaces that are needed to describe algebraic equations, accommodating composite actions (such as skipping a step in a derivation), modeling learning, and optimizing the design of assessments."
281,1349774,CAREER: Bayesian Nonparametric Learning for Large-Scale Structure Discovery,IIS,Robust Intelligence,3/15/14,2/9/17,Erik Sudderth,RI,Brown University,Continuing grant,Weng-keen Wong,10/31/17,"$401,480.00 ",,sudderth@uci.edu,BOX 1929,Providence,RI,29129002,4018632777,CSE,7495,"1045, 7495, 9150",$0.00 ,"CAREER: Bayesian Nonparametric Learning for Large-Scale Structure Discovery<br/><br/>This CAREER project will advance the state-of-the-art for automated discovery of structure within data as diverse as images and video, natural language, audio sequences, and social and biological networks.  Contemporary applications of statistical machine learning are dominated by parametric models.  This approach constructs models of pre-determined size (with a finite-dimensional vector of parameters which) are tuned using training data.  To be effective, the underlying structure of such models must be manually specified by experts with application-specific knowledge.  This presumed structure imposes limits on what can possibly be learned even from very big datasets.<br/><br/>Bayesian nonparametric models instead define distributions on models of arbitrary size with infinite-dimensional spaces of functions, partitions, or other combinatorial structures.  They lead to flexible, data-driven unsupervised learning algorithms, and models whose internal structure continually grows and adapts to new observations.  Bayesian nonparametric models, while promising, are an incompletely-developed technology posing significant challenges to practice.  This CAREER project will increase the practical feasibility and impact of Bayesian nonparametric approaches by pursuing three interrelated themes:<br/><br/>1) Nonparametric Model Design and Evaluation.  New families of models for data with hierarchical, spatial, temporal, or relational structure are investigated.  Quantitative validation of the statistical assumptions and biases inherent in these models will be emphasized, evaluating whether these align with the empirical statistics of significant application areas.<br/><br/>2) Reliable Structure Discovery.  Statistical inference algorithms which move beyond the local moves of standard (and widely used) Monte Carlo and variational methods will be developed.  Compelling examples indicate that local optima are a significant issue for contemporary methods, so a family of novel algorithms is proposed, which dynamically adjust model complexity as learning proceeds.<br/><br/>3) Scalable and Extensible Nonparametric Learning.  Common patterns across a wide range of popular nonparametric models are identified, which suggest a corresponding family of scalable and parallelizable online learning algorithms.  The ""memoized"" online variational inference algorithm avoids some practical instabilities and sensitivities of conventional methods, while allowing provably correct optimization of the nonparametric model structure and complexity.<br/><br/>An extensible ""BNPy: Bayesian Nonparametric Learning in Python"" software package is under development to allow easy application of the novel learning algorithms to a wide range of current and future BNP models.  The education and outreach plan of this CAREER project leverages this software to create interdisciplinary undergraduate research teams exploring applications in the natural and social sciences, and a week-long summer school on Bayesian nonparametrics to be held twice at Brown University's Institute for Computational and Experimental Research in Mathematics (ICERM)."
282,1449650,"Investigating the Impact of Co-Learning Systems in Providing Customized, Real-Time Student Feedback",DUE,"S-STEM-Schlr Sci Tech Eng&Math, IUSE",12/1/14,8/21/14,Conrad Tucker,PA,Pennsylvania State Univ University Park,Standard Grant,Heather Watson,11/30/18,"$287,990.00 ",Soundar R. Kumara,conradt@andrew.cmu.edu,110 Technology Center Building,UNIVERSITY PARK,PA,168027000,8148651372,EHR,"1536, 1998","8209, 9178",$0.00 ,"A current frontier in STEM education is the development of individualized learning environments that can help students learn challenging topics. The need is especially acute during active learning sessions and laboratory activities.  This project will develop co-learning systems to enhance student performance in STEM laboratory activities. Co-learning systems consist of computer systems equipped with sensors. These systems can learn from humans and in turn interact with humans providing them with customized, real-time performance feedback.  The lessons learned from the project can also be applied in other areas of STEM education such as the K-12 environment where co-learning robotic laboratory tutors could help alleviate some of the demands on the time and attention of science teachers.<br/><br/>The specific research objective of this Improving Undergraduate STEM Education (IUSE) project is to test the hypothesis that co-learning systems are able to enhance student performance in undergraduate STEM laboratory activities by providing them with customized, real-time performance feedback. The project will utilize existing commercial, off-the-shelf technologies in development of the systems.  Sensor input will consist of audio, video, depth, skeletal, and 3D mesh data collected using a multimodal sensing device. The co-learning systems will be integrated into the laboratory environments to capture and translate data pertaining to STEM classroom environments and student interactions during laboratory activities. The type of data will include audio data pertaining to student verbal queries, skeletal data pertaining to student gesture patterns, and the content of student work.  The project will investigate machine learning algorithms suitable for discovering knowledge pertaining to student learning during STEM laboratory activities.  Work will assess the students' perception of co-learning systems and evaluate the ability of co-learning systems to improve performance during laboratories. The investigators will compare student learning outcomes between comparable student groups subject to different amounts of interaction with the co-learning systems. These results will provide information about the potential of co-learning systems to augment traditional teaching and provide an effective, automated, personal STEM learning environment."
283,1421168,RI: Small: Robot Developmental Learning of Skilled Actions,IIS,"Robust Intelligence, NRI-National Robotics Initiati",9/1/14,6/24/19,Benjamin Kuipers,MI,Regents of the University of Michigan - Ann Arbor,Standard Grant,Erion Plaku,8/31/20,"$446,823.00 ",,kuipers@umich.edu,3003 South State St. Room 1062,Ann Arbor,MI,481091274,7347636438,CSE,"7495, 8013","7495, 7923, 8086",$0.00 ,"The goal of this project is to show how a robot --- using a continuous stream of visual and tactile data --- can learn to work at a human level of skill in tasks normally done by humans.  To function at a human level, it must be able to plan with ""object-level"" abstractions such as putting a red block into the box, and it must also be able to grasp objects and move them while avoiding bumping into things and causing damage to its surroundings.  This project is inspired by human cognitive development.  A baby learns about objects and actions by bootstrapping from early regularities and unreliable actions to hierarchies of more complex and reliable actions.  The hypothesis to be tested is that this bootstrap learning approach allows a robot to achieve human levels of skillful and robust action in a wide range of human-dominated environments.<br/><br/>This project draws on extensive prior work on foundational knowledge representations and machine learning.  Learning begins by detecting low-level contingencies --- regularities among observed events --- and refining them into increasingly accurate predictive rules, that can be used to define reliable actions.  For a given rule, a simple MDP model is formulated, and reinforcement learning methods learn a policy for accomplishing an action at the next level of the action hierarchy.  Learned actions are initially unreliable, but policies and actions improve with experience.  Attention is focused where learning is likely to be most productive by intrinsic motivation methods that reward actions that result in successful learning, including the important special case of rewarding attempts to imitate the successful actions of other agents."
284,1451986,EAGER: Collaborative Research: Learning Relations between Extreme Weather Events and Planet-Wide Environmental Trends,CCF,CyberSEES,9/1/14,8/22/14,Arindam Banerjee,MN,University of Minnesota-Twin Cities,Standard Grant,Sylvia Spengler,8/31/17,"$100,000.00 ",,banerjee@cs.umn.edu,200 OAK ST SE,Minneapolis,MN,554552070,6126245599,CSE,8211,"7916, 8231",$0.00 ,"Extreme events, such as heat waves, cold spells, extreme precipitation, and severe storms, play a significant role in the loss of lives and damage to ecosystems and infrastructure, presenting fundamental challenges to sustainability. Under anticipated trends in planet-scale environmental trends, there is considerable uncertainty in the projected changes in the intensity, duration, and frequency of extreme events. Reducing these uncertainties is a grand challenge that will require substantial advances in both the environmental and data sciences. The proposed research seeks to advance both the environmental science that underpins predictions of extreme events, and the data science required to identify relations between variables in massive data sets. The results of this research will provide a basis for improving predictions of extreme events for use in sustainability planning. This project will educate and cross-train graduate students in both disciplines, allowing them to contribute to this new emerging field.  The proposed research will also inform course development, and will be disseminated through tutorials, conferences, and seminars. The team's involvement with workshops, the GW Sustainability Institute, and GW Planet Forward will help to broaden the impact through public outreach.<br/><br/>The proposed research will advance machine learning and statistical modeling of large-scale and regional events by: (1) using new tools in sparse regression in high dimensions, (2) identifying nonlinear relations in data, and (3) learning relations in spatiotemporal data that are non-stationary over space and time. The results of this research will advance understanding of extreme weather events and their relation to planet-wide environmental trends. Such relations will be learned by applying new statistical algorithms to analyze extensive climate model simulations which generate very large data sets. The findings will be validated against observations, and the learned relations will be compared between different models to assess consistency and robustness, and to validate models."
285,1506586,SHF: Medium: Collaborative: Transfer Learning in Software Engineering,CCF,"Software & Hardware Foundation, SOFTWARE ENG & FORMAL METHODS",8/2/14,6/1/16,Timothy Menzies,NC,North Carolina State University,Continuing Grant,Sol Greenspan,6/30/18,"$464,609.00 ",,timm@ieee.org,2701 Sullivan DR STE 240,Raleigh,NC,276950001,9195152444,CSE,"7798, 7944","7924, 7944, 9150",$0.00 ,"The goal of the research is to enable software engineers to find software development best practices from past empirical data. The increasing availability of software development project data, plus new machine learning techniques, make it possible for researchers to study the generalizability of results across projects using the concept of transfer learning. Using data from real software projects, the project will determine and validate best practices in three areas: predicting software development effort; isolating software detects; effective code inspection practices. <br/><br/>This research will deliver new data mining technologies in the form of transfer learning techniques and tools that overcome current limitations in the state-of-the-art to provide accurate learning within and across projects. It will design new empirical studies, which apply transfer learning to empirical data collected from industrial software projects. It will build an on-line model analysis service, making the techniques and tools available to other researchers who are investigating validity of principles for best practice. <br/><br/>The broader impacts of the research will be to make empirical software engineering research results more transferable to practice, and to improve the research processes for the empirical software engineering community.  By providing a means to test principles about software development, this work stands to transform empirical software engineering research and enable software managers to rely on scientifically obtained facts and conclusions rather than anecdotal evidence and one-off studies. Given the immense importance and cost of software in commercial and critical systems, the research has long-term economic impacts."
286,1441563,DIP: Developing Crosscutting Concepts in STEM with Simulation and Embodied Learning,IIS,"S-STEM-Schlr Sci Tech Eng&Math, Discovery Research K-12, Cyberlearn & Future Learn Tech",9/1/14,11/7/16,Robb Lindgren,IL,University of Illinois at Urbana-Champaign,Standard Grant,Amy Baylor,8/31/19,"$1,349,504.00 ","Jose Mestre, Alan Craig, Guy Garnett, Wai-Tat Fu",robblind@illinois.edu,1901 South First Street,Champaign,IL,618207406,2173332187,CSE,"1536, 7645, 8020","8045, 8089, 8842",$0.00 ,"The Cyberlearning and Future Learning Technologies Program funds efforts that will help envision the next generation of learning technologies and advance what we know about how people learn in technology-rich environments. Development and Implementation (DIP) Projects build on proof-of-concept work that showed the possibilities of the proposed new type of learning technology, and project teams build and refine a minimally-viable example of their proposed innovation that allows them to understand how such technology should be designed and used in the future and that allows them to answer questions about how people learn, how to foster or assess learning, and/or how to design for learning. This proposal uses advances in multimodal immersive interfaces, such as sensors that allow motion detection (like the Microsoft Kinect) to examine questions about how learners think with their bodies as they make sense of science concepts like 'scale' or 'rates of change'. The project will help create ""simulation theatres for embodied learning,"" or rooms with immersive technology that allow students to interact with science simulations and simultaneously express ideas by moving their bodies. Research studies will examine whether gestures students use carry over from one science discipline to the next, and whether this type of interaction helps them transfer what they know in one science domain to others. At the end of the project, we should have 1. a technology platform that can be used to help research how students use gesture to understand science concepts, 2. information about how well this tool supports learning across disciplines, and 3. novel psychology research about how people think with their bodies.<br/><br/>This project seeks to extend and refine emerging theories of embodied learning and embodied design. Embodied interactions have shown promise for increasing learning in specific STEM concepts, but there is less known about how body movement and gesture promote understanding abstract and crosscutting ideas that may facilitate learning transfer. This project examines explicitly whether persistent schemes of embodied interactions with computer simulations make it easier for learners to engage with, and learn from, new simulations of novel STEM topics. This project will also make intellectual advances in computational gesture recognition and processing, for instance through single-instance machine learning algorithms, real-time training, and modeling of paraterized gestures to capture full-body gestures to create a highly flexible gesture-learning environment that will enable training based on individual subjects without having to build a large database of gestures in order to achieve reliable recognition. By developing (1) an easy to use, low cost, and highly reconfigurable system for recognizing learning gestures and (2) an integrated set of learning simulations that rely on embodied interactions to investigate a broad range of STEM topics using consistent interface schemes, the project will be able to investigate how gestural congruency can be used to support learners' conceptions of STEM disciplines. Research studies will use 12-15 middle-school students in the initial phases to help identify candidate gestures for cross-disciplinary gestural metaphors. Three later iterations will use approximately 50 students per iteration to examine whether interacting with the system can engage embodied metaphors that support transfer of learning from the domain of a STEM simulation to other domains, including development of instruments for assessing transfer."
287,1451945,EAGER: Collaborative Research: Learning Relations between Extreme Weather Events and Planet-Wide Environmental Trends,CCF,CyberSEES,9/1/14,8/22/14,Timothy Delsole,VA,George Mason University,Standard Grant,Sylvia J. Spengler,8/31/17,"$99,939.00 ",,tdelsole@gmu.edu,4400 UNIVERSITY DR,FAIRFAX,VA,220304422,7039932295,CSE,8211,"7916, 8231",$0.00 ,"Extreme events, such as heat waves, cold spells, extreme precipitation, and severe storms, play a significant role in the loss of lives and damage to ecosystems and infrastructure, presenting fundamental challenges to sustainability. Under anticipated trends in planet-scale environmental trends, there is considerable uncertainty in the projected changes in the intensity, duration, and frequency of extreme events. Reducing these uncertainties is a grand challenge that will require substantial advances in both the environmental and data sciences. The proposed research seeks to advance both the environmental science that underpins predictions of extreme events, and the data science required to identify relations between variables in massive data sets. The results of this research will provide a basis for improving predictions of extreme events for use in sustainability planning. This project will educate and cross-train graduate students in both disciplines, allowing them to contribute to this new emerging field.  The proposed research will also inform course development, and will be disseminated through tutorials, conferences, and seminars. The team's involvement with workshops, the GW Sustainability Institute, and GW Planet Forward will help to broaden the impact through public outreach.<br/><br/>The proposed research will advance machine learning and statistical modeling of large-scale and regional events by: (1) using new tools in sparse regression in high dimensions, (2) identifying nonlinear relations in data, and (3) learning relations in spatiotemporal data that are non-stationary over space and time. The results of this research will advance understanding of extreme weather events and their relation to planet-wide environmental trends. Such relations will be learned by applying new statistical algorithms to analyze extensive climate model simulations which generate very large data sets. The findings will be validated against observations, and the learned relations will be compared between different models to assess consistency and robustness, and to validate models."
288,1427425,NRI: Collaborative Research: Shall I Touch This?: Navigating the Look and Feel of Complex Surfaces,IIS,National Robotics Initiative,7/15/14,8/18/14,Trevor Darrell,CA,University of California-Berkeley,Standard Grant,Jie Yang,6/30/17,"$600,000.00 ",,trevor@eecs.berkeley.edu,Sponsored Projects Office,BERKELEY,CA,947101749,5106433891,CSE,8013,8086,$0.00 ,"This project improves autonomous robotic perception so that future co-robots can glance around any scene and accurately estimate how it would feel to grasp or step on all of the visible surfaces.  Just as people do, robots should use such these physical predictions to guide their interactions with the world, for example avoiding dangerous ice patches on the ground when walking and driving, and adeptly anticipating the grasp force needed to pick up everything from ice cubes to stuffed animals. These research activities are accompanied by significant outreach efforts, including a new program on ""Look and Touch Robotics"" to get middle-school students, particularly those from underrepresented groups, excited about computer science, engineering, and robotics. This program uses simple experiments to highlight the dual importance of visual and haptic information during interactions with physical objects, along with demonstrations of a robot showing visuo-haptic intelligence.  This project also integrates research and education by involving undergraduates in the research and via hands-on projects in the vision and robotics classes taught by the Principal Investigators.<br/><br/>This research involves extensive collection of data from real objects and surfaces using both visual and haptic sensors.  The recorded interactions are analyzed to uncover visual clues that can allow a robot to infer the physical characteristics of the surface, such as slipperiness, hardness, and roughness.  This problem is addressed using deep learning, a recently developed approach that has been successful in enabling robots to visually recognize a wide variety of objects in diverse circumstances.  The research team also builds the database of visuo-haptic recordings and the learned cross-modal sensory, and makes it available to other robotics researchers at the end of the project."
289,1409823,CHS: Medium: Collaborative Research: Social Learning in Mixed Human-Robot Groups for People with Disabilities,IIS,HCC-Human-Centered Computing,9/1/14,7/20/20,Aman Behal,FL,The University of Central Florida Board of Trustees,Continuing Grant,Ephraim Glinert,8/31/21,"$1,094,043.00 ","Peter Hancock, Ladislau Boloni",abehal@mail.ucf.edu,4000 CNTRL FLORIDA BLVD,Orlando,FL,328168005,4078230387,CSE,7367,"7367, 7924, 9251",$0.00 ,"Assistive robots promise to improve the lives of many people with disabilities in the near future.  But whether due to traumatic spinal cord injury, early onset multiple sclerosis, or the common effects of advancing age, the variety of physical and mental disabilities, and the different psychological reaction of each individual to them, make it impossible to program one-size-fits-all behaviors for assistive robots.  To achieve its full potential, the assistive robot must learn to match the type and degree of assistance offered to the disability level and preferences of the user, as well as to the user's environment and the level of trust between the user and the robot.  Thus, training the robot to fit the individual user is essential - but requiring all users to train all aspects of robot behavior is unrealistic. In this collaborative project involving faculty at two institutions, the PIs argue that a possible solution may derive from the observation that whenever a user needs to train a robot for a new behavior, it is likely that there are other users with similar disabilities, preferences and environments who might also benefit from this behavior.  The PIs will develop techniques which enable the learning of behaviors in human+robot pairs, the identification of possible beneficiaries of the new behaviors, and the transfer of these behaviors to these beneficiaries (where transferring a behavior from one human+robot pair to another might involve the transfer of code and data for the robot and/or the transfer of skills to the human user).  This research will demonstrate how mixed human+robot interaction can alter the relationship between users and their environment, while also rendering physical interaction between robot and human safer and more efficient.  The work will have broad national impact because of the expected rapid growth in coming years of the elderly segment of the population.<br/><br/>The PIs will pursue four thrusts to achieve their vision.  They will design adaptive algorithms and controllers (e.g., for sliding-scale robot autonomy) which allow a robot to be an effective facilitator of user interaction with novel environments during activities of daily living (ADLs).  They will develop models of human+robot trust in the context of assistive robot technology, and examine the effect of trust on the user experience.  They will implement social agents through which the community of users with a specific disability via their social networks can help in the creation and adoption of new solutions for ADL tasks.  And they will validate the ability of human+robot exchanges to increase functionality and performance of ADLs for disabled individuals.  The research will build on recent advances in robot control, psychological models of social learning, and models of social networks, as well as machine learning techniques of collaborative filtering and recommendation.  Project outcomes will include the creation of social agents that can interact on behalf of the user, discover learning opportunities, and actively participate in the transfer of learning.  The work will contribute to our understanding of how users can partner, both individually and collectively, with assistive robots, and will answer open questions relating to the interoperability and intelligibility of knowledge developed in one learning system to another."
290,1439011,XPS:FULL:SDA: Reflex Tree - A New Computer and Communication Architecture for Future Smart Cities,CCF,"Special Projects - CNS, CRCNS-Computation Neuroscience, Exploiting Parallel&Scalabilty",10/1/14,4/25/19,Tao Wei,RI,University of Rhode Island,Standard Grant,Marilyn McClure,9/30/20,"$865,769.00 ","Haibo He, Qing Yang",wei@ele.uri.edu,RESEARCH OFFICE,KINGSTON,RI,28811967,4018742635,CSE,"1714, 7327, 8283","8089, 8091, 9150, 9251",$0.00 ,"This project studies a new computing and communication architecture, reflex-tree, with massive parallel sensing, data processing, and control functions designed to meet the challenges imposed by future smart cities. The central feature of this novel reflex-tree architecture is inspired by a fundamental element of the human nervous system -- reflex arcs, or neuro-muscular reactions and instinctive motions in response to urgent situations that do not require the direct intervention of the brain. The scientific foundation and engineering framework built by this project will pave the way for enhanced monitoring and management of critical smart city infrastructure, from gas/oil pipelines, water management, communication networks, and power grids, to public transportation and healthcare. The interdisciplinary and collaborative nature of the project will inspire broader participation in related areas of research.<br/><br/>Within the human body, a neural reflex arc is able to cause an individual to immediately react to a source of discomfort without the need for direct control from the brain. The reflex-tree architecture mimics such human neural circuits, using massive numbers of intermediate computing nodes, edge devices, and sensors to gather, process, and, most importantly, to react to data concerning critical infrastructure elements. Key innovations of the proposed reflex-tree architecture include: 1) A novel, 4-level, large scale, and application-specific hierarchical computing and communication structure capable of carrying out sensor-based decision-making processes. The required computation and nodal computing power increases at each successive stage in the hierarchy, with the level-1 cloud performing the most complex tasks. 2) A densely distributed fiber-optic sensing network and parallel machine learning algorithms will be developed targeting smart city applications. 3) Novel, complementary machine intelligence algorithms will be developed, providing accurate control decisions via multi-layer adaptive learning, spatial-temporal association, and complex system behavior analysis. 4) New parallel algorithms and software run-time environments will be proposed and developed that are specifically tailored to the novel reflex-tree system architecture.<br/><br/>To demonstrate the feasibility and performance of the reflex-tree architecture, a proof-of-concept prototype will be constructed utilizing a miniaturized, laboratory-scale municipal gas pipeline system. The prototype will incorporate a complete 4-level reflex-tree--a distributed fiber-optic sensing network deployed alongside pipelines, edge devices performing data classification using parallel SVM, intermediate nodes performing massively-parallel spatial and temporal machine learning, and the cloud as the root node running sophisticated parallel behavioral analysis and decision making tasks. The resulting system is a cross layer, high performance, and massively parallel computing platform, providing a foundational sensing and computer architecture for future smart cities."
291,1450501,Shape of Educational Data,DRL,REAL,10/1/14,3/30/15,Colleen Ganley,FL,Florida State University,Standard Grant,Finbarr Sloane,9/30/17,"$189,444.00 ",Sara Hart,ganley@psy.fsu.edu,"874 Traditions Way, 3rd Floor",TALLAHASSEE,FL,323064166,8506445260,EHR,7625,"7625, 7916",$0.00 ,"This study of the 'shape' of STEM educational assessment data will exploit recent advances in computational topology, machine learning and cognitive science.  Applications of computational topology to learning theory are both new and innovative. It is anticipated that these new tools will provide critical insights into student learning of Calculus.<br/><br/>Funded by NSF's Research on Education and Learning (REAL) program, this early concept grant for exploratory research will apply advanced topological, geometric, and Bayesian methods to analyze the shape of data generated by students taking Calculus via a massive open online course (MOOC) system (and other data sources). This project will link data analyses from three leading learning platforms in the service of better understanding student learning of Calculus. One potential high payoff is the deployment of richer analyses of test and performance data to support a recommendation system to support to students reaching their learning goals.  Further, the project will seed a new community of researchers across disciplines that typically do not interact. The proposed line of work has the potential to fundamentally change the way we think about assessment data in support of mathematics learning and perhaps other STEM content."
292,1451954,EAGER: Collaborative Research: Learning Relations between Extreme Weather Events and Planet-Wide Environmental Trends,CCF,CyberSEES,9/1/14,8/22/14,Claire Monteleoni,DC,George Washington University,Standard Grant,Sylvia Spengler,8/31/17,"$99,993.00 ",,cmontel@colorado.edu,1922 F Street NW,Washington,DC,200520086,2029940728,CSE,8211,"7916, 8231",$0.00 ,"Extreme events, such as heat waves, cold spells, extreme precipitation, and severe storms, play a significant role in the loss of lives and damage to ecosystems and infrastructure, presenting fundamental challenges to sustainability. Under anticipated trends in planet-scale environmental trends, there is considerable uncertainty in the projected changes in the intensity, duration, and frequency of extreme events. Reducing these uncertainties is a grand challenge that will require substantial advances in both the environmental and data sciences. The proposed research seeks to advance both the environmental science that underpins predictions of extreme events, and the data science required to identify relations between variables in massive data sets. The results of this research will provide a basis for improving predictions of extreme events for use in sustainability planning. This project will educate and cross-train graduate students in both disciplines, allowing them to contribute to this new emerging field.  The proposed research will also inform course development, and will be disseminated through tutorials, conferences, and seminars. The team's involvement with workshops, the GW Sustainability Institute, and GW Planet Forward will help to broaden the impact through public outreach.<br/><br/>The proposed research will advance machine learning and statistical modeling of large-scale and regional events by: (1) using new tools in sparse regression in high dimensions, (2) identifying nonlinear relations in data, and (3) learning relations in spatiotemporal data that are non-stationary over space and time. The results of this research will advance understanding of extreme weather events and their relation to planet-wide environmental trends. Such relations will be learned by applying new statistical algorithms to analyze extensive climate model simulations which generate very large data sets. The findings will be validated against observations, and the learned relations will be compared between different models to assess consistency and robustness, and to validate models."
293,1415719,SBIR Phase I: Adding Value to the World's Free Online Educational Content to Upgrade the Online Learning Experience,IIP,SMALL BUSINESS PHASE I,7/1/14,5/19/14,Alpana Verma-Alag,CA,"Upgrademe, Inc",Standard Grant,Glenn H. Larsen,12/31/14,"$150,000.00 ",,alpana@upgrademeinc.com,2044 Finley Place,Santa Clara,CA,950503266,4087180135,ENG,5371,"5371, 8031, 8032, 8039",$0.00 ,"This SBIR Phase I project's broader/commercial impact of the proposed innovation is in the creation of a platform that can personalize the online learning experience using freely available online educational resources. The rising cost of education and professional development along with an increasing need for the same, create a need for affordable but high quality educational resources. The proposed project will add value to the online educational resources that are freely available and allow users to accomplish their educational goals at low or no cost. School and college students will be able to accomplish test preparation and the study of STEM subjects via resources that are best fit for their grade, ability and learning style. Professionals will be able to access courses that help them enhance their existing skills and learn new ones. As users' goals, interests, interactions and learning styles are revealed; the software will be able to provide high quality recommendations and enhanced personalization. Barriers that currently exist for learners based on income and geography can be eliminated by this disruptive technology. A freemium/ premium subscription model will keep the company viable despite providing free resources to the majority of users.<br/><br/>The project aims to enhance the value of currently available free online education resources to make it a better fit for learners' interests, goals, and learning styles. High quality online educational content is being generated by an increasing number of agencies and a lot of this content is freely available throughout the web. However, what is lacking is the customization of this ever-growing content to meet an individual user's specific learning needs. There is a strong need in the market to leverage the world's online educational content and build a web-based product with the ability to aggregate educational content from a wide variety of sources and provide it to the user with a high degree of personalization. Incorporating user goals, interests, past behaviors of learning, and user interactions into the users profile are important for providing the user with highly specific recommendations and creating a personalized experience. The use of machine learning and big data analytics, as well as other state of the art techniques will be key components in building such a product."
294,1426787,NRI: Collaborative Research: Shall I Touch This?: Navigating the Look and Feel of Complex Surfaces,IIS,National Robotics Initiative,7/15/14,4/29/15,Katherine Kuchenbecker,PA,University of Pennsylvania,Standard Grant,Jie Yang,6/30/18,"$408,000.00 ",,kuchenbe@seas.upenn.edu,Research Services,Philadelphia,PA,191046205,2158987293,CSE,8013,"8086, 9251",$0.00 ,"This project improves autonomous robotic perception so that future co-robots can glance around any scene and accurately estimate how it would feel to grasp or step on all of the visible surfaces.  Just as people do, robots should use such these physical predictions to guide their interactions with the world, for example avoiding dangerous ice patches on the ground when walking and driving, and adeptly anticipating the grasp force needed to pick up everything from ice cubes to stuffed animals. These research activities are accompanied by significant outreach efforts, including a new program on ""Look and Touch Robotics"" to get middle-school students, particularly those from underrepresented groups, excited about computer science, engineering, and robotics. This program uses simple experiments to highlight the dual importance of visual and haptic information during interactions with physical objects, along with demonstrations of a robot showing visuo-haptic intelligence.  This project also integrates research and education by involving undergraduates in the research and via hands-on projects in the vision and robotics classes taught by the Principal Investigators.<br/><br/>This research involves extensive collection of data from real objects and surfaces using both visual and haptic sensors.  The recorded interactions are analyzed to uncover visual clues that can allow a robot to infer the physical characteristics of the surface, such as slipperiness, hardness, and roughness.  This problem is addressed using deep learning, a recently developed approach that has been successful in enabling robots to visually recognize a wide variety of objects in diverse circumstances.  The research team also builds the database of visuo-haptic recordings and the learned cross-modal sensory, and makes it available to other robotics researchers at the end of the project."
295,1421823,SHF: Small: Introducing Next Generation I/O Accelerator,CCF,Software & Hardware Foundation,8/1/14,7/9/14,Qing Yang,RI,University of Rhode Island,Standard Grant,Yuanyuan Yang,7/31/19,"$479,998.00 ",,qyang@ele.uri.edu,RESEARCH OFFICE,KINGSTON,RI,28811967,4018742635,CSE,7798,"7923, 7941, 9150",$0.00 ,"Big data applications demand high speed, reliable, and energy  <br/>efficient data storage systems. Traditional storage architectures have  <br/>fundamental limitations because of legacy systems that have centered  <br/>on spinning hard disk drives. With rapid advances in nonvolatile  <br/>memory technologies such as NAND-gate flash, phase change memory, Memristor, and  <br/>magnetic RAM, a great opportunity arises for revolutionizing storage  <br/>architectures. The objective of this research is to start a paradigm  <br/>shift in storage architecture to meet the increasing demand of big  <br/>data applications. It is envisioned that future storage systems will  <br/>have machine intelligence that learns, analyzes, predicts, and  <br/>controls the system at runtime dynamically. A novel accelerator  <br/>architecture is introduced with machine intelligence to enable high  <br/>speed processing of storage data operations that are critical to high  <br/>performance computing in general and big data computing in particular.<br/><br/>   The newly introduced I/O accelerator, residing either in a  <br/>many-core CPU chip or on a storage controller board, enables  <br/>sufficiently accurate predictions for effective optimization of  <br/>storage I/Os. With new architecture features, the proposed I/O  <br/>accelerator can carry out complicated I/O tasks in the speed  <br/>comparable to the emerging nonvolatile memories, which is critical to  <br/>I/O performance because it no longer operates in milliseconds as  <br/>spinning disks do. The project will explore and implement the I/O  <br/>accelerator that can effectively deal with the complexity and high  <br/>dimensionality of factors related to diverse storage technologies, a  <br/>large variation of application workloads, different  <br/>reliability/availability requirements, and power consumptions of  <br/>various storage components. The result is a new heterogeneous storage  <br/>architecture that is optimized for future computing infrastructure.  <br/>With the accelerator as an enabler, comprehensive methodology will be  <br/>investigated that proactively learns system behavior to anticipate  <br/>long-term trends and to respond quickly to fast changing I/O events.  <br/>The new architecture is believed to be the first of the kind providing  <br/>dynamic optimizations by means of 1) intelligent data placements and  <br/>replacements across heterogeneous devices, 2) optimal resource  <br/>allocation and provisioning to applications' workloads, 3) effective  <br/>data deduplication based on content locality, and 4) smart policy  <br/>decision on data protection and recovery adaptive to different data  <br/>types. Furthermore, the new accelerator enables fast in-situ data  <br/>analytics in active storage systems. <br/><br/>This research project is expected  to have the following broader impacts: <br/>1) In today's cloud computing and big data  <br/>applications, servers generate a large amount of I/Os that can take  <br/>full advantage of the new storage architecture. 2) The new accelerator  <br/>can be incorporated into many core CPUs as a specialized core for  <br/>future heterogeneous processors. 3) The new storage architecture will  <br/>speed up the adoption of emerging storage class memories. 4) The new  <br/>methodology will stimulate more research in applying machine learning  <br/>to storage systems. 5) The new CPU-and-data centric Computer Engineering curriculum will train  <br/>both graduate and undergraduate students for real world needs. 6) The  <br/>outreach program will continue the success stories of prior NSF  <br/>projects to help the economic development of the state of Rhode Island and the  <br/>nation."
296,1411817,Variational Analysis of Optimal Value Functions and Applications to Nonsmooth Optimization,DMS,APPLIED MATHEMATICS,6/15/14,6/15/14,Mau Nguyen,OR,Portland State University,Standard Grant,Michael Steuerwalt,7/31/17,"$112,288.00 ",,mnn3@pdx.edu,1600 SW 4th Ave,Portland,OR,972070751,5037259900,MPS,1266,9251,$0.00 ,"Variational analysis serves as the mathematical foundation for non-smooth optimization problems in which the cost functions to be minimized are not necessarily differentiable. Because of the non-differentiability, traditional calculus-based methods are not applicable. Through this research project, the principal investigator and his colleagues will develop new applications of variational analysis designed to solve a number of important non-smooth optimization problems in the areas of facility location, computational geometry, and machine learning.  They will develop and implement numerical algorithms for large-scale location problems, some involving different types of distance metrics, etc. The methods being built will be used to study other non-smooth optimization models in computational geometry and machine learning. The new knowledge in variational analysis this project anticipates will advance the solution of practical models in non-smooth optimization.<br/><br/>This project aims at developing new applications of variational analysis to non-smooth optimization. The principal investigator and his colleagues study generalized differentiation properties of a class of optimal value functions in both convex and non-convex settings. Functions of this type, are intrinsically non-differentiable, and play an important role in the theory of variational analysis and its applications. In particular, the PI and his colleagues focus on two classes of optimal value functions: the minimal time function, which is a natural extension of the closest distance function, and the maximal time function, which is an extension of the farthest distance function. Generalized differentiation properties of the optimal value function are used to study necessary and sufficient conditions on initial data that guarantee different properties of the optimal value function such as continuity, Lipschitz continuity, and differentiability. Results obtained here contribute to development of numerical algorithms  for the solution of non-smooth optimization problems in facility location, computational geometry, and machine learning. Generalized differentiation properties of the optimal value function as well as advanced smoothing techniques and fast gradient methods are investigated in order to develop effective numerical algorithms for solving these problems."
297,1417056,"Advances in Interdisciplinary Statistics and Combinatorics, October 10-12, 2014",DMS,STATISTICS,6/1/14,4/21/14,Sat Gupta,NC,University of North Carolina Greensboro,Standard Grant,Gabor Szekely,5/31/15,"$10,000.00 ","Shan Suthaharan, Jan Rychtar",sngupta@uncg.edu,1111 Spring Garden Street,Greensboro,NC,274125013,3363345878,MPS,1269,7556,$0.00 ,"This award supports the participation of junior researchers in the  international conference ""Advances in Interdisciplinary Statistics and Combinatorics,"" held at The University of North Carolina at Greensboro on October 10-12, 2014.  The conference brings together statisticians and researchers from other disciplines to discuss the use of statistical techniques in research programs in a variety of disciplines including anthropology, biology, computer science, economics, education, education research methodology, environmental science, information systems, medicine, psychology, and public health. A major emphasis is on training young researchers through two workshops, one on Big Data and Machine Learning, and the other on Mathematical Biology and Game Theory. In addition to these two workshops, the conference includes a variety of sessions on topics that highlight the use of statistical methods in other disciplines.  <br/><br/>The main goal of the workshop on Mathematical Biology and Game Theory is to introduce undergraduates and graduate students to the mathematical modeling that is an essential part of any research in applied mathematics, mathematical biology in particular. The workshop focuses on game theoretical models that are used for situations where several entities interact (directly or indirectly) with one another and where each entity acts in its own interest (potentially in conflict with the interests of others).  The workshop on Big Data and Machine Learning, designed for graduate students, will provide a forum for researchers from academia and industry to exchange information and develop new research directions in areas of big data and machine learning focusing on scalability, reliability, and security. These workshops will help to advance the next generation of researchers on some of the most important contemporary topics in statistical science.  The meeting also offers an opportunity to students from underrepresented groups to be part of a high profile conference to broaden their vision of research in mathematical sciences and advance their career goals. More details are available at the conference website http://www.uncg.edu/mat/aisc/2014/index.html"
298,1356655,ABI Innovation: An Integrative Approach to Identifying Highly Heritable Subtypes of Complex Phenotypes,DBI,"IIBR: Infrastructure Innovatio, FET-Fndtns of Emerging Tech, ADVANCES IN BIO INFORMATICS",7/1/14,8/5/19,Jinbo Bi,CT,University of Connecticut,Standard Grant,Peter McCartney,6/30/20,"$605,717.00 ","Erin Connor, John Cole, Henry Kranzler",jinbo.bi@uconn.edu,438 Whitney Road Ext.,Storrs,CT,62691133,8604863622,BIO,"084Y, 089Y, 1165","1165, 7931, 9179, 9251",$0.00 ,"Identifying genetic variation underlying complex phenotypes aids the understanding of their biology. Complex phenotypes characterized by a variety of features are often associated with substantial phenotypic variation. Current statistical methods are ineffective to address this phenotypic heterogeneity, and hence lack of power to associate genetic variants with the phenotype. This project aims to design new algorithms that differentiate homogenous subtypes of a complex phenotype that are most informative in genetic analysis, and identify genetic variants that are associated with the subtypes but cannot be detected by the non-differentiated phenotype. The validity of the subtypes will be proved in multiple scales including the evidence from genomic structure and phenotypic features. The new algorithms will be validated in the areas of genetic selection for complex traits of agriculturally-important animals and plants. This project serves a vehicle to train graduate students in the multidisciplinary methods involving computer science and biology, and allow them to apply the methods in a variety of biological fields. A new course in the bioinformatics field will be developed for senior undergraduate students. High school educational materials will also be developed to educate high school students about how to mathematically model biological data so it solves biological problems.<br/><br/>This project will derive novel analytics based on quantitative genetics theory and machine learning theory to refine complex phenotypes for enhanced discovery of genotype-phenotype correlations. Using empirical and statistically rigorous methods, this project will derive composite traits, as functions of multiple phenotypic features, that are optimized with respect to narrow-sense heritability, and that map readily to specific genomic regions. Two statistical models for estimating narrow-sense heritability will be considered: one based on sample pedigrees, and the other directly uses the whole-genome markers. To identify multi-scale evidence of a subtype that is characterized by a composite trait, a new machine learning framework will be derived to jointly analyze genotypes and phenotypes. By testing the algorithms in the analysis of large-scale biological databases, the new algorithms will derive highly heritable composite traits for feed efficiency of dairy cattle and adaptive traits of soybean to improve their genetic selection. The algorithms developed in this project will also advance the machine learning field by defining and addressing new research problems, such as the joint model inference using data from multiple sources, probabilistic clustering based on matrix decomposition, and quadratic optimization for heritability estimation. This project will yield user-friendly software tools that can be broadly deployed to biological research areas that study genetics of complex phenotypes. The validated methods and software will be disseminated through the PI?s laboratory website http://www.labhealthinfo.uconn.edu/home/ which provides more information of this project."
299,1407649,Probability Theory and Statistics in High and Infinite Dimensions: Empirical Processes Theory and Beyond,DMS,"PROBABILITY, STATISTICS",5/1/14,3/13/14,Vladimir Koltchinskii,GA,Georgia Tech Research Corporation,Standard Grant,Gabor J. Szekely,4/30/15,"$23,000.00 ",Jon Wellner,vlad@math.gatech.edu,Office of Sponsored Programs,Atlanta,GA,303320420,4048944819,MPS,"1263, 1269",7556,$0.00 ,"The conference on ""Probability Theory and Statistics in High and Infinite Dimensions: Empirical Processes Theory and Beyond"" will focus on empirical processes and other methods of high-dimensional probability and their role in a variety of problems related to statistical inference for high-dimensional data as well as to nonparametric inference. The conference will be held on June 23-25, 2014 at the Center for Mathematical Sciences, University of Cambridge, UK. The list of topics includes concentration inequalities, exponential and moment bounds for Gaussian, empirical and other related processes, nonasymptotic bounds for random matrices, complexity penalization methods and oracle inequalities in high-dimensional inference, sparse recovery and low rank matrix recovery, high-dimensional problems in machine learning, nonparametric estimation and hypotheses testing, Bayesian nonparametrics.<br/><br/>High-dimensional probability and statistics have had impact far beyond mathematical sciences. The methods of high-dimensional probability have been crucial in understanding the performance of new machine learning algorithms and in the design of contemporary methods of analysis of big data. Bringing together researchers who have made and are currenly making important contributions to this area would facilitate the exchange of ideas leading to much better understanding of the role of high-dimensional phenomena in statistical inference and machine learning. The conference would be especially beneficial for junior participants entering these research areas.  This award will support the participation of approximately twenty US-based participants in the conference.<br/><br/>Conference web site:  www.statslab.cam.ac.uk/~nickl/Site/2014.html"
300,1415460,Eager: Discrete Optimization Algorithms for 21st Century Algorithms,CCF,INFORMATION TECHNOLOGY RESEARC,3/1/14,2/12/14,George Nemhauser,GA,Georgia Tech Research Corporation,Standard Grant,Tracy J. Kimbrel,2/28/17,"$300,000.00 ","Santosh Vempala, Maria-Florina Balcan, Santanu Dey",george.nemhauser@isye.gatech.edu,Office of Sponsored Programs,Atlanta,GA,303320420,4048944819,CSE,1640,"7723, 7916",$0.00 ,"Motivation.<br/>Discrete optimization problems have been studied by both the optimization and computer science communities. This EAGER project proposes to develop new approaches by synthesizing the ideas from the two communities by interdisciplinary collaboration between computer scientists and optimization specialists in mathematics and engineering. The goal is to develop rigorous and systematic approaches to discrete optimization problems by combining insights and state-of-the-art methods from integer programming (e.g., cutting planes and branch-and-bound) and algorithmic ideas from computer science (including online algorithms and machine learning). To be able to evaluate and compare methods on real-world instances, PIs plan to develop a theory of ""natural instances"". This theory starts from the application side, tries to model properties of the instances that arise from that application and then give guarantees for algorithms on such instances. The project plans to incorporate machine learning into algorithms to solve integer programs.  PIs propose a systematic development of an algorithmic theory of cutting planes to identify families of instances for which they are provably efficient and to take advantage of the computational benefits of sparse cutting planes. <br/><br/>Intellectual Merit. The ideas that are proposed here are novel and intellectually challenging. The notion of natural systems, insofar as known, has not been investigated previously. While machine learning algorithms currently use optimization technology, the reverse has not been systematically studied. Cutting planes are mainly used on an ad-hoc basis. There is no systematic theory regarding problems for which cutting plane algorithms are provably efficient. It is known that sparsity is needed in the linear algebra of cutting plane algorithms, but there is very little known about polyhedra that are defined by only sparse inequalities.<br/><br/>Broader Impact. This EAGER project is motivated by a range of optimization problems of high societal impact from transportation and supply chain logistics such as palletizing, package delivery, petro-chemical cargo routing, vehicle and mobile robot routing to energy production and distribution. The ultimate goal is to study algorithmic optimization methods which over the next decade could lead to more than a billion dollars in annual savings in energy production and distribution, a 50% reduction in fuel consumption in supply chains and elimination of energy shortages caused by extreme weather disturbances. This will be possible by the creation of optimization algorithms that are orders of magnitude faster, more robust and capable of dealing with massive and messy data. This EAGER will provide research training for both graduate and postgraduate students and intensive collaboration among multiple EAGERs, and will provide rapid dissemination of the research throughout the relevant communities by hosting a workshop at Georgia Tech."
301,1350983,CAREER: New Representations of Probability Distributions to Improve Machine Learning --- A Unified Kernel Embedding Framework for Distributions,IIS,Robust Intelligence,5/15/14,5/7/18,Le Song,GA,Georgia Tech Research Corporation,Continuing Grant,Rebecca Hwa,4/30/21,"$499,721.00 ",,lsong@cc.gatech.edu,Office of Sponsored Programs,Atlanta,GA,303320420,4048944819,CSE,7495,"1045, 7495",$0.00 ,"Computational intelligence touches our lives daily.  Web searches, weather prediction, detecting financial fraud, medicine and education benefit from this ubiquitous technology.  Problems in computational intelligence such as image classification and predicting properties of new materials produce copious amounts of high-dimensional, complex data.  Many algorithms in computational intelligence rely on probability distributions, and such data can carry unusual distributions that challenge traditional methods of modeling.  (For example, they are typically not textbook distributions such as the Gaussian.)  In some applications, the data input to the algorithms are themselves probability distributions.  Existing techniques are cannot both capture unusual distributions and scale to millions of data points without stalling the computation.  There is a pressing need for a flexible, efficient framework for representing, learning, and reasoning about datasets arising from these problems.<br/><br/>This project will address these challenges by developing a novel and unified framework to represent and model, learn, and use probability distributions in computational intelligence.  To evaluate the utility of the new techniques, the project will test them on difficult real-world problems in computer image analysis, materials science, and flow cytometry (a biotechnology technique used for cell counting, cell sorting, and protein engineering).<br/><br/>The project, an NSF CAREER award, will integrate the research results with several education intiatives.  New curricula will be designed for both undergraduate and graduate students, with empahsis on students from under-represented groups.  A new online course will be created to make the results accessible to massive online masters students.  Finally, advanced high school math teachers will be engaged to design problems related to the reserach for use in a math competition for advanced high school students.<br/><br/>This project will (1) create a novel and unified nonparametric kernel framework for distributional data and distributions with fine-grained statistical properties, and (2) develop principled and scalable algorithms for nonparametric analysis of big data. The unified kernel embedding framework will advance large scale nonparametric data analysis significantly, and play an important synergistic role in bridging together traditionally separate research areas in data analysis, including kernel methods, graphical models, optimization, nonparametric Bayesian methods, functional analysis and tensor data analysis. In addition to advances in algorithmic methods, the applications to large-scale image classification, flow cytometry, and materials property prediction have the potential for transformative impact on society."
302,1445755,"CIRCLE: Catalyzing and Integrating Research, Collaboration, and Learning in Computing, Mathematics, and their Applications",CCF,"Special Projects - CNS, Special Projects - CCF, CYBERINFRASTRUCTURE, IIS Special Projects",8/15/14,6/18/20,David Pennock,NJ,Rutgers University New Brunswick,Continuing Grant,Tracy Kimbrel,7/31/21,"$1,199,934.00 ","Rebecca Wright, Tamra Carpenter",dpennock17@gmail.com,33 Knightsbridge Road,Piscataway,NJ,88543925,8489320150,CSE,"1714, 2878, 7231, 7484","7361, 7482, 7556, 8090",$0.00 ,"The DIMACS Project on Catalyzing and Integrating Research, Collaboration, and Learning in Computing, Mathematics, and their Applications provides a resource for a large community of researchers and educators in computer science and related mathematical and statistical areas as well as their collaborators in fields such as biology, chemistry, physics, engineering, public health, business, and the social sciences. The project enhances and encourages programs that emphasize fundamental methods and theory to advance computer science and related mathematics while also encouraging their application in areas with the potential to impact the infrastructure and resources society depends on, including healthcare, environmental sustainability, homeland security, information security, energy, and business. The project also reflects the importance of education at DIMACS and works to build and support communities that facilitate entry into and promote retention and success in computing disciplines.  This award partially supports the DIMACS Center infrastructure required to achieve project goals. The programs enabled by the infrastructure this project supports will directly reach an estimate of roughly 2000 people each year, and through them, many others.<br/><br/>Research programs enabled by the project are organized around special focus programs consisting of workshops, research workshop groups, tutorials, and a visitor program, on topics including Cybersecurity; Information Sharing and Dynamic Data Analysis; Algorithms and Energy; Sustainability; Theoretical Foundations and Scalability of Machine Learning; Health and Medical Informatics; Cryptography; and Analytics of Preference, Opinion, Recommendation, and Comparison. Educational programs integrate research and education across levels from precollege through postdoctoral via activities including an extensive Research Experiences for Undergraduates program; a Reconnect program run at satellite locations around the country for 2-year and 4-year college faculty highlighting recent research topics relevant to the classroom; a year-round program of workshops for middle and high school teachers; and development of classroom materials in areas including bio-mathematics, computational thinking, and planning for sustainability.  Connections with researchers in education enhance understanding of the cognitive mechanisms at play when students master computational thinking skills, allowing creation of research-informed materials that facilitate teaching and enhance learning. Several DIMACS programs use multi-layer mentoring models for providing mentoring to students while instilling in them the capacity to mentor, both enhancing their own skills and serving to build excitement for computing and computing-related careers in the next generation."
303,1349677,Collaborative Research: A behavioral and computational investigation of the generality and transferability of category representations,BCS,"PERCEPTION, ACTION & COGNITION",7/15/14,8/1/16,Sebastien Helie,IN,Purdue University,Standard Grant,Lawrence Gottlob,6/30/18,"$380,326.00 ",,shelie@purdue.edu,Young Hall,West Lafayette,IN,479072114,7654941055,SBE,7252,"7252, 7956, 8089, 8091, 8605, 9251",$0.00 ,"How people acquire and use different types of knowledge is a fundamental issue in cognitive science, applicable to problems in education, training, and the development of expertise. For example, learning to categorize types of materials (such as natural vs. synthetic, or polymers vs. ceramics) can be accomplished using textbooks, but it can also be accomplished with hands on experience in the field. Different types of training likely lead to different forms of knowledge, and the form of knowledge may constrain how, and in what situations, the knowledge can be used. Thus, the best way to train a person may differ depending not only on the type of information being learned, but also on the situations in which the knowledge will need to be used. The investigators will examine how to promote the learning of different forms of knowledge in different situations. They will also investigate the neural and computational bases of the differences in forms of knowledge in order to develop a unifying theory of how knowledge acquisition and application varies across situations in predictable ways. A larger goal is to determine how knowledge, once learned, can be transferred to new situations. This project will advance scientific understanding of human knowledge acquisition and its use. The project also has the potential to foster the development of new tools that may improve the training of students and professionals.<br/><br/>Categorization researchers have made an enormous contribution to the understanding of the many ways in which knowledge can be represented and used. A current challenge facing the field of categorization, and cognitive science more generally, is how to best integrate these findings with the broader goal of understanding the extent to which different types of knowledge representations generalize to new situations. The proposed research utilizes a combination of behavioral, neuroimaging, and computational modeling techniques to address these challenges. Specifically, the investigators will explore (1) the factors influencing the development of different types of category representations, (2) the psychological functions and brain networks supporting category representations, and (3) the utility of different types of category representations for supporting performance in new tasks and/or with new stimuli. In addition, this research will highlight important relationships between machine learning techniques and methods used in cognitive science. As a result, the research should be of broad interest to psychologists, computer scientists, and the general cognitive science community."
304,1513324,RAPID: III: Data Collection and Risk Evaluation Learning in Identifying High Risk Ebola Subpopulations for the Intervention and Prevention of Large-scale Ebola Virus Spreading,IIS,"Information Technology Researc, Info Integration & Informatics",12/1/14,12/11/15,Fengjun Li,KS,University of Kansas Center for Research Inc,Standard Grant,Sylvia Spengler,11/30/16,"$188,730.00 ","Bo Luo, Fengjun Li, Alfred Ho, Guoqing Chen",fli@ittc.ku.edu,2385 IRVING HILL RD,Lawrence,KS,660457568,7858643441,CSE,"1640, 7364","001Z, 1640, 7364, 7914, 9150",$0.00 ,"The 2014 Ebola epidemic is the largest in history, affecting multiple countries in West Africa, and now impacting the US and other countries worldwide.  The US Center for Disease Control and Prevention (CDC) and partners are taking precautions to prevent the further spread of Ebola within the United States.  There is a lack of public understanding of the risks associated with Ebola; witness the inconsistently applied local responses (such as quarantines) that do not match CDC recommendations.  This project will develop technology to enable individuals to evaluate risks associated with their own past and planned activities and travel.  This will both enable those at risk to take appropriate action, and reduce unwarranted demand on the healthcare system by reassuring those whose activities have not placed them at risk.  This project will use data gathered from the CDC and other public sources to develop risk models, and develop a mobile app that will use this data along with the user's own location and activity history and plans to report individual risk to the user.  An individual's data never leaves their own device, ensuring personal privacy.  The resulting lessons learned will ease the process of developing similar individualized risk assessment tools for future epidemics, providing long-term benefits beyond the Ebola virus epidemic.<br/><br/>The research will address three main issues.  The first is focused crawling of structured (CDC Contact Tracing reports) and unstructured (social media, web blogs) information on time, location, and activities of Ebola patients.  A second research challenge is patient activity modeling:  Given the returned information, developing a time/space/activity model determining the risk of the patient acting as a transmission agent.  Finally, the project will develop a mobile app that tracks time, location, and activities of the mobile device user, and retrieves the patient activity models developed from public data to determine if the user is at risk of infection.  This is a complex problem, as the data may be non-specific and require inferential techniques to estimate risk (e.g., being in the same time/location as a transmission agent poses very different risk if the location is a sports stadium as opposed to a restaurant); the project will develop ontologies for activities to use in estimating risk.  The project will use expert opinion to seed regression models for risk assessment.  Lessons learned from this project will also identify challenges for future research in information integration, risk analysis, machine learning, and privacy preserving technologies."
305,1406356,2014-2016 Talbot Workshops,DMS,TOPOLOGY,6/15/14,6/7/14,Haynes Miller,MA,Massachusetts Institute of Technology,Standard Grant,Joanna Kania-Bartoszynska,5/31/17,"$67,690.00 ",,hrm@math.mit.edu,77 MASSACHUSETTS AVE,Cambridge,MA,21394301,6172531000,MPS,1267,7556,$0.00 ,"This project supports three years of MIT Talbot workshops. This is an annual event organized by graduate students. The 2014 MIT Talbot Workshop took place March 16-22 in Pigeon Forge, Tennessee. Professor Marc Levine of the Universitaet Duisburg-Essen led the workshop, which was entitled Motivic Homotopy Theory.  Since their initiation in 2004, the MIT Talbot Workshops have provided a unique opportunity for some thirty young researchers - graduate students and postdocs - to spend a week with an expert in their field in an intimate and informal setting. Each workshop is on a different topic, chosen for its broad interest and currency. Qualities of clarity, leadership, and generosity are important in the selection of mentor. Numerous strong research relationships have had their origin in these workshops over the years.<br/><br/>Motivic homotopy theory, the subject of the 2014 workshop, is a vibrant and rapidly expanding research area with origins in the solution of the Milnor conjecture about algebraic K-theory by Vladimir Voevodsky (who won the Fields Medal for this work). It has attracted an increasing number of participants from both algebraic geometry and algebraic topology, because of its intrinsic beauty and its proven power as a tool for attacking problems in algebraic K-theory and, more recently, in homotopy theory. It provided an ideal subject for the Talbot Workshop, bringing together very bright and motivated young researchers from distinct fields in a context which encouraged deep learning and collaboration, under the inspired leadership of Marc Levine, one who has made profound contributions to the subject.<br/><br/>The website http://math.mit.edu/conferences/talbot/ contains descriptions of and reports resulting from all eleven MIT Talbot Workshops."
306,1362112,Planning Grant:  I/UCRC for Identification Technology Research,ITR,INDUSTRY/UNIV COOP RES CENTERS,1/1/14,5/20/14,Arun Ross,MI,Michigan State University,Standard Grant,Dmitri Perkins,12/31/15,"$14,080.00 ",Anil Jain,rossarun@cse.msu.edu,Office of Sponsored Programs,East Lansing,MI,488242600,5173555040,CSE,5761,"5761, 8039",$0.00 ,"Michigan State University (MSU) proposes to join the existing Center for Identification Technology Research (CITeR) as a university site. This addition will expand the research portfolio of CITeR as well as bring new affiliates whose partnership will be instrumental in furthering the frontiers of identification technology. This extended collaboration framework will broaden CITeRs leadership role in transitioning biometric research into practice while influencing socio-legal policies emerging from the use of this technology. Biometrics is the science of automatically recognizing individuals based on their physical, behavioral and physiological attributes such as fingerprints, iris, voice, and gait. As a discipline, biometrics relies on several fields including engineering, statistics, mathematics, biology, physics, jurisprudence, and computational sociology. As a technology, biometrics has applications in cyber-security, law enforcement, surveillance, forensics, multimedia, mobile devices, and healthcare. While the field has significantly matured over the past two decades, several fundamental problems are yet to be addressed. A concerted research, education, and industry-engagement model is needed to effectively leverage and deploy biometric solutions for the benefit of society at large. MSU I/UCRC site proposes to include faculty and affiliates in the areas of cyber security, mobile computing, genomics and big data. The goal of the planning meeting will be to formulate a research roadmap for engaging the research expertise of MSU faculty in furthering the societal and technological impact of CITeR while simultaneously addressing the needs of the affiliates. MSUs expertise in DNA processing, forensic analysis, cloud computing, deep learning and data mining will be used to design next generation identification systems focusing on rapid DNA identification, forensic data analysis, cloud-based biometrics, genomics, and large-scale biometric analytics. Data security and privacy will be an integral component of this research agenda and will benefit affiliates in the field of identity management, healthcare, access control, personalization, forensics, computer security, national security, and genomic analysis. <br/><br/>The active involvement of graduate and undergraduate researchers will ensure the development of a strong biometric workforce consisting of future leaders, visionaries, researchers, professionals, and technicians. Collaboration with experts from other fields such as forensics, bioinformatics, health science and cybersecurity will expand the domain-of-influence of biometrics whilst creating novel opportunities for inter-disciplinary research and education. The site directors will connect with existing programs in MSU such as ADAPP (Advancing Diversity through the Alignment of Policies and Practices) and MSU WIC (Women in Computing) in order to promote the involvement of underrepresented groups in STEM research and education. The research conducted in CITeR will be showcased to middle school and high school students through community activities such as MSU Science, Engineering and Technology Day; MSU College of Engineering Design Day; and MSU CSE Research Open House."
307,1351028,CAREER: Apprenticeship Learning for Robotic Manipulation of Deformable Objects,IIS,Robust Intelligence,3/15/14,3/10/14,Pieter Abbeel,CA,University of California-Berkeley,Standard Grant,Reid Simmons,2/28/19,"$500,000.00 ",,pabbeel@cs.berkeley.edu,Sponsored Projects Office,BERKELEY,CA,947101749,5106433891,CSE,7495,"1045, 7495",$0.00 ,"This project considers the problem of apprenticeship learning, in which a robot first gets access to demonstrations of a task and ought to learn from these demonstrations how to perform that task in new, yet similar, situations.  This line of work has already shown significant promise, including in helicopter control where it enabled autonomous helicopter aerobatics at the level of the best human pilots.  However, fundamental limitations remain, and robotic capabilities to manipulate deformable objects are currently still well below human level. The approach followed builds on, and extends, non-rigid registration algorithms, which can capture how scenes with deformable objects relate to each other.  Such registration is extrapolated to morph a demonstrated manipulation trajectory into a good trajectory for a new scene.  New machine learning algorithms are developed to enable choosing the optimal training demonstration and the optimal morphing objective while accounting for external constraints, such as avoiding collisions and satisfying joint limits. Infrastructure is being built for large-scale data collection of demonstrations and theoretical and empirical characterizations are developed for how much data is needed for a given task.  Concrete challenge tasks considered are knot tying, cloth and fabric manipulation, surgical suturing, and small surgical procedures. Results will be incorporated into the PI's graduate robotics course and the source code will be shared with the robotics community."
308,1349737,Collaborative Research: A behavioral and computational investigation  of the generality and transferability of category representations,BCS,"Perception, Action & Cognition",7/15/14,6/28/14,Shawn Ell,ME,University of Maine,Standard Grant,Lawrence Gottlob,6/30/19,"$260,086.00 ",,shawn.ell@maine.edu,5717 Corbett Hall,ORONO,ME,44695717,2075811484,SBE,7252,"7252, 9150",$0.00 ,"How people acquire and use different types of knowledge is a fundamental issue in cognitive science, applicable to problems in education, training, and the development of expertise. For example, learning to categorize types of materials (such as natural vs. synthetic, or polymers vs. ceramics) can be accomplished using textbooks, but it can also be accomplished with hands on experience in the field. Different types of training likely lead to different forms of knowledge, and the form of knowledge may constrain how, and in what situations, the knowledge can be used. Thus, the best way to train a person may differ depending not only on the type of information being learned, but also on the situations in which the knowledge will need to be used. The investigators will examine how to promote the learning of different forms of knowledge in different situations. They will also investigate the neural and computational bases of the differences in forms of knowledge in order to develop a unifying theory of how knowledge acquisition and application varies across situations in predictable ways. A larger goal is to determine how knowledge, once learned, can be transferred to new situations. This project will advance scientific understanding of human knowledge acquisition and its use. The project also has the potential to foster the development of new tools that may improve the training of students and professionals.<br/><br/>Categorization researchers have made an enormous contribution to the understanding of the many ways in which knowledge can be represented and used. A current challenge facing the field of categorization, and cognitive science more generally, is how to best integrate these findings with the broader goal of understanding the extent to which different types of knowledge representations generalize to new situations. The proposed research utilizes a combination of behavioral, neuroimaging, and computational modeling techniques to address these challenges. Specifically, the investigators will explore (1) the factors influencing the development of different types of category representations, (2) the psychological functions and brain networks supporting category representations, and (3) the utility of different types of category representations for supporting performance in new tasks and/or with new stimuli. In addition, this research will highlight important relationships between machine learning techniques and methods used in cognitive science. As a result, the research should be of broad interest to psychologists, computer scientists, and the general cognitive science community."
309,1350984,CAREER: Efficient Learning of Personalized Strategies,IIS,Robust Intelligence,6/1/14,5/6/15,Emma Brunskill,PA,Carnegie-Mellon University,Standard Grant,James Donlon,12/31/17,"$680,210.00 ",,ebrun@cs.stanford.edu,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,CSE,7495,"1045, 7495, 9251",$0.00 ,"Online retailers frequently provide tailored product or movie recommendations. But the power of automated personalization, driven by data and statistics, could be far greater: imagine the impact on poverty reduction if all children had a personalized, self-improving tutoring system as part of their education. To realize this vision requires personalization systems that reason about both the immediate impact of a recommended item (e.g. will a learner immediately learn from a video lecture) as well as its longer term impact. For example, a recommended item or intervention may cause a user to change his/her preferences, state of knowledge, or reveal information about the user that was previously unknown. This requires methods for creating personalized strategies: adaptive rules about what decisions to make (whether or which ad to show, which pedagogical activity to provide) in which circumstances to maximize for long term outcomes. <br/><br/>This research involves developing new data-driven, machine learning approaches to construct such personalized strategies for related individuals, and using them towards improving the effectiveness of online mathematics educational systems.  The project frames personalized strategy creation as sequential decision making under uncertainty research. Though there have been many advances in sequential decision making under uncertainty, existing approaches have focused primarily on other application areas, like robotics, and fail to account or leverage for some of the special features that arise when interacting with people. These include that accurate simulation of people is difficult but prior data is often available, and that individuals are often related. This project contributes algorithms for mining existing datasets to create and precisely bound the expected performance of new high-quality strategies and for online policy learning across a series of similar sequential decision making tasks."
310,1409942,CSR: Medium: Building next-generation cloud infrastructure using RDMA,CNS,Computer Systems Research (CSR,8/1/14,9/7/17,Jinyang Li,NY,New York University,Continuing grant,Marilyn McClure,7/31/18,"$678,457.00 ",,jinyang@cs.nyu.edu,70 WASHINGTON SQUARE S,NEW YORK,NY,100121019,2129982121,CSE,7354,"7924, 9102",$0.00 ,"As computation and data are moved to the ""cloud"", there is a need to significantly increase the capacity and performance of the underlying data centers through improved hardware and communication infrastructure.  A key component of the communication infrastructure is the network implementation, currently based upon Ethernet. Technological trends suggest that next-generation data center networks will be based upon RDMA (Remote Direct Memory Access), a powerful communciation technology used today in high performance computer systems. While RDMA offers enormous performance potential, current cloud infrastructures are not designed to exploit this potential.  This project will investigate RDMA-based system designs for distributed storage systems and in-memory applications.<br/><br/>Specifically, the project research agenda consists of (a) building a high performance distributed key-value store and a B-tree based store (b) developing several applications in image search and deep learning whose distribution is made feasible by RDMA's ultra-low-latency and high throughput; (c) evaluating the performance of the resulting systems and applications at scale on the NSF-funded PRObE testbed."
311,1408652,The dynamics of updating and transmitting individual and collective memories,SMA,SPRF-IBSS,7/1/14,6/2/14,Thomas Griffiths,CA,University of California-Berkeley,Standard Grant,Josie S. Welkom,6/30/16,"$172,425.00 ",Jordan Suchow,tomg@princeton.edu,Sponsored Projects Office,BERKELEY,CA,947101749,5106433891,SBE,8209,,$0.00 ,"Memory and culture form the foundation of human knowledge. The accumulation of knowledge across generations drives scientific progress, technological advancement, and the maintenance of societies through shared social norms. Understanding the fundamental cognitive processes that shape memory and cultural transmission has traditionally fallen within the purview of psychologists and anthropologists. However, advances in fields beyond the social sciences, including statistics, machine learning, evolutionary dynamics, and network science offer a rich set of computational techniques for describing information transfer in individuals and groups that can be fruitfully applied to understand memory maintenance and cultural transmission. The proposed project, if successul, will create a software platform for running experiments on personal and cultural memory online, thereby enabling others to conduct their own experiments using these paradigms. Additionally, the public looks toward memory research with the hope that it will one day provide techniques to help them remember more. Better understanding the processes that affect maintenance of personal and collective memories will reveal the conditions under which memories and cultural innovations are best maintained and possible techniques for improving them. Any progress towards this goal would provide a major benefit to society. Planned dissemination of the results of the proposed project are aimed at increasing public understanding of science through visualization of cultural memory dynamics. The Fellow has a history of public engagement and activities supporting inclusion and broadening participation, and continues to be engaged in these activities.<br/><br/>The goal of the proposed project is to construct computational models that capture the cognitive dynamics of memory and culture, with the ultimate goal of understanding (and eventually controlling and improving) the processes that shape them. The proposed project uses behavioral experiments, simulations, and analytic calculations to determine the cognitive dynamics of personal and cultural memory in three settings. Objective 1 considers the dynamics of updating in short-term memory. Objective 2 considers the dynamics of updating culturally-transmitted knowledge when individuals have varying amounts of information about the social context in which they are placed. Objective 3 considers the dynamics of learning in cultural memory, exploring whether individuals can adjust their own behavior to better sustain memories in a group. Each objective makes connections between cognitive processes and formal tools from machine learning, evolutionary dynamics, and network science."
312,1408717,Collaborative Research: CDS&E: Systematic Multiscale Modeling using the Knowledgebase of Interatomic Models (KIM),DMR,"CONDENSED MATTER & MAT THEORY, CI REUSE, CDS&E",10/1/14,6/29/15,James Sethna,NY,Cornell University,Continuing grant,Daryl Hess,9/30/18,"$442,599.00 ",,jps6@cornell.edu,373 Pine Tree Road,Ithaca,NY,148502820,6072555014,MPS,"1765, 6892, 8084","7237, 7433, 8084, 9216",$0.00 ,"NONTECHNICAL SUMMARY<br/><br/>This award supports OPENKIM which supports the community of researchers using computer simulations of atoms based on Newton's Laws to attack materials science, chemistry, engineering, and physics problems enabling the discovery of new materials, the design of new devices, the understanding of biochemical processes and much more.  Atomistic simulations play a key role in realistic scientific, engineering, and industrial applications. These simulations increasingly use fitted interatomic models (IMs), mathematical prescriptions that describe the forces acting on atoms when they interact, to predict the properties of materials, the way they respond to external stresses, and to design innovative nanostructures, tiny structures of atoms some 100,000 times smaller than a human hair. In the past the potential of atomistic simulations of this kind has been limited by several factors: (1) the lack of a standardized application programing interface has made it difficult to transfer IMs from one simulation program to another; (2) the lack of a curated electronic repository for storing and exchanging computer implementations of IMs has made it difficult to reproduce published results; (3) the lack of tools for comparing the accuracy of IMs made it difficult to use IMs with confidence in new applications. These limitations have been addressed by the creation of the Open Knowledgebase of Interatomic Models (OpenKIM), a collaborative online materials project to rationalize, standardize, and characterize IMs. This award supports OpenKIM as it goes forward in important ways that will facilitate scientific and engineering progress in fields from growing electronic circuits to airplane manufacture. It will make sharp evaluations and comparisons between rival IMs and simulation methods, allowing computational researchers to rapidly explore alternative published IMs or develop and validate new ones for their use. It will also facilitate replication of results in scientific simulations. This project will extend OpenKIM in order to draw the computational chemistry and molecular biology communities into this materials endeavor, facilitating communication between two communities with common goals and interests but hitherto divided by language, units, and computational conventions. Students and post-docs in the group have the opportunity of collaborating with an international, interdisciplinary group of well-known scientists and engineers on a cross-section of challenging scientific problems, such as the role of defects in determining properties of materials and the effect of unsatisfied chemical bonds in electronic device operation. By lowering the barriers to entry into computational materials science, OpenKIM is facilitating the entry of underrepresented groups and those from developing nations into this technologically and scientifically central field.<br/><br/><br/><br/>TECHNICAL SUMMARY<br/><br/>This award supports OpenKIM, a collaborative online materials project to rationalize, standardize, and characterize interatomic models (IMs) used to represent energies and forces between atoms in materials simulations. This project is aimed to support, extend, and leverage OpenKIM to do science. The Principal Investigators will blend the wisdom and experience of the materials community with advanced methods from machine learning, data mining, and information geometry to radically simplify and make more rigorous the field of atomistic simulations of materials. OpenKIM represents an unusual opportunity to answer fundamental scientific questions. With full and open access, the PIs anticipate many researchers will use the rich OpenKIM Repository to address scientific and methodological questions of the field. The PIs will support these activities by incorporating new IMs, reference data, and tests, by extending the KIM standard to support long-range electrostatic fields, Monte Carlo, and biomolecular bonded force fields, and by continuing to provide documentation, talks, workshops, and tutorials on KIM. To further the KIM mission, the PIs will address two broad and fascinating issues of critical importance to successful sequential multiscale modeling: (1) What key features does an IM need to reproduce in order to accurately model phenomenon X at a continuum scale? The project will provide tools to answer this question, by (a) developing functional forms for anisotropic materials properties to encapsulate the behavior of known defects and interfaces which are properties already identified as vital for continuum simulation of microstructure evolution, and (b) using manifold-learning methods gleaned from information geometry theory, which applies the techniques of differential geometry to the field of probability theory, to find empirical heuristics or rules that provide insight into the higher scale behavior of a class of IMs, and insight on the real world. (2) How reliable will a given IM be for a given application X? The PIs will address this component of uncertainty quantification, also called IM transferability, by (a) using machine-learning techniques to identify key interatomic configurations which strongly correlate with important continuum scale materials properties and using statistical methods to estimate IM uncertainties for these configurations, and (b) using the large uncertainties in IM fitted parameters to provide Bayesian information geometry estimates for the systematic errors in IM predictions."
313,1408211,Collaborative Research: CDS&E: Systematic Multiscale Modeling using the Knowledgebase of Interatomic Models (KIM),DMR,"CONDENSED MATTER & MAT THEORY, CI REUSE, CDS&E",10/1/14,7/7/15,Ellad Tadmor,MN,University of Minnesota-Twin Cities,Continuing grant,Daryl Hess,9/30/18,"$997,401.00 ",Ryan Elliott,tadmor@aem.umn.edu,200 OAK ST SE,Minneapolis,MN,554552070,6126245599,MPS,"1765, 6892, 8084","7237, 7433, 8084, 9216",$0.00 ,"NONTECHNICAL SUMMARY<br/><br/>This award supports OPENKIM which supports the community of researchers using computer simulations of atoms based on Newton's Laws to attack materials science, chemistry, engineering, and physics problems enabling the discovery of new materials, the design of new devices, the understanding of biochemical processes and much more.  Atomistic simulations play a key role in realistic scientific, engineering, and industrial applications. These simulations increasingly use fitted interatomic models (IMs), mathematical prescriptions that describe the forces acting on atoms when they interact, to predict the properties of materials, the way they respond to external stresses, and to design innovative nanostructures, tiny structures of atoms some 100,000 times smaller than a human hair. In the past the potential of atomistic simulations of this kind has been limited by several factors: (1) the lack of a standardized application programing interface has made it difficult to transfer IMs from one simulation program to another; (2) the lack of a curated electronic repository for storing and exchanging computer implementations of IMs has made it difficult to reproduce published results; (3) the lack of tools for comparing the accuracy of IMs made it difficult to use IMs with confidence in new applications. These limitations have been addressed by the creation of the Open Knowledgebase of Interatomic Models (OpenKIM), a collaborative online materials project to rationalize, standardize, and characterize IMs. This award supports OpenKIM as it goes forward in important ways that will facilitate scientific and engineering progress in fields from growing electronic circuits to airplane manufacture. It will make sharp evaluations and comparisons between rival IMs and simulation methods, allowing computational researchers to rapidly explore alternative published IMs or develop and validate new ones for their use. It will also facilitate replication of results in scientific simulations. This project will extend OpenKIM in order to draw the computational chemistry and molecular biology communities into this materials endeavor, facilitating communication between two communities with common goals and interests but hitherto divided by language, units, and computational conventions. Students and post-docs in the group have the opportunity of collaborating with an international, interdisciplinary group of well-known scientists and engineers on a cross-section of challenging scientific problems, such as the role of defects in determining properties of materials and the effect of unsatisfied chemical bonds in electronic device operation. By lowering the barriers to entry into computational materials science, OpenKIM is facilitating the entry of underrepresented groups and those from developing nations into this technologically and scientifically central field.<br/><br/><br/><br/>TECHNICAL SUMMARY<br/><br/>This award supports OpenKIM, a collaborative online materials project to rationalize, standardize, and characterize interatomic models (IMs) used to represent energies and forces between atoms in materials simulations. This project is aimed to support, extend, and leverage OpenKIM to do science. The Principal Investigators will blend the wisdom and experience of the materials community with advanced methods from machine learning, data mining, and information geometry to radically simplify and make more rigorous the field of atomistic simulations of materials. OpenKIM represents an unusual opportunity to answer fundamental scientific questions. With full and open access, the PIs anticipate many researchers will use the rich OpenKIM Repository to address scientific and methodological questions of the field. The PIs will support these activities by incorporating new IMs, reference data, and tests, by extending the KIM standard to support long-range electrostatic fields, Monte Carlo, and biomolecular bonded force fields, and by continuing to provide documentation, talks, workshops, and tutorials on KIM. To further the KIM mission, the PIs will address two broad and fascinating issues of critical importance to successful sequential multiscale modeling: (1) What key features does an IM need to reproduce in order to accurately model phenomenon X at a continuum scale? The project will provide tools to answer this question, by (a) developing functional forms for anisotropic materials properties to encapsulate the behavior of known defects and interfaces which are properties already identified as vital for continuum simulation of microstructure evolution, and (b) using manifold-learning methods gleaned from information geometry theory, which applies the techniques of differential geometry to the field of probability theory, to find empirical heuristics or rules that provide insight into the higher scale behavior of a class of IMs, and insight on the real world. (2) How reliable will a given IM be for a given application X? The PIs will address this component of uncertainty quantification, also called IM transferability, by (a) using machine-learning techniques to identify key interatomic configurations which strongly correlate with important continuum scale materials properties and using statistical methods to estimate IM uncertainties for these configurations, and (b) using the large uncertainties in IM fitted parameters to provide Bayesian information geometry estimates for the systematic errors in IM predictions."
314,1412958,AF: Large: Theory of Computation - Pushing the State-of-the-Art,CCF,"CYBERINFRASTRUCTURE, Algorithmic Foundations",9/1/14,8/1/18,Avi Wigderson,NJ,Institute For Advanced Study,Continuing Grant,Tracy Kimbrel,8/31/20,"$2,000,003.00 ","TONI PITASSI, Ran Raz",avi@math.ias.edu,EINSTEIN DRIVE,PRINCETON,NJ,85404907,6097348000,CSE,"7231, 7796","7925, 7927",$0.00 ,"This project is aimed at understanding a variety of fundamental questions in the theory of computation.  It will be carried out via the postdoctoral mentoring program at the Institute for Advanced Study, and as such the specific topics of focus will evolve with the postdocs present each year. Current foci include, among others, the following:<br/><br/>- The power of formulas. <br/>Formulas is the most basic mathematical and computational descriptive mechanisms. Understanding their minimal length for natural problems captures at once limitation on the space requirements, as well as the potential parallelism inherent in the problem. The award will focus on the most challenging direction, which has so far resisted attack - proving limitation of formulas. More generally, the researchers will pursue proving limitations of other natural computational models, especially arithmetic computation.<br/><br/>- The power of relaxations<br/>The ""meta-algorithms"": Linear and semi-definite relaxations of integer programs, are among the most fruitful and powerful techniques for solving (or finding approximate solutions) to optimization problems. The award will focus on understanding the limits of these techniques. This work ties in naturally to understanding the limitations of natural proof systems and of natural strategies for search algorithms.<br/><br/>- Peeking inside the ""black-box""<br/>One of the most useful paradigms in programming and learning is the encapsulation of objects as black-boxes, to which only input-output access is allowed.  With this utility come limitations which can hopefully overcome if we are allowed some access into the internal workings of the black box.  Modeling such access, in scientific experiments, machine learning and computational complexity is a challenge the researchers plan to pursue.<br/><br/><br/>Computational complexity, a foundational core of computer science, has proved itself a remarkably deep and fruitful fountain of problems, ideas and techniques over the past decades.  The research agenda is expected to be a driver of innovation in Theoretical Computer Science and related disciplines. Some of the areas of study have potential implications outside theory, especially machine learning, coding theory, scientific discovery and more. <br/><br/>On the educational side, the mentoring program furthers the quality of IAS postdocs to serve as outstanding teachers, graduate advisors and academic leaders and innovators in one of the most exciting branches of science today.  Whether IAS alumni pursue a career in academia or industry their impact on technology and on the training of new generations undergraduate and graduate education is immense."
315,1419219,"Conference on Nonparametric Statistics for Big Data, June 4-6, 2014",DMS,STATISTICS,6/1/14,3/21/14,Brian Yandell,WI,University of Wisconsin-Madison,Standard Grant,Gabor Szekely,5/31/15,"$10,000.00 ","Xiaotong Shen, Hao Zhang",byandell@wisc.edu,21 North Park Street,MADISON,WI,537151218,6082623822,MPS,1269,7556,$0.00 ,"The Department of Statistics at the University of Wisconsin at Madison will host a Workshop on Nonparametric Statistics for Big Data (June 4-6, 2014, Madison, WI). Nonparametric statistics is a fundamental area of statistics, at the interface of mathematics, statistics, data mining, engineering, and computer science. The complexity and scale of big data impose tremendous challenges for knowledge discovery; they meanwhile demand more powerful and flexible analysis techniques. In recent years, the field of nonparametric statistics has seen significant development in theory, methods, and computation to address emerging issues in big data analysis. New breakthroughs in nonparametric theory have broadened the horizon of classical large-sample asymptotic inferences to accommodate high and ultra-high dimensional situations. A variety of cutting-edge statistical methodologies and state-of-art computational algorithms have been created for big data visualization, geometric representation, dimension reduction, and modeling and inference. These tools have made significant impacts on sciences, engineering, and industry. A broad range of topics will be covered in the workshop, including sparse nonparametric regression, regularization and feature selection, high-dimensional inference and theory, spatial and environmental statistics, image analysis, functional data analysis, as well as related topics in statistical machine learning such as supervised learning, clustering, network analysis, large-scale optimization, computational biology and bioinformatics.<br/><br/>Due to recent technology advances, Big Data are collected ubiquitously in many scientific investigations, such as in biological, genomic, medical, climate, social, and environmental sciences. Given the complexity and huge range of systems being measured, a wealth of new ""nonparametric"" tools have been emerging that require few assumptions and that adapt to the patterns found in big data. This workshop will bring together broad interdisciplinary expertise from mathematics, statistics, computer science, machine learning, engineering, and biomedical research to highlight cutting-edge research from nationally and internationally renowned scholars and researchers. The workshop will use NSF funding for travel awards to attract graduates students and young researchers, with special attention to women and underrepresented minorites. It will create a unique opportunity for young researchers to interact with leading scientists. Through 30 plenary talks (expository, intermediate, and advanced), open floor discussions, and two poster sections, the workshop will promote new connections and collaborations. Further, this workshop provides an important review that will highlight future research directions nonparametric statistics for big data analysis.<br/><br/>Workshop web site:  http://www.stat.wisc.edu/workshop-npbigdata"
316,1417916,"IMA SUMMER SCHOOL ON MODERN APPLICATIONS OF REPRESENTATION THEORY (SUPPLEMENTARY FUNDING), July 20 - August 6, 2014",DMS,"ALGEBRA,NUMBER THEORY,AND COM, COMM & INFORMATION FOUNDATIONS",6/1/14,4/20/16,Jason Morton,IL,University of Chicago,Standard Grant,Matthew  Douglass,7/31/16,"$39,920.00 ","Lek-Heng Lim, Jason Morton",morton@math.psu.edu,6054 South Drexel Avenue,Chicago,IL,606372612,7737028669,MPS,"1264, 7797",7556,$0.00 ,"A three-week long summer school for graduate students in ""Modern Applications of Representation Theory"" will take place at the University of Chicago from July 20 to August 6, 2014 as part of the Institute for Mathematics and Applications (IMA) Graduate Students Summer Programs.  In mathematics symmetries are described by abstract objects called groups.  Representation theory, originating in the work of mathematicians such as Issai Schur and Ferdinand Frobenius at the end of the 19th century, studies how the structure of groups can be captured by simpler, linear objects, in particular, matrices.  Apart from making groups more concrete, it has long been realized that this is critical for understanding how groups act on other objects.  For example, in physics one is interested in how the symmetry groups of nature, such as translations and rotations, act on physical systems.  This is why, starting in the 1920's, the representation theory of unitary groups, in particular, has been central to the development of quantum mechanics and particle physics.   Recently, surprising connections have also been uncovered between representation theory and problems of a seemingly very different character, such as finding the number of operations required on a computer to carry out arithmetic computations, aligning a large number of noisy images of the same molecule taken by a certain type of electron-microscope, and ranking problems in statistical machine learning.  The aim of the summer school is to engage graduate students from across the sciences in research in these exciting new areas.<br/><br/>In recent years representation theory has found new applications in a range of domains from cryo-electron microscopy, through machine learning, to holographic algorithms.  The fact that the subject is usually taught for an audience in pure mathematics makes it challenging for graduate students in more applied disciplines to learn the material. Conversely, mathematicians are often not aware of the new, exciting applications of representation theory.  This summer school attempts to bridge this gap by starting with a short introduction to representation theory, followed by a series of mini-courses on some of the most recent work on using representation theoretical ideas in imaging, signal processing, holographic algorithms, quantum computing, algebraic and geometric computational complexity theory, non-commutative Fourier transforms, and other areas.  Each of these subjects is presented by one of the leading experts in the area.  The website for the summer school can be found here:  <br/><br/>www.ima.umn.edu/event/index.php?event_id=PISG7.20-8.6.14"
317,1415496,EAGER: Physical Flow and other Industrial Challenges,CCF,INFORMATION TECHNOLOGY RESEARC,3/1/14,2/12/14,Prasad Tetali,GA,Georgia Tech Research Corporation,Standard Grant,Tracy J. Kimbrel,2/28/17,"$300,000.00 ","Henrik Christensen, Sebastian Pokutta, George Nemhauser",tetali@math.gatech.edu,Office of Sponsored Programs,Atlanta,GA,303320420,4048944819,CSE,1640,"7723, 7916",$0.00 ,"Motivation.  At the core of many societal challenges, particularly in view of sustainability and efficiency, there are hard optimization problems. While these underlying problems can be solved for smaller setups using optimization methods, realistic problem sizes are still prohibitive. Apart from the immediate challenges arising from sheer size, in today's fast-paced and intertwined economies we additionally face the challenge of addressing real-time aspects; otherwise, the obtained solutions might not apply anymore: the problem changed faster than it was solved. This EAGER addresses specific optimization problems arising from societal challenges and will motivate the study of the underlying optimization problems. For this, new methods at the intersection of continuous and discrete optimization as well as machine learning and randomization will be developed.<br/> <br/>Intellectual Merit. PIs set out to investigate general solution strategies to address problem size, real-time requirements, as well as uncertainty aspects using recent developments from theoretical computer science and modern optimization theory. At the core of this proposal is the study of real-world challenges related to cargo-vessel routing, palletizing, vehicle and mobile bot routing and related problems. A main difference to classical approaches is that the real-time requirements will not be added on top but will be integral to the whole design process, which is likely to result in better algorithms. Apart from the challenges from the aforementioned problems, PIs intend to develop an online expert system - Ask Minmax. This platform will leverage machine learning techniques to make the theory of optimization methods and tools accessible to industry and other personnel that face challenging optimization problems.<br/> <br/>Broader Impact. A key objective of this proposal is outreach. This EAGER will develop a consulting tool that will allow industry and the broader society to learn about most useful methods in algorithms and optimization. In particular, this tool is geared towards helping the user to make a decision regarding which methods are most likely to be applicable to their problem: in an interactive way the tool will collect information from the user about their optimization problem. The system will then determine the most likely model and provide information. Large-scale challenges in palletizing and routing require the combination of various heuristics and algorithms. A second component that will significantly impact industry is the developing of key performance indicators for such algorithms. These metrics will help to gauge efficiency of a particular heuristic and/or algorithms, which will be important when algorithms are combined within a larger solution. Mentoring and collaborating with a graduate student, a shared postdoctoral student across multiple EAGERs, as well as hosting a workshop in relevant frontier topics at the Institute for Mathematics and Applications (IMA) are all parts of the broader impact of this EAGER."
318,1440166,EAGER: Science in the Time of Big Data,DEB,LONG TERM ECOLOGICAL RESEARCH,6/15/14,6/14/14,Debra Peters,NM,New Mexico State University,Standard Grant,Douglas Levey,5/31/18,"$289,696.00 ",Kris Havstad,deb.peters@ars.usda.gov,Corner of Espina St. & Stewart,Las Cruces,NM,880038002,5756461590,BIO,1195,"7916, 9150, 9169, EGCH",$0.00 ,"The scientific community is awash in 'big data' but few practicing ecologists use these data to answer important ecological questions. They rely instead on the traditional approach of collecting new, experimental data focused on particular species, habitats, or problems. In addition, the data-intensive computational methods commonly needed to analyze big datasets are not easily accessible to most researchers. This high-risk, high-reward project could dramatically alter both the ways in which ecologists address questions and the types of questions that they tackle. It therefore represents a major contribution to NSF's efforts to extend ecological research in new directions to provide answers to more complex questions. <br/><br/>A knowledge-driven, open access system that 'learns' and becomes more efficient and easier to use as data streams increase in variety and size is needed for timely scientific progress in an era of big data. This approach is centered on establishing linkages between databases and hypothesis-based inquiry that result in the derivation of new or refined hypotheses as a result of improved access to dynamic databases. The investigators recently implemented a hypothesis-driven, process-based analytical methodology that was conceptually integrated with a data-intensive machine learning approach. This integrated approach allowed them to use multiple long-term datasets to narrow a diverse suite of mechanistic explanations to a single, most likely process. This process was then tested by a short-term experiment that saved time and money and yielded a more definitive answer than the more traditional approach described above. To further this approach, this project will test, refine, and automate this new integrative effort to develop a prototype cyber-infrastructure capable of significantly advancing the environmental sciences. Open access data, programming scripts, and derived data products will reduce the time lag for knowledge transfer from an individual to the research community, likely increase the speed of scientific progress, and provide a filter and memory for how to deal with large amounts of data of mixed quality. A postdoctoral researcher will work collaboratively with computer scientists, ecologists, and eco-informatics experts from three universities (New Mexico State University, University of Texas El Paso, and Evergreen College) and one corporation (Microsoft) to develop, test, and automate this knowledge-learning analytics system. Two workshops will be organized to test the ability of the system to learn while using diverse datasets and to introduce the approach to a wide variety of users."
319,1407557,Statistical learning via multivariate density estimation,DMS,STATISTICS,8/1/14,6/12/17,Wing Hung Wong,CA,Stanford University,Continuing grant,Gabor Szekely,7/31/18,"$599,539.00 ",,whwong@stanford.edu,450 Jane Stanford Way,Stanford,CA,943052004,6507232300,MPS,1269,,$0.00 ,"The overall goal of the project is to develop methodologies of density estimation in multiple dimensions, and to develop new tools based on this methodology for selected problems in data compression, image analysis and graphical model inference. Density estimation is a fundamental problem in statistics but traditional approaches such as kernel density estimation are not well suited to handle the large multivariate data sets in current applications. The research in this project is centered on the methodology and application of multivariate density estimation. By creating effective methods for this problem, this project will also benefit many other research problems in applied statistics and machine learning where density estimation can be used as a building block for the solution, for example, image segmentation, data compression and network modeling.  <br/>      <br/>Specifically, the project will address the question of how to infer a partition of the sample space that will reveal the structure of the underlying data distribution. The partition will be learned from the observed data based on a Bayesian nonparametric approach which imposes minimal assumptions on the distribution to be estimated. Efficient and scalable algorithms for such inferences will be designed for the analysis of large data sets in multiple dimensions. The theoretical properties of the estimates, such as asymptotic consistency and convergence rates, will also be investigated."
320,1408635,"AF: Medium: Algorithmic Explorations of Networks, Markets, Evolution, and the Brain",CCF,ALGORITHMIC FOUNDATIONS,4/1/14,9/20/16,Christos Papadimitriou,CA,University of California-Berkeley,Continuing grant,Tracy Kimbrel,1/31/18,"$870,607.00 ",,cp3007@columbia.edu,Sponsored Projects Office,BERKELEY,CA,947101749,5106433891,CSE,7796,"7924, 7926, 7927, 7932, 8091",$0.00 ,"Computer science is not just the scientific discipline behind the information technology revolution; it is also an apt framework for understanding the world around us.  This project is about applying the point of view of algorithms -- and their antithesis, complexity -- to understanding phenomena and challenges in a variety of domains, including the Internet, markets, evolution, and the brain.  To understand the Internet and the networks and markets it entails and enables, one must combine algorithms with ideas from economics and game theory.  This research will focus on online markets and particularly their dynamic (that is, multi-stage) nature, on incentives for improving congestion in network routing and air traffic, on algorithms for propagating influence in social networks, as well as new kinds of algorithms that take their inputs from competitors (who may choose to misrepresent their data).  The PI will continue his research on how computational insights can shed light on some key problems in evolution,  including certain rigorous connections between natural selection, machine learning, and a problem in Boolean logic.  Finally, the PI will work to reconcile learning algorithms with new insights from neuroscience.<br/><br/>The project includes research on certain crucial problems at the interface between computation and game theory/economics/networks,  while continuing past work employing computational concepts to elucidate evolution and, more recently, neuroscience.  The PI will  study the important problem of dynamic mechanism design in economics from the point of view of computational complexity and approximate implementation. He will also study mechanisms for managing congestion, with possible applications to air traffic control.  The project will explore the computational and graph-theoretic properties of several novel and promising game-theoretic models of network creation.  It will study from the complexity standpoint Nash equilibria with continuous strategies, and extensions of the Nash equilibrium concept beyond utility theory.  The project will also explore new and timely modes of computation in which all inputs (ultimately, all computational components) are provided by selfish rational agents. In evolution, the PI will explore the connections between learning algorithms, games, and natural selection, and a different connection between Boolean satisfiability and the emergence of novelty. The PI also plans to develop a new genre of learning algorithms that are more faithful to the new insights we are gaining into the brain.  Finally, from the standpoint of algorithms and complexity, the PI will look at several computational problems ranging from network variants of the set cover problem to linear programming and optimizing multivariate polynomials."
321,1355899,ABI Innovation: A Probabilistic Approach to Meta-Analysis of Biological Network Interface,DBI,ADVANCES IN BIO INFORMATICS,7/1/14,6/13/14,Su-In Lee,WA,University of Washington,Standard Grant,Jennifer Weller,6/30/18,"$689,713.00 ",,suinlee@cs.washington.edu,4333 Brooklyn Ave NE,Seattle,WA,981950001,2065434043,BIO,1165,"9178, 9179",$0.00 ,"Genes do not work alone, but rather in an intricate network of interactions to regulate fundamental cellular processes.  Biological network inference has become a key analysis tool in modern biology allowing scientists to gain a  better understanding of basic biology such as evolution, molecular biology and genetics. Network inference algorithms have played significant roles in understanding gene networks from molecular data but are limited by the large search space of the network structure and data insufficient for statistical power. The goal of this project is to develop a computational framework, based on statistical and machine learning techniques, for effectively integrating multiple heterogeneous data sets to infer gene networks accurately. The developed methods will be applied to answer important biological questions, such as: How is a gene network reshaped in the evolutionary process of yeast species?, How do genes and proteins interact with one another underlying a certain biological trait?, and Which genes contain causative sequence variations that influence important biological traits?. This project is expected to dramatically increase the applicability of network learning algorithms in a wide variety of applications, especially those with smaller sample sizes. The implementation of the developed methods will be made publicly available, which can help many other biologists to study gene networks in their research problems. This project is interdisciplinary in nature and has significant emphasis on interdisciplinary education, through project courses and outreach activities. It will have a long-term effect of advancing the field of biology, by increasing the number of students in computer science inspired to solve biology problems.<br/><br/>Network inference from high-throughput biological data has significantly contributed to advancing our knowledge of molecular biology, evolution and genetics. Its major drawback is that, due to the exponentially large search space of the network structure, the sample size provided by a single dataset is often not large enough to obtain valid inference results. However, simply appending datasets from different studies is unlikely to be successful, because in many cases they contain different variables and overly heterogeneous samples. The goal of this project is to develop an innovative probabilistic approach to integrate multiple heterogeneous datasets, by jointly modeling one or more networks represented by these datasets. This project addresses the problem of how to integrate datasets containing heterogeneous samples (Aim 1), different sets of variables (Aim 2) and fundamentally different types of measurements (Aim 3). The developed algorithms in each Aim is applied to the following problems: 1) learning a network model that can represents evolutionary rewiring of gene regulatory networks across Saccharomyces species; 2) learning a joint latent network model that can infer a very high-dimensional network underlying biofilm phenotypes in S. cerevisiae by combining datasets with different variables; and 3) identifying hubs in the gene network from many expression datasets, to guide quantitative trait loci studies in S. cerevisiae."
322,1406578,CHS: Medium: Design Tools for Physical Computing Objects,IIS,HCC-Human-Centered Computing,8/1/14,4/29/20,Michael Jones,UT,Brigham Young University,Continuing Grant,Ephraim Glinert,7/31/21,"$1,123,577.00 ","Michael Jones, Kevin Seppi",jones@cs.byu.edu,A-285 ASB,Provo,UT,846021231,8014223360,CSE,7367,"7367, 7924, 9150",$0.00 ,"As computers decline in cost and interactive computation moves off of the desktop, computation can assume a variety of physical forms. At present, the creation of new computer interfaces is limited to already-available formats, such as smartphones, and even then is constrained to relatively simplistic computer interfaces. Computing in the future will push beyond desktops, laptops, tablets and smartphones into objects that fit into every part of our lives. But for this to happen, design tools are needed that can rapidly adapt computing to different physical forms and task needs. This project will study toolkits for the development of interactive physical objects. The project will increase national competitiveness in the invention and development of new uses of computation, and make possible rich and diverse usability research with physical computing objects in the wild rather than in the lab. Creating physical computing objects will allow a greater proportion of society to use computers in ways that fit their work requirements in fundamentally new ways. The rapid design and creation of such devices will open new markets for consumer computing.<br/><br/>These ends are achieved through three interrelated efforts: (1) A pluggable toolkit for creating sensor/actuator systems that form the computational basis for such objects. This toolkit will be unique in that the components when plugged together not only form an initial prototype of the device but self-reveal their physical and computational characteristics to automatically support the other design tools. A prototype constructed with this toolkit will inherently contain sufficient information to drive its own fabrication. By plugging together the prototype our tools will automatically know how to perform a custom fabrication of the electronics at a size that can be readily deployed. (2) Development of physical form via 3D printing such that the physical shape, sensors and actuators integrate with the software and electronics so as to easily prototype a complete physical/computational object. One goal is to suppress the challenges of mechanical design so that designers can focus on shape and usability. (3) Interactive machine learning techniques will be created to easily develop the mapping between human activities (as detected by sensors) and their recognition in software. It is known that putting humans in the training loop for machine learning changes the way training sets are built. Algorithms will be developed that both adapt to and exploit this behavior. This project will produce new knowledge of the concepts that are most difficult for designers when creating physical computational objects."
323,1421908,III: Small: Reconstructing viral population without using a reference genome,IIS,"ADVANCES IN BIO INFORMATICS, Information Technology Researc, Cross-BIO Activities, Info Integration & Informatics",9/1/14,7/19/15,Raj Acharya,PA,Pennsylvania State Univ University Park,Continuing Grant,Sylvia Spengler,5/31/17,"$500,000.00 ","Mary Poss, Paul Medvedev",acharya@cse.psu.edu,110 Technology Center Building,UNIVERSITY PARK,PA,168027000,8148651372,CSE,"1165, 1640, 7275, 7364","7364, 7923, 8750",$0.00 ,"Next-generation sequencing (NGS), which allows sampling millions of short DNA sequences from a genome, has revolutionized the field of genomics. One area of particular importance is the reconstruction of genomes (haplotypes) from a viral population, which is a fundamental problem in virology, evolutionary biology, and human health. Though there have been several methods developed to take advantage of NGS data, those are limited to populations for which a reference genome is available. This excludes many important cases, such as RNA viruses or certain HIV/HCV viral populations. In such situations, the haplotypes are sufficiently divergent as to render the reference meaningless. Moreover, most algorithms are not robust in the presence of recombination, which is a common occurrence in many viral populations. The achievement of this project's aims will allow for the full potential of NGS data to be realized in the field of virology. In particular, it will help to propel the understanding of viral population dynamics and give biologists powerful tools to understand disease progression and enable novel treatment and prevention strategies.  The algorithms and software developed will be made freely available for use through software sharing platforms like GitHub or Galaxy. The PIs will offer a strong educational component including (a) graduate and undergraduate classes that use the output of the proposed research, and (b) development of a seminar series. The PIs will (a) train future generations of scientists and engineers to enhance and use bioinformatic/genomic cyber resources; (b) facilitate creative, cyber-enabled boundary-crossing collaborations, including those with industry and international dimensions, to advance the frontiers of science and engineering and broaden participation in STEM fields.<br/><br/>This project?s aim is to develop probabilistic De Bruijn graphs and network flow on such graphs for the reconstruction of viral population when a reference is not available. Given NGS data, the algorithms should determine the number, sequences, and relative frequencies of the haplotypes.  This project's proposed algorithms are based on a unique combination of established techniques (e.g. maximum likelihood, expectation-maximization, clustering, Lander Waterman statistics) with novel propositions for probabilistic De Bruijn graphs, machine learning, and network flows that are of interest in other applications. The PI and Co-PIs have complementary backgrounds in virology, machine learning, network flow, and genome reconstruction problems."
324,1439069,XPS: FULL: CCA: Collaborative Research: Automatically Scalable Computation,CCF,Exploiting Parallel&Scalabilty,9/1/14,8/26/14,Jonathan Appavoo,MA,Trustees of Boston University,Standard Grant,tao li,8/31/15,"$85,000.00 ",Steven Homer,jappavoo@bu.edu,881 COMMONWEALTH AVE,BOSTON,MA,22151300,6173534365,CSE,8283,,$0.00 ,"The era of performance scaling by increasing the performance of<br/>individual processors is over, having been replaced by the era of<br/>massive parallelism via multiple cores.  Amdahl's law tells us that<br/>our ability to parallelize computation is limited by the inherently<br/>sequential portion of a computation.  This unfortunate combination<br/>of facts paints a bleak picture for the future of scalable software.<br/>This work explores a radical new approach to parallelism with the<br/>potential to bypass Amdahl's Law. The approach used involves making<br/>informed predictions about computation likely to happen in the<br/>future, proactively executing likely computations in parallel with<br/>the actual computation, and then ""jumping forward in time"" if the<br/>actual execution stumbles upon any of the predicted computations<br/>that have already been completed.  This research touches many areas<br/>within Computer Science, i.e., architecture, compilers, machine learning,<br/>systems, and theory.  Additionally, exploiting massively parallel<br/>computation will produce immediate returns in multiple scientific<br/>fields that rely on computation.  The research here provides an<br/>approach to speedup on such real-world problems.<br/><br/>The approach used in this research views computational execution<br/>as moving a system through the enormously high dimensional space<br/>represented by its registers and memory of a conventional single-threaded<br/>processor.  It uses machine learning algorithms to observe execution<br/>patterns to make predictions about likely future states of the<br/>computation.  Based on these predictions, the system launches<br/>potentially large numbers of speculative threads to execute from<br/>these likely computations, while the actual computation proceeds<br/>serially.  At strategically chosen points, the main computation<br/>queries the speculative executions to determine if any of the<br/>completed computation is useful; if it is, the main thread uses the<br/>speculative computation to immediately begin execution where the<br/>speculative computation left off, achieving a speed-up over the<br/>serial execution.  This approach has the potential to be infinitely<br/>scalable: the more cores, memory, and communication bandwidth<br/>available, the greater the potential for performance improvement.<br/>The approach also scales across programs -- if the program running<br/>today happens upon a state encountered by a program running yesterday,<br/>the program can reuse yesterday's computation."
325,1350133,CAREER: Scaling up Modeling and Statistical Inference for Massive Collections of Time Series,IIS,Info Integration & Informatics,6/15/14,3/31/20,Emily Fox,WA,University of Washington,Continuing Grant,Maria Zemankova,5/31/21,"$549,178.00 ",,ebfox@uw.edu,4333 Brooklyn Ave NE,Seattle,WA,981950001,2065434043,CSE,7364,"1045, 7364",$0.00 ,"Consider the task of predicting influenza rates at a very large set of spatial locations. Modeling each region independently does not leverage the information from related regions and can lead to poor predictions, especially in the presence of missing observations. Likewise, imagine estimating the value of every house in the United States. Capturing trends within a neighborhood is key; however, each neighborhood only has a few recent house sales. The challenges presented by these increasingly prevalent massive time series are endemic to a wide range of applications, from crime modeling for police resource allocation to forecasting consumer trends and social networks: the individual data streams often include only infrequent observations such that each alone does not provide sufficient data for accurate inferences. However, the structured relationships between them offer an opportunity to share information.  A key question is how to discover these relationships.  <br/><br/>This project takes a computationally-driven Bayesian nonparametric approach, trading off flexibility and scalability, to address the challenges of massive collections of infrequently observed time series. Our approaches exploit correlation among the data streams, e.g., among related regions, while enabling data-driven discovery of sparse dependencies. The multi-resolution and modular forms also allow incorporation of heterogeneous side information. Key to the success of the proposed methods is scalable Bayesian posterior inference. We focus on (i) parallel computations exploiting sparse graph dependencies, (ii) multi-resolution inference, and (iii) online algorithms for dependent data.<br/><br/>This project represents an ambitious cross-disciplinary effort, integrating ideas from machine learning, systems, engineering, and statistics. The work addresses a largely ignored question in the discussion on big data: How to cope with modeling and computational issues when the data has crucial structure across time, especially arising from individually sparse and disparate measurement sources. The tools developed will significantly broaden the scope of scientific questions that can be addressed. Results from this work will be publicly disseminated, including through open source software, and our industry partners aim to transition the technology into real-world systems. This project also involves developing (i) exciting and intensive programs harnessing existing infrastructure, UW DawgBytes, to increase the exposure of K-12 students, and especially girls, to machine learning; and (ii) curriculum training students in both statistical and computational thinking.<br/><br/>For further information, see the project website at http://www.stat.washington.edu/~ebfox/CAREER.html."
326,1417674,"Extreme-scale algorithms for geometric graphical data models in imaging, social and network science",DMS,CDS&E-MSS,8/1/14,8/24/16,Andrea Bertozzi,CA,University of California-Los Angeles,Continuing Grant,Christopher Stark,12/31/18,"$299,999.00 ",,bertozzi@math.ucla.edu,10889 Wilshire Boulevard,LOS ANGELES,CA,900951406,3107940102,MPS,8069,"7433, 8084, 9263",$0.00 ,"Big data is important right now; and extreme scale hardware, programming models and storage solutions are being developed. However the connection between transformational algorithms and the big data sets needs to be addressed.  Researchers talk often about the upcoming ""big data problem,"" yet skim over the relevant algorithms that can truly attack the problems in a concrete fashion, often because the traditional means of data analysis are not prevalent in the high-performance computing (HPC) world, and in concert, the HPC expertise is not prevalent in the data world. This project will solidify this connection, making the ""big data"" problem real by attacking the issues through identifiable concrete algorithmic research and by implementing the methods on the latest hardware platforms. Recently the principal investigator has developed scalable desktop algorithms that bridge that gap, leveraging fast spectral solvers to compute solutions of sparse classification problems for big data.  This project will build on these scalable algorithms to implement them on several large-scale platforms and will address important application areas, for which desktop computing is insufficient.  Examples of application areas for this project include high dimensional hyperspectral video data for chemical and biological agents, a problem of importance to homeland security, and statistical analysis of spatio-temporal multimodal crime data, and large-scale social network analysis.<br/><br/>This project focuses on a new class of data-clustering algorithms that are designed to solve variants of the minimum cut problem on graphs for big data applications such as hyperspectral video data analysis, statistical analysis of spatio-temporal multimodal crime data, and large-scale social network analysis.  Semi-supervised and unsupervised machine learning problems are included in the class of problems considered.  The graph mincut problem is equivalent to total variation minimization on a graph and is a popular model for machine learning applications, except for its computational complexity.  Building on ideas such as diffuse interfaces and dynamic thresholding, originally developed for physical sciences models and subsequently transferred to low dimensional image processing applications, this project will develop methods to solve the true graph cut problem by leveraging recent advances in scalable spectral graph algorithms.  New codes for these methods will be developed for large parallel architectures.  The research will advance both theoretical algorithmic issues and application areas."
327,1350740,CAREER: Sustaining Moore's Law Through Introspective Computing: A Comprehensive System For Reliability and Energy Optimization in Modern Computing Devices,CCF,Software & Hardware Foundation,2/1/14,9/20/17,Timothy Miller,NY,SUNY at Binghamton,Continuing Grant,Yuanyuan Yang,1/31/19,"$482,000.00 ",,millerti@cs.binghamton.edu,4400 VESTAL PKWY E,BINGHAMTON,NY,139026000,6077776136,CSE,7798,"1045, 7923, 7941, 9251",$0.00 ,"Whether one is concerned about data-center carbon footprint or battery life of mobile devices, the microprocessor is a dominating consumer of energy.  Transistor technology scaling, whose rate is expressed by Moore?s Law, is a regular progression of transistor size reductions down to nanometer dimensions.  Historically, this scaling has led to significant improvements in performance and energy efficiency, but more recently scaling has created severe reliability challenges due to difficulties in building components at near-atomic scale.  To ensure correctness, chips are operated with static and worst-case safety margins that account for more than 70% of the total energy used by a CPU.  This research program specifically addresses that energy wastage by intelligently tightening safety margins and making them dynamic in order to ensure reliable operation with dramatic reductions in expended energy.  The success of this research effort will lead to substantial reduction in energy wasted by semiconductor devices for the purpose of improving battery life, environmental impact, and operating costs.  It will also encourage the use of continuous self-adjustment and adaptation across an array of computing technologies.<br/><br/>In addition to being dependent on the power supply voltage and device temperature, the power and switching delay of a transistor varies substantially with random dopant fluctuation and aging.  In current practice, the worst-case combination of factors that affect transistor power consumption and delay are used to size device geometries and define an operating voltage guard band.  This ensures reliable operation but leads to unnecessary energy wastage, as the worst-case combinations are unlikely to occur in reality.  In this work, machine learning is used to correlate environmental and controllable factors that affect circuit delay and power and dynamically predict the minimum safe guard band.  If error-resilient components are used, the guard band can be eliminated entirely.  To realize maximum benefit, the system design is optimized across the boundaries of circuit, architectural, and software layers.  Combining machine learning, proactive closed-loop control, and a cost/benefit-driven approach to actuator and on-chip sensor allocation, circuit designers and architects are provided with a comprehensive methodology for creating introspective computing devices that dramatically lower energy and adapt automatically to all environmental and workload conditions."
328,1424875,Analyses of Overly Dispersed Covariance within Latent Structures and Applications in Psychological and Behavioral Research,SES,"Methodology, Measuremt & Stats",9/15/14,6/28/18,Edward Ip,NC,Wake Forest University School of Medicine,Standard Grant,Cheryl Eavey,8/31/19,"$279,076.00 ",,eip@wakehealth.edu,Medical Center Blvd,Winston-Salem,NC,271571023,3367162382,SBE,1333,,$0.00 ,"This research project will investigate models for covariation of multiple variables over time and apply the various models to analyze data from the social and behavioral sciences.  Understanding covariation often is a first step in the study of causation and possible mechanisms that lead to specific outcomes.  The study of covariation over time may reveal interesting and important patterns within a system of interconnecting variables.  In economics and finance, for example, studying the change in patterns of covariation of market data has important implications for asset management and portfolio diversification.  Stock-market indexes across the world often are correlated, but the correlations during bear markets and crisis periods tend to be much higher than during normal times.  Covariation in cognitive domains, such as memory, reasoning, and speed of processing information may be used to assess early cognitive impairment.  For example, divergence in performance across domains within the overall trend of general cognitive decline due to age could indicate problems.  The project will develop tools for the research community to facilitate the interpretation of covariation in data.  The project also will train graduate students and postdoctoral researchers.<br/><br/>This project will examine different approaches for studying overly dispersed covariation in the context of modeling with latent structures.  Overly dispersed covariation refers to sources of variation that drive the association between variables but are not captured by regular latent structures.  The project will use state-of-the-art tools from statistics and machine learning to examine data from the social and behavioral sciences.  A unique intellectual contribution of the project will be the adaptation of these toolsets to social and behavioral science data that often emphasize multiple outcome variables rather than multiple predictor variables as in the case of statistics and machine learning.  The project will be organized by several exemplary applications including (1) response consistency in attitudinal survey, (2) patterns of cognitive impairment, (3) dynamics of change in forming friendship among children, and (4) cognitively demanding daily activities in older adults.  The applications are broad in their respective content.  While they illustrate different and specific strategies for handling overly dispersed covariation, they serve as prototypical examples for further development of similar applications in other fields of study."
329,1409003,RI: Medium: The Foundations of a Manipulation Repertoire,IIS,"Information Technology Researc, Robust Intelligence",7/15/14,9/22/16,Matthew Mason,PA,Carnegie-Mellon University,Continuing Grant,Erion Plaku,6/30/19,"$1,076,210.00 ","Michael Erdmann, Siddhartha Srinivasa",matt.mason@cs.cmu.edu,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,CSE,"1640, 7495","1640, 7495, 7924",$0.00 ,"This project is based on two theses: (1) autonomous robotic manipulation requires a large repertoire of actions; and (2) autonomous manipulation does not decouple into separate arm and hand functions. The project's goal is to establish the principles and techniques to endow a robot with a large repertoire of manipulative actions, many of them involving an intimate coordination of arm and hand.  The approach is to develop these principles and techniques using physics-based models as well as machine learning of empirical models.  The project is developing and testing these principles and techniques by attacking several challenge tasks. The work is organized to address three primary challenges: (1) identifying actions; (2) modeling actions; and (3) orchestrating actions.  For the first challenge, identifying actions, the project is adapting previous physics-based manipulation research, along with ordinary robotic engineering of behaviors inspired by humans and existing robotic systems.  For the second challenge, modeling actions, the project is augmenting physics-based models work with empirical stochastic modeling drawn from previous applications of machine learning.  For the third challenge, orchestrating actions, the project is adapting previous work on sensor-based planning and control. The project's expected broader impacts includes more capable robots, which will simplify deployment, enable new applications and improve existing applications, which ultimately serves to improve productivity, services, and national economic competitiveness."
330,1430780,SBIR Phase II:  Serendipitous Search System Using Lateral Analogy to Match Potential Solutions to Unmet Needs:Feasibility Study Based on Screening Approved Drugs for Repurposing,IIP,SBIR Phase II,12/1/14,12/23/17,Brian Sager,CA,Leonardo Innovations Inc.,Standard Grant,Muralidharan Nair,1/31/19,"$1,310,747.00 ",,brian_m_sager@yahoo.com,423 8th Avenue,Menlo Park,CA,940251848,6502244508,ENG,5373,"165E, 169E, 4080, 5373, 8032, 8034, 8035, 9139, HPCC",$0.00 ,"The broader impact/commercial potential of this project is to accelerate the pace<br/>of research and development to enable more rapid deployment of technologies<br/>into commercial / industrial contexts. In many fields, information is expanding at<br/>such an exponential rate that finding relevant results to technical knowledge<br/>searches is increasingly difficult. Further, content is expanding so fast that most<br/>fields are rapidly forming sub-disciplines, leading to the ?silo-ing? of different<br/>knowledge sub-domains, a clear challenge to both academia and industry. We<br/>need ever better ways to organize and present information to users. There are<br/>disadvantages of the current search engines, mostly relating to excessive<br/>similarity in search results. Further, while these engines present information<br/>relating to a known search target, they are less effective at presenting<br/>unexpected results for information that a user has never heard of but that would<br/>be useful. What is therefore needed is an exploration system giving searchers a<br/>strong serendipitous element with a maximum likelihood of results from diverse,<br/>unexpected, and potentially provocative sources. This will break down silos by<br/>providing a rapid, relevant means for knowledge-transfer between different<br/>disciplines, fostering interdisciplinary innovation. This system has been designed<br/>to provide a means for systematic, automated discovery.<br/><br/>This Small Business Innovation Research (SBIR) Phase 2 project is focused on<br/>optimizing and scaling a serendipitous document search system for repurposing<br/>technologies by analogy into lateral fields. Both by sub-parsing discrete content<br/>into ontologically separable entities, such as capability, characteristic, and<br/>composition, and by comparatively assessing certain of these attributes between<br/>such entities, the attribute relatedness of these entities can be used to drive their<br/>self-assembly into related attribute networks. This approach provides a<br/>significant value proposition for drug repurposing, which is the current focus of<br/>this project. To scale the pair-wise comparison and network assembly of millions<br/>of documents, a map-reduce based text-processing framework will be developed<br/>so that massively parallel computations can be carried out in a time- and costefficient<br/>manner. A distributed search engine technology will be deployed to<br/>enable rapid querying of the emerging document relationship network. A series of<br/>machine learning algorithms will then be used to determine potentially hidden<br/>structural architectural features within the document relationship network.<br/>Machine learning will elucidate the nature of the relationships in drug networks<br/>through analyses of inter-node relationships and sub-graph motifs (termed<br/>?innovation motifs?). Documents including U.S. patents and scientific papers will<br/>be processed in the system."
331,1407537,Non-gaussian graphical models via additive conditional independence and nonlinear dimension reduction,DMS,STATISTICS,7/15/14,7/8/14,Bing Li,PA,Pennsylvania State Univ University Park,Standard Grant,Gabor Szekely,6/30/17,"$210,000.00 ",,bing@stat.psu.edu,110 Technology Center Building,UNIVERSITY PARK,PA,168027000,8148651372,MPS,1269,,$0.00 ,"Statistical networks and graphical models are two of the most important components of contemporary data analysis. They have important applications in Genomics, sociology, machine learning, study of the internet, and homeland security. Current statistical graphical models require strong assumptions in order to be computationally feasible for estimating large-scale networks but these assumptions also severely limit their applications.  In this project the principal investigator will lay out the groundwork for developing a new class of statistical graphical models that do not rely on these strong assumptions but at the same time retain the computational simplicity of the current models. The new class of models will greatly expand the scope and capability of current methods for analyzing networks that are becoming increasingly prevalent in modern applications.<br/><br/>In this project the principal investigator will develop a class of nonparametric graphical models that avoid the Gaussian or copula Gaussian assumptions. This new class of models can handle intrinsically nonlinear interactions that cannot be captured by a copula Gaussian model. A fully nonparametric approach, however, would involve high-dimensional kernels, which perform poorly due to the ""curse of dimensionality."" This disadvantage is especially noticeable for large-scale  networks. For this reason, the principal investigator will introduce two dimension reduction mechanisms into the nonparametric approach: additive conditional independence and nonlinear sufficient dimension reduction. Additive conditional independence is a new statistical relation that resembles the Gaussian interaction structure without being restricted by the Gaussian (or copula Gaussian) distributional assumption. The graphical models based on additive conditional independence can capture intrinsically nonlinear interactions and at the same time avoid high-dimensional kernels. The second mechanism incorporates ideas and techniques from the most recent advances in nonlinear sufficient dimension in statistics and machine learning into the graphical models to reduce the dimension of the mapping kernels."
332,1422830,AF: Small: Geometry and High-dimensional Inference,CCF,ALGORITHMIC FOUNDATIONS,8/1/14,4/25/17,Mikhail Belkin,OH,Ohio State University,Standard Grant,Rahul Shah,1/31/18,"$450,000.00 ",Mikhail Belkin,mbelkin@cse.ohio-state.edu,Office of Sponsored Programs,Columbus,OH,432101016,6146888735,CSE,7796,"7923, 7926",$0.00 ,"Data analysis is ubiquitous in a broad range of application fields, from computer graphics to geographic information systems, from sensor networks to social networks, from economics to medicine. It represents a fundamental problem in computational science. The project will advance the theoretical understanding of fundamental issues behind data analysis, and develop practical algorithms that will be useful for a broad range of problems in science and engineering.<br/><br/>The project addresses the fundamental problem of reconstructing structure of probability distributions from sampled data. It will investigate the use of tensor-based and other higher order methods, in particular those that allow for efficient optimization. The project lies at the interface of theoretical computer science, machine learning, signal processing and statistics and will have potential impact in all of these fields. In recent years there has been a resurgence of interest in tensor methods in data analysis and inference, particularly in theoretical computer science. These methods will prove useful in a variety of applications in machine learning, signal processing and other fields.<br/><br/>The project will develop algorithms for solving a range of problems including blind source separation, spectral clustering, inference in mixture models and estimating geometry of distributions. It will analyze the complexity of these and related problems. In particular, it will strive to understand the computational efficiencyand dependence on the dimension of the space, studying ""the curses and blessings of dimensionality"". It will also address a somewhat mysterious discrepancy between sample and algorithmic complexity in our understanding of many high dimensional inference problems.<br/><br/>The results of this work will be disseminated to the broad scientific community through publications in journals, conferences and presentations in various venues, including tutorials. The goals of this project include to implement the practical algorithms and to make the software available online. The research results will also be incorporated in the curriculum of graduate classes taught by the PI and the co-PI. Graduate students supported by this project will receive extensive training in theory, algorithm development and applications."
333,1414568,I-Corps:  Innovative Use of Internet Classifieds in Law Enforcement Investigations,IIP,I-Corps,2/1/14,1/22/14,Artur Dubrawski,PA,Carnegie-Mellon University,Standard Grant,Steven Konsek,7/31/15,"$50,000.00 ",,awd@cs.cmu.edu,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,ENG,8023,,$0.00 ,"This project addresses the use of machine learning techniques on massive on-line information for identifying patterns useful in law enforcement practice and public policy. The quantity and complexity of public data make it impractical for manual browsing and tracking by investigative forces seeking to identify illegal activities. For example, there is a need to automate the process of detection of informative patterns in human trafficking, and especially sex trafficking. Ubiquity of the Internet provides the perpetrators, who take advantage of trafficking victims, with the ability to solicit services on advertisement web sites. This data is publicly available and contains information potentially useful to law enforcement investigators to understand crime patterns and to track the perpetrators. This team proposes to study the commercialization opportunity of a prototype analytic tool which they developed for the task. This technology may save time and increasing law enforcement productivity and effectiveness by providing new leads that would otherwise be missed.<br/><br/>This project provides a framework for demonstrating practical utility of machine learning research previously funded by NSF in a well-focused context of its societally important application. The proposed innovation has the potential to revolutionize the investigative process in the US at local, state, and federal levels. The vast amount of data online is currently underutilized. This project seeks to remedy this for general benefit of society. It would improve efficiency of law enforcement practice at all levels (local, state, federal), facilitate cross-agency collaboration, as well as enhance public policy studies. The proposed innovation will also have a broader application in leveraging public sources of data for detection and mapping of patterns of other illicit behaviors (e.g. sale of stolen and counterfeit goods) and non-illicit behaviors (e.g. economic activity, job seeking, lifestyle, and public health)."
334,1438983,XPS: FULL: CCA: Collaborative Research: Automatically Scalable Computation,CCF,Exploiting Parallel&Scalabilty,9/1/14,8/26/14,Margo Seltzer,MA,Harvard University,Standard Grant,Yuanyuan Yang,8/31/15,"$115,000.00 ","David Brooks, Ryan Adams",margo@eecs.harvard.edu,1033 MASSACHUSETTS AVE,Cambridge,MA,21385369,6174955501,CSE,8283,,$0.00 ,"The era of performance scaling by increasing the performance of<br/>individual processors is over, having been replaced by the era of<br/>massive parallelism via multiple cores.  Amdahl's law tells us that<br/>our ability to parallelize computation is limited by the inherently<br/>sequential portion of a computation.  This unfortunate combination<br/>of facts paints a bleak picture for the future of scalable software.<br/>This work explores a radical new approach to parallelism with the<br/>potential to bypass Amdahl's Law. The approach used involves making<br/>informed predictions about computation likely to happen in the<br/>future, proactively executing likely computations in parallel with<br/>the actual computation, and then ""jumping forward in time"" if the<br/>actual execution stumbles upon any of the predicted computations<br/>that have already been completed.  This research touches many areas<br/>within Computer Science, i.e., architecture, compilers, machine learning,<br/>systems, and theory.  Additionally, exploiting massively parallel<br/>computation will produce immediate returns in multiple scientific<br/>fields that rely on computation.  The research here provides an<br/>approach to speedup on such real-world problems.<br/><br/>The approach used in this research views computational execution<br/>as moving a system through the enormously high dimensional space<br/>represented by its registers and memory of a conventional single-threaded<br/>processor.  It uses machine learning algorithms to observe execution<br/>patterns to make predictions about likely future states of the<br/>computation.  Based on these predictions, the system launches<br/>potentially large numbers of speculative threads to execute from<br/>these likely computations, while the actual computation proceeds<br/>serially.  At strategically chosen points, the main computation<br/>queries the speculative executions to determine if any of the<br/>completed computation is useful; if it is, the main thread uses the<br/>speculative computation to immediately begin execution where the<br/>speculative computation left off, achieving a speed-up over the<br/>serial execution.  This approach has the potential to be infinitely<br/>scalable: the more cores, memory, and communication bandwidth<br/>available, the greater the potential for performance improvement.<br/>The approach also scales across programs -- if the program running<br/>today happens upon a state encountered by a program running yesterday,<br/>the program can reuse yesterday's computation."
335,1405579,Performance of Networked Passive Radar Systems with Multiple Transmitters and Receivers,ECCS,CCSS-Comms Circuits & Sens Sys,9/1/14,1/5/15,Rick Blum,PA,Lehigh University,Standard Grant,Lawrence Goldberg,8/31/19,"$241,373.00 ",,rblum@eecs.lehigh.edu,Alumni Building 27,Bethlehem,PA,180153005,6107583021,ENG,7564,"153E, 9251",$0.00 ,"The goal of this project is to demonstrate the outstanding but untapped potential of passive radar systems with multiple transmitters and receivers. Passive radar is a powerful approach that uses existing ambient communication signals such as radio and television broadcasts, or satellite, cellular and WiFi signals, to detect, image or classify objects and estimate their position and motion. Since passive radar uses existing communication signals it can drastically reduce cost, complexity and energy usage while being especially important in emergency settings where one needs to quickly deploy a radar. Through theoretical analysis, algorithm assessment, the project will demonstrate the tremendous performance gains obtained through moderate increases in the numbers of transmit and receive antennas for realistic radar system models. These contributions should have significant impact to signal processing, sensor networking, machine learning and radar systems research. As new statistical problems will be considered, new theory developed should provide contributions in mathematics and statistics while leading to practical algorithms and ultimately improved radar systems for air traffic control, homeland security, law enforcement (through-wall imaging), surveillance, ocean monitoring, weather monitoring, and environmental monitoring. These investigations should provide contributions relating to the performance analysis of multiple target cases in active radar, sonar, ultrasound, acoustics and other similar active and nonactive sensor technologies. It should encourage new applications for smart homes, businesses and cars.  This project will also offer ample opportunities for educating graduate students, preferably from under-represented groups, in the important cross-disciplinary areas of signal processing and energy via coordination between this research project, classes and Lehigh's Integrated Networks for Electricity (INE) initiative, which the PI is leading.  The sensing research in this project couples well with several activities within the INE initiative. Research results will also be incorporated into current and future Lehigh classes with the hope that class notes will evolve into a book and short course on networked passive radar to provide broad educational impact.<br/><br/><br/>The optimum possible performance for realistically estimating the position and velocity vectors of objects using a passive radar with M transmit and N receive stations will be derived for the first time. Based on presented preliminary results for a simplified system model, the project is expected to demonstrate the tremendous performance gains obtained through moderate increases in MN for realistic system models. These gains have not been observed to date and should encourage a tremendous increase in research activity on passive radar technology with MN > 1. These contributions should have significant impact to signal processing, sensor networking, machine learning and radar systems research.  The proposed approach will employ local/nonlocal/Bayesian/nonBayesian bounds for finite MN performance; recent convergence results for sums of dependent random variables to guide enlightening asymptotic analysis; carefully chosen models for correlated reflection coefficients, correlated noise and other important degradations; enhanced models based on electromagnetic theory;  recently developed target and clutter models; and the most promising signals of opportunities, including MIMO communication signals which show significant promise. The well-developed topics of multiuser/iterative detection and interference channels will be employed to include the degradation incurred when estimating the transmitted signals of opportunity and to account for the any components of the direct path signals that may leak into what is thought to be only the reflected signals. The impact of simultaneously employing several different types of signals of opportunity and different station placements will be uncovered."
336,1421193,SHF: SMALL: NONSTANDARD COMPUTATIONAL MODELS OF LINEAR LOGIC,CCF,Software & Hardware Foundation,9/1/14,5/11/16,Stephan Zdancewic,PA,University of Pennsylvania,Standard Grant,Anindya Banerjee,8/31/19,"$458,000.00 ",,stevez@cis.upenn.edu,Research Services,Philadelphia,PA,191046205,2158987293,CSE,7798,"7923, 7943, 9251",$0.00 ,"TItle: SHF: Small: Nonstandard Computational Models of Linear Logic<br/><br/>Much of the interesting software being developed today relies on mathematical underpinnings that can best be expressed in terms of linear algebra (e.g. large scale matrices or graph data) and statistics (e.g. machine learning algorithms or ""big data"" analysis). Current programming languages aren't especially suited to working with such kinds of data, and so provide little built-in support to help scientists and software developers.  Conversely, many powerful mathematical techniques have been developed in the contexts of linear algebra and statistics, but those techniques have not been applicable to problems in programming language semantics.  This research project seeks to develop a theoretical foundation that connects the seemingly disparate topics of programming languages and these mathematical domains.<br/><br/>The technical approach taken in this work is to develop ""nonstandard"" models of linear logic, which is an expressive and low-level framework for understanding program semantics.  The intellectual merits are found in developing novel connections between well-established, but distinct, mathematical domains, connecting proof theory and program semantics to representations in vector spaces and categories of probability measures.  The broader impacts of this work are best understood through its potential long-term applications, which include: smooth integration of programming language constructs for working with numerical data (like Matlab) with support for higher-order functions and abstract datatypes; new techniques for proof search based on numerical methods; and, better programming languages for expressing machine learning or probabilistic algorithm."
337,1464092,NeTS: Small: Collaborative Research: Distributed Robust Spectrum Sensing and Sharing in Cognitive Radio Networks,CNS,Networking Technology and Syst,9/1/14,10/16/14,Jie Yang,FL,Florida State University,Standard Grant,Thyagarajan Nandagopal,8/31/17,"$134,706.00 ",,jyang5@fsu.edu,"874 Traditions Way, 3rd Floor",TALLAHASSEE,FL,323064166,8506445260,CSE,7363,7923,$0.00 ,"The future Cognitive Radio Networks (CRNs) will consist of heterogeneous devices such as smartphones, tablets and laptops moving dynamically. Accurate and robust spectrum sensing and identification of unauthorized spectrum usage are essential components of spectral efficiency in future radio systems. This project aims to utilize consensus-based cooperation featuring self-organizable and scalable network structure to capture the swarming behaviors of spectrum users and providing cooperative spectrum sensing in a fully distributed manner. By using a combination of control theory and machine learning techniques, the project designs secure weighted average consensus for cooperative spectrum sensing that can not only capture the swarming behaviors in CRNs with heterogeneous devices, but also is robust to practical channel conditions. Robust localization approaches are developed grounded on dynamic signal strength mapping, which have the capability to localize multiple malicious users. Additionally, the new techniques are validated using an actual testbed with on-campus deployment and system demonstration to industrial collaborators. The integration of control theory with dynamic spectrum access will enable a new revolution in the way for enhancing spectrum efficiency in CRNs. The project serves as a pioneer in exploiting multi-disciplinary knowledge (e.g., control systems and machine learning techniques) to achieve a more efficient spectrum usage in future radio systems, aiming to alleviate the increasing crowdness of the spectrum occupancy and support the co-existence of heterogeneous devices. This project also carries out a broad range of education and outreach activities to encourage students to pursue careers in the fields of science and engineering. Research results will be disseminated to academia and industry through presentations and publications in meetings, conferences and journals."
338,1420255,CHS: Small: Detecting Misinformation Flows in Social Media Spaces During Crisis Events,IIS,HCC-Human-Centered Computing,9/15/14,5/11/16,Kate Starbird,WA,University of Washington,Continuing Grant,William Bainbridge,8/31/17,"$515,046.00 ","Robert Mason, Emma Spiro",kstarbi@uw.edu,4333 Brooklyn Ave NE,Seattle,WA,981950001,2065434043,CSE,7367,"7367, 7923, 9251",$0.00 ,"This research seeks both to understand the patterns and mechanisms of the diffusion of misinformation on social media and to develop algorithms to automatically detect misinformation as events unfold.  During natural disasters and other hazard events, individuals increasingly utilize social media to disseminate, search for and curate event-related information.  Eyewitness accounts of event impacts can now be shared by those on the scene in a matter of seconds.  There is great potential for this information to be used by affected communities and emergency responders to enhance situational awareness and improve decision-making, facilitating response activities and potentially saving lives.  Yet several challenges remain; one is the generation and propagation of misinformation.  Indeed, during recent disaster events, including Hurricane Sandy and the Boston Marathon bombings, the spread of misinformation via social media was noted as a significant problem; evidence suggests it spread both within and across social media sites as well as into the broader information space. <br/><br/>Taking a novel and transformative approach, this project aims to utilize the collective intelligence of the crowd - the crowdwork of some social media users who challenge and correct questionable information - to distinguish misinformation and aid in its detection.  It will both characterize the dynamics of misinformation flow online during crisis events, and develop a machine learning strategy for automatically identifying misinformation by leveraging the collective intelligence of the crowd.  The project focuses on identifying distinctive behavioral patterns of social media users in both spreading and challenging or correcting misinformation.  It incorporates qualitative and quantitative methods, including manual and machine-based content analysis, to look comprehensively at the spread of misinformation.  The primary research site is Twitter, because it is public, it facilitates rapid information dissemination, and it has gained exposure as a highly used medium during disaster events.  This investigation expands beyond Twitter to study information flows across other social media and the surrounding Internet by tracing URL links to their original sources. <br/><br/>This research offers empirical, theoretical and applied contributions to the field of human-computer interaction in the areas of social computing, crisis informatics, and crowdsourcing.  Empirically, it enhances our understanding of the flow of misinformation in online spaces.  It builds on previous studies to include a more nuanced view of misinformation by examining several types of behavioral actions, including correction, speculation and challenges to misinformation.  Moreover, the project maps information contagion within a particular social media network and across different platforms (using URL analysis) to identify patterns of information diffusion, or signatures, that can be used to detect and classify different types of misinformation. Theoretically, it contributes to a growing understanding of crowdwork, crowdsourcing, and collective intelligence within online social networks, specifically looking to understand and describe how the connected crowd performs as a massive sensor network that detects misinformation during crisis events.   Finally, it aims to leverage these empirical and theoretical contributions to develop solutions for the real-time detection of misinformation on social media."
339,1439126,XPS: FULL: FP: Collaborative Research: Taming parallelism: optimally exploiting high-throughput parallel architectures,CCF,Exploiting Parallel&Scalabilty,9/1/14,8/6/14,Milind Kulkarni,IN,Purdue University,Standard Grant,Anindya Banerjee,8/31/19,"$329,571.00 ",,milind@purdue.edu,Young Hall,West Lafayette,IN,479072114,7654941055,CSE,8283,,$0.00 ,"Title: XPS: FULL: FP: Collaborative Research: Taming parallelism: Optimally exploiting high-throughput parallel architectures<br/><br/>Over the past decade, computer manufacturers have focused on producing ""multicore"" chips, that package multiple, powerful computing cores on a single chip. Researchers have invested significant effort in developing methods for writing programs that can run efficiently on these cores. The basic idea is to allow programmers to write programs using a high-level programming model and to rely on an underlying compiler and runtime system to efficiently schedule these programs on multicore platforms. However, due to power and heat dissipation concerns, emerging ""throughput-oriented"" computing systems increasingly rely on far simpler computing cores to deliver parallel computing performance. These cores are much more efficient than traditional multicores, and can deliver much higher performance. Practitioners across numerous fields -- bioinformatics, data analytics, machine learning, etc. -- are deploying these systems to harness their power. Unfortunately, existing high level programming models are targeted to multicore chips, and do not produce code that can run effectively on these new systems. As a result, practitioners are forced to rewrite their applications, with painstaking low-level optimization and scheduling. This project will develop schemes to adapt applications written for multicore systems to run efficiently on throughput-oriented processors. The intellectual merits are novel program optimizations that will transform multicore-oriented programs into forms that map efficiently to throughput-oriented processors, scheduling mechanisms that ensure that these throughput-oriented processors do not waste computational resources, and scheduling policies that ensure that the mechanisms are used effectively. The project's broader significance and importance are that programmers will be able to write portable, high-performant and energy-efficient programs for both traditional multicore systems as well as throughput-oriented systems. Moreover, high-level programming models will be used to program the throughput-oriented machines, thus leading to significant reduction of programming effort for practitioners in many science and engineering disciplines. Finally, outreach efforts enhance the project by providing training and mentoring to a diverse group of students.<br/><br/>Languages like Cilk provide support for ""dynamic multithreading"", which allows programmers to identify all of the parallelism in their program, while relying on sophisticated runtime systems to map that parallelism to available parallel execution hardware at runtime. However, Cilk-style execution is inappropriate for the vector-based parallelism found in SIMD units, GPUs and the Xeon Phi; vector parallelism requires finding identical computations performed on different data units. This project investigates a series of transformations that will morph Cilk-style programs into programs that expose vectorizable parallelism, allowing dynamic multithreading programs to be mapped to emerging throughput-oriented architectures. The enabling transformation involves transforming task parallel applications into data-parallel applications by identifying similar tasks being performed at different points in the computation. This project develops a series of scheduling mechanisms and provably efficient scheduling policies that ensure that parallelizing dynamic multithreading applications on throughput-oriented architectures are effective. In this manner, this project enables portable applications that run efficiently both on multicores and on vector-based architectures."
340,1413610,Eyes on the future: optimizing science output for next generation surveys with joint crowdsourced and automated classification techniques,AST,EXTRAGALACTIC ASTRON & COSMOLO,9/1/14,6/12/14,Claudia Scarlata,MN,University of Minnesota-Twin Cities,Standard Grant,Nigel Sharp,8/31/18,"$625,952.00 ",Lucy Fortson,scarlata@astro.umn.edu,200 OAK ST SE,Minneapolis,MN,554552070,6126245599,MPS,1217,1206,$0.00 ,"Future galaxy surveys will produce so much data that astronomers will no longer be able to rely on their previous methods of classifying them in order to extract the science of galaxy formation and evolution through cosmic history.  Although ""crowd-sourced"" galaxy classifications tap into a vast resource of volunteer labor, even major efforts like the Galaxy Zoo (GZ) will not be able to keep up.  This project will build on the GZ database, extending methods to other epochs in the Universe, and simultaneously use the results of the citizen science work together with machine learning to develop new automated classification tools.  Using the proven research value of involving the public and the broader community, and with a new generation of intelligent computer methods, this study will build on the best of both.<br/><br/>While crowd-sourced galaxy classifications have proven their worth on a decade of data from the Sloan Digital Sky Survey (SDSS), there remain two major challenges to making them a standard component of the data processing pipelines for the next generation of surveys.  The first is proving the utility of the method at high redshifts, where more galaxies have irregular or clumpy morphologies.  The second acknowledges that even crowdsourcing does not have the capacity for the data volume and rates that are to come, requiring new more sophisticated machine classification algorithms.  This project will develop catalogs for high-redshift crowd-sourced data and for simulated galaxies, and develop a new automated classification tool that extends to higher redshifts with a training pipeline adaptable to multiple galaxy surveys.  It includes three science projects: (a) directly constrain galaxy size and mass growth rates; (b) measure any relationship between bar-dominated disks and fueling of active galactic nuclei, and (c) quantify the demographics and evolution of disk sub-structures.  Efforts to automate morphological classifications using parametric and non-parametric techniques have been reasonably successful for SDSS, but did not extend to deriving the necessary detailed structural parameters.  The GZ crowd-sourcing project has been successful beyond expectations, providing scientifically viable parameters from volunteer work and leading to over a hundred peer-reviewed papers.  This study will extend both of these approaches in preparation for much larger future surveys, which expect to produce as much data per night as ten years of SDSS.<br/><br/>The high-level catalogs to be produced, and the new classification algorithms to be used, are to be released publically, and will be a valuable resource for the community.  Along with informal guided-inquiry projects, GZ will be implemented into undergraduate astronomy courses.  Students involved in this work will get well-defined PhD projects and acquire technical skills valuable in their future professional careers."
341,1424013,The Institutional Foundations of Judicial Independence,SES,LAW AND SOCIAL SCIENCES,9/15/14,12/20/17,Matthew Hall,IN,University of Notre Dame,Standard Grant,Brian Bornstein,8/31/18,"$128,096.00 ","Jason Windett, Jonathan Crabtree, Thu-Mai Christian",matt.hall@nd.edu,940 Grace Hall,NOTRE DAME,IN,465565708,5746317432,SBE,1372,,$0.00 ,"The state courts of last resort are significant final decisionmakers in many areas states govern, including family, many business disputes, and criminal law.  Second, it has long been a matter of common scholarly wisdom that courts do not tend to go against dominant political coalitions or major public policy initiatives of executives who are in power.  We have little understanding of the mechanisms, and whether judicial selection methods matter or whether public opinion seems to influence decisions.  We also have little systematic evidence over time about what these state courts of last resort do, or how decisions are influenced by public opinion or methods of judicial selection.  While some databases have been assembled, they have been coded by hand.  Advances in machine learning allow coding of cases for subject matter and date of a much larger number of cases than we currently have available.  The database of cases this project will generate will be useful to many scholars.<br/><br/>The development of web-scraping software tools for state court of last resort decisions is a significant innovation in the field, allowing coding of a much broader range of cases than have currently been available to scholars.  The plan to check machine coding with human coders will allow assessment of these new tools.  This project  will allow assessing change over time in courts and influences on judicial decisions in a way that relies on the most current methodologies regarding measuring public opinion and judicial decisions."
342,1427536,MRI: Acquisition of Big Data Training and Research Laboratory,CNS,MAJOR RESEARCH INSTRUMENTATION,8/1/14,8/20/14,Taghi Khoshgoftaar,FL,Florida Atlantic University,Standard Grant,Rita V. Rodriguez,7/31/17,"$600,000.00 ","Pierre-Philippe Beaujean, Hari Kalva, Xingquan Zhu, Ramesh Teegavarapu",taghi@cse.fau.edu,777 GLADES RD,BOCA RATON,FL,334316424,5612970777,CSE,1189,1189,$0.00 ,"This project, procuring and acquiring a large computing cluster appropriate for Big Data research, aims to enable research in a number of fields of national concern such as bioinformatics, ocean energy, social media mining, environmental and climate modeling, image processing and analysis emergency response, health and medical informatics, national security, infrastructure maintenance and reliability, law enforcement, commerce, and manufacturing. Of particular note are seven projects spanning a wide range of application domains. These include the<br/>- Extension of LexisNexis's High Performance Computing Cluster (HPCC) platform to incorporate a Wider Range of Algorithms;<br/>- Analysis of Big Data for Bioinformatics;<br/>- Use of Machine Condition Monitoring and Prognostic Health Monitoring to Improve Ocean Turbine Reliability;<br/>- General Challenges found when Mining Streaming data;<br/>- Analysis of Underwater Acoustic Signals for Tasks such as Unexploded Ordinance Detection; <br/>- Use of Machine Learning to Improve Video Compression Schemes; and<br/>- Challenges of Climate Modeling.<br/>All these demand the advanced computation resources under this acquisition. The study of Big Data encompasses the analysis of extremely large datasets, building models which are able to incorporate vast numbers of instances and features in order to make reliable predictions and connections.<br/><br/>Maintaining and promoting the growth of Big Data has become an essential activity to ensure that the problems that now seem insurmountable may be solved tomorrow. Two aspects of this growth are developing and providing courses for students focused on the tools and techniques necessary for Big Data research, and more focused training in these tools for existing researchers whose area of expertise lies in other aspects of research. Best practices will be followed for cultural diversity, involving students and researchers from underrepresented groups."
343,1447440,EAGER: Characterizing Regime Shifts in Data Streams using Computational Topology - the Mathematics of Shape,CMMI,"Dynamics, Control and System D",8/1/14,7/17/14,Elizabeth Bradley,CO,University of Colorado at Boulder,Standard Grant,Jordan Berg,7/31/16,"$61,490.00 ",James Meiss,lizb@cs.colorado.edu,"3100 Marine Street, Room 481",Boulder,CO,803031058,3034926221,ENG,7569,"034E, 035E, 036E, 7916, 8024, 9102",$0.00 ,"Time-series data arise in a wide array of engineered systems, including network traffic, vibration sensors on machine tools, acoustic sensors on reactor containment vessels, and many other examples. The development of efficient and effective methods to characterize the patterns in such data has widespread utility in engineering, commerce and other fields. Methods for characterizing patterns in these streams could be used to detect malware attacks on a network, a lathe bearing that is degrading, or an impending containment failure in a reactor. Common challenges include observability - situations when sensors are expensive or difficult to deploy, or when they perturb the behavior under examination - as well as high information content, noise, and rapid regime shifts. The ultimate goal of this EArly-Grant for Exploratory Research (EAGER) project is to use computational topology, the fundamental mathematics of shape, to deal with these challenges. Shape is perhaps the roughest notion of structure and can be particularly robust to contamination of the signal. The specific goal of this study is to develop new methods for identifying and categorizing the temporal patterns associated with the regime shifts a stream of data. <br/><br/>A topological approach to time series analysis is distinct from standard methods of the machine learning and stream-mining communities, which typically use probabilistic approaches and often implicitly assume linearity. This project seeks to extract nonlinear structure not necessarily visible in a regresssion or spectral approach. Indeed, a regime shift need not correspond to a change in the frequency content of a signal, but could nevertheless be represented as a shift in the homology (e.g., Betti numbers) of the embedded signal. A goal is to develop techniques useful to engineers and scientists for the detection of incipient system failure or rapid evaluation of state changes from hidden causes. Existing algorithms of computational topology often require lengthy computations, especially for large data sets in many dimensions.  However, since not all of those variables may be observable, one may have to reconstruct the full dynamics from partial measurements--e.g., using the process called delay-coordinate embedding. This project seeks rapid evaluation of Betti numbers based on incomplete, partial embeddings. A novel aspect is that the dynamics gives rise to a multivalued map on a simplicial complex, a ""witness map."" Selection of multiple parameters in the algorithms will be based on persistent homology, previously developed only for the analysis of static data sets and for a single parameter. The ultimate goal is robust and rapid regime detection for a limited data stream from a ""black-box"" source."
344,1439062,XPS: FULL: FP: Collaborative Research: Taming parallelism: optimally exploiting high-throughput parallel architectures,CCF,Exploiting Parallel&Scalabilty,9/1/14,8/6/14,Kunal Agrawal,MO,Washington University,Standard Grant,Anindya Banerjee,8/31/20,"$330,250.00 ",,kunal@cse.wustl.edu,CAMPUS BOX 1054,Saint Louis,MO,631304862,3147474134,CSE,8283,,$0.00 ,"Title: XPS: FULL: FP: Collaborative Research: Taming parallelism: Optimally exploiting high-throughput parallel architectures<br/><br/>Over the past decade, computer manufacturers have focused on producing ""multicore"" chips, that package multiple, powerful computing cores on a single chip. Researchers have invested significant effort in developing methods for writing programs that can run efficiently on these cores. The basic idea is to allow programmers to write programs using a high-level programming model and to rely on an underlying compiler and runtime system to efficiently schedule these programs on multicore platforms. However, due to power and heat dissipation concerns, emerging ""throughput-oriented"" computing systems increasingly rely on far simpler computing cores to deliver parallel computing performance. These cores are much more efficient than traditional multicores, and can deliver much higher performance. Practitioners across numerous fields -- bioinformatics, data analytics, machine learning, etc. -- are deploying these systems to harness their power. Unfortunately, existing high level programming models are targeted to multicore chips, and do not produce code that can run effectively on these new systems. As a result, practitioners are forced to rewrite their applications, with painstaking low-level optimization and scheduling. This project will develop schemes to adapt applications written for multicore systems to run efficiently on throughput-oriented processors. The intellectual merits are novel program optimizations that will transform multicore-oriented programs into forms that map efficiently to throughput-oriented processors, scheduling mechanisms that ensure that these throughput-oriented processors do not waste computational resources, and scheduling policies that ensure that the mechanisms are used effectively. The project's broader significance and importance are that programmers will be able to write portable, high-performant and energy-efficient programs for both traditional multicore systems as well as throughput-oriented systems. Moreover, high-level programming models will be used to program the throughput-oriented machines, thus leading to significant reduction of programming effort for practitioners in many science and engineering disciplines. Finally, outreach efforts enhance the project by providing training and mentoring to a diverse group of students.<br/><br/>Languages like Cilk provide support for ""dynamic multithreading"", which allows programmers to identify all of the parallelism in their program, while relying on sophisticated runtime systems to map that parallelism to available parallel execution hardware at runtime. However, Cilk-style execution is inappropriate for the vector-based parallelism found in SIMD units, GPUs and the Xeon Phi; vector parallelism requires finding identical computations performed on different data units. This project investigates a series of transformations that will morph Cilk-style programs into programs that expose vectorizable parallelism, allowing dynamic multithreading programs to be mapped to emerging throughput-oriented architectures. The enabling transformation involves transforming task parallel applications into data-parallel applications by identifying similar tasks being performed at different points in the computation. This project develops a series of scheduling mechanisms and provably efficient scheduling policies that ensure that parallelizing dynamic multithreading applications on throughput-oriented architectures are effective. In this manner, this project enables portable applications that run efficiently both on multicores and on vector-based architectures."
345,1449202,EAGER: A Corpus of Aligned Speech and ANS Sensor Data,IIS,"HCC-Human-Centered Computing, Robust Intelligence",8/1/14,7/22/14,Elizabeth Shriberg,CA,SRI International,Standard Grant,Tatiana Korelsky,7/31/15,"$49,990.00 ","Andreas Kathol, Massimiliano de Zambotti",elizabeth.shriberg@sri.com,333 RAVENSWOOD AVE,Menlo Park,CA,940253493,7032478529,CSE,"7367, 7495","7367, 7495, 7916",$0.00 ,"Despite a sizeable literature on emotional speech and speech under stress, little is understood about how features in continuous speech vary with subtle and real-world-relevant changes in physiological state within any particular speaker. This EArly Grant for Exploratory Research relates speech features to direct measures of physiological activation, rather than to categorical hand-annotated labels of emotion or state. The study collects and analyzes a corpus of speech and autonomic nervous system (ANS) sensor data to discover what changes occur in speech features when a person is exposed to different activation-relevant emotional, cognitive, stress-related conditions. The broader significance and impact is discovery of cues in speech that can be used to estimate changes in a speaker's physiological activation level when no sensors are available. Applications include health care (monitoring physical, mental, cognitive states), education and learning (monitoring engagement), social interaction (monitoring activation level), and law enforcement/intelligence (monitoring behavioral changes of high interest individuals).<br/><br/>In Phase 1 (Corpus Collection), the project creates a 40-subject corpus of time-aligned speech and physiological signals. Activation is measured using state-of-the-art methods to extract cardiovascular (ECG), blood pressure, respiration rate, and skin conductance signals. Each subject participates in five conditions: (1) neutral baseline; (2) emotional (description of emotionally salient pictures); (3) stressed (speaking task incentivized for accuracy and completion time); (4) cognitive load (speaking task with a visual distractor, incentivized for task completion and distractor task accuracy); and (5) computer-directed speech (task requiring perfect recognition from a speech recognizer). In Phase 2 (Analysis), sensor output is post-processed to calibrate the signals and look for changes. These changes are then compared to a range of automatically extracted features (based on acoustics, prosody, discourse patterns, and disfluency patterns) from the time-aligned speech. Analyses and machine learning experiments then examine which speech feature changes correlate with changes in sensor output, both within and across speakers. Results shed light on how information from natural continuous speech can be used to estimate changes in a speaker?s physiological activation level in ongoing, subtle and everyday contexts."
346,1409551,TWC: Medium: Collaborative: Data is Social: Exploiting Data Relationships to Detect Insider Attacks,CNS,"Special Projects - CNS, Secure &Trustworthy Cyberspace",10/1/14,4/19/18,Hung Ngo,NY,SUNY at Buffalo,Standard Grant,Shannon Beck,9/30/19,"$975,999.00 ","Oliver Kennedy, Shambhu Upadhyaya, Varun Chandola",hungngo@buffalo.edu,520 Lee Entrance,Buffalo,NY,142282567,7166452634,CSE,"1714, 8060","7434, 7924, 9178, 9251",$0.00 ,"Insider attacks present an extremely serious, pervasive and costly security problem under critical domains such as national defense and financial and banking sector. Accurate insider threat detection has proved to be a very challenging problem.  This project explores detecting insider threats in a banking environment by analyzing database searches.<br/>                                                                                   <br/>This research addresses the challenge by formulating and devising machine learning-based solutions to the insider attack problem on relational database management systems (RDBMS), which are ubiquitous and are highly susceptible to insider attacks. In particular, the research uses a new general model for database provenance, which captures both the data values accessed or modified by a user's activity and summarizes the computational path and the underlying relationship between those data values. The provenance model leads naturally to a way to model user activities by labeled hypergraph distributions and by a Markov network whose factors represent the data relationships. The key tradeoff being studied theoretically is between the expressivity and the complexity of the provenance model.  The research results are validated and evaluated by intimately collaborating with a large financial institution to build a prototype insider threat detection engine operating on its existing operational RDBMS.  In particular, with the help of the security team from the financial institution, the research team addresses database performance, learning scalability, and software tool development issues arising during the evaluation and deployment of the system. Research results are reported via technical papers and disseminated through conferences and journals, through a new research webpage at the UB's NSA- and DHS-certified center of excellence (CAE) in Information Assurance, and at the center's future workshops."
347,1356792,ABI Innovation: Computational Methods for Bioacoustic Avian Species Monitoring,DBI,"FIELD STATIONS, ADVANCES IN BIO INFORMATICS",9/1/14,9/7/16,Raviv Raich,OR,Oregon State University,Continuing grant,Peter McCartney,8/31/18,"$790,858.00 ","Xiaoli Fern, Matthew Betts",raich@eecs.oregonstate.edu,OREGON STATE UNIVERSITY,Corvallis,OR,973318507,5417374933,BIO,"1104, 1165",,$0.00 ,"This project focuses on the development of a computational framework for intelligent monitoring of avian species from in situ audio recordings. The system will gather audio from a collection of microphones to automatically and adaptively infer spatio-temporal presence/absence and abundance for different bird species. The system will adaptively seek, request, and incorporate expert feedback to improve the ability to adjust to a new environment. By enabling the automatic collection of bird data at extremely fine temporal resolutions over relatively large spatial scales, we will address two questions of fundamental ecological and conservation importance (1) Can animal behavior buffer species against the effects of environmental changes? Specifically, do birds have the capacity to shift breeding territories in instances of rapid or extreme environmental changes (e.g., weather)? (2) How does loss and fragmentation of tropical forest alter the distributions of bird species?<br/><br/>This research will build upon the recently developed Multi-Instance Multi-Label framework from machine learning, and seek to adaptively and actively train species presence/absence and abundance estimation models with the goal of maximizing the learning efficiency while minimizing the human labeling efforts. The system's unique ability to actively identify and adapt to new species (and environments) will greatly reduce the overhead required to deploy and update the bioacoustic monitoring system. This project will provide computational innovations for studying biodiversity as a function of global habitat loss and climate change, contributing significant scientific knowledge about ecosystems and their responses to human activities. The project will also provide a new technology standard for collecting bird population data that could be deployed worldwide for monitoring a diverse and evolving set of species. Research-based education and training opportunities offered by this project will help prepare a new generation of researchers in the emerging area of Ecosystem Informatics at Oregon State University. Outreach activities will 1) involve high school educators in workshops organized by the Oregon Natural Resources Education (ONRE) program, 2) organize annual data challenges/competitions in association with international conferences and workshops, 3)recruit female undergraduates and K-12 students from under-represented groups to careers in computer science and engineering through REU programs. Further information on this project can be found at http://eecs.oregonstate.edu/research/bioacoustics/."
348,1449211,EAGER: Collaborative Research: Wireless Sensing of Speech Kinematics and Acoustics for Remediation,IIS,HCC-Human-Centered Computing,9/1/14,8/18/17,Maysam Ghovanloo,GA,Georgia Tech Research Corporation,Standard Grant,Ephraim Glinert,8/31/18,"$150,000.00 ",,mgh@getech.edu,Office of Sponsored Programs,Atlanta,GA,303320420,4048944819,CSE,7367,"7367, 7916",$0.00 ,"Speech is a complex and intricately timed task that requires the coordination of numerous muscle groups and physiological systems.  While most children acquire speech with relative ease, it is one of the most complex patterned movements accomplished by humans and thus susceptible to impairment.  Approximately 2% of Americans have imprecise speech either due to mislearning during development (articulation disorder) or as a result of neuromotor conditions such as stroke, brain injury, Parkinson's disease, cerebral palsy, etc.  An equally sizeable group of Americans have difficulty with English pronunciation because it is their second language.  Both of these user groups would benefit from tools that provide explicit feedback on speech production clarity.  Traditional speech remediation relies on viewing a trained clinician's accurate articulation and repeated practice with visual feedback via a mirror.  While these interventions are effective for readily viewable speech sounds (visemes such as /b/p/m/), they are largely unsuccessful for sounds produced inside the mouth.  The tongue is the primary articulator for these obstructed sounds and its movements are difficult to capture.  Thus, clinicians use diagrams and other low-tech means (such as placing edible substances on the palate or physically manipulating the oral articulators) to show clients where to place their tongue.  While sophisticated research tools exist for measuring and tracking tongue movements during speech, they are prohibitively expensive, obtrusive, and impractical for clinical and/or home use.  The PIs' goal in this exploratory project, which represents a collaboration across two institutions, is to lay the groundwork for a Lingual-Kinematic and Acoustic sensor technology (LinKa) that is lightweight, low-cost, wireless and easy to deploy both clinically and at home for speech remediation.<br/><br/>PI Ghovanloo's lab has developed a low-cost, wireless, and wearable magnetic sensing system, known as the Tongue Drive System (TDS).  An array of electromagnetic sensors embedded within a headset detects the position of a small magnet that is adhered to the tongue.  Clinical trials have demonstrated the feasibility of using the TDS for computer access and wheelchair control by sensing tongue movements in up to 6 discrete locations within the oral cavity.  This research will leverage the sensing capabilities of the TDS system and PI Patel's expertise in spoken interaction technologies for individuals with speech impairment, as well as Co-PI Fu's work on machine learning and multimodal data fusion, to develop a prototype clinically viable tool for enhancing speech clarity by coupling lingual-kinematic and acoustic data.  To this end, the team will extend the TDS to track tongue movements during running speech, which are quick, compacted within a small area of the oral cavity, and often overlap for several phonemes, so the challenge will be to accurately classify movements for different sound classes.  To complement this effort, pattern recognition of sensor spatiotemporal dynamics will be embedded into an interactive game to offer a motivating, personalized context for speech motor (re)learning by enabling audiovisual biofeedback, which is critical for speech modification.  To benchmark the feasibility of the approach, the system will be evaluated on six individuals with neuromotor speech impairment and six healthy age-matched controls."
349,1449266,EAGER: Collaborative Research: Wireless Sensing of Speech Kinematics and Acoustics for Remediation,IIS,HCC-Human-Centered Computing,9/1/14,7/3/14,Rupal Patel,MA,Northeastern University,Standard Grant,Ephraim Glinert,8/31/18,"$149,994.00 ",Yun Fu,rupal@vocaliD.ai,360 HUNTINGTON AVE,BOSTON,MA,21155005,6173733004,CSE,7367,"7367, 7916",$0.00 ,"Speech is a complex and intricately timed task that requires the coordination of numerous muscle groups and physiological systems.  While most children acquire speech with relative ease, it is one of the most complex patterned movements accomplished by humans and thus susceptible to impairment.  Approximately 2% of Americans have imprecise speech either due to mislearning during development (articulation disorder) or as a result of neuromotor conditions such as stroke, brain injury, Parkinson's disease, cerebral palsy, etc.  An equally sizeable group of Americans have difficulty with English pronunciation because it is their second language.  Both of these user groups would benefit from tools that provide explicit feedback on speech production clarity.  Traditional speech remediation relies on viewing a trained clinician's accurate articulation and repeated practice with visual feedback via a mirror.  While these interventions are effective for readily viewable speech sounds (visemes such as /b/p/m/), they are largely unsuccessful for sounds produced inside the mouth.  The tongue is the primary articulator for these obstructed sounds and its movements are difficult to capture.  Thus, clinicians use diagrams and other low-tech means (such as placing edible substances on the palate or physically manipulating the oral articulators) to show clients where to place their tongue.  While sophisticated research tools exist for measuring and tracking tongue movements during speech, they are prohibitively expensive, obtrusive, and impractical for clinical and/or home use.  The PIs' goal in this exploratory project, which represents a collaboration across two institutions, is to lay the groundwork for a Lingual-Kinematic and Acoustic sensor technology (LinKa) that is lightweight, low-cost, wireless and easy to deploy both clinically and at home for speech remediation.<br/><br/>PI Ghovanloo's lab has developed a low-cost, wireless, and wearable magnetic sensing system, known as the Tongue Drive System (TDS).  An array of electromagnetic sensors embedded within a headset detects the position of a small magnet that is adhered to the tongue.  Clinical trials have demonstrated the feasibility of using the TDS for computer access and wheelchair control by sensing tongue movements in up to 6 discrete locations within the oral cavity.  This research will leverage the sensing capabilities of the TDS system and PI Patel's expertise in spoken interaction technologies for individuals with speech impairment, as well as Co-PI Fu's work on machine learning and multimodal data fusion, to develop a prototype clinically viable tool for enhancing speech clarity by coupling lingual-kinematic and acoustic data.  To this end, the team will extend the TDS to track tongue movements during running speech, which are quick, compacted within a small area of the oral cavity, and often overlap for several phonemes, so the challenge will be to accurately classify movements for different sound classes.  To complement this effort, pattern recognition of sensor spatiotemporal dynamics will be embedded into an interactive game to offer a motivating, personalized context for speech motor (re)learning by enabling audiovisual biofeedback, which is critical for speech modification.  To benchmark the feasibility of the approach, the system will be evaluated on six individuals with neuromotor speech impairment and six healthy age-matched controls."
350,1351362,CAREER: Geometric approaches to hierarchical and nonparametric model-based inference,DMS,"STATISTICS, Division Co-Funding: CAREER",7/1/14,7/2/18,Xuanlong Nguyen,MI,Regents of the University of Michigan - Ann Arbor,Continuing Grant,Gabor Szekely,6/30/20,"$400,000.00 ",,xuanlong@umich.edu,3003 South State St. Room 1062,Ann Arbor,MI,481091274,7347636438,MPS,"1269, 8048",1045,$0.00 ,"Hierarchical and nonparametric models present some of the most fundamental and powerful tools in modern statistics. Despite valuable advances made in the past decades, there are several widely recognized and emerging problems. First, even as these models are increasingly applied to large data sets and complex domains, a statistical theory for inferential behaviors of the hierarchy of latent variables present in the models is not yet available. Second, local inference methods based on sampling, although simple to derive, tend to converge too slowly, thereby losing their effectiveness. Third, most existing methods are incapable of handling highly distributed data sources, which are increasingly responsible for the influx of big data. Addressing these challenges requires fundamentally new ideas in theory, modeling and algorithms that must account for the contrast and interplay between the global geometry of an inference problem and the need for decentralization of inference and algorithmic implementation.  This project aims to make fundamental contributions toward advancing hierarchical model-based inference. They include a statistical theory for the latent hierarchy of variables and for analyzing the effects of transfer learning. They also include variational inference algorithms based on the global geometry of latent structures and geometric analyses of the tradeoffs between statistical and computational efficiencies. Both the algorithms and theory are unified by the use of Wasserstein geometry, which arises from the mathematical theory of optimal transportation.  Moreover, scalable hierarchical models will be developed that can exploit highly distributed data sources and decentralized inference architectures.<br/><br/>This research will improve our ability to manage, analyze and make decisions with large-scale, high dimensional and complex data, especially in the research and applications of networks and the environment. The decentralized detection algorithms for highly distributed data sources have the potential of advancing the state of the art technologies that support data-driven and high-performance distributed computing architectures. As such, this research has the potential of extending the capabilities of the real-time detection and tracking devices currently deployed in the health-care and security domains. The optimal transport based theory will deepen our understanding of hierarchical Bayesian inference, a fundamental concept of modern statistics. The algorithms and geometric analyses will provide useful tradeoffs between statistical and computational complexity, an important issue lying in the interface of Statistics and Computer Science. This research will also provide support for broadening the current statistics curriculum at the University of Michigan. The PI will integrate the teaching of statistical and computational tools with modern applications, by developing synthesis courses which interact closely with research topics of the project.  This provides an excellent opportunity to train students with a broad base of knowledge and cross-disciplinary skills in the fields of statistics, probability, machine learning, distributed computation and networked systems."
351,1409303,TWC: Medium: Collaborative: Data is Social: Exploiting Data Relationships to Detect Insider Attacks,CNS,Secure &Trustworthy Cyberspace,10/1/14,8/25/14,Xuanlong Nguyen,MI,Regents of the University of Michigan - Ann Arbor,Standard Grant,Shannon Beck,9/30/18,"$239,974.00 ",,xuanlong@umich.edu,3003 South State St. Room 1062,Ann Arbor,MI,481091274,7347636438,CSE,8060,"7434, 7924",$0.00 ,"Insider attacks present an extremely serious, pervasive and costly security problem under critical domains such as national defense and financial and banking sector. Accurate insider threat detection has proved to be a very challenging problem.  This project explores detecting insider threats in a banking environment by analyzing database searches.<br/>                                                                                   <br/>This research addresses the challenge by formulating and devising machine learning-based solutions to the insider attack problem on relational database management systems (RDBMS), which are ubiquitous and are highly susceptible to insider attacks. In particular, the research uses a new general model for database provenance, which captures both the data values accessed or modified by a user's activity and summarizes the computational path and the underlying relationship between those data values. The provenance model leads naturally to a way to model user activities by labeled hypergraph distributions and by a Markov network whose factors represent the data relationships. The key tradeoff being studied theoretically is between the expressivity and the complexity of the provenance model.  The research results are validated and evaluated by intimately collaborating with a large financial institution to build a prototype insider threat detection engine operating on its existing operational RDBMS.  In particular, with the help of the security team from the financial institution, the research team addresses database performance, learning scalability, and software tool development issues arising during the evaluation and deployment of the system. Research results are reported via technical papers and disseminated through conferences and journals, through a new research webpage at the UB's NSA- and DHS-certified center of excellence (CAE) in Information Assurance, and at the center's future workshops."
352,1408910,RI: Medium: Collaborative Research: Next-Generation Statistical Optimization Methods for Big Data Computing,IIS,Robust Intelligence,8/15/14,8/17/17,Han Liu,NJ,Princeton University,Continuing grant,Weng-keen Wong,10/31/18,"$500,000.00 ",,hanliu@northwestern.edu,Off. of Research & Proj. Admin.,Princeton,NJ,85442020,6092583090,CSE,7495,"7495, 7924",$0.00 ,"This project develops a new generation of optimization methods to address data mining and knowledge discovery challenges in large-scale scientific data analysis. The project is constructed in the context that modern computing architectures are enabling us to fit complex statistical models (Big Models) on large and complex datasets (Big Data).  However, despite significant progress in each subfield of Big Data, Big Model, and modern computing architecture, we are still lacking  powerful optimization  techniques to effectively integrate these key components.<br/><br/>One important bottleneck is that many general-purpose optimization  methods are not specifically designed for statistical learning problems. Even some of them are tailored to utilize specific problem structures, they have not actually incorporated sophisticated statistical thinking into algorithm design and analysis. To tackle this bottleneck, the project extends traditional theory to open new possibilities for nontraditional optimization  problems, such as nonconvex and infinite-dimensional examples.  The project develops deeper theoretical understanding of several challenging  issues in optimization (such as nonconvexity), develops new algorithms that will lead to better practical methods in the big data era, and demonstrates the new methods on challenging bio-informatics problems.<br/><br/>The project is closely related to NSF's mission to promote Big Data research, and will have broad impacts. In the Big Data era, we see an urgent need for powerful optimization methods to handle the increasing complexity of modern datasets.  However, we still lack adequate methods, theory, and computational techniques.  By simultaneously addressing these aspects, this project will deliver novel and useful statistical optimization methods that benefit all relevant scientific areas. The project will deliver easy-to-use software packages which directly help scientists to explore and analyze complex datasets.  Both PIs will also design and develop new classes to teach modern techniques in handling big data optimization problems. All the course materials - including lecture notes, problem sets, source code, solutions and working  examples - will be freely  accessed online.  Moreover, both PIs will write tutorial  papers and disseminate the results of this research through the internet, academic conferences, workshops,  and journals.  Through senior theses and potentially the REU (Research Experiences for Undergraduates) program, the proposed project will also actively include undergraduates and engage under-represented minority groups.<br/><br/>To achieve these goals, this project develops (i) a new research area named statistical optimization, which incorporates sophisticated statistical thinking into modern optimization, and will effectively bridge machine learning, statistics, optimization,  and stochastic analysis; (ii) new theoretical frameworks and computational methods for nonconvex and infinite-dimensional optimization, which will motivate effective optimization methods with theoretical  guarantees that are applicable to a wide variety of prominent statistical models; (iii) new scalable optimization methods, which aim at fully harnessing the horsepower of modern large-scale distributed computing infrastructure.  The project will shed new theoretical light on large-scale optimization, advance practice through novel algorithms and software, and demonstrate the methods on challenging bio-informatics problems."
353,1407939,RI: Medium: Collaborative Research: Next-Generation Statistical Optimization Methods for Big Data Computing,IIS,Robust Intelligence,8/15/14,8/13/17,Tong Zhang,NJ,Rutgers University New Brunswick,Continuing grant,Rebecca Hwa,7/31/18,"$499,955.00 ",,tzhang@stat.rutgers.edu,33 Knightsbridge Road,Piscataway,NJ,88543925,8489320150,CSE,7495,"7495, 7924",$0.00 ,"This project develops a new generation of optimization methods to address data mining and knowledge discovery challenges in large-scale scientific data analysis. The project is constructed in the context that modern computing architectures are enabling us to fit complex statistical models (Big Models) on large and complex datasets (Big Data).  However, despite significant progress in each subfield of Big Data, Big Model, and modern computing architecture, we are still lacking  powerful optimization  techniques to effectively integrate these key components.<br/><br/>One important bottleneck is that many general-purpose optimization  methods are not specifically designed for statistical learning problems. Even some of them are tailored to utilize specific problem structures, they have not actually incorporated sophisticated statistical thinking into algorithm design and analysis. To tackle this bottleneck, the project extends traditional theory to open new possibilities for nontraditional optimization  problems, such as nonconvex and infinite-dimensional examples.  The project develops deeper theoretical understanding of several challenging  issues in optimization (such as nonconvexity), develops new algorithms that will lead to better practical methods in the big data era, and demonstrates the new methods on challenging bio-informatics problems.<br/><br/>The project is closely related to NSF's mission to promote Big Data research, and will have broad impacts. In the Big Data era, we see an urgent need for powerful optimization methods to handle the increasing complexity of modern datasets.  However, we still lack adequate methods, theory, and computational techniques.  By simultaneously addressing these aspects, this project will deliver novel and useful statistical optimization methods that benefit all relevant scientific areas. The project will deliver easy-to-use software packages which directly help scientists to explore and analyze complex datasets.  Both PIs will also design and develop new classes to teach modern techniques in handling big data optimization problems. All the course materials - including lecture notes, problem sets, source code, solutions and working  examples - will be freely  accessed online.  Moreover, both PIs will write tutorial  papers and disseminate the results of this research through the internet, academic conferences, workshops,  and journals.  Through senior theses and potentially the REU (Research Experiences for Undergraduates) program, the proposed project will also actively include undergraduates and engage under-represented minority groups.<br/><br/>To achieve these goals, this project develops (i) a new research area named statistical optimization, which incorporates sophisticated statistical thinking into modern optimization, and will effectively bridge machine learning, statistics, optimization,  and stochastic analysis; (ii) new theoretical frameworks and computational methods for nonconvex and infinite-dimensional optimization, which will motivate effective optimization methods with theoretical  guarantees that are applicable to a wide variety of prominent statistical models; (iii) new scalable optimization methods, which aim at fully harnessing the horsepower of modern large-scale distributed computing infrastructure.  The project will shed new theoretical light on large-scale optimization, advance practice through novel algorithms and software, and demonstrate the methods on challenging bio-informatics problems."
354,1460316,"CAREER: Towards Context-Aware, Self-Organizing Wireless Small Cell Networks",CNS,"CAREER: FACULTY EARLY CAR DEV, Special Projects - CNS, Networking Technology and Syst",8/15/14,5/30/17,Walid Saad,VA,Virginia Polytechnic Institute and State University,Continuing grant,Monisha Ghosh,12/31/18,"$358,441.00 ",,walids@vt.edu,Sponsored Programs 0170,BLACKSBURG,VA,240610001,5402315281,CSE,"1045, 1714, 7363","1045, 7363, 9251",$0.00 ,"Providing seamless, high quality wireless service anytime and anywhere requires substantial structural changes in today's macro-cellular networks. One such change, introducing small cell base stations, is seen as a highly promising solution. However, it requires meeting fundamental challenges: 1) nodes? self-organization, 2) network heterogeneity, and 3) high sensitivity of resource allocation to the system parameters. The proposed research addresses these challenges by exploring a dimension that has often been overlooked: the user's context. To achieve this goal, first, machine learning techniques are proposed to extract context from three dimensions: device, geo-location, and social metrics. Then, context-aware resource management schemes are developed by advancing novel techniques from matching theory - a powerful tool from economics and game theory. Subsequently, the learned context is leveraged to devise cooperative small cell models using new tools from coalitional game theory. Comprehensive evaluation is done via testbed implementation and software simulations. <br/><br/><br/>The developed analytical tools will lay the foundations of context-aware, self-organizing small cell networks and will impact multiple disciplines such as communications, game theory, and social sciences. The generated results will provide fresh ideas for developing new small cell products. The research is fully integrated into the educational plan via incorporation in new and existing courses as well as training students via mentoring, participation in testbed development, and internships at industrial labs. A developed small cell educational tool will foster this integration via new hands-on activities and demonstrations to the community. Specialized outreach activities will contribute to increasing the participation of minority high school students in science and engineering."
355,1441883,"Support for the U.S. participants of the Seventh International Conference on High Dimensional Probability, May 26-31, 2014",DMS,PROBABILITY,5/1/14,4/21/14,Christian Houdre,GA,Georgia Tech Research Corporation,Standard Grant,Tomek Bartoszynski,4/30/15,"$10,000.00 ","Jan Rosinski, Dmitriy Panchenko",houdre@math.gatech.edu,Office of Sponsored Programs,Atlanta,GA,303320420,4048944819,MPS,1263,7556,$0.00 ,"This award supports participation in the Seventh International Conference on High Dimensional Probability (HDP VII) taking place during May 26-31, 2014 at the Institut d'tudes Scientifiques de Cargse (IESC), France.  High Dimensional Probability theory is the mathematical root for the analysis of Big Data, and developments in the theory and its connections to other disciplines will benefit science and society as a whole.<br/><br/>The topics of the conference will include Limit Theorems, Empirical Processes and their Applications, High Dimensional Statistics, Statistical Learning theory, Spin Glass Methods, Convex Geometry and Applications as well as Random Matrices and Additive Combinatorics. The meeting will consist of a mix of formal and informal discussions and presentations. It is not only intended to present the current state of the art in the field, but also to point out important open problems and to set new directions for the field. The information exchange taking place during HDP VII will enhance cross-pollination among the disciplines of mathematics, statistics, machine learning, and computer science.  Particular attention has been paid to bring as speakers and participants, and ultimately into the future leadership of this conference series, junior researchers and researchers from under-represented groups.<br/><br/>More information is available at <br/>http://hdpvii.gatech.edu <br/>and <br/>http://www.iesc.univ-corse.fr/en"
356,1429910,EAGER: Automatically Generating Formal Human-Computer Interface Designs From Task Analytic Models,IIS,HCC-Human-Centered Computing,1/1/14,1/30/14,Matthew Bolton,NY,SUNY at Buffalo,Standard Grant,Ephraim Glinert,8/31/16,"$149,976.00 ",,mbolton@buffalo.edu,520 Lee Entrance,Buffalo,NY,142282567,7166452634,CSE,7367,"7367, 7916",$0.00 ,"The concurrent nature of human-computer interaction (HCI) can result in situations unanticipated by designers.  Usability may not always be properly maintained or human operators may not be able to complete the task goals that a system was designed to support.  This can result in poor adoption of the system, decreased productivity with its use, or unsafe operating conditions.  Mathematical tools and techniques called ""formal methods"" exist for modeling and providing proof-based evaluations of different elements of HCI including the human-computer interface, the human operator's task analytic behavior, and usability.  Unfortunately, these approaches require the creation of formal models of interface designs, something that is non-standard practice and prone to modeling error.  This project will show that a formal-methods approach can be used to automatically generate formal human-computer interface designs that are guaranteed to adhere to usability properties and to support human operator tasks.  Specifically, a system that uses the L* machine learning algorithm will be created that will generate formal interface designs using task analytic behavior models and formal representations of usability properties.<br/><br/>The researchers will implement an interface generation system, test its performance with a suite of benchmark examples, and evaluate its ability to generate an interface for a realistic application.  To implement the generator, the researchers will first construct an oracle system capable of accepting or rejecting interface state transition sequences based on analyst-specified task models and usability properties.  This oracle system will be connected to an implementation of the L* algorithm that will progressively learn a formal interface model by observing how generated sequences of interface state transitions are accepted or rejected by the oracle.  Artificial test cases that exploit the different features of the system will be used to generate interface designs, and formal verification will be used to check that the designs exhibit the intended properties.  The system will be used to generate the human-computer interface for programming a patient controlled analgesia pump, a medical device that automatically delivers pain medication to patients intravenously.  The generated interface will then be compared against the formal interface design standard that exists for these devices.<br/><br/>The automatic generation of human-computer interface designs from task analytic models and usability properties constitutes a novel approach to user-centered design.  By using this method in the creation of interfaces, designs will be guaranteed to always exhibit certain properties.  This will potentially help ensure that designs will be accepted by users, improve the associated system's efficiency, and facilitate safer operation.  The formal representation of user interfaces that result from the implementation of this method will also permit HCI designers to pursue formal analysis and verification of other interface properties, and will facilitate the automated generation of test cases for usability verification and certification purposes.<br/><br/>Broader Impacts:  The proposed research has the potential to significantly change the way human-computer interfaces are designed.  By guaranteeing that generated interfaces are always usable, this research could improve the usability and safety of user interfaces across many domains.  The performance guarantees of the generated designs could allow development and testing times to be reduced, thus decreasing development and software costs.  This work will also enhance the education and research experience of UIC's diverse engineering student body.  The computational resources acquired for this work will be made available to student for research projects and study results will be incorporated into the curriculum of the PI's graduate and undergraduate courses.  Project results will be presented at conferences by student researchers and published with open access in high quality journals.  A dedicated website will be used to rapidly disseminate results and tools produced during this effort."
357,1405747,NeTS: JUNO: Cognitive Security: A New Approach to Securing Future Large Scale and Distributed Mobile Applications,CNS,Special Projects - CNS,5/1/14,8/15/14,Wenjing Lou,VA,Virginia Polytechnic Institute and State University,Standard Grant,John Brassil,4/30/18,"$299,999.00 ",Thomas Hou,wjlou@vt.edu,Sponsored Programs 0170,BLACKSBURG,VA,240610001,5402315281,CSE,1714,7071,$0.00 ,"Authentication is the first step in securing a networked application. Traditional authentication relies on cryptographic algorithms that verify users identify based on certain pre-configured secrets. In a world with ""trillions"" of mobile objects (""computation in everything"")  rigorous key management techniques are difficult, the traceability of pre-configured secrets is compromised, and crypto-based authentication schemes become ineffective.<br/><br/>This project will investigate an alternative approach, referred to as cognitive security, to address this compelling problem. Cognitive security aims to supplement the crypto-based solutions with unique, unforgeable, and robust credentials that are inherent to the network entities such as mobile devices and users. For example, a location claim can be verified by a location ""fingerprint"" constructed from the ambient radio signals presented at the said location at the said time. A user's identity can be verified by knowledge the verifier learned from the user's online social networks.  These credentials are physical properties of the mobile devices or knowledge naturally known to the users, which do not have to be pre-configured or remembered. Mechanisms will be developed so that these credentials can be collected or learnt through the normal network operations and be used to securely verify a device or a user's identity or claims.  <br/><br/>This is an international collaborative project, between US and Japan. The project will involve multiple areas of information technology, including security, wireless communications and networking, and machine learning. Cognitive security is a promising new approach to mobile network security where the security of the device or secrets stored in the device cannot be guaranteed."
358,1352259,CAREER: Theory and Methods for Simultaneous Variable Selection and Rank Reduction,DMS,"STATISTICS, Division Co-Funding: CAREER",6/1/14,5/22/18,Yiyuan She,FL,Florida State University,Continuing grant,Gabor Szekely,5/31/19,"$400,000.00 ",,yshe@stat.fsu.edu,"874 Traditions Way, 3rd Floor",TALLAHASSEE,FL,323064166,8506445260,MPS,"1269, 8048",1045,$0.00 ,"The data explosion in all fields of science creates an urgent need for methodologies for analyzing high dimensional multivariate data. The project deepens and broadens existing sparsity and low rank statistical theories and methods by making the following major scientific achievements: (a) an innovative selectable reduced rank methodology through simultaneous variable selection and projection, with guaranteed lower error rate than existing variable selection and rank reduction rates in theory, which paves the way to new frontiers in high dimensional statistics and information theory; (b) fast but simple-to-implement algorithms that can deal with all popular penalty functions (possibly nonconvex) in computation with guaranteed global convergence and local optimality, to ensure the practicality of the proposed approaches in big data applications; (c) a generic extension to non-Gaussian models capable of taking into account the correlation between multivariate responses, with a universal algorithm design based on manifold optimization; (d) a unified robustification scheme that can both identify and accommodate gross outliers occurring frequently in real data, to overcome the non-robustness of many conventional multivariate tools; (e) general-purpose model selection methods serving variable selection and/or rank reduction and achieving the finite-sample optimal prediction error rate with theoretical guarantee. <br/>  <br/>The need to recover low-dimensional signals from high dimensional multivariate noisy data permeates all fields of science and engineering. Hence a project of this nature, designed to develop transformative theory and methods for simultaneous variable selection and rank reduction, finds applications in a wide range of disciplines and areas such as machine learning, signal processing, and biostatistics, among others. By cross-fertilizing ideas from statistics, mathematics, engineering, and computer science, the integrated research and education help students develop critical thinking through cross-disciplinary training, and assist students in becoming life-long learners. The investigator uses the rich topics in this project to inspire the learning and discovery interest of the public and students of all ages. The educational plan consists of course development, student mentoring, outreach, and recruiting underrepresented students."
359,1429467,MRI: Acquisition of a High Performance Computer to Integrate Data Intensive Research and Education: Bringing HPC to South Jersey,OAC,Major Research Instrumentation,9/1/14,11/12/15,Nidhal Bouaynaya,NJ,Rowan University,Standard Grant,Edward Walker,8/31/17,"$397,024.00 ","Beena Sukumaran, Robi Polikar, Chun Wu",bouaynaya@rowan.edu,Office of Sponsored Programs,Glassboro,NJ,80281701,8562564057,CSE,1189,1189,$0.00 ,"This proposal seeks support for the acquisition of a shared high-performance computing (HPC) cluster to be used for data- and computation-intensive applications. The infrastructure supports research in three research areas: computational biology and bioinformatics; machine learning and data fusion; and infrastructure resilience and environmental sustainability. In each of these areas, there are multiple projects that benefit from the infrastructure. <br/>This HPC cluster is the first such public infrastructure in South Jersey. Coupled with education and research programs focused on real-world problems, the HPC cluster aids classroom instruction in relevant courses in parallel processing and advanced computer architecture. HPC computation is incorporated into the curriculum of many general courses including the courses in Biomedical engineering, Computer Science, Chemistry and Biochemistry, Mathematics, Biological Sciences, Bioinformatics, and Electrical and Computer Engineering program.<br/>The HPC facilitates expanded educational opportunities at the undergraduate level in all Science, Technology, Engineering and Math (STEM) areas."
360,1405873,CI-P: Collaborative Research: HomeSHARE - Home-based Smart Health Applications across Research Environments,CNS,CCRI-CISE Cmnty Rsrch Infrstrc,9/1/14,5/21/14,Katherine Connelly,IN,Indiana University,Standard Grant,William Bainbridge,8/31/16,"$68,968.00 ",Katie Siek,connelly@indiana.edu,509 E 3RD ST,Bloomington,IN,474013654,3172783473,CSE,7359,7359,$0.00 ,"This award supports planning for the ""Home-based Smart Health Applications across Research Environments"" (HomeSHARE) test bed, intended to be a geographically-distributed, in-situ test bed to design, develop, and evaluate pervasive home-based technologies. Addressing the growing societal need to support an aging population, researchers continue to create innovative home-based systems to support older adults as they age in-place. However, researchers find it hard to assess how generally useful systems will be because they lack resources, access to diverse populations and research infrastructure. The work supported by this award will identify the next steps to developing a robust infrastructure to support better research in this area, leading eventually to better home-based systems for aging in place.<br/><br/>Specifically, the research objectives of this planning grant are to identify the needs of various CISE research communities for a HomeSHARE testbed with respect to hardware, software, data sharing and coordination mechanisms. The PIs will work with researchers to better understand what technologies are required for a standard, baseline HomeSHARE installation, what technologies may be needed beyond the proposed baseline package for specific projects, what processes should be in place to add technologies to a subset of homes, and how the PIs can make each HomeSHARE site as self-sufficient and easy to maintain as possible. To determine and validate these requirements, the PIs will facilitate workshops at top conferences in pervasive healthcare, ubiquitous computing, human-computer interaction, software engineering, machine learning, privacy-enhanced technologies, and gerontology to identify research community needs and work with experts in each domain to confirm findings.<br/><br/>In addition to proposing an initial test bed of 100 homes, the PIs will address the issues around sustaining such a test bed over time, as well as growing the initial HomeSHARE test bed in the future. HomeSHARE will be informed by best practices in industry and academia to ensure that researchers can easily utilize the test bed and results can be shared for replication and further analysis. This award supports planning HomeSHARE for a single target domain, aging-in-place, to allow for common, shared parameters for the recruitment of participants across multiple research studies, but the creation of this test bed will inform other CISE home-based research efforts and accelerate the pace of discoveries and their implementation as well.<br/><br/>The broader impacts of the HomeSHARE planning activity include professional development opportunities for researchers who take part in HomeSHARE planning workshops, and cross-disciplinary outreach to gerontology research communities. If implemented, the HomeSHARE test bed itself will have a positive societal impact by supporting development systems for the next generation of older adults to age in place, thus improving the quality of life for older adult participants. The supported research has the further potential to increase economic competitiveness for health care, to increase diversity of the STEM workforce by attracting members of underrepresented groups by providing research opportunities with real-world impacts, and to provide innovative interdisciplinary educational opportunities."
361,1405682,CI-P: Collaborative Research: HomeSHARE - Home-based Smart Health Applications across Research Environments,CNS,CCRI-CISE Cmnty Rsrch Infrstrc,9/1/14,5/21/14,George Demiris,WA,University of Washington,Standard Grant,William Bainbridge,8/31/15,"$9,709.00 ",,gdemiris@upenn.edu,4333 Brooklyn Ave NE,Seattle,WA,981950001,2065434043,CSE,7359,7359,$0.00 ,"This award supports planning for the ""Home-based Smart Health Applications across Research Environments"" (HomeSHARE) test bed, intended to be a geographically-distributed, in-situ test bed to design, develop, and evaluate pervasive home-based technologies. Addressing the growing societal need to support an aging population, researchers continue to create innovative home-based systems to support older adults as they age in-place. However, researchers find it hard to assess how generally useful systems will be because they lack resources, access to diverse populations and research infrastructure. The work supported by this award will identify the next steps to developing a robust infrastructure to support better research in this area, leading eventually to better home-based systems for aging in place.<br/><br/>Specifically, the research objectives of this planning grant are to identify the needs of various CISE research communities for a HomeSHARE testbed with respect to hardware, software, data sharing and coordination mechanisms. The PIs will work with researchers to better understand what technologies are required for a standard, baseline HomeSHARE installation, what technologies may be needed beyond the proposed baseline package for specific projects, what processes should be in place to add technologies to a subset of homes, and how the PIs can make each HomeSHARE site as self-sufficient and easy to maintain as possible. To determine and validate these requirements, the PIs will facilitate workshops at top conferences in pervasive healthcare, ubiquitous computing, human-computer interaction, software engineering, machine learning, privacy-enhanced technologies, and gerontology to identify research community needs and work with experts in each domain to confirm findings.<br/><br/>In addition to proposing an initial test bed of 100 homes, the PIs will address the issues around sustaining such a test bed over time, as well as growing the initial HomeSHARE test bed in the future. HomeSHARE will be informed by best practices in industry and academia to ensure that researchers can easily utilize the test bed and results can be shared for replication and further analysis. This award supports planning HomeSHARE for a single target domain, aging-in-place, to allow for common, shared parameters for the recruitment of participants across multiple research studies, but the creation of this test bed will inform other CISE home-based research efforts and accelerate the pace of discoveries and their implementation as well.<br/><br/>The broader impacts of the HomeSHARE planning activity include professional development opportunities for researchers who take part in HomeSHARE planning workshops, and cross-disciplinary outreach to gerontology research communities. If implemented, the HomeSHARE test bed itself will have a positive societal impact by supporting development systems for the next generation of older adults to age in place, thus improving the quality of life for older adult participants. The supported research has the further potential to increase economic competitiveness for health care, to increase diversity of the STEM workforce by attracting members of underrepresented groups by providing research opportunities with real-world impacts, and to provide innovative interdisciplinary educational opportunities."
362,1451380,EAGER: Example-based Audio Editing,IIS,HCC-Human-Centered Computing,9/1/14,8/21/14,Paris Smaragdis,IL,University of Illinois at Urbana-Champaign,Standard Grant,Ephraim Glinert,5/31/18,"$150,000.00 ",,paris@illinois.edu,1901 South First Street,Champaign,IL,618207406,2173332187,CSE,7367,"7367, 7916",$0.00 ,"Contemporary users of technology interact with photos and video by editing them, but still use audio only passively, by capturing, storing, transmitting, and playing it back. These two different ways of interacting with contemporary media persist because current software tools make it very difficult for general users to manipulate audio. This project will develop novel technologies that will make audio editing and manipulation accessible to non-experts. These tools will allow a user to guide the software with audio editing requests by vocalizing the desired edits, providing before/after examples of the desired effects, or by presenting other recordings that exhibit the desired audio manipulations. For example, a user might issue a command to the software to equalize sounds by using a booming voice for more bass, or a nasal tone for middle frequencies; to add echoes by mimicking the desired effect by uttering ""hello, hello, hello ..."" with each successive ""hello"" in a lower volume; or to add reverb by providing examples of recordings with the desired reverb. Making it easier for general computer users to manipulate and edit audio recordings can impact many fields, such as medical bioacoustics, seismic signal analysis, underwater monitoring, audio forensics, surveillance applications, oil exploration probing, conversational data gathering, and mechanical vibration measuring. The goals of this project are to provide novel and practical audio tools that will allow non-expert practitioners from these fields to easily achieve required audio manipulations.<br/><br/>The project will exploit modern signal processing and machine learning techniques to produce more intuitive interfaces that help people accomplish what are currently difficult audio editing tasks. This will include developing novel estimators to extract editing-intent parameters directly from audio recordings. The project will focus on three different editing operations: equalization, noise control, and echo/reverberation. A number of different approaches will be explored for each operation. For example, for equalization, one approach will have users select before and after sounds to identify their desired modification, and the system will then use spectral deconvolution estimations to directly compute the transfer function that maps the spectrum of the before sound to that of the after sound, and apply that function to the audio recording that the user is editing. For noise control, one approach will have users vocalize what types of noise to remove, and then match the user's input with the corresponding component in the recording that is being edited by using low-rank spectral decomposition. For reverb and echo, one approach will have users utter ""one, two, three, ..."" to illustrate the desired number of repetitions, temporal spacing, and attenuation between echoes, and then use voice detection measurements to extract the echo parameters, while correcting for vocalization errors such as random inconsistency in the echo spacing. The project will create new theories of how human guidance and automated audio-intelligent processing can work in tandem to solve fundamental and practical problems."
363,1443217,I-Corps:  SmartSpiro,IIP,I-Corps,7/1/14,6/30/14,Ashutosh Sabharwal,TX,William Marsh Rice University,Standard Grant,Rathindra DasGupta,12/31/15,"$50,000.00 ",,ashu@rice.edu,6100 MAIN ST,Houston,TX,770051827,7133484820,ENG,8023,,$0.00 ,"Spirometry, the measurement of breath, is one of the commonly used tests in the diagnosis of pulmonary and respiratory illnesses.  Today, patients can only have spirometry tests conducted at a clinic or a hospital's Pulmonary Function Test Lab. Since accurate spirometry requires an expert technician's guidance, all diagnostic spirometries can only be conducted at these locations. As a result, the wait times for spirometries are often long, e.g. 3 months in Houston and nearly impossible even in large towns in India.  This research team will develop SmartSpiro, the first clinical grade portable spirometer that brings spirometry to the masses.<br/>  <br/>The innovation in SmartSpiro is that it performs a role of a trained technician by algorithmically finding errors in spirometry, using a combination of signal processing and machine learning. Current prototype can already detect all the major spirometry errors with 94% accuracy and it will be improved as algorithms are further refined. In addition to error correction, to improve ease of use SmartSpiro for non-expert users, the team will be adding local language customization and spoken word feedback capabilities. They will also be designing customizable incentive-driven displays on the spirometer hardware to support better patient compliance. SmartSpiro aims to commercialize a device which caters to under-served patient segments and pulmonary conditions, e.g. developing countries in India, or rural areas in the developed countries like Houston. In addition, SmartSpiro's automated test capability will allow for use of spirometry outside of testing labs, and empower point-of-care pulmonary testing (e.g. at the patient?s bedside) which is currently infeasible using current spirometry technology."
364,1409506,NeTS: Medium: Collaborative Research: Tango: Performance and Fault Management in Cellular Networks through Device-Network Cooperation,CNS,Networking Technology and Syst,8/1/14,8/21/18,Saurabh Bagchi,IN,Purdue University,Continuing Grant,Alexander Sprintson,7/31/19,"$524,334.00 ","Dan Goldwasser, Yuan Qi",sbagchi@purdue.edu,Young Hall,West Lafayette,IN,479072114,7654941055,CSE,7363,7924,$0.00 ,"Cellular networks have become a part of the infrastructure that we rely on without pausing to think of the enormous complexity that underlies such networks. These networks serve an amazing diversity of mobile devices and operate in an amazing diversity of radio environments. Failures in such networks are not uncommon and we take their consequences in our stride as a fact of life, complaining about call drops and data disconnections. Similarly, we also encounter performance degradations, either in voice quality of calls or connection speeds. With the fast increasing amount of traffic on cellular networks and the cellular spectrum crunch, these conditions are expected to become more severe in the near to mid-term future. Further, in today's networks, there is no facility to provide services to a device by orchestrating an entire network of geographically dispersed mobile devices, such as, for providing a ""video"" of the path of an oncoming tornado.<br/><br/>In this project, the researchers put forward the view that rather than reactive management of failures or performance degradations, there is a need to build into cellular networks, proactive management of such events. The primary difficulty for this is that, to the end devices, the network is considered essentially as a black box. Conditions about the network, such as, congestion or overload of some elements of the cellular infrastructure, are not made visible to the applications on the mobile end devices. Likewise, much of the application eco-system within a device is not made visible to the network. Thus, it is not known to the network what is the latency requirement of some application or what demand it is going to place in the near future on the network. The proposed solution approach will turn this black box approach on its head and enable proactive fault and performance management and an entirely new class of applications through cooperation between the network and the devices, a Tango of sorts.<br/><br/>The project goals will be accomplished through two synergistic project thrusts. In the first, called ""Fault Management and Orchestrated Services"", the team will develop computational and statistical models for the real-time cellular data and analytics tools for the same, machine learning algorithms for predicting hard and soft failures, and mitigation actions for avoiding predicted failures. It will also develop orchestrated services between the cellular network and the end users to achieve system-wide goals under the broad umbrella of ""participatory sensing"". In the second thrust, called ""Performance-driven Design"", the team will develop an API that will allow the cellular devices and the cellular network to cooperate for achieving common desired system-level properties, such as, higher data bandwidth. It will provide advanced traffic management schemes through back-filling of delay-tolerant traffic and computation offloading from the mobile devices, leveraging a team member's recent efforts in the Cirrus Clouds Project.  The project will result in the development of a miniature LTE/UMTS testbed for open experimentation, open data access, and evaluation of the protocols mentioned above."
365,1408173,Dynamic behavioral and neural effects of cognitive control on language processing,SMA,SPRF-IBSS,7/1/14,5/19/14,Nina Hsu,MD,University of Maryland College Park,Standard Grant,Josie S. Welkom,6/30/16,$0.00 ,"Luiz Pessoa, Jared Novick",ninahsu@umd.edu,3112 LEE BLDG 7809 Regents Drive,COLLEGE PARK,MD,207425141,3014056269,SBE,8209,,$0.00 ,"As people navigate their world, they sometimes face conflicting sources of information due to new rules, cues, or plans of action. In memory, for instance, the presence of familiar but irrelevant information can interfere with a person's ability to recognize an object correctly. While processing language, evidence from discrepant sources of input can result in misinterpretation. In these situations, cognitive control allows people to adjust thoughts and actions on-the-fly to mediate conflicting representations in both linguistic and nonlinguistic contexts. Yet, little is known about whether engaging cognitive control on one task influences or causes performance shifts on another. For example, is the ability to revise incorrect interpretations during language processing affected by prior experience of information-conflict, even in a nonlinguistic context? This project is designed to understand the interplay among multiple cognitive systems, whether the same cognitive control functions operate systematically across conflict in different domains, and to characterize the behavioral and neurobiological mechanisms that underlie their interaction. Specifically, this research investigates the causal relationship between cognitive control and language processing. To understand meaning and intent, people must be able to efficiently process spoken and written language input. Developing and honing these abilities takes considerable practice, especially when reading and listening to language also means dealing with conflicting sources of information. Such information-conflict can hinder a person's ability to make sense of complex material, which can lead to high rates of interpretation error. The work validates, informs, and improves prior training work through rigorous assessment of the underlying causal neurobiological mechanisms. Additionally, the Fellow is fully committed to STEM education and training in cognitive neuroscience. These experiments provide training opportunities for the Fellow and members of her research team. Results are disseminated to target audiences and submitted to academic conferences and journals. Moreover, as a woman, the Fellow has been involved in STEM outreach to underrepresented local groups. In this way, she hopes to increase interest in STEM fields, encouraging diverse and underrepresented groups to engage in scientific research.<br/><br/>This project makes important contributions to understanding the human computational system that supports the real-time interpretation and re-interpretation of sentences. The experiments adopt converging eye-tracking and neuroimaging techniques to address a central issue in cognitive science: how language processing is affected by the engagement status of the cognitive control system. These studies represent the first methodical investigation into how nonlinguistic factors shape the time-course of language comprehension in both brain and behavior. Because cognitive control deficits affect patients' memory and language skills alike, elucidating the dynamic interplay between these cognitive systems has major health implications. The results from this research inform an understanding of shared language, memory, and cognitive control functions in the human mind and brain, which can be applied to public knowledge about how various cognitive systems develop typically and atypically, and how they fail following injury to the underlying neurobiological structures. As a result, the PI-team can draw conclusions about the causal nature of certain language and memory processes, which can be disseminated to and used in clinical, educational, and government settings. The first study tests how dynamic cognitive control engagement (turning it on or off depending on previous exposure to conflict in a nonlinguistic task) affects real-time language processing, indexed by fine-grained eye-movement patterns to objects in a scene as listeners carry out spoken instructions. Study 2 takes a neurobiological approach to examine cortical changes during language processing depending on whether cognitive control has been triggered by preceding experience. Study 3 investigates the extent to which ostensibly different memory and language tasks share a common conflict-control mind state by testing whether machine-learning algorithms can accurately classify brain-activation patterns broadly across domains. By pairing eye-tracking and fMRI techniques, this work offers converging evidence for a general-purpose cognitive control system whose deployment can shape how a person interprets language and how specified brain systems respond to linguistic input that is ripe for misanalysis."
366,1418812,Geometric Methods for Graph Partitioning,DMS,COMPUTATIONAL MATHEMATICS,8/15/14,8/6/14,Braxton Osting,CA,University of California-Los Angeles,Standard Grant,Leland Jameson,10/31/14,"$79,000.00 ",Dominique Zosso,osting@math.utah.edu,10889 Wilshire Boulevard,LOS ANGELES,CA,900951406,3107940102,MPS,1271,9263,$0.00 ,"The proposed activity is to develop and analyze new computational methods for a graph partitioning problem based on the Beltrami energy. This problem has diverse applications in machine learning and image analysis (medical, satellite, and material). For example, a clear need for such methods in imaging has been identified by collaborators at the California NanoSystems Institute, where the proposed work will directly impact fundamental research in nano science and foster the understanding of amyloid beta sheets. Such amyloids are associated with the pathology of more than 20 serious human diseases including Alzheimer's and other neurodegenerative diseases. Thanks to the multidisciplinary nature of the activity, awareness and literacy outside one's field will also be mutually fostered for all involved parties.<br/><br/>The PIs, together with their students and collaborators, will seek new methods that combine variational arguments with ideas from geometry and partial differential equations in order to extend and overcome the limitations of existing methods. The research has three goals. The first goal concerns fundamental theoretical questions raised by the proposed model: analyze the existence, uniqueness, and properties of the minimizers of the variational problems; establish a generalized isoperimetric inequality related to the Beltrami functional in the continuum; and explore relations to existing theorems and conjectures about optimal partitions. A graph analogue of the Beltrami energy is formulated and is the foundation for a graph partitioning objective. The PIs have identified a relaxation of this objective and propose an in-depth study of a provably convergent rearrangement method for its solution. The second goal addresses the important computational and numerical aspects of the proposed graph partitioning model. An efficient optimization strategy is key for the framework to be usable in practical applications. Here, promising primal-dual methods from convex optimization will be utilized, as well as other competitive state-of-the-art methods. To develop the most efficient solution method, it will be crucial to explore the similarities to other models, including those for non-negative matrix factorization and those related to motion by mean curvature. The third goal is to address concrete, real-world problems and to engage the developed algorithms in practical applications of societal importance."
367,1516340,RAPID: SCH: NODE: A Real-Time Smartphone Epidemiological Tool,IIS,"Information Technology Researc, Smart and Connected Health",12/1/14,12/2/14,Henry Kautz,NY,University of Rochester,Standard Grant,Wendy Nilsen,5/31/16,"$132,504.00 ","Solomon Abiola, Ray Dorsey",kautz@cs.rochester.edu,"518 HYLAN, RC BOX 270140",Rochester,NY,146270140,5852754031,CSE,"1640, 8018","001Z, 1640, 7526, 7914, 8018",$0.00 ,"In West African countries, cell phone penetration can range from 40 to 80% and upwards, with over 2 million Android devices in Sierra Leone, Guinea, and Liberia alone. The investigators propose the delivery of a smartphone application that will improve care-seeking, epidemiology, and prevention by monitoring civilian location patterns, habits (e.g. walking, sleeping), and resource needs (e.g. hand sanitizers or gloves). This project addresses three key problems. First, how well can smartphones sensors support real-time monitoring and prediction of the scale of an infectious disease (in this case Ebola) compared to stochastic epidemiology models? Second, how can smartphone communications be used to educate a population about prophylactic behaviors and change such behaviors? Third, is mHealth (mobile health) sustainable in West Africa, or is the infrastructure too underdeveloped to support such innovations over the long term? Our project is novel in that it uses passively obtained location information, sensor data, and dynamic surveys to understand the health and potential for infection among the population in real time to supplement CDC, WHO and UN efforts.<br/><br/>This RAPID proposes to seek to use machine learning to classify data obtained from phone sensors (e.g. location in a village with high disease rate, reduced movement, self-photo of rashes) along with survey questions to determine if users are developing symptoms that may be similar to Ebola. The project will also look at human mobility patterns compared to stochastic epidemiological models to determine the efficacy of real-time cell phone tracking. The coupling of these sensor modalities will be used to model users as a noisy sensor and compare how this information agrees or predicts historical trends. Finally, the project will use features of behavioral science to explore what feedback does to affect user behavior, such as, knowing about their risk for Ebola."
368,1350655,CAREER: Toward Cooperative Interference Mitigation for Heterogeneous Multi-Hop MIMO Wireless Networks,CNS,Networking Technology and Syst,7/1/14,5/26/15,Ming Li,UT,Utah State University,Continuing grant,Thyagarajan Nandagopal,10/31/15,"$195,035.00 ",,lim@email.arizona.edu,Sponsored Programs Office,Logan,UT,843221415,4357971226,CSE,7363,"1045, 9150",$0.00 ,"The ever-growing number of wireless systems and the scarcity for available spectrum necessitates highly efficient spectrum sharing among disparate wireless networks. Many of them are heterogeneous in hardware capabilities, wireless technologies, or protocol standards. The resulting cross-technology interference (CTI) can be detrimental to the performance of co-locating networks if not properly mitigated. Current interference management approaches mostly follow the interference-avoidance paradigm, where transmissions are separated in frequency, time, or space to enable spectrum sharing, rather than to reduce or eliminate interference. This project explores cooperative interference mitigation (CIM), a new coexistence paradigm among heterogeneous multi-hop wireless networks. By exploiting recent advances in multi-input multi-output (MIMO) interference cancellation (IC) techniques, the proposed approach allows disparate networks to cooperatively cancel/mitigate their CTI to enhance everyone?s performance. This research focuses on the following objectives: 1) Develop tractable models/frameworks to analyze the theoretical limits and performance bounds of CIM for heterogeneous multi-hop networks, considering various forms of network heterogeneity; 2) Study the incentives of CIM through a novel game theoretic framework, that characterizes the conditions of mutual cooperation and thwarts selfish or malicious behavior; 3) Design distributed performance-approaching algorithms to achieve CIM and integrate them into practical network/MAC layer protocols, by exploiting machine learning tools and implicit inter-system communications. The expected outcomes also include the development of various simulation toolkits and system prototypes for experimental validation. <br/><br/>The integrated education plan includes cross-discipline curriculum development, student mentoring and outreach. The proposed research will have broad impacts on unplanned heterogeneous multi-hop networks that share spectrum resources, such as current and future networks in unlicensed bands, and secondary networks in TV white spaces. Applications will benefit multiple domains including healthcare, energy, emergency services and military etc. Major results will be disseminated via conference and journal publications, software packages, talks and tutorials."
369,1318688,CIF: SMALL: RUI: Novel Detection Approaches with Comprehensive Hybrid Intelligent Systems for Multimedia Forensics,CCF,SIGNAL PROCESSING,1/1/14,8/7/13,Qingzhong Liu,TX,Sam Houston State University,Standard Grant,Phillip Regalia,8/31/18,"$249,997.00 ",,qxl005@shsu.edu,P O Box 2448,Huntsville,TX,773412448,9362943621,CSE,7936,"7923, 7936, 9229",$0.00 ,"Multimedia forensics is increasingly becoming an important research area for protecting public safety and enhancing national security; steganalysis and forgery detection are becoming two active research areas, even as they are still in their inception. The shortage of comprehensive detection systems has severely impeded progress and practical applications in the field. This study will facilitate the work of forensic practitioners in criminal investigations and will deliver to the community benchmark datasets, performance evaluation standards and models, software design and the analysis toolkit in order to design and test new steganalysis/forgery detection system approaches. Additionally, as part of the research education plan, undergraduate women, minorities, and first-generation college students will be extensively involved. A further contribution to an educational pipeline for computer science involves local high school students through the creation of a multimedia forensics and knowledge discovery laboratory. <br/><br/>The research involves designing comprehensive hybrid intelligent systems for steganalysis and forgery detection with the capability to discriminate the operations of steganography, general processing and forgery manipulation. This research will make contributions to engineering by providing the optimal classification/regression models associated with the identification of minimal optimal feature sets, benefitting a broad range of applications. The scientific objectives of the research involve validation of new paradigms, shift-recompression-based derivative/wavelet differential pattern analyses on multimedia data, to merge high dimensional rich models that were used for steganalysis, and apply judicious feature mining and machine learning techniques to handle high dimensionality and establish comprehensive hybrid intelligent forensics systems to recognize different categories of multimedia data and reveal past processing history."
370,1409589,NeTS: Medium: Collaborative Research: Tango: Performance and Fault Management in Cellular Networks through Device-Network Cooperation,CNS,Networking Technology and Syst,8/1/14,6/5/17,Mostafa Ammar,GA,Georgia Tech Research Corporation,Continuing grant,Monisha Ghosh,7/31/18,"$375,000.00 ",,ammar@cc.gatech.edu,Office of Sponsored Programs,Atlanta,GA,303320420,4048944819,CSE,7363,7924,$0.00 ,"Cellular networks have become a part of the infrastructure that we rely on without pausing to think of the enormous complexity that underlies such networks. These networks serve an amazing diversity of mobile devices and operate in an amazing diversity of radio environments. Failures in such networks are not uncommon and we take their consequences in our stride as a fact of life, complaining about call drops and data disconnections. Similarly, we also encounter performance degradations, either in voice quality of calls or connection speeds. With the fast increasing amount of traffic on cellular networks and the cellular spectrum crunch, these conditions are expected to become more severe in the near to mid-term future. Further, in today's networks, there is no facility to provide services to a device by orchestrating an entire network of geographically dispersed mobile devices, such as, for providing a ""video"" of the path of an oncoming tornado.<br/><br/>In this project, the researchers put forward the view that rather than reactive management of failures or performance degradations, there is a need to build into cellular networks, proactive management of such events. The primary difficulty for this is that, to the end devices, the network is considered essentially as a black box. Conditions about the network, such as, congestion or overload of some elements of the cellular infrastructure, are not made visible to the applications on the mobile end devices. Likewise, much of the application eco-system within a device is not made visible to the network. Thus, it is not known to the network what is the latency requirement of some application or what demand it is going to place in the near future on the network. The proposed solution approach will turn this black box approach on its head and enable proactive fault and performance management and an entirely new class of applications through cooperation between the network and the devices, a Tango of sorts.<br/><br/>The project goals will be accomplished through two synergistic project thrusts. In the first, called ""Fault Management and Orchestrated Services"", the team will develop computational and statistical models for the real-time cellular data and analytics tools for the same, machine learning algorithms for predicting hard and soft failures, and mitigation actions for avoiding predicted failures. It will also develop orchestrated services between the cellular network and the end users to achieve system-wide goals under the broad umbrella of ""participatory sensing"". In the second thrust, called ""Performance-driven Design"", the team will develop an API that will allow the cellular devices and the cellular network to cooperate for achieving common desired system-level properties, such as, higher data bandwidth. It will provide advanced traffic management schemes through back-filling of delay-tolerant traffic and computation offloading from the mobile devices, leveraging a team member's recent efforts in the Cirrus Clouds Project.  The project will result in the development of a miniature LTE/UMTS testbed for open experimentation, open data access, and evaluation of the protocols mentioned above."
371,1442642,CyberSEES: Type 2: Collaborative Research: Tenable Power Distribution Networks,CCF,CyberSEES,9/1/14,8/11/14,Xiuzhen Cheng,DC,George Washington University,Standard Grant,richard brown,8/31/17,"$227,738.00 ",,cheng@gwu.edu,1922 F Street NW,Washington,DC,200520086,2029940728,CSE,8211,8208,$0.00 ,"This project advances modeling and computational frameworks to guarantee the sustainability of power distribution networks from environmental, economic, and social perspectives. Environmental sustainability is addressed by accounting for the increased uncertainty associated with the rapid and ad-hoc integration of renewable generation and elastic loads. With regard to economic sustainability, computational and inference foundations are put forth to ensure effective and secure market operations in the face of the imminent emergence of large-scale distribution-level electricity markets. Lastly, social sustainability is effected through cyber innovations focused on increasing economic utility and enhancing cyber security. <br/><br/>The proposed research aims for broad socio-technical advances in electricity distribution networks. Successful project completion will offer cyber innovations that enable the systematic integration of stochastic renewable generation while improving end-user satisfaction. Given the ubiquity of the suite of research tools and methodologies, the utility of the proposed research goes well beyond the envisioned application area to the broader fields of optimization, stochastic processes, control systems, machine learning, statistical signal processing, and cyber security. Broader transformative impact will result from pragmatic test cases proposed for validation, involvement of undergraduates in research, and outreach activities."
372,1402266,EAGER: An Exploratory Study of Multi-Hazard Management through Multi-Source Integration of Physical and Social Sensors,CNS,Computer Systems Research (CSR,5/1/14,6/29/15,Calton Pu,GA,Georgia Tech Research Corporation,Standard Grant,M. Mimi McClure,4/30/17,"$308,000.00 ",,calton@cc.gatech.edu,Office of Sponsored Programs,Atlanta,GA,303320420,4048944819,CSE,7354,"7916, 9178, 9251",$0.00 ,"Natural and man-made disasters can cause significant material damages and human suffering. For example, Superstorm Sandy of 2012 is estimated to have caused more than $68 billion in damages and killed at least 286 people in seven countries. Improving the preparation for, response to, and recovery from disasters can reduce damages, relieve human suffering, and speed up recovery. Among disasters, a multi-hazard is a sequence of disasters in which the first disaster causes the subsequent disasters, making it far more difficult for emergency response teams to handle all of them. For example, the March 11, 2011, Tohoku, Japan, earthquake triggered an unprecedented tsunami, which led to flooding at, and partial meltdown of, the Fukushima Daiichi Nuclear Power Plant. A more frequent example of multi-hazards is landslides, which can be triggered by many causes including earthquakes, rainfall, and man-made environmental changes.<br/><br/>While the detection of a single disaster usually only requires one kind of dedicated sensor, for example, seismographs can detect earthquakes reliably, multi-hazards often require a combination of various kinds of sensors for the detection of the multiple events in the sequence. Indeed, the detection of multi-events in general and multi-hazards in particular is a non-trivial problem due to the various kinds of events involved and the large number of combinations that make offline combinatorial analysis impractical. In the case of landslides, their detection is complicated further by the several possible and unrelated causes of landslides (e.g., earthquake and rainfall), each requiring a different kind of sensor.<br/><br/>In this project, the team is building a landslide detection system, called LITMUS, that integrates data from two physical sensors -- USGS Global Seismographic Network (GSN), NASA Tropical Rainfall Monitoring Mission (TRMM) -- with data from pervasive social media platforms. This integration of multiple heterogeneous sensors in LITMUS is an illustrative example of successfully applying big data software tools and analytics techniques to solve real-world problems. Specifically, the team is extending geo-tagging to relevant data items, which are filtered in several stages to reduce noise and false positives, and applying machine learning, information retrieval, and semantic web techniques to each data stream. Finally, filtered social media data are being cross-referenced with physical events from the same geo-location to generate supporting evidence for landslide detection. A LITMUS prototype has been detecting more landslides around the world than traditional landslide reporting systems: tests with live streaming data show that the combined result is a list of landslide events that has included the USGS authoritative list, plus many other confirmed landslides around the world."
373,1423663,CIF: Small: Structured Signal Recovery from Noisy Measurements via Convex Programming: A Framework for Analyzing Performance,CCF,COMM & INFORMATION FOUNDATIONS,8/1/14,8/4/14,Babak Hassibi,CA,California Institute of Technology,Standard Grant,Richard Brown,7/31/17,"$400,000.00 ",,hassibi@caltech.edu,1200 E California Blvd,PASADENA,CA,911250600,6263956219,CSE,7797,"7923, 7936",$0.00 ,"With the advent of ubiquitous sensing (multi-modal sensors, imaging systems and cameras, etc.), various complex social networks, and the deluge of health-care data (DNA sequences, micro-arrays, etc.), society is now officially in the era of Big Data. In such a setting, the ability to systematically and efficiently derive structured models, and recover reliable and actionable information, from barrages of high dimensional data will have far-reaching impact on engineering challenges and on everyday life. Unfortunately, the data is often noisy, inaccurate, or partially missing. This research will develop a comprehensive theory to assess the performance of a very wide class of algorithms designed for this purpose which are based on convex programming techniques.  Such performance guarantees will assist practitioners in a wide array of applications in signal processing, machine learning, statistics and data analysis.<br/><br/>Recent years has witnessed some spectacular theoretical and algorithmic advances in convex optimization and compressed sensing that have changed how large noisy data sets are handled. Despite these successes, key challenges remain, including the need for a comprehensive theory that accurately predicts the performance of the algorithms and goes beyond the customary ?order-wise? performance guarantees. The investigators will pursue an ambitious research program to give exact performance evaluations for a wide variety of convex-optimization-based signal recovery methods, including the classical LASSO and its variants.  The framework can deal with a wide array of signal-to-noise ratios, different measurement matrix ensembles, and a variety of cost functions and signal structures. The techniques draw upon a host of ideas in high-dimensional geometry, statistics, and signal processing and are the culmination of a flurry of activity by several different research communities."
374,1457664,GENSIPS 2014: Workshop on Genomic Signal Processing and Statistics,CCF,ALGORITHMIC FOUNDATIONS,12/1/14,11/12/14,Peng Qiu,GA,Georgia Tech Research Corporation,Standard Grant,Mitra Basu,11/30/15,"$10,000.00 ","Dongmei Wang, Yufei Huang",peng.qiu@bme.gatech.edu,Office of Sponsored Programs,Atlanta,GA,303320420,4048944819,CSE,7796,7931,$0.00 ,"Computational systems biology is a multi-disciplinary research area that combines biology, computer science, engineering and statistics. The goal of this research area is to develop and apply computational methods for analyzing of complex biological phenomena. The 2014 IEEE Workshop on Genomic Signal Processing and Statistics (GENSIPS) aims to bring together expertise in biology, computer science, signal processing, engineering and statistics, and provide a forum to discuss challenges confronting the research community. GENSIPS'14 teamed up with the 2014 IEEE Global Conference on Signal and Information Processing (GlobalSIP'14), a flagship conference in IEEE Signal Processing Society that brings together 13 symposia and 2 workshops. GENSIPS is one of the two workshops, and is synergistic to symposiums on algorithms, big data and machine learning. Such a partnership is anticipated to foster collaborations and interdisciplinary research ideas. <br/><br/><br/>Intellectual Merit: The theme of GENSIPS-14 is going to be centered around computational systems biology and signal processing. One major goal is to provide a forum for addressing research problems at the forefront of this area, including next-generation sequencing, integration of multi-modality genomics data, modeling of regulatory and signaling networks, single-cell analysis, metagenomics, and several other topics in this fast growing field. <br/><br/>Broader Impact: The partnership with GlobalSIP-14 attracts participations from the signal processing society. To enable signal processing researcher to jump-start in computational systems biology, GENSIPS-14 offers three tutorials on drug effect prediction, single-cell analysis, and imaging informatics. <br/><br/>Another important mission of GENSIPS-14 is to promote the participation of graduate students and post-doctoral researchers (especially women and minority), and foster education and development of the next-generation scientists in computational systems biology. GENSIPS-14 will provide travel awards to encourage graduate students to participate. Specifically, 20-30% of the awards will be allocated to encourage women and minority students to participate and submit papers to GENSIPS-14."
375,1414813,EAPSI: Diagnosing the heat engines of massive tropical cloud systems,OISE,EAPSI,6/1/14,5/29/14,Matthew Bowers,IN,Bowers Matthew C,Fellowship,Anne L. Emig,5/31/15,"$5,070.00 ",,,,West Lafayette,IN,479062626,,O/D,7316,"5924, 5978, 7316",$0.00 ,"Accurately simulating tropical cloud systems has long been identified as a critical component of global weather and climate predictions. Indeed, prediction bias in the tropics is known to corrupt forecasts in higher latitudes, impeding the mitigation of weather and climate hazards posed to the global society. The dominant 20--90 day feature of the tropical atmosphere is a massive, eastward-propagating cloud system known as the Madden-Julian Oscillation (MJO). Due to the elusive nature of its physical origins and its influence on global weather and climate, accurately predicting the MJO is a demanding and critical scientific challenge. This research aims to provide a novel procedure for evaluating the ability of weather and climate models to realistically simulate the MJO and for diagnosing the sources of bias in model simulations. The methodology is based on analyzing and contrasting the heat engines, or energetics, of both observed and simulated MJO systems. The research will be conducted in collaboration with Dr.Wei-Ting Chen, a noted expert on tropical meteorology and model simulation of the MJO at the National Taiwan University. <br/><br/>This research will involve contrasting features of observed MJO convection signals with those produced by model simulations. In particular, for both observational and model data, statistical machine learning techniques will be utilized to identify stages (or phases) of both predominantly equatorially-symmetric and antisymmetric MJO signals. Various energetics terms are to be computed within the cyclic framework, enabling an examination of energy transformation processes throughout the MJO life cycle. Contrasting features of the energy transformation processes in observations versus model simulations provides a powerful basis for assessing model representation of the MJO and diagnosing the sources of model biases, tasks which are fundamental for improving weather and climate simulations. This NSF EAPSI award is funded in collaboration with the National Science Council of Taiwan."
376,1421443,SHF: Small: Specializing Compilers For High Performance Computing Through Coordinated Data and Algorithm Optimizations,CCF,Software & Hardware Foundation,8/1/14,4/22/19,Qing Yi,CO,University of Colorado at Colorado Springs,Standard Grant,Almadena Chtchelkanova,7/31/20,"$493,631.00 ",,qyi@uccs.edu,"1420, Austin Bluffs Parkway",Colorado Springs,CO,809183733,7192553153,CSE,7798,"7923, 7942, 7943, 9251",$0.00 ,"This research brings about a new methodology for developing compilers, where the data structure and algorithm implementations of software applications are independently normalized and categorized into commonly occurring patterns, compiler optimizations are made customizable components that can be flexibly composed, and all optimizations are closely coordinated and collectively specialized to attain a highest level of performance. The pattern-based specialization specifically targets a number of domains, e.g., dense/sparse matrix codes, stencil computations, and graph/machine learning algorithms, which are critical to scientific computing.  A uniform annotation interface is provided for developers to concisely document the higher-level semantics of abstractions provided by varying domain-specific and parallel programming libraries, thereby allowing the development of specially customized library-aware compilers that can automatically coordinate the uses of library abstractions to maximize the overall performance of large scale multiprocessor applications. Automated optimization tuning support is provided to support the performance portability of applications on modern heterogeneous computing platforms.<br/><br/>The deliverables of this research include a collection of specialized compiler optimizers, distributed open source online, with associated auto-tuning toolkits to target them for varying modern multi-core and GPU platforms, and with a graphical user interface for users to interactively invoke these optimizers.  These optimizers, together with their interactive configuration interfaces, are expected to fundamentally change how high performance computing applications are developed, while providing computational specialists a toolset to automatically generate optimized library kernels without manually composing assembly codes."
377,1422925,NeTS: Small: Big Data and Optical Lightpaths-Driven Software Defined Networking,CNS,"CCRI-CISE Cmnty Rsrch Infrstrc, Networking Technology and Syst",10/1/14,9/2/14,T. S. Eugene Ng,TX,William Marsh Rice University,Standard Grant,Ann Von Lehmen,9/30/19,"$443,431.00 ",,eugeneng@rice.edu,6100 MAIN ST,Houston,TX,770051827,7133484820,CSE,"7359, 7363","7363, 7923",$0.00 ,"Science and engineering are increasingly relying on data and the ability to process a massive amount of data to solve hard problems and drive fundamental discoveries and innovations. Challenges arising from this trend are often referred to as ""Big Data"" problems. Examples of big data processing applications include seismic data analysis, data-intensive text processing, assembly of large genomes, machine learning, data mining, and social-network analysis. <br/><br/>This project will investigate new directions in software defined networking (SDN) that are motivated by the networking challenges stemming from big data processing applications and by the potential benefits of using optical lightpaths for big data transport.  The project will develop effective solutions for jointly configuring a rich set of optical devices and SDN switches to realize network services that meet the needs of big data applications. Specifically, the project will develop optical device resource allocation algorithms, topology design and routing algorithms, comparisons between greedy and guaranteed resource allocation policies, co-scheduling systems for traffic and network, techniques for data shuffle transmissions, and co-designed application and network controllers. The approaches, algorithms, and software developed by this project will be evaluated in a realistic experimental infrastructure called BOLD.  <br/><br/>The project may have far reaching societal impacts beyond the computing discipline. Results from the project can dramatically speed up a wide range of computational scientific discoveries. Optical networking devices consume very little power, yet can support enormous data rates; the project results could lead to a more environmentally sustainable future for the IT industry. The project activities will provide exciting opportunities for training and education of undergraduate and graduate students, and particularly under-represented minority students, in cutting edge big data-driven networking. Finally, software, data, and curriculum materials produced by this project will be disseminated as free open-source resources for the wider community's use."
378,1445606,Bridges: From Communities and Data to Workflows and Insight,OAC,"XD-Extreme Digital, Innovative HPC, Data Cyberinfrastructure",12/1/14,7/17/20,Nicholas Nystrom,PA,Carnegie-Mellon University,Cooperative Agreement,Robert Chadduck,11/30/20,"$20,895,167.00 ","Michael Levine, Ralph Roskies, JRay Scott, John Urbanic, Paola Buitrago, Jason Sommerfield",nystrom@psc.edu,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,CSE,"7476, 7619, 7726",7433,$0.00 ,"1.  Abstract: Nontechnical Description<br/><br/>The Pittsburgh Supercomputing Center (PSC) will provide an innovative and groundbreaking high-performance computing (HPC) and data-analytic system, Bridges, which will integrate advanced memory technologies to empower new communities, bring desktop convenience to HPC, connect to campuses, and intuitively express data-intensive workflows.<br/><br/>To meet the requirements of nontraditional HPC communities, Bridges will emphasize memory, usability, Title and effective data management, leveraging innovative new technologies to transparently benefit applications and lower the barrier to entry.<br/>Three tiers of processing nodes with shared memory ranging from 128GB to 12TB will address an extremely broad range of user needs including interactivity, workflows, long-running jobs, virtualization, and high capacity. Flexible node allocation will enable interactive use for debugging, analytics, and visualization. Bridges will also include a shared flash memory device to accelerate Hadoop and databases. <br/><br/>Bridges will host a variety of popular gateways and portals through which users will easily be able to access its resources. Its many nodes will allow long-running jobs, flexible access to interactive use (for example, for debugging, analytics, and visualization, and access to nodes with more memory. Bridges will host a broad spectrum of application software, and its familiar operating system and programming environment will support high-productivity programming languages and development tools.<br/><br/>Bridges will address data management at all levels. Its shared Project File System, connected to processing nodes by a very capable, appropriately scaled fabric, will provide high-bandwidth, low-latency access to large datasets. Storage on each node will provide local filesystem space that is frequently requested by users and will prevent congestion to the shared filesystem. A set of nodes will be optimized for and dedicated to running databases to support gateways, workflows, and applications. Dedicated web server nodes will enable distributed workflows.<br/><br/>Bridges will introduce powerful new CPUs and GPUs, and a new interconnection fabric to connect them. These new technologies will be supported by extremely broad set of applications, libraries, and easy-to-use programming languages and tools.<br/>Bridges will interoperate with and complement other NSF Advanced Cyberinfrastructure resources and large scientific instruments such as telescopes and high-throughput genome sequencers, and it will provide convenient bridging to campuses.<br/><br/>Bridges will enable important advances for science and society. By supporting pioneers who set examples in fields not traditionally users of HPC, and by lowering the barrier of entry, this project will spur others to recognize the power of the technology and transform their fields, as has happened in traditional HPC fields such as physics and chemistry. The project will engage students in research and systems internships, develop and offer training to novices and experts, extend the impact of the new system to minority schools and EPSCoR states, impact the undergraduate and graduate curriculum at many universities, raise the level of computational awareness at four-year colleges, and support the introduction of computational thinking into high schools.<br/><br/>2. Abstract: Technical Description<br/><br/>The Pittsburgh Supercomputing Center will substantially increase the scientific output of a large community of scientific and engineering researchers who have not traditionally used high-performance computing (HPC) resources. This will be accomplished by the acquisition, deployment, and management of Bridges, a HPC system designed for extreme flexibility, functionality, and usability. Bridges will be supported by operations, user service, and networking staff attuned to the needs of these new user communities, and it will offer a wide range of software appropriate for nontraditional HPC research communities. Users will be able to access Bridges through a variety of popular gateways and portals, and the system will provide development tools for gateway building.<br/> <br/>Innovative capabilities to be introduced by Bridges are:<br/><br/>  1.  Three tiers of processing nodes will offer 128GB, 3TB, and 12TB of hardware-supported, coherent shared memory per node to address an extremely broad range of user needs including interactivity, workflows, long-running jobs, virtualization, and high capacity. The 12TB nodes, featuring a proprietary, high-bandwidth internal communication fabric, will be particularly valuable for genome sequence assembly, graph analytics, and machine learning. Bridges will also include a shared flash memory device to accelerate Hadoop and databases. Flexible node allocation will enable interactive use for debugging, analytics, and visualization.<br/><br/>  2.  Bridges will provide integrated, full-time relational and NoSQL databases to support metadata, data management and efficient organization, gateways, and workflows. Database nodes will include SSDs for high IOPs and will be allocated through an extension to the XRAC process. Dedicated web server nodes with high-bandwidth connections to the national cyberinfrastructure will enable distributed workflows. The system topology will provide balanced bandwidth for nontraditional HPC workloads and data-intensive computing.<br/><br/>  3.  Bridges will introduce powerful new CPUs (Intel Haswell and Broadwell), GPUs (NVIDIA GK210 and GP100), the innovative, high-performance Intel Omni Scale Fabric to support increasingly productive development of advanced applications, supported by an extremely broad set of applications, libraries, and easy-to-use programming languages and tools such as OpenACC, parallel MATLAB, Python, and R.<br/><br/>  4.  A shared Project File System (PFS) will provide high-bandwidth, low-latency access to large datasets. Each node will also provide distributed, high-performance storage to support many emerging applications, intermediate and temporary storage, and reduce congestion on the shared PFS.<br/><br/>Bridges will enable important advances for science and society. By supporting pioneers who set examples in fields not traditionally users of HPC, and by lowering the barrier of entry, this project will spur others to recognize the power of the technology and transform their fields, as has happened in traditional HPC fields such as physics and chemistry. The project will engage students in research and systems internships, develop and offer training to novices and experts, extend the impact of the new system to minority schools and EPSCoR states, impact the undergraduate and graduate curriculum at many universities, raise the level of computational awareness at four-year colleges, and support the introduction of computational thinking into high schools."
379,1350941,CAREER: Carbon Footprint Modeling and Elastic Caching for Greening Services,CNS,CSR-Computer Systems Research,1/1/14,3/12/18,Christopher Stewart,OH,Ohio State University,Continuing Grant,Marilyn McClure,12/31/19,"$462,813.00 ",,cstewart@cse.ohio-state.edu,Office of Sponsored Programs,Columbus,OH,432101016,6146888735,CSE,7354,1045,$0.00 ,"As Internet services scale, their growing energy and carbon footprints present conflicting challenges.  On one hand, services must keep energy costs low.  On the other, they should make costly investments to undo environmental harm caused by their energy footprint.  Carbon offsets are transferable certificates that undo greenhouse gas emission, even when offset holders are located in carbon-heavy regions, making them attractive to large-scale, geographically distributed services.  This research project lays the foundation for greening services, a new class of Internet services that buy carbon offsets for user requests routed through their servers (i.e., a service that makes other services green).  People who use popular large-scale services could undo their carbon footprint by simply routing their requests through a greening service.  The greening service would manage costs.<br/><br/>The intellectual challenge for greening services is to model or confirm carbon footprints for servers outside of their control.  The key insight is that emerging trends within cloud computing, e.g., energy-efficient servers, auto scaling, and open source software, provide uniformity.  Dissimilarity between services, in terms of response times and energy footprints, is increasingly due to service-specific features.  We use black-box machine learning approaches to infer these features.<br/><br/>Beyond greening services, the proposed research will help system managers identify performance bugs, especially costly bugs that shift energy consumption toward datacenters with high energy costs.  As part of the proposed research, the PI will conduct outreach to underserved institutions and to local, Columbus, OH, area high schools."
380,1409836,CIF: Medium: Collaborative Research: Estimating simultaneously structured models: from phase retrieval to network coding,CCF,Comm & Information Foundations,8/15/14,6/5/17,Maryam Fazel,WA,University of Washington,Continuing Grant,Phillip Regalia,7/31/19,"$500,005.00 ",,mfazel@uw.edu,4333 Brooklyn Ave NE,Seattle,WA,981950001,2065434043,CSE,7797,"7924, 7935",$0.00 ,"In modern data-intensive science and engineering, researchers are faced with estimating models where available observations are far fewer than the dimension of the model to be estimated. The key to the success of compressed sensing, matrix completion, and other problems of this type, is to properly exploit knowledge about the ""structure"" of the model. While structures such as sparsity have been separately studied, the problem of ""simultaneous structures"" has been neglected, since it is implicitly assumed by practitioners that simply combining known results for each structure would solve the joint problem. Interestingly, the PIs recently proved that this approach can result in a significant gap.<br/><br/>This proposal will develop theory and computationally tractable methods for estimating simultaneously structured models with minimal observations. It combines (1) a top-down approach to understand the fundamental limitations based on the geometry of how structures interact, and (2) a problem-specific, bottom-up approach to exploit domain knowledge in constructing appropriate penalties. This work addresses a variety of applications including (1) sparse principal component analysis, a central problem in statistics seeking approximate but sparse eigenvectors, (2) sparse phase retrieval and quadratic compressed sensing in signal processing, and (3) code design for communications and network coding.<br/><br/>The ability to systematically derive structured models from data will have far-reaching impact on engineering challenges in the era of Big Data and ubiquitous computing. Handling models with multiple structures poses deep theoretical and computational challenges that this proposal focuses on. Applications in machine learning, signal processing, and network coding are discussed. The PIs will incorporate research results in their teaching, organize technical workshops to bring together mathematicians and engineers, and seek the involvement of undergraduate students in this work through summer research programs."
381,1332516,Implementation Project: Transforming Computational STEM Education at Claflin University,HRD,"Hist Black Colleges and Univ, Cross-BIO Activities",6/1/14,9/16/19,Verlie Tisdale,SC,Claflin University,Continuing Grant,Claudia Rankins,5/31/21,"$2,020,000.00 ","Arezue Boroujerdi, Brent Munsell, Rajagopalan Bhaskaran, Ananda Mondal",vtisdale@claflin.edu,400 Magnolia Street,Orangeburg,SC,291154498,8035355540,EHR,"1594, 7275","041Z, 9150, 9178",$0.00 ,"Implementation Projects in the Historically Black Colleges and Universities - Undergraduate program provide support to design, implement, study and assess comprehensive institutional efforts to increase the numbers of students and the quality of their preparation by strengthening science, technology, engineering and mathematics (STEM) education and research. This implementation project at Claflin University focuses on the disciplines of biology, chemistry, computer science and mathematics to provide an array of computational technologies in order to support and educate the next-generation of data scientists who can work in a multi-disciplinary environment.  Claflin University has a proven record of graduating a significant number of African-American students in STEM fields and through this implementation project that number is projected to increase. The project is guided and informed by an on-going evaluation.<br/><br/>The goals of the proposed project are to increase graduation and placement rates of students in the STEM workforce; to infuse research and education in a STEM-based computational curriculum; to promote computational science research to improve the quality of experiences and prepare STEM students to compete in a global society; and to use the strengths and assets of core partners to propel students towards a STEM career. The project will create relationships with federal and national laboratories, industry and other universities. The project involves faculty and undergraduate students to conduct research in the areas of metabolites and biomolecules using analytical instrumentation and methodology; in methods for retrieving and analyzing biological data; as well as in optimization, computational complexity, statistical modeling and machine learning."
382,1422078,TWC: TTP Option: Small: Collaborative: Integrated Smart Grid Analytics for Anomaly Detection,CNS,"Special Projects - CNS, Secure &Trustworthy Cyberspace",10/1/14,5/29/15,William Adams,MI,Regents of the University of Michigan - Ann Arbor,Standard Grant,David Corman,9/30/18,"$444,023.00 ","George Michailidis, Michael Kallitsis",wjadams@umich.edu,3003 South State St. Room 1062,Ann Arbor,MI,481091274,7347636438,CSE,"1714, 8060","7434, 7923",$0.00 ,"The modernized electric grid, the Smart Grid, integrates two-way communication technologies across power generation, transmission and distribution, in order to deliver electricity efficiently, securely and cost-effectively. On the monitoring and control side, it employs real-time monitoring offered by a messaging-based advanced metering infrastructure (AMI), which ensures the grid's stability and reliability, as well as the efficient implementation of demand response schemes to mitigate bursts demand. The efficient implementation of these features presents a number of challenges, but also opportunities for technology development in engineering, networking, and data analytics.<br/><br/>The intellectual contributions include the development of a framework that encompasses fast signal processing and machine learning algorithms, together with multi-sensor information to assess the health of the network. Specifically, the study will investigate algorithms for adaptive detection over streaming, high-dimensional and potentially missing value data. Furthermore, the project, via industry collaborators and power provisioners, will provide a comprehensive empirical evaluation with real-world data; this includes an open-source proof-of-concept prototype for quickly inferring nefarious activity in home-area networks, and a cloud-based testbed for examining realistic scenarios in a wide-area setting."
383,1442493,CyberSEES: Type 2: Collaborative Research: Tenable Power Distribution Networks,CCF,CyberSEES,9/1/14,8/11/14,George Michailidis,MI,Regents of the University of Michigan - Ann Arbor,Standard Grant,Phillip Regalia,4/30/15,"$244,764.00 ",,gmichail@ufl.edu,3003 South State St. Room 1062,Ann Arbor,MI,481091274,7347636438,CSE,8211,8208,$0.00 ,"This project advances modeling and computational frameworks to guarantee the sustainability of power distribution networks from environmental, economic, and social perspectives. Environmental sustainability is addressed by accounting for the increased uncertainty associated with the rapid and ad-hoc integration of renewable generation and elastic loads. With regard to economic sustainability, computational and inference foundations are put forth to ensure effective and secure market operations in the face of the imminent emergence of large-scale distribution-level electricity markets. Lastly, social sustainability is effected through cyber innovations focused on increasing economic utility and enhancing cyber security. <br/><br/>The proposed research aims for broad socio-technical advances in electricity distribution networks. Successful project completion will offer cyber innovations that enable the systematic integration of stochastic renewable generation while improving end-user satisfaction. Given the ubiquity of the suite of research tools and methodologies, the utility of the proposed research goes well beyond the envisioned application area to the broader fields of optimization, stochastic processes, control systems, machine learning, statistical signal processing, and cyber security. Broader transformative impact will result from pragmatic test cases proposed for validation, involvement of undergraduates in research, and outreach activities."
384,1438996,XPS: FULL: CCA:  Scalable Approximate Computing for Data Parallel Applications,CCF,Information Technology Researc,8/1/14,7/17/14,Scott Mahlke,MI,Regents of the University of Michigan - Ann Arbor,Standard Grant,Yuanyuan Yang,7/31/18,"$850,000.00 ","Zhuoqing Mao, Jason Mars, Lingjia Tang",mahlke@eecs.umich.edu,3003 South State St. Room 1062,Ann Arbor,MI,481091274,7347636438,CSE,1640,,$0.00 ,"Data parallel applications are an important class of problems where energy consumption is a barrier to future performance scaling.  These applications provide a paradigm where thousands of threads are concurrently executed on parallel hardware.  A key characteristic shared by many of these applications is that absolute correctness of the output is not essential for proper operation.  This opens up a new design dimension for microprocessors to trade off performance and energy consumption with output correctness.  Image and video processing applications are well known candidates for approximation as users can tolerate occasional dropped frames or small losses in resolution during video playback.  Machine learning and data analysis on massive data sets provide opportunities to process subsets of input data in a fraction of the time, while still yielding representative results.  This project will design and develop the enabling application analysis, compiler, and run-time software technologies to facilitate scalable approximate computing.  Our goal is to increase the execution efficiency of data-parallel applications by an order of magnitude on commodity hardware by trading off small, user-controlled levels of output accuracy for increased efficiency.  By creating enabling technologies, we expect approximate computing to become pervasive, enabling data-intensive computing to be integrated into more aspects of life.  Outreach activities will introduce data-parallel computing and energy efficiency to a new generation of high school students through an effort to reach out to local high schools.<br/><br/>This project uses a vertically integrated approach that combines deep application analysis, automatic generation of approximate kernels, and a run-time system that seamlessly manages approximate execution.  First, deep analysis of compute-intensive mobile and datacenter applications will automatically identify code regions that are amenable to approximation.  Second, an idiom recognition and replacement approach is used to identify common computation patterns and synthesize approximate versions with varying degrees of accuracy.  Third, sampling-based approaches as well as predictive strategies that perform approximate checking will be explored to ensure that output quality degradation does not exceed a user-specified threshold.  Finally, a run-time compiler and management layer orchestrates the usage of approximate kernels and error checking while ensuring user error thresholds are honored."
385,1442686,CyberSEES: Type 2: Collaborative Research: Tenable Power Distribution Networks,CCF,CyberSEES,9/1/14,5/16/19,Georgios Giannakis,MN,University of Minnesota-Twin Cities,Standard Grant,Phillip Regalia,11/30/19,"$620,202.00 ",Sairaj Dhople,georgios@umn.edu,200 OAK ST SE,Minneapolis,MN,554552070,6126245599,CSE,8211,8208,$0.00 ,"This project advances modeling and computational frameworks to guarantee the sustainability of power distribution networks from environmental, economic, and social perspectives. Environmental sustainability is addressed by accounting for the increased uncertainty associated with the rapid and ad-hoc integration of renewable generation and elastic loads. With regard to economic sustainability, computational and inference foundations are put forth to ensure effective and secure market operations in the face of the imminent emergence of large-scale distribution-level electricity markets. Lastly, social sustainability is effected through cyber innovations focused on increasing economic utility and enhancing cyber security. <br/><br/>The proposed research aims for broad socio-technical advances in electricity distribution networks. Successful project completion will offer cyber innovations that enable the systematic integration of stochastic renewable generation while improving end-user satisfaction. Given the ubiquity of the suite of research tools and methodologies, the utility of the proposed research goes well beyond the envisioned application area to the broader fields of optimization, stochastic processes, control systems, machine learning, statistical signal processing, and cyber security. Broader transformative impact will result from pragmatic test cases proposed for validation, involvement of undergraduates in research, and outreach activities."
386,1354041,Collaborative Research:  Integral Projection Models for Populations in Varying Environments:  Construction and Analysis,DEB,"POP & COMMUNITY ECOL PROG, MATHEMATICAL BIOLOGY",6/1/14,5/30/14,Robin Snyder,OH,Case Western Reserve University,Standard Grant,Douglas Levey,3/31/19,"$102,847.00 ",,res29@case.edu,"Nord Hall, Suite 615",CLEVELAND,OH,441064901,2163684510,BIO,"1182, 7334","8007, 9169, EGCH",$0.00 ,"All environments are variable and uncertain: some years are hotter, some wetter; predators wax and wane in abundance. How organisms buffer themselves against this variability, and exploit it, are major challenges for ecology. This project will combine new statistical and mathematical theory with long-term data sets on semi-arid plant communities to address two challenges in understanding how variable environments affect populations. The first goal will be to identify which environmental variables most strongly affect plant demographic rates such as survival, growth, and fecundity. This work will draw upon long-term studies on natural communities that provide observations on thousands of individual plants over multiple decades, along with numerous environmental variables that are measured at high spatial and temporal resolution. With so many potential explanatory variables, standard statistical methods for variable selection are unstable and unreliable. This project will combine data mining methods from machine learning techniques with traditional statistical theory to identify important environmental drivers, and build more reliable demographic models. Ecological questions that will be addressed include whether demographic rates respond primarily to resource availability (e.g., soil moisture) versus non-resource variables such as temperature; how environment and competition interact; and how strongly past conditions affect current performance. The empirical data sets will be used to answer these questions and look for generalities across multiple communities. A second goal is to understand the individual mechanisms that underlie effects of environmental variation. For example, if a plant population's growth is most affected by rainfall, is it because many plants die immediately, or because of long-term effects such as decreased life expectancy and smaller size throughout life? To address such questions, statistical methods from discrete-state random process theory will be extended to continuous states (e.g., individual plant size) and varying environments, and applied to the fitted plant demographic models. <br/><br/>An expected transformative outcome from this project will be new methods for demographic modeling and analysis, and much of its broader significance will be the applications of those tools in ecology, conservation biology and invasive species management.  Statistical products from this work will be disseminated freely as R code modules that users can adapt to their own study systems. The project will also support the research training and mentoring of a doctoral student in statistics, and a postdoctoral student in quantitative ecology."
387,1456172,I-Corps:  Exploring the Commercialization Potential of ChatCoder,IIP,I-Corps,9/1/14,2/29/16,April Edwards,PA,Ursinus College,Standard Grant,Steven Konsek,9/30/16,"$50,000.00 ",,e2unlimitedtech@gmail.com,601 E. Main Street,Collegeville,PA,194260000,6104093005,ENG,8023,,$0.00 ,"Current parental control software does not provide parents and kids with the one tool they desperately want and need ? a method for halting cyber-aggression in progress, rather than reporting the event after it occurs. Although most cyberbullying and cyber-predation acts occur over an extended period of time, current programs can only detect acts based on keywords in text; they don?t offer tools to actually stop the abuse and they don?t capture context. Concern about cyberbullying and cyber-predation among parents and authorities has led to a booming parental control software market and to the passage of anti-cyberbullying laws in many states. The proposed innovation offers better recognition of cyber-abuse and as well as response capabilities; thereby providing a more dynamic, interactive, and empowering resource for youths and their parents.<br/><br/>The team has developed machine learning algorithms that are able to detect approximately 80% of cyberbullying communication, and 84% of predatory conversation. These algorithms were developed using data that was collected and labeled for research purposes. The algorithms need to be adapted for real-time communication, as well as continuously updated as additional data become available. The theoretical model for victim response is in development. The model identifies the type, context and severity of bullying or predation, along with categories of potential responses from which victims can choose. The team has completed the proof-of-concept and is ready to move forward with the development of a software prototype."
388,1447795,BIGDATA: F: DKM: Addressing the two V's of Veracity and Variety in Big Data,IIS,"Info Integration & Informatics, Data Cyberinfrastructure, Big Data Science &Engineering",9/1/14,8/3/16,Nitesh Chawla,IN,University of Notre Dame,Standard Grant,Sylvia Spengler,8/31/20,"$1,000,000.00 ","Dong Wang, Thanuka Wickramarathne",nchawla@nd.edu,940 Grace Hall,NOTRE DAME,IN,465565708,5746317432,CSE,"7364, 7726, 8083","7433, 8083",$0.00 ,"Data of questionable quality have led to significantly negative economic and social impacts on organizations, leading to overrun in costs, lost revenue, and decreased efficiencies. The issues on data reliability, credibility, and provenance have become even more daunting when dealing with the variety of data, especially data that are not directly collected by an organization, but from the third-party sources such as social media, data brokers, and crowdsourcing. To address such issues, this project aims to develop a Data Valuation Engine (DVE) that solves the critical problem of data reliability, credibility and provenance, and provides accountability and quality processes right from data acquisition. The DVE leverages and innovates techniques in estimation theory, data fusion and machine learning to fill a critical gap in data accountability and quality, thereby providing a transformative step in countering the ubiquitous data quality issues found in almost every application domain from business to environment to health to national security. The DVE will be integrated in the Hadoop ecosystem and will be agnostic to the data source, application or analytics, and provided as a hosted solution to the community.  The user will interact with DVE by providing the data sources and relevant data necessary to solve a problem. <br/><br/><br/>The DVE in this project will be developed in a largely application-independent manner. The key challenges to develop this engine include: (i) How to generate the data quality indication labels to score data sources and the content of data based on various factors such as reliability, credibility, uncertainty and confidence? (ii) How to integrate data from various sources with different labeled scores? (iii) How to robustly evaluate the proposed engine in a broad spectrum of applications that serve as a proxy of a variety of real-world scenarios? The research plan has been designed to synergistically address the above challenges with a robust evaluation plan. Given the generality of the proposed methods, models and system, the project will potentially impact variety of applications of science, engineering, and social science and have broad environmental, economic, and health benefits. The PIs will release open source software and applicable data. The PIs will also provide a hosted DVE platform for a broad user and participant base. This project is also providing students with greater exposure to the areas of big data analytics, cloud computing, data fusion and data mining, both in courses and research experiences."
389,1421407,CSR: Small: Behavior Based User Authentication for Mobile Devices,CNS,"CSR-Computer Systems Research, Secure &Trustworthy Cyberspace",8/15/14,8/6/14,Alex Liu,MI,Michigan State University,Standard Grant,Marilyn McClure,7/31/18,"$500,000.00 ",,alexliu@cse.msu.edu,Office of Sponsored Programs,East Lansing,MI,488242600,5173555040,CSE,"7354, 8060","7434, 7923",$0.00 ,"Mobile devices equipped with touch screens have increasingly rich functionality, enhanced computing power, and greater storage capacity. These devices often contain private information such as personal photos, emails, and even corporate data. Therefore, it is crucial to have secure yet convenient user authentication mechanisms for touch screen devices. However, the widely used password/PIN/pattern based solutions are susceptible to shoulder surfing (as mobile devices are often used in public settings where shoulder surfing often happens either purposely or inadvertently) and smudge attacks (as oily residues left by fingers on touch screens can be recognized by impostors) and are sometimes inconvenient for users to input when they are walking or driving.<br/><br/>The goal of this project is to develop a behavior based user authentication approach for touch screen devices. Rather than authenticating users solely based on what they input (such as a password/PIN/pattern), Behavioral Authentication is based upon how users provide input input. Specifically, a user is first asked to perform certain actions, such as gestures/signatures, on touch screens and then the behavior feature information (such as velocity magnitude and device acceleration) is extracted from the actions to authenticate the user based on machine learning techniques. The intuition behind the proposed approach is that people have consistent and distinguishing behavior of performing gestures and signatures on touch screens. Compared with current user authentication schemes for touch screen devices, the proposed approach is significantly more difficult to compromise because it is nearly impossible for impostors to reproduce the behavior of others doing gestures/signatures through shoulder surfing or smudge attacks - they can see it, but they cannot do it.<br/><br/>This project will advance the knowledge and understanding of behavior based user authentication on touch screen devices. This is potentially transformative research with high-impact. If successful, this project will not only yield a theoretical foundation for behavior based user authentication on touch screen devices but also invite future research along this direction."
390,1359301,REU Site: Next-Generation Bioinformatics for Genomics-enabled Research in the Life Sciences,DBI,"RSCH EXPER FOR UNDERGRAD SITES, EPSCoR Co-Funding",10/1/14,6/28/14,Paul Anderson,SC,College of Charleston,Standard Grant,Sally O'Connor,3/31/18,"$334,662.00 ",Andrew Shedlock,andersonpe2@cofc.edu,66 GEORGE ST,CHARLESTON,SC,294240001,8439534973,BIO,"1139, 9150","9150, 9250",$0.00 ,"This REU Site award to the College of Charleston, Charleston, SC, will support the training of 10 students in a 10-week program during the summers of 2015-2017. The REU program is open to all undergraduate students who are citizens, nationals or permanent residents of the United States. Students trained in the program will gain skills in lab research, develop their critical thinking and problem-solving skills, understand the process of science, and be able to communicate their research results to their peers and the general public. Students will have an opportunity to present their results at a national conference. The REU program provides an experience to students that is typically not available to them at their home institutions. Students from schools with limited opportunities for research and from underrepresented groups are encouraged to apply.<br/><br/>This REU Site program will focus on integrative research that spans the areas of genomics, bioinformatics, data mining, data science, molecular biology and evolution.  Example interdisciplinary projects include the application of next-generation DNA sequencing data to understand and investigate genomes through the development and application of computational tools and algorithms from fields such as data mining, machine learning, and data science. Teams of both biologists and computer scientists will employ novel and existing algorithms for integrating molecular and organismal biology with studies of ecology, behavior, and evolution. <br/><br/>A common web-based assessment tool used by all REU programs funded by the Division of Biological Infrastructure (Directorate for Biological Sciences) will be used to determine the effectiveness of the training program. Students are required to be tracked after the program and must respond to an automatic email sent via the NSF reporting system. More information is available by contacting the PI (Dr. Paul Anderson at andersonpe2@cofc.edu) or co-PI (Dr. Andrew M. Shedlock at shedlockam@cofc.edu)."
391,1421896,SBE: Small: An Analysis of the Relationship Between Cyberaggression and Self-Disclosure among Diverse Youths,SES,Secure &Trustworthy Cyberspace,9/1/14,8/8/14,April Edwards,PA,Ursinus College,Standard Grant,Sara Kiesler,3/31/18,"$349,160.00 ",Lynne Edwards,e2unlimitedtech@gmail.com,601 E. Main Street,Collegeville,PA,194260000,6104093005,SBE,8060,"7434, 7923, 8239, 9102, 9178, 9229, 9251, SMET",$0.00 ,"Youths of the digital age live parallel lives online and in the real world, frequently disclosing personal information to cyberfriends and strangers, regardless of race, class or gender. Race and gender do make a difference, however, when these online disclosures lead to acts of cyberaggression. The PIs' previous work revealed that some youths are resistant to cyberaggression and that there are differences in perceptions of cyberbullying among youths from different cultural and racial backgrounds.  This research aims to explore the relationship between youths' self-disclosures, cultural backgrounds, and their perceptions of cyberaggression.<br/><br/>The PIs conduct a longitudinal, interdisciplinary study that builds upon their ongoing cyberaggression pattern recognition research by: 1) using surveys and focus groups to test and refine their theories about self-disclosure, perception, cultural difference, and cyberaggression communication patterns, 2) using machine learning to develop detection and response technologies for use in applications designed to protect youths, 3) using focus groups to evaluate the applications, and 4) making the data collected from this project available to the research community. This work is important to understand the role of self-disclosure in cybervictmization among youths, and provides the theoretical groundwork for the development of effective response strategies that can be employed by youths when they are attacked online. The data from this study will provide a rich source of material for other researchers in both computer science and in the social and behavior sciences."
392,1447634,BIGDATA: F: DKA: CSD: Iterative Crowdsourced Hypothesis Generation,IIS,"OFFICE OF MULTIDISCIPLINARY AC, Big Data Science &Engineering",9/15/14,9/4/14,James Bagrow,VT,University of Vermont & State Agricultural College,Continuing Grant,Sara Kiesler,8/31/20,"$599,937.00 ","Joshua Bongard, Peter Dodds, Paul Hines, Christopher Danforth",james.bagrow@uvm.edu,85 South Prospect Street,Burlington,VT,54050160,8026563660,CSE,"1253, 8083","7433, 8083, 8251, 9150",$0.00 ,"Establishing causal relationships -- for example, that cigarette smoking causes lung cancer -- is one of the most challenging aspects of scientific research. Computers excel at calculation, but are unable to separate cause-and-effect from mere correlation. Humans, on the other hand, can make logical conclusions based on their experiences but, in the modern era of Big Data, there are far too many potential relationships for humans to manually examine. This research aims to build a crowdsourcing web platform to use the knowledge of interested non-experts (Hunch) and the algorithmic power of computers (Crunch) to discover and test causal relationships in large-scale data. Algorithms identify potential relationships and users are asked to validate them. Further, users are able to propose their own hypotheses that can subsequently be validated, creating an accelerating feedback loop of scientific discovery. The goal of systematically discovering causal relationships has the potential for broad societal impact, and virtually anyone with web access can participate directly in this scientific research.<br/><br/>To support this goal, the researchers are developing novel statistical methods that determine the data types of crowd-suggested observables on the fly. For example, are 'wages' and 'gender' real-valued or binary variables? Finally, the crowd is a relatively limited resource. To use it efficiently, machine learning algorithms would identify which substructures in the correlational network are most likely to be causal, and then focus the crowd's efforts towards them. These efficient, adaptive methods allow causal relationships to be combined into larger chains that explain growing numbers of causes and effects."
393,1407470,NeTS: Medium: Collaborative Research: An App-Centric Transport Architecture for the Internet,CNS,Networking Technology and Syst,9/1/14,9/20/19,Hari Balakrishnan,MA,Massachusetts Institute of Technology,Continuing Grant,Deepankar Medhi,8/31/20,"$800,000.00 ",,hari@csail.mit.edu,77 MASSACHUSETTS AVE,Cambridge,MA,21394301,6172531000,CSE,7363,7924,$0.00 ,"The Transmission Control Protocol and its socket interface are used by most network applications today---viewed as a software program, it is likely the most popular one in the world. Over thirty years old, this program is now showing signs of age. Web, mobile, and embedded applications use it in unanticipated ways (many interacting flows of different lengths with different objectives); it does not properly support mobile devices with multiple networks; it does not allow an application to adapt quickly to changing network conditions; it does not work well in fast datacenter networks; and it has a fruitless, adversarial relationship with in-network ""middleboxes"".<br/><br/>App-Centric Transport (ACT) aims to solve these problems. Unlike today's transport architecture, ACT takes application objectives into account in making its transmission decisions.  ACT uses a computer-generated transmission control program, which takes models of the workload and objectives to automatically synthesize protocols.  This program (Remy) serves both as a way to implement protocols and as a design tool to answer fundamental questions about congestion control. ACT explores centralized control over transmission and path selection for datacenters, and uses machine learning classifiers to select the best network on mobile devices. It aims to resolve tussles between endpoint transport and in-network middleboxes by viewing middlebox interposition as a generalization of multi-path transport. Project participants are collaborating with Facebook and Google on some of these ideas. The education plan introduces protocol design contests where students can run and measure their protocols over trace data and compare their protocols on a leaderboard."
394,1445386,EAGER: Ferroelectric Memristive Devices Emulating Synapses in Subcortical Information Processors,ECCS,"GOALI-Grnt Opp Acad Lia wIndus, EPMD-ElectrnPhoton&MagnDevices",7/1/14,5/1/15,Santosh Kurinec,NY,Rochester Institute of Tech,Standard Grant,Usha Varshney,6/30/17,"$165,108.00 ",Dhireesha Kudithipudi,skkemc@rit.edu,1 LOMB MEMORIAL DR,ROCHESTER,NY,146235603,5854757987,ENG,"1504, 1517","7916, 9251",$0.00 ,"Neuromorphic computing is an interdisciplinary field that aspires to create physical architecture and design principles based on biological nervous systems for applications in vision systems, auditory systems and autonomous robots.  There is an increasing interest in developing electronic analog circuits to mimic neuro-biological architectures present in the nervous system.  In the nervous system of the brain, a synapse is a structure that permits a neuron to pass an electrical or chemical signal to another cell. This exploratory research proposes to create a two-terminal memristive device that can emulate the function of a synapse. The resistance of the device will change depending on the amount, direction, and duration of voltage applied. The device has advantage of maintaining its state until another voltage pulse is applied over conventional computer memory, which requires regular charge to maintain its state.  The principle behind the proposed approach is to make dipoles in a film switch up or down depending on voltage polarity in ferroelectric materials. If the thickness of the ferroelectric layer is made small enough, it can allow tunneling of electrons that is a function of the relative density of dipoles in up or down position thus preserving a memory similar to that of the synapse, thereby making electronic analog circuits to mimic brain. The proposed novel synapse circuits will enable integration of complex systems with power-constrained devices. The research on hafnium oxide (HfO2) based FTJ will also open the door for further scaling of FE capacitor based random access memory and enable the fabrication of FE field effect transistor (FE-FET) based memory. The graduate students would have a significant opportunity in advancing their interdisciplinary skills in semiconductors device design and fabrication, integrated circuit design, machine learning, and neuroscience.  <br/><br/>This proposal explores a two-terminal memristive device based on newly discovered ferroelectricity in CMOS compatible high permittivity dielectric, HfO2 doped with silicon or aluminum.   The switching mechanism in FTJ is driven by polarization that is relatively immune from stochastic variations observed in other resistive memory devices. Fabrication of a high permittivity HfO2 based ferroelectric device will allow thinner films which can be scaled. The goal of the proposed research is to explore the design and fabrication of HfO2 based ferroelectric tunnel junction (FTJ) memristive device and its characterization to generate models. In addition, neuron circuits with multiple signaling types for behavioral emulation of biological neurons will be designed, synaptic circuits based on the proposed memristor device models will be trained, and the feasibility of incorporating the neuron and synapse circuits into subcortex-inspired information processing (SIIP) system will be evaluated. The outcome of the proposed exploratory research will result in a novel device that can mimic synaptic behavior and can be integrated with conventional CMOS electronics thereby forming the basis for the next generation of intelligent computing."
395,1422218,III: Small: Collaborative Research: Functional Network Discovery for Brain Connectivity,IIS,"CRCNS-Computation Neuroscience, Info Integration & Informatics",8/1/14,2/29/16,Ian Davidson,CA,University of California-Davis,Standard Grant,Sylvia Spengler,7/31/19,"$315,977.00 ",Owen Carmichael,davidson@cs.ucdavis.edu,OR/Sponsored Programs,Davis,CA,956186134,5307547700,CSE,"7327, 7364","7364, 7923, 8089, 8091, 9251",$0.00 ,"Neuroscience is at a moment in history where mapping the connectivity of the human brain non- invasively and in vivo has just begun with many unanswered questions.  While the anatomical structures in the brain have been well known for decades, how they are used in combination to form task specific networks has still not been completely explored. Understanding what these networks are, and how they develop, deteriorate, and vary across individuals will provide a range of benefits from disease diagnosis, to understanding the neural basis of creativity, and even in the very long term to brain augmentation. Though machine learning and data mining has made significant inroads into real world practical applications in industry and the sciences, most existing work focuses on lower-level tasks such as predicting labels, clustering and dimension reduction. This requires the practitioner to shoe-horn their more complex tasks, such as network discovery, into the algorithm's settings. <br/><br/>The focus of this grant is a transition to more complex higher-level discovery tasks and in particular, eliciting networks from spatio-temporal data represented as a tensor. Here the spatio-temporal data is an fMRI scan of a person represented as a four dimensional tensor with each entry in the tensor being a data point that indicates the brain activity at that time and location. The overall problem focus is to simplify this data into a cognitive network consisting of identifying active regions of the brains and the interactions that occur between them. The work will consist of three intertwined tasks as follows: i) Supervised and Semi-supervised Network Discovery, ii) Complex Network Discovery and iii) Network Discovery in Populations. In the supervised/semi-supervised setting, the networks discovered involves coordinated activity among some combination of anatomical structures Since all or some of the structures are given along with their boundaries, this is termed a supervised (or semi-supervised) problem. With complex network discovery the team will move beyond finding a single network of coordinated activity to finding multiple networks with complex (beyond coordinates) relationships between the structures/regions. Finally with network discovery in populations , the previous work that studies an individual scan will be expanded to a population of scans.  A population may be a collection of individuals performing the same task or a single individual's scans collected over time. Studying such populations allows addressing innovative questions such as: ""How does one individual's network change over the course of development, aging, or disease?"" and ""How do the networks differ for one group of individuals to that of another group?"""
396,1461138,Geometric Methods for Graph Partitioning,DMS,COMPUTATIONAL MATHEMATICS,7/1/14,9/4/14,Braxton Osting,UT,University of Utah,Standard Grant,Leland Jameson,7/31/16,"$79,000.00 ",,osting@math.utah.edu,75 S 2000 E,SALT LAKE CITY,UT,841128930,8015816903,MPS,1271,"9150, 9263",$0.00 ,"The proposed activity is to develop and analyze new computational methods for a graph partitioning problem based on the Beltrami energy. This problem has diverse applications in machine learning and image analysis (medical, satellite, and material). For example, a clear need for such methods in imaging has been identified by collaborators at the California NanoSystems Institute, where the proposed work will directly impact fundamental research in nano science and foster the understanding of amyloid beta sheets. Such amyloids are associated with the pathology of more than 20 serious human diseases including Alzheimer's and other neurodegenerative diseases. Thanks to the multidisciplinary nature of the activity, awareness and literacy outside one's field will also be mutually fostered for all involved parties.<br/><br/>The PIs, together with their students and collaborators, will seek new methods that combine variational arguments with ideas from geometry and partial differential equations in order to extend and overcome the limitations of existing methods. The research has three goals. The first goal concerns fundamental theoretical questions raised by the proposed model: analyze the existence, uniqueness, and properties of the minimizers of the variational problems; establish a generalized isoperimetric inequality related to the Beltrami functional in the continuum; and explore relations to existing theorems and conjectures about optimal partitions. A graph analogue of the Beltrami energy is formulated and is the foundation for a graph partitioning objective. The PIs have identified a relaxation of this objective and propose an in-depth study of a provably convergent rearrangement method for its solution. The second goal addresses the important computational and numerical aspects of the proposed graph partitioning model. An efficient optimization strategy is key for the framework to be usable in practical applications. Here, promising primal-dual methods from convex optimization will be utilized, as well as other competitive state-of-the-art methods. To develop the most efficient solution method, it will be crucial to explore the similarities to other models, including those for non-negative matrix factorization and those related to motion by mean curvature. The third goal is to address concrete, real-world problems and to engage the developed algorithms in practical applications of societal importance."
397,1409415,TWC: Medium: Collaborative: Aspire: Leveraging Automated Synthesis Technologies for Enhancing System Security,CNS,Secure &Trustworthy Cyberspace,8/1/14,8/5/14,Prateek Mittal,NJ,Princeton University,Standard Grant,Sol Greenspan,7/31/18,"$400,000.00 ",,pmittal@princeton.edu,Off. of Research & Proj. Admin.,Princeton,NJ,85442020,6092583090,CSE,8060,"7434, 7924",$0.00 ,"Designing secure systems and validating security of existing systems are hard challenges facing our society. For implementing secure applications, a serious stumbling block lies in the generation of a correct system specification for a security policy. It is non-trivial for both system designers and end users to express their intent in terms of formal logic. Similar challenges plague users' trying to validate security properties of existing applications, such as web or cloud based services, which often have no formal specifications. Thus, there is an urgent need for mechanisms that can bridge the gap between expressions of user intent and system specifications. This research designs an approach and a system called Aspire that is able to translate user intent into security specifications. <br/><br/>Aspire takes as input, expressions of user intent such as a system demonstration,  application input-output examples, or natural language. Aspire leverages recent  developments in the field of automated synthesis technologies that can consider such examples of user intent as input to the synthesis of security specifications. Aspire combines such inputs, along with a domain specific language for security applications, to synthesize a candidate set of possible outputs. The user can either choose a candidate output or provide more examples to guide the synthesis process. In this iterative fashion, the user can generate system specifications, policies, or properties. Aspire uses concepts from the domain of formal methods, machine learning, and programming languages to perform synthesis. Aspire is applicable to a variety of domains including web, mobile, and cloud applications. The output of Aspire's synthesis can either be used for analyzing security vulnerabilities, or for compilation and testing with real systems."
398,1407454,NeTS: Medium: Collaborative Research: An App-Centric Transport Architecture for the Internet,CNS,Networking Technology and Syst,9/1/14,6/1/17,Bryan Ford,CT,Yale University,Continuing Grant,Deepankar Medhi,8/31/19,"$400,000.00 ",,bryan.ford@yale.edu,Office of Sponsored Projects,New Haven,CT,65208327,2037854689,CSE,7363,7924,$0.00 ,"The Transmission Control Protocol and its socket interface are used by most network applications today---viewed as a software program, it is likely the most popular one in the world. Over thirty years old, this program is now showing signs of age. Web, mobile, and embedded applications use it in unanticipated ways (many interacting flows of different lengths with different objectives); it does not properly support mobile devices with multiple networks; it does not allow an application to adapt quickly to changing network conditions; it does not work well in fast datacenter networks; and it has a fruitless, adversarial relationship with in-network ""middleboxes"".<br/><br/>App-Centric Transport (ACT) aims to solve these problems. Unlike today's transport architecture, ACT takes application objectives into account in making its transmission decisions.  ACT uses a computer-generated transmission control program, which takes models of the workload and objectives to automatically synthesize protocols.  This program (Remy) serves both as a way to implement protocols and as a design tool to answer fundamental questions about congestion control. ACT explores centralized control over transmission and path selection for datacenters, and uses machine learning classifiers to select the best network on mobile devices. It aims to resolve tussles between endpoint transport and in-network middleboxes by viewing middlebox interposition as a generalization of multi-path transport. Project participants are collaborating with Facebook and Google on some of these ideas. The education plan introduces protocol design contests where students can run and measure their protocols over trace data and compare their protocols on a leaderboard."
399,1407400,"RUI:   Classification, regression, and density estimation with missing variables",DMS,STATISTICS,9/1/14,7/31/16,Majid Mojirsheibani,CA,"The University Corporation, Northridge",Continuing Grant,Gabor Szekely,8/31/18,"$119,999.00 ",,majid.mojirsheibani@csun.edu,18111 Nordhoff Street,Northridge,CA,913308309,8186771403,MPS,1269,9229,$0.00 ,"This project develops statistical theory and methods for nonparametric classification and curve estimation in the presence of missing or incomplete data. Many data sets have missing values; these include the data from biomedical studies, remote sensing, as well as social sciences. There are a number of classical approaches for handing the missing data.  Many of the existing results first impute for the missing values and then apply a standard statistical technique to carry out inferences.  However, a study of the theoretical validity of such techniques can become intractable due to the loss of independence assumption in the data; this is particularly true for distribution-free statistical methods. The new results of the Principal Investigator (PI) will answer a number of fundamental questions in statistical classification and pattern recognition with applications to biomedical, remote sensing, and social sciences.  The new results will also solve many important theoretical problems at the intersection of machine learning and statistical classification.<br/><br/>A long-standing problem in classification with missing covariates involves the situation where missing covariates can appear in both the data and in the new unclassified observation. This is fundamentally different from the simpler problem where missing covariates appear in the data only. In the latter case, standard methods based on Horvitz-Thompson inverse weighting can be used to construct asymptotically optimal classifiers. One part of the PI's research project focuses on this challenging case of classification with missing covariates. The PI will develop new asymptotically optimal local-averaging-type classifiers, such as kernel and partitioning rules.  Another part of this project concentrates on the continuation and refinements of the PI's previous efforts on combined classification and estimation, based on recently obtained results in the literature.  The PI is currently developing new methods to combine several individual classifiers in such a way that the asymptotic error of the resulting classifier will be at least as good as that of the best individual classifier.  The PI will also develop methods to combine several regression function estimators in an optimal way. Tools from the empirical process theory will be used to establish the large-sample optimality of the resulting classifiers and estimators. The third part of the project focuses on the weak convergence of various norms of kernel density estimates in the presence of missing data. The PI will study weighted bootstrap approximations of these statistics. Such results will allow someone to construct correct confidence bands for the unknown density in the presence of missing values. The main tools here are the strong approximation theorems that allow one to replace the weighted bootstrapped empirical processes by a sequence of Brownian bridges."
400,1404163,Robust Identification and Model Validation for a Class of Nonlinear Dynamic Systems and Applications,ECCS,EPCN-Energy-Power-Ctrl-Netwrks,8/15/14,8/6/14,Mario Sznaier,MA,Northeastern University,Standard Grant,Radhakisan Baheti,7/31/19,"$380,000.00 ","Octavia Camps, Ferdinand Hellweger",msznaier@coe.neu.edu,360 HUNTINGTON AVE,BOSTON,MA,21155005,6173733004,ENG,7607,092E,$0.00 ,"Robust Identification and Model Validation for a Class of Nonlinear Dynamic Systems and Applications<br/><br/>The project seeks to develop a comprehensive framework for obtaining data driven models for a class of nonlinear systems that arise in the context of a broad range of applications that entail extracting information from high volume data streams.  Obtaining these models is the first step towards developing a new class of systems with substantially enhanced capabilities to extract information sparsely encoded in multimodal, extremely large data sets. In particular, as a proof of concept, this project will focus on sustainable water quality management in urban environments, a problem that affects over one billion people in the Developing World and leads to losses estimated at over $2 billion/year in the US alone.  From an education standpoint, this theme will be used to link a full range of distinct undergraduate and graduate courses, from systems theory, environmental and hydrologic engineering to machine learning, with emphasis on robustness and computational complexity. Undergraduate students will be engaged in research through the OUR Charles (Opportunities for Undergraduate Research on the Charles River) program. <br/><br/><br/>While control of switched systems has made considerably progress in the past few years, the problem of identifying and validating hybrid models amenable to be used by these methods is far from solved. The present proposal aims at closing this gap by developing a computationally tractable framework for robust identification and model (in)validation of switched Hammerstein/Wiener systems. Its conceptual backbone is a combination of systems theory, semi-algebraic geometry and convex optimization elements that emphasizes robustness and computational complexity issues.  The main idea is to recast the identification and model (in)validation of switched systems into a sparse semi-algebraic optimization form and to exploit recent advances in convex optimization to develop  scalable, computationally tractable methods to solve these problems.<br/><br/>The advantages of the proposed approach include the ability to: (a) Address problems beyond the capabilities of existing techniques due to a combination of a lack of a comprehensive theoretical framework and poor scaling properties. <br/>(b) Directly accommodate and respect features that are key to success in the relevant application domains, such as sparse interconnection structures. (c) Exploit a hitherto largely unexplored connection between systems identification and the problem of extracting actionable information from very large data sets."
401,1407828,Statistical Methodology and Applications to Engineering and Economics,DMS,STATISTICS,8/15/14,6/9/17,Tze Lai,CA,Stanford University,Continuing Grant,Gabor Szekely,7/31/19,"$399,663.00 ",,lait@stanford.edu,450 Jane Stanford Way,Stanford,CA,943052004,6507232300,MPS,1269,,$0.00 ,"The past three years witnessed the beginning of a new era in financial markets and in the US health care system, following the health care and financial reform legislation in 2010. A long-term objective of the research is to develop innovative statistical methodologies and to combine them with advances in high-performance computing and communication networks for addressing the challenges in quantitative finance and health care in this new era. The research will lead to advances and innovations in statistical methods in biomedicine, economics and engineering, paving the way for timely applications to clinical and translational medical research, health care, homeland security, environmental change, and risk management. A broader impact of this research is the training of the next generation of scientists in academia, industry, and government, by involving graduate students in all phases of the research, and by developing new course material built around the research and its applications.<br/><br/>The research projects can be broadly divided into four areas. The first is multi-arm bandits with covariates, also called ""contextual bandits"" in machine learning, and their applications to personalized strategies in medicine and electronic business, in particular, to genomic-guided personalized cancer treatments and biomarker- guided treatments for depression in neuroscience. The second area is fault detection and surveillance in network models for manufacturing systems, for systemic risk in financial markets, for homeland security, and for environment or global change. The third area is the development of efficient adaptive particle filters in nonlinear state-space models that have far-ranging applications in engineering and economics, with robotics and stochastic adaptive control as the main focus in this research. The fourth area includes dynamic empirical Bayes modeling of joint default risk for multiple firms in credit markets, of loan loss risk in retail banking and of insurance claims, macroeconomic time series modeling and forecasting, and data analytics for health care cost and preventive management."
402,1353039,Collaborative Research:  Integral Projection Models for Populations in Varying Environments:  Construction and Analysis,DEB,"POP & COMMUNITY ECOL PROG, MSPA-INTERDISCIPLINARY",6/1/14,11/30/17,Stephen Ellner,NY,Cornell University,Standard Grant,Douglas Levey,5/31/19,"$259,997.00 ",Giles Hooker,spe2@cornell.edu,373 Pine Tree Road,Ithaca,NY,148502820,6072555014,BIO,"1182, 7454","8007, 9169, EGCH",$0.00 ,"All environments are variable and uncertain: some years are hotter, some wetter; predators wax and wane in abundance. How organisms buffer themselves against this variability, and exploit it, are major challenges for ecology. This project will combine new statistical and mathematical theory with long-term data sets on semi-arid plant communities to address two challenges in understanding how variable environments affect populations. The first goal will be to identify which environmental variables most strongly affect plant demographic rates such as survival, growth, and fecundity. This work will draw upon long-term studies on natural communities that provide observations on thousands of individual plants over multiple decades, along with numerous environmental variables that are measured at high spatial and temporal resolution. With so many potential explanatory variables, standard statistical methods for variable selection are unstable and unreliable. This project will combine data mining methods from machine learning techniques with traditional statistical theory to identify important environmental drivers, and build more reliable demographic models. Ecological questions that will be addressed include whether demographic rates respond primarily to resource availability (e.g., soil moisture) versus non-resource variables such as temperature; how environment and competition interact; and how strongly past conditions affect current performance. The empirical data sets will be used to answer these questions and look for generalities across multiple communities. A second goal is to understand the individual mechanisms that underlie effects of environmental variation. For example, if a plant population's growth is most affected by rainfall, is it because many plants die immediately, or because of long-term effects such as decreased life expectancy and smaller size throughout life? To address such questions, statistical methods from discrete-state random process theory will be extended to continuous states (e.g., individual plant size) and varying environments, and applied to the fitted plant demographic models. <br/><br/>An expected transformative outcome from this project will be new methods for demographic modeling and analysis, and much of its broader significance will be the applications of those tools in ecology, conservation biology and invasive species management.  Statistical products from this work will be disseminated freely as R code modules that users can adapt to their own study systems. The project will also support the research training and mentoring of a doctoral student in statistics, and a postdoctoral student in quantitative ecology."
403,1353078,Collaborative Research:  Integral Projection Models for Populations in Varying Environments:  Construction and Analysis,DEB,POP & COMMUNITY ECOL PROG,6/1/14,5/30/14,Peter Adler,UT,Utah State University,Standard Grant,Douglas Levey,5/31/18,"$289,999.00 ",,peter.adler@usu.edu,Sponsored Programs Office,Logan,UT,843221415,4357971226,BIO,1182,"8007, 9150, 9169, EGCH",$0.00 ,"All environments are variable and uncertain: some years are hotter, some wetter; predators wax and wane in abundance. How organisms buffer themselves against this variability, and exploit it, are major challenges for ecology. This project will combine new statistical and mathematical theory with long-term data sets on semi-arid plant communities to address two challenges in understanding how variable environments affect populations. The first goal will be to identify which environmental variables most strongly affect plant demographic rates such as survival, growth, and fecundity. This work will draw upon long-term studies on natural communities that provide observations on thousands of individual plants over multiple decades, along with numerous environmental variables that are measured at high spatial and temporal resolution. With so many potential explanatory variables, standard statistical methods for variable selection are unstable and unreliable. This project will combine data mining methods from machine learning techniques with traditional statistical theory to identify important environmental drivers, and build more reliable demographic models. Ecological questions that will be addressed include whether demographic rates respond primarily to resource availability (e.g., soil moisture) versus non-resource variables such as temperature; how environment and competition interact; and how strongly past conditions affect current performance. The empirical data sets will be used to answer these questions and look for generalities across multiple communities. A second goal is to understand the individual mechanisms that underlie effects of environmental variation. For example, if a plant population's growth is most affected by rainfall, is it because many plants die immediately, or because of long-term effects such as decreased life expectancy and smaller size throughout life? To address such questions, statistical methods from discrete-state random process theory will be extended to continuous states (e.g., individual plant size) and varying environments, and applied to the fitted plant demographic models. <br/><br/>An expected transformative outcome from this project will be new methods for demographic modeling and analysis, and much of its broader significance will be the applications of those tools in ecology, conservation biology and invasive species management.  Statistical products from this work will be disseminated freely as R code modules that users can adapt to their own study systems. The project will also support the research training and mentoring of a doctoral student in statistics, and a postdoctoral student in quantitative ecology."
404,1532231,CRCNS Data Sharing: An open data repository for cognitive neuroscience: The OpenfMRI Project,OAC,"Cognitive Neuroscience, CRCNS-Computation Neuroscience, Data Cyberinfrastructure",9/1/14,6/19/15,Russell Poldrack,CA,Stanford University,Standard Grant,Robert Chadduck,8/31/16,"$262,470.00 ",,poldrack@stanford.edu,450 Jane Stanford Way,Stanford,CA,943052004,6507232300,CSE,"1699, 7327, 7726",7327,$0.00 ,"Functional magnetic resonance imaging (fMRI) has become the most common tool for cognitive neuroscience, because it provides a safe, non-invasive, and powerful means to image human brain function. Based on recent rates of publication, there are currently more than 2000 fMRI studies being performed every year worldwide.  The aggregation of data across multiple studies can provide the ability to answer questions that cannot be answered based on a single study. For example, using datasets from multiple domains one can start to investigate to what degree a region is selectively engaged in relation to a particular mental process, as opposed to being generally engaged across a broad range of tasks and processes. In addition, it provides the ability to integrate across specific tasks to obtain stronger empirical generalizations about mind-brain relationships, and to better understand the nature of individual variability across different measures. Recent work in neuroimaging analysis has focused on the application of methods such as machine learning techniques to understand the coding of information at the macroscopic level, and network analysis techniques to understand the interactions inherent in large-scale neural systems. The availability of a large testbed of high-quality fMRI data from published studies would also provide an important resource for the development of these and other new analytic techniques for fMRI data.  However, sharing of raw fMRI data is challenging due to the large size of the datasets and the complexity of the associated metadata, and there is currently no infrastructure for the open sharing of new fMRI datasets.<br/><br/>This project, OpenfMRI, will provide a new infrastructure for the broad dissemination of raw data within cognitive neuroscience, addressing a critical need by providing an open data sharing resource for neuroimaging.  The initial project is already online at http://www.openfmri.org with a limited number of datasets.  The full project will greatly expand this repository by providing access to a large number of fMRI datasets from several prominent neuroimaging labs, spanning across a broad range of cognitive domains. Utilizing the substantial computational resources of the Texas Advanced Computing Center, the project will also perform standard fMRI analyses on all data in the repository using a common analysis pipeline, thus providing directly comparable analysis results for all of the studies in the database.  The OpenfMRI project will support the development of infrastructural elements to make sharing of data by additional investigators more straightforward.<br/><br/>The repository of data that will be created by the OpenfMRI project will also serve as an important resource for teaching by providing students with the ability to replicate the analyses from published studies using the same data. By providing any researcher in the world with the ability to acquire large fMRI datasets, it will also provide all researchers with the ability to work with the same state-of-the-art datasets, regardless of institution. By creating the infrastructure for open sharing of research data, the project will also enhance the impact of other NSF-funded neuroimaging research projects by providing an infrastructure that can be used to make their data available. The planned work has the potential to benefit society by improving education, health, and human productivity through an increased understanding of mental function and its relationship to brain function."
405,1422569,AF: Small: Randomized Algorithms and Stochastic Models,CCF,Algorithmic Foundations,6/1/14,6/3/14,Aravind Srinivasan,MD,University of Maryland College Park,Standard Grant,Tracy Kimbrel,5/31/19,"$450,000.00 ",,srin@cs.umd.edu,3112 LEE BLDG 7809 Regents Drive,COLLEGE PARK,MD,207425141,3014056269,CSE,7796,"7923, 7926",$0.00 ,"The fundamental role played by randomness in computation is one of the key discoveries of research in algorithms and complexity over the last four decades. This manifests itself in at least two ways: through randomized algorithms, and through stochastic models for uncertain data and/or processes. This fundamental role has been accentuated in our Big Data age through tools such as sampling which are vital for streaming and sub-linear algorithms, as well as through the stochasticity that is often inherent in machine learning. In this project, the PI aims to develop or improve some basic techniques in randomized algorithms and stochastic optimization, as well as to apply them to classical and modern problems in combinatorial optimization.<br/><br/>The broader impact of this project will be in several directions including the following. Graduate students will be involved closely in this research, and undergraduate research teams will be exposed to related ideas in algorithms, probabilistic methods, and optimization. High-school students will be taught the foundations of this research, especially the Lovasz Local Lemma and its facets, both through individual mentoring and through group-based teaching. Appropriate aspects of this work will be integrated into the PI's classes. Finally, this work will extend to key applications in areas including vaccination problems and the operation of alternative-energy plants.<br/><br/>This project aims to develop tools that will be general and useful in their own right, as well as techniques that will lead to improvements for targeted, fundamental applications. These applications include well-known problems in combinatorial optimization including asymmetric traveling salesperson, k-median, and stochastic matching, as well as basic issues in application areas such as public health and social networks (vaccination) and alternative energy (operations planning for wind and solar energy plants under stochastic uncertainty). The proposed techniques encompass ""going beyond"" the powerful Lovasz Local Lemma, new iterated applications of Local Lemma-like techniques and their generalizations to problems such as asymmetric traveling salesperson, the nexus of dependent rounding, submodularity, and (matrix) concentration bounds, as well as new types of differential-equation techniques in discrete optimization. As a particular example, work of the PI and his collaborators has shown that the Moser-Tardos algorithm has facets that can help us get well beyond what is guaranteed by the Lovasz Local Lemma; this project aims to go significantly further in this broad direction, with the goal that generalizations of a powerful tool such as the Local Lemma will have new applications in discrete optimization."
406,1422676,III: Small: Using Knowledge Resources to Improve Information Retrieval,IIS,Info Integration & Informatics,9/1/14,8/8/14,Jamie Callan,PA,Carnegie-Mellon University,Standard Grant,Maria Zemankova,8/31/18,"$498,554.00 ",,callan@cs.cmu.edu,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,CSE,7364,"7364, 7923",$0.00 ,"Current search engines understand how humans use language, but they do not understand the language itself.  They match the words in a query to the words in a document and words that are linked somehow to the document (e.g., ""Click here to get the employee handbook"") to find documents that might satisfy the query.  Then they use statistical methods and the behavior of other people who searched for similar information to rank these potential matches.  Although current technology works well most of the time, it sometimes fails badly because the search engine does not really understand the meanings of the documents that it ranks.  Recently, companies, research organizations, and volunteer communities have begun to create large knowledge graphs that describe important, essential, or well-known information.  Knowledge graphs are similar in spirit to Wikipedia, but they are designed to be used by computers instead of humans.  For example, a knowledge graph might contain the entities ""Cleveland Cavaliers"" and ""LeBron James"", and these two entities might be connected by an ""employs"" relationship.  Information can be entered by people with moderate expertise, and by machine learning software, thus it is practical to build large knowledge graphs that cover a wide range of human knowledge.  Freebase, which is now owned by Google, is a well-known knowledge graph that contains 2.5 billion ""facts"" about 44 million ""topics"" and is growing rapidly.  Currently knowledge graphs are used for just a few well-defined tasks, for example, to produce the info boxes that Google displays next to some search results.  New methods of using knowledge graphs for more varied tasks are of significant scientific and commercial interest.  This project develops new methods of using knowledge graphs to improve the accuracy of search engines, especially for vague, ambiguous, or poorly-specified queries.  The search engine uses the knowledge graph to identify the probable meanings of query terms, and then uses this knowledge to improve its ability to identify documents that match those meanings.  The project is of practical significance for its potential to improve search engine accuracy on queries that are currently difficult.  It is of scientific significance for its potential to inject greater understanding of meaning and relationships into search engines.  The project is of educational significance because it provides opportunities for graduate student to do class projects and independent studies that lead to participation in the National Institute of Standards and Technology's (NIST) annual TREC conference, which is a semi-competitive annual event that attracts some of the best research groups from around the world.<br/><br/>Knowledge graphs are less structured than typical relational databases and semantic web resources but more structured than the text stored in full-text search engines.  The weak semantics used in these semi-structured information resources is sufficient to support interesting applications, but is also able to accommodate contradictions, inconsistencies, and mistakes, which makes them easier to scale to large amounts of information.  The typical use of a semi-structured resource treats it like a structured resource that has somewhat restricted functionality.  The application must understand the semantics associated with each type of entity, attribute, and relation that it uses.  Although this approach is effective, the need to understand the semantics of entity types and relation types limits the application's ability to automatically incorporate new types of information as the resource evolves and grows.  This project develops new methods of using semi-structured information resources that make fewer assumptions about the structure and semantics of a semi-structured knowledge resource, thus enabling them to make full use of the resource as it grows and evolves.  The resource is treated as a network of entities and relations that are each described by a ""bag of words"" description.  Entities and relations are retrieved using extensions of full-text retrieval methods.  Evidence such as estimates of authority or related language models can be associated with entity and relation types, and propagated along specific network links to improve entity and relation models.  This project applies this general architecture to make several improvements in the accuracy of a full-text search engine, for example, providing an alternative method of answering entity-attribute queries and a more stable and effective method of query expansion.  Research results are disseminated through scientific publications, open-source software, and the project's web site (http://www.cs.cmu.edu/~callan/Projects/IIS-1422676/)."
407,1421780,RI: Small: Parallel Methods for Large-Scale Probabilistic Inference,IIS,Robust Intelligence,9/1/14,9/11/14,Ryan Adams,MA,Harvard University,Continuing grant,Weng-keen Wong,5/31/18,"$450,000.00 ",,rpa@princeton.edu,1033 MASSACHUSETTS AVE,Cambridge,MA,21385369,6174955501,CSE,7495,"7495, 7923",$0.00 ,"We are undergoing a revolution in data.  We have grown accustomed to constant upheaval in computing -- quicker processors, bigger storage and faster networks -- but this century presents the new challenge of almost unlimited access to raw data.  Whether from sensor networks, social computing, or high-throughput cell biology, we face a deluge of data about our world.  Scientists, engineers, policymakers, and industrialists need to use these enormous floods of data to make better decisions.  This research project is about providing foundations for tools to achieve these goals. Simple models give only coarse understanding.  The world is sophisticated and dynamic, providing rich information.  Furthermore, representation of uncertainty is critical to discovering patterns in complex data.  Not only are many natural processes intrinsically random, but our knowledge is always limited.  The calculus of probability allows us to represent this uncertainty and design algorithms to act effectively in an unpredictable world. The gold standard for probabilistic analysis is Markov chain Monte Carlo (MCMC), a way to identify hypotheses about the unobserved structure of the world that are consistent with observed data.  It is a powerful and principled way to perform data analysis, but traditional MCMC methods do not map well onto modern computing environments.  MCMC is a sequential procedure that cannot generally take advantage of the parallelism offered by multi-core desktops and laptops, cloud computing, and graphical processing units.  This research will develop new methods for MCMC that are provably correct, but that take advantage of large-scale parallel computing. There are a variety of broader impacts of this work.  In addition to the core technical contributions, the project engages in deep scientific collaborations.  New photovoltaic materials will lead to better solar cells and more sustainable energy production.  New techniques for uncovering genetic regulatory mechanisms will lead to better understanding of disease. Quantitative models of mouse activity will give insight into the neural basis of behavior and provide a deeper understanding of brain disorders.  <br/><br/>From a technical point of view, this work pursues two complementary approaches to large-scale Bayesian data analysis with MCMC: 1) a novel general-purpose framework for sharing of information between parallel Markov chains for faster mixing, and 2) a new computational concept for speculative parallelization of individual Markov chains. These theoretical and practical explorations, combined with the release of associated open source software, will yield more robust and scalable probabilistic modeling. It will develop provably-correct foundations and efficient new algorithms for parallelization of Markov transition operators for posterior simulation.  These operators will be used in three collaborations that are representative of the methodological demands for large-scale statistical inference: 1) predicting the efficiencies of novel organic photovoltaic materials, 2) discovering new genetic regulatory mechanisms, and 3) quantitative neuroscientific models for mouse behavior. While this proposal focuses on the generalizable technical challenges of these problems, these collaborations provide compelling examples of how machine learning can be broadly transformative.<br/><br/>Finally, the project includes a significant outreach component, engaging with local middle schoolers, and involving underrepresented minorities in summer research."
408,1409204,CIF: Medium: Collaborative Research: Estimating simultaneously structured models: from phase retrieval to network coding,CCF,Comm & Information Foundations,8/15/14,9/13/16,Babak Hassibi,CA,California Institute of Technology,Continuing grant,Phillip Regalia,7/31/18,"$500,000.00 ",,hassibi@caltech.edu,1200 E California Blvd,PASADENA,CA,911250600,6263956219,CSE,7797,"7924, 7935",$0.00 ,"In modern data-intensive science and engineering, researchers are faced with estimating models where available observations are far fewer than the dimension of the model to be estimated. The key to the success of compressed sensing, matrix completion, and other problems of this type, is to properly exploit knowledge about the ""structure"" of the model. While structures such as sparsity have been separately studied, the problem of ""simultaneous structures"" has been neglected, since it is implicitly assumed by practitioners that simply combining known results for each structure would solve the joint problem. Interestingly, the PIs recently proved that this approach can result in a significant gap.<br/><br/>This proposal will develop theory and computationally tractable methods for estimating simultaneously structured models with minimal observations. It combines (1) a top-down approach to understand the fundamental limitations based on the geometry of how structures interact, and (2) a problem-specific, bottom-up approach to exploit domain knowledge in constructing appropriate penalties. This work addresses a variety of applications including (1) sparse principal component analysis, a central problem in statistics seeking approximate but sparse eigenvectors, (2) sparse phase retrieval and quadratic compressed sensing in signal processing, and (3) code design for communications and network coding.<br/><br/>The ability to systematically derive structured models from data will have far-reaching impact on engineering challenges in the era of Big Data and ubiquitous computing. Handling models with multiple structures poses deep theoretical and computational challenges that this proposal focuses on. Applications in machine learning, signal processing, and network coding are discussed. The PIs will incorporate research results in their teaching, organize technical workshops to bring together mathematicians and engineers, and seek the involvement of undergraduate students in this work through summer research programs."
409,1359632,EAGER: Consumer Response to Security Incidences and Data Breach Notification,SES,Secure &Trustworthy Cyberspace,6/15/14,5/28/14,Rahul Telang,PA,Carnegie-Mellon University,Standard Grant,chu-hsiang chang,9/30/16,"$199,908.00 ",Artur Dubrawski,rtelang@andrew.cmu.edu,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,SBE,8060,"7434, 7916, 8225",$0.00 ,"Despite intense media attention on data breaches and identity thefts, very little is known about how users respond to adverse security events. Effective security policy making and firm response critically depends on how users respond when the firm holding their data is breached. Many argue that users may not pay adequate attention to security events or data breaches. They may ignore firms' breach notifications, especially if they receive many such notifications or when the notices are not specific enough. Even when users respond, it may be nuanced and subtle and not readily measured. This project examines users' responses to security events by assembling a large and unique field dataset. By collaborating with a large financial institution, the research team has access to detailed data for more than half million customers for more than six years spread over different geographies in the US. <br/> <br/>User response can range from leaving the firm to aversion to Internet and mobile based banking to changes in adopting other banking services. Combining econometrics techniques with machine learning techniques, this project aims to identify degree of behavior changes in user behavior due to an adverse security event or breach notification. The research team aims to estimate how the user response is conditioned by user characteristics (demographic), market characteristics (competition), and breach characteristics. If successful, this project could answer a wide variety of questions that may prove helpful for how firms can successfully overcome such events and how policy makers can fine tune their policy."
410,1419415,Broadening Participation in Visualization (BPViz) Workshop,OAC,"Information Technology Researc, CYBERINFRASTRUCTURE",2/1/14,1/23/14,Vetria Byrd,SC,Clemson University,Standard Grant,Sushil K Prasad,3/31/16,"$25,000.00 ",,vlbyrd@purdue.edu,230 Kappa Street,CLEMSON,SC,296345701,8646562424,CSE,"1640, 7231","1640, 7231, 7361, 7556, 9102, 9150",$0.00 ,"The objective of this project is to broaden participation of women and underrepresented groups in visualization. The PI proposes to establish an annual two-day workshop and other activities designed to increase the participation of women and underrepresented groups in visualization.Visualization plays a significant role in the exploration and understanding of data across all disciplines with a universal goal: gaining insight into the complex relationships that exist within the data. The need to diversify a field with such far reaching influences by providing career mentoring advice, overviews of past accomplishments, and future research directions in visualization is important. The proposed workshops will inform, inspire and encourage participants to engage in the multidisciplinary dynamics of visualization.<br/>The first supported workshop is scheduled for February 10-11, 2014, and will be hosted at Clemson University in South Carolina. <br/>A total of 20 participants at the undergraduate, graduate, post-graduate, and early career faculty level will be invited to participate in the workshop. The workshop will be organized as a series of presentations, panels with networking opportunities, and mentoring activities. To encourage collaborations, participants will present their work and explore the work of others during a poster session. Participants will be recruited via a number of different venues including, but not limited to: The Anita Borge Institute/Grace Hopper listserv, association listservs (CRA, CRA-W, CDC, AAPHDCS, Hispanics in Computing, and Women in Machine Learning), Broadening Participation Communities (A4RC, EL Alliance) and discipline-specific mailing lists (IEEE-Vis). Undergraduates will be recruited by on-campus organizations (Women in Science and Engineering and The Charles H. Houston Center). The inclusion of undergraduates will ensure the level of interest in visualization spans across academic and professional domains. Outcomes of the workshops will be assessed by surveying participants at the start and at the end of the workshop."
411,1422914,CIF: Small: High Resolution EEG Signal Analysis for Seizure Detection and Treatment,CCF,Comm & Information Foundations,7/15/14,1/12/15,Yao Wang,NY,New York University,Standard Grant,Phillip Regalia,6/30/18,"$494,663.00 ",Jonathan Viventi,yw523@nyu.edu,70 WASHINGTON SQUARE S,NEW YORK,NY,100121019,2129982121,CSE,7797,"7923, 7936",$0.00 ,"The investigators have developed flexible, active, multiplexed recording devices to enable interface with thousands of electrodes implanted on the surface of the brain. While this technology has enabled a much finer view of the electrical activity of the brain, the analytical methods to process, categorize and respond to the huge volumes of data produced by these devices are presently lacking. Many existing neurological data analyses rely on manual inspection. With new neural interfaces with thousands of channels, the data volume is infeasible for manual review. Further, manual inspection can miss subtle features that automated machine learning techniques can detect. In this research, the investigators develop efficient and sensitive automated methods to analyze micro-electrocorticographic (ECoG) data from patients with epilepsy. These methods are used to segment, categorize and predict spatiotemporal epileptiform discharge (or spike) patterns. Understanding the ordering and relationships between these patterns is a key to developing better seizure detection and prediction techniques and ultimately better therapies for patients with epilepsy.<br/><br/>This research comprises four interconnected components. The first component develops techniques for detecting and isolating spike segments, and for extracting features that capture the spatio-temporal pattern of each spike.  The second component develops unsupervised clustering algorithms that can identify distinct clusters of spike motion patterns based on carefully chosen features. The thir-d component develops classifiers that can categorize each spike into a few classes (inter-ical, pre-ictal, ictal and post-ictal) based on not only its spatio-temporal pattern, but also the patterns of past spikes. The final component develops methods to predict spike wavefront locations. The combination of these methods will enable seizure prediction and real-time responsive brain stimulation to suppress seizures."
412,1421879,TWC: TTP Option: Small: Collaborative: Integrated Smart Grid Analytics for Anomaly Detection,CNS,"Special Projects - CNS, Secure &Trustworthy Cyberspace",10/1/14,1/27/16,Samir Tout,MI,Eastern Michigan University,Standard Grant,David Corman,9/30/18,"$238,668.00 ",,stout@emich.edu,Office of Research Development,YPSILANTI,MI,481972212,7344873090,CSE,"1714, 8060","7434, 7923, 9178, 9251",$0.00 ,"The modernized electric grid, the Smart Grid, integrates two-way communication technologies across power generation, transmission and distribution, in order to deliver electricity efficiently, securely and cost-effectively. On the monitoring and control side, it employs real-time monitoring offered by a messaging-based advanced metering infrastructure (AMI), which ensures the grid?s stability and reliability, as well as the efficient implementation of demand response schemes to mitigate bursts demand. The efficient implementation of these features presents a number of challenges, but also opportunities for technology development in engineering, networking, and data analytics.<br/><br/>The intellectual contributions include the development of a framework that encompasses fast signal processing and machine learning algorithms, together with multi-sensor information to assess the health of the network. Specifically, the study will investigate algorithms for adaptive detection over streaming, high-dimensional and potentially missing value data. Furthermore, the project, via industry collaborators and power provisioners, will provide a comprehensive empirical evaluation with real-world data; this includes an open-source proof-of-concept prototype for quickly inferring nefarious activity in home-area networks, and a cloud-based testbed for examining realistic scenarios in a wide-area setting."
413,1440769,SI2-SSE:  AMASS - An Automated Monitoring AnalySis Service for Cyberinfrastructure,OAC,Software Institutes,9/1/14,8/15/16,Shava Smallen,CA,University of California-San Diego,Standard Grant,Alan Sussman,8/31/18,"$515,205.00 ","Lawrence Saul, Sameer Tilak",ssmallen@sdsc.edu,Office of Contract & Grant Admin,La Jolla,CA,920930621,8585344896,CSE,8004,"7433, 8005, 9251",$0.00 ,"A science gateway is a community-developed set of tools, applications, and data collections that are integrated via a portal or a suite of applications. It provides easy, typically browser-based, access to supercomputers, software tools, and data repositories to allow researchers to focus on their scientific goals and less on the cyberinfrastructure. These gateways are fostering collaboration and exchange of ideas among thousands of researchers from multiple communities ranging from atmospheric science, astrophysics, chemistry, biophysics, biochemistry, earthquake engineering, geophysics, to neuroscience, and biology. However due to limited development and administrative personnel resources, science gateways often leverage only a small subset of the NSF-funded CI to mitigate the complexities involved with using multiple resource and services at scale in part due to software and hardware failures. Since many successful science gateways have had unprecedented growth in their user base and ever increasing datasets, increasing their usage of CI resources without introducing additional complexity would help them meet this demand.<br/><br/>In response to this need, an Automated Monitoring AnalySis Service (AMASS) will be built to provide a flexible and extensible service for automated analysis of monitoring data initially focused on science gateways.  AMASS will be based on data mining and machine learning techniques and emerging big data technologies to analyze monitoring data for improving the reliability and operational efficiency of CI as well as progress on fundamental questions in systematic and population biology, computational neuroscience, and biophysics communities.  Along with AMASS, a simulation framework will be built for testing automated analysis algorithms and adaptive execution techniques.  An intuitive query API will be provided for science gateway software to use and will be integrated into the following three target science gateways that  will drive the project's research and development: the Cyberinfrastructure for Phylogenetic Research (CIPRES), the Neuroscience Gateway (NSG), and UltraScan. The proposed approach does not require any changes to the end user applications, and the software developments will significantly enhance the science productivity and user satisfaction of science gateways by integrating monitoring data into their infrastructure to enable adaptive execution of their applications, allowing scientists to answer more sophisticated questions without having to understand the complexities of a large-scale distributed environment. The developed software products will be available as open source products under an Apache License and will be integrated into the NSF-funded SciGap project in order to impact a broader range of science gateways."
414,1440700,SI2-SSE: Enhancing the PReconditioned Iterative MultiMethod Eigensolver Software with New Methods and Functionality for Eigenvalue and Singular Value Decomposition (SVD) Problems,OAC,"OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, Software Institutes",9/1/14,5/8/18,Andreas Stathopoulos,VA,College of William and Mary,Standard Grant,Alan Sussman,8/31/18,"$451,884.00 ",,andreas@cs.wm.edu,Office of Sponsored Programs,Williamsburg,VA,231878795,7572213966,CSE,"1253, 1712, 8004","062Z, 7433, 8004, 8005, 8400, 9216, 9251",$0.00 ,"The numerical solution of large, sparse Hermitian Eigenvalue Problems (HEP) and Generalized HEP (GHEP) for a few extreme eigenvalues is one of the most important but also computationally intensive tasks in a variety of applications. Examples abound in spectral graph partitioning, large scale spectral graph analysis, structural engineering, electromagnetics, lattice Quantum Chromodynamics, and electronic structure applications from atomic scale physics to molecular scale materials science. Closely related is the problem of computing a partial Singular Value Decomposition (SVD) of a matrix, which finds everyday use in numerous applications including data mining and machine learning. The importance of the problem is evidenced by the significant resources that have been invested over the last decade in developing high quality eigenvalue software packages. However, these packages still do not include the near-optimal methods that have made the package PRIMME the software to beat. PRIMME, or PReconditioned Iterative MultiMethod Eigensolver, is a software package developed in 2005 for the solution of HEP.  PRIMME brings state-of-the-art preconditioned iterative methods from ""bleeding edge"" to production, with a flexible, yet highly usable interface.  Yet, it is its focus on numerical robustness and computational efficiency that has gained PRIMME the recognition as one of the best eigenvalue packages. This success calls for a new effort to extend PRIMME with some long awaited functionality but also to include new algorithms to address some outstanding problems in eigenvalue computations. This work is critical to many groups whose research depends on the lattice QCD and materials science software packages that PRIMME will improve through collaborations. PRIMME already has a PETSc interface, and with the proposed development of Hypre and Trilinos interfaces, it will be accessible by a far wider community of users. The most requested feature, however, has been a MATLAB interface. This will unleash the power of an ""industrial strength"" software to end users. Last but not least, this project will educate and train two graduate and several undergraduate students in the art of high performance numerical software.<br/><br/>Specific goals for this projects include: PRIMME extension to GHEP, with special attention to ill conditioned mass matrices; PRIMME extension to SVD, with special attention to obtaining results at high accuracy (the solution must include not only PRIMME's robust components but a combination of known and new methods, as well as a dynamic way to choose between them); implementation of new methods and techniques for the solution of highly interior eigenvalue problems and for the computation of a large number of eigenvalues; interoperability with DOE libraries and MATLAB, and improved means of dissemination. As a numerical linear algebra kernel, PRIMME has a large potential audience in the computational sciences community. However, two specific collaborations will provide real-world, challenging problems and serve as a stress-test evaluator of the resulting methods and software. One involves the lattice QCD group at the DOE's Jefferson Lab, and the other involves the high performance computing and materials science group at IBM, Zurich."
415,1441484,SaTC: STARSS: Hardware Authentication through High-Capacity PUF-Based Secret Key Generation and Lattice Coding,CNS,Secure &Trustworthy Cyberspace,10/1/14,9/8/14,Michael Orshansky,TX,University of Texas at Austin,Standard Grant,Nina Amla,9/30/18,"$306,667.00 ",Sriram Vishwanath,orshansky@mail.utexas.edu,"3925 W Braker Lane, Ste 3.340",Austin,TX,787595316,5124716424,CSE,8060,"7434, 8225",$0.00 ,"Hardware authentication is one of the critical needs in the emerging discipline of design for assurance and design for security. It is concerned with establishing the authenticity and provenance of Integrated Circuits (ICs) reliably and inexpensively at any point in a chip's life-time. Physical unclonable functions (PUFs) have significant promise as basic primitives for authentication since they can serve as intrinsically-generated hardware roots-of-trust within specific authentication protocols. PUFs are pseudo-random functions that exploit the randomness inherent in the IC manufacturing to generate random output strings. <br/><br/>The central challenge in realizing the potential of strong PUFs is their vulnerability to model-building attacks using machine learning (ML) methodologies. Despite the effort devoted to PUFs over the past decade, there is still a lack of a strong silicon PUF that is both reliable and ML-resilient. This project develops a strong PUF that exploits the physics of nanometer-scale CMOS to attain security properties that are superior to existing designs. This new PUF exploits the physics of scaled transistors, namely subthreshold operation and drain-induced barrier lowering, to realize continuous nonlinearity. The project also investigates effective algorithmic techniques, based on novel codes and lattice-constructions, to enhance the overall structure as well as the robustness of the resulting PUF output to enable high-capacity secret key generation. The strong PUFs developed in this project possess a very large input-output space, making them invaluable to many potential security applications."
416,1449410,"Information Technology, Automation, and the U.S. Workforce",IIS,INFORMATION TECHNOLOGY RESEARC,8/15/14,1/24/17,Jon Eisenberg,DC,National Academy of Sciences,Standard Grant,Sylvia J. Spengler,3/31/17,"$380,480.00 ",,jeisenbe@nas.edu,500 FIFTH STREET NW,Washington,DC,200012721,2023342254,CSE,1640,"7367, 7556",$0.00 ,"Recent publications and media reports have revived longstanding concerns about the potential negative effects of automation and other applications of information technology (IT) on the U.S. workforce, and prompted new discourse about implications and potential responses. Substitution of technology for labor where cost effective is not a new phenomenon.  Today though there are indications that IT's growing scope and scale of use and the discovery and use of new computing techniques such as machine learning is changing what can and will be automated. The ultimate limits to automation and how the workforce--and the nation--will adapt are not clear. To the extent that IT can assume some or all of the responsibilities of existing jobs, it reduces the number of workers required and changes the nature of the work that remains, e.g., as people increasingly work with automated assistants. The phenomenon is of great consequence for individuals, who must make decisions about their own education, training, and career development; for businesses and other institutions whose labor needs will change; and for policy makers, who must understand the implications for productivity, the economy, unemployment, the social ""safety net,"" and future education and training needs.<br/><br/>A National Academies study will consider the possible impacts of automation and other applications of IT on the U.S. Workforce. It will consider current knowledge and open questions about the drivers of increased automation; the types and scale of jobs that might be affected; the societal implications of these changes; the time frame for impact; and implications for education, training, and workforce development. Through testimony, discussions convened by the committee, a literature review, and committee deliberations, the committee will examine currently available sources of information, consider how different disciplines could contribute knowledge, explore where additional data would help, and frame research questions aimed at better understanding the phenomenon. The committee's consensus report will set forth a research agenda and describe types and sources of data and analysis that would enhance our understanding of the workforce impacts of IT and automation.  The project will thus provide a more nuanced understanding of the workforce impacts of information technology, stimulate interdisciplinary research to improve our understanding, and provide information to inform future policy making."
417,1446737,CPS: Synergy: Sensor Network-Based Lower-Limb Prosthetic Optimization and Control,CNS,CYBER-PHYSICAL SYSTEMS (CPS),12/1/14,9/9/14,Ou Bai,VA,Virginia Commonwealth University,Standard Grant,Sylvia J. Spengler,10/31/15,"$901,925.00 ","Zhixiu Han, Douglas Murphy, Ding-Yu Fei, Ashraf Gorgey",obai@fiu.edu,P.O. Box 980568,RICHMOND,VA,232980568,8048286772,CSE,7918,8235,$0.00 ,"More than one million people including many wounded warfighters from recent military missions are living with lower-limb amputation in the United States. This project will design wearable body area sensor systems for real-time measurement of amputee's energy expenditure and will develop computer algorithms for automatic lower-limb prosthesis optimization. The developed technology will offer a practical tool for the optimal prosthetic tuning that may maximally reduce amputee's energy expenditure during walking. Further, this project will develop user-control technology to support user's volitional control of lower-limb prostheses. The developed volitional control technology will allow the prosthesis to be adaptive to altered environments and situations such that amputees can walk as using their own biological limbs. An optimized prosthesis with user-control capability will increase equal force distribution on the intact and prosthetic limbs and decrease the risk of damage to the intact limb from the musculoskeletal imbalance or pathologies. Maintenance of health in these areas is essential for the amputee's quality of life and well-being.  Student participation is supported.<br/><br/>This research will advance Cyber-Physical Systems (CPS) science and engineering through the integration of sensor and computational technologies for the optimization and control of physical systems. This project will design body area sensor network systems which integrate spatiotemporal information from electromyography (EMG), electroencephalography (EEG) and inertia measurement unit (IMU) sensors, providing quantitative, real-time measurements of the user's physical load and mental effort for personalized prosthesis optimization. This project will design machine learning technology-based, automatic prosthesis parameter optimization technology to support in-home prosthesis optimization by users themselves. This project will also develop an EEG-based, embedded computing-supported volitional control technology to support user?s volitional control of a prosthesis in real-time by their thoughts to cope with altered situations and environments. The technical advances from this project will provide wearable and wireless body area sensing solutions for broader applications in healthcare and human-CPS interaction applications. The explored computational methods will be broadly applicable for real-time, automatic target recognition from spatiotemporal, multivariate data in CPS-related communication and control applications. This synergic project will be implemented under multidisciplinary team collaboration among computer scientists and engineers, clinicians and prosthetic industry engineers. This project will also provide interdisciplinary, CPS relevant training for both undergraduate and graduate students by integrating computational methods with sensor network, embedded processors, human physical and mental activity recognition, and prosthetic control."
418,1346452,STTR Phase I: Using Big Data to Support Supply Chain Analytics and Optimization,IIP,STTR PHASE I,1/1/14,2/26/14,Arun Aryasomayajula,NJ,"Optimal Solutions, Inc.",Standard Grant,Glenn H. Larsen,12/31/14,"$225,000.00 ",Jaroslaw Zola,arun@osiopt.com,17 Kershaw Ct.,Bridgewater,NJ,88072595,9083931316,ENG,1505,"1505, 8032, 8033, 8039",$0.00 ,"This SBIR Phase I project proposes to demonstrate the feasibility of delivering a radically game-changing Big Data - based Supply Chain Analytics Platform. A complex supply chain, in consumer goods manufacturing and distribution, for example, involves data analysis and decision support in many areas. Today's tools and techniques used in supply chain analytics are too restrictive as they rely on the company's internal factors and are unable to effectively incorporate the increasing volume of readily available, external data. The quality of analytics directly impacts the product quality, cost of fulfillment, customer satisfaction, and ultimately the company's financial health. It is proposed to introduce Big Data concepts including distributed text mining, machine learning, and scalable analytics to support supply chain analytics. The proposed innovation will integrate increasingly available structured and unstructured external data to enhance supply chain decisions. This will enable companies to be more customer-focused, make better decisions, and become more profitable. The key results from this research will include algorithms and methods to pre-process inherently unstructured Big Data into quantitative and qualitative descriptors suitable to be inputs for creating new indicators for decision making. <br/><br/>The benefits of better supply chain analytics are clear and significant including better products, better customer service, reduced waste and costs, and increased quality The broader/commercial impact of the proposed innovation is in the area of enhancing product quality and reducing costs for consumer goods companies, increasing the safety of products, and rapid new products introduction into the market based on customer feedback, etc. The proposed research will also significantly lower the technology adoption barriers - both technology-wise and cost-wise barriers to embracing Big Data - while delivering next generation analytics capabilities for businesses and will ""democratize"" the use of Big Data for companies of all sizes. This is expected to make U.S. companies more competitive resulting in job creation in the U.S. and reducing the outflow of jobs overseas. The potential impact of the proposed research on the business community is significant, and validated in various surveys and studies recently published documenting the benefits of applying Big Data to business."
419,1417062,"STTR Phase I: Extended Analog Computer Development as a Digitally Configurable, High Speed, High Order, and Low Power Analog Matched Filter",IIP,STTR PHASE I,7/1/14,6/24/15,Gregory Mattes,IN,Analog Computing Solutions,Standard Grant,Muralidharan S. Nair,12/31/15,"$252,036.00 ",Ken Yoshida,greg@analogcomputingsolutions.com,316 N Jordan Ave,Bloomington,IN,474057513,6364482934,ENG,1505,"1505, 163E, 4080, 8035, 9139, HPCC",$0.00 ,"The broader impact/commercial potential of this project lies in the ability of the Extended Analog Computer (EAC) to perform filtering operations not possible with current technology. The low power, near instantaneous filtering of complex analog signals, differentiates the EAC technology from DSP products currently in the marketplace. The development of prosthetics with advanced control characteristics is reaching a computational limit due to the need for neuromuscular waveform recognition and classification in the context of real-time, low power, small form factor computation. EACs promise to surmount these challenges, improving functionality of prosthetics and the quality of life for amputees. In 2010, the global prosthetics market was $3 billion and is expected to reach $4.5 billion by 2017, with myoelectric prosthetics representing a small, but growing fraction of this market. While the myoelectric prosthetics industry represents an initial customer base, this generic computing technology's financial upside can best be estimated by segmenting the multi-billion dollar market for digital signal processing and analog computing devices. The intent is to solve computing problems in niche markets within this broad potential marketplace.<br/><br/>This Small Business Technology Transfer Research (STTR) Phase I project is focused on the development of the Extended Analog Computer (EAC) for application to myoelectric prosthetics. New myoelectric interface techniques, such as targeted muscle reinnervation (TMR) are simplifying the use of advanced multi-degree of freedom prosthetics by amputees. However, the dramatic increase in the number and density of electrode sites, and need to implant multi-electrode structures into targeted muscles will increase the signal processing requirements beyond the capacity of traditional mobile digital signal processing (DSP). The EAC is a radical departure from the digital computer, deriving its computational power by taking advantage of the intrinsic solutions to partial differential equations represented as an analog voltage manifold in space. The proposed research aims to implement automatic machine learning/training methods to automatically configure networks of EAC sheets in a radial basis function network (RBFN). The research also explores the effect of the geometry of the input and output points on the sheet to optimize them for the TMR application. Finally, a physical instantiation enabling stand-alone, real-time operation of an EAC-RBFN will be developed. Using recorded data from intramuscular electrode arrays, the performance of the EAC will be tested against standard DSP techniques."
420,1405951,CI-P: Collaborative Research: HomeSHARE - Home-based Smart Health Applications across Research Environments,CNS,CCRI-CISE Cmnty Rsrch Infrstrc,9/1/14,5/21/14,Blaine Reeder,CO,University of Colorado at Denver-Downtown Campus,Standard Grant,William Bainbridge,8/31/15,"$9,657.00 ",,blaine.reeder@missouri.edu,"F428, AMC Bldg 500",Aurora,CO,800452571,3037240090,CSE,7359,7359,$0.00 ,"This award supports planning for the ""Home-based Smart Health Applications across Research Environments"" (HomeSHARE) test bed, intended to be a geographically-distributed, in-situ test bed to design, develop, and evaluate pervasive home-based technologies. Addressing the growing societal need to support an aging population, researchers continue to create innovative home-based systems to support older adults as they age in-place. However, researchers find it hard to assess how generally useful systems will be because they lack resources, access to diverse populations and research infrastructure. The work supported by this award will identify the next steps to developing a robust infrastructure to support better research in this area, leading eventually to better home-based systems for aging in place.<br/><br/>Specifically, the research objectives of this planning grant are to identify the needs of various CISE research communities for a HomeSHARE testbed with respect to hardware, software, data sharing and coordination mechanisms. The PIs will work with researchers to better understand what technologies are required for a standard, baseline HomeSHARE installation, what technologies may be needed beyond the proposed baseline package for specific projects, what processes should be in place to add technologies to a subset of homes, and how the PIs can make each HomeSHARE site as self-sufficient and easy to maintain as possible. To determine and validate these requirements, the PIs will facilitate workshops at top conferences in pervasive healthcare, ubiquitous computing, human-computer interaction, software engineering, machine learning, privacy-enhanced technologies, and gerontology to identify research community needs and work with experts in each domain to confirm findings.<br/><br/>In addition to proposing an initial test bed of 100 homes, the PIs will address the issues around sustaining such a test bed over time, as well as growing the initial HomeSHARE test bed in the future. HomeSHARE will be informed by best practices in industry and academia to ensure that researchers can easily utilize the test bed and results can be shared for replication and further analysis. This award supports planning HomeSHARE for a single target domain, aging-in-place, to allow for common, shared parameters for the recruitment of participants across multiple research studies, but the creation of this test bed will inform other CISE home-based research efforts and accelerate the pace of discoveries and their implementation as well.<br/><br/>The broader impacts of the HomeSHARE planning activity include professional development opportunities for researchers who take part in HomeSHARE planning workshops, and cross-disciplinary outreach to gerontology research communities. If implemented, the HomeSHARE test bed itself will have a positive societal impact by supporting development systems for the next generation of older adults to age in place, thus improving the quality of life for older adult participants. The supported research has the further potential to increase economic competitiveness for health care, to increase diversity of the STEM workforce by attracting members of underrepresented groups by providing research opportunities with real-world impacts, and to provide innovative interdisciplinary educational opportunities."
421,1405723,CI-P: Collaborative Research: HomeSHARE - Home-based Smart Health Applications across Research Environments,CNS,CCRI-CISE Cmnty Rsrch Infrstrc,9/1/14,5/21/14,Kelly Caine,SC,Clemson University,Standard Grant,William Bainbridge,8/31/15,"$10,967.00 ",,caine@clemson.edu,230 Kappa Street,CLEMSON,SC,296345701,8646562424,CSE,7359,7359,$0.00 ,"This award supports planning for the ""Home-based Smart Health Applications across Research Environments"" (HomeSHARE) test bed, intended to be a geographically-distributed, in-situ test bed to design, develop, and evaluate pervasive home-based technologies. Addressing the growing societal need to support an aging population, researchers continue to create innovative home-based systems to support older adults as they age in-place. However, researchers find it hard to assess how generally useful systems will be because they lack resources, access to diverse populations and research infrastructure. The work supported by this award will identify the next steps to developing a robust infrastructure to support better research in this area, leading eventually to better home-based systems for aging in place.<br/><br/>Specifically, the research objectives of this planning grant are to identify the needs of various CISE research communities for a HomeSHARE testbed with respect to hardware, software, data sharing and coordination mechanisms. The PIs will work with researchers to better understand what technologies are required for a standard, baseline HomeSHARE installation, what technologies may be needed beyond the proposed baseline package for specific projects, what processes should be in place to add technologies to a subset of homes, and how the PIs can make each HomeSHARE site as self-sufficient and easy to maintain as possible. To determine and validate these requirements, the PIs will facilitate workshops at top conferences in pervasive healthcare, ubiquitous computing, human-computer interaction, software engineering, machine learning, privacy-enhanced technologies, and gerontology to identify research community needs and work with experts in each domain to confirm findings.<br/><br/>In addition to proposing an initial test bed of 100 homes, the PIs will address the issues around sustaining such a test bed over time, as well as growing the initial HomeSHARE test bed in the future. HomeSHARE will be informed by best practices in industry and academia to ensure that researchers can easily utilize the test bed and results can be shared for replication and further analysis. This award supports planning HomeSHARE for a single target domain, aging-in-place, to allow for common, shared parameters for the recruitment of participants across multiple research studies, but the creation of this test bed will inform other CISE home-based research efforts and accelerate the pace of discoveries and their implementation as well.<br/><br/>The broader impacts of the HomeSHARE planning activity include professional development opportunities for researchers who take part in HomeSHARE planning workshops, and cross-disciplinary outreach to gerontology research communities. If implemented, the HomeSHARE test bed itself will have a positive societal impact by supporting development systems for the next generation of older adults to age in place, thus improving the quality of life for older adult participants. The supported research has the further potential to increase economic competitiveness for health care, to increase diversity of the STEM workforce by attracting members of underrepresented groups by providing research opportunities with real-world impacts, and to provide innovative interdisciplinary educational opportunities."
422,1416509,IBSS: Spatiotemporal Modeling of Human Dynamics Across Social Media and Social Networks,SMA,Interdiscp Behav&SocSci IBSS,9/1/14,9/6/18,Ming-Hsiang Tsou,CA,San Diego State University Foundation,Standard Grant,Brian Humes,2/29/20,"$999,887.00 ","Jay Lee, Jean Mark Gawron, Ruoming Jin, Brian Spitzberg",mtsou@mail.sdsu.edu,5250 Campanile Drive,San Diego,CA,921822190,6195945731,SBE,8213,"8213, 8605, 9196, 9197",$0.00 ,"This interdisciplinary research project will examine human dynamics across social media and social networks and will focus on the modeling of information diffusion over both time and space.  It also will study the connection between online activities and real world human behaviors.  Because the research team will employ a broad range of different research approaches to study the processes through which people's behavior is reflected in electronic media, the project will yield new communication theories, knowledge-discovery tools, and computational models.  It will facilitate the convergence of information and insights across a diverse set of fields, including spatial science, social media, communication, computer science, and social behavioral analysis.  The project also will facilitate the adoption of computational applications and modeling approaches in behavioral and social science research.  The investigators will conduct tests of two different scenarios (public response to disaster warnings and alerts and referendums of controversial social topics at state and national levels) to refine a new theory about communication dealing with memes, or reproducible messages.  One product of the project will be a prototype social media outreach platform designed and tested in coordination with the city of San Diego's Office of Emergency Services.  The prototype will facilitate rapid dissemination of official alerts and warnings notifications during disaster events via multiple social media channels to targeted populations.  The platform will be designed to identify and recruit 1,000 social media volunteers based on their social network influence factors, thereby enabling government agencies to communicate more effectively to the public and to be better prepared for both natural disasters and human-related crises.  The project also will provide research and educational opportunities for students in multiple disciplines, including geography, linguistics, computer science, social science, and communication.<br/><br/>People move in time and space, and while they do so, they communicate. With modern communication technologies, they leave traces of and refer to their behavior while they engage in everyday behavior.  These informational echoes have enormous potential to provide insights into people's behavior.  The research team will analyze the diffusion patterns of human messages, activities, and communications using computational methods, such as social network analysis, geographic information systems, and machine-learning, as well as traditional social scientific research methods, such as qualitative analysis, inferential statistics, and behavior analysis.  The investigators will build an interdisciplinary research framework for studying human dynamics and information diffusion from a spatiotemporal modeling perspective.   They will validate and improve the Multilevel Model of Meme Diffusion (M3D) communication theory for online human communications across social media and social networks.  They will analyze the dynamic changes of spatiotemporal patterns with two scenarios of human dynamics (disaster warnings/alerts and referendum/propositions of controversial social topics) using computational predictive methods and agent-based modeling (ABM) approaches.  They also will develop effective and accessible data processing, visualization, and analytical tools for social scientists to study human dynamics and information diffusion by combining high-performance computing, Web-based geographic information system tools, agent-based modeling, and open-source software.  This project is supported through the NSF Interdisciplinary Behavioral and Social Sciences Research (IBSS) competition."
423,1362453,Multi-scale geometry of bi-Lipschitz and quasiconformal maps,DMS,ANALYSIS PROGRAM,6/1/14,5/19/14,Leonid Kovalev,NY,Syracuse University,Standard Grant,Marian Bocea,5/31/19,"$119,479.00 ",,lvkovale@syr.edu,OFFICE OF SPONSORED PROGRAMS,SYRACUSE,NY,132441200,3154432807,MPS,1281,,$0.00 ,"The notion of distance is fundamental both to human perception of the physical space around us, and to the idealization of this space in geometry. The distillation of the most salient properties of distance leads to the concept of a metric space.<br/>Applications of this concept abound in computer science, machine learning, mathematical genetics, statistics and other disciplines where quantitative dissimilarity of objects is of interest. A proper way to understand metric spaces is to consider their transformations that shrink or stretch the distance on the space by a bounded amount. These are called bi-Lipschitz transformations, and they are at the focus of the proposed investigation. As an example of an application of such transformations, one can mention that accurate representations of a given, possibly quite irregular, metric space within a Euclidean space is an essential aspect of  data visualization and analysis.<br/><br/>Normed linear spaces provide a convenient model to which less regular metric spaces can be compared. Thus, it is of interest to know when a given metric space is bi-Lipschitz equivalent to a normed space, or can be embedded into one. This is one of the principal problems addresses in the proposal. Even for two-dimensional spaces (metric planes) our understanding of bi-Lipschitz equivalence is far from complete. One approach to this problem is to introduce and study numeric invariants of metric spaces, such as dimension-type concepts which capture the size or degree of connectivity within the space. A different, but related approach involves analytic properties of functions supported on the space: Poincare inequality and various extension properties (Sobolev, Lipschitz, etc). The proposal involves both of these approaches. In particular, the quantitative aspects of the bi-Lipschitz extension problem will be investigated."
424,1350677,CAREER: Comparative Network Biology to Study the Evolution of Regulatory Networks,DBI,"ADVANCES IN BIO INFORMATICS, Information Technology Researc, Cross-BIO Activities, Info Integration & Informatics, Algorithmic Foundations",8/1/14,8/4/14,Sushmita Roy,WI,University of Wisconsin-Madison,Standard Grant,Peter McCartney,7/31/20,"$506,579.00 ",,sroy@biostat.wisc.edu,21 North Park Street,MADISON,WI,537151218,6082623822,BIO,"1165, 1640, 7275, 7364, 7796","1045, 7931, 8750, 9179",$0.00 ,"Central to how living cells accomplish diverse biological functions are regulatory networks that control what genes need to be activated under different environmental conditions. As evolution is the ultimate tinkerer of living systems systematic comparisons of how regulatory networks evolve to drive species-specific differences are critical to understand cellular functions. Through advances in genomics, it is now possible to measure the activity levels of almost all genes for many species. This provides a unique opportunity to systematically compare these gene activity levels across multiple organisms and link changes in activity to changes in the networks of individual species. However, this is challenging because, first, such comparisons require the regulatory networks to be known in not one but multiple species including those that are poorly characterized, and second, computational methods to compare molecular datasets across species other than DNA sequence are in their infancy. This project will address these challenges by developing novel computational methods to identify and compare regulatory networks across multiple species. <br/><br/>The overarching goal of this project is to establish a comparative network biology framework to study evolutionary dynamics of regulatory networks. In Aim 1, algorithms for regulatory network reconstruction that combine probabilistic models of evolution with network reconstruction will be developed, that take into account both sequence and expression determinants of regulatory networks. The network reconstruction algorithms will incorporate structural constraints of regulatory networks such as modular organization, where sets of genes are coordinately regulated to achieve specific cellular functions. In Aim 2, methods to identify and correlate regulatory network patterns of divergence to phenotypic states will be developed. In Aim 3, methods from Aims 1 and 2 will be applied to published and newly generated expression datasets in yeast and land plant phylogenies to study the evolution of stress response, carbon metabolism and plant-microbe relationships. Network predictions will be interpreted and tested by yeast and plant collaborators in the laboratory. This research project intersects several computational (graph theory, algorithms, machine learning) and biological (gene regulation and evolution) areas of research, providing an excellent training opportunity for scientists at the interface of these areas. The planned outreach activities include developing a Network biology course, and network biology boot camp for a broader audience including high school teachers. The PI will organize summer research internships in Network biology for undergraduate students from underrepresented and minority groups through the Computational Biology and Biostatistics Summer Research Opportunities (CBB SROP) at UW Madison, and participate in general public outreach from the Town Center at the Wisconsin Institute for Discovery (a public venue for regular school field trips and Science festivals). Software tools and resources developed by this proposal will be made publicly available through dedicated websites."
425,1409915,TWC: Medium: Collaborative: Aspire: Leveraging Automated Synthesis Technologies for Enhancing System Security,CNS,Secure &Trustworthy Cyberspace,8/1/14,8/5/14,Dawn Song,CA,University of California-Berkeley,Standard Grant,Sol Greenspan,7/31/20,"$800,000.00 ",,dawnsong@cs.berkeley.edu,Sponsored Projects Office,BERKELEY,CA,947101749,5106433891,CSE,8060,"7434, 7924",$0.00 ,"Designing secure systems and validating security of existing systems are hard challenges facing our society. For implementing secure applications, a serious stumbling block lies in the generation of a correct system specification for a security policy. It is non-trivial for both system designers and end users to express their intent in terms of formal logic. Similar challenges plague users' trying to validate security properties of existing applications, such as web or cloud based services, which often have no formal specifications. Thus, there is an urgent need for mechanisms that can bridge the gap between expressions of user intent and system specifications. This research designs an approach and a system called Aspire that is able to translate user intent into security specifications. <br/><br/>Aspire takes as input, expressions of user intent such as a system demonstration,  application input-output examples, or natural language. Aspire leverages recent  developments in the field of automated synthesis technologies that can consider such examples of user intent as input to the synthesis of security specifications. Aspire combines such inputs, along with a domain specific language for security applications, to synthesize a candidate set of possible outputs. The user can either choose a candidate output or provide more examples to guide the synthesis process. In this iterative fashion, the user can generate system specifications, policies, or properties. Aspire uses concepts from the domain of formal methods, machine learning, and programming languages to perform synthesis. Aspire is applicable to a variety of domains including web, mobile, and cloud applications. The output of Aspire's synthesis can either be used for analyzing security vulnerabilities, or for compilation and testing with real systems."
426,1352373,CAREER:Predicting the Surface Structures of Crystalline Materials,DMR,CONDENSED MATTER & MAT THEORY,3/1/14,3/9/18,Tim Mueller,MD,Johns Hopkins University,Continuing Grant,Daryl Hess,2/28/21,"$400,000.00 ",,tmueller@jhu.edu,1101 E 33rd St,Baltimore,MD,212182686,4439971898,MPS,1765,"1045, 7433, 8084",$0.00 ,"TECHNICAL SUMMARY<br/><br/>This CAREER award supports the development and application of machine learning and data mining methods to predict the surface structures of crystalline materials in a variety of chemical environments.  The PI will develop a three-step process which is designed to minimize the computational expense of predicting surface structures by maximizing the re-use of existing data.  In the first step, evolutionary algorithms will be used to develop a re-usable library of likely surface reconstructions for bulk structure types.  In the second step a combination of evolutionary algorithms and data mining methods will be developed to determine the most likely surface structures for a particular material surface.  In the third step, ab-initio calculations and cluster expansions will be used to identify the particular surface structures with the lowest energy.  The structure prediction process will be developed, validated, and applied to three technologically important systems: perovskite-structured oxides, Au-Pd alloys, and spinel-structured oxides.  <br/><br/>The research will be integrated with an educational outreach program that is designed to strengthen the pipeline of researchers who have both the interest and ability to discover and design new materials through computational research.  At the elementary school level, the PI has volunteered to partner with a master teacher at a majority-minority, low-income Baltimore City public school to share scientific knowledge, help construct an effective curriculum, and design a hands-on exercise intended to educate and excite students about STEM activities.  At the middle school level, the PI will teach computer programming skills to Baltimore City students who are participating in a VEX robotics competition.  At the high school level, a female student from a nearby high school will participate in the research project as member of the research team.  The PI will work with the graduate student to develop an online tutorial that covers fundamental topics in materials surface science, and elements of this tutorial will be integrated into the core curriculum of the Department of Materials Science and Engineering at Johns Hopkins University.<br/><br/>NONTECHNICAL SUMMARY<br/><br/>This CAREER award supports the development and application of advanced computational and data mining methods to predict how atoms are arranged on the surfaces of materials.  The ability to use computers to predict the properties of material surfaces will facilitate the design of new materials for a wide range of technologies including batteries, catalysts, and sensors.  However before a property of a surface can be predicted, it is first necessary to predict the atomic structure, or how the atoms are arranged, on the surface.  The PI will address this challenging problem by developing a method to accurately predict material surface structures with low computational cost.  This will be accomplished by combining a variety of computational tools in a way that leverages existing knowledge about the surface structures to predict the surface structure of a new material.  The method developed in this research will be used to predict the surface structures of three representative classes of materials that were chosen for their importance in technologies such as batteries and catalysts. <br/><br/>The research will be integrated with an educational outreach program that is designed to strengthen the pipeline of researchers who have both the interest and ability to use computers to discover and design new materials.  At the elementary school level, the PI has volunteered to partner with a master teacher at a majority-minority, low-income Baltimore City public school to share scientific knowledge, help construct an effective curriculum, and design a hands-on exercise intended to educate and excite students about science and engineering.  At the middle school level, the PI will teach computer programming skills to Baltimore City students who are participating in a robotics competition.  At the high school level, a female student from a nearby high school will participate in the research project as member of the research team.  The PI will work with the graduate student to develop an online tutorial that covers fundamental topics in materials surface science, and elements of this tutorial will be integrated into the core curriculum of the Department of Materials Science and Engineering at Johns Hopkins University."
427,1449653,EAGER: Monolithic 3D Integration of Resistive Random Access Memory (ReRAM): A Technological Exploration,CCF,Software & Hardware Foundation,9/1/14,5/6/15,Shimeng Yu,AZ,Arizona State University,Standard Grant,Sankar Basu,8/31/17,"$258,000.00 ",,shimeng.yu@ece.gatech.edu,ORSPA,TEMPE,AZ,852816011,4809655479,CSE,7798,"7916, 7945, 9251",$0.00 ,"With the rapid growth of big data, next generation non-volatile memory technologies are needed to enable a large capacity and fast bandwidth. The resistive random access memory (ReRAM) technology is one of the emerging candidates beyond the scaling limit of today's FLASH technology. The research and development of the ReRAM technology will benefit nearly every digital device available today from consumer electronics to enterprise electronics. It may also spawn new applications involving the computation on the exascale of data, e.g. data mining, machine learning, and bio-informatics etc. thus benefitting the semiconductor industry at large. Student summer internships are planned with the industrial partners, which will be invaluable for broadening the knowledge and skills of the students. The tools and techniques developed in this research will be used in the newly developed seminar course on memory design at ASU. The PI will make simulator tools available for use by other educators, researchers, and industry practitioners. Dissemination of research findings will also be carried out through conference tutorials, panel discussions, and workshops. A concerted effort will be made to involve underrepresented groups and undergraduate students in this research, and also to promote the K-12 students to major in science and engineering. <br/><br/>This proposal aims to tackle the fundamental technological challenges for the monolithic 3D integration of ReRAM. Although the ReRAM technology shows very attractive characteristics at single cell level, at the array level its lower integration density limits its economical competiveness over the mainstream NAND FLASH technology. The project examines the use of monolithic 3D integration of ReRAM as a promising approach for enabling ultra-high density cross-point memory architecture. The proposed research activities focus on the device-level engineering with the design targets from the circuit-level array modeling, aiming at enabling the large-scale integration of 3D ReRAM technology. Array macro model that can assess integration density, cost per bit, read/write margin, access latency, and power/energy consumption will be developed and calibrated by SPICE simulation. Device prototypes for 3D ReRAM will be fabricated and optimized towards the desired targets given by the array macro model. All device engineering activities will leverage the existing knowledge of the electrode/oxide interface engineering and oxide/oxide interface engineering and further contribute to new understandings of relevant issues."
428,1422088,CSR: Small: Bridging Efficiency and Low Latency in Warehouse-scale Computing,CNS,CSR-Computer Systems Research,8/1/14,8/4/14,Christoforos Kozyrakis,CA,Stanford University,Standard Grant,Marilyn McClure,7/31/18,"$466,783.00 ",,kozyraki@stanford.edu,450 Jane Stanford Way,Stanford,CA,943052004,6507232300,CSE,7354,7923,$0.00 ,"Computing is now an essential tool and a catalyst for innovation for all aspects of human endeavor, including healthcare, education, science, commerce, government, and entertainment. An increasing amount of computing is performed on private and public clouds, primarily due to the cost and scalability benefits for both the end-users and operators of the warehouse-scale systems that host clouds.  We have come to expect that these systems provide millions of users with instantaneous, personalized, and contextual access to petabytes of data. The goal of this project is to improve the capabilities and efficiency of warehouse-scale systems. Specifically, we aim to reconcile the presumed incompatibility between low-latency processing at massive scales and efficiency in terms of energy consumption and resource usage. We aim to improve energy and resource efficiency in warehouse-scale systems by factors of 2x-5x while allowing for low-latency processing at massive scales. Equally important, we aim to improve our understanding of the tradeoffs between scalability, low latency, and energy or resource efficiency in modern computing systems.<br/><br/>The project focuses on on-line, data-intensive workloads, such as search, social networking, real-time analytics, and machine learning analysis, that occupy thousands of servers in warehouse-scale systems and pose significant scaling challenges. Their strict latency constraints, large state requirements, and high communication fan-out makes it difficult to apply known techniques for power reduction and resource sharing across applications. Hence, they typically consume a significant percentage of peak power and use non-shared servers even during the frequent periods of medium or low user traffic. To a large extent, low latency and high efficiency are considered incompatible for these workloads. To bridge this gap, the project uses a cross-layer approach that monitors end-to-end workload performance and quality-of-service to guide system-wide power management and resource management. The first step is to develop a power management system that improves the energy proportionality of warehouse-scale systems during periods of low or medium load without compromising latency guarantees. The second step is to develop a system-wide resource management system that allows aggressive server sharing between latency-critical workloads and other workloads during periods of low or medium load without compromising latency guarantees. The third step is to design operating system policies for performance isolation between co-located workloads within a server. The final step is to use the insights from the previous steps to evaluate the efficacy of existing and proposed server architectures with respect to energy and resource efficiency for on-line, data-intensive workloads."
429,1421100,III: Small: Collaborative Research: Functional Network Discovery for Brain Connectivity,IIS,Info Integration & Informatics,8/1/14,7/28/14,Jieping Ye,AZ,Arizona State University,Standard Grant,Sylvia Spengler,4/30/15,"$200,000.00 ",,jpye@umich.edu,ORSPA,TEMPE,AZ,852816011,4809655479,CSE,7364,"7364, 7923",$0.00 ,"Neuroscience is at a moment in history where mapping the connectivity of the human brain non- invasively and in vivo has just begun with many unanswered questions.  While the anatomical structures in the brain have been well known for decades, how they are used in combination to form task specific networks has still not been completely explored. Understanding what these networks are, and how they develop, deteriorate, and vary across individuals will provide a range of benefits from disease diagnosis, to understanding the neural basis of creativity, and even in the very long term to brain augmentation. Though machine learning and data mining has made significant inroads into real world practical applications in industry and the sciences, most existing work focuses on lower-level tasks such as predicting labels, clustering and dimension reduction. This requires the practitioner to shoe-horn their more complex tasks, such as network discovery, into the algorithm's settings. <br/><br/>The focus of this grant is a transition to more complex higher-level discovery tasks and in particular, eliciting networks from spatio-temporal data represented as a tensor. Here the spatio-temporal data is an fMRI scan of a person represented as a four dimensional tensor with each entry in the tensor being a data point that indicates the brain activity at that time and location. The overall problem focus is to simplify this data into a cognitive network consisting of identifying active regions of the brains and the interactions that occur between them. The work will consist of three intertwined tasks as follows: i) Supervised and Semi-supervised Network Discovery, ii) Complex Network Discovery and iii) Network Discovery in Populations. In the supervised/semi-supervised setting, the networks discovered involves coordinated activity among some combination of anatomical structures Since all or some of the structures are given along with their boundaries, this is termed a supervised (or semi-supervised) problem. With complex network discovery the team will move beyond finding a single network of coordinated activity to finding multiple networks with complex (beyond coordinates) relationships between the structures/regions. Finally with network discovery in populations , the previous work that studies an individual scan will be expanded to a population of scans.  A population may be a collection of individuals performing the same task or a single individual's scans collected over time. Studying such populations allows addressing innovative questions such as: ""How does one individual's network change over the course of development, aging, or disease?"" and ""How do the networks differ for one group of individuals to that of another group?"""
430,1423298,CSR: Small: Reinventing Formal Methods for Cyber-Physical Systems,CNS,"CSR-Computer Systems Research, Software & Hardware Foundation",9/1/14,8/21/14,Ashish Tiwari,CA,SRI International,Standard Grant,Marilyn McClure,8/31/17,"$439,237.00 ",,tiwari@csl.sri.com,333 RAVENSWOOD AVE,Menlo Park,CA,940253493,7032478529,CSE,"7354, 7798","7923, 8206",$0.00 ,"Much of the world's critical infrastructure is now networked and controlled by computers, including the systems that provide our electric power, control our automobiles and airplanes, and provide medical care.  It is essential that we do all we can to ensure that these cyber-physical systems are safe and secure.  Due to the growth in the complexity of software residing inside modern cyber-physical systems, the costs of verification and validation now dominate the overall development cost.  Thus, verification and validation are expected to become a limiting constraint on future systems unless we are willing to accept lower levels of safety and security. This project addresses this problem, by contributing to the development of formal verification tools that can not only reduce the cost of verification, but also find errors earlier in the design cycle to reduce overall development cost and increase assurance.<br/><br/>The goal of the project is to make transformative, rather than evolutionary, progress in the field of formal verification of hybrid dynamical systems. The project research activities include development of new formal verification techniques that are designed specifically for cyber-physical systems, exploration of novel ways to integrate them, and implementation and evaluation of their effectiveness. Novel ideas explored in this project include: (1) abstracting the dynamics, rather than just the state space of the system; (2) abstracting the property and initial sets, but not the state space and dynamics of the system; (3) observing that time acts as glue that combines any decomposition of a hybrid system, to develop compositional verification techniques that exchange information only pertaining to the amount of time elapsed; (4) using machine learning on simulation traces to develop a scalable approach for solving constraints that arise when automating the new verification techniques."
431,1422018,TWC: Small: CrowdVerify: Using the Crowd to Summarize Web Site Privacy Policies and Terms of Use Policies,CNS,"Special Projects - CNS, Secure &Trustworthy Cyberspace",10/1/14,3/24/16,Jason Hong,PA,Carnegie-Mellon University,Standard Grant,Dan Cosley,9/30/17,"$515,290.00 ",,jasonh@cs.cmu.edu,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,CSE,"1714, 8060","7434, 7923, 9178, 9251",$0.00 ,"Everyday web users have little guidance in handling the growing number of privacy issues they face when they go online. Many web sites - some legitimate, some less so - have behaviors many would consider unexpected or undesirable. These include popular and well-known web sites, as well as web sites that aim to dupe customers with ""free"" trials.  These kinds of sites often detail their behaviors in privacy policies and terms of use pages, but these policies are rarely read, hard to understand, and sometimes intentionally obfuscated with legal jargon, small text, and pale fonts. The goal of this research is to develop new techniques to pinpoint and summarize the most surprising and most important parts of policies. The results of this research will be made publicly available on a web site and through web browser extensions.<br/><br/>The major research activity for this research will be to design, implement, and evaluate CrowdVerify, a system that combines crowdsourcing with machine learning techniques to flag the most important and unexpected behaviors of web sites. The core idea is to slice up a given policy into smaller text segments, have crowd workers compare different segments, and then aggregate the results together. A number of competitor scoring systems will also be evaluated for rating the importance of segments, including ELO, Glicko, and TrueSkill. Using these results, computational models will be built that can predict what people find most surprising as well as most important in web policies."
432,1343363,EARS: A TV Whitespace Communication System for Connected Vehicles,CNS,Information Technology Researc,1/1/14,9/11/13,Suman Banerjee,WI,University of Wisconsin-Madison,Standard Grant,Monisha Ghosh,12/31/18,"$963,247.00 ","Xiaojin Zhu, Xinyu Zhang",suman@cs.wisc.edu,21 North Park Street,MADISON,WI,537151218,6082623822,CSE,1640,"7363, 7976",$0.00 ,"The project is focused on designing a holistic communication stack that leverages TV whitespaces for enabling ""connected vehicles"" and their many advantages in ushering smarter and safer transportation services. The combination of low cost of the unlicensed TV whitespace spectrum and its longer communication ranges matches the needs of vehicular connectivity quite well.  Yet realizing TV whitespace communications faces unique constraints and challenges including, e.g., protecting primary incumbents, scanning through many channel configurations subject to spatial-temporal variations, dealing with asymmetric uplink/downlink transmit power constraints due to FCC regulations concerning mobile and static nodes.  The proposed research addresses these challenges through a range of techniques. It augments spectrum sensing techniques from sensors distributed across moving vehicles to enhance the accuracy of spectrum databases. It employs innovative PHY and MAC layer techniques including variable spreading codes, MIMO and directional antennas, complemented with network/transport layer protocols that adapt to uplink/downlink asymmetry.  These research tasks integrate machine learning techniques to explore the predictable mobility and other unique advantages of the vehicular whitespace network. <br/><br/>This project is developing a new communication technology for improved and robust wide-area connectivity for vehicles.  All developed techniques are expected to be deployed in a practical setting on a realistic vehicular testbed in Madison, WI. The research work engages collaboration with many industry partners and is expected to influence regulatory and standardization bodies.  The researchers are also making educational impact through public lectures, developing new curriculum, and motivating women and minorities in future STEM careers through appropriate PI engagement at high school and undergraduate levels."
433,1363411,Designing Complex Engineering Systems using Multi-Agent Coordination Approaches,CMMI,SYS-Systems Science,8/1/14,7/18/14,Irem Tumer,OR,Oregon State University,Standard Grant,Georgia-Ann Klutke,7/31/18,"$200,000.00 ","Kagan Tumer, Christopher Hoyle",irem.tumer@oregonstate.edu,OREGON STATE UNIVERSITY,Corvallis,OR,973318507,5417374933,ENG,8085,"067E, 068E, 8024, 8043, 9102",$0.00 ,"The design of a growing class of complex systems, such as state-of-the-art aircraft, advanced power systems, unmanned aerial vehicles, and autonomous automobiles, requires the knowledge of many experts. But though each expert may be able to design a particular subsystem efficiently, the combination of those ""optimal"" subsystems does not generally lead to a ""good"" system-level design.  This is because the interactions among these subsystems are critical to the performance of the complex system, and capturing the interactions becomes increasingly difficult as the systems become more and more complex. The long-term goal of this project is to address this design-decomposition problem by deriving designer-subsystem objective functions in such a way that, when all subsystem designers achieve their objectives, the overall design goals are also achieved. <br/><br/>Traditionally, two broadly different approaches are used to design complex engineered systems:  (i) a centralized design approach where the impacts of all potential system states and behaviors resulting from design decisions must be accurately modeled. (ii) a mediated design approach where externally imposed intermediate design targets are provided to determine the tradeoffs among competing designs. The mediated approach avoids the modeling difficulties of the centralized approach, but at the cost of expensive (in time and money) mediation mechanisms.  The project's approach is a hybrid of the two approaches, providing a method in which decisions can be reconciled without the need for either detailed interaction models or arbitrary design targets.  A key insight on which this project will build is that complex systems design is fundamentally similar to the multiagent coordination problem, that lies at the intersection of game theory, machine learning, and control theory. In both instances, the decisions at the lower level (subsystems or agents) and the interactions at that level, lead to global behavior (complex system or multiagent system.)  In this project, designer-subsystem pairs will be aligned with agents. Corresponding designer-subsystem objectives will then be derived based on system-level objectives while implicitly accounting for subsystem interactions. These objectives will replace the traditional intermediate design targets with principled objectives that, if met, ensure desired overall system performance. This project will aim to show that an approach based on objective functions, previously successfully applied to the operation of complex multiagent systems, can in a modified form also serve as an approach for the design of complex engineered systems."
434,1344613,SCH: INT: Supporting Healthy Sleep Behaviors through Ubiquitous Computing,IIS,Smart and Connected Health,1/1/14,9/13/13,Julie Kientz,WA,University of Washington,Standard Grant,Sylvia Spengler,12/31/18,"$1,384,217.00 ","James Fogarty, Nathaniel Watson, Carol Landis",jkientz@uw.edu,4333 Brooklyn Ave NE,Seattle,WA,981950001,2065434043,CSE,8018,"8018, 8062",$0.00 ,"Sleep is one of the key aspects of good health, along with a healthy diet and regular exercise. Computing researchers have recently worked to understand how systems can support nutrition and exercise, but sleep has been relatively under-studied despite its significant health benefits. The right amount of quality sleep can improve both physical and mental health and is associated with a lower risk for heart disease, diabetes, depression, and obesity. However, sleep disorders are often undiagnosed, and many people are unaware of how their activities or environments affect sleep. Ubiquitous computing has the opportunity to help through self-monitoring, awareness, and identification of strategies to promote healthy sleep behaviors.<br/><br/>This interdisciplinary research agenda will involve the design, development, and evaluation of novel ubiquitous computing approaches to support good sleep health and behaviors. This research will combine expertise in human-centered design, computer science, sleep medicine, and nursing. The researchers' previous formative work with target users and sleep experts has informed design requirements for technologies in this field. The work will focus on building on those results through three main activities.  First, they will apply machine learning to model sleep patterns based on a person's smartphone usage to unobtrusively sense and predict sleep duration and timing.  Then they will employ a human-centered design process to develop and study the feasibility and initial efficacy of two novel software tools to assist individuals in sensing, recording, and visualizing the behavioral (e.g., caffeine use, food intake) and environmental factors (e.g., noisy environment, light levels) that can disrupt their sleep.  And then, they will develop and assess the feasibility and initial efficacy of a new technique and tool for assessing, modeling, and visualizing the impact of sleep deprivation on users' reaction time, cognitive functioning, and mood to help them prioritize sleep.<br/><br/>This research will bring into focus the domain of sleep as a new area for human-centered computing research. The design and evaluation of new applications for sleep will further knowledge of how technology can be designed for long-term health tracking and behavior change, and the designs and evaluations move beyond what is currently being addressed in industry. The technical contributions are novel approaches to monitoring sleep and an expansion of knowledge about how technologies can adapt to meet the unique health needs of different users. Finally, the research seeks to unite the fields of sleep research and computing research to develop solutions for better understanding and treating sleep disorders.<br/><br/>This work has the potential to significantly affect the lives of the estimated 40.6 million individuals in the U.S. with sleep disorders or sleep deprivation, which helps address a major public health issue. In addition, the economic cost of sleep deprivation has been estimated to be $63.8 billion per year. The research will have immediate impact by allowing free access to new behavior change technologies developed through this project. In addition, the research will also impact education by using sleep research as a means for attracting women and minorities to computing research and engaging students in interdisciplinary design teams through student projects and directed research groups."
435,1503177,CAREER: Cooperative Motion Planning for Human-Operated Robots,IIS,"CAREER: FACULTY EARLY CAR DEV, Robust Intelligence",8/15/14,7/6/16,Kris Hauser,NC,Duke University,Continuing grant,Reid Simmons,9/30/18,"$378,465.00 ",,kkhauser@illinois.edu,"2200 W. Main St, Suite 710",Durham,NC,277054010,9196843030,CSE,"1045, 7495","1045, 7495",$0.00 ,"This proposal outlines a research and educational plan to advance decision-making techniques for robots that cooperate with human operators. Because humans far exceed the abilities of state-of-the-art robots in vision, creativity, and adaptability, interest is rapidly growing in a human-centered approach to robotics: combining the strengths of humans with the superior precision and repeatability of robots. And yet, our available motion planning tools, while powerful at computing motions for complex autonomous tasks, are poorly suited for human-centered applications that demand responsive and natural motions. This proposal hypothesizes that a new cooperative motion planning paradigm will support major advances in intuitiveness and task performance of human-operated robots such as intelligent vehicles, tele-surgery systems, search and-rescue robots, and household robots. This hypothesis is echoed in an educational plan that aims to train engineers with cross-disciplinary strengths that bridge both the technical and social dimensions of robotics. Initial human subjects studies on novice operators with the PI's cooperative motion planning algorithms suggest that the technique leads to dramatic reductions in task completion time and collision rate in cluttered environments. The proposed work will conduct further investigations along this line of research to 1) identify characteristics of cooperative planners - such as optimality, responsiveness, and completeness - that yield effective human-operator systems, both in terms of objective performance metrics and subjective preferences, 2) to design planners that optimize cooperativity metrics under computational resource and communication constraints, and 3) to enhance the capabilities of such planners to assist operators in complex manipulation tasks.<br/><br/>The planners developed in this research and the rich datasets acquired via user studies will serve as resources to help human-robot interaction (HRI) researchers design safe and socially acceptable robot behaviors. Moreover, advances in cooperative motion planning may have long-term social and economic impact by enabling new applications of robotics in driver assist systems, space exploration, medicine, household robotics, manufacturing, and construction. Research is integrated with education in a range of activities that include CS curriculum development, development of a new graduate course on optimization and machine learning, and in new software libraries for robotics education. New modules on motion planning, behavior recognition, and HRI will be incorporated in AI and robotics courses. An REU is requested for each summer of the grant and will be recruited from a minority-serving institution in cooperation with the Alliance for the Advancement of African-American Researchers in Computing (A4RC). One or more IU undergraduates will be involved in research and mentored according to the Undergraduate Research Opportunities in Computing (UROC) program, with preference given to minority and women students."
436,1352013,CAREER: Stochastic processes and embeddings on networks,DMS,"PROBABILITY, Division Co-Funding: CAREER",8/1/14,9/8/16,Allan Sly,CA,University of California-Berkeley,Continuing grant,Tomek Bartoszynski,9/30/17,"$269,770.00 ",,allansly@princeton.edu,Sponsored Projects Office,BERKELEY,CA,947101749,5106433891,MPS,"1263, 8048",1045,$0.00 ,"Random processes on networks play a major role in a wide range of areas including mathematical physics, machine learning, and theoretical computer science. One such question we will address involves analysing algorithms for detecting sub-communities within a social network by investigating the graph of friendship relations. The proposal addresses questions from combinatorics and computer science about when random combinatorial and computational problems have solutions, such as when there is a colouring in a random graph.  Ideas from statistical physics give predictions for these thresholds which we will mathematically prove.  It also studies questions of how long it takes random processes on networks, such as the Glauber dynamics Markov chain, to reach their equilibrium distribution and how this depends on their initial starting position.  Such processes are used as algorithms for sampling high dimensional distributions.  The proposal focuses on development of mathematical theory with a view to better understanding these problems.  Finally the proposal will support the development of new graduate courses in discrete probability and stochastic process on random graphs as well as providing research opportunities for graduate and undergraduate researchers.<br/><br/>The main theme of this proposal is the development of new theory and applications across a range of stochastic processes on networks. One aspect involves studying phase transitions of Gibbs measures on random graphs, particularly random constraint satisfaction problems.  Here we hope to establish conjectures from statistical physics for a range of models such as the chromatic and independence numbers on random graphs.  A second theme is the development of new tools for establishing rough isometries and other geometric notions of closeness between random metric spaces.  In particular we will consider whether independent copies of Poisson processes, percolation clusters and SLE curves are rough isometries or quasi-symmetries.  Finally the proposal will consider Markov random fields such as the Ising model on lattices.  At high temperatures it will consider the question of universality of the cutoff phenomena as well as the effect of different initial conditions on the mixing time.  At low temperatures it will pursue a better understanding of Ising interfaces in order to establish rapid mixing results."
437,1443133,Spectral Algorithms: From Theory to Practice,CCF,SPECIAL PROJECTS - CCF,9/1/14,5/2/14,Richard Karp,CA,University of California-Berkeley,Standard Grant,Tracy Kimbrel,8/31/15,"$30,000.00 ",,karp@cs.berkeley.edu,Sponsored Projects Office,BERKELEY,CA,947101749,5106433891,CSE,2878,"7796, 7926",$0.00 ,"The workshop on ""Taking Spectral Algorithms from Theory to Practice"" is to be held at the Simons Institute for the Theory of Computing, Berkeley, CA, on October 27-31, 2014.  The workshop will explore the applicability of recent developments in spectral graph theory to graph partitioning problems that arise in areas such as image segmentation, machine learning, and scientific computing.  <br/><br/>The funds ($30,000) will be used to support the participation of approximately 25 attendees in the workshop.  Airfare and lodging will be reimbursed.  Participation by women and members of underrepresented minorities will be a factor in the selection."
438,1451081,"BRAIN EAGER: Discovery and characterization of neural circuitry from behavior, connectivity patterns and activity patterns",DBI,"ADVANCES IN BIO INFORMATICS, Activation",9/1/14,8/18/14,Carey Priebe,MD,Johns Hopkins University,Standard Grant,Peter McCartney,8/31/16,"$300,000.00 ",,cep@jhu.edu,1101 E 33rd St,Baltimore,MD,212182686,4439971898,BIO,"1165, 7713","7916, 8091",$0.00 ,"Johns Hopkins University is awarded a grant for research leading to an improved understanding of how the brain is connected. Drosophila larvae, with 10000 neurons (and about 1000 neuron types), offer an opportunity to determine how an entire nervous system generates behavior.  The research combines information from three sources: a neuron activity map of the entire Drosphila larval  nervous system; a library of neuronal lines yielding a neuron behavior map; and a ""wiring diagram"" or connectome for the entire larval nervous system. Together, the neuron-behavior map, the neuron-activity map, and the connectome complement one another, laying the groundwork for a brain-wide understanding of the principles by which brains generate behavior.<br/><br/>The technical goals for this project will be to develop principled statistical pattern recognition & machine learning methods for clustering neurons based on three different data sets, both individually and jointly. The extent to which clusters obtained from the three datasets agree, and the manner in which they disagree, will reveal how the structure of neural circuits relates to their function and how brains generate behavior. Current methods for discovery and characterization of neural circuitry from behavior, connectivity patterns and activity patterns - fusion and inference from multiple disparate data sources- are insufficient; the approach developed in this project will yield improved methods developed in conjunction with neuroscientists."
439,1409612,Computer Science meets Anthropology: A novel approach for reconstructing locomotion from fossil human footprints,SMA,SPRF-IBSS,8/1/14,5/9/14,Kevin Hatala,DC,George Washington University,Standard Grant,Josie S. Welkom,7/31/16,"$202,474.00 ","Stephen Gatesy, Brian Richmond, Gabriel Sibley",k.hatala@chatham.edu,1922 F Street NW,Washington,DC,200520086,2029940728,SBE,8209,,$0.00 ,"Bipedal locomotion is a unique and fundamental behavior that has driven and shaped the evolution of the human species. Yet the evolutionary history of human bipedalism remains poorly understood. This project is the first to apply interdisciplinary techniques to understand the evolution of bipedalism. The PI also engages the broader public in this research through ongoing collaborations with the Smithsonian Institution's Human Origins Program. Results of this research, and its implications for understanding the evolution of our species, are presented through regular lectures and participation in educational programs at the museum's Hall of Human Origins. The PI and co-PIs recruit and train research assistants from The George Washington University's Trachtenberg Scholars Program. This program supports the education and training of promising students from the District of Columbia public school system, who typically come from groups historically underrepresented in STEM disciplines. Consistent efforts are also made to disseminate results to a diverse range of professional audiences in biological anthropology, evolutionary biology, and computer science. <br/><br/>In recent years, discoveries of several sites preserving fossil human footprints have offered an exciting new form of data from which to understand the evolution of bipedalism. However, novel techniques are required to analyze the wealth of information contained within these diverse sources of data. This project is transformative in its experimental approach for linking footprint morphologies to the patterns of locomotion that produced them. Experimental biomechanics will be bridged with techniques for robotic vision, to understand how details of locomotion are recorded by 3-D footprint morphology. Those data are used to develop powerful machine learning algorithms, applying technologies from a rapidly-growing field at the intersection of computer science and statistics, to build detailed reconstructions of locomotion from fossil human footprints. By doing so, this project sheds new light on the evolutionary history of the human species."
440,1345813,SBIR Phase I: Autonomous Integration of EMR Information for Patient-Centric Open API Access,IIP,SMALL BUSINESS PHASE I,1/1/14,11/27/13,Anil Sethi,CA,Gliimpse LLC,Standard Grant,Jesus Soriano Molla,6/30/14,"$149,818.00 ",,anil@gliimpse.com,645 Harrison St,San Francisco,CA,941070000,4105991529,ENG,5371,"5371, 8018, 8032, 8038, 8039, 8042",$0.00 ,"This Small Business Innovation Research (SBIR) Phase I project proposes to develop a tool for autonomous capture, translation, and integration of proprietary clinical information from Electronic Medical records into a standard public format. The project addresses ongoing healthcare regulations such as the PPACA which require doctors to provide patient access to medical data through electronic means, as well as efficiently share with other providers. Due to the nature of existing Electronic Medical Record systems, data sharing is difficult and discouraged. . Further an Open API will be provided to allow authorized third parties to access such data. Research will focus on data capture, parsing, and machine learning techniques to ensure accuracy and performance. It is expected that multiple methods will be evaluated and at least one method will be implemented in a prototype system. <br/><br/>The broader impact/commercial potential of this project will enhance the capability of patients to control their own medical data, its sharing, who has access, and analysis by hospitals and research institutions for ongoing medical research. By placing control of the patient?s data in the hands of the patient, they can choose to keep or share any or all medical data to any provider, or for research purposes. The rapid extraction and integration of data from multiple sources and provision of an open API will remove medical data from the control of vendors with no economic incentive to ensure data sharing. By allowing a single platform for integration, interrogation, and sharing, research and other software platforms can be built on top of the API to enhance medical research, patient awareness, and physician awareness of all data sources, including hospitals, other doctors, third party applications, and the growing use of ?Quantified Self? measurement devices. Significant increases in efficiency for provider use and reduced expenditures on custom coded EMR solutions are anticipated, eliminating the requirement for millions of dollars of consulting, and improved patient care through efficient use of doctor time and provision of relevant information."
441,1346113,SBIR Phase I: Low-cost Time-domain Reflectometer for Soil Water Content Reporting in Precision Agriculture,IIP,SMALL BUSINESS PHASE I,1/1/14,12/4/13,Scott Anderson,ID,"Acclima, Inc.",Standard Grant,Muralidharan S. Nair,6/30/14,"$150,000.00 ",,scott@acclima.com,1763 W Marcon Lane #175,Meridian,ID,836428203,2088871470,ENG,5371,"1185, 5371, 8035, 9139, 9150, HPCC",$0.00 ,"This Small Business Innovation Research (SBIR) Phase I project will develop new technology for the separation of ultra-fast-rise-time incident and reflected step functions needed for the development of an integrated differential Time Domain Reflectometer (TDR) to be used in soil water content measurement. Because of residual reflections from circuit board striplines terminated in variable soil loads and reflections from wetting fronts and discontinuities in the soil, the derivation of propagation time, permittivity and water content are subject to errors caused by the vector addition of the spurious, shorter-term reflections with the main reflection from the end of the waveguide. The characteristics of the interfering spurious signals vary with the soil environment. The overall waveform appearing at the TDR digitizer is a composite image of an incident step, a reflected wave and the set of spurious signals. The research required is that of characterizing the spurious signal patterns and of developing machine learning and signal processing algorithms to remove their impact from the derivation of reflected wave propagation time. Identification and timing of wetting fronts will also be included to provide sensor users with infiltration monitoring capability. <br/><br/>The broader impact/commercial potential of this project is to provide an affordable and easily deployed means of accurately measuring soil water content, including the credible measurement of water uptake by food crops. This capability will facilitate significant increases in water use efficiency - the foundation for growing high quality food with less water. The value proposition to crop growers is reduced marginal costs and increased crop yield and quality, with a potential payback time of one to two growing seasons. Commercial adoption of soil moisture sensing technology will help manage the increased cost and demand for fresh water, of which over 70% of consumption goes to agriculture. The technology will also provide a stable and credible measurement tool for studying and monitoring watershed dynamics for flood and drought prediction, for monitoring dams, mudslide areas and levees for potential failure and for studying the impact of soil water dynamics on weather. Current tools for these measurements are either heavily regulated (neutron probe), difficult to install (Time Domain Transmissometer probe) or provide unstable readings with changing soil electrical conductivity and compaction (capacitive probes)."