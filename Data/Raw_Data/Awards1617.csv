"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"1619362","RI: AF: Small: Deep Learning Theory","IIS","Robust Intelligence, Algorithmic Foundations","07/01/2016","06/10/2016","Peter Bartlett","CA","University of California-Berkeley","Standard Grant","Rebecca Hwa","12/31/2019","$490,000.00","","bartlett@stat.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","7495, 7796","7495, 7923, 7926","$0.00","Deep learning has recently emerged as a major advance in machine learning and AI.  This technology for learning from data has provided field-changing performance improvements in image classification and speech recognition, it has displayed impressive performance across a large variety of areas (including natural language processing, robotics, audio processing, and computational chemistry), and it has become a central ingredient in AI systems.  But despite these successes, our understanding of these methods is incomplete. The broad goal of this research project is to address this grand challenge: to develop analysis techniques that enable us to understand when and why deep learning methods will be successful, and to design effective methods with explicit performance guarantees.  Successful research outcomes have a significant potential for practical impact in the large and growing set of application areas where these methods are used.<br/><br/>The project aims to understand the performance of deep learning methods - in particular to elucidate what aspects are essential for their success - and hence to develop principled design techniques and performance guarantees.  The objectives are: to characterize the performance impacts of the critical features of current neural network architectures: scale, depth, nonlinearities, and regularization; to develop analysis techniques that facilitate our understanding of the approximation and estimation properties of deep architectures; to identify the boundary between easy and hard learning problems for deep networks; and to develop methods with explicit performance guarantees for optimization in deep neural networks.  Successful research outcomes are likely to increase our understanding of deep learning methods, to provide performance guarantees for these methods, and to facilitate the principled design of novel deep learning methods."
"1648728","I-Corps: Artificial Intelligence and Deep Learning System for Product Search","IIP","I-Corps","08/15/2016","08/08/2016","Carol Mimura","CA","University of California-Berkeley","Standard Grant","Steven Konsek","01/31/2017","$50,000.00","","carolm@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project are in various applications in ecommerce, security, robotics, etc. This project's technology enables computer systems and their users to better contextually understand products and locate them. For example, in e-commerce, this technology enables people to make informed decisions about product selection. This may enhance the e-tail experience and help to eliminate wasted time as people can easily and precisely find things they desire. The aggregated data on product searches can be used by suppliers to better understand and plan for demand. Also, in robotics, as another example, computer vision is an essential element of the technology and the multi-object detection system built into software will enable robots to be more effective as they will ""understand"" products or objects in the context of our world. In security, this technology will enable better detection of theft and threats. Overall, the technology has many applications - many that can make lives more convenient and safer.<br/><br/>This I-Corps project is based on machine learning technology.  Billions of data points that include images and textual information about products are trained into artificial neural networks. These neural networks understand products with context with both images and natural language. Further, the neural networks are able to self-learn with new information from the internet. This technology was developed after researching various machine learning methods for large scale objection detection and search. The result is that neural networks provide the most scalable and efficient technology in terms of object detection. The uniqueness of this system is that parts of the learning in the neural network can be changed without affecting the rest of the network and the network can self-learn from the internet about products and other choice-centric decisions."
"1618485","RI: Small: An Optimization Framework for Understanding Deep Networks","IIS","Robust Intelligence","07/01/2016","05/08/2018","Rene Vidal","MD","Johns Hopkins University","Standard Grant","Rebecca Hwa","06/30/2020","$458,000.00","","rvidal@cis.jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","CSE","7495","7495, 7923, 9251","$0.00","The past few years have seen a dramatic increase in the performance of pattern recognition systems due to the introduction of deep neural networks. However, the mathematical reasons for this success remain elusive. A key challenge is that the problem of learning the parameters of a neural network is a non-convex optimization problem, which makes finding the globally optimal parameters extremely difficult. Another challenge is that there is currently very limited theory about how the network architecture should be constructed (i.e., number of layers, number of neurons per layer, connectivity patterns, etc.). The goal of this project is to develop an optimization framework that provides theoretical insights for the success of current network architectures and guides the design of novel architectures with guarantees of global optimality.<br/> <br/>This project will develop a mathematical framework for the analysis of a broad class of non-convex optimization problems, including matrix factorization, tensor factorization, and deep learning. In particular, this project will study the problem of minimizing the sum of a loss function and a regularization function, both of which can be non-convex, but should satisfy a certain ""positive homogeneity"" property. By properly designing positively homogeneous regularizers that constrain the ""network size,"" this project aims to show that, under certain conditions, all local minima are globally optimal, and one can find a global minimum from any initialization using a local descent strategy. A deeper understanding of the mathematical properties of deep networks will impact not only machine learning and optimization, where our understanding of non-convex problems continues to be very limited, but also application areas such as computer vision, speech and natural language processing, where deep networks currently give state-of-the-art results."
"1614024","EAPSI: Generating Word Embeddings using Extreme Learning Machines for Classifying Clinical Texts","OISE","EAPSI","06/15/2016","07/08/2016","Paula Lauren","MI","Lauren                  Paula","Fellowship Award","Anne Emig","05/31/2017","$5,400.00","","","","Royal Oak","MI","480735310","","O/D","7316","5927, 5978, 7316","$0.00","In 1950, the computer scientist Alan Turing proposed a test for true artificial intelligence. In Turing's view, a computer must be considered intelligent if it could understand human language.  After more than 60 years of research, this is still an ongoing effort. Recent methods, including neural language models that use advanced statistics, have made great strides towards realizing Turing's vision.  This study builds on existing research to explore methods for improving computer-based natural language understanding.  The research will be conducted under the mentorship of Professor Guang-bin Huang, a noted expert on machine learning, of Nanyang Technological University.<br/><br/>Natural language processing (NLP) involves the development of computer-based algorithms to understand natural language. Statistical language models are typically used for various NLP tasks, including machine translation and text categorization.  Language models based on neural networks, also known as neural embeddings, map words (or phrases) to a numerical representation in a low-dimensional space. Typically, neural networks use back-propagation for training a neural network, which results in slow training. Extreme Learning Machines (ELM) is a type of neural network, where hidden neurons are randomly generated hidden nodes. This study involves the use of ELM for faster training in the generation of neural embeddings.<br/><br/>This award under the East Asia and Pacific Summer Institutes program supports summer research by a U.S. graduate student and is jointly funded by NSF and the National Research Foundation of Singapore."
"1614653","RI: Small: Model-Based Deep Reinforcement Learning for Domain Transfer","IIS","Robust Intelligence","09/01/2016","06/20/2016","Sergey Levine","WA","University of Washington","Standard Grant","Weng-keen Wong","11/30/2016","$479,279.00","","sergey.levine@gmail.com","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7495","7495, 7923","$0.00","The goal of this project is to develop machine learning algorithms that can enable automated decision making and control in applications that require autonomous agents to interact with the real world. In particular, the project will examine two application areas: autonomous robots and educational agents that interact with human students to facilitate learning. The principal technical development investigated in this project will center around applications of deep neural networks (deep learning) to efficiently learn predictive models of the world, such as the physical environment of the robot or the behavior of a human student using an interactive educational agent. Deep learning has enabled impressive advances in passive perception domains such as computer vision and speech recognition, but typically requires very large amounts of data to succeed. This is often a major challenge in interactive settings, where a robot cannot interact with its environment for weeks or months just to learn a single behavior. To address this challenge, this project will investigate how predictive models can be transferred from prior tasks into a new task. The technologies developed as part of this project could enable substantially more sophisticated autonomous systems that can adapt quickly to new situations through transfer. Economic impact could include new consumer robotics products and improved education through intelligent automation.<br/><br/>Reinforcement learning holds the promise of automating complex decision making and control in the presence of uncertainty. For a wide range of real-world problems, from robotic control and autonomous vehicles to interactive educational tools, this would provide dramatic improvements in capability and reduction in engineering cost. However, applying reinforcement learning to complex, unstructured environments and real-world problems with raw inputs, such as images and sounds, remains tremendously difficult. Deep learning has shown a great deal of promise for tackling complex learning problems, especially ones that require parsing high-dimensional, raw sensory signals, but the most successful applications of deep learning use very large amounts of labeled data. This is at odds with the demands of reinforcement learning, where the goal is typically to learn an effective policy using the minimal amount of interaction. This projects aims to address this challenge by developing algorithms for model-based deep reinforcement learning, where a generalizable model is learned from past experience on related but different tasks, and then transferred to a new task to learn it very quickly, directly using raw sensory inputs."
"1700697","RI: Small: Model-Based Deep Reinforcement Learning for Domain Transfer","IIS","Robust Intelligence","09/01/2016","10/24/2016","Sergey Levine","CA","University of California-Berkeley","Standard Grant","Rebecca Hwa","08/31/2020","$479,279.00","","sergey.levine@gmail.com","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","7495","7495, 7923","$0.00","The goal of this project is to develop machine learning algorithms that can enable automated decision making and control in applications that require autonomous agents to interact with the real world. In particular, the project will examine two application areas: autonomous robots and educational agents that interact with human students to facilitate learning. The principal technical development investigated in this project will center around applications of deep neural networks (deep learning) to efficiently learn predictive models of the world, such as the physical environment of the robot or the behavior of a human student using an interactive educational agent. Deep learning has enabled impressive advances in passive perception domains such as computer vision and speech recognition, but typically requires very large amounts of data to succeed. This is often a major challenge in interactive settings, where a robot cannot interact with its environment for weeks or months just to learn a single behavior. To address this challenge, this project will investigate how predictive models can be transferred from prior tasks into a new task. The technologies developed as part of this project could enable substantially more sophisticated autonomous systems that can adapt quickly to new situations through transfer. Economic impact could include new consumer robotics products and improved education through intelligent automation.<br/><br/>Reinforcement learning holds the promise of automating complex decision making and control in the presence of uncertainty. For a wide range of real-world problems, from robotic control and autonomous vehicles to interactive educational tools, this would provide dramatic improvements in capability and reduction in engineering cost. However, applying reinforcement learning to complex, unstructured environments and real-world problems with raw inputs, such as images and sounds, remains tremendously difficult. Deep learning has shown a great deal of promise for tackling complex learning problems, especially ones that require parsing high-dimensional, raw sensory signals, but the most successful applications of deep learning use very large amounts of labeled data. This is at odds with the demands of reinforcement learning, where the goal is typically to learn an effective policy using the minimal amount of interaction. This projects aims to address this challenge by developing algorithms for model-based deep reinforcement learning, where a generalizable model is learned from past experience on related but different tasks, and then transferred to a new task to learn it very quickly, directly using raw sensory inputs."
"1618477","RI: Small: Unraveling and Building Top-Down Generators in Deep Convolutional Neural Networks","IIS","Robust Intelligence","07/01/2016","06/28/2016","Zhuowen Tu","CA","University of California-San Diego","Standard Grant","Kenneth Whang","06/30/2020","$449,999.00","","zhuowen.tu@gmail.com","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7495","7495, 7923, 8089","$0.00","Deep learning has recently significantly advanced research fields that are closely related to artificial intelligence. The fundamental problem of knowledge representation however remains open and the role of top-down process in deep learning is yet not very clear. For example, to train a deep learning algorithm to detect simply the translation of a dog in an image, a data-driven way of training deep learning would require generating thousands of samples by moving the dog around in the image. However, a top-down model, if available, can directly detect translation using two variables along the axes. The main goal of this project is to explore a path to discover, learn, and build embedded deep learning models, accounting for a rich family of top-down spatial transformation and geometric composition in convolutional neural networks. The resulting models provide a transparent way of understanding the embedded top-down transformation process through neural network layers. The learned neurally-inspired top-down knowledge representation will benefit studies across multiple disciplines, including visual perception, brain sciences, cognitive modeling, and decision making. <br/><br/>The current practice in deep learning, for example convolutional neural networks (CNN), is largely dominated by data-driven bottom-up approaches. While the performances of various applications using convolutional neural networks (CNN) are impressive, there nevertheless exists a big gap between what bottom CNN can offer and what comprehensive intelligence requires. These strongly bottom-up CNN characteristics leave a big room for one to provide deep learning with the ability to also incorporate top-down information for effective knowledge representation, network learning, cognitive modeling, and visual inference. This project is about building a roadmap towards developing top-down generators. This is done by unraveling the role of explicit top-down knowledge representation and propagation, by studying the feature flows produced inside the convolutional neural networks, by building robust analysis-by-synthesis methods that combine top-down and bottom-up processes, and by creating explicit generative models to assist a wide range of applications. The benefit of studying the top-down generators to a broad family of applications is greatly intriguing, including but not limited to: creating network internal data augmentation, building object detection, developing scene understanding systems; modeling compositional and contextual object configurations; and performing zero-shot learning."
"1613002","Canonical Linear Methods and Hierarchical Non-Linear Methods in High-Dimensional Statistics","DMS","STATISTICS","07/01/2016","06/27/2019","Bin Yu","CA","University of California-Berkeley","Continuing Grant","Gabor Szekely","06/30/2021","$600,000.00","","binyu@stat.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1269","7433, 8083","$0.00","Statistics is at the heart of extracting meaningful information from big data. Its primary tasks include estimation and uncertainty assessment. The latter is crucial in big data analysis for sound decision making. For the former, the methods employed in deep learning machines, such as those behind Google's Brain and AlphaGo and Microsoft's Cortana, beg understanding. This research project is intended to bridge practice and theory of statistics in these areas. It aims to provide accessible uncertainty measures for linear modeling of big data and to derive insights into how deep learning works, based on mathematical analysis. <br/><br/>This research project develops and analyzes linear and non-linear high-dimensional statistical inferential methods that are easily accessible by practitioners in data science. In the linear case, it develops and analyzes inferential methods based on well-established bootstrap, lasso, partial ridge, and random projection methods. In the non-linear case, it takes the first steps to explain in a principled manner the impressive success of deep learning in practical problems such as image classification and speech recognition. In particular, statistical properties of these methods will be studied under linear and Neyman-Rubin high dimensional models, and via analytical and simulation means. A generative model of a two-layer neural network (or hierarchical non-linear model) will be explored to understand and compare deep learning with other methods, analytically and through simulation studies. Improvements over deep learning as a general supervised learning method are sought by enforcing biologically meaningful constraints from brain connectivity research."
"1555079","CAREER: Biologically inspired neural network models for robust speech processing","IIS","Robust Intelligence","06/01/2016","07/22/2020","Nima Mesgarani","NY","Columbia University","Continuing Grant","Kenneth Whang","05/31/2021","$502,210.00","","nm2764@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","7495","1045, 7495, 8091","$0.00","The recent parallel breakthroughs in deep neural network models and neuroimaging techniques have significantly advanced the current state of artificial and biological computing. However, there has been little interaction between these two disciplines, resulting in simplistic models of neural systems with limited prediction, learning and generalization abilities. The goal of this project is to create a coherent theoretical and mathematical framework to understand the computational role of distinctive features of biological neural networks, their contribution to the formation of robust signal representations, and to model and integrate them into the current artificial neural networks. These new bio-inspired models and algorithms will have adaptive and cognitive abilities, will better predict experimental observations, and will advance the knowledge of how the brain processes speech. In addition, the performance of these models should approach human abilities in tasks mimicking cognitive functions, and will motivate new experiments that can further impose realistic constraints on the models. <br/><br/>This interdisciplinary project lies at the intersection of neurolinguistics, speech engineering, and machine learning, uniting the historically separated disciplines of neuroscience and engineering. The proposed innovative approach integrates methods and expertise across various disciplines, including system identification, signal processing, neurophysiology, and systems neuroscience. The aim of this proposal is to analyze and transform the artificial neural network models to accurately reflect the computational and organizational principles of biological systems through three specific objectives: I) to create analytic methods that can provide insights into the transformations that occur in artificial neural network models by examining their representational properties and feature encoding, II) to model and implement the local, bottom-up, adaptive neural mechanisms that appear ubiquitously in biological systems, and III) to model the top-down, knowledge driven abilities of cognitive systems to implement new computations in response to the task requirements. Accurate computational models of the neural transformations will have an overarching impact in many disciplines including artificial intelligence, neurolinguistics, and systems neuroscience. More realistic neural network models will not only result in human-like pattern recognition technologies and better understanding of how the brain solves speech perception, but can also help explain how these processes are impaired in people with speech and language disorders. Therefore, the proposed project will advance the state-of-the-art in multiple disciplines."
"1618662","RI: Small: Collaborative Research: On-Line Learning Algorithms for Path Experts with Non-Additive Losses","IIS","Robust Intelligence","09/01/2016","09/07/2016","Mehryar Mohri","NY","New York University","Standard Grant","Rebecca Hwa","08/31/2021","$275,000.00","","mohri@cims.nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","7495","7495, 7923","$0.00","On-line learning algorithms are increasingly adopted as the key solution to modern learning applications with very large data sets of several hundred million or billion points.  These algorithms process one sample at a time with an update per iteration that is typically computationally cheap and easy to implement. Additionally, these algorithms benefit from a rich theoretical foundation.  The objective of this research is to advance on-line learning by broadening its applicability to a variety of applications including machine translation, speech recognition, other natural language processing applications, handwriting recognition, computer vision, bioinformatics, and many other areas which can benefit the society. Expert skills and student talent will be combined to create effective theoretical and algorithmic solutions and open-source software tools tested experimentally that can benefit a wide community.<br/><br/>Most learning problems admit some structure. In such problems, experts can be viewed as paths in a directed graph with each edge corresponding to a sub-structure corresponding to a word, phoneme, character, or image patch.  Current on-line algorithms with path experts are limited to additive losses and therefore are not applicable in many important applications where the loss is non-additive. We will create the theoretical foundation for designing efficient on-line algorithms for learning with path experts with non-additive losses.  Such non-additive losses are the relevant loss functions in most important applications such as machine translation, speech recognition, pronunciation modeling, parsing, image processing and other areas.  Carefully designed weighted automata and semiring tools will be used to devise on-line algorithms for path experts with non-additive losses.  The theoretical analysis of the algorithms will be complemented by a thorough experimental evaluation in a variety of applications."
"1619271","RI: Small: Collaborative Research: On-Line Learning Algorithms for Path Experts with Non-Additive Losses","IIS","Robust Intelligence","09/01/2016","09/07/2016","Manfred Warmuth","CA","University of California-Santa Cruz","Standard Grant","Rebecca Hwa","08/31/2019","$175,000.00","","manfred@cse.ucsc.edu","1156 High Street","Santa Cruz","CA","950641077","8314595278","CSE","7495","7495, 7923","$0.00","On-line learning algorithms are increasingly adopted as the key solution to modern learning applications with very large data sets of several hundred million or billion points.  These algorithms process one sample at a time with an update per iteration that is typically computationally cheap and easy to implement. Additionally, these algorithms benefit from a rich theoretical foundation.  The objective of this research is to advance on-line learning by broadening its applicability to a variety of applications including machine translation, speech recognition, other natural language processing applications, handwriting recognition, computer vision, bioinformatics, and many other areas which can benefit the society. Expert skills and student talent will be combined to create effective theoretical and algorithmic solutions and open-source software tools tested experimentally that can benefit a wide community.<br/><br/>Most learning problems admit some structure. In such problems, experts can be viewed as paths in a directed graph with each edge corresponding to a sub-structure corresponding to a word, phoneme, character, or image patch.  Current on-line algorithms with path experts are limited to additive losses and therefore are not applicable in many important applications where the loss is non-additive. We will create the theoretical foundation for designing efficient on-line algorithms for learning with path experts with non-additive losses.  Such non-additive losses are the relevant loss functions in most important applications such as machine translation, speech recognition, pronunciation modeling, parsing, image processing and other areas.  Carefully designed weighted automata and semiring tools will be used to devise on-line algorithms for path experts with non-additive losses.  The theoretical analysis of the algorithms will be complemented by a thorough experimental evaluation in a variety of applications."
"1618193","RI: Small: Linguistic Semantics and Discourse from Leaky Distant Supervision","IIS","Robust Intelligence","08/01/2016","07/14/2020","Hal Daume","MD","University of Maryland College Park","Continuing Grant","Tatiana Korelsky","07/31/2020","$413,116.00","","hal@umiacs.umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","7495","7495, 7923, 9251","$0.00","This project studies novel algorithms for building artificial intelligence (AI) systems that can learn to improve their performance with a human in the loop. Many recent AI successes are driven by large, expensive and difficult-to-collect datasets. This yields systems that are deep, but narrow. The goal of this project is to build technology that will allow AI systems to learn from their interactions with people. The project focuses on key applications related to natural language understanding: building technology to understand the meanings of individual sentences, and integrate those meanings into the meaning of a discourse or dialog.  One specific application pursued herein relates to extracting biomedical knowledge from text, which will pave the way to helping biomedical researchers develop novel hypotheses.  The work will fund students from underrepresented groups in STEM, and encourage cross-disciplinary education at the graduate and undergraduate levels. Finally, the work will be communicated to the public not just with scientific papers, but internationally through social media and locally through visits to middle schools and high schools.<br/><br/>Natural language processing (and other fields of artificial intelligence) have had enormous success by training supervised  learning systems on large labeled datasets (""corpora"").  Unfortunately, curating such corpora is infeasible except for very specific problems. This happens either because it is too expensive, or it is too difficult to get human labelers to agree on an annotation standard.  Instead of relying solely on human labeled data, this project develops algorithms that can learn from human interaction.  These systems can continually improve their performance based on downstream performance supervision, often with a human in the loop. This work leverages recent developments on the structured contextual bandits learning framework which provides a theoretically grounded and computationally efficient way in which to develop novel approaches to distant supervision. This resulting learning techniques will push advances in natural language understanding: semantic parsing and discourse interpretation. Furthermore, the underlying imitation learning technology is broadly applicable, including novel applications to recurrent neural network models.  To aid adoption by the research community, code and data from this project will be released open source."
"1618044","RI: Small: Collaborative Research: Unsupervised Transcription of Early Modern Documents","IIS","Robust Intelligence","09/01/2016","06/03/2016","Taylor Berg-Kirkpatrick","PA","Carnegie-Mellon University","Standard Grant","D.  Langendoen","08/31/2020","$249,458.00","","tberg@eng.ucsd.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7495","7298, 7495, 7923","$0.00","Recently, researchers in the social sciences and humanities have made increasing use of digital technologies in their work, seeking to answer important questions about human artifacts based on new kinds of analyses. However, since many of their methods are statistical in nature, they require a large amount of digitally readable text to operate. For example, to ask statistical questions about how the legal rights of women have changed during the past five centuries, a large and unbiased sample of court proceedings spanning that time period has to be accessible in digital form. Unfortunately, for many time periods this data is not available, not because the historical documents have been lost, but because they cannot be efficiently transcribed. In particular, the 400 years just after the invention of the printing press (the early modern period, ca. 1450-1850) represents a critical dark period for such research because documents from this period are notoriously hard to transcribe into machine-readable text with automatic methods for three reasons: they use obscure and unknown fonts, their text differs from modern language, and historical printing processes were imprecise. This proposal seeks to address these issues by treating transcription as a type of code-breaking and using machine learning to induce font and text structure directly from unannotated document images without relying on annotated examples,  an approach called unsupervised learning. As a result, the proposal aims not just to digitize existing early modern corpora in major libraries, but also to produce a tool that researchers can use to digitize data at scale themselves and that is sufficiently flexible to develop new representations, for example, of non-standard character sets. <br/><br/>The proposed approach treats the problem of document transcription as a linguistic decipherment problem, leveraging modeling techniques from work on decrypting historical ciphers. The key idea is that while properties like font and text structure are document-specific and therefore difficult to treat generally with supervised techniques, these phenomena are in fact regular within individual documents. For example, while the shape of a particular character in an obscure historical font may be unknown to the system, that shape is in fact regular;  every time the character is printed it uses the same template. Models that leverage this kind of regularity by incorporating it as an assumption can constrain the otherwise difficult unsupervised learning problem and make it feasible. This proposal introduces a class of generative models with this goal in mind, designed to learn fonts and predict accurate transcriptions in an unsupervised fashion by capturing the core properties of the process that generated the input data: the historical printing process. These models represent the specific types of printing and typesetting noise exhibited by early modern documents, treat typesetting as a latent variable, and jointly consider possible character segmentations and transcriptions during inference. Their parameters can be estimated efficiently, directly from images of historical documents without accompanying transcriptions. Further, by treating damaged portions of the input documents as latent variables, this proposal aims to automatically reconstruct damaged documents using the same approach. The unsupervised techniques developed here may have uses in other areas of natural language processing where annotated training data is hard to obtain; for example, in personalized speech recognition and grounded semantics."
"1618460","RI: Small: Collaborative Research: Unsupervised Transcription of Early Modern Documents","IIS","Robust Intelligence","09/01/2016","06/03/2016","Dan Klein","CA","University of California-Berkeley","Standard Grant","D.  Langendoen","08/31/2020","$249,876.00","","klein@cs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","7495","7495, 7923","$0.00","Recently, researchers in the social sciences and humanities have made increasing use of digital technologies in their work, seeking to answer important questions about human artifacts based on new kinds of analyses. However, since many of their methods are statistical in nature, they require a large amount of digitally readable text to operate. For example, to ask statistical questions about how the legal rights of women have changed during the past five centuries, a large and unbiased sample of court proceedings spanning that time period has to be accessible in digital form. Unfortunately, for many time periods this data is not available, not because the historical documents have been lost, but because they cannot be efficiently transcribed. In particular, the 400 years just after the invention of the printing press (the early modern period, ca. 1450-1850) represents a critical dark period for such research because documents from this period are notoriously hard to transcribe into machine-readable text with automatic methods for three reasons: they use obscure and unknown fonts, their text differs from modern language, and historical printing processes were imprecise. This proposal seeks to address these issues by treating transcription as a type of code-breaking and using machine learning to induce font and text structure directly from unannotated document images without relying on annotated examples,  an approach called unsupervised learning. As a result, the proposal aims not just to digitize existing early modern corpora in major libraries, but also to produce a tool that researchers can use to digitize data at scale themselves and that is sufficiently flexible to develop new representations, for example, of non-standard character sets. <br/><br/>The proposed approach treats the problem of document transcription as a linguistic decipherment problem, leveraging modeling techniques from work on decrypting historical ciphers. The key idea is that while properties like font and text structure are document-specific and therefore difficult to treat generally with supervised techniques, these phenomena are in fact regular within individual documents. For example, while the shape of a particular character in an obscure historical font may be unknown to the system, that shape is in fact regular;  every time the character is printed it uses the same template. Models that leverage this kind of regularity by incorporating it as an assumption can constrain the otherwise difficult unsupervised learning problem and make it feasible. This proposal introduces a class of generative models with this goal in mind, designed to learn fonts and predict accurate transcriptions in an unsupervised fashion by capturing the core properties of the process that generated the input data: the historical printing process. These models represent the specific types of printing and typesetting noise exhibited by early modern documents, treat typesetting as a latent variable, and jointly consider possible character segmentations and transcriptions during inference. Their parameters can be estimated efficiently, directly from images of historical documents without accompanying transcriptions. Further, by treating damaged portions of the input documents as latent variables, this proposal aims to automatically reconstruct damaged documents using the same approach. The unsupervised techniques developed here may have uses in other areas of natural language processing where annotated training data is hard to obtain; for example, in personalized speech recognition and grounded semantics."
"1552687","CAREER: Scaling-up Resistive Synaptic Arrays for Neuro-inspired Computing","CCF","Software & Hardware Foundation","02/01/2016","03/22/2018","Shimeng Yu","AZ","Arizona State University","Continuing Grant","Sankar Basu","12/31/2018","$239,688.00","","shimeng.yu@ece.gatech.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","7798","1045, 7945, 8089","$0.00","Neuro-inspired deep learning algorithms have demonstrated their power in executing intelligent tasks such as image and speech recognition. However, training of such deep neural networks requires huge amount of computational resources that are not affordable for mobile applications. Hardware acceleration of deep learning, with orders of magnitude improvement in speed and energy efficiency, remains a grand challenge for the conventional hardware based on silicon CMOS technology and von-Neumann architecture. As the learning algorithms extensively involve matrix operations, neuro-inspired architectures that leverage the distributed computing in the neuron nodes and localized storage in the synaptic networks are very attractive. The ultimate goal of this project is to advance the neuro-inspired computing with emerging nano-device technologies towards a self-learning chip. A chip that learns in real-time and consumes low-power can be placed at frontend sensors, bringing broad benefits for a number of current applications. The PI will establish close collaboration with industry through student internships and technology transfer. The plan for integration of research and education will train students with interdisciplinary skills. The cross-layer nature of this project ranging from semiconductor device, circuit design, electronic design automation, and machine learning is expected to provide an ideal platform for this educational goal.<br/><br/>The technical goal of this project is to overcome the challenges that prevent scaling up of the crossbar array size for neuro-inspired architecture. Resistive devices with continuous multilevel states have been proposed to function as synaptic weights in the crossbar architecture. However, with the increase of the array size, issues associated with device yield, device variability, and array parasitics will arise and may degrade the system performance. The PI plans to tackle these challenges by exploiting hierarchical research efforts from devices, circuits and architectures. The outcome of the research includes device compact model, circuit-level benchmark simulator for estimating the area/latency/power of the crossbar array macro, and architectural tool for efficiently mapping the learning algorithms into the crossbar architecture.  The PI has established a custom fabrication channel for tape-out of resistive devices on top of CMOS peripheral circuits via his collaboration with academic partners. The prototype chip with measured data is expected to make a strong impact on this field, which previously relied on the simulations for predicting large-scale array performance."
"1647616","SBIR Phase I:  Software for Developing Consumer-Driven Health Care Solutions","IIP","SBIR Phase I","12/15/2016","12/08/2016","Mary Kay O'Connor","MO","PatientsVoices, Inc.","Standard Grant","Jesus Soriano Molla","02/28/2018","$225,000.00","","maryk.oconnor@patientsvoices.net","5317 NW Bluff Way","Parkville","MO","641521130","8168660363","ENG","5371","5371, 8018, 8023, 8032, 8042, 9150","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is the development of software that automatically identifies and labels problems that patients encounter when they receive care.  This information tells health care providers exactly what they need to fix to improve patient care.  Applications of the software include: a) the developing different software versions for different health care situations, the technology can be used to improve the patient experience in clinics, emergency departments, primary care settings, outpatient centers, etc.; b) Early interventions.  Healthcare leaders are asking for an early warning system.  By recording patient conversations with nurses and doctors while they are still in the hospital, this feedback can be used to identify and resolve problems before the patient is discharged; c) New health care delivery models.  One hospital wants to reduce the length of stay for heart patients; doctors want to be sure that patients are ready to go home.  The software is analyzing the stories of heart patients who did well versus patients who had problems when they left the hospital early.  Their feedback will help keep patients out of the hospital and if they are admitted, improve their care before and after they leave; d) Following the doctor's orders.  When chronic care patients don't follow through on the treatment plan recommended by their doctor, their health problems often get worse.  This software can be used to collect feedback from chronically ill patients and identify patterns in why these patients are not following doctor's orders solutions that motivate and engage patients can be implemented; and e) Other industries.  This software can be used to analyze patient feedback during clinical trials.  The resulting information will make it easier to recruit and retain patients in clinical trials as well as improve clinical outcomes.  Ultimately, results from these analyses could be used to inform FDA decision making within the pharmaceutical industry. <br/><br/>The proposed project develops software that uses advanced Natural Language Processing (NLP) techniques to analyze patients? responses about their health care experiences in interviews and open response survey questions in order to provide hospitals with concrete, actionable information on how to improve care and patient outcomes. To date, hospitals have relied primarily on surveys to inform their attempts to improve patient experiences.<br/>This research will develop an NLP application for mining patient feedback across health care settings. Data where patients freely tell their ""stories"" provides a clearer and more precise view of the patient's experience than a standard survey with ratings on predefined questions. The challenge is that the variability of expression and experiences requires sophisticated techniques to be able to classify the information, determine the sentiment, and extract essential details that can provide actionable recommendations to hospitals. The team proposes a combination of data annotation, pattern matching, and machine learning techniques for classification and information extraction of core concepts like problem root causes from unstructured patient feedback. Since interviews comprise much of the most informative data, we will also evaluate which speech recognition technologies can best convert audio to text, for subsequent classification and information extraction."
"1629559","XPS: FULL: Broad-Purpose, Aggressively Asynchronous and Theoretically Sound Parallel Large-scale Machine Learning","CCF","Exploiting Parallel&Scalabilty","09/01/2016","02/06/2018","Eric Xing","PA","Carnegie-Mellon University","Standard Grant","Wei Ding","08/31/2020","$625,379.00","Garth Gibson","epxing@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","8283","","$0.00","Many artificial intelligence (AI) applications such as image understanding and natural language processing rely on Machine Learning (ML) methods to automatically extract valuable knowledge from Big Data (Big Learning). Efficient ML requires not only expertise in advanced mathematical models and algorithms, but also experiences with large computer clusters where issues such as machine failures, memory/network bottlenecks, inter-machine latencies must be properly handled through complex system programming. Such demand on ""dual skill"" often prevents democratizing large-scale AI to wide user communities, and necessitates a new framework that bridges ML and the distributed computing environment of a cluster with a single-machine-like simple interface, allowing ML practitioners to be agnostic about the backend details, and able to quickly prototype or deploy ML programs on clusters. Solutions to such a need remain rare. In this project the PIs develop a new general purpose framework for ML on distributed systems, offering highly efficient and theoretically justified protocols (e.g. communication, scheduling, and partitioning functions) to orchestrate a heterogeneous computer cluster to become programmable and act like a single big computer, and execute distributed ML programs correctly and at a speed orders of magnitude faster than current systems such as Hadoop and Spark. With this new framework, data scientists will be able to conduct ML analytics with complex models on massive data without the need for dedicated engineering and infrastructure teams, allowing Big Learning more readily accessible to society.<br/> <br/>Specifically, over a four year span, the proposed research focuses on three technical aims: (1) Building a System Framework for Big Learning, by developing a new architecture that supports both data- and model-parallel execution of large ML programs, using intelligent scheduler, parameter server, and consistency controller that are configurable to provide flexible options for model/data parallelization, synchronization schemes, load balance, fault tolerance, and multi-instance tenancy; (2) Building a Multi-Level-Abstraction Programming Interface, which supports easy parallel programming of both basic and advanced ML algorithms for large-scale applications; and (3)Conducting theoretical analysis of distributed ML algorithms on the proposed system, based on unique insights such as block consistency and error-tolerance under bounded synchronism. The goal is to develop a system framework to achieve general, automatic, and effective parallelization of ML programs."
"1661755","CAREER: A New Neat Framework for Statistical Machine Learning","IIS","Robust Intelligence","09/01/2016","01/24/2017","Pradeep Ravikumar","PA","Carnegie-Mellon University","Continuing Grant","Rebecca Hwa","08/31/2019","$229,400.00","","pradeepr@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7495","1045","$0.00","The pendulum in Artificial Intelligence (AI) research has periodically swung from so called ""neat"" or mathematically rigorous approaches, and ""scruffy"" or more adhoc approaches. In recent years, real-world data across varied fields of science and engineering are increasingly complex, and involve a large number of variables, which has resulted in a surge of scruffier methods. This proposal develops a general ""neat"" framework for such modern settings by leveraging state of the art developments in two of the most popular subfields of machine learning methods: graphical models and high-dimensional statistical methods. These developments have in common that a complex model parameter is expressed as a superposition of simple components, which is then leveraged for tractable inference and learning.<br/><br/>Our unified framework results not only in a unified picture of these developments but also provides newer methods to work with such high-dimensional data. The research thus impacts problems across science and engineering wherever statistical machine learning approaches are being used (such as genomics, natural language processing and image analysis, to name a few). The work on a unified framework for statistical machine learning problems is highly coupled with a push for imparting training to students on what we call ""comptastical"" thinking. This combines both computational and statistical thinking required for addressing the problems of limited computation and limited data inherent in modern statistical AI application domains. The proposal also develops an infrastructure for component-based courses with relationally organized lecture module components."
"1637585","AitF: Collaborative Research: Algorithms for Probabilistic Inference in the Real World","CCF","Algorithms in the Field","09/01/2016","08/30/2016","Aravindan Vijayaraghavan","IL","Northwestern University","Standard Grant","A. Funda Ergun","08/31/2021","$399,939.00","","aravindv@northwestern.edu","750 N. Lake Shore Drive","Chicago","IL","606114579","3125037955","CSE","7239","","$0.00","Statistical models provide a powerful means of quantifying uncertainty, modeling prior beliefs, and describing complex dependencies in data.  The process of using a model to answer specific questions, such as inferring the state of several random variables given evidence observed about others, is called probabilistic inference.  Probabilistic graphical models, a type of statistical model, are often used in diverse applications such as medical diagnosis, understanding protein and gene regulatory networks, computer vision, and language understanding.  On account of the central role played by probabilistic graphical models in a wide range of automated reasoning applications, designing efficient algorithms for probabilistic inference is a fundamental problem in artificial intelligence and machine learning.<br/> <br/>Probabilistic inference in many of these applications corresponds to a complex combinatorial optimization problem that at first glance appears to be extremely difficult to solve.  However, practitioners have made significant strides in designing heuristic algorithms to perform real-world inference accurately and efficiently.  This project focuses on bridging the gap between theory and practice for probabilistic inference problems in large-scale machine learning systems.  The PIs will identify structural properties and methods of analysis that differentiate real-world instances from worst-case instances used to show NP-hardness, and will design efficient algorithms with provable guarantees that would apply to most real-world instances.  The project will also study why heuristics like linear programming and other convex relaxations are so successful on real-world instances.  The efficient algorithms for probabilistic inference developed as part of this project have the potential to be transformative in machine learning, statistics, and more applied areas like computer vision, social networks and computational biology.  To help disseminate the research and foster new collaborations, a series of workshops will be organized bringing together the theoretical computer science and machine learning communities.  Additionally, undergraduate curricula will be developed that use machine learning to introduce students to concepts in theoretical computer science."
"1637544","AitF: Collaborative Research: Algorithms for Probabilistic Inference in the Real World","CCF","Algorithms in the Field","09/01/2016","08/30/2016","David Sontag","NY","New York University","Standard Grant","Tracy Kimbrel","01/31/2017","$399,999.00","","dsontag@mit.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","7239","","$0.00","Statistical models provide a powerful means of quantifying uncertainty, modeling prior beliefs, and describing complex dependencies in data.  The process of using a model to answer specific questions, such as inferring the state of several random variables given evidence observed about others, is called probabilistic inference.  Probabilistic graphical models, a type of statistical model, are often used in diverse applications such as medical diagnosis, understanding protein and gene regulatory networks, computer vision, and language understanding.  On account of the central role played by probabilistic graphical models in a wide range of automated reasoning applications, designing efficient algorithms for probabilistic inference is a fundamental problem in artificial intelligence and machine learning.<br/> <br/>Probabilistic inference in many of these applications corresponds to a complex combinatorial optimization problem that at first glance appears to be extremely difficult to solve.  However, practitioners have made significant strides in designing heuristic algorithms to perform real-world inference accurately and efficiently.  This project focuses on bridging the gap between theory and practice for probabilistic inference problems in large-scale machine learning systems.  The PIs will identify structural properties and methods of analysis that differentiate real-world instances from worst-case instances used to show NP-hardness, and will design efficient algorithms with provable guarantees that would apply to most real-world instances.  The project will also study why heuristics like linear programming and other convex relaxations are so successful on real-world instances.  The efficient algorithms for probabilistic inference developed as part of this project have the potential to be transformative in machine learning, statistics, and more applied areas like computer vision, social networks and computational biology.  To help disseminate the research and foster new collaborations, a series of workshops will be organized bringing together the theoretical computer science and machine learning communities.  Additionally, undergraduate curricula will be developed that use machine learning to introduce students to concepts in theoretical computer science."
"1633857","BIGDATA: F: Open-World Foundations for Big Uncertain Data","IIS","Big Data Science &Engineering","09/01/2016","09/06/2016","Guy Van den Broeck","CA","University of California-Los Angeles","Standard Grant","Sylvia Spengler","08/31/2020","$432,202.00","","guyvdb@cs.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","8083","7433, 8083","$0.00","Driven by the need to learn from vast amounts of text data, efforts throughout natural language processing, information extraction, databases, and AI are coming together to build large-scale knowledge bases. These systems continuously crawl the web to extract relational data from text, and have already populated their databases with millions of entities and billions of tuples. Large-scale probabilistic knowledge bases are revolutionizing the way we access data. They are now routinely used by scientists to build knowledge bases of publications, by law enforcement to extract information from the dark web, and by regular search engine users who find their results augmented with structured information. Such knowledge bases are inherently probabilistic: to go from raw text to structured data, a sequence of statistical machine learning techniques associate probabilities with database tuples. This project revisits the semantics underlying such systems, and provide a more adequate foundational framework. In particular, the closed-world assumption of probabilistic databases, that facts not in the database have probability zero, clearly conflicts with their everyday use, and obstructs the progress in this area.<br/><br/>More specifically, this project develops a new semantic foundation based on the open-world assumption, that facts not in the database are possible, but have unknown probability. It designs the basic algorithms for query answering in this setting, both exact and approximate. Moreover, in a deep theoretical component, this project studies fundamental questions of data and domain complexity that are unique to open-world reasoning about big uncertain data. Finally it develops proof-of-concept applications in machine learning and data mining, and additional knowledge-representation layers that strengthen open-world reasoning. The developed semantics provide meaningful answers when some tuple probabilities are not precisely known. The developed algorithms allow for efficient query answering, even when reasoning about the open world, in time linear in the database size for tractable queries. This project provides a scientific leap at the fundamental, semantic level. It also provides a context for training undergraduate and graduate students in subjects spanning databases, artificial intelligence, theory, and machine learning, and will target the integration of probabilistic knowledge bases into computer science curricula."
"1649242","EAGER:   Deep Learning for Microarchitectural Prediction","CCF","Software & Hardware Foundation","08/01/2016","07/27/2016","Daniel Jimenez","TX","Texas A&M Engineering Experiment Station","Standard Grant","Yuanyuan Yang","07/31/2019","$150,000.00","","djimenez@acm.org","400 Harvey Mitchell Pkwy S","College Station","TX","778454645","9798626777","CSE","7798","7916, 7941","$0.00","Computer programs often have highly predictable behavior.  Microprocessors use predictors to improve program performance and efficiency. Decisions made by a program can often be predicted with good accuracy, and patterns of data usage can be predicted to improve system efficiency and performance. However, incorrect predictions can lead to poor performance or lost opportunities for improving efficiency. This project proposes to use deep learning to improve prediction in microprocessors. Deep learning is a technology that has been used to improve computer vision and other pattern recognition tasks in large computing systems, but so far it has not been applied at the very small scale and tight timing margins of improving microprocessors. The project will likely result in improved microprocessors, as well as educational, mentoring, and career opportunities for under-represented groups in computer science. The PI will incorporate the research into classroom teaching. The Ph.D. students trained through this project will enhance industrial and academic workforce. The PI will continue to recruit women and minority graduate students into his research program for this project. Outreach to under-represented groups will include PI leadership and participation at CRA-W mentoring workshops for women and minority graduate students.<br/><br/>The goal of the proposed research is to exploit deep learning to design new microarchitectural predictors capable of exploiting previously untapped levels of predictability in program behavior to improve performance, power, and energy. Deep neural networks will be used to greatly improve the accuracy of microarchitectural predictors. This project will first explore latency-tolerant cache locality predictors, then move to control-flow prediction that has tighter timing constraints. Proposed predictors will be evaluated in a variety of contexts representing modern workloads at scales from mobile phones to datacenters. The research incurs a high-risk because no deep neural network has even been developed to operate at the sub-nanosecond level. However, the research offers a high-payoff due to the tremendous potential to improve performance. Results will be manifested through students' theses and dissertations as well as publication in top-tier architecture venues."
"1647600","SBIR Phase I:  A Cocktail Party Technology: Real-Time Conversation Separation from Background Voices and Sounds","IIP","SMALL BUSINESS PHASE I","12/01/2016","04/13/2017","Shibani Abhyankar","CT","Yobe Inc","Standard Grant","Peter Atherton","08/31/2017","$224,588.00","","shibani@yobeinc.com","745 Atlantic Av","Boston","CT","021110000","2032041158","ENG","5371","5371, 8032","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is that it will for the first time make it possible to create voice technologies whose performance in speech and speaker recognition does not significantly degrade due to the presence of interfering voices or environmental sounds. This issue has kept many voice technologies out of both the mobile and IoT markets. It is expected that the company's unique artificial intelligence platform will deliver a fully scalable, real-time software solution. Solving this challenge will make the currently noisy world of smartphones more realistic for voice technologies (like voice authentication) that to date have avoided the space. <br/><br/>This Small Business Innovation Research Phase I project concerns a novel technology that is the result of innovatively combining advanced signal processing, broadcast studio methodologies, and artificial intelligence techniques to perform aggressive separation of voice from background voices and sounds. It also automatically repairs the biometrics of the separated voice signals on the basis of empirically formulated signal-dependent rules. The technology has already been demonstrated through informal tests to be significantly better than existing technologies in separating two-person conversations from highly overlapped background voices and sounds captured on a pair of closely spaced (few centimeters) omnidirectional microphones. This SBIR Phase I project seeks to firmly establish the clear superiority of this technology over any other existing voice separation technology. The ultimate goal of the project is to demonstrate that when the proposed technology is properly optimized as a frontend to state-of-the-art automatic speech recognition or state-of-the-art automatic speaker recognition, the recognition error rates in noisy multi-voice environments are comparable to those obtained in noiseless single-voice environments."
"1644560","I-Corps: Development of a machine vision system for high-throughput computational behavioral analysis","IIP","I-Corps","08/01/2016","07/21/2016","Thomas Serre","RI","Brown University","Standard Grant","Steven Konsek","01/31/2017","$50,000.00","","Thomas_Serre@brown.edu","BOX 1929","Providence","RI","029129002","4018632777","ENG","8023","9150","$0.00","The broader impact/commercial potential of this I-Corps project is the promise to revolutionize bio-medical research via the development of machine vision algorithms for automating video analysis and behavioral monitoring. Many areas of the life sciences demand the manual annotation of large amounts of video data. However, the robust quantification of complex behaviors imposes a major bottleneck and a number of controversies in behavioral studies have arisen because of the inherent biases and challenges associated with the manual annotation of behavior. Many of these issues will be resolved with the use of objective quantitative computerized techniques.  The goal of the project is to leverage machine learning and computer vision to analyze large volumes of data and discover novel visual features of behavior that are literally hidden to the naked eye.<br/><br/>This I-Corps project proposes the large-scale development, testing, and research application of algorithms and software for automating the monitoring and analysis of behavior. We have developed an initial high-throughput system for the automated monitoring and analysis of rodent behavior. The approach capitalizes on recent developments in the area of deep learning, which is a branch of machine learning that enables neural networks composed of multiple processing stages to learn visual representations with multiple levels of abstraction. The current system accurately recognizes a myriad of normal and abnormal rodent behaviors at a level indistinguishable from human when scoring typical behaviors of a singly housed mouse from video. The proposed activities will bring algorithms closer to commercial deployment by addressing the fundamental problem of visual recognition in biological, cognitive, and psychological research."
"1620216","Three dimensional deep wavelet scattering for quantum energy interpolation","DMS","COMPUTATIONAL MATHEMATICS","09/01/2016","09/08/2016","Matthew Hirn","MI","Michigan State University","Standard Grant","Leland Jameson","08/31/2020","$191,775.00","","mhirn@msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","MPS","1271","9263","$0.00","Physical quantities are often computed as solutions to a system of complex mathematical equations, which may require huge computations for intricate physical states. Quantum chemistry calculations of molecular energies is such an example. Indeed, computing the energy of a molecule, given the charges and positions of its nuclei, is a central issue in computational chemistry with important applications in molecular dynamics, materials science, and drug discovery. Machine learning algorithms do not simulate the physical system but estimate solutions by learning from a training set of known examples. However, such learning algorithms may require a number of examples that is exponential in the system dimension, and are thus intractable; this phenomena is referred to as the ""curse of dimensionality."" This proposal will develop a novel approach for the estimation of molecular energies based on the ""scattering transform."" The scattering transform estimates molecular energies, and circumvents the curse of dimensionality, by utilizing a multiscale, multilevel architecture that takes advantage of physical invariants. The resulting algorithms have the potential to significantly speed up the computation of highly accurate molecular energy estimates, leading to large scale atomistic simulations with greatly improved accuracy, speed, and adaptability, thus shifting the paradigm of multiscale modeling. The PI will additionally mentor an undergraduate student and train a graduate student in this field, thus setting up the potential for dissemination of the core ideas to a broader audience.<br/><br/>The scattering transform has the structure of a deep convolutional network, but is composed of iterated wavelet transforms and complex modulus operators. Such networks have been used in computer vision for the analysis and classification of two dimensional images and audio tasks involving one dimensional signals. A multiscale three dimensional scattering transform network is novel both in practice (multiscale 3D) and design (for quantum chemistry), and has the chance to influence these types of architectures moving forward. A systematic approach will attack the primary object on several fronts: (1) development of efficient 3D filters with the appropriate symmetry and stability properties; (2) rigorous error analysis of the scattering regression algorithm for various components of the molecular energy functional; (3) deeper understanding of the scattering network via provable relations with fast multipole methods. The methods used to carry out these objectives will include: (i) wavelet filter design and efficient signal processing algorithms; (ii) utilization of Littlewood-Paley Theory in conjunction with polynomial (Taylor) approximation theory; (iii) multiscale analysis; (iv) numerical experiments to validate methods. By rigorously linking deep learning architectures with physical chemistry, the research in this proposal will take place at the interface of data science and scientific computation, for the mutual gain of both."
"1618061","RI: Small: Concatenative Resynthesis for Very High Quality Speech Enhancement","IIS","Robust Intelligence","06/15/2016","07/03/2017","Michael Mandel","NY","CUNY Brooklyn College","Continuing Grant","Tatiana Korelsky","05/31/2021","$457,958.00","","mim@sci.brooklyn.cuny.edu","Office of Research & Sponsored P","Brooklyn","NY","112102889","7189515622","CSE","7495","7495, 7923, 9251","$0.00","Environmental noise is one of the largest problem for users of voice technologies, such as hearing aids, mobile phones, and automatic speech recognition. Current approaches to source separation and speech enhancement typically attempt to modify the noisy signal in order to make it more like the original, leading to distortions in target speech and residual noise. In contrast, this project uses the innovative approach of driving a speech synthesizer using information extracted from the noisy signal to create a brand new, high quality, noise-free version of the original sentence.  Improvements in noise suppression and speech quality from this approach are expected to have important broader impacts for both the 36 million Americans who are hearing impaired and the 200 million Americans who use smart phones. The project is also being incorporated into the curriculum in a diverse urban college and into established outreach programs to nearby high schools with the goal of encouraging members of under-represented groups to pursue careers in science and engineering.<br/><br/>This project aims to produce a high quality speech resynthesis system by modifying a concatenative speech synthesizer to use a unit-selection function based on a novel deep neural network (DNN) architecture. Preliminary results have shown this approach to work well for a small-vocabulary, speaker-dependent task, and the current project expands this to the large-vocabulary, speaker-dependent setting in three ways. First, it seeks to improve the intelligibility of the synthesized speech by utilizing perceptually motivated input features, more flexible training signals, and traditional speech enhancement. Second, it seeks to improve the system's scalability by training DNNs to embed noisy and clean speech into a joint low-dimensional space in which similarity can be efficiently computed. And third, it seeks to improve the quality of the synthesized speech by incorporating sequential models of speech units based on acoustic, phonetic, and linguistic compatibility. The use of speech synthesis models in speech enhancement is a departure from traditional approaches and has the potential to make a transformative impact on the quality of enhanced speech."
"1629898","CI-SUSTAIN: Collaborative Research: Extending a Large Multimodal Corpus of Spontaneous Behavior for Automated Emotion Analysis","CNS","Special Projects - CNS, CCRI-CISE Cmnty Rsrch Infrstrc","09/01/2016","05/05/2017","Lijun Yin","NY","SUNY at Binghamton","Standard Grant","Balakrishnan Prabhakaran","08/31/2021","$491,581.00","","lijun@cs.binghamton.edu","4400 VESTAL PKWY E","BINGHAMTON","NY","139026000","6077776136","CSE","1714, 7359","7359, 9251","$0.00","This project will extend and sustain a widely-used data infrastructure for studying human emotion, hosted at the lead investigator's university and available to the research community.  The first two versions of the dataset (BP4D and BP4D+) contain videos of people reacting to varied emotion-eliciting situations, their self-reported emotion, and expert annotations of their facial expression. Version 1, BP4D (n=41), has been used by over 100 research groups and supported a successful community competition around recognizing emotion.  The second version (BP4D+) adds participants (n = 140), thermal imaging, and measures of peripheral physiology.  The current project greatly broadens and extends this corpus to produce a new dataset (BP4D++) that enables deep-learning approaches, increases generalizability, and builds research infrastructure and community in computer and behavioral science.  The collaborators will (1) increase participant diversity; 2) add videos of pairs of people interacting to the current mix of individual and interviewer-mediated video; 3) increase the number of participants to meet the demands of recent advances in ""big data"" approaches to machine learning; and 4) expand the size and scope of annotations in the videos. They will also involve the community through an oversight and coordinating consortium that includes researchers in computer vision, biometrics, robotics, and cognitive and behavioral science. The consortium will be composed of special interest groups that focus on various aspects of the corpus, including groups responsible for completing the needed annotations, generating meta-data, and expanding the database application scope.  Having an infrastructure to support emotion recognition research matters because computer systems that interact with people (such as phone assistants or characters in virtual reality environments) will be more useful if they react appropriately to what people are doing, thinking, and feeling.  <br/><br/>The team will triple the number of participants in the combined corpora to 540.  They will develop a dyadic interaction task and capture data from 100 interacting dyads to support dynamic modeling of interpersonal influence across expressive behavior and physiology, as well as analysis of emotional synchrony.  They will increase the density of facial annotations to about 15 million frames in total, allowing the database to become sufficiently large to support deep-learning approaches to multimodal emotion detection. These annotations will be accomplished through a hybrid approach that combines expert coding using the Facial Action Coding System, automated face analysis, and crowdsourcing with expert input from the research community.  Finally, the recorded data will be augmented with a wide range of meta-data derived from 2D videos, 3D videos, thermal videos, and physiological signals.  To ensure the community is involved in sustaining the infrastructure, in addition to the governance consortium described above, the investigators will involve the community in jointly building both APIs that allow adding meta-data and annotations and tools to support the submission and evaluation of new recognition algorithms, then organizing community-wide competitions using those tools.  The research team will also reach out to new research communities around health computing, biometrics, and affective computing to widen the utility of the enhanced infrastructure, grow the community of expert annotators through training workshops, and build an educational community around the infrastructure that facilitates the development and sharing of course materials that use it.  Long-term, the infrastructure will be funded through a combination of commercial licensing and support from the lead university's system administration group."
"1629716","CI-SUSTAIN: Collaborative Research: Extending a Large Multimodal Corpus of Spontaneous Behavior for Automated Emotion Analysis","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","09/01/2016","07/25/2016","Jeffrey Cohn","PA","University of Pittsburgh","Standard Grant","Balakrishnan Prabhakaran","07/31/2020","$300,955.00","","jeffcohn@pitt.edu","300 Murdoch Building","Pittsburgh","PA","152603203","4126247400","CSE","7359","7359","$0.00","This project will extend and sustain a widely-used data infrastructure for studying human emotion, hosted at the lead investigator's university and available to the research community.  The first two versions of the dataset (BP4D and BP4D+) contain videos of people reacting to varied emotion-eliciting situations, their self-reported emotion, and expert annotations of their facial expression. Version 1, BP4D (n=41), has been used by over 100 research groups and supported a successful community competition around recognizing emotion.  The second version (BP4D+) adds participants (n = 140), thermal imaging, and measures of peripheral physiology.  The current project greatly broadens and extends this corpus to produce a new dataset (BP4D++) that enables deep-learning approaches, increases generalizability, and builds research infrastructure and community in computer and behavioral science.  The collaborators will (1) increase participant diversity; 2) add videos of pairs of people interacting to the current mix of individual and interviewer-mediated video; 3) increase the number of participants to meet the demands of recent advances in ""big data"" approaches to machine learning; and 4) expand the size and scope of annotations in the videos. They will also involve the community through an oversight and coordinating consortium that includes researchers in computer vision, biometrics, robotics, and cognitive and behavioral science. The consortium will be composed of special interest groups that focus on various aspects of the corpus, including groups responsible for completing the needed annotations, generating meta-data, and expanding the database application scope.  Having an infrastructure to support emotion recognition research matters because computer systems that interact with people (such as phone assistants or characters in virtual reality environments) will be more useful if they react appropriately to what people are doing, thinking, and feeling.  <br/><br/>The team will triple the number of participants in the combined corpora to 540.  They will develop a dyadic interaction task and capture data from 100 interacting dyads to support dynamic modeling of interpersonal influence across expressive behavior and physiology, as well as analysis of emotional synchrony.  They will increase the density of facial annotations to about 15 million frames in total, allowing the database to become sufficiently large to support deep-learning approaches to multimodal emotion detection. These annotations will be accomplished through a hybrid approach that combines expert coding using the Facial Action Coding System, automated face analysis, and crowdsourcing with expert input from the research community.  Finally, the recorded data will be augmented with a wide range of meta-data derived from 2D videos, 3D videos, thermal videos, and physiological signals.  To ensure the community is involved in sustaining the infrastructure, in addition to the governance consortium described above, the investigators will involve the community in jointly building both APIs that allow adding meta-data and annotations and tools to support the submission and evaluation of new recognition algorithms, then organizing community-wide competitions using those tools.  The research team will also reach out to new research communities around health computing, biometrics, and affective computing to widen the utility of the enhanced infrastructure, grow the community of expert annotators through training workshops, and build an educational community around the infrastructure that facilitates the development and sharing of course materials that use it.  Long-term, the infrastructure will be funded through a combination of commercial licensing and support from the lead university's system administration group."
"1629856","CI-SUSTAIN: Collaborative Research: Extending a Large Multimodal Corpus of Spontaneous Behavior for Automated Emotion Analysis","CNS","Special Projects - CNS, CCRI-CISE Cmnty Rsrch Infrstrc","09/01/2016","05/05/2017","Qiang Ji","NY","Rensselaer Polytechnic Institute","Standard Grant","Balakrishnan Prabhakaran","08/31/2020","$223,426.00","","qji@ecse.rpi.edu","110 8TH ST","Troy","NY","121803522","5182766000","CSE","1714, 7359","7359, 9251","$0.00","This project will extend and sustain a widely-used data infrastructure for studying human emotion, hosted at the lead investigator's university and available to the research community.  The first two versions of the dataset (BP4D and BP4D+) contain videos of people reacting to varied emotion-eliciting situations, their self-reported emotion, and expert annotations of their facial expression. Version 1, BP4D (n=41), has been used by over 100 research groups and supported a successful community competition around recognizing emotion.  The second version (BP4D+) adds participants (n = 140), thermal imaging, and measures of peripheral physiology.  The current project greatly broadens and extends this corpus to produce a new dataset (BP4D++) that enables deep-learning approaches, increases generalizability, and builds research infrastructure and community in computer and behavioral science.  The collaborators will (1) increase participant diversity; 2) add videos of pairs of people interacting to the current mix of individual and interviewer-mediated video; 3) increase the number of participants to meet the demands of recent advances in ""big data"" approaches to machine learning; and 4) expand the size and scope of annotations in the videos. They will also involve the community through an oversight and coordinating consortium that includes researchers in computer vision, biometrics, robotics, and cognitive and behavioral science. The consortium will be composed of special interest groups that focus on various aspects of the corpus, including groups responsible for completing the needed annotations, generating meta-data, and expanding the database application scope.  Having an infrastructure to support emotion recognition research matters because computer systems that interact with people (such as phone assistants or characters in virtual reality environments) will be more useful if they react appropriately to what people are doing, thinking, and feeling.  <br/><br/>The team will triple the number of participants in the combined corpora to 540.  They will develop a dyadic interaction task and capture data from 100 interacting dyads to support dynamic modeling of interpersonal influence across expressive behavior and physiology, as well as analysis of emotional synchrony.  They will increase the density of facial annotations to about 15 million frames in total, allowing the database to become sufficiently large to support deep-learning approaches to multimodal emotion detection. These annotations will be accomplished through a hybrid approach that combines expert coding using the Facial Action Coding System, automated face analysis, and crowdsourcing with expert input from the research community.  Finally, the recorded data will be augmented with a wide range of meta-data derived from 2D videos, 3D videos, thermal videos, and physiological signals.  To ensure the community is involved in sustaining the infrastructure, in addition to the governance consortium described above, the investigators will involve the community in jointly building both APIs that allow adding meta-data and annotations and tools to support the submission and evaluation of new recognition algorithms, then organizing community-wide competitions using those tools.  The research team will also reach out to new research communities around health computing, biometrics, and affective computing to widen the utility of the enhanced infrastructure, grow the community of expert annotators through training workshops, and build an educational community around the infrastructure that facilitates the development and sharing of course materials that use it.  Long-term, the infrastructure will be funded through a combination of commercial licensing and support from the lead university's system administration group."
"1614717","CSR: Small: Enabling Deep Neural Networks for Mobile-Cloud Applications","CNS","CSR-Computer Systems Research","10/01/2016","08/11/2016","Arvind Krishnamurthy","WA","University of Washington","Standard Grant","Marilyn McClure","06/30/2020","$427,037.00","","arvind@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7354","7923","$0.00","Over the past three years, Deep Neural Networks (DNNs) have become the dominant approach to solving a variety of important problems in computing. This includes problems in speech recognition, machine translation, handwriting recognition and many computer vision problems like face, object, and scene recognition. Although they are renowned for their excellent recognition performance, DNNs are also known to be computationally intensive: networks commonly used for speech, visual and language understanding tasks routinely consume hundreds of MB of memory and Gflops of computing power, typically the province of server-class computers. However, the relevance of the above applications to the mobile setting and the potential for developing new applications provides a strong case for executing DNNs on mobile devices.<br/><br/>This project is to build an execution framework for deep-neural networks on mobile-cloud platforms so as to enable a broad class of emerging applications such as continuous mobile vision.  In particular, this work will look at enabling a large suite of DNN-based face, scene and object processing algorithms based on applying DNNs to video streams from wearable devices.  This framework, given an arbitrary DNN, will compile it down to a resource-efficient variant at modest loss in accuracy.  The project plans include developing novel techniques to specialize DNNs to contexts and to share resources across multiple simultaneously executing DNNs. Finally, it will create a run-time system for managing the optimized models generated. Using the challenging continuous mobile vision domain as a case-study, the plan is to demonstrate that these techniques yield very significant reductions in DNN resource usage, including orders of magnitude reduction in memory use and instructions executed, in common mobile settings."
"1560345","REU Site: Advances of Machine Learning in Theory & Applications (AMALTHEA)","CCF","RSCH EXPER FOR UNDERGRAD SITES","03/15/2016","03/09/2016","Georgios Anagnostopoulos","FL","Florida Institute of Technology","Standard Grant","Joseph Maurice Rojas","02/29/2020","$359,998.00","Anthony Smith","georgio@fit.edu","150 W UNIVERSITY BLVD","MELBOURNE","FL","329016975","3216748000","CSE","1139","9250","$0.00","The Advances of MAchine Learning in THEory and Applications (AMALTHEA) Research Experiences for Undergraduates (REU) Site aims to provide top quality educational experiences to a diverse community of undergraduate students through research participation in the area of Machine Learning (ML). The relevance and importance of ML is not limited to specialized technological innovations, as it was in the past. Nowadays, it also increasingly influences everyday life through its contributions to applications such as voice/face recognition, credit fraud detection, intelligent recommendation systems and many others. Furthermore, ML is inherently multi-disciplinary as it draws from advances in multiple disciplines such as engineering, computing, statistics, mathematics, physics and biology, to name a few major ones. Since its start in 2009, AMALTHEA, one of the first ML-focused REU sites, involves 10 undergraduate students per year from a broad spectrum of disciplines, and the educational experience spans 10 weeks in the summer. The participants are exposed to cutting-edge ML research, as well as professional development activities, such as technical seminars and career-related workshops. Moreover, these participants perform closely-mentored research, whose results are going to impact the field of ML itself, as well as how ML is applied in other scientific disciplines. Over the 2016-2019 time span, the project will directly impact a diverse group of 30 motivated students, the majority of which may not have access to such research participation opportunities otherwise.<br/><br/>The project's thrust area is the theory of ML and how it can be integrated and applied to important real-life problems, hence exposing participants to both theory and applications. Past projects include applications such as automated anuran recognition from frog calls, uncovering criminal networks from crime locations, human-object interaction recognition, modelling of group dynamics in virtual worlds, speaker-independent speech recognition and license plate recognition among others. On the other hand, past contributions to the theory of ML have been steered towards topics such as functional data analysis, wavelet-based density estimation, non-linear dimensionality reduction, kernel methods and anomaly detection to name a few. Short video highlights of such projects can be found on AMALTHEA's YouTube channel located at https://goo.gl/2JhYoF. Finally, additional information about these topics and their outcomes can be found on the project's web site located at http://www.amalthea-reu.org."
"1616216","Collaborative Research: Productivity Prediction of Microbial Cell Factories using Machine Learning and Knowledge Engineering","MCB","Systems and Synthetic Biology","08/01/2016","07/30/2016","Forrest Sheng Bao","OH","University of Akron","Standard Grant","Devaki Bhaya","09/30/2018","$232,523.00","","fsb@iastate.edu","302 Buchtel Common","Akron","OH","443250001","3309722760","BIO","8011","144E, 1757, 7465","$0.00","Over the past decade, systems and synthetic biology approaches provided novel mechanism to enhance the production of diverse chemicals and biofuels from renewable resources in laboratory settings. However, it is still rare for synthetically modified strains to meet the production requirement for commercialization. Strain development falls into the tedious and costly design-build-test-learn cycle because existing modeling approaches failed to capture the complicated metabolic responses in such engineered cells. This proposal will explore an alternate, data-driven approach that has the potential to predict the productivity of synthetic organisms by leveraging the vast array of microbial cell factory publications. Using Artificial Intelligence approaches such as Machine Learning and Knowledge Representation, one can abstract ""previous lessons'' hidden in published data to facilitate a priori estimations of the metabolic output by engineered hosts given a set of specific genetic instructions and fermentation growth conditions. The resulting platform can assist current constraint-based models to design the most effective strategies for producing value-added chemicals. On the educational front, this proposal will offer educational and research training opportunities in synthetic biology, computer programming, and artificial intelligence for graduate students to provide them with a non-conventional career pathway. <br/><br/>Synthetic biology relies on extensive genetic modification and pathway engineering, which often result in unexpected physiological changes or metabolic shifts that reduce the productivity and stability of the hosts. The investigators conceived of a creative, multidisciplinary approach that relies on artificial intelligence-inspired methods for predicting the performance of two distinct unicellular cell factories (Escherichia coli and Saccharomyces cerevisiae). These platforms can be used to quantify the factors that govern microbial productivity (yield, titer, and growth rate), including the type and availability of metabolic precursors; the elements that constitute a biosynthetic pathway; fermentation conditions; and the specific genetic modification to optimize the system. By extracting and classifying information derived from referenced publications within the last 20 years, one can construct a ''knowledge base'' containing sufficient samples of bio-production assemblies. This information will then inform the building of cellular factories using supervised machine learning and non-monotonic logic programming to estimate the productivity of hosts. The data-driven platform will also be integrated into genome scale models to project physiological changes of specific mutant strains. This novel approach will reduce the need for costly design-build-test bench work. Key outcomes from this project include: (1) a database to standardize synthetic biology studies, (2) machine learning models to recognize lessons and patterns hidden in published data, and (3) integration of machine learning with flux balance models, leading to the design of strains with high chances of success in industry settings."
"1616619","Collaborative Research: Productivity Prediction of Microbial Cell Factories using Machine Learning and Knowledge Engineering","MCB","Systems and Synthetic Biology","08/01/2016","07/30/2016","Yinjie Tang","MO","Washington University","Standard Grant","Devaki Bhaya","07/31/2019","$245,474.00","","yinjie.tang@seas.wustl.edu","CAMPUS BOX 1054","Saint Louis","MO","631304862","3147474134","BIO","8011","144E, 1757, 7465","$0.00","Over the past decade, systems and synthetic biology approaches provided novel mechanism to enhance the production of diverse chemicals and biofuels from renewable resources in laboratory settings. However, it is still rare for synthetically modified strains to meet the production requirement for commercialization.  Strain development falls into the tedious and costly design-build-test-learn cycle because existing modeling approaches failed to capture the complicated metabolic responses in such engineered cells. This proposal will explore an alternate, data-driven approach that has the potential to predict the productivity of synthetic organisms by leveraging the vast array of microbial cell factory publications. Using Artificial Intelligence approaches such as Machine Learning and Knowledge Representation, one can abstract ""previous lessons'' hidden in published data to facilitate a priori estimations of the metabolic output by engineered hosts given a set of specific genetic instructions and fermentation growth conditions. The resulting platform can assist current constraint-based models to design the most effective strategies for producing value-added chemicals. On the educational front, this proposal will offer educational and research training opportunities in synthetic biology, computer programming, and artificial intelligence for graduate students to provide them with a non-conventional career pathway. <br/><br/>Synthetic biology relies on extensive genetic modification and pathway engineering, which often result in unexpected physiological changes or metabolic shifts that reduce the productivity and stability of the hosts. The investigators conceived of a creative, multidisciplinary approach that relies on artificial intelligence-inspired methods for predicting the performance of two distinct unicellular cell factories (Escherichia coli and Saccharomyces cerevisiae). These platforms can be used to quantify the factors that govern microbial productivity (yield, titer, and growth rate), including the type and availability of metabolic precursors;  the elements that constitute a biosynthetic pathway; fermentation conditions; and the specific genetic modification to optimize the system. By extracting and classifying information derived from referenced publications within the last 20 years, one can construct a ''knowledge base'' containing sufficient samples of bio-production assemblies. This information will then inform the building of cellular factories using supervised machine learning and non-monotonic logic programming to estimate the productivity of hosts. The data-driven platform will also be integrated into genome scale models to project physiological changes of specific mutant strains. This novel approach will reduce the need for costly design-build-test bench work. Key outcomes from this project include: (1) a database to standardize synthetic biology studies, (2) machine learning models to recognize lessons and patterns hidden in published data, and (3) integration of machine learning with flux balance models, leading to the design of strains with high chances of success in industry settings."
"1621712","SBIR Phase I:  Virtual Learning Assistants for Constructed Response Assessment","IIP","SMALL BUSINESS PHASE I","07/01/2016","06/22/2016","Dharmendra Kanejiya","CA","Cognii, Inc.","Standard Grant","Benaiah Schrag","06/30/2017","$225,000.00","","dharm@cognii.com","169 11th St","San Francisco","CA","941032533","6178991744","ENG","5371","5371, 8031, 8032, 8039","$0.00","This SBIR Phase I project focuses on creating scalable Virtual Learning Assistant (VLA) technology for constructed response assessment. The best pedagogies responsible for improving learning outcomes generally involve (i) constructed response assessments and (ii) one-to-one tutoring. Students learn the best when they are given an opportunity to construct answers in their own words (instead of selecting from multiple choices) and when they receive immediate guidance and coaching in a one-to-one conversation with a human tutor. However, the costs and time associated with the constructed response assessment and one-to-one tutoring are significant, making them very difficult to scale. The proposed project will apply the most advanced technologies such as Artificial Intelligence and Natural Language Processing to solve both these problems. Students will benefit from the interactive formative assessment that engages them in a natural language conversation. This innovation is applicable across the grade levels in K-12, higher education, and adult learning and across the subjects areas such as English language arts, STEM and humanities. It will facilitate implementation of more rigorous academic standards and make online education more effective. This innovation will improve students' learning outcomes, save teachers' time and reduce the cost of delivering high quality engaging education on a large scale. <br/><br/>This project will create a new type of virtual assistant technology that is exclusively focused on education. The proposed Virtual Learning Assistant (VLA) will advance the conversational AI technology to create pedagogically rich learning and assessment environments for any topic in a content area. The VLA is uniquely distinct from general purpose virtual assistants in its ability to evaluate an answer instead of merely serving information. This project will investigate and create various algorithms for processing natural language input arising in an educational setting across different subjects or topics. The resulting web based product will allow teachers to create new high quality assessment items with minimal input and assign them to their students. When a student answers a question, the VLA will analyze it instantly for linguistic syntax and semantics using statistical and deterministic knowledge representations. The VLA will generate not only a numerical score reflecting the accuracy of the answer, but also a qualitative feedback that will guide the student towards conceptual mastery of the topic. As part of this Phase I research, a pilot study will be conducted involving teachers and students to study the efficacy of the VLA and to verify its usability and feasibility."
"1607486","US-German Research Proposal: Neurocomputation in the Visual Periphery: Experiments and Models","IIS","CRCNS-Computation Neuroscience, Robust Intelligence","12/01/2016","09/19/2017","Ruth Rosenholtz","MA","Massachusetts Institute of Technology","Continuing Grant","Kenneth Whang","11/30/2020","$681,746.00","","rruth@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7327, 7495","7327, 8089, 8091","$0.00","Peripheral vision comprises over 99.99% of the visual field. Its strengths and limitations strongly constrain visual perception -- what humans can see at a glance, and the processes by which they move their eyes to piece together information about the world. Peripheral vision differs from foveal vision in complex and interesting ways, most importantly due to ""crowding,"" in which identifying a peripheral stimulus can be substantially impaired by the presence of other, nearby stimuli. This project will examine the nature of the encoding in visual cortex, through development and testing of a set of models of peripheral vision. These models will be targeted at answering key questions about the neurobiological mechanisms. The collaborating investigators, in the US and Germany, will develop models and create a benchmark dataset of behavioral results to be explained. The models and dataset will be made freely available, to aid other researchers and to inform the development of applications such as heads up displays and user interfaces. This work will provide insight into what features are encoded in visual cortex, as well as what tradeoffs may have led the visual system to develop that encoding. Understanding those tradeoffs may inform computer vision which, like human vision, faces constraints on processing capacity. <br/><br/>The development of new model variants will be based on insights from neurophysiology, natural image statistics, sparse coding, and the recent success of convolutional neural networks in artificial intelligence. The investigators will gather benchmark behavioral phenomena far richer than existing crowding datasets, through a combination of studying natural image tasks and model-driven experiments. They will then compare predictions of the new models, as well as of Dr. Rosenholtz's existing high-performing model of peripheral vision, on the benchmark dataset. Doing so will identify the best-performing model(s), and answer key questions about the nature of pooling computations and of non-linear operators, and about the complexity, nature, and purpose of the features encoded by peripheral vision.<br/><br/>A companion project is being funded by the Federal Ministry of Education and Research, Germany (BMBF)."
"1622256","SBIR Phase I:  Cloud Based Artificial Intelligence for Trend Analysis Using Sensor Data","IIP","SMALL BUSINESS PHASE I","07/01/2016","06/20/2016","Laura Kassovic","CA","MbientLab","Standard Grant","Rick Schwerdtfeger","06/30/2017","$225,000.00","","laura@mbientlab.com","848 Girard St","San Francisco","CA","941341920","4084069149","ENG","5371","033E, 152E, 5371, 8033","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase 1 project is to address the problem of data interpretation, one of the most important and fastest growing issues caused by the influx of wearable technologies. As with all technology, wearable devices are increasing in popularity and decreasing in cost every day. Businesses rushing to catch this wave of technology paradigm are met with the complex problem of how to interpret the data gathered in a way that is accurate and useful to consumers. Many of these businesses are companies with products that were previously completely unrelated with computer or smartphone technologies. As such, they do not have the in-house expertise to not only correctly gather such data, but then analyze it for patterns that could be deemed useful in identifying behavior or conditions for the consumer. By providing a boxed solution that makes machine learning based data analysis possible for the average engineer, our project is aimed to help these businesses cross that hurdle.<br/><br/>This Small Business Innovation Research (SBIR) Phase 1 project seeks to bring Artificial Intelligence and Machine Learning systems for use in the hands of non-data/computer scientists. While machine learning and AI techniques are widely used these days in many applications ranging from Google search to Uber rides, they remain fairly esoteric topics with a high learning curve just to understand, let alone apply. We plan to address this issue differently from previous competitors by building a highly intuitive web UI on top of our existing hardware sensor platform. This allows us to leverage the data gathering and processing consistency of our hardware, along with our proprietary SDKs to ensure properly labelled and clean data. As a result, we will have a much easier time developing basic digital processing filters as well as applying machine learning techniques to the data in order to solve generic classification problems."
"1620992","SBIR Phase I:  From Search to Research with Fast Patent-document Correlations","IIP","SMALL BUSINESS PHASE I","07/01/2016","06/22/2016","Monte Shaffer","KY","Entrepreneurial Innovation","Standard Grant","Ben Schrag","06/30/2017","$225,000.00","","monte.shaffer@gmail.com","921 Beasley Street #210","Lexington","KY","405091583","5095927592","ENG","5371","5371, 8031, 8032, 8039, 9150, 9180","$0.00","This SBIR Phase I project attempts to address a fundamental question for innovators: is my idea already patented?  This do-it-yourself-initially service will empower small-business enterprises with synthesized research of over 10 million patent documents within 10 minutes based on the concepts contained within the idea. Such a comprehensive, real-time, low-cost offering is currently unavailable for small-business entrepreneurs.  Current search tools do not synthesize the results into an executive summary, do not allow an entire document to be entered as the search input, and do not perform real-time concept/correlation computations.  This proposed innovation will enable the inventor to submit an entire document (the idea) as the search query; then, utilizing network mathematics and artificial-intelligence algorithms, this service will synthesize search results in real-time summarizing what patent documents are most related to the idea based on natural-language processing.  Such a service for small-business entrepreneurs would enable them to initially ascertain the novelty of their idea and give them an on-the-go education about the natural language used in patent documents in comparison to their idea.  This meta-innovation would objectively ascertain the intellectual-property merit of any proposed technology, enable small-business innovators, and foster the acceleration of innovation development in the United States.<br/><br/>The development of latent semantic analysis (LSA) has enabled algorithm development to extract latent (or hidden) semantic structure from documents addressing two important word-sense ambiguity issues that text-matching search cannot: polysemy (single term with multiple meanings; i.e., strike as to hit [verb], to start up [verb], or to cease working [noun]) and synonymy (multiple terms with single meaning; i.e., car and automobile).  Albeit robust, this concept-search approach for large document collections is not tractable due to the high-complexity computational requirements for performing matrix singular value decomposition (SVD) necessary for LSA.   Approximation techniques that use subset approaches necessarily introduce some amount of systematic error.  To ascertain the most relevant documents in a large collection for a given focal document, this proposed innovation (search-subset LSA) will subset using proprietary search methodologies without any systematic error, reducing both the number of documents to compare and the number of terms to analyze thereby making real-time document correlations possible.  The aims of this research are: to identify the optimal subset approach for comprehensive nomological capture of top-correlation candidates, to ascertain optimal input parameters for the focal query document, and to develop a statistical test to confirm that no systematic bias is present in this approach."
"1620022","Collaborative Research:   Algorithms for Large-Scale Stochastic and Nonlinear Optimization","DMS","OE Operations Engineering, COMPUTATIONAL MATHEMATICS","08/01/2016","06/17/2016","Jorge Nocedal","IL","Northwestern University","Standard Grant","Leland Jameson","07/31/2020","$270,000.00","","j-nocedal@northwestern.edu","750 N. Lake Shore Drive","Chicago","IL","606114579","3125037955","MPS","006Y, 1271","071E, 072E, 073E, 077E, 078E, 9102, 9263","$0.00","The promise of artificial intelligence has been a topic of both public and private interest for decades. Starting in the 1990s the field has been benefited from the rapidly evolving and expanding field of machine learning. The intelligent systems that have been borne out of machine learning, such as search engines, recommendation platforms, and speech and image recognition software, have become an indispensable part of modern society.  Rooted in statistics and relying heavily on the efficiency of numerical algorithms, machine learning techniques capitalize on increasingly powerful computing platforms and the availability of very large datasets.  One of the pillars of machine learning is mathematical optimization, which, in this context, involves the computation of parameters for a system designed to make decisions based on yet unseen data. The goal of this project is to develop new optimization algorithms that will enable the continuing rise of the field of machine learning.<br/>  <br/>The research consists of two projects, which are thematically related and address the solution of optimization problems that are nonlinear, high dimensional, stochastic, involve very large data sets and in some cases are non-convex. Two families of algorithms will be developed to garner the benefits of both stochastic gradient methods and batch methods, while avoiding their shortcomings. One of these algorithms uses a gradient aggregation approach that re-uses gradient values computed at previous iterations. The challenge is to design an algorithm that is efficient in minimizing testing error, not just training error. The second approach employs adaptive sampling techniques to reduce the noise in stochastic gradient approximations as the optimization progresses. An important aspect of this research is the design of an efficient strategy for incorporating second-order information that captures curvature of the optimized loss function, even in the case when Hessian estimates are based on inaccurate gradients. In all cases, the goal is research is to design and implement algorithms in software, and test them on realistic machine learning applications."
"1620070","Collaborative Research:  Algorithms for Large-scale Stochastic and Nonlinear Optimization","DMS","COMPUTATIONAL MATHEMATICS","08/01/2016","06/17/2016","Richard Byrd","CO","University of Colorado at Boulder","Standard Grant","Leland Jameson","07/31/2020","$136,371.00","","richard@cs.colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","MPS","1271","9263","$0.00","The promise of artificial intelligence has been a topic of both public and private interest for decades. Starting in the 1990s the field has been benefited from the rapidly evolving and expanding field of machine learning. The intelligent systems that have been borne out of machine learning, such as search engines, recommendation platforms, and speech and image recognition software, have become an indispensable part of modern society.  Rooted in statistics and relying heavily on the efficiency of numerical algorithms, machine learning techniques capitalize on increasingly powerful computing platforms and the availability of very large datasets.  One of the pillars of machine learning is mathematical optimization, which, in this context, involves the computation of parameters for a system designed to make decisions based on yet unseen data. The goal of this project is to develop new optimization algorithms that will enable the continuing rise of the field of machine learning.<br/>  <br/>The research consists of two projects, which are thematically related and address the solution of optimization problems that are nonlinear, high dimensional, stochastic, involve very large data sets and in some cases are non-convex. Two families of algorithms will be developed to garner the benefits of both stochastic gradient methods and batch methods, while avoiding their shortcomings. One of these algorithms uses a gradient aggregation approach that re-uses gradient values computed at previous iterations. The challenge is to design an algorithm that is efficient in minimizing testing error, not just training error. The second approach employs adaptive sampling techniques to reduce the noise in stochastic gradient approximations as the optimization progresses. An important aspect of this research is the design of an efficient strategy for incorporating second-order information that captures curvature of the optimized loss function, even in the case when Hessian estimates are based on inaccurate gradients. In all cases, the goal is research is to design and implement algorithms in software, and test them on realistic machine learning applications."
"1619855","Conference on Statistical Machine Learning and Data Science","DMS","STATISTICS","06/15/2016","06/09/2016","Yufeng Liu","NC","University of North Carolina at Chapel Hill","Standard Grant","Gabor J. Szekely","05/31/2017","$15,000.00","","yfliu@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","MPS","1269","7556","$0.00","The University of North Carolina at Chapel Hill will host a three day conference on Statistical Machine Learning and Data Science from June 6-8, 2016. (See http://www.unc.edu/~yfliu/sldm2016/.) The objective is to bring together researchers in statistical learning and data science from academia, industry, and government in a stimulating atmosphere focused on the development of statistical learning theory, methods and applications. Statistical machine learning is a relatively new discipline, evolving from machine learning methods of artificial intelligence and multivariate statistics. It also plays an essential role for the new important area of data science and big data. NSF funding will provide travel support to increase the number of junior researchers who are able to attend.<br/><br/>The Section on Statistical Machine Learning and Data Science of the American Statistical Association will hold a meeting at the University of North Carolina at Chapel Hill from June 6-8, 2016. Topics of the conference include, but are not limited to, big data analytics, classification, computational biology, covariance estimation, graphical models, high dimensional data, learning theory, model selection, network analysis, precision medicine, and signal and image processing. This award will provide travel support for junior researchers."
"1637547","RI:   Doctoral Student Consortium at the Twenty Fourth International Conference on Case-Based Reasoning","IIS","Robust Intelligence","07/15/2016","07/17/2018","Ashok Goel","GA","Georgia Tech Research Corporation","Standard Grant","James Donlon","12/31/2018","$10,000.00","","ashok.goel@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7495","7495, 7556","$0.00","This project will support of the Doctoral Student Consortium at the Twenty Fourth International Conference on Case-Based Reasoning (ICCBR'16) to be held at Georgia Institute of Technology in Atlanta, USA, from October 31st through November 2nd 2016. ICCBR is an international conference series covering all aspects of research on case-based reasoning, such as memory, problem solving, decision making, learning, recommender systems, analogy, creativity, and applications in industry, business and government. Research on case-based reasoning often is interdisciplinary, with connections to artificial intelligence and intelligent agents, cognitive science and cognitive computing, human-centered computing and interactive systems, and human learning as well as machine learning.<br/><br/>With the resurgence of cognitive systems cognitive computing as evidenced by IBM's Watson system and Apple?s Siri program among many others, the ICCBR community is growing once again. The ICCBR conferences attempt to foster research carried out in the original spirit of AI, which aimed to design and implement computer programs that exhibited the breadth, generality, and flexibility often observed in human intelligence on one hand, and use the design of AI systems for insights into human intelligence on the other. We expect ICCBR'16 to significantly increase our understanding of case-based cognitive systems, and add momentum to the development of new theories, techniques and tools for case-based cognitive computing. The conference sessions and workshops will help develop human research capital by enabling interactions between senior and junior researchers and catalyzing new collaborations. The doctoral consortium will increase the exposure and visibility of young graduate student researchers in these areas, and train them by providing early input and feedback in the field in an interactive and constructive environment."
"1560478","Research Experience for Undergraduates in UAV Technologies","EEC","","08/01/2016","07/21/2016","Subodh Bhandari","CA","Cal Poly Pomona Foundation, Inc.","Standard Grant","Mary Poats","01/31/2020","$380,001.00","Fang Tang","sbhandari@cpp.edu","3801 West Temple, Bldg 55","Pomona","CA","917682557","9098692948","ENG","P226","7736, 9250","$0.00","This Research Experiences for Undergraduates (REU) Site program at California State Polytechnic University, Pomona (CPP), offers state-of-the-art, multi-disciplinary research experiences in unmanned aerial vehicles (UAV) technologies, engineering, and computer science to diverse and talented cohorts of undergraduates, particularly women and Hispanic students, from 2 and 4 year institutions with limited or no research opportunities. UAV's have the potential of replacing manned aircraft for dull, dirty, and dangerous missions. In addition, UAV's are less expensive than manned aircraft and pose no risk to human operators. Military applications include intelligence, surveillance, and reconnaissance (ISR), battlefield damage assessment, and force protection.  Civilian applications include remote sensing, scientific research, search and rescue missions, border patrol, surveillance of disaster-affected areas, aerial photography, aerial mapping for geotechnical survey, vegetation growth analysis, crop dusting, and precision agriculture.  The UAV industry is the fastest growing sector of the aerospace industry.  However, there is a lack of professionals entering the workforce for UAV related jobs.  This REU program is designed to increase students' interest in UAV technologies by means of first-hand experience on UAV research with direct mentorship by faculty advisors from various departments within the CPP Colleges of Engineering and Science.  <br/><br/>This REU Site offers undergraduates, in collaboration with CPP faculty and graduate students, opportunities to conduct research during a 10-week summer program, on state-of-the-art technologies and advanced research projects in UAV flight dynamic and control, computer vision, artificial intelligence, embedded systems, and robotics. In addition to their research, students will participate in weekly research seminars, research meetings, and professional development seminars. The seminars will include topics such as literature review, writing a scientific paper, improving written and oral communication skills, technical presentations, graduate education, career paths, resume building, and ethics in science and engineering. The 10-week program will also include outreach activity. The students will give presentations on UAV technologies, engineering, and computer science to K-12 students at local schools. This will enhance students' communication skills, allow them to see the broader implications of their research, and see how they can positively impact society through research. The discoveries made during these collaborations will be communicated to the broader scientific community via publications and presentations. <br/><br/>This site is supported by the Department of Defense in partnership with the NSF REU program."
"1565516","CRII: CIF: Fast Algorithms for Learning Graphical Models from Scarce Data","CCF","","03/01/2016","02/16/2016","Guy Bresler","MA","Massachusetts Institute of Technology","Standard Grant","richard brown","02/28/2018","$175,000.00","","guy@MIT.EDU","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","026y","7797, 7936, 8089, 8228","$0.00","Graphical models (GMs) are a powerful framework used to succinctly represent complex high-dimensional phenomena. Statistical dependence between variables is represented combinatorially via edges in a graph, and this allows both model interpretability and computationally efficient inference. For these reasons, GMs are at the core of machine learning and artificial intelligence and have been used in a variety of applied fields, including finance, operations research, biology, signal processing, and social networks. For large complex data with non-obvious structure, the central problem is that of learning an appropriate model. Learning a graphical model presents both a computational and statistical challenge. The combinatorial nature of the problem means that there are a huge number of possible models to explore. At the same time, the high-dimensional nature of modern applications means that the number of data-points is often much smaller than the dimension of the ambient parameter space: learning algorithms must therefore make efficient use of the data, which is scarce relative to the problem size. Existing approaches to learning graphical models achieve either statistical efficiency or computational efficiency, but not both.<br/><br/>This research aims for the best of both worlds: extreme computational and statistical efficiency. While practical applications demand such efficiency, it is unlikely to be attainable in complete generality, for all models. The question is, what features of real-world systems allow for tractable learning? The research entails identifying specific model subclasses of interest and developing algorithms with provable performance guarantees. Concretely, the research provides new information-theoretic lower bounds on the amount of data required to learn, and informed by these lower bounds, gives fast (computationally efficient) algorithms that are statistically near optimal."
"1639630","Computational Challenges in Machine Learning","CCF","SPECIAL PROJECTS - CCF","07/01/2016","05/13/2016","Richard Karp","CA","University of California-Berkeley","Standard Grant","Tracy Kimbrel","06/30/2017","$20,000.00","","karp@cs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","2878","7926","$0.00","The goal of this workshop is to advance the algorithmic frontier of machine learning. Target areas include Bayesian statistics, in which many of the core algorithmic problems bear similarity to problems that have been studied intensively in the theoretical computer science community; and large-scale optimization, in which a host of interesting challenges arise at the interface of theory and practical deployment.<br/><br/>The workshop will bring together researchers in algorithms, statistics, mathematics and artificial intelligence. It will be open to all potential participants, and the workshop findings (including videotapes of presentations) will be distributed to the public for comments and engagement. The organizers  will encourage students to attend the workshop, and will actively recruit scientists from a diversity of backgrounds to contribute to a wide range of algorithmic topics."
"1640681","SL-CN: Mapping, Measuring, and Modeling Perceptual Expertise","SMA","Science of Learning","09/01/2016","08/11/2016","Isabel Gauthier","TN","Vanderbilt University","Standard Grant","Soo-Siang Lim","08/31/2020","$749,955.00","Thomas Palmeri","isabel.gauthier@vanderbilt.edu","Sponsored Programs Administratio","Nashville","TN","372350002","6153222631","SBE","004Y","","$0.00","This Science of Learning Collaborative Network brings together researchers from Vanderbilt University, Carnegie-Mellon University, and University of California-San Diego to investigate how and why people differ in their ability to recognize, remember, and categorize faces and objects. Many important real-world problems, such as forensics, medical imaging, and homeland security demand precise visual understanding from human experts. Understanding individual differences in high-level visual cognition has received little attention compared to other aspects of human performance. Recent studies indicate that there likely is far greater variability than commonly acknowledged in the ability to learn high-level visual skills and that such ability is poorly predicted by general intelligence. This project supports a collaborative interdisciplinary research network that aims to develop measures of individual differences in visual recognition, relate behavioral and neural markers of individual differences, develop models that explain individual differences, and relate models with neural data. Because outcomes in many real-world domains depend on decisions based on visual information, developing measures, markers, and models of individual differences can have broader impacts on identifying real-world visual talent and improving visual performance and training. Students and fellows conducting research as part of this collaborative network, including female scientists and underrepresented minorities, will be mentored by scientists from multiple disciplines, providing them with an understanding far deeper than that achievable by a single discipline.<br/><br/>The project will support the activities of a collaborative research network on the study of individual differences in visual recognition. The scientists involved in these interdisciplinary efforts include experts in brain imaging at ultra-high field strength, cutting-edge methods in the development of psychological tests, and cognitive and ""deep"" convolutional neural network models of high-level vision. The project will investigate how functional brain activity and anatomical brain structure can predict the quality and time-course of visual performance and visual learning. The team will develop and validate tests of visual ability that can be used to make precise predictions about brain activity and behavioral performance. These brain measures and behavioral tests will be related to deep convolutional neural network models; such models are the most successful computer vision models to date, and higher layers of these hierarchical networks provide outstanding models of brain areas critical to object recognition. So far these models have not been used to understand individual differences. Instead of the typical approach seeking to achieve the best performance possible, the collaborative team will seek models that can mirror human variability, making errors when people make errors, being slow when people are slow, and displaying a range of visual abilities and learning as observed in humans.<br/><br/>The award is from the Science of Learning-Collaborative Networks (SL-CN) Program, with funding from the SBE Division of Behavioral and Cognitive Sciences (BCS), the SBE Office of Multidisciplinary Activities (SMA), and the CISE Division of Computer and Network Systems (CNS)."
"1566382","CRII: RI: Towards Abstractive Summarization of Meetings","IIS","CRII CISE Research Initiation","09/01/2016","03/02/2016","Lu Wang","MA","Northeastern University","Standard Grant","Tatiana Korelsky","08/31/2019","$147,649.00","","wangluxy@umich.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","026Y","7495, 8228","$0.00","Meeting is a common way to collaborate, share information and exchange opinions. Many available meeting transcripts, however, are lengthy, unstructured, and thus difficult to navigate. It would be time-consuming for users to access important meeting output by reading the full transcripts. Consequently, automatically generated meeting summaries is of great value to people and businesses alike by providing quick access to the essential content of past meetings. The core objective of this research project is to automatically generate abstract-style focused meeting summaries to help users digest the vast amount of meeting content in an easy manner. It helps the research community to better understand the characteristics of the meeting domain, define the summarization task in meetings in a more consistent way, improve speech summarization evaluation metrics, and allow the wide use of speech summarization techniques in many applications (such as generating meeting minutes or lecture outlines). The broader impacts of this project includes sharing insights on conversational text with social scientists, providing natural language processing research training to students, and contributing effective methods for meeting summarization to the general public.<br/><br/>This research project aims at constructing abstractive summaries of meetings by developing computational models for important outcome identification and natural language summary generation, as well as designing objective summary evaluation methods. Existing meeting summarization systems remain largely extractive: Their summaries are comprised exclusively of patchworks of utterances selected directly from the meetings to be summarized. Although relatively easy to construct, extractive approaches simply present a set of utterances as the final summary, and fall short of producing concise and readable summaries, largely due to the spontaneous nature of spoken dialogue. This project formulates a new framework that accounts for the special aspects of meetings and use them to identify the utterances that contain important outcomes. A discriminative learning-based latent variable model trained with rich features is utilized to jointly capture topic shifting and extract utterances with important outputs. To perform sentence planning and surface realization in one single process, a neural network-based natural language generation model is developed. Objective evaluation methods are designed to measure various aspects for the quality of generated summaries."
"1564840","ABI Innovation: DeepStruct:  Learning representations of protein 3-d structures and their interfaces using deep architectures","DBI","ADVANCES IN BIO INFORMATICS","07/01/2016","07/05/2016","Asa Ben-Hur","CO","Colorado State University","Standard Grant","Peter McCartney","06/30/2020","$570,295.00","Charles Anderson","asa@cs.colostate.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","BIO","1165","","$0.00","Proteins perform many cellular functions, made possible by complex networks of interactions; knowing the location of the interaction sites on the proteins is key for understanding exactly how they work.  Important applications include designing drugs and therapeutic agents.  Experimental techniques for determining the interfaces between proteins are expensive and time consuming, so computational structural biologists seek to predict these mathematically. Current prediction methods use a limited number of features hand-crafted by an expert.  An alternate approach is to learn the important features directly from all of the data, using a method called deep neural networks.  This proposal explores a combined approach: use expert intuition for some features but add the power of unsupervised learning with deep neural networks to learn additional, novel features. The results will enrich the way protein structural features are understood in terms of their functional properties, whether those are catalytic sites, protein-binding sites or other sites important to the protein structure. Certainly in the prediction of protein structure itself machine learning scoring methods are showing great promise. Aspects of the research will be used in courses offered through a recently awarded NSF-NRT training grant, The training grant establishes an interdisciplinary program at the interfaces of biology, engineering, math/statistics and computer science.  The program prepares students for a variety of career paths.  Research and education experiences will provide students with valuable expertise in a computational area that is highly valued by top technology firms, such as Google and Facebook, which have research teams exploring the possibilities of deep neural networks.<br/><br/>This work proposes a paradigm shift in the field of protein interface prediction and scoring: from hand-crafted features and standard off-the-shelf classifiers to an approach that augments existing features with automatic learning of the features that characterize the 3-d structures of proteins, combined with the use of learning algorithms that are specifically designed for the characteristics of the problem.  The proposed approach has multiple novel aspects: the proposed learning approach leverages information contained in the entire protein data bank (PDB) to learn features that characterize protein structures at multiple scales and levels of abstraction. It introduces a novel neural network architecture and regularization terms that constrain the solution towards biologically relevant results.  The primary alternative to this machine learning-based interface prediction uses docking simulations; however, current docking energy functions are not accurate enough, so that a near-native solution is often not ranked high enough on the list of outcomes to be useful.  Extensions of the proposed architectures for interface prediction will be employed for re-scoring docking solutions to improve their predictive success. A workflow that integrates docking and machine learning-based interface prediction and scoring is proposed to explore the synergism between these tasks. Information on the progress made on the project is available through the project website:  http://www.cs.colostate.edu/~asa/projects.html."
"1636983","WORKSHOP: Doctoral Consortium at HCOMP 2016","IIS","HCC-Human-Centered Computing","06/01/2016","05/18/2016","Haoqi Zhang","IL","Northwestern University","Standard Grant","Ephraim Glinert","05/31/2017","$23,772.00","","hq@northwestern.edu","750 N. Lake Shore Drive","Chicago","IL","606114579","3125037955","CSE","7367","7367, 7556","$0.00","This is funding to support a Doctoral Consortium (workshop) of approximately 12 promising doctoral students (at least 8 of whom will be from U.S. educational institutions) along with distinguished research faculty, to be held in conjunction with the HCOMP 2016 Conference on Human Computation & Crowdsourcing, which will take place October 31-November 3 in Austin, Texas, and which is sponsored by the Association for the Advancement of Artificial Intelligence (AAAI).  HCOMP is a cross-disciplinary conference that combines human-centered methods and traditional computer science to address fundamental issues in human computation and crowdsourcing.  It brings together researchers and practitioners from diverse areas such as human-computer interaction, psychology, economics, social computing, machine learning, information retrieval, databases and systems.  More information about the conference is available online at http://www.humancomputation.com/2016/.  The Doctoral Consortium will be a research-focused meeting that immediately precedes the conference, on October 30, and will enhance the scientific workforce in this emerging research area by developing a group of promising young researchers interested in human computation and crowdsourcing. The award will also enable these young researchers to attend the HCOMP 2016 conference, thus allowing them to interact with other researchers and conference events; to learn of potential career paths within academia and industry; to access an international network of researchers who can support their professional development; and to observe the interdisciplinary nature, diversity and interrelationships of research in human computation.  The Doctoral Consortium Chairs will select about six additional distinguished researchers to serve as faculty mentors; this group also will serve as the review committee for student applications. Students will be selected based on a paper giving an overview of the student's dissertation research, an explanation of why the student wants to participate in the Doctoral Consortium, a CV, and a letter of support from the student's advisor. The organizers will give preference to students who are most in need of mentoring and joining a peer group.  Moreover, the organizers will promote diversity among the selected students by selecting no more than two students from any one school, and by prioritizing the selection of women and underrepresented minorities. <br/><br/>The full-day Doctoral Consortium will include activities to guide the research of these promising young researchers. The Consortium will allow participants to interact with established researchers and with other students, through presentations, question-answer sessions, panel discussions, and invited presentations. Each participant will give a short presentation on their research and will receive feedback from at least one faculty mentor and from fellow students. The feedback will be geared toward helping the student participants understand and articulate how their research is positioned relative to other work on human computation and crowdsourcing.  The feedback will also address whether the students' topics are adequately focused for thesis research projects, whether their methods are correctly chosen and applied, and whether their results are being appropriately analyzed and presented.  Activities led by the faculty will include a panel discussion to give students more information about the process and lessons of research and life in academia and industry. To further integrate the Doctoral Consortium participants into the conference itself, students will have a chance to present their work as posters in an interactive poster session and their papers will be posted online on the workshop webpage. These activities will benefit the participants by offering each fresh perspectives and comments on their work from researchers outside their own institution, both from faculty and other students; providing a supportive setting for mutual feedback on participants' current research and guidance on future research directions; and enabling participants to form a cohort of new researchers."
"1622905","STTR Phase I: Science, Technology, and Bullying Prevention in an App: Students to School Change","IIP","STTR PHASE I","07/01/2016","06/21/2016","Kenneth Bain","NC","Mobile Cinema Park, INC.","Standard Grant","Glenn H. Larsen","12/31/2016","$225,000.00","Stephen Leff","kenneth.bain@mobilecinemapark.com","4045 Payne Road","High Point","NC","272651227","3367400530","ENG","1505","118E, 1505, 8031, 8032, 8039, 9177","$0.00","This STTR Phase I project aims to transform bullying prevention for middle school students through a revolutionary bullying prevention app. Bullying is the most common form of aggression and almost all states are mandating schools provide programming or policies. This project builds on the strong foundation of a one-of-a-kind, scientifically-grounded 90-minute 3D, interactive bullying prevention assembly for middle school students. The assembly is portable, easy to run and scalable, and evaluations demonstrated large immediate positive effects in students' desire to make changes to school climate and in their problem-solving skills, empathy, and confidence in handling bullying. This project addresses the challenge of ensuring that these immediate gains are translated into changes over time through the development of a novel app through community-based participatory research which combines empirically-based strategies with feedback from students, teachers, and administrators. This app will allow students to practice strategies learned from the assembly in a manner consistent with the growing demand for game-based mobile learning. Multiple teams will be hired throughout the US thus generating income for tax revenue and jobs.<br/><br/>The high-risk, high reward technological innovation challenge in phase 1 is aimed to develop new technological platforms that will enable each user to build his own set of events from different scenarios, all through simple touch screen capabilities and build an artificial intelligence machine learning system that will understand the student's free language. In order to achieve this goal, the organization will work with a technological consultant and develop the content using scriptwriters, focus groups, and community-based participatory research. This will create the first student-driven, downloadable bullying prevention app for use in concert with a scientifically grounded bullying prevention show. The app will build upon the 90 minute multi-media show for 7th and 8th graders in which there is a powerful 3D narrative, video illustrations for how to deal successfully with bullying situations, and an audience interactive component through handheld voting devices. Together the show and app will positively impact a school's academic and social-emotional climate. In addition, the show and student-motivated app are significantly more feasible than the teacher time- and cost-intensive models of other bullying prevention programs."
"1629459","XPS: FULL: Collaborative Research: Parallel and Distributed Circuit Programming for Structured Prediction","CCF","Exploiting Parallel&Scalabilty","08/15/2016","08/11/2016","Vivek Sarkar","TX","William Marsh Rice University","Standard Grant","Anindya Banerjee","07/31/2018","$410,000.00","","vsarkar@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","CSE","8283","","$0.00","This project develops a system for ""circuit programming,"" which allows a programmer to focus on the high-level solution to a problem rather than on the details of how the computation is organized. Circuit programming consists of writing rules that describe how data items depend on one another. The intellectual merits lie in the design of a new programming language for specifying these rules, along with the algorithms whereby the computer automatically finds efficient strategies for managing the necessary computations on available parallel hardware.  The project's broader significance and importance lie in its potential to streamline work in areas such as artificial intelligence and machine learning.  With the growing complexity of systems in these areas and their need to process big data in depth, research and teaching typically get bogged down in programming details, especially for parallel platforms; this project aims to delegate those details to automatic methods.<br/><br/>The research develops a programming system for Dyna, a circuit programming language that enables concise specification of large function graphs that may be cyclic and/or infinite. Dyna employs (1) a pattern-matching notation that augments pure Prolog with evaluation and aggregation and (2) an object-like mechanism for dynamically defining new sub-circuits as modifications of old ones.  This project is building an adaptive system that can mix forward and backward chaining to seek a fixpoint of the circuit and to update this fixpoint as the inputs change.  The system will perform compile-time and runtime analysis of the Dyna program and will map it to Habanero, a system for scheduling parallel computations on multicore processors, with extensions for task priorities, task cancellation, GPU execution, and distributed execution."
"1552097","CAREER: Pursuing New Tools for Approximation Algorithms","CCF","Algorithmic Foundations","03/01/2016","02/23/2020","Shayan Gharan","WA","University of Washington","Continuing Grant","Tracy Kimbrel","02/28/2021","$400,000.00","","shayan@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7796","1045, 7926","$0.00","Many of the large-scale industries are now employing sophisticated algorithms to solve variants of fundamental optimization problems. For example, Amazon uses a variant of the Traveling Salesman Problem (TSP) known as the vehicle routing problem for routing Amazon-Fresh Trucks. Uber solves a variant of TSP to route its shared-ride services. Many of the social networks including Facebook and Google+ solve variants of constraints satisfaction problems for their social targeting tasks. These optimization problems have ubiquitous applicability but they are computationally challenging in the sense that many are known to be NP-hard. This means that under standard assumptions they cannot be solved optimally by algorithms which terminate in reasonable time. The field of approximation algorithms attempts to develop efficient algorithms that find solutions close to the optimum. These approximation algorithms have found many applications in the real world. The project will advance state-of-the-art in approximation algorithms which will not only have impact on industry but also contribute to our fundamental understanding of P vs NP issue, which is at the core of computer science.<br/><br/>This project aims to develop new tools and techniques to obtain improved approximation algorithms for fundamental optimization problems, including the TSP and Constraint Satisfaction problems. The project intends to prove new algebraic properties of stable polynomials and use them to study graphs from an algebraic point of view. These tools will lead to design a new class of approximation algorithms. These new tools coming out of this project will be incorporated in the next generation of courses in approximation algorithms that focus on algebraic techniques. Although grounded in computer science theory, the project will also attract many graduate students outside of theory from applied fields like machine learning and artificial intelligence forming basis for interdisciplinary research."
"1637443","NRI: Collaborative Research: Learning Deep Sensorimotor Policies for Shared Autonomy","IIS","NRI-National Robotics Initiati","09/01/2016","09/08/2016","Sergey Levine","WA","University of Washington","Standard Grant","Weng-keen Wong","01/31/2017","$500,000.00","","sergey.levine@gmail.com","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","8013","8086","$0.00","Assistive robots have the potential to transform the lives of persons with upper extremity disabilities, by helping them perform basic daily activities, such as manipulating objects and feeding. However, human control of assistive robots presents substantial challenges. The high dimensionality of robotic arms means that joystick-like interfaces are unnatural hard to use intuitively, and motions resulting from direct teleoperation are often slow, imprecise, and severely limited in their dexterity. This research address these challenges by developing learning algorithms for shared autonomy, where the robot anticipates the user's intent and provides a degree of assistive autonomy to ensure fluid and successful motions. This research will also pave the way for future research that can bootstrap from teleoperation and build towards full robot autonomy. <br/><br/>The research proposes a hierarchical and multi-phased approach to shared autonomy, using techniques from deep learning and reinforcement learning. The system begins by using deep inverse reinforcement learning to quickly ascertain the user's high-level goal, such as whether the user wants to grasp a particular object or operate an appliance, from raw sensory inputs. This goal inference layer supplies objectives to the lower control layer, which consists of deep neural network control policies that can directly process raw sensory input about the environment and the user to make decisions. These policies choose low-level controls to satisfy the high-level objective while minimizing disagreement with the user's commands. The algorithms will be deployed and tested on a wheelchair-mounted robot arm with the potential to assist users with upper extremity disabilities to perform activities of daily living."
"1629564","XPS: FULL: Collaborative Research: Parallel and Distributed Circuit Programming for Structured Prediction","CCF","Exploiting Parallel&Scalabilty","08/15/2016","08/11/2016","Jason Eisner","MD","Johns Hopkins University","Standard Grant","Anindya Banerjee","07/31/2019","$415,000.00","","jason@cs.jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","CSE","8283","","$0.00","This project develops a system for ""circuit programming,"" which allows a programmer to focus on the high-level solution to a problem rather than on the details of how the computation is organized. Circuit programming consists of writing rules that describe how data items depend on one another. The intellectual merits lie in the design of a new programming language for specifying these rules, along with the algorithms whereby the computer automatically finds efficient strategies for managing the necessary computations on available parallel hardware.  The project's broader significance and importance lie in its potential to streamline work in areas such as artificial intelligence and machine learning.  With the growing complexity of systems in these areas and their need to process big data in depth, research and teaching typically get bogged down in programming details, especially for parallel platforms; this project aims to delegate those details to automatic methods.<br/><br/>The research develops a programming system for Dyna, a circuit programming language that enables concise specification of large function graphs that may be cyclic and/or infinite. Dyna employs (1) a pattern-matching notation that augments pure Prolog with evaluation and aggregation and (2) an object-like mechanism for dynamically defining new sub-circuits as modifications of old ones.  This project is building an adaptive system that can mix forward and backward chaining to seek a fixpoint of the circuit and to update this fixpoint as the inputs change.  The system will perform compile-time and runtime analysis of the Dyna program and will map it to Habanero, a system for scheduling parallel computations on multicore processors, with extensions for task priorities, task cancellation, GPU execution, and distributed execution."
"1549078","SBIR Phase I:  Big Data Analytics for Facility Operations and Management","IIP","SBIR Phase I","01/01/2016","09/06/2016","Xuesong Liu","PA","LeanFM Technologies, Inc.","Standard Grant","Peter Atherton","10/31/2016","$149,850.00","","udi@leanfmtech.com","100 S Commons","Pittsburgh","PA","152120000","4129532517","ENG","5371","5371, 8032","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is to help owners and operators of commercial and institutional buildings to improve resource allocation by analyzing data from built infrastructure to enable smarter decision-making supported by detailed, measureable, real-time knowledge. By automatically integrating building information that is stored using various software applications and formats, this innovation enables owners and facilities managers to efficiently search for information and respond to emergency and failures, and proactively plan for operation and maintenance tasks. This innovation also applies artificial intelligence to automatically conduct big data analysis and identify opportunities to improve energy efficiency and operating performance of assets and indoor environment. Organizations can not only save operating budget by reducing equipment failures and energy waste, but also improve the quality of life and productivity for occupants. <br/><br/>This Small Business Innovation Research (SBIR) Phase I project is aimed at developing middleware technology to automatically integrate and analyze both structured and unstructured data from facilities design and operations. Facilities maintenance and operating is the longest phase in the life-cycle of buildings, accounting for more than 60% of the total cost of ownership. Owners and facilities managers are faced with the challenges of efficiently managing aging and crowded building infrastructure to extend the life of assets and control costs. However, fragmented and under-analyzed building information results in most maintenance work being conducted reactively to address problems that have already caused significant loss or waste. The vision of this innovation is to develop a fully commercialized software package to enable facilities managers to be more proactive in improving building occupant comfort, aligning limited resources where they have the most significant impact, and reducing wasted energy through optimized mechanical controls. This project aims to demonstrate the conceptual feasibility of using big data analytics and machine learning to revolutionize facilities operating and maintenance decisions. The results from this applied research will include algorithms and methods to combine structured data with field collected unstructured data into qualitative and quantitative output appropriate for improved decision making."
"1700696","NRI: Collaborative Research: Learning Deep Sensorimotor Policies for Shared Autonomy","IIS","NRI-National Robotics Initiati","09/01/2016","12/06/2016","Sergey Levine","CA","University of California-Berkeley","Standard Grant","Rebecca Hwa","08/31/2020","$500,000.00","","sergey.levine@gmail.com","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","8013","8086","$0.00","Assistive robots have the potential to transform the lives of persons with upper extremity disabilities, by helping them perform basic daily activities, such as manipulating objects and feeding. However, human control of assistive robots presents substantial challenges. The high dimensionality of robotic arms means that joystick-like interfaces are unnatural hard to use intuitively, and motions resulting from direct teleoperation are often slow, imprecise, and severely limited in their dexterity. This research address these challenges by developing learning algorithms for shared autonomy, where the robot anticipates the user's intent and provides a degree of assistive autonomy to ensure fluid and successful motions. This research will also pave the way for future research that can bootstrap from teleoperation and build towards full robot autonomy. <br/><br/>The research proposes a hierarchical and multi-phased approach to shared autonomy, using techniques from deep learning and reinforcement learning. The system begins by using deep inverse reinforcement learning to quickly ascertain the user's high-level goal, such as whether the user wants to grasp a particular object or operate an appliance, from raw sensory inputs. This goal inference layer supplies objectives to the lower control layer, which consists of deep neural network control policies that can directly process raw sensory input about the environment and the user to make decisions. These policies choose low-level controls to satisfy the high-level objective while minimizing disagreement with the user's commands. The algorithms will be deployed and tested on a wheelchair-mounted robot arm with the potential to assist users with upper extremity disabilities to perform activities of daily living."
"1622082","SBIR Phase I:  Large-Scale Behavioral Analysis Utilizing Convolutional Neural Networks and Its Application to In-store Retail Marketing","IIP","SMALL BUSINESS PHASE I","07/01/2016","06/22/2016","Everett Berry","IN","Perceive, Inc.","Standard Grant","Peter Atherton","09/30/2017","$225,000.00","","everett@perceiveinc.com","9059 Technology Ln","Fishers","IN","460382828","7654308561","ENG","5371","5371, 8033","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is to develop an infrastructure for exposing and interpreting a previously unavailable dataset: fine-grained human interaction with a physical environment. Humans are continuously building and shaping the world but there exists little data to examine these effects. Beyond retail, this technology could affect how teachers layout classrooms, how disaster workers provide relief, or how factories keep their workers safe. The subtle physical details that affect humans everyday will be understood and investigated in ways not possible without the proposed system. This technology will benefit society specifically by improving the economic efficiency of retailers and broadly by increasing scientific understanding of how humans interact with their physical environments.<br/><br/>This Small Business Innovation Research (SBIR) Phase I project uses a neural network and generic 3D scene reconstruction in combination with low-cost, wireless cameras to model an environment with accurate object classification and spatial relationships. Recently, neural networks have proven adept at a variety of image classification tasks but their applications in video classification, namely for human actions, have been less explored. 3D scene reconstruction has made similar advances, progressing from images to videos but has always required some prior knowledge of the physical scene. Within the past year, multiple groups have proposed methods for completely generic scene reconstruction from multiple view cameras. Finally, energy harvesting methods for devices such as cameras and wireless transmitters have been demonstrated to be feasible in laboratory experiments but have not been incorporated into commercial products. In this project the two computer vision algorithms will be developed in parallel with camera hardware so that the software and hardware systems may be integrated and demonstrated by the end of the project."
"1637748","NRI: Collaborative Research: Learning Deep Sensorimotor Policies for Shared Autonomy","IIS","NRI-National Robotics Initiati","09/01/2016","09/08/2016","Siddhartha Srinivasa","PA","Carnegie-Mellon University","Standard Grant","James Donlon","10/31/2017","$499,792.00","","siddhartha.srinivasa@gmail.com","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","8013","8086","$0.00","Assistive robots have the potential to transform the lives of persons with upper extremity disabilities, by helping them perform basic daily activities, such as manipulating objects and feeding. However, human control of assistive robots presents substantial challenges. The high dimensionality of robotic arms means that joystick-like interfaces are unnatural hard to use intuitively, and motions resulting from direct teleoperation are often slow, imprecise, and severely limited in their dexterity. This research address these challenges by developing learning algorithms for shared autonomy, where the robot anticipates the user's intent and provides a degree of assistive autonomy to ensure fluid and successful motions. This research will also pave the way for future research that can bootstrap from teleoperation and build towards full robot autonomy. <br/><br/>The research proposes a hierarchical and multi-phased approach to shared autonomy, using techniques from deep learning and reinforcement learning. The system begins by using deep inverse reinforcement learning to quickly ascertain the user's high-level goal, such as whether the user wants to grasp a particular object or operate an appliance, from raw sensory inputs. This goal inference layer supplies objectives to the lower control layer, which consists of deep neural network control policies that can directly process raw sensory input about the environment and the user to make decisions. These policies choose low-level controls to satisfy the high-level objective while minimizing disagreement with the user's commands. The algorithms will be deployed and tested on a wheelchair-mounted robot arm with the potential to assist users with upper extremity disabilities to perform activities of daily living."
"1562098","RI: Medium: Collaborative Research: Text-to-Image Reference Resolution for Image Understanding and Manipulation","IIS","Robust Intelligence","06/01/2016","05/08/2020","Mohit Bansal","NC","University of North Carolina at Chapel Hill","Continuing Grant","Jie Yang","05/31/2021","$275,000.00","","mbansal@cs.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","7495","7495, 7924","$0.00","This project develops new technologies at the interface of computer vision and natural language processing to understand text-to-image relationships. For example, given a captioned image, the project develops techniques which determine which words (e.g. ""woman talking on phone"", ""The farther vehicle"") correspond to which image parts. From robotics to human-computer interaction, there are numerous real-world tasks that benefit from practical systems to identify objects in scenes based on language and understand language based on visual context. In particular, the project develops the first language-based image authoring tool which allows users to edit or synthesize realistic imagery using only natural language (e.g. ""delete the garbage truck from this photo"" or ""make an image with three boys chasing a shaggy dog""). Beyond the immediate impact of creating new ways for users to access and author digital images, the broader impacts of this work include three focus areas: the development of new benchmarks for the vision and language communities, outreach and undergraduate research, and leadership in promoting diversity. <br/><br/>At the core of the project are new techniques for large-scale text-to-image reference resolution (TIRR) that enable systems to automatically identify the image regions that depict entities described in natural language sentences or commands. These techniques advance image interpretation by enabling systems to perform partial matching between images and sentences, referring expression understanding, and image-based question answering. They also advance image manipulation by enabling systems that can synthesize images starting from a textual description, or modify images based on natural language commands. The main technical contributions of the project are:  (1) benchmark datasets for TIRR with comprehensive large-scale gold standard annotations that will make TIRR a standard task for recognition; (2) principled new representations for text-to-image annotations that expose the compositional nature of language using the formalism of the denotation graph; (3) new models for TIRR that perform an explicit alignment (grounding) of words and phrases to image regions guided by the structure of the denotation graph; (4) applications of TIRR methods to referring expression understanding and visual question answering; and (5) applications of TIRR to image creation and manipulation based on natural language input."
"1561968","RI: Medium: Collaborative Research: Text-to-Image Reference Resolution for Image Understanding and Manipulation","IIS","Robust Intelligence","06/01/2016","09/01/2017","James Hays","GA","Georgia Tech Research Corporation","Continuing Grant","Jie Yang","05/31/2019","$275,000.00","","hays@gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7495","7495, 7924","$0.00","This project develops new technologies at the interface of computer vision and natural language processing to understand text-to-image relationships. For example, given a captioned image, the project develops techniques which determine which words (e.g. ""woman talking on phone"", ""The farther vehicle"") correspond to which image parts. From robotics to human-computer interaction, there are numerous real-world tasks that benefit from practical systems to identify objects in scenes based on language and understand language based on visual context. In particular, the project develops the first language-based image authoring tool which allows users to edit or synthesize realistic imagery using only natural language (e.g. ""delete the garbage truck from this photo"" or ""make an image with three boys chasing a shaggy dog""). Beyond the immediate impact of creating new ways for users to access and author digital images, the broader impacts of this work include three focus areas: the development of new benchmarks for the vision and language communities, outreach and undergraduate research, and leadership in promoting diversity. <br/><br/>At the core of the project are new techniques for large-scale text-to-image reference resolution (TIRR) that enable systems to automatically identify the image regions that depict entities described in natural language sentences or commands. These techniques advance image interpretation by enabling systems to perform partial matching between images and sentences, referring expression understanding, and image-based question answering. They also advance image manipulation by enabling systems that can synthesize images starting from a textual description, or modify images based on natural language commands. The main technical contributions of the project are:  (1) benchmark datasets for TIRR with comprehensive large-scale gold standard annotations that will make TIRR a standard task for recognition; (2) principled new representations for text-to-image annotations that expose the compositional nature of language using the formalism of the denotation graph; (3) new models for TIRR that perform an explicit alignment (grounding) of words and phrases to image regions guided by the structure of the denotation graph; (4) applications of TIRR methods to referring expression understanding and visual question answering; and (5) applications of TIRR to image creation and manipulation based on natural language input."
"1563727","RI: Medium: Collaborative Research: Text-to-Image Reference Resolution for Image Understanding and Manipulation","IIS","Robust Intelligence","06/01/2016","09/01/2017","Svetlana Lazebnik","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Jie Yang","05/31/2021","$550,000.00","Julia Hockenmaier","slazebni@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7495","7495, 7924","$0.00","This project develops new technologies at the interface of computer vision and natural language processing to understand text-to-image relationships. For example, given a captioned image, the project develops techniques which determine which words (e.g. ""woman talking on phone"", ""The farther vehicle"") correspond to which image parts. From robotics to human-computer interaction, there are numerous real-world tasks that benefit from practical systems to identify objects in scenes based on language and understand language based on visual context. In particular, the project develops the first language-based image authoring tool which allows users to edit or synthesize realistic imagery using only natural language (e.g. ""delete the garbage truck from this photo"" or ""make an image with three boys chasing a shaggy dog""). Beyond the immediate impact of creating new ways for users to access and author digital images, the broader impacts of this work include three focus areas: the development of new benchmarks for the vision and language communities, outreach and undergraduate research, and leadership in promoting diversity. <br/><br/>At the core of the project are new techniques for large-scale text-to-image reference resolution (TIRR) that enable systems to automatically identify the image regions that depict entities described in natural language sentences or commands. These techniques advance image interpretation by enabling systems to perform partial matching between images and sentences, referring expression understanding, and image-based question answering. They also advance image manipulation by enabling systems that can synthesize images starting from a textual description, or modify images based on natural language commands. The main technical contributions of the project are:  (1) benchmark datasets for TIRR with comprehensive large-scale gold standard annotations that will make TIRR a standard task for recognition; (2) principled new representations for text-to-image annotations that expose the compositional nature of language using the formalism of the denotation graph; (3) new models for TIRR that perform an explicit alignment (grounding) of words and phrases to image regions guided by the structure of the denotation graph; (4) applications of TIRR methods to referring expression understanding and visual question answering; and (5) applications of TIRR to image creation and manipulation based on natural language input."
"1637736","NRI: Robots that Learn to Communicate through Natural Human Dialog","IIS","NRI-National Robotics Initiati","09/01/2016","08/01/2016","Raymond Mooney","TX","University of Texas at Austin","Standard Grant","Tatiana Korelsky","08/31/2020","$936,906.00","Peter Stone","mooney@cs.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","8013","8086","$0.00","Robots are increasingly capable and are on the threshold of becoming a ubiquitous technology. For robots to be truly useful, people must be able to effectively communicate their needs in everyday human language. Although there is a growing body of research on natural-language processing for human-robot interaction, it typically requires some form of explicit supervision provided by an engineering expert and involves unnatural, laborious training to obtain robustness and coverage. This project involves the development of human-robot dialog systems that learn to communicate with users through natural dialog, learning from repeated normal user interactions to become more robust and capable. The project supports the education of students in the areas of natural-language processing, human-robot interaction, and machine learning, where there is significant demand for educated personnel.  It is integrated with the university's  Freshman Research Initiative, which gets undergraduate students involved in research in their first year.<br/><br/>In order to develop human-robot dialog systems that learn to improve their communication skills through normal user interactions, the project integrates and adapts learning techniques from three currently disparate technical areas: semantic parsing, spoken dialog management, and perceptual language grounding.  The project adapts and integrates techniques for semantic-parser learning using combinatory categorial grammar (CCG), dialog management using Partially Observable Markov Decision Processes (POMDPs), and multi-modal language grounding using both visual and haptic sensors, in order to develop a dialog system for communicating with robots that comprise the Building Wide Intelligence (BWI) system being developed at the University of Texas at Austin. The research integrates the PI's expertise in semantic parsing and language grounding with the co-PI's expertise in robotics and reinforcement learning, forming a unique interdisciplinary team for developing novel and effective systems for human-robot interaction. The project includes rigorous evaluations using controlled experiments on a range of tasks using both on-line simulations with crowdsourced users, and natural user interaction with a mobile robot platform consisting of a wheeled Segway base and a Kinova robot arm being developed for the BWI system."
"1641005","Doctoral Consortium on Natural Language Processing for Computational Social Science","IIS","LINGUISTICS, ROBUST INTELLIGENCE","07/01/2016","05/11/2016","Jacob Eisenstein","GA","Georgia Tech Research Corporation","Standard Grant","Donald T. Langendoen","06/30/2017","$20,000.00","","jacobe@gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","1311, 7495","1311, 7495, 7556","$0.00","Computational methods such as natural language processing and machine learning have the potential to revolutionize social science. At the same time, as computer technology is increasingly embedded in everyday life, the social science implications are increasingly pressing. Unfortunately, existing research communities and educational pathways provide few opportunities for interdisciplinary exchange of ideas. The doctoral consortium and associated workshop that this proposal supports will fill this gap, by bringing together doctoral students working in natural language processing and those working in relevant areas of social science. Moreover, participating doctoral students will be paired with faculty mentors in disciplines outside of their current areas of study, thus providing for each student a novel perspective that is likely to be both challenging and inspirational.<br/><br/>Empirical Methods in Natural Language Processing (EMNLP) is one of the premier annual conferences at the intersection of natural language processing and machine learning; it attracts some of the best computational researchers in this discipline. It has also recently made efforts towards outreach to the social sciences, for example by inviting a prominent social science researcher to give a keynote address in 2015. The doctoral consortium and accompanying workshop on Computational Social Science and Natural Language Processing, which will be held at the 2016 EMNLP conference in Austin, Texas in November 2016, will continue this outreach effort in a new direction. <br/><br/>Funding will be used primarily to support students in the social sciences to attend the meeting, and they will be given an opportunity to present their current dissertation progress. In addition, keynote speakers from social science disciplines will be invited to the workshop. The invited speakers can then be paired with computer science doctoral students, and the social science doctoral students paired with EMNLP ""regulars"" that have expertise in relevant areas of natural language processing. These mentorship pairings are expected to significantly impact the direction of ongoing doctoral research, bringing natural language processing and social science closer together. Of particular interest is sociolinguistics, an empirical branch of linguistics, where the potential for impact from computational methods looks to be particularly significant.<br/><br/>The Linguistics Program in the Division of Behavioral and Cognitive Sciences is co-funding this workshop."
"1640078","E2CDA: Type I: Collaborative Research: Energy Efficient Learning Machines (ENIGMA)","CCF","Energy Efficient Computing: fr","09/01/2016","09/17/2018","Subhasish Mitra","CA","Stanford University","Continuing Grant","Sankar Basu","08/31/2020","$678,480.00","H.-S. Philip Wong","subh@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","015Y","7945","$0.00","The project will aim to develop computing hardware and software that improve the energy efficiency of learning machines by many orders of magnitude. In doing so it will enable large societal adoption of such machines, paving the way for new applications in diverse areas such as manufacturing, healthcare, agriculture, and many others. For example, machines that learn the behavioral trends of individual human beings by collecting data from myriads of sensors may be able to design the most appropriate drugs. Similarly, one may envision machines that learn trends in the weather and thereby assist in predicting the most optimized preparations for the next crop cycle. The possibilities are literally endless. However, the canonical learning machines of today need huge amount of energy, significantly hindering their adoption for widespread applications. The goal of this project will be to explore, evaluate and innovate new hardware and software paradigms that could reduce energy dissipation in learning machines by a significant amount. The team of researchers consists of experts in mathematics, neuroscience, electronic devices and materials and computer circuit and system design that will foster a unique platform for both innovative research and interdisciplinary training of graduate students.<br/><br/>We are witnessing a regimental shift in the computing paradigm. For a  vast  number  of  applications, cognitive functions such as classification, recognition, synthesis, decision-making and  learning  are gaining rapid importance in a world that is infused with sensing modalities, often paraphrased under a common term of ""Big Data"", that are in critical need of efficient information-extraction. This is in sharp contrast to the past when the central objective of computing was to perform calculations on numbers and produce results with extreme numerical accuracy. We aim to approach this problem by exploiting cognitive models that have shown efficacy in ""one shot"" learning. In this approach, the information is represented by means of high dimensional (HD) vectors. These vectors follow a set of predetermined mathematical operations that ensure that the resulting vector after such operations is unique. The uniqueness can in turn be used as ""learning"" and the predefined nature of mathematical operations make the learning ""one shot"". When paired with traditional artificial neural network or deep learning algorithms,  such  ""one  shot"" learning could significantly reduce the number of necessary computing operations, leading to orders of magnitude reduction in energy dissipation. We shall explore the entire computer hierarchy, staring from materials and devices, all the way up to system design and optimization to exploit the unique capabilities afforded by the HD computing, with the ultimate objective of realizing energy efficient learning machines (ENIGMA)."
"1640060","E2CDA: Type I: Collaborative Research: Energy Efficient Learning Machines (ENIGMA)","CCF","Energy Efficient Computing: fr","09/01/2016","09/18/2018","Sayeef Salahuddin","CA","University of California-Berkeley","Continuing Grant","Sankar Basu","08/31/2020","$1,108,185.00","Bruno Olshausen, Jan Rabaey","sayeef@eecs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","015Y","7945","$0.00","The project will aim to develop computing hardware and software that improve the energy efficiency of learning machines by many orders of magnitude. In doing so it will enable large societal adoption of such machines, paving the way for new applications in diverse areas such as manufacturing, healthcare, agriculture, and many others. For example, machines that learn the behavioral trends of individual human beings by collecting data from myriads of sensors may be able to design the most appropriate drugs. Similarly, one may envision machines that learn trends in the weather and thereby assist in predicting the most optimized preparations for the next crop cycle. The possibilities are literally endless. However, the canonical learning machines of today need huge amount of energy, significantly hindering their adoption for widespread applications. The goal of this project will be to explore, evaluate and innovate new hardware and software paradigms that could reduce energy dissipation in learning machines by a significant amount. The team of researchers consists of experts in mathematics, neuroscience, electronic devices and materials and computer circuit and system design that will foster a unique platform for both innovative research and interdisciplinary training of graduate students.<br/><br/>We are witnessing a regimental shift in the computing paradigm. For a  vast  number  of  applications, cognitive functions such as classification, recognition, synthesis, decision-making and  learning  are gaining rapid importance in a world that is infused with sensing modalities, often paraphrased under a common term of ""Big Data"", that are in critical need of efficient information-extraction. This is in sharp contrast to the past when the central objective of computing was to perform calculations on numbers and produce results with extreme numerical accuracy. We aim to approach this problem by exploiting cognitive models that have shown efficacy in ""one shot"" learning. In this approach, the information is represented by means of high dimensional (HD) vectors. These vectors follow a set of predetermined mathematical operations that ensure that the resulting vector after such operations is unique. The uniqueness can in turn be used as ""learning"" and the predefined nature of mathematical operations make the learning ""one shot"". When paired with traditional artificial neural network or deep learning algorithms,  such  ""one  shot"" learning could significantly reduce the number of necessary computing operations, leading to orders of magnitude reduction in energy dissipation. We shall explore the entire computer hierarchy, staring from materials and devices, all the way up to system design and optimization to exploit the unique capabilities afforded by the HD computing, with the ultimate objective of realizing energy efficient learning machines (ENIGMA)."
"1608762","CDS&E: Enabling Time-critical Decision-support for Disaster Response and Structural Engineering through Automated Visual Data Analytics","CMMI","CDS&E","07/15/2016","12/21/2016","Shirley Dyke","IN","Purdue University","Standard Grant","Joanne Culbertson","06/30/2019","$299,999.00","Bedrich Benes","sdyke@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","ENG","8084","7433, 9102","$0.00","After a disaster, teams of trained engineers are charged with the task of collecting perishable data. These building reconnaissance teams collect data and information from the buildings that experienced the disaster, including photographs and measurements in the region. This information is collected to better understand the consequences of these events, and to improve the design of future structures. An enormous amount of images and videos is generated in just a few days, and to gather the most critical information in the time allowed, the engineers on these teams must quickly make daily decisions on where and what data to collect to achieve their mission. This research, which harnesses powerful computer vision methods to address real world civil engineering problems, aims to develop efficient methods to analyze and organize the collected images in the field, thereby enabling teams to collect the most useful data for building resilient communities worldwide. The project will leverage decades of experience in field missions from project researchers and domestic and international collaborators. A diverse set of students will be engaged in interdisciplinary research with international opportunities.<br/><br/>The application of computer vision methods to address disaster response and structural engineering problems is not simple or straightforward. This project will systematically build the knowledge needed for their successful implementation in time-critical situations. Engineers with significant field-mission experience will annotate images. These records will provide the basis for determining the visual contents needed to make decisions in the field and how the contents are spatially interconnected in the images. This forms the foundation for determining the prior knowledge that can and must be included in the deep neural network structures to facilitate rapid decision-making in the field. To quantitatively evaluate the approach, a reconnaissance testbed will be established using a diverse set of images from past data collection missions. The computational time and accuracy will be measured and documented to establish a detailed profile of the classification results. This knowledge will enable the team collecting data during a reconnaissance mission to maximize the value of the data they collect by ensuring that they can successfully perform a given task, in a certain amount of time, applied to a suite of images. This capability will provide the evidence on which to base recommendations for further investigations and/or changes to design guidelines."
"1552635","CAREER: Interactive Training of Semantic Parsers via Paraphrasing","IIS","Robust Intelligence","02/01/2016","02/10/2020","Percy Liang","CA","Stanford University","Continuing Grant","D.  Langendoen","01/31/2021","$550,000.00","","pliang@cs.stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","7495","1045, 7495","$0.00","With the increase in popularity of virtual assistants such as Siri, there is a renewed demand for deep and robust language understanding.  Statistical semantic parsing is a promising paradigm for addressing this demand.  The key obstacle in building statistical semantic parsers is obtaining adequate training data.  This CAREER project aims to develop a new interactive framework for building a semantic parser, where the system, acting like a foreign speaker of English, asks users to paraphrase utterances that the computer already understands into ones that the computer doesn't. The framework opens up intriguing applications in education.  One such application is a bidirectional tutoring system, in which the system poses questions to the student.  The student must both answer and paraphrase the question, thereby both practicing the course material and providing training data to the system.  Natural language is a universal entry point, which can increase engagement and promote diversity. High-quality semantic parsers can drastically improve the way humans interact with computers.  In the longer term, this work can also have a significant impact on the way natural language processing systems are built.  Currently, the prevailing paradigm is very much a train-and-deploy one, whereas there are many more opportunities for improvement and personalization if deployed systems were to learn on-the-fly.<br/><br/>This project develops a new interactive framework for building a semantic parser, which aims to obtain complete coverage in a given domain.  The key idea is for the system to choose logical forms, generate probe utterances that capture their semantics, and ask users to paraphrase them into natural input utterances.  In the process, the system learns about linguistic variation and novel high-level concepts.  The data is then used to train a paraphrasing-based semantic parsing model.  Existing paraphrasing models are either transformation-based, which excel at capturing structural regularities in language or are vector-based, which excel at capturing soft similarity.  The project develops novel models to capture both. The framework developed in this project improves the state-of-the-art of natural language processing and machine learning in three ways.  First, the framework departs from the classic paradigm of gathering a dataset and learning a model; instead, an interactive system interleaves the two steps.  Second, the framework learns high-level concepts, which is crucial for natural language understanding, since words often represent complex concepts.  Finally, it resolves a classic tension between the rigidity of logical representations and the flexibility of continuous representations by capturing both in a unified model."
"1549549","STTR Phase I:  Book Discovery through Literary DNA","IIP","STTR Phase I","01/01/2016","10/19/2016","Holly Payne","CA","SkywriterRX, Inc.","Standard Grant","Peter Atherton","03/31/2017","$224,111.00","Thamar Solorio","holly@skywriterrx.com","86 Lomita Drive","Mill Valley","CA","949411446","2034345991","ENG","1505","1505, 8032","$0.00","The broader impact/commercial potential of this Small Business Technology Transfer (STTR) Phase I project will be to bring modern data analytics to the book publishing industry and apply machine learning to extract and articulate human emotion as applied to the reading of literature for the first time in history. This innovation will dramatically change the way books are discovered, resulting in the first commercial version of a book recommendation system based on the experiential reading value of books. With approximately 1.4 million new books published each year, it's extremely difficult for authors to connect with readers and for readers to find the book that is just right for them. Current recommendation systems are based on purchase history or social networks and fail to provide what readers told us are the most important factors in their reading satisfaction: writing style and how a book will make them feel. The proposed STTR project will lead to a commercially marketable product that deeply personalizes the book discovery process and perpetuates literacy. Not only will the innovation help authors and readers connect, but on an even greater scale, it will impact the way books are written, acquired, distributed and sold.<br/><br/>This Small Business Technology Transfer (STTR) Phase I project proposes to tackle the next challenge in text classification: the higher level experience of reading a book. The computational model of books will learn the relationship between content, genre, author's writing style, and the mixture of sentiments in the book that, together, define how a book will make a reader feel. The opportunity is to extend research beyond what is already possible in analyzing thematic content in texts and stylistic marks that characterize authors' writeprint into those systems that can also understand and articulate the reading experience itself. The knowledge derived from the successful completion of this research represents a new frontier in natural language processing and machine learning akin to machine reading. Using supervised learning to perform the classification of reading experience for books, the project proposes to develop a large corpus of human annotated books to use for training, development and evaluation of the approaches examined. The goal is to initially use multiple human annotators to create the training set from which the machine learning system will be trained. Then we apply machine learning to 19 million current books to generate deeply personalized book recommendations."
"1631403","Collaborative Research: NCS-FO: Learning Efficient Visual Representations From Realistic Environments Across Time Scales","IIS","ECR-EHR Core Research, IntgStrat Undst Neurl&Cogn Sys","09/01/2016","08/10/2016","Per Sederberg","OH","Ohio State University","Standard Grant","James Donlon","08/31/2018","$510,469.00","","pbs5u@virginia.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","7980, 8624","8089, 8091, 8551","$0.00","Computer vision algorithms examine images and make sense of what these images depict. Current computer vision algorithms  are able to interpret images at the level of a typical middle school student for many image interpretation tasks.  Recent advances in computer vision have led to rapid technological advances which are still unfolding but affect not only the technology industry, but education,  national security and health care. However, these new algorithms are as yet poorly understood and do not describe how natural learners such as a typical middle school student learn to understand the visual world.  This proposal draws together a team of cognitive psychologists, neuroscientists, and computer scientists to develop a new class of algorithms for computer vision inspired by the way people learn.  <br/><br/>The key insight of this proposal is that human learners, unlike many leading computer vision techniques, make extensive use of the temporal structure of visual experience to extract structure.  In the real world the image  on the human retina is almost never static.   Changes in eye position and movements of the head and body create a rich and complex temporal structure over a range of scales from hundreds of milliseconds up to days and weeks. This proposal a) develops databases of realistic and dynamically changing images in the real world and in immersive virtual reality environments, b) develops computational models for learning visual representations from temporally structured experiences  and, c) examines the brain structures supporting representations integrating time and space across scales using fMRI. The algorithms pursued in this project are inspired by recent theoretical work in the neuroscience of scale-invariant memory.  However, because the databases will be made publicly available, other researchers will be able to develop other algorithms that exploit temporal and spatial correlations.  Taken together, these efforts are intended to catalyze a new generation of techniques for human-like machine learning algorithms with applications in computer vision."
"1622950","STTR Phase I:  User-Friendly Spirometer and Mobile App for Self-Management and Home Monitoring of Asthma Patients","IIP","STTR PHASE I","07/01/2016","06/22/2016","Charvi Shetty","CA","KNOX Medical Diagnostics Inc.","Standard Grant","Jesus Soriano Molla","06/30/2017","$225,000.00","Ngoc Ly","charvi@aluna.io","175 Bluxome Street Unit 234","San Francisco","CA","941071552","4153200690","ENG","1505","1505, 8018, 8038, 8042","$0.00","The broader impact/commercial potential of this Small Business Technology Transfer (STTR) Phase I project will close the gap between the hospital and home for proper asthma care. The greatest hurdles for proper lung assessment are costly devices ranging from $1000 to $30,000, and mandatory oversight from a skilled lab technician to ensure proper use. These hospital lung function tests allow for early detection of declining lung performance up to several days in advance of visible symptoms like coughing or wheezing, but no option for reliable assessment in the home currently exists. The proposed technology incorporates machine learning into an easy to use mobile-app that replicates a trained lab technician's coaching. In combination with an affordable consumer device that measures lung function, the technology makes proper lung assessment accessible outside the hospital, for regular tracking between office visits supported with physician-guided suggestions for reducing unnecessary and costly emergency visits and hospitalizations.<br/> <br/>The proposed project offers a preventative care solution for 10 million children with asthma in the US alone. This solution entails an engaging mobile-app game controlled by a handheld device that measures lung capacity, which together provide parents with an action plan to prevent their child?s asthma symptoms before they occur. Guidelines will be developed in consultation with pediatric pulmonologists to gain results as reliable as tests conducted under the guidance of a respiratory therapist in the hospital. A large database of expert labeled lung measurements will be used to train and test the neural network model to detect and decipher feature patterns and correlations. If imprecise information is collected, the app's machine-learned algorithm will determine the cause of failure and suggest a corrective action for the next attempt. This ensures that only properly collected lung measurements trigger recommendations for action. By the end of this proposal period, the applicant will have a machine learning algorithm on a mobile-app that matches the efficacy of in-person coaching by a trained lab technician, as well as evaluate the feasibility of the proposed technology for Phase II considerations."
"1631460","Collaborative Research: NCS-FO: Learning Efficient Visual Representations From Realistic Environments Across Time Scales","IIS","ECR-EHR Core Research, IntgStrat Undst Neurl&Cogn Sys","09/01/2016","08/10/2016","Marc Howard","MA","Trustees of Boston University","Standard Grant","James Donlon","08/31/2020","$479,015.00","","marc777@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","7980, 8624","8089, 8091, 8551","$0.00","Computer vision algorithms examine images and make sense of what these images depict. Current computer vision algorithms  are able to interpret images at the level of a typical middle school student for many image interpretation tasks.  Recent advances in computer vision have led to rapid technological advances which are still unfolding but affect not only the technology industry, but education,  national security and health care. However, these new algorithms are as yet poorly understood and do not describe how natural learners such as a typical middle school student learn to understand the visual world.  This proposal draws together a team of cognitive psychologists, neuroscientists, and computer scientists to develop a new class of algorithms for computer vision inspired by the way people learn.  <br/><br/>The key insight of this proposal is that human learners, unlike many leading computer vision techniques, make extensive use of the temporal structure of visual experience to extract structure.  In the real world the image  on the human retina is almost never static.   Changes in eye position and movements of the head and body create a rich and complex temporal structure over a range of scales from hundreds of milliseconds up to days and weeks. This proposal a) develops databases of realistic and dynamically changing images in the real world and in immersive virtual reality environments, b) develops computational models for learning visual representations from temporally structured experiences  and, c) examines the brain structures supporting representations integrating time and space across scales using fMRI. The algorithms pursued in this project are inspired by recent theoretical work in the neuroscience of scale-invariant memory.  However, because the databases will be made publicly available, other researchers will be able to develop other algorithms that exploit temporal and spatial correlations.  Taken together, these efforts are intended to catalyze a new generation of techniques for human-like machine learning algorithms with applications in computer vision."
"1635309","A Natural Language Based Data Retrieval Engine for Automated Digital Data Extraction for Civil Infrastructure Projects","CMMI","CIS-Civil Infrastructure Syst","09/01/2016","07/25/2016","Hyungseok Jeong","IA","Iowa State University","Standard Grant","Cynthia Chen","11/30/2018","$285,305.00","Stephen Gilbert, Evgeny Chukharev-Hudilainen","djeong@tamu.edu","1138 Pearson","AMES","IA","500112207","5152945225","ENG","1631","029E, 036E, 039E","$0.00","This research project will create new knowledge and resources to significantly enhance the reusability of digital data during the lifecycle of civil infrastructure assets. The rapid development of digital technologies is transforming how civil infrastructure asset data and information is produced, exchanged, and managed throughout its life cycle. Despite growing digital data availability, such data cannot be fully exploited without the ability to infer meaning from the varying data terminologies entered by practitioners. The lack of common understanding of the same data, or similar data given in different terms, preclude data exchange or can lead to extraction of the wrong data and misinterpretation. This research project will leverage the advancements in linguistics and computer science to develop a novel approach that can recognize users' intention from their natural language input and automatically extract the desired data from heterogeneous datasets. The results of this research will benefit the construction industry by accelerating the industry's transition to digital data-based project delivery and asset management. The research will also broaden engineering education by creating advanced course materials both at undergraduate and graduate levels.<br/><br/>Diversity in data terminology creates an important hurdle for computer-to-computer communication, creating a big burden to end users who must perform the role of middleware in digital data exchange. This issue exists throughout the life cycle of a civil infrastructure asset. This project will develop a computational theory and a platform for its implementation to analyze users' plain English data requirements, and automatically match their intention to the data entities in heterogeneous source datasets based on semantic equivalence. To accomplish this goal, the research team will: a) utilize Natural Language Processing and machine learning techniques to recognize user's intention from their natural language queries, b) translate text-based domain knowledge into an extensive civil engineering machine-readable dictionary that defines meanings of technical terms using a text-based automated ontology learning method, c) design an algorithm that finds the most semantic-relevant data entities in digital data sets for a given keyword input, and d) test the performance of the algorithm in terms of its accuracy using civil infrastructure text documents such as technical specifications, design manuals, and guidelines. The research outcomes will provide fundamental tools and resources for other researchers and industry professionals for various text-mining and intelligence-inference systems. It will facilitate seamless data exchange between various proprietary software applications used during the life cycle of civil infrastructure assets, including applications involving design evaluation and selection, digital model construction, and regulation compliance checking."
"1564032","III: Medium: Collaborative Research: Deep Learning in Spectroscopic Domains","IIS","Info Integration & Informatics","05/01/2016","08/31/2017","Mario Parente","MA","University of Massachusetts Amherst","Continuing Grant","Sylvia Spengler","04/30/2021","$804,249.00","Sridhar Mahadevan, Mario Parente","mparente@ecs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","7364","7364, 7924","$0.00","Many problems in science today require the analysis of massive datasets. <br/>This project investigates the fundamental problem of extracting latent hidden regularities from high-dimensional scientific data sets, specifically from two different types of spectroscopic measurements -- three-dimensional hyperspectral imaging used in remote sensing of the Earth and other planets, and one-dimensional spectral signals arising from chemical analyses from laser-induced breakdown spectroscopy (LIBS), such as used currently by the Curiosity rover on Mars. The project is applying recent advances in deep learning, optimization, and machine learning to practical real-world scientific applications involving the analysis of materials from Earth and outer space, such as Mars, as well as the mapping of Martian and terrestrial surfaces through hyperspectral imagery.<br/><br/>Deep learning uses multi-layer neural networks to construct a hierarchy of latent representations of high-dimensional datasets.  This project designs novel architectures and algorithms for deep learning, and applies them to spectroscopic domains, such as LIBS and hyperspectral imaging. Three challenges from spectroscopic domains guide the research. First, in many applications such as the Curiosity rover on Mars, the number of available LIBS spectra are limited as it requires an active sensing operation followed by transmission of data by a robot situated millions of miles from Earth. A further challenge is that data from Mars is inherently unlabeled, and instrumental variations and terrain variations between Earth and Mars require solving a key transfer learning problem. For hyperspectral imaging, the project is extending work on deep learning applied to two-dimensional images to data that involves two spatial dimensions as well as the third spectral dimension, where images are recorded at multiple wavelengths. This project explores a variety of ways of designing new convolutional neural networks and other approaches that can effectively exploit the third spectral dimension."
"1564083","III: Medium: Collaborative Research: Deep Learning in Spectroscopic Domains","IIS","Info Integration & Informatics","05/01/2016","08/31/2017","Melinda Dyar","MA","Mount Holyoke College","Continuing Grant","Sylvia Spengler","04/30/2021","$394,198.00","","mdyar@mtholyoke.edu","50 College Street","South Hadley","MA","010756456","4135382000","CSE","7364","7364, 7924, 9102, 9229","$0.00","Many problems in science today require the analysis of massive datasets. <br/>This project investigates the fundamental problem of extracting latent hidden regularities from high-dimensional scientific data sets, specifically from two different types of spectroscopic measurements -- three-dimensional hyperspectral imaging used in remote sensing of the Earth and other planets, and one-dimensional spectral signals arising from chemical analyses from laser-induced breakdown spectroscopy (LIBS), such as used currently by the Curiosity rover on Mars. The project is applying recent advances in deep learning, optimization, and machine learning to practical real-world scientific applications involving the analysis of materials from Earth and outer space, such as Mars, as well as the mapping of Martian and terrestrial surfaces through hyperspectral imagery. <br/><br/>Deep learning uses multi-layer neural networks to construct a hierarchy of latent representations of high-dimensional datasets. This project designs novel architectures and algorithms for deep learning, and applies them to spectroscopic domains, such as LIBS and hyperspectral imaging. Three challenges from spectroscopic domains guide the research. First, in many applications such as the Curiosity rover on Mars, the number of available LIBS spectra are limited as it requires an active sensing operation followed by transmission of data by a robot situated millions of miles from Earth. A further challenge is that data from Mars is inherently unlabeled, and instrumental variations and terrain variations between Earth and Mars require solving a key transfer learning problem. For hyperspectral imaging, the project is extending work on deep learning applied to two-dimensional images to data that involves two spatial dimensions as well as the third spectral dimension, where images are recorded at multiple wavelengths. This project explores a variety of ways of designing new convolutional neural networks and other approaches that can effectively exploit the third spectral dimension."
"1618134","RI: AF: Small: Collaborative Research: Differentially Private Learning: From Theory to Applications","IIS","Robust Intelligence, Secure &Trustworthy Cyberspace","09/01/2016","07/21/2016","Kilian Weinberger","NY","Cornell University","Standard Grant","Rebecca Hwa","08/31/2021","$249,940.00","","kilianweinberger@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7495, 8060","7434, 7495, 7923, 7926","$0.00","Practical privacy-preserving machine learning methods are currently of critical importance in medical, financial and consumer applications, among others. The aim of this project is to develop practical private machine learning algorithms that can be easily implemented by practitioners in any field that holds sensitive data, while keeping robust privacy guarantees. The proposed research will extend the existing rigorous theoretical guarantees of differential privacy to reach the requirements of modern machine learning algorithms in concrete practical settings. The generated intellectual merit therefore spans all the way from the theory to practical algorithms. The resulting methods have the potential to benefit existing real-world applications in many high impact domains. The PIs have several ongoing and successful collaborations with medical practitioners and researchers and will evaluate the resulting algorithms on real patient data in high impact medical applications. All algorithms will be made publicly available as open source. Both PIs are dedicated towards actively hiring minorities and involving undergraduate students in research.<br/><br/>Differential privacy (DP) is now recognized as one of the most rigorous and potentially usable notions of statistical privacy, and has become a full-fledged research field.  The aim of this research is to provide reliable privacy guarantees for practical machine learning algorithms, which is invaluable to protect individuals who volunteer their sensitive data for research purposes. We identify several areas with high impact potential and propose four concrete research thrusts. (1) Private Causal Inference. Causal inference is one of the most promising new directions in machine learning, that recently has become practical. Some of the most interesting causal questions deal however with medical or government policy data, which are inherently sensitive. We propose to unite the recent breakthroughs in both fields (causal inference and DP) and derive a practical and theoretically sound method to ensure differentially private causal inference. (2) Privacy for Bayesian Global Optimization. The success of deep learning has created a surge in popularity for Bayesian Global Optimization (BGO) for hyper-parameter tuning. Simultaneously, recent publications have tied the stability properties of differential privacy to generalization in adaptive data analysis. We propose to unite these recent developments and improve the generalization of BGO using insights from DP. Here, we are not protecting individuals from privacy leaks, but algorithms from overfitting-allowing for fine trade-offs of ""privacy"" vs. efficacy. (3) Private Communication-Efficient Distributed Learning. In response to the growth of data distributed over multiple machines, we aim to design practical private and communication-efficient algorithms for supervised and unsupervised learning problems. This work will build off our recent work on distributed learning and clustering algorithms. (4) Practical Private Active Learning. In the age of big data, there has been tremendous interest both in machine learning and its application areas on designing active learning algorithms that most efficiently utilize the available data, while minimizing the need for human intervention. Recently there have been exciting results on understanding statistical and computational principles (including work by the PIs). This research will develop new foundations and new practical well-founded active learning algorithms that are not only statistically and computationally efficient, but also differentially private."
"1618714","RI: AF: Small: Collaborative Research: Differentially Private Learning: From Theory To Applications","IIS","Robust Intelligence, Secure &Trustworthy Cyberspace","09/01/2016","07/21/2016","Maria-Florina Balcan","PA","Carnegie-Mellon University","Standard Grant","Rebecca Hwa","08/31/2019","$249,729.00","","ninamf@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7495, 8060","7434, 7495, 7923, 7926","$0.00","Practical privacy-preserving machine learning methods are currently of critical importance in medical, financial and consumer applications, among others. The aim of this project is to develop practical private machine learning algorithms that can be easily implemented by practitioners in any field that holds sensitive data, while keeping robust privacy guarantees. The proposed research will extend the existing rigorous theoretical guarantees of differential privacy to reach the requirements of modern machine learning algorithms in concrete practical settings. The generated intellectual merit therefore spans all the way from the theory to practical algorithms. The resulting methods have the potential to benefit existing real-world applications in many high impact domains. The PIs have several ongoing and successful collaborations with medical practitioners and researchers and will evaluate the resulting algorithms on real patient data in high impact medical applications. All algorithms will be made publicly available as open source. Both PIs are dedicated towards actively hiring minorities and involving undergraduate students in research.<br/><br/>Differential privacy (DP) is now recognized as one of the most rigorous and potentially usable notions of statistical privacy, and has become a full-fledged research field.  The aim of this research is to provide reliable privacy guarantees for practical machine learning algorithms, which is invaluable to protect individuals who volunteer their sensitive data for research purposes. We identify several areas with high impact potential and propose four concrete research thrusts. (1) Private Causal Inference. Causal inference is one of the most promising new directions in machine learning, that recently has become practical. Some of the most interesting causal questions deal however with medical or government policy data, which are inherently sensitive. We propose to unite the recent breakthroughs in both fields (causal inference and DP) and derive a practical and theoretically sound method to ensure differentially private causal inference. (2) Privacy for Bayesian Global Optimization. The success of deep learning has created a surge in popularity for Bayesian Global Optimization (BGO) for hyper-parameter tuning. Simultaneously, recent publications have tied the stability properties of differential privacy to generalization in adaptive data analysis. We propose to unite these recent developments and improve the generalization of BGO using insights from DP. Here, we are not protecting individuals from privacy leaks, but algorithms from overfitting-allowing for fine trade-offs of ""privacy"" vs. efficacy. (3) Private Communication-Efficient Distributed Learning. In response to the growth of data distributed over multiple machines, we aim to design practical private and communication-efficient algorithms for supervised and unsupervised learning problems. This work will build off our recent work on distributed learning and clustering algorithms. (4) Practical Private Active Learning. In the age of big data, there has been tremendous interest both in machine learning and its application areas on designing active learning algorithms that most efficiently utilize the available data, while minimizing the need for human intervention. Recently there have been exciting results on understanding statistical and computational principles (including work by the PIs). This research will develop new foundations and new practical well-founded active learning algorithms that are not only statistically and computationally efficient, but also differentially private."
"1640428","Workshop:   Uphill Battles in Language Technology","IIS","ROBUST INTELLIGENCE","06/15/2016","06/16/2016","Michael White","OH","Ohio State University","Standard Grant","Tatiana D. Korelsky","05/31/2017","$31,977.00","Annie Louis, Michael Roth, Bonnie Webber, Luke Zettlemoyer","mwhite@ling.ohio-state.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","7495","7495, 7556","$0.00","While recent approaches to Natural Language Processing (NLP) based on<br/>machine learning from large quantities of data have delivered valuable<br/>technology, this success has obscured earlier, more visionary goals of<br/>getting computers to understand stories, to engage in natural,<br/>cooperative dialogues with people, and to translate text and speech<br/>fluently and accurately from one human language to another.  As such,<br/>this workshop highlights the value of bringing these early visionary<br/>goals back into view. <br/><br/>The workshop features established researchers who carried<br/>out early work in a major sub-field of speech and language processing<br/>engaging in a moderated public dialogue with younger researchers<br/>involved with significant new ideas and/or methods. The workshop is<br/>organized into four sessions, each with a moderator and invited<br/>speakers, on the topics of Dialogue and Speech, Natural Language<br/>Generation, Text Understanding, and Grounded Language. Dialogue in<br/>each session is enriched through questions solicited beforehand from<br/>prospective attendees, as well as impromptu questions from the<br/>audience. The workshop includes graduate student scribes who<br/>collaborate to deliver a summary of each session, and a poster session<br/>highlights the students' research.  In addition to the workshop<br/>proceedings, the workshop's outcome is a public report based on notes<br/>contributed by the speakers and the session summaries produced by the<br/>team of student scribes."
"1631815","SBIR Phase II:  Total Holographic Characterization of Colloids Through Holographic Video Microscopy","IIP","SBIR Phase II","09/15/2016","06/24/2019","Laura Philips","NY","Spheryx, Inc","Standard Grant","Benaiah Schrag","08/31/2020","$1,284,006.00","","laphilips@gmail.com","330 E 38th St, Apt 48J","New York","NY","100162784","6077380100","ENG","5373","090E, 115E, 165E, 5371, 5373, 7218, 8033, 8240, 9251","$0.00","This Small Business Innovation Research (SBIR) Phase II project will enable a commercial implementation of holographic video microscopy, a fast, precise and flexible technology for measuring the properties of individual colloidal particles suspended in fluid media. This disruptive technology solves critical manufacturing problems across industries that work with colloidal dispersions.  Demonstrated applications include: 1) monitoring the growth of nanoparticle agglomerates in precision slurries used to polish semiconductor wafers where scratches due to slurry agglomerates are responsible for waste valued at $1 billion annually; 2) tracking concentrations of dangerous contaminants in wastewater streams; and 3) measuring the concentration of protein aggregates in biopharmaceuticals, a safety concern noted by the Food and Drug Administration (FDA) in this $250 billion industry.  Holographic video microscopy is unique among particle-characterization technologies in providing comprehensive information about the size, shape and composition of individual particles in real time and in situ. Having access to this wealth of data facilitates product development, creates new opportunities for process control and provides a new tool for quality assurance across a broad spectrum of industries enabling safer, less expensive products for consumers while providing cost savings to manufacturers.<br/><br/>The technical objectives of this project are: 1) to optimize the design of the underlying holographic microscopy system without compromising the quality of results; 2) to enable quantitative concentration determination including corrections for perturbations introduced by flow dynamics; 3) to expand the domain of operation to characterize non-spherical particles and 4) to apply machine-learning algorithms for automated robust operation.  Using holographic video microscopy for commercial applications requires adaptation and innovation in the design of the prototype instrument that was used to demonstrate feasibility. Streamlining the optical train will require advanced modeling and the creation of new methods of correcting optical aberrations to enable ease of manufacture. Additional improvements in design will include advances in improving microfluidic flow control to generate accurate concentration determination, to adapt holographic analysis algorithms for characterizing the structure of aspheric particles, and to extend analytical capabilities for turbid fluids. Finally, innovative machine-learning using neural network algorithms demonstrated significant improvements for analytical robustness in Phase I and will be extended to a wider range of applications. The Phase II effort will enable holographic video microscopy of real-world samples with typical measurement times of a few minutes."
"1617767","RI: Small: Inverse Rendering by Co-Evolutionary Learning","IIS","Robust Intelligence","06/15/2016","05/26/2017","Jia Deng","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Jie Yang","10/31/2018","$466,716.00","","jiadeng@princeton.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7495","7495, 7923, 9251","$0.00","This project addresses the problem of inverse rendering: recovering 3D shape, material, and lighting from a single image. Inverse rendering is a fundamental problem in computer vision; it recovers the basic properties of a visual scene, and serves as a foundation for higher-level scene understanding such as recognizing objects, actions, and functionalities. Despite its fundamental importance, inverse rendering remains difficult. Solving inverse rendering can significantly advance computer vision and benefit a wide variety of applications from autonomous driving to assisting the visually impaired. This project develops new machine learning algorithms to advance the state of the art of inverse rendering. In addition, the project contributes to education and diversity by integrating research results into courses at various levels and by recruiting underrepresented groups to participate in this research. <br/><br/>This research advances inverse rendering technologies using computer graphics and machine learning. In particular, the research team develops two machine learning systems that co-evolve as adversaries: a rendering system that learns to compose 3D scenes and renders images using a graphics engine, and an inverse rendering system that learns to recover shape, material, and lighting from the rendered images. To develop the rendering system, the research team investigates new learning algorithms for adaptive, automatic scene composition. To develop the inverse rendering system, the research team investigates new learning algorithms that integrate neural networks and physics-based vision."
"1637941","NRI: Real-Time Semantic Computer Vision for Co-Robotics","IIS","NRI-National Robotics Initiati","09/01/2016","04/13/2018","Nuno Vasconcelos","CA","University of California-San Diego","Standard Grant","Jie Yang","08/31/2020","$727,116.00","","nuno@ece.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","8013","8086, 9251","$0.00","This project develops real-time object recognition algorithms that generate extensive semantic object descriptions as a side effect of recognition. This side-information includes perceived object costs, attributes (object properties), and affordances (actions afforded by objects). With these, the act of recognizing a ""door knob"" would automatically produce the information that this is a ""flexible"" object, ""made of metal,"" which ""can be grasped"" and ""can be twisted,"" but ""cannot be eaten."" For robotics, this information is sometimes more important than the recognition of the object itself. The project enables robots to perform zero shot learning, e.g. learn to recognize door knobs by simply being told that these are objects that ""are flexible, made of metal, can be grasped and twisted but not eaten."" The research has applicability in areas such as manufacturing, intelligent systems, assisted living, and homeland security. Educationally, the project provides an exciting opportunity for undergraduate research.<br/><br/>This research develops new methods for top-down (task-driven) regularization of deep learning algorithms, though a combination of structural and loss-based regularizers. Structural regularizers constrain object and scene recognition models to guarantee speed and automatic generation of rich mid-level semantic (MLS) descriptions as a side effect of recognition. Loss-based regularizers penalize errors in the multiple semantic outputs of these models, enabling simultaneously high performance in object recognition, MLS predictions, and zero-shot learning. The resulting learning algorithms will endow robots with human-like abilities to infer rich MLS descriptions of objects and scenes as a ""side effect"" of object recognition and scene classification, in real-time. These contributions will be developed in the context of a new co-robotics problem, person-following unmanned aerial vehicles, where computer vision plays a mission critical role for tasks such as control and semantic motion planning but whose requirements in terms of speed and MLS inference are far superior to what is feasible today."
"1629913","II-New: Collaborative: A Mixed Reality Environment for Enabling Everywhere Data-Centric Work","CNS","Special Projects - CNS, CCRI-CISE Cmnty Rsrch Infrstrc, IntgStrat Undst Neurl&Cogn Sys","10/01/2016","08/29/2018","Aidong Lu","NC","University of North Carolina at Charlotte","Standard Grant","Balakrishnan Prabhakaran","09/30/2020","$493,140.00","Shaoting Zhang","alu1@uncc.edu","9201 University City Boulevard","CHARLOTTE","NC","282230001","7046871888","CSE","1714, 7359, 8624","7359, 8089, 8091, 8551, 9251","$0.00","This infrastructure project will develop an open source software toolkit, called OpenMR, to support building ""mixed reality"" data analysis systems that project data into the physical world using a new class of display devices such as Microsoft Hololens and Oculus Rift. Through OpenMR, these lightweight, wearable, mobile devices will tap into data-intensive infrastructures hosted in the cloud, with the goal of developing systems that allow users to perform data-intensive tasks from anywhere, without requiring heavy dedicated large-format displays supported by dedicated local computers.  To pursue this research, the investigators will acquire both dedicated cloud-computing servers (to support data analysis) and mixed reality hardware devices (to create the interfaces).  They will develop OpenMR to connect this hardware, to support common analysis tasks such as selecting, filtering, and classifying data, and to create data displays in the physical world. To both demonstrate the toolkit and advance data analysis research, they will build a number of prototype mixed reality interfaces for researchers whose work requires analyzing a large amount of data in domains including weather, biology, and medical imaging.  In addition to advancing those specific research areas, studying these prototypes with real users will support research around the underlying data analysis techniques, the cognitive science of how people interact with data in the physical world, and the design principles needed to build mixed reality systems.  This, in turn, will make these emerging technologies more likely to succeed and spread, and increase the chance of finding potential 'killer apps' for these systems.  The infrastructure will also directly support education and research at the partner universities around data visualization, computer graphics, computer vision, and machine learning, while the release of the toolkit will benefit the wider community.  This research is timely and important because as smart devices, in particular virtual and mixed reality devices such as Google Glass, Microsoft Hololens, Oculus Rift and Google Cardboard, become commonplace, these devices will play an increasingly important role relative to traditional laptop and digital computers when interacting with digital information. <br/><br/>The long-term vision of the project is to develop a mixed reality research infrastructure to support everywhere data-centric innovations, providing immersive, intuitive, location-free, advanced machine learning, data analysis, reduction, summary and storage tools.  This includes advanced support for the full pipeline of data-centric work in mixed reality spaces through the OpenMR open source toolkit, including front end visualization and interaction that leverages awareness of available rendering spaces and hardware along with effective visualization patterns in 2D and 3D spaces to optimize interaction; key components of data analysis and machine learning on the middle layers including automatic, generic feature engineering and joint optimization of classification performance and effective identification of discriminating features; and high-performance computing and cost-sensitive job management on the server.  The team will evaluate OpenMR's efficiency, stability, scalability, functionality, flexibility, and ease of adoption through a number of mechanisms, including self-evaluations and documentation of the design process, review from domain experts, and evaluation with both expert and novice users on data analysis tasks that cur across the specific application domains described above.  The toolkit itself will be released on the GitHub open source platform during the third year of the project after it has reached an initial level of maturity and usefulness.  The investigators will publicize OpenMR through a Youtube channel with a set of demonstration videos; outreach to relevant researchers interested in immersive visualization, visual analytics, multi-sensory human-computer interaction, machine learning with human-in-the-loop, and high-performance computing; and collaboration with undergraduates in the Students, Technology, Academia, Research, and Service Computing Corps consortium."
"1629890","II-New: Collaborative: A Mixed Reality Environment for Enabling Everywhere Data-Centric Work","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","10/01/2016","07/25/2016","Jian Huang","TN","University of Tennessee Knoxville","Standard Grant","Balakrishnan Prabhakaran","09/30/2020","$350,703.00","","huangj@eecs.utk.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","CSE","7359","7359, 9150","$0.00","This infrastructure project will develop an open source software toolkit, called OpenMR, to support building ""mixed reality"" data analysis systems that project data into the physical world using a new class of display devices such as Microsoft Hololens and Oculus Rift. Through OpenMR, these lightweight, wearable, mobile devices will tap into data-intensive infrastructures hosted in the cloud, with the goal of developing systems that allow users to perform data-intensive tasks from anywhere, without requiring heavy dedicated large-format displays supported by dedicated local computers.  To pursue this research, the investigators will acquire both dedicated cloud-computing servers (to support data analysis) and mixed reality hardware devices (to create the interfaces).  They will develop OpenMR to connect this hardware, to support common analysis tasks such as selecting, filtering, and classifying data, and to create data displays in the physical world. To both demonstrate the toolkit and advance data analysis research, they will build a number of prototype mixed reality interfaces for researchers whose work requires analyzing a large amount of data in domains including weather, biology, and medical imaging.  In addition to advancing those specific research areas, studying these prototypes with real users will support research around the underlying data analysis techniques, the cognitive science of how people interact with data in the physical world, and the design principles needed to build mixed reality systems.  This, in turn, will make these emerging technologies more likely to succeed and spread, and increase the chance of finding potential 'killer apps' for these systems.  The infrastructure will also directly support education and research at the partner universities around data visualization, computer graphics, computer vision, and machine learning, while the release of the toolkit will benefit the wider community.  This research is timely and important because as smart devices, in particular virtual and mixed reality devices such as Google Glass, Microsoft Hololens, Oculus Rift and Google Cardboard, become commonplace, these devices will play an increasingly important role relative to traditional laptop and digital computers when interacting with digital information. <br/><br/>The long-term vision of the project is to develop a mixed reality research infrastructure to support everywhere data-centric innovations, providing immersive, intuitive, location-free, advanced machine learning, data analysis, reduction, summary and storage tools.  This includes advanced support for the full pipeline of data-centric work in mixed reality spaces through the OpenMR open source toolkit, including front end visualization and interaction that leverages awareness of available rendering spaces and hardware along with effective visualization patterns in 2D and 3D spaces to optimize interaction; key components of data analysis and machine learning on the middle layers including automatic, generic feature engineering and joint optimization of classification performance and effective identification of discriminating features; and high-performance computing and cost-sensitive job management on the server.  The team will evaluate OpenMR's efficiency, stability, scalability, functionality, flexibility, and ease of adoption through a number of mechanisms, including self-evaluations and documentation of the design process, review from domain experts, and evaluation with both expert and novice users on data analysis tasks that cur across the specific application domains described above.  The toolkit itself will be released on the GitHub open source platform during the third year of the project after it has reached an initial level of maturity and usefulness.  The investigators will publicize OpenMR through a Youtube channel with a set of demonstration videos; outreach to relevant researchers interested in immersive visualization, visual analytics, multi-sensory human-computer interaction, machine learning with human-in-the-loop, and high-performance computing; and collaboration with undergraduates in the Students, Technology, Academia, Research, and Service Computing Corps consortium."
"1602428","SCH: EXP: RadiOptiMeter: Long-Term and Fine-Grained Breathing Volume Monitoring for Sleep Disordered Breathing (SDB)","IIS","Smart and Connected Health","09/01/2016","08/19/2016","Tam Vu","CO","University of Colorado at Denver","Standard Grant","Wendy Nilsen","09/30/2018","$575,000.00","Min-Hyung Choi, Ann Halbower","tam.vu@colorado.edu","MS F428, AMC Bldg 500","Aurora","CO","800452570","3037240090","CSE","8018","8018, 8061","$0.00","Sleep disordered breathing (SDB) in children is considered to be a public health problem with serious consequences such as decreased cognitive function, poor school performance, daytime sleepiness and increased cardiovascular risk. Current diagnosis of SDB is performed in hospital sleep laboratories by monitoring patients with a host of cardiorespiratory sensors attached at various positions on the patient?s body. This obtrusive form of monitoring is inconvenient to patients and require tremendous amount of attention from technicians to ensure study quality. Children, especially, tolerate the study very poorly; often removing sensors, having trouble sleeping, and necessitating repeat investigations. This project aims to develop a new method to remotely and continuously monitor breathing volume and breathing patterns of human subjects during sleep studies using optical signals assisted by radio frequency signals. We introduce RadiOptiMeter, a hybrid radio-optical breath volume monitoring approach that couple the unique characteristics of radio frequency (RF) signals with image stream captured by a depth-CO2-thermal camera to accurately estimate breathing volume of sleeping patients from afar. This study is the first step in the development of non-invasive respiratory monitoring. Utilizing this novel and non-invasive device to measure breathing during sleep will begin a research program to promote future diagnosis of SDB in the comfort of the child?s home, with no in-hospital laboratory expenses or the expense of the disposable medical equipment which equates to thousands of dollars a year in each sleep laboratory.<br/> <br/>This project will investigate a new method to continuously monitor breathing volume and breathing patterns of humans during in-hospital sleep studies using radio frequency and optical signals. We introduce RadiOptiMeter, a hybrid radio-optical breath volume monitoring approach that couple the unique characteristics of radio frequency (RF) signals with image stream captured by a depth-CO2-thermal camera to accurately and continuously estimate breathing volume of sleeping patients from afar. We propose techniques to address challenges brought about by the body movement during sleep, environmental wireless signal noises, and the diversity of patient populations. An expected outcome is a robust and accurate breathing volume monitoring system for SDB studies. The cooperation between each of the proposed devices allows us to exploit the synergistic effects of the devices to cover the limitations imposed by each device type and provides system redundancies. These redundancies ensure reliability for long-term monitoring tasks, which are critical for clinical applications. Our proposed research will make the following key contributions to enable new non-contact vital signal monitoring system: (1) Analytical models, experimental tools, and evaluation results of a breathing volume estimation method using a vision-based system (VVE) which include 4D volumetric model and skeletal structure analysis from depth- CO2-thermal (DCT) camera outputs. (2) Analytical models, experimental hardware and software components, and evaluation results of a RF-based breathing volume estimation (RVE) system, that uses neural-network-based machine learning for chest displacement- to-volume matching. (3) A hybrid radio-optical breathing volume estimation system (RadiOptiMeter) that synergistically combines the VVE and RVE to perform continuous and fine-grain monitoring. RadiOptiMeter includes body movement tracking, automatic antenna steering, and a set of controlling and synchronizing algorithms for a harmonic integration of the whole system. This collaborative effort between researchers at the Department of Computer Science and Engineering and medical doctors at Sleep Medicine Research at the Children?s Hospital Colorado is the first step in the development of non-invasive respiratory monitoring. A novel non-invasive device to measure breathing during sleep will be the first step in the necessary foundational research to promote future diagnosis of SDB in the comfort of the child?s home, with no in-hospital laboratory expenses or the expense of the disposable medical equipment which equates to thousands of dollars a year in each sleep laboratory. This project also provides an excellent methodd to train graduate students to conduct this vision-based research project. The RadiOptiMeter concepts serves as an exciting and appealing tool for structuring a variety of educational activities. Moreover, the project results will be disseminated through scholarly publications and active outreach through our existing and potential industrial partners."
"1600813","Derived Torelli Theorems, Brauer Degeneration and Universality, and Foundations of Algebraic Vision","DMS","ALGEBRA,NUMBER THEORY,AND COM","07/01/2016","06/29/2018","Max Lieblich","WA","University of Washington","Continuing Grant","James Matthew Douglass","06/30/2020","$315,000.00","","lieblich@math.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","MPS","1264","","$0.00","This award supports research on several projects in algebraic geometry and allied fields. Focused primarily on solving polynomial equations, algebraic geometry is an ancient subject that plays a key role in numerous fields of mathematics, both pure and applied. It is a linchpin of modern number theory, a heavy hammer of contemporary cryptography, and a crucial input into the computer vision systems that are transforming geography, archaeology, medicine, the automotive industry, and consumer smartphones. While it continues to expand its abstract foundations in astounding new directions, algebraic geometry is also cutting new paths into data science, statistics, and machine learning. The questions under study in this project attack both ends of this spectrum. One part of the project will focus on problems related to so-called Torelli theorems, which seek to capture and quantify the essential linearity of algebro-geometric objects, and on the Brauer group, which is an object that tightly binds algebraic geometry to mathematical physics, non-commutative algebra, and number theory. A second part of the project aims to broaden the algebro-geometric foundations of computer vision, bringing new approaches to deep problems that lie at the heart of cutting-edge applications of computer vision.<br/><br/>The first part of this research project concerns Torelli theorems and focuses on Torelli-type statements for various mixtures of derived categories and Chow theory, building on the investigator's earlier work with collaborators. This is one way of trying to get Torelli theorems in a positive characteristic setting, and the underlying ideas have already paid dividends related to the Tate conjecture for surfaces over finite fields. Research on the Brauer group aims at new degeneration methods via semistable reduction of maximal orders and universality questions for rational Brauer groups of projective spaces. The goal is to gain structural insight into Brauer classes and attack several old problems, such as the cyclicity conjecture. The part of the project on computer vision will introduce functorial methods into the study of multiview reconstruction and resection, leading to new ways of compactifying the natural incidence correspondences that occur in these problems. The work aims to help lay new flexible foundations for algebraic geometry in computer vision that will advance the relationship between the subjects."
"1614279","EAPSI: Developing a Semantic Attributes Learner through Machine Learning Approaches","OISE","EAPSI","06/01/2016","05/25/2016","Diana Kim","NJ","Kim                     Diana          S","Fellowship Award","Anne Emig","05/31/2017","$5,400.00","","","","Branchburg","NJ","088769998","","O/D","7316","5942, 5978, 7316","$0.00","This project aims to prove that machine learning approaches in computer vision can discover two key components of fine art classification: first, how humans recognize and classify different visual styles for a target object, and second, what semantic visual attributes they use to finalize their classification decision. Working with a large data set of fine art paintings, the project will investigate a computational procedure to identify a list of word descriptions of different visual styles that is interpretable to humans and is further valid to encode all styles of painting. It can be difficult to provide objective grounds that necessarily determine a visual style: even for the art expert, it is not easy to explain why Claude Monet?s Poppies is classified as impressionist based on its attributes. If the computational algorithm automatically finds semantic attributes determining visual styles that are recognizable to human observers, the result will provide scientific analysis of the human visual perceptual process which is known to be complex to specify. After stabilization, the algorithm will generate annotations describing visual styles for a massive image data set without expensive human work. This data set will be useful data set for future computer vision research. This project will be conducted in collaboration with Professor Seung Wan Hwang in the Data Intelligence Lab at Yonsei University in Korea. Professor Hwang has devised qualitative and quantitative methods to find semantic attributes through data pattern analysis.<br/><br/>This award supports a research study to design an attributes learner algorithm from datasets that will enable classification of fine art painting styles, and produce extended datasets containing valuable features of the art work that can be annotated automatically via learned attributes generators. Rather than an expensive training set of annotations to learn the attributes of interest, the PI will design a learner which automatically harvests attributes without human supervision. This approach eliminates the need for a predefined (and potentially subjective) vocabulary of semantic attributes which require expert annotation. The project will use numeric high dimensional data gotten through a Deep Artificial Neural Net (ANN) model. The ANN model is trained through a big image data targeting art style inference. With the unsupervised deep architecture that correlates images and textural data through a shared hidden layer, it is expected that the hidden layer?s positive or negative variables will be interpreted as informative attributes. The data set will include some amount of redundant and hard-to-decipher information, so it requires compression and translation to human-interpretable concepts. Since available ground truth information of the data set is limited to authors, year, and art style, cooperative work with the hosting researcher will focus on the extraction of pattern information between the ground truth information and numeric data. There has been similar research work related to feature extraction in academia, but regarding the new subject of Fine Art style and unsupervised attributes learning, this research will be innovative. <br/><br/>This award under the East Asia and Pacific Summer Institutes program supports summer research by a U.S. graduate student and is jointly funded by NSF and the National Research Foundation of Korea."
"1740371","Collaborative Research: Modeling Social Interaction and Performance in STEM Learning","DRL","REAL","08/01/2016","06/19/2017","Yoav Bergner","NY","New York University","Standard Grant","Gregg Solomon","08/31/2018","$103,504.00","","ybb2@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","EHR","7625","","$0.00","This Research on Education and Learning (REAL) project arises from an October 2014 Ideas Lab on Data-intensive Research to Improve Teaching and Learning. The intentions of that effort were to: (1) bring together researchers from across disciplines to foster novel, transformative, multidisciplinary approaches to using the data in large education-related data sets to create actionable knowledge for improving STEM teaching and learning environments in the medium term; and (2) revolutionize learning in the longer term. In this project, researchers from the Educational Testing Service, Columbia University Teachers' College, Arizona State University, and North Carolina State University will conduct data-driven, exploratory analyses to identify key places where social interactions impact learning outcomes in specific learning environments, with the goal of improving teaching and learning in large-scale STEM courses.<br/> <br/>This research takes advantage of data traces left in large-scale blended and online learning environments (including massively open online courses, or MOOCs).  The researchers will develop a comprehensive model for social learning in the context of such courses that will enable assessment of both the collaborative needs of individuals within the context of a class, and the quality of collaborations they are carrying out. Such diagnoses will allow both instructors and automated systems to provide advice to learners about the peers they might work with to enhance their learning (e.g., regarding the kinds of social interactions that will foster better understanding and development of important disciplinary capabilities). An interdisciplinary team of investigators with expertise in theory-driven educational data mining, natural-language processing, psychometrics, social-network analysis, and computer support for collaborative learning will collaborate to explore when learners in blended and online classes benefit from social interactions, and to understand how to identify more and less productive collaborative interactions.  The researchers will use data from three blended and online classes (e.g., log files capturing collaborative discussions, individual and collaborative interactions around well-instrumented examples, peer tutoring sessions, pair programming labs, paired projects) and a variety of data analysis approaches (e.g., text analysis, machine learning) to determine: (1) which cognitive, social, and affective dimensions of need and interaction can be identified from available data; (2) which analyses are useful in providing action-oriented collaboration advice; and (3) what additional types of data may be needed for making such recommendations. This exploration will be grounded in theories of social interactions for learning (e.g., self-explanation, dialectic with oneself and others, zone of proximal development, social learning theory of Bandura, peripheral and centripetal participation)."
"1629888","II-NEW: GEARS - An Infrastructure for Energy-Efficient Big Data Research on Heterogeneous and Dynamic Data","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","09/01/2016","08/09/2016","Ming Zhao","AZ","Arizona State University","Standard Grant","Wendy Nilsen","08/31/2021","$750,000.00","K. Selcuk Candan, Huan Liu, Hasan Davulcu, Fengbo Ren","mingzhao@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","7359","7359","$0.00","Big data technologies have been successfully applied to many disciplines for knowledge discovery and decision making, but the further growth and adoption of the big data paradigm face several critical challenges. First, it is challenging to meet the performance needs of modern big data problems which are inherently more difficult, e.g., learning of heterogeneous and imprecise data, and have more stringent performance requirements, e.g., real-time analysis of dynamic data. Second, power consumption is becoming a serious limiting factor to the further scaling of big data systems and the applications that it can support. These challenges demand a new type of big data systems that incorporate unconventional hardware capable of accelerating data processing and accesses while lowering the system's power consumption. Therefore, this project is developing the needed computational infrastructure to support GEARS (an enerGy-Efficient big-datA Research System) for studying heterogeneous and dynamic data using heterogeneous computing and storage resources. GEARS is a one-of-kind, energy-efficient big-data research infrastructure based on cohesively co-designed software and hardware components. It enables a variety of important studies on heterogeneous and dynamic data and advances the scientific knowledge in computer science as well as other data-driven disciplines. It enhances the training of a large body of undergraduate and graduate students, including many from underrepresented groups, by supporting unique research and education activities. Finally, it also benefits the society by contributing new open-source solutions and with potential commercial applications in support of heterogeneous and dynamic data analysis. <br/><br/> The hardware of GEARS includes a cluster of data nodes equipped with heterogeneous processors and storage devices and fine-grained power management capability. The software is developed upon widely-used big data frameworks to support unified programming across CPUs, GPUs, and FPGAs and transparent data access across a deep storage hierarchy integrating DRAM, NVM, SSD, and HDD. GEARS also enables novel systems and algorithms research on learning heterogeneous and dynamic data, including (1) new algorithm partitioning and scheduling schemes for using heterogeneous accelerators and optimizing the performance and energy efficiency of big data tasks; (2) new I/O scheduling and data staging strategies for performance and energy efficiency of the deep big-data storage hierarchy; (3) multi-phase, out-of-core decomposition techniques for large-scale tensors; (4) real-time visual analytics system that links streaming media with simulations for anticipatory analytics; (5) multi-modal deep learning methods with heterogeneous social data; (6) new computational tools for real-time analysis of social unrest using social media; (7) scalable, adaptive, and interactive team detection and assemble system for designing high-performing teams using big network data; (8) rare category analysis and heterogeneous learning algorithms for fast and accurate rare event discoveries with large and heterogeneous social data; and (9) new distributed machine learning framework for learning semantic knowledge from Web-scale images/videos with incomplete/noisy textual annotations. All project results will be shared with the broader community via the project website (http://gears.asu.edu). Publications will be listed on the website with links to their publishers. Data and software downloads will listed on the website with instructions on how to use them. Source code will be hosted on GitHub and a direct link to the repository will also be listed on the project website."
"1648576","SBIR Phase I:  Energy-Efficient Perception for Autonomous Road Vehicles","IIP","SMALL BUSINESS PHASE I","12/15/2016","12/04/2016","Forrest Iandola","CA","DeepScale, Inc","Standard Grant","Peter Atherton","05/31/2017","$225,000.00","","forrest@deepscale.ai","1232 Royal Crest Dr","San Jose","CA","951312912","6502000082","ENG","5371","5371, 8032","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project will be to allow consumers to buy vehicles enhanced by Advanced Driver Assistance Systems (ADAS) that are more robust and more accurate. Advanced Driver Assistance Systems in general, and fully autonomous vehicles in particular, promise a number of advantages, such as: (1) reducing the number of traffic fatalities in the US and abroad, (2) enabling humans to spend less time driving and more time on other activities, and (3) reducing fossil-fuel emissions. Practical implementations of Advanced Driver Assistance Systems require a few key elements: sensors, perception systems, motion planning systems, and control/actuation systems. Based on extensive discussions with key individuals at automakers and automotive suppliers, developing robust and accurate perception systems is the biggest obstacle toward developing mass-producible autonomous road vehicles.<br/><br/>This Small Business Innovation Research (SBIR) Phase I project will create perception systems that utilize the rapidly evolving technologies of deep learning for computer vision. Specifically, the company will utilize deep learning to provide perceptual systems that are: 1) more robust in the presence of diverse and rapidly evolving sensor configurations; 2) more accurate due to the early fusion of sensor data; and 3) more accurate due to the application of state-of-the-art deep learning algorithms for computer vision. The company is already engaged in developing partnerships with automotive OEMs and semiconductor suppliers that will enable it to deliver proofs-of-concept of its unique approach."
"1622265","SBIR Phase I:  A New Paradigm for Physical Security Information:  A Platform Integrating Social Media and Online News with Information Sharing Across Trusted Networks","IIP","SMALL BUSINESS PHASE I","07/01/2016","06/24/2016","Hollis Hurst","WA","Stabilitas Intelligence Communications, Inc.","Standard Grant","Peter Atherton","06/30/2017","$225,000.00","","chris@stabilitas.io","6701 Fox Ave S","Seattle","WA","981083417","7706053035","ENG","5371","5371, 8032","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase 1 project is as follows.  Commercially, the technology described herein has the capability to provide faster, granular, dynamic information about safety and security, globally, to firms, universities, governments, and NGOs.  This project uses novel methods to improve Natural Language Processing and Machine Learning.  As a result, better geo-parsing of digital sources of news about security will result in risk content that can be aggregated, displayed, and analyzed in original ways.  This enables security managers at these organizations to better understand risk and protect their staff, providing a higher quality of care.  For non-governmental organizations (including firms), this enables improved decision-making about operations, travel, and investment.  For governments, this enables improved physical security resource allocation.  Socially, this project has the potential to improve transparency and accountability regarding trends about safety and security, by improving the aggregation and visualization of data.  As an example, groups of firms and governments in emerging markets can collectively identify previously unnoticed patterns of insecurity, in support of public accountability.  <br/><br/>This Small Business Innovation Research (SBIR) Phase I project is an innovation over the state of the art in the following ways.  First, this project builds on current geo-parsing extraction methodologies by adding methodologies unique to the safety and security space.  Second, this project uses external data sources for cross correlations to improve the ""aboutness"" and granularity of extracted reports.  Third, this project exploits contributions from users at the organizational level - as well as individuals.  That is, this project supports the growth of an ecosystem in which human users of information also contribute to the quality, volume, and timeliness of that information. This contribution is also intended to improve the geo-parsing methodologies via machine learning.  The opportunity is the improvement of geo-parsing extraction mechanisms.  The research objectives are to test the hypotheses that NLP algorithms can exploit patterns unique to the safety and security space; that external sources of news can be exploited for improved granularity and ""aboutness"" scores; and that user-generated content can serve to support an ecosystem of information sharing. The anticipated results are that the above innovations will result in usability scoring sufficient for the safety and security use case."
"1617917","RI: Small: Texture2Text: Rich Language-Based Understanding of Textures for Recognition and Synthesis","IIS","Robust Intelligence, Unallocated Program Costs","09/01/2016","09/22/2016","Subhransu Maji","MA","University of Massachusetts Amherst","Continuing Grant","Jie Yang","08/31/2020","$450,000.00","","smaji@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","7495, 9199","7495, 7923","$0.00","This project develops techniques at the interface of vision and natural language to understand and synthesize textures. For example, given a texture the project develops techniques that provide a description of the pattern (e.g., ""the surface is slippery"", ""red polka-dots on a white background""). Techniques for semantic understanding of textures benefit a large number of applications ranging from robotics where understanding material properties of surfaces is key to interaction, to analysis of various forms of imagery for meteorology, oceanography, conservation, geology, and forestry. In addition, the project develops techniques that allow modification and synthesis of textures based on natural language descriptions (e.g., ""make the wallpaper more zig-zagged"", ""create a honeycombed pattern""), enabling new human-centric tools for creating textures. In addition to the numerous applications enabled by this project, the broader impacts of the work include: the development of new benchmarks and software for computer vision and language communities, undergraduate research and outreach, and collaboration with researchers and citizen scientists in areas of conservation.<br/><br/>This research maps visual textures to natural language descriptions and vice versa. The research advances computer vision by providing texture representations that are robust to realistic imaging conditions, clutter, and occlusions in natural scenes; content retrieval by providing new ways to search and retrieve textures using descriptions; and image manipulation by providing new ways to create and modify textures using descriptions. The main technical contributions of the project are: (1) principled architectures that combine aspects of texture models with deep learning to enable end-to-end learning of texture representations; (2) techniques for understanding the properties of these representations through visualizations; (3) a large-scale benchmark to evaluate techniques for language-based texture understanding; (4) new models for texture captioning; (5) applications of texture representations for fine-grained recognition and semantic segmentation; and (6) techniques for retrieving and creating textures using natural language descriptions."
"1622765","STTR Phase I:  Real-time Automatic Analysis of Electroencephalograms in an Intensive Care Environment Using Deep Learning","IIP","STTR Phase I","07/01/2016","06/29/2016","Meysam Golmohammadi","PA","BioSignal Analytics, Inc","Standard Grant","Jesus Soriano Molla","06/30/2017","$223,897.00","Joseph Picone","tuf76412@temple.edu","3711 Market St Ste 800","Philadelphia","PA","191045532","6099023633","ENG","1505","1505, 8018, 8023, 8032, 8042","$0.00","The broader impact / commercial potential of this Small Business Technology Transfer Phase I project is enabling real-time seizure detection in intensive care units ? especially units at hospitals without 24/7 neurologist coverage to interpret scans in a timely manner. High performance real-time detection of critical EEG events in an ICU setting will increase the use of brain monitoring in critical care, thereby improving patient outcomes, increasing the efficiency of healthcare and decreasing the cognitive burden placed on caregivers. Current approaches to automatic detection suffer from unacceptably high false alarm rates that overwhelm care providers, and are of limited use in this environment. A reliable service would expand access to quality care for 877,500 neurologically compromised critical care patients in 4,000+ community hospitals in the United States. The market opportunity for real-time seizure detection in the ICU is approximately $80M per year. <br/> <br/>The proposed project will develop an assistive technology for EEG analysis to support clinicians in evaluating EEG signals for medically important events in an ICU environment. Analysis of EEG signals requires a highly trained neurologist, and is time consuming and expensive since identifying rare clinical events requires analysis of long data streams. Most community hospitals do not have 24/7 access to trained neurologists and can not provide continuous EEG monitoring to detect non-convulsive seizures in neurologically compromised patients. Reliable automatic detection improves patient access to long-term brain monitoring by auto-scanning EEG signals and flagging sections of the signal that need further review by a clinician. The tool reduces the amount of data needing manual review by two orders of magnitude, offering substantial productivity gains in a clinical setting. The project will leverage an innovative approach for integrating hidden Markov models, deep learning and active learning to allow the rapid development of a high performance machine learning system from minimal amounts of manually annotated data.  The resulting automatic analysis will achieve 95% detection accuracy for seizures with a false alarm rate of 1 per 8-hour period."
"1602248","Doctoral Dissertation Research: Public Beliefs and Responses to Industrial Sites","SES","SOCIOLOGY","03/15/2016","03/14/2016","Michael Macy","NY","Cornell University","Standard Grant","Toby Parcel","02/28/2018","$11,062.00","Fedor Dokshin","mwm14@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","SBE","1331","1331, 9179","$0.00","Public responses to industrial projects shape the distribution of health and environmental risks within society. They also influence the social acceptance, government regulation, and economic viability of new technologies and industries. This dissertation uses the context of the unfolding boom in gas and oil production enabled by hydraulic fracturing technology to investigate how alternative conceptions of the risks and benefits of industrial projects structure the mobilization of public opposition and support. First, the research distinguishes between NIMBY (""Not in my backyard"") opponents who mobilize on the basis of perceived local impacts, and ideological opponents who react to the industry from a larger set of political beliefs and values. Second, the researchers examine support for industrial siting that has received little attention in existing scholarship. The project introduces public comments as a new source of data for measuring public opinion. The project has potentially important implications for understanding political dynamics behind decisions to site and expand a rapidly-growing industry. The methods used for this study should be broadly applicable to other areas of study that can benefit from ""big data.""<br/><br/>Current energy development in the United States brings renewed social science to explain public reactions to industrial siting. This research tests ""NIMBY"" (not in my backyard) against a complementary explanation of public response to industrial siting based on residents' ideological commitments to help to reconcile inconsistent empirical findings about the relationship between proximity and opposition. The project will examine 91,000 public comments submitted during the regulatory reviews of hydraulic fracturing in two states, Illinois and New York. First, researchers will generate a comprehensive mapping of the relationship between geographic proximity and mobilization for and against proposed hydraulic fracturing projects. Second, the researchers will combine the comments with measures of community context and with individual-level measures of ideology to evaluate competing explanations of mobilization for, and against, industrial siting in a series of statistical analyses. Finally, as a direct test of the proposed theoretical mechanism, the researchers will use natural language processing techniques to code the body of comments for the different ways that commenters talk about hydraulic fracturing. For research on industrial siting, public comments data offer three specific advantages: (1) they offer a behavioral measure of opposition and support of an industry (2) they are geocoded, allowing for precise measurement of a commenter?s proximity to proposed industrial sites, and (3) the text of public comments gives unprecedented insight into the different conceptions that people develop of industry impacts. The results of the analysis, especially the relative effects of proximity and political ideology, may have implications for understanding the dynamics of other policy debates. Finally, the proposed project will demonstrate innovative approaches based on natural language techniques and machine learning concepts to studying these dynamics using existing public records, as well as the use of rigorous multiple methods."
"1653286","EMNLP 2016 Student Scholarship Program","IIS","ROBUST INTELLIGENCE","09/01/2016","08/31/2016","Vincent Ng","TX","University of Texas at Dallas","Standard Grant","Donald T. Langendoen","08/31/2017","$7,500.00","","vince@hlt.utdallas.edu","800 W. Campbell Rd., AD15","Richardson","TX","750803021","9728832313","CSE","7495","7495, 7556","$0.00","Established as part of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP), the EMNLP 2016 Student Scholarship Program offers scholarships that subsidize travel, conference and housing expenses to selected students attending the conference, with preference given to financially needy students who may otherwise not be able to attend. By bringing together a pool of student researchers with the scientific and engineering knowledge of NLP from all over the world, these scholarships allow the awardees to make contact with other researchers in the field, discuss research ideas, receive critical feedback on their work, and possibly establish collaborations with them. This provides the students involved with invaluable opportunities for professional growth. In the long run, the Student Scholarship Program can contribute to the maintenance and development of a diverse computational natural language workforce.<br/><br/>Organized by SIGDAT, the Association for Computational Linguistics' special interest group for linguistic data and corpus-based approaches to NLP, EMNLP is an annual conference at the intersection of NLP and machine learning. Since its inception two decades ago, EMNLP has gradually established its reputation as one of the premier conferences in NLP that attracts some of the best computational researchers in the field. It would therefore be beneficial for SIGDAT to establish a long-term relationship with the NSF through this pioneering NSF-sponsored Student Scholarship Program, which, if successful, can become an integral part of future EMNLP conferences."
"1633631","NRT-DESE: Team Science for Integrative Graduate Training in Data Science and Physical Science","DGE","NSF Research Traineeship (NRT)","09/15/2016","09/09/2016","Padhraic Smyth","CA","University of California-Irvine","Standard Grant","Vinod Lohani","08/31/2021","$2,967,150.00","Pierre Baldi, James Randerson, Daniel Whiteson, Maritza Salazar","smyth@ics.uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","EHR","1997","026Z, 7433, 9179, SMET","$0.00","Massively parallel computers simulate data about molecular phenomena at previously unimaginable scales, satellites scan the planet capturing vast sets of measurements about ecosystem health, and particle accelerators generate tremendous amounts of data revealing fundamental properties of the smallest building blocks of matter; all with potentially broad societal benefits in areas such as drug discovery, energy conservation, and materials science. To fully realize these benefits will require a workforce with the technical skills to extract useful information from massive scientific data sets, calling for new approaches to graduate student training that emphasize expertise in data-driven science. This National Science Foundation Research Traineeship (NRT) award to the University of California Irvine (UCI) will tackle this challenge by creating a training ecosystem comprised of leading UCI, national-laboratory, and private-sector researchers across particle physics, earth science, chemistry, statistics and machine learning; all bound together by expertise in the emerging Science of Team Science. The project anticipates training over sixty (60) MS and PhD students, including twenty (20) funded trainees, from diverse backgrounds in computational statistics, machine learning, earth science, particle physics, synthetic chemistry, and team science. After graduation, students from this program will have both the technical and team-science skills to be leaders in the emerging field of data-driven science, and to participate in and lead interdisciplinary research teams at national laboratories, in academia, and in industry labs.<br/><br/>The research agenda of the program seeks to create the foundation from which bridges can be built between the traditional scientific route of building interpretable models based on physical principles and data-driven modeling approaches that can provide high fidelity predictions but may lack clear interpretability in terms of the underlying science. The program will involve a number of interrelated research themes across multiple disciplines in the information and physical sciences, including machine learning (e.g. temporal and spatial data modeling, multi-scale models, deep learning, and scalable learning algorithms), particle and astroparticle physics (e.g. accelerator based experiments), earth systems science (e.g. reducing ecosystem response prediction uncertainties), and chemistry (e.g. prediction of physical properties of small molecules). A significant aspect of the program is an emphasis on team science as a core theme. Students will collaborate in small interdisciplinary research teams consisting of students and faculty with different disciplinary skills, and will take part in team-science workshops leading to student-led development of a team-science certificate in years 3 to 5 of the program. Summer internships for student participants, at both national and industry research laboratories, will serve to reinforce the students' academic training via participation in large-scale interdisciplinary data science research projects.<br/><br/>The NSF Research Traineeship (NRT) Program is designed to encourage the development and implementation of bold, new potentially transformative models for STEM graduate education training. The Traineeship Track is dedicated to effective training of STEM graduate students in high priority interdisciplinary research areas, through the comprehensive traineeship model that is innovative, evidence-based, and aligned with changing workforce and research needs."
"1611642","NSF Postdoctoral Fellowship in Biology FY 2016","DBI","Collections Postdocs","09/01/2016","05/25/2016","William Kuhn","NJ","Kuhn                    William        R","Fellowship Award","Amanda Simcox","08/31/2018","$138,000.00","","","","Bloomfield","NJ","07003","","BIO","001Y","","$0.00","William R. Kuhn<br/><br/>Proposal Number: 1611642<br/><br/>This action funds an NSF Postdoctoral Research Fellowship in Biology for FY 2016, Research Using Biological Collections. The fellowship supports a research and training plan for the Fellow to take transformative approaches to grand challenges in biology that employ biological collections in highly innovative ways.  The title of the research plan for this fellowship to William R. Kuhn is ""Leveraging face-detection methods to identify insects from field photos, automatically."" The host institution for this fellowship is the University of Tennessee (Knoxville), and the sponsoring scientist is Dr. Mongi Abidi.  <br/><br/>This work aims to automate the identification of species, thereby helping to alleviate the 'taxonomic impediment,' i.e., an urgent need for more taxonomy from fewer taxonomists. The impetus for doing this research is that although understanding Earth's species is one of the grand challenges of the twenty-first century, training and funding for the field of taxonomy (identifying and describing species) has declined markedly over the past several decades. The Fellow is integrating existing computer vision and machine-learning methods to build an automatic system for identifying species from photographs of them in their habitat.  The Fellow has three main research objectives: (1) develop and train a model to locate the subjects in images by modifying an existing method for detecting human faces in photographs; (2) characterize the images based on the appearance of the subjects' body parts by adapting an algorithm for describing the key features of an image; and (3) predict species identity by comparing features from unknown images to those of known species, utilizing a robust machine-learning framework. The Fellow is developing a system to identify dragonfly and damselfly (Odonata) species, but the underlying code will allow researchers to train systems for other organisms. The Fellow is utilizing citizen scientist data on OdonataCentral (odonatacentral.org), a digital collection of species records and imagery of Odonata from the Western Hemisphere. Images of the 600+ species included in this digital collection are being used to train the most taxonomically-broad automatic identification system ever created, and the ability of this software to accept field-based images makes it extremely versatile.<br/><br/>The Fellow is receiving advanced training in image analysis, computer vision, and machine learning, and becoming proficient in software design and programming. The Fellow will be able use these skills to solve problems in biology in the future, when he is running his own lab. The Fellow is also developing his skills as a mentor and scientific communicator. He is mentoring two undergraduates and one graduate student in computer science, encouraging them to focus on solving biological problems. The odonate identification system is being integrated into OdonataCentral as well the mobile app that it powers, Dragonfly ID. This will allows both researchers and citizen scientists to make rapid identifications in the field, for applications such as assessing biodiversity and monitoring water quality. Ultimately this research will benefit taxonomists studying other organisms, since the Fellow is releasing his source code, allowing others to train and implement their own automatic identification systems."
"1646542","CPS: Synergy: Image Modeling and Machine Learning Algorithms for Utility-Scale Solar Panel Monitoring","ECCS","EPCN-Energy-Power-Ctrl-Netwrks, CPS-Cyber-Physical Systems","10/01/2016","06/03/2020","Andreas Spanias","AZ","Arizona State University","Standard Grant","Radhakisan Baheti","09/30/2021","$616,000.00","Rajapandian Ayyanar, Cihan Tepedelenlioglu, Pavan Turaga","spanias@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","ENG","7607, 7918","155E, 7918, 9102, 9251","$0.00","The aim of this collaborative project is to increase the efficiency of utility scale solar arrays using sensors, machine learning and signal processing methods to detect faults and optimize power. New cyber-computing strategies, that rely on sensor data and imaging methods to predict solar panel shading, are used to improve efficiency.  A programmable 18kW testbed that consists of 104 panels equipped with sensors, actuators and cameras is used to validate all theoretical results and test new approaches for using solar analytics to optimize power generation.  Machine learning and dynamic image modeling algorithms are used to control each individual panel and change connection topologies to optimize power for different cloud, load, and fault conditions.<br/><br/>Outcomes of the CPS project include advances in: a) cloud movement modeling and shading prediction using computer vision algorithms, b) PV fault detection and optimization methods that will switch array topologies dynamically while limiting PV inverter transients, d) experimental (testbed) validation of all array monitoring methods, and e) secure wireless sensor and data fusion.  Theoretical and experimental research which enables real-time analytics and remote connection topology control may influence PV array standards and smart grid initiatives. The project tasks also include: education activities, outreach at high schools, and engagement with several organizations including minority and HBCU institutions to enhance diversity."
"1619448","CIF: SMALL: Information Theoretic Foundations of Data Science","CCF","Comm & Information Foundations","08/01/2016","09/19/2017","Alon Orlitsky","CA","University of California-San Diego","Continuing Grant","Phillip Regalia","07/31/2019","$323,629.00","","alon@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7797","7797, 7923, 7935","$0.00","The advent of modern data generation, collection, communication, retention, and application has brought about an important new discipline termed data science. It draws upon diverse techniques from statistics, machine learning, computer science, and information theory to analyze, understand, and most importantly, utilize, data. It therefore significantly impacts diverse fields ranging from medicine to marketing, manufacturing, finance, and security. Much of the initial work in this area has centered on simple models to to facilitate analysis. This project is focused on bringing these results closer to real-world applications such as natural language processing. The results of this work are expected to lead to a better ability to predict future events, detect anomalies, and classify data.<br/><br/>The research in this project is focused on three technical thrusts: (i) extending data science concepts to practical regimes, (ii) utilizing structure, and (iii) understanding the effect of memory in data sources. With regards to (i), the objective is to develop optimal distribution estimators not just for the traditional Kullback-Leibler divergence, but also for other metrics, and to do so for arbitrary parameter values, not just restricted asymptotic regimes. With regards to (ii), the objective is to utilize structure to improve distribution estimates and to resolve a conundrum where theorists and practitioners employ opposing distribution estimation techniques. With regards to (iii), the general objective is to study the problem of estimating distributions with memory. Preliminary results have uncovered an interesting dichotomy between compression and learning when memory is present. Through these technical thrusts, this project will study complex and realistic data models and derive results with important theoretical implications as well as applications in a variety of fields."
"1566481","CRII: RI: Learning to Predict Temporal Interestingness for Videos","IIS","Robust Intelligence","04/01/2016","04/25/2017","Eakta Jain","FL","University of Florida","Standard Grant","Jie Yang","03/31/2020","$182,634.00","","ejain@cise.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","CSE","026y, 7495","7367, 7495, 8228, 9251","$0.00","This project examines the role that implicit feedback from viewers can play in learning a temporal interestingness function for videos. The key insight is that by leveraging pupil dilation as ground truth, supervised machine learning approaches can be applied to this problem. The ubiquitous presence of cameras in every phone, and the ability to share content with the entire world have made videos a powerful tool in the hands of everyday people. This project is to address the challenge that viewers have ever-shortening attention spans, and constructing a succinct message is hard. The project provides research and training opportunities for both undergraduate and graduate students in computer vision, machine learning, and human-centered computing.<br/> <br/>This project collects a corpus of eye-tracking data as viewers watch a collection of videos, via an off-the-shelf eye-tracking device with the objective of investigating the effectiveness of a controlled brightness calibration method to separate pupillary light reflex from pupillary emotional response. The emotional response data is leveraged as dense labels for a supervised learning approach towards predicting a video interestingness function, and algorithms are developed to cut videos to their most succinct portions based on this interestingness function. A predictive model of video interestingness could potentially impact video retrieval, summarization, and search. This would impact fields as diverse as communication and online education. Further, just as research in image saliency was hugely accelerated by the use of eye-tracking data for training, validation, and benchmarking, this project can lead to a similar unification of diverse approaches, and efforts, around an implicit, temporally dense source of ground truth for temporal interestingness in videos."
"1564381","Collaborative Research: The Impact of Research Costs on the Rate and Direction of Scientific Discovery","SMA","SciSIP-Sci of Sci Innov Policy","04/01/2016","04/26/2016","Florenta Teodoridis","CA","University of Southern California","Standard Grant","Cassidy Sugimoto","03/31/2020","$156,189.00","","teodorid@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","SBE","7626","7626","$0.00","Understanding scientific and technical progress requires measures of both the rate and direction of resources directed to innovative efforts and the outputs produced.  This project leverages advances in computational power to map the evolution of research fields based on researchers? project portfolios in ideas spaces.  The research takes advantages of shocks, such as changes in policies and research costs, to use empirical research techniques to understand the factors that affect the directions into which science and technology evolve.  This work has important implications for understanding the rate and direction of technological change.  <br/> <br/>Specifically, the project uses techniques based on machine learning and natural language processing that measure the incidence and configuration of keywords in published research to quantify the similarity of groups of such articles to define idea space. And to subsequently measure the ways in which idea space evolve in response to shocks in research costs and public policies. The research applies these techniques to three contexts: (a) how changes in the costs of research materials affect research trajectories in motion-sensing technology, (b) how researchers in quantum computing change their project portfolios in response to a controversial approach that differs from an established research paradigm; and (c) how pharmaceutical firm research trajectories change in response to news about rivals? drug discovery projects<br/>"
"1564368","Collaborative Research: The Impact of Research Costs on the Rate and Direction of Scientific Discovery","SMA","SciSIP-Sci of Sci Innov Policy","04/01/2016","04/26/2016","Jeffrey Furman","MA","National Bureau of Economic Research Inc","Standard Grant","Cassidy Sugimoto","03/31/2021","$300,772.00","","furman@bu.edu","1050 Massachusetts Avenue","Cambridge","MA","021385398","6178683900","SBE","7626","7626","$0.00","Understanding scientific and technical progress requires measures of both the rate and direction of resources directed to innovative efforts and the outputs produced.  This project leverages advances in computational power to map the evolution of research fields based on researchers? project portfolios in idea spaces.  The research takes advantages of shocks, such as changes in policies and research costs, to use empirical research techniques to understand the factors that affect the directions into which science and technology evolve.  This work has important implications for understanding the rate and direction of technological change.  <br/> <br/>Specifically, the project uses techniques based on machine learning and natural language processing that measure the incidence and configuration of keywords in published research to quantify the similarity of groups of such articles to define idea space and to subsequently measure the ways in which idea space evolve in response to shocks in research costs and public policies. The research applies these techniques to three contexts: (a) how changes in the costs of research materials affect research trajectories in motion-sensing technology, (b) how researchers in quantum computing change their project portfolios in response to a controversial approach that differs from an established research paradigm; and (c) how pharmaceutical firm research trajectories change in response to news about rivals' drug discovery projects."
"1614261","EAPSI: Investigating a Novel Approach for Biological Named Entity Recognition in Text  Mining","OISE","EAPSI","06/15/2016","07/27/2016","Dally Shvets","NC","Shvets                  Dahlia         E","Fellowship Award","Anne Emig","05/31/2017","$5,400.00","","","","Charlotte","NC","282627455","","O/D","7316","5924, 5978, 7316","$0.00","Currently, using computational resources to extract information from biological literature faces many challenges due to the domain specific terminology of biological texts. In order to read and obtain information from the vast amount of available biological literature, a large amount of funds, time and manpower are required. The amount of research literature and data is always increasing, and a scalable solution to gleaning information from biological text is necessary. This project aims to develop a new technique for extracting specific annotations from biological literature. For this project, the PI will travel to Academia Sinica in Taipei, Taiwan to work with Dr. WenLian Hsu, whose expertise in the areas of natural language processing and text mining of biological literature is necessary for the implementation of this approach.<br/><br/>DNA methylation is regarded as a potential biomarker in the diagnosis and treatment of cancer. Recent studies have identified relationships between aberrant gene methylation and cancer development. In previous approaches of mining biological literature, a combination of manual curation and machine learning-based approaches have been used to extract gene methylation cancer relations. This project will utilize a novel statistical pattern based approach which focuses on identifying deep and important linguistic patterns described for disease terms and physical interactions. This approach has a unique flexible matching mechanism that captures existing disease terms or protein names within the text, regardless of their previous presence in the training data. This project provides a solution to the resource limitation of reading and analyzing large amounts of biological literature in a short amount of time.<br/><br/>This award under the East Asia and Pacific Summer Institutes program supports summer research by a U.S. graduate student and is jointly funded by NSF and the Ministry of Science and Technology of Taiwan."
"1621507","SBIR Phase I: The Teacher Practice Feedback Tool- real-time information for teachers who want to measure and improve their talk time, wait time, and question-asking skills.","IIP","SMALL BUSINESS PHASE I","07/01/2016","07/25/2016","Kimberly Mitchell","WA","Earshot LLC","Standard Grant","Glenn H. Larsen","12/31/2016","$203,193.00","","kimberly@inquirypartners.com","1100 NE Campus Pkwy","Seattle","WA","981056605","9178226074","ENG","5371","118E, 5371, 8031, 8032, 8039","$0.00","This SBIR Phase I project seeks to improve teaching by providing teachers with real-time, personalized feedback. Most teachers receive little regular, actionable feedback about their teaching to help them improve. Questions and discussions are the currency of great teachers, especially as teachers help students develop critical thinking, collaboration, communications and creativity skills, those skills in highest demand for careers in science, technology, engineering and math. This project is to design, develop, test and refine a mobile app that provides powerful, real-time instructional data for teachers at all grade levels and subject areas.<br/><br/>Through the automatic analysis of questions and discussion, teachers will have access to useful data about communication in their classrooms to help them continuously improve instruction. This project is designed to help teachers incorporate more inquiry-based instruction methods into their teaching practice. Inquiry is a research-based approach to increasing student competencies. The intended outcomes for using this application are for teachers to talk less and ask more questions, to allow sufficient wait time before calling on students, and for teachers to incorporate more and higher-level questioning into their teaching. The technology can be sold to directly teachers at schools and universities throughout the United States on a subscription basis, enabling teachers to control their own data and decide how it is used, as well as ensuring that the tool is used for improvement, rather than for evaluative purposes.  The technology being developed for this project builds upon recent advancements in natural language processing and voice analysis. This project uses machine learning and algorithms to develop a user-friendly app that analyzes teacher voice data around four key data points: talk time, wait time, question level (higher-order and lower-order questions based on a proprietary taxonomy), and question frequency. The objective of this project is to design, test and refine the first version of the app, to be used on an iPhone using an external microphone. Teachers representing various grade levels, subject areas, levels of experience, and voice characteristics will pilot this technology and provide feedback to inform development. This project will employ research methodologies such as pre and post surveys, observations, interviews, and think-aloud sessions, to inform usability and feasibility."
"1664786","TWC SBE: TTP Option: Medium: Collaborative: EPICA: Empowering People to Overcome Information Controls and Attacks","CNS","Secure &Trustworthy Cyberspace","10/14/2016","07/12/2017","Marshini Chetty","NJ","Princeton University","Standard Grant","Shannon Beck","07/31/2019","$194,829.00","","marshini@uchicago.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","8060","7434, 7924, 9102","$0.00","This project studies the security of representative personalized services, such as search engines, news aggregators, and on-line targeted advertising, and identifies vulnerabilities in service components that can be exploited by pollution attacks to deliver contents intended by attackers.<br/><br/>This project also develops defense-in-depth countermeasures against pollution attacks. These include new server-side mechanisms to prevent the various cross-site-request-forgery schemes that allow an attacker to insert actions. The defense mechanisms also include a distributed data collection, measurement and analysis framework to detect anomalies in browsing behaviors and information contents that are indicative of pollution of user profiles or population preferences. The new information analysis techniques use machine learning and natural language processing to identify differences (e.g., missing information) that are significant or important to a user. The project also develops tools to alert users and guide them to understand and repair profiles, and studies regulatory models to incentivize the industry to adopt a more transparent practice.<br/><br/>This project develops an evaluation framework to facilitate the development and adoption of technologies. The evaluation plan includes user studies involving real, diverse user groups on the Internet.<br/><br/>To transition technologies to practice, this project makes the tools freely available, and deploys data collection and measurement systems on the Internet. This project also educates users about pollution attacks and engages with users to improve the usability of the tools."
"1651902","EAGER: Vision-Based Activity Forecasting by Mining Temporal Causalities","IIS","Robust Intelligence","09/01/2016","08/18/2016","Yun Fu","MA","Northeastern University","Standard Grant","Jie Yang","08/31/2019","$180,000.00","","y.fu@neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","7495","7495, 7916","$0.00","This project explores methodologies for forecasting long-term human group activity from videos. Forecasting future activities in real-world videos is an emerging computer vision problem with important applications in visual surveillance for security. This project systematically and rigorously formulates long-term group activity forecasting problem as causalities of visual entities, and designs visual intelligence systems using machine learning and data mining methods. The research considers multiple visual identities of different types, models their interactions, and mines the sequential causalities between these visual entities in videos. These causalities are used for forecasting future group and individual activities. The project creates new mathematical models for describing and simplifying understanding statistical properties of human activity videos. The project leads to important and timely technology that can help to design future video analysis systems with optimal performance in understanding and searching activities from videos. The project tightly integrates research and education activities for the purpose of providing young researchers with project-based learning opportunities in an interdisciplinary environment that offers exceptional professional and personal growth opportunities. <br/><br/>This research discovers complex causality patterns between visual entities from noisy visual data, in order to gain rich and useful knowledge for the forecasting of future visual activities. This essentially bridges the gap between human understandable visual semantics and high-dimensional noisy visual data. The research enables to efficiently capture interactions between multiple visual entities and their temporal causalities, and provides rich knowledge for guiding long-term group activity forecasting. The project also explores several innovative ways to leverage rich sequential context and builds progress level-invariant features. This naturally enriches feature representations from temporally partially observed data, and allows building more time efficient activity prediction machines. Moreover, the project develops an effective forecasting model that can elegantly utilize causalities mined from visual data for long-term forecasting. The developed technologies can lead to new intelligent systems."
"1648560","SBIR Phase I:  High Performance Sense and Avoid","IIP","SMALL BUSINESS PHASE I","12/15/2016","12/11/2016","Olivier Coenen","CA","QELZAL CORPORATION","Standard Grant","Muralidharan S. Nair","09/30/2017","$224,950.00","","olivier.coenen@qelzal.com","4225 EXECUTIVE SQ STE 420","La Jolla","CA","920371499","6504270360","ENG","5371","5371, 6840, 8034, 8035, HPCC","$0.00","The broader impact/commercial potential of this project is to develop and provide novel computation capabilities to the marketplace that mimic and apply the way our brain computes beyond deep learning systems and much closer to how the brain actually operates. Although<br/>the technology is initially targeted for the commercial drone market, the technology can be applied to consumer and hobbyist drone market, self-driving cars and advanced driver assistance systems, autonomous navigation and guiding systems with obstacle avoidance for<br/>robots, ballistics tracking and counter-drone capabilities for military and defense, and surveillance and counter-drone for public safety and security. This project has the potential to revolutionize robotic and machine vision by providing capabilities that simply do not exist today.<br/><br/>This Small Business Innovation Research (SBIR) Phase I project investigates novel ways, algorithms and software implementations that leverage expertise in natural vision systems, deep learning and machine learning to make use of electro-optical sensors, which<br/>respond in new ways to form the basis of an Airborne Based Sense and Avoid (ABSAA) system. It leverages the benefits of bio-inspired computation, and investigates how to combine information from multiple sensors in a common representation. The project will study novel ways to achieve robust detection, segmentation, clustering, discrimination and classification with these novel sensors. It will also extend methods and algorithms for state estimation, tracking and prediction in the inherent sensor representation. The project will also address the real-time constraints and will attempt to leverage recent hardware implementations, which can provide a complete embedded system for ABSAA system that is low SWaP (size, weight and power) and in the long run, low cost as well because it has the potential to benefit from economy of scale. The theoretical and algorithmic advances generated by the project have the potential to affect<br/>machine and robotic vision well beyond the project focused application to ABSAA."
"1617861","CHS: Small: Data-Driven Material Understanding and Decomposition","IIS","HCC-Human-Centered Computing","07/01/2016","04/18/2019","Kavita Bala","NY","Cornell University","Continuing Grant","Ephraim Glinert","06/30/2020","$510,212.00","","kb@cs.cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7367","7367, 7923, 9251","$0.00","We are in daily contact with a rich range of materials (metals, woods, fabrics, granites, etc.) that contribute to how we understand the world.  Recognizing and modeling real-world materials have long been core challenges in computer vision and graphics.  Recently, scene understanding has experienced an explosion of research activity driven by deep learning models trained on large-scale datasets.  But the focus has mainly been on objects; materials have received less attention, and it has predominantly focused on careful measurements in laboratory settings.  Of course, there is a large gap between materials in the real world and these laboratory settings.  The PI's goal is to bridge that gap, to enable material understanding ""in the wild.""  Toward this end, her group recently released large-scale crowdsourced datasets (OpenSurfaces, Intrinsic Images in the Wild, MINC) that are already being used extensively in the research community.  Using this data to develop new material segmentations and recognition algorithms, the PI's team has produced state-of-the-art methods which open up new possibilities for data-driven material understanding that will impact a wide range of applications such as interior design, material editing, visual search, and robotics.  Project outcomes (including new datasets, annotations, and code) will be made fully open and public.  The PI actively mentors underrepresented minorities at Cornell, and is working with Women in Computing at Cornell (WICC) and Girls Who Code (GWC) to reach middle and high school students.  This research will build prototypes of the annotation tools, and integrate them into summer workshops at Cornell aimed at high school minority students.  The PI's group will also organize a Material Understanding Competition (MUC) to drive innovation in material recognition and segmentation, and intrinsic image decomposition.<br/><br/>This project includes two major technical thrusts in material understanding:<br/><br/>1.  Intrinsic images for material understanding.  Intrinsic image decomposition aims to decompose images into intrinsic properties such as material and illumination.  This decomposition is ill-posed and challenging for images in the wild.  This work will collect new pairwise shading and depth annotations for intrinsic image decomposition; introduce a new perceptual metric to evaluate algorithms; solve for joint material recognition and intrinsic decomposition; and develop proof-of-concept applications for image-based editing using intrinsic image decomposition.<br/><br/>2.  Material recognition for semantic understanding.  Recognizing materials in the wild is extremely challenging.  This work will collect large-scale material annotations with ""click"" data and train weakly supervised recognition algorithms; collect fine-grained material data for subcategories like wood and metal; develop new algorithms for coarse and fine-grain recognition; and develop proof-of-concept applications for intelligent material search, and material assignment to shapes."
"1617801","CIF: Small: Collaborative Research: Scalable Nonconvex Optimization with Statistical Guarantees for Information Computing in High Dimensions","CCF","Comm & Information Foundations","07/01/2016","06/29/2016","Yiyuan She","FL","Florida State University","Standard Grant","Phillip Regalia","09/30/2020","$285,000.00","","yshe@stat.fsu.edu","874 Traditions Way, 3rd Floor","TALLAHASSEE","FL","323064166","8506445260","CSE","7797","7923, 7936","$0.00","The data explosion in all fields of science recently creates an urgent need for methodologies for analyzing high dimensional data with low-dimensional structure. The research, devoted to developing transformative theory and methods for scalable nonconvex optimization with statistical guarantees in noisy settings, has applications in a wide range of disciplines such as signal processing, machine learning, operations research, and computer vision. The investigators propose methodologies of statistics-guided nonconvex optimization and optimization-assisted statistical analysis to study convergence rate, acceleration, and statistical accuracy of iterative nonconvex algorithms for signals with mixed types of structural parsimony. The project cross-fertilizes ideas from statistics, operations research, engineering, and computer science and has education tightly coupled with research.  The integrated research and education help students develop critical thinking through cross-disciplinary training, and assist students in becoming life-long learners. The investigators use the rich topics in this project to inspire the learning and discovery interest of the public and students of all ages; in addition, the outreach activities help attract minority and female students to careers in science.<br/><br/>The research performs statistical-accuracy guided algorithmic analysis of general majorization-minorization algorithms for fast and stable signal recovery. Scalable and randomized acceleration schemes are proposed and studied in big-data applications. The investigators develop an innovative optimization-based statistical methodology for analyzing multi-regularized sparse estimators in high dimensions. The project applies the techniques to robust principle component estimation, hierarchical modeling, network learning, among others.  The research creates a fusion of optimization and statistics for information computing in high dimensions, and deepens and broadens existing compressed sensing and nonconvex optimization theories and methods."
"1617815","CIF: Small: Collaborative Research: Scalable Nonconvex Optimization with Statistical Guarantees for Information Computing in High Dimensions","CCF","Comm & Information Foundations","07/01/2016","06/29/2016","Dapeng Wu","FL","University of Florida","Standard Grant","Phillip Regalia","06/30/2020","$215,000.00","","wu@ece.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","CSE","7797","7923, 7936","$0.00","The data explosion in all fields of science recently creates an urgent need for methodologies for analyzing high dimensional data with low-dimensional structure. The research, devoted to developing transformative theory and methods for scalable nonconvex optimization with statistical guarantees in noisy settings, has applications in a wide range of disciplines such as signal processing, machine learning, operations research, and computer vision. The investigators propose methodologies of statistics-guided nonconvex optimization and optimization-assisted statistical analysis to study convergence rate, acceleration, and statistical accuracy of iterative nonconvex algorithms for signals with mixed types of structural parsimony. The project cross-fertilizes ideas from statistics, operations research, engineering, and computer science and has education tightly coupled with research.  The integrated research and education help students develop critical thinking through cross-disciplinary training, and assist students in becoming life-long learners. The investigators use the rich topics in this project to inspire the learning and discovery interest of the public and students of all ages; in addition, the outreach activities help attract minority and female students to careers in science.<br/><br/>The research performs statistical-accuracy guided algorithmic analysis of general majorization-minorization algorithms for fast and stable signal recovery. Scalable and randomized acceleration schemes are proposed and studied in big-data applications. The investigators develop an innovative optimization-based statistical methodology for analyzing multi-regularized sparse estimators in high dimensions. The project applies the techniques to robust principle component estimation, hierarchical modeling, network learning, among others.  The research creates a fusion of optimization and statistics for information computing in high dimensions, and deepens and broadens existing compressed sensing and nonconvex optimization theories and methods."
"1553485","CAREER: Advancing the Frontier in System Architectures for Artificially Intelligent Services and Applications","CCF","Special Projects - CCF, Software & Hardware Foundation","01/15/2016","09/09/2019","Jason Mars","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Yuanyuan Yang","12/31/2020","$470,000.00","","profmars@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","2878, 7798","1045, 7798, 7941","$0.00","The way society interacts with today's mobile technologies is rapidly changing as such devices - omnipresent in our lives - become increasingly personal and knowledgeable. Intelligent assistants (IAs), such as Apple's Siri, Google's Google Now, Microsoft's Cortana, and Amazon's Echo, are continuing to become more sophisticated and are expected to grow in popularity as wearables continue to gain traction. The computing capabilities required to support these technology trends continue to evolve toward intelligent systems that use sophisticated machine learning and computer vision algorithms on the critical path of audio and vision interactions. In this proposed work, PI Jason Mars aims to understand how future cloud and mobile systems should be designed (and co-designed) to support increasing demand from users. This proposed work advances both 1) the efficiency of current computing platforms to reduce their energy, cost, and environmental footprint, and 2) the expansion of open end-to-end systems with state-of-the-art algorithmic capabilities to enable new, more sophisticated, intelligent technologies and usher in future research and inquiry. <br/><br/><br/>PI Mars' technical approach incorporates three pillars of innovation. These pillars span application insights, to inform the design of underlying systems, vertical innovation to advance the cross-layer design of the system stack for emerging applications and services, and horizontal innovation to reason about a computational fabric spanning mobile hardware and cloud hardware to create a unified platform for computation. In addition to having impact on national interests, economic advancement, and technology in general, this proposed work incorporates significant innovation in undergraduate, and graduate education. The Intelleco and DeepSirius systems described in this proposal will be used to design an undergraduate and graduate course for state-of-the-art datacenter design. In addition all artifacts and teaching materials will be disseminated broadly through open source and creative commons."
"1626236","MRI: Development of an Autonomous, Connected and Data-Driven Vehicle for Multi-Disciplinary Research and Project-Based Learning","CNS","Special Projects - CNS","10/01/2016","09/13/2016","Xinming Huang","MA","Worcester Polytechnic Institute","Standard Grant","Rita Rodriguez","09/30/2019","$300,000.00","Xiangnan Kong, Raghvendra Cowlagi, William Michalson, Yehia Massoud","xhuang@wpi.edu","100 INSTITUTE RD","WORCESTER","MA","016092247","5088315000","CSE","1714","1189","$0.00","This project, developing a connected and autonomous vehicle, aims to create WAVE, a shared 'live' testbed/platform for faculty and students to investigate many aspects of the driveless car. Involving research, experiments, and data collection, these aspects include: safety, reliability, performance, connectivity, interaction, and social impacts. Similar to the history of wireless communications research over the past decade, researchers often utilized SDR platforms to prototype and evaluate their proposed communication systems. As connected and autonomous cars hit the road in the future, there will be many issues. A prototype vehicle becomes absolutely necessary for these research projects to collect data, conduct experiments, and prove concepts. The 'live' testbed will be shared by faculty and students across the university to carry out their research and educational missions. A large number of research activities and student projects will utilize this instrument, which will serve the dual purposes of enabling multidisciplinary research and project-based learning. The instrument will enable multidisciplinary research activities that involve, but are not limited to, computer vision and machine learning, V2V and V2I communications, optimal controls and reliability, big data analysis, human-robot interactions, driver behavior and human factors, social impacts and autonomous vehicle policies.<br/><br/>Although a fully autonomous car is not currently commercially available, key components such as sensors, processing platforms, and drive-by-wire devices can readily be integrated to build a prototype. The instrument development team consists of experts from multiple fields, including robotics, electrical, and mechanical engineering, as well as computer science. The main efforts not only includes the hardware/sensors installation, but also the software development for vehicle control, data processing, and path planning. Comparing to the autonomous vehicle prototypes in the literature, the proposed WAVE testbed exhibits three new properties. It will be a 1. Connected vehicle equipped with DSRC (Dedicated Short Range Communications) modules that can evaluate V2V (Vehicle-to Vehicle) and V2I (Vehicle to Infrastructure) active-safety systems; 2. Data-driven platform that will collect big data from roads, vehicles, and drivers; the data will be shared openly with the research community; 3. Real-world tool for university faculty to develop curriculum for project-based learning which will enhance student experience through undergraduate research."
"1632803","RI: Medium: Collaborative Research: Learning to Su","IIS","ROBUST INTELLIGENCE","01/01/2016","09/12/2017","Fei Sha","CA","University of California-Los Angeles","Continuing grant","Jie Yang","05/31/2018","$248,076.00","","feisha@usc.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","7495","7495, 7924","$0.00","Today there is far more video being captured - by consumers, scientists, defense analysts, and others - than can ever be watched.  With this explosion of video data comes a pressing need to develop automatic video summarization algorithms.  Video summarization takes a long video as input and produces a short video as output, while preserving its information content as much as possible.   As such, summarization techniques have great potential to make large video collections substantially more efficient to browse, search, disseminate, and facilitate communication.  Such increased efficiency will play a vital role in many important application areas.  For example, with reliable summarization systems, a primatologist gathering long videos of her animal subjects could quickly browse a week's worth of their activity before deciding where to inspect the data most closely.  A young student searching YouTube to learn about Yellowstone National Park could see at a glance what content exists, much better than today's simple thumbnail images can depict.  An intelligence agent could rapidly sift through reams of aerial video, reducing the resources required to analyze surveillance data to identify suspicious activities.<br/><br/>This project develops new machine learning and computer vision algorithms for video summarization.  Unsupervised methods, which are the cornerstone of nearly all existing approaches, have become increasingly limiting due to their reliance on hand-crafted heuristics.  By instead posing video summarization as a supervised learning problem, this project investigates a markedly different formulation of the task.  The research team is investigating four key new ideas: powerful probabilistic models for learning to select the optimal subset of video frames for summarization, semi-supervised learning models and co-summarization algorithms for leveraging the abundance of multiple related videos, algorithms for exploiting photos on the Web to improve summarization, and evaluation protocols that assess summaries in a way that aligns well with human comprehension.  The broader impact of the proposed research includes practical tools for video summarization, scientific advances that appeal broadly to several communities, publicly disseminated research results, inter-disciplinarily trained graduate students, and outreach activities to engage young students in STEM education and career paths."
"1647887","I-Corps: Semantic Video - from Video to Descriptions","IIP","I-Corps","08/15/2016","08/09/2016","Sudeep Sarkar","FL","University of South Florida","Standard Grant","Steven Konsek","07/31/2017","$50,000.00","","sarkar@usf.edu","4019 E. Fowler Avenue","Tampa","FL","336172008","8139742897","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project involves computer vision analysis of video, using both visual and auditory cues, to create descriptions of the content. The technology has a large variety of potential applications from law enforcement to surveillance to consumer applications. These include enabling the efficient storage and retrieval of large volumes of camera data.  Smart surveillance systems can be enhanced with features that allows for summarization of daylong video footages as a list of security-relevant events.  The technology can also allow automated organization of large collections of multimedia data.<br/><br/>This I-Corps project involves commercialization feasibility research for a computer vision technology for expressing video content in terms of natural language text and grammar, i.e. semantics. This project builds on a video analysis framework that leverages state-of-the-art methods for object detection and action recognition in a unified formalism encoded in terms of a mathematical and statistical approach known as pattern theory. The video analysis approach can (i) handle structural variability of complex events without requiring large training data while exploiting easily available ontological information, (ii) overcome classification errors of machine learning classifiers of actions and objects, (iii) accommodate scene clutter, i.e. extraneous objects that do not in the activity present in the scene, (iv) and manage sequences of elementary events, all without retraining. The formalism allows for the easy incorporation of temporal, spatial, and logical constraints. This team has demonstrated this system on standard datasets used to benchmark performance in computer vision for human activity recognition tasks."
"1639266","I-Corps:  Conceptualizing and Validating an Occupant-aware Predictive Control System","IIP","I-Corps","05/01/2016","04/28/2016","John Taylor","VA","Virginia Polytechnic Institute and State University","Standard Grant","Steven Konsek","05/31/2017","$50,000.00","","jet@gatech.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","ENG","8023","","$0.00","Building ventilation systems are not responsive to occupants, operating according to predetermined schedules to satisfy the maximum number of people that could be in a space. It's comparable to a car that only runs in the highest gear: ineffective, inflexible, and inefficient. Vast amounts of energy are wasted while making people too cold or hot in their offices. Based on a study conducted by Pacific Northwest National Labs there is a potential for 16% energy savings through incorporation of high resolution real-time occupancy data in building automation systems that control heating, ventilation and air conditioning (HVAC) equating to roughly $2.7 billion annually. A survey conducted by Center for Built Environment at UC Berkeley showed that merely 11% of buildings fulfill standards for thermal comfort, and 26% meet air quality requirements, severely impairing occupant satisfaction and productivity. The small array of occupancy sensing products currently on the market are cumbersome, too costly, and tend to focus on narrow aspects of building operation. As a result they've achieved adoption of less than 1% of the total nonresidential real estate market. This I-Corps team is  working on a passive sensing system to address this industry-wide blind spot with a cost-effective and easy to deploy system in order to increase building energy efficiency and improve occupant comfort.<br/><br/>This team is working on a distributed sensing solution that will enable building HVAC control systems to respond to and anticipate building occupancy. The complete initial version of the system will include machine learning and computer vision algorithms embedded in the small fully wireless sensors processing data from a low-cost RGB camera and passive infrared sensor that will be able to detect both stationary and moving people across a coverage area of approximately 600 square feet in an open space. By the end of the program the team intends to have completed a software prototype for the image processing portion capable of delivering real-time occupancy counting and prediction using sample building image streams. More importantly, the team plans to have validated the demand in the market for the proposed product through the interviews, investigated the potential privacy concerns from prospective clients, explored value propositions outside of energy efficiency and comfort from advanced occupancy data, and identified the types of real estate clients that we should focus on. The proposed product has the potential to greatly reduce building energy consumption and improve occupant comfort throughout the US, and more broadly make the built environment far more responsive and data-driven. The team's interviews over the course of I-Corps program will be vital in shaping the business model to bring this application of computer vision and machine learning research and development to market."
"1647419","SBIR Phase I:  Personalizing Online Clothing Shopping","IIP","SMALL BUSINESS PHASE I","12/15/2016","12/12/2016","Tamara Berg","NC","Shopagon, LLC","Standard Grant","Peter Atherton","11/30/2017","$225,000.00","","berg.tamara@gmail.com","510 N Greensboro St","Carrboro","NC","275101728","6465093361","ENG","5371","5371, 8032","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project will be to improve the experience of online shopping for clothing, currently the largest segment online in the US with $51 Billion in sales for the last year and a focus for millions of consumers and thousands of companies.  The innovation in this project would improve the ability to discover and compare the visual appearance and style of clothing items, allowing more effective shopping over an increasingly large marketplace with millions of items, and improving the efficiency of the online clothing market.  In addition, the project will improve the ability of computational algorithms to automatically parse and understand clothing style, part of building computers to understand our daily world.<br/><br/>This Small Business Innovation Research (SBIR) Phase I project will develop novel techniques for identifying, comparing, and predicting shopper preferences for clothing styles.  These will be based on computer vision to recognize visual features of clothing styles and machine learning to build models of shopper preference from their interactions while shopping.  Development will include representation learning for visual appearance of clothing style as well as for the factors that contribute to personal preference for style.  These models will be used to automate search and recommendation systems for clothing shopping, opening up new business opportunities for machine-learning enabled personalization."
"1543841","CBMS Conference: Topological Data Analysis: Topology, Geometry and Statistics, May 23-27, 2016; Austin, TX","DMS","INFRASTRUCTURE PROGRAM","01/01/2016","11/10/2015","Lizhen Lin","TX","University of Texas at Austin","Standard Grant","Jennifer Slimowitz Pearl","12/31/2016","$37,541.00","Rachel Ward, Peter Mueller","lizhen.lin@nd.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","MPS","1260","7556","$0.00","This award will support a 5-day conference in topological data analysis at the University of Texas at Austin in the spring of 2016.  Topological data analysis (TDA) has recently emerged as an active new field of research, which has generated great interest across mathematics, statistics, computer science, machine learning, and electrical engineering communities. TDA is being applied to image analysis, neuroscience, networks analysis, morphology, genetics,cancer research and other problems. The interdisciplinary nature of TDA naturally leads to the literature and the active researchers being scattered across different fields. This lack of a cohesive disciplinary home makes it difficult for junior researchers and in particular graduate students from statistics to obtain exposure to the field. The proposed workshop strives to fill this gap by focusing on tutorial and overview talks on topological data analysis and providing hands-on data analysis sessions. The conference will feature Professor Sayan Mukherjee from Duke University as the principal lecturer, and five additional invited speakers.  The goal of the conference is to introduce graduate students and junior researchers to TDA, an active new field, which lies at the exciting intersection of topology, geometry, and statistics. This conference will also serve as an opportunity to foster research collaborations and chart possible future directions for research.<br/><br/>The program will provide an overview of how geometry and topology can be used for statistical inference. The proposed outline develops a framework for how geometry and topology is used for some common tasks in statistical inference including mixture models, modeling surfaces and shapes, extensions of spectral clustering, as well as machine learning aspects such as semisupervised learning. There will be some applied and data analysis aspects to the lecture where some common Topological Data Analysis codes will be used to model shape data, specifically computerized tomography (CT) scans of bones and organs.  Applied aspects of the program will include applications of the methodology developed in quantitative genetics, statistical genetics, as well as computer vision applications. Another component of the program is focusing on the role of geometry in statistics. Some of the invited speakers such as Professors Rabi Bhattacharya and Susan Holmes will deliver lectures on this topics."
"1556103","SBIR Phase II:  Guided Positioning System for Ultrasound","IIP","SBIR Phase II","04/15/2016","09/21/2018","Charles Cadieu","CA","Bay Labs, Inc.","Standard Grant","Peter Atherton","03/31/2020","$1,376,062.00","","mgmt@baylabs.io","1479 Folsom Street","San Francisco","CA","941033734","4154245616","ENG","5373","165E, 169E, 5373, 8032, 8033","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project will be in the field of healthcare.  The United States spends approximately $9,000 per person per year on healthcare.  Ultrasound medical imaging is a medical imaging technology that could lower costs by providing an alternative to higher-cost imaging techniques.  The technology created during this Phase II project is expected to increase the quality, value, and accessibility of medical ultrasound, which would in turn reduce medical imaging costs in the US healthcare system.  Furthermore, the company's technology is expected to bring ultrasound to more clinical settings and improve system-wide efficiencies in the diagnosis and treatment of disease.  The technology also has commercial potential in the international market, with $5.8B spent annually on medical ultrasound devices worldwide.  Finally, by improving the utility of ultrasound, the technology will lead to improved patient care and may ultimately save lives.<br/><br/>This Small Business Innovation Research (SBIR) Phase II project will develop deep learning technology for ultrasound imaging in medicine.  Ultrasound imaging has numerous benefits including real-time image acquisition, non-invasive scanning, low-cost devices, and no known side-effects (it is non-ionizing).  However, variability in quality has encumbered its adoption and utility.  As a result, more expensive imaging is typically utilized, often exposing patients to ionizing radiation.  Our objective is to develop, improve, and test machine learning techniques, based on deep learning, to improve ultrasound acquisition and interpretation.  We expect this project will create novel technologies that make ultrasound easier to use and improve the quality of ultrasound examinations.  The end result will improve the quality, value, and accessibility of medical ultrasound examinations, will result in cost savings to the healthcare system, will produce improvements in patient care, and will support a sustainable business opportunity."
"1617955","AF:Small: Nearest Neighbor Search in High Dimensional Spaces","CCF","Algorithmic Foundations","09/01/2016","05/17/2016","Alexandr Andoni","NY","Columbia University","Standard Grant","Tracy Kimbrel","08/31/2020","$449,960.00","","andoni@cs.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","7796","7923, 7926, 7929","$0.00","The goal of this project is to advance the state of the art of algorithms for the nearest neighbor search (NNS) problem. The NNS problem is one of the central computational problems arising when dealing with modern massive datasets.  For example, it underlies a classical classification rule in machine learning: to label a new object (such as an image), one can simply find the most similar objects (the nearest neighbors) in a preprocessed database, and use the label of the objects found. More generally, NNS is a key algorithmic tool in many areas including databases, data mining, information retrieval, computer vision, computational geometry, signal processing, bioinformatics, and others.  In such applications, the objects are usually represented in a high-dimensional space: e.g., a 20x20 image is naturally represented by a 400-dimensional vector, with one coordinate per pixel.<br/><br/>The PI intends to study the high-dimensional NNS problem, by addressing both foundational algorithmic questions, as well as applied aspects. The project aims for both scientific and educational impact. First, the study of the NNS problem is instrumental in developing fundamental concepts in areas such as high-dimensional computational geometry as well as sublinear space algorithms (including concepts such as dimension reduction, sketching, metric embeddings, etc). Second, due to the numerous applications of NNS, its efficient implementations are used widely in industry. Overall, the PI aims to foster a stronger connection between the theory and practice of NNS by code dissemination, public lectures, and student training. The PI's affiliation with Columbia's Data Science Institute puts the PI in a particularly good position to accomplish these goals.<br/><br/>To accomplish the project's algorithmic goals, the PI will leverage the recently developed approach of data-dependent hashing, where the hash function itself adapts to a given dataset. As a proof-of-concept, the PI and co-authors recently demonstrated that this new approach to NNS leads to algorithms outperforming the classical NNS algorithms (such as those based on the Locality-Sensitive Hashing). The project aims to develop this methodology further to its maturity, extend it to other relevant metrics (similarity measures) which have traditionally resisted efficient solutions, develop practical versions of the algorithms, and to understand the limits of these techniques."
"1549864","STTR Phase I:  Dynamic Robust Hand Model for Gesture Intent Recognition","IIP","STTR PHASE I","01/01/2016","12/18/2015","Raja Jasti","CA","ZeroUI Inc","Standard Grant","Muralidharan S. Nair","12/31/2016","$225,000.00","Karthik Ramani","raja@zeroui.com","10570 Whitney Way","Cupertino","CA","950144442","4088630555","ENG","1505","1505, 4080, 6840, 8035, HPCC","$0.00","The broader impact/commercial potential of this project stems from addressing the important hand gesture based input challenges of VR and AR industries that are expected to grow to $150B by 2020. Piper Jaffray identifies VR as the next mega trend and estimates the VR market to be worth more than $60B by 2025. Piper Jaffray highlights new market opportunities for peripheral devices that bring hands and feet into VR. This technology if successful in mitigating the high technical risks represents a huge leap in the state of the art in 3D hand models for gesture recognition and has the potential to be the industry standard for AR, VR and 3D applications. Our company will commercialize the project by licensing this technology as a hand model SDK to the AR/VR and 3D camera device makers and application developers to bring highly interactive VR/AR and 3D gesture applications to gaming, entertainment, education, healthcare, design, architecture, and manufacturing.<br/><br/>This Small Business Technology Transfer Research (STTR) Phase I project develops a breakthrough innovation in 3D hand gesture intent recognition that can robustly work across different 3D cameras, orientations, positions and occlusions. It addresses a key challenge in gesture recognition while enabling natural spatial interactions for Virtual and Augmented Reality (VR/AR) and many other applications enabled by 3D depth cameras. It solves the following key challenges faced by existing academic and commercial hand models and involves very high technical risks: 1) robustness under heavy occlusions 2) invariance to view-point changes 3) low computational training and tracking complexity 4) discriminative to frequent gesture/micro-gesture sequences. We will tackle these by developing a novel dynamic, robust hand-tracking model inspired by a machine learning technique that is not commonly used by computer vision community. We will achieve this by developing the following objectives 1) hand pose hypothesis generation using trained classifiers 2) hand model fitting using joint matrix factorization and completion 3) user study and evaluation of the hand model."
"1556058","SBIR Phase II:  Robotic System for the Sorting of Recyclable Waste","IIP","SBIR Phase II","04/15/2016","11/29/2018","Matanya Horowitz","CO","Cognitive Robotics","Standard Grant","Muralidharan Nair","09/30/2020","$1,426,690.00","","mhorowit@caltech.edu","17795 W 59th Dr","Golden","CO","804031103","7204700812","ENG","5373","116E, 165E, 169E, 5373, 6840, 8035, 8240, 9139, 9231, 9251, HPCC","$0.00","The broader impact/commercial potential of this project will be to change the fundamental economics of the recycling process. Although it is estimated that up to 95% of the waste stream could be recycled, only a third of the 250 million tons of municipal solid waste that are generated each year in the United States is currently diverted. Greater diversion would provide immense savings in landfill and processing costs, and benefit the environment as well. Tens of millions of pounds of greenhouse gas emissions from virgin material mining may be eliminated, and pollution from landfill waste reduced. The existing sorting process is expensive and unprofitable, requiring human workers to manually sort debris, an extremely dull, dirty, and dangerous profession. This innovation has the potential to eliminate these trade-offs between cost and environmental damage. <br/><br/>This Small Business Innovation Research (SBIR) Phase 2 project will create a scalable, integrated robotic system that autonomously sorts material for recycling. This advance in autonomous systems is made possible by a series of innovations in robotics: (1) tremendous improvements in both computer vision and robotic manipulation, allowing for the system to be deployed with virtually zero retrofitting in existing facilities; (2) new motion planning techniques that allow for trajectories to be generated in real time, customized for the characteristics of the waste, safety, and any uncertainty in individual objects? position;&#8232; and (3) modern machine learning techniques that allow the system to classify waste at levels approaching human performance, with a continual training signal obtained via human supervision. These innovations pave the way for a new era in recycling, where waste is sorted cheaply, safely, and reliably on a universal scale."
"1564521","Collaborative Research: ABI Development: A User-friendly Tool for Highly Accurate Video Tracking","DBI","ADVANCES IN BIO INFORMATICS","08/01/2016","07/20/2016","Anna Dornhaus","AZ","University of Arizona","Standard Grant","Peter McCartney","07/31/2020","$206,774.00","","dornhaus@email.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","BIO","1165","","$0.00","Biological science has made great strides recently both by capitalizing on automated methods of data collection and by enabling researchers to share results efficiently via public databases. This project will achieve the same for applications that use videos to track movement of animals, cells, or robots, by producing freely available software for efficient and automatic extraction of movement data and directly adding such data to a public database (the KNB Data Repository). The software will be easy to use and adapt to new types of videos, issues that have so far been roadblocks to widespread adoption of existing video tracking tools. The ability to share both movement data and videos will stimulate collaboration among researchers. Both functionalities will enable fundamentally new advances in such areas as animal group behavior, behavioral genetics, cell biology, and collective robotics, and other fields that record the movements of many individuals. The software, because of its ease of use and enabled access to research videos from the database, will also serve as a tool in teaching at the K-12 and college level. In addition, this project will serve to train several college and graduate students in both biology and computer science; such interdisciplinary training is essential for advances in biological research today.<br/><br/>This project will implement a unique combination of clear, current graphical user interface design to improve usability with state-of-the-art machine learning techniques to improve movement tracking accuracy. In addition, the developed software will enable users to visualize results for validation and analysis, and include functionality for users to correct any remaining tracking errors. This will enable users to get scientific-quality data output without having to employ multiple software applications and without having to manually post-process data files. In the context of the project, several workshops will be held and a website developed to improve accessibility for students and researchers in biology. The project will also develop a direct link to the existing KNB scientific data repository, such that users can access the repository, compare their results, or complete meta-analyses easily. Besides advancing biological research, this will also generate an extensive resource for computer vision scientists by providing a large collection of videos with accurate user annotation for improving core algorithms such as object detection and tracking.  More information may be found at http://www.abctracker.org."
"1649126","EAGER: Exploring Children's Use of Online Social Networks Using the KidGab Network","IIS","HCC-Human-Centered Computing","07/15/2016","08/27/2019","Tracy Hammond","TX","Texas A&M Engineering Experiment Station","Standard Grant","William Bainbridge","10/31/2019","$191,545.00","Cara Wallis, Stephanie Valentine","hammond@tamu.edu","400 Harvey Mitchell Pkwy S","College Station","TX","778454645","9798626777","CSE","7367","7367, 7916, 9251","$0.00","This proposal will support research into why children use social networks and how they influence each other using KidGab, a social network managed by the research team and designed for pre-teen Girl Scouts.  Although pre-teens regularly use social networks, relatively little is known about how they (versus adults) behave online or how this affects their well-being because most popular social networks close pre-teens' accounts when detected, while purpose-built networks for pre-teens are heavily restricted in terms of both what users can do and what data is available for study.  Building on their existing work with Girl Scout councils in Texas, the research team will develop new features and activities for KidGab and conduct outreach workshops with new councils.  This will allow the team to study how children respond to different recruitment and motivating ('gamification') strategies, in particular, looking at the relative value of adult- versus peer-created content and collaborative versus individual activities in encouraging continued use.  Through looking at how children create and adopt each other's drawings in visually-focused activities, the team will also develop novel methods for studying creativity, conformity, and influence in social networks.  In addition to making progress on these specific questions, the team's long-term research goal is to build a large enough network and dataset that both they and other researchers can conduct future studies and analyses.  More broadly, the team will create useful online content and design guidance for building social networks for pre-teens that support positive outcomes such as identity development and personal connection while reducing bad outcomes such as oversharing and cyberbullying.<br/><br/>Preliminary work by the team shows that the regular release of new content is critical to retaining participation; thus, the first main component of the proposal is to develop ways for children in the network to generate and share content such as personality quizzes, and images that network members can use virtual currency to buy and display on their profiles.  This will allow the team to (1) compare the uptake of adult-created versus peer-created content and their effects on encouraging long-term participation, (2) examine the kinds of content children prefer to generate and consume while generating a library of child-created content, and (3) study larger questions about identity creation and exploration.  The second main component is to develop sketching-based activities in which participants are given a creativity task and their work is made visible through the network so that other participants can adopt ideas from it in their own work.  The team will manually code key features of sketches generated for a given task and study their propagation using link-analysis algorithms such as PageRank and the Hubs and Authorities Algorithm, interpreting the degree to which a participant is a hub or authority as the likelihood that they are influenced by or influence others.  By looking at a variety of specific tasks and variations in instructions that prime behaviors, as well as characteristics of participants, the team will develop insights into key drivers of influence in pre-teens' social networks.  Further, the manually annotated sketches will provide training data for computer vision and machine learning algorithms for sketch analysis. The team will deploy these content creation mechanisms through events held with individual Scout councils geographically near those who have already participated in the network; such a strategy will best leverage the team's existing relationships with nearby councils while increasing the chance of recruiting dense sub-networks to encourage long-term retention."
"1564850","Collaborative Research: ABI Development: A User-friendly Tool for Highly Accurate Video Tracking","DBI","ADVANCES IN BIO INFORMATICS","08/01/2016","07/13/2018","Min Shin","NC","University of North Carolina at Charlotte","Continuing Grant","Peter McCartney","07/31/2021","$1,361,015.00","","mcshin@uncc.edu","9201 University City Boulevard","CHARLOTTE","NC","282230001","7046871888","BIO","1165","","$0.00","Biological science has made great strides recently both by capitalizing on automated methods of data collection and by enabling researchers to share results efficiently via public databases. This project will achieve the same for applications that use videos to track movement of animals, cells, or robots, by producing freely available software for efficient and automatic extraction of movement data and directly adding such data to a public database (the KNB Data Repository). The software will be easy to use and adapt to new types of videos, issues that have so far been roadblocks to widespread adoption of existing video tracking tools. The ability to share both movement data and videos will stimulate collaboration among researchers. Both functionalities will enable fundamentally new advances in such areas as animal group behavior, behavioral genetics, cell biology, and collective robotics, and other fields that record the movements of many individuals. The software, because of its ease of use and enabled access to research videos from the database, will also serve as a tool in teaching at the K-12 and college level. In addition, this project will serve to train several college and graduate students in both biology and computer science; such interdisciplinary training is essential for advances in biological research today.<br/><br/>This project will implement a unique combination of clear, current graphical user interface design to improve usability with state-of-the-art machine learning techniques to improve movement tracking accuracy. In addition, the developed software will enable users to visualize results for validation and analysis, and include functionality for users to correct any remaining tracking errors. This will enable users to get scientific-quality data output without having to employ multiple software applications and without having to manually post-process data files. In the context of the project, several workshops will be held and a website developed to improve accessibility for students and researchers in biology. The project will also develop a direct link to the existing KNB scientific data repository, such that users can access the repository, compare their results, or complete meta-analyses easily. Besides advancing biological research, this will also generate an extensive resource for computer vision scientists by providing a large collection of videos with accurate user annotation for improving core algorithms such as object detection and tracking.  More information may be found at http://www.abctracker.org."
"1564678","Collaborative Research: ABI Development: A User-friendly Tool for Highly Accurate Video Tracking","DBI","ADVANCES IN BIO INFORMATICS","08/01/2016","07/20/2016","Matthew Jones","CA","University of California-Santa Barbara","Standard Grant","Peter McCartney","07/31/2020","$86,443.00","","jones@nceas.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","BIO","1165","","$0.00","Biological science has made great strides recently both by capitalizing on automated methods of data collection and by enabling researchers to share results efficiently via public databases. This project will achieve the same for applications that use videos to track movement of animals, cells, or robots, by producing freely available software for efficient and automatic extraction of movement data and directly adding such data to a public database (the KNB Data Repository). The software will be easy to use and adapt to new types of videos, issues that have so far been roadblocks to widespread adoption of existing video tracking tools. The ability to share both movement data and videos will stimulate collaboration among researchers. Both functionalities will enable fundamentally new advances in such areas as animal group behavior, behavioral genetics, cell biology, and collective robotics, and other fields that record the movements of many individuals. The software, because of its ease of use and enabled access to research videos from the database, will also serve as a tool in teaching at the K-12 and college level. In addition, this project will serve to train several college and graduate students in both biology and computer science; such interdisciplinary training is essential for advances in biological research today.<br/><br/>This project will implement a unique combination of clear, current graphical user interface design to improve usability with state-of-the-art machine learning techniques to improve movement tracking accuracy. In addition, the developed software will enable users to visualize results for validation and analysis, and include functionality for users to correct any remaining tracking errors. This will enable users to get scientific-quality data output without having to employ multiple software applications and without having to manually post-process data files. In the context of the project, several workshops will be held and a website developed to improve accessibility for students and researchers in biology. The project will also develop a direct link to the existing KNB scientific data repository, such that users can access the repository, compare their results, or complete meta-analyses easily. Besides advancing biological research, this will also generate an extensive resource for computer vision scientists by providing a large collection of videos with accurate user annotation for improving core algorithms such as object detection and tracking.  More information may be found at http://www.abctracker.org."
"1564386","ODOMATIC: Automatic Species Identification, Functional Morphology, and Feature Extraction to alleviate the taxonomic impediment and broaden citizen science tools.","DBI","ADVANCES IN BIO INFORMATICS","08/01/2016","07/05/2016","Jessica Ware","NJ","Rutgers University Newark","Standard Grant","Peter McCartney","07/31/2019","$432,015.00","Gareth Russell, John Abbott","jware@amnh.org","Blumenthal Hall, Suite 206","Newark","NJ","071021896","9739720283","BIO","1165","1165","$0.00","The naming and classifying of living organisms is a fundamental principal upon which biology is founded. Indeed, almost every facet of our lives - from the food we eat to the medicine we take, the houses we live in to the natural scenery we admire - has a connection to the biodiversity present on Earth. All species must be named and cataloged in order for humans to understand the world around us. The scientific field of taxonomy, responsible for identifying and describing species, has incurred a reduction in its specialist workforce in recent decades; this is unfortunate because the need for taxonomy has never been greater as human activities result in unprecedented decline in the diversity of life. This project will help to reduce the burden placed on current and future taxonomists to identify and catalog biodiversity by introducing a set of open-source, web-based tools for identifying species from images and for measuring imaged specimens for comparative studies. These software tools will be made freely available through the publicly-funded CyVerse web platform. In addition, a system for identifying dragonflies and damselflies called ODOMATIC will be implemented on an existing web presence, OdonataCentral, allowing researchers and enthusiasts to accurately identify these insects from images of their wings. A series of free workshops will be offered in diverse urban communities in New Jersey and Alabama to increase the diversity of OdonataCentral?s user base, encourage participation in the STEM fields, and generally increase appreciation and understanding of the natural world. A series of Google Hangout events will encourage international user-ship for ODOMATIC from World Dragonfly Association members in Latin America, Africa, and Asia. In addition, undergraduate students from Rutgers University-Newark will be recruited to help in designing and training ODOMATIC, giving them valuable experience in research, programming and taxonomy.<br/> <br/>This work will help to reduce the taxonomic impediment - an urgent need for more taxonomic products, like species descriptions and specimen identifications, from fewer taxonomist workers - by addressing the time-consuming, but necessary, task that is species identification. Objectives of this project include release of a newly-developed system for automatically identifying Odonata (dragonflies and damselflies) from images of their wings, which uses computer vision and machine learning to characterize and classify species.  An interface will be deployed making species identification using this system accessible through the OdonataCentral website (odonatacentral.org).  In addition, stand-alone tools will be developed for automatically describing and placing geometric morphometric landmarks on specimens in biological imagery for use in morphology-based comparative studies.  The successful completion of these objectives will benefit the odonatological community by providing dragonfly and damselfly identification.  More broadly, the tools created here will allow biologists studying other groups of organisms to rapidly extract morphological data from images of their specimens."
"1548784","SBIR Phase I: Mitigating Fall Risks for Older Adults Through Automated Environmental Assessment","IIP","SMALL BUSINESS PHASE I","01/01/2016","12/21/2015","David Pietrocola","MD","Luvozo PBC","Standard Grant","Jesus Soriano Molla","08/31/2016","$150,000.00","","david@luvozo.com","4467 Technology Drive","College Park","MD","207420000","2026885016","ENG","5371","010E, 5342, 5371, 8018, 8032, 8042","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project addresses the costly and life-altering occurrence of falls among older adults. One in three adults over the age of 65 fall each year, and 31% of those falls are attributed to environmental hazards such as floor clutter, rugs, poor lighting, and low contrast. Costs associated with falls will reach $55 billion by 2020 according to the Centers for Disease Control and Prevention (CDC). The economic, social, and health impacts of falls will only continue to get worse as the older adult demographic increases from 40 million today to 72 million in the next 15 years. Through this project, advances in hazard detection algorithms and decision support approaches will contribute to helping prevent millions of falls among older and disabled Americans each and every year. By expanding access to expert environmental fall hazard assessments, the rate of hospitalizations and costly readmissions attributed to falls can be reduced.<br/><br/>The proposed project will develop and test a proof-of-concept for an automated fall hazard assessment system. In the face of an aging demographic, falls are becoming a national and global issue. A low-cost fall assessment system can facilitate the identification and mitigation of environmental hazards before they cause a fall. This project will research and develop computer vision, machine learning, and sensor fusion software algorithms to implement automated detection of potential environmental fall hazards and a risk-based assessment of those hazards for indoor spaces. A custom sensor suite will be used to detect hazards, while a mathematical model of risk will assess the collection of hazards in the context of the area and resident characteristics to suggest appropriate interventions. The project?s outcomes will advance the state of the art in fall prevention technology for older adults, and will be the first proof-of-concept to test the effectiveness of such methods to supplement expert home assessments."
"1651190","EAGER: Linguistic Event Extraction and Integration (LEXI): A New Approach to Speech Analysis","IIS","Robust Intelligence","09/01/2016","04/13/2018","Stefanie Shattuck-Hufnagel","MA","Massachusetts Institute of Technology","Standard Grant","D.  Langendoen","08/31/2019","$238,449.00","Jeung-Yoon Choi","sshuf@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7495","116E, 7495, 7916, 9251","$0.00","This exploratory project develops a new system for speech signal analysis that can be used to improve automatic speech recognition (ASR) systems, and provide a testable model of human speech perception. The system is based on finding important events in the speech signal, i.e. 'acoustic edges' where the signal changes suddenly because the mouth closes or opens during the formation of a consonant (like /p/ or /s/), or a vowel (like /a/ or /u/). These abrupt changes, called Landmarks, are especially informative, because they (and the parts of the signal near them) are richly informative about the speaker's intended words and their sounds. Focusing on these events results in greater computational efficiency, by identifying the linguistically relevant information in the speech signal, rather than measuring every part of the signal. This focus on individual cues to speech sounds also means that the system can deal with non-typical speech produced by children, older people, speakers with foreign accents, or those with clinical speech disabilities. As a result, this system will bring the benefits of ASR to speakers who are not well served by current recognition systems, making it possible for more people to use cell phones, tablets and laptops. While existing systems work well for typical speakers by using statistical analysis of large samples of typical speech, they leave many people underserved. The Landmark-based system will also provide a tool for testing whether human speech recognition depends on finding the individual cues to the sounds of words, even when those cues are very different in different contexts, and so can lead to the development of a new model of human speech perception.<br/><br/>The system works by extracting speech-related measurements from the signal, such as fundamental frequency, formant frequencies, spectral band energies and their derivatives, and interpreting these measures as acoustic cues for distinctive features. Innovative aspects of the system include the use of Landmarks, which are the most robust of the acoustic feature cues and are related to articulatory manner features. Once the landmark acoustic cues are found, other acoustic cues related to place and voicing features, and to prosodic structure, can also be found. The extraction of distinctive features and prosodic structure provides the first abstract linguistic units that can be extracted from the physical continuous signal, and this information is used to identify words, and to construct a representation of the entire utterance. To develop and evaluate the performance of this innovative system, speech databases consisting of isolated vowel-consonant-vowel sequences, read continuous speech, read radio-style speech, and spontaneous speech will be hand-labeled with Landmarks and other acoustic cues. Results of this basic speech research project will support the development of new approaches to ASR, will provide a testable computational model of human speech production, and will produce material suitable for development of a tutorial to train students in engineering, linguistics and cognitive science to label acoustic feature cues."
"1627041","Doctoral Dissertation Research:  The production and perception of breathy voice during nasal sounds","BCS","DDRI Linguistics","07/15/2016","07/15/2016","Marc Garellek","CA","University of California-San Diego","Standard Grant","William Badecker","07/31/2018","$5,423.00","Amanda Ritchart","mgarellek@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","SBE","8374","1311, 9179","$0.00","Language users often adjust or enhance their speech to be better understood in different contexts, such as when speaking in a loud room, or to a person with an accent, or when speaking on the phone. The study of speech enhancement strategies addresses important questions about human language, including how language is perceived by listeners and represented in the mind. Answering these questions is also vital to both speech therapy and speech technology. For speech therapy, this understanding can improve methods used in diagnosis and treatment of disordered speech. For speech technology, this understanding can improve speech recognition and synthesis, which are both used daily by the public, such as when calling a company's customer service. Knowledge of the way we enhance speech also helps researchers understand how and why languages change over time. This understanding is crucial for educating the public on linguistic diversity, change, and preservation.<br/><br/>This research seeks to understand what enhancements occur when making nasal sounds, which are found in almost all languages of the world, including American English. Nasality is a common feature associated with an individual's voice: in any language, some speakers sound more nasal than others. This research focuses on how the voice is used to make sounds nasal. First, the researchers investigate whether speakers of American English and Brazilian Portuguese produce nasals with voice adjustments from the larynx. Second, they investigate how the perception of voice influences the perception of nasal sounds. These two languages differ in their use of nasal sounds, and so together they provide a better understanding of how different types of nasal sounds are made in different languages. This knowledge will help clarify how this particular type of sound is produced and perceived, which has the potential to improve techniques used to treat disordered speech as well as software used in speech recognition and synthesis."
"1617253","CHS: Small: Game for Cleft Speech Therapy","IIS","HCC-Human-Centered Computing","07/15/2016","05/28/2020","Sri Kurniawan","CA","University of California-Santa Cruz","Continuing Grant","Ephraim Glinert","06/30/2021","$548,000.00","Su-hua Wang, Travis Tollefson, Christina Roth","srikur@soe.ucsc.edu","1156 High Street","Santa Cruz","CA","950641077","8314595278","CSE","7367","7367, 7923, 9251","$0.00","Orofacial clefts (i.e., cleft lip, cleft lip and palate, and isolated cleft palate, as well as the rare median, lateral [transversal], and oblique facial clefts) are among the most common congenital anomalies.  Approximately 20 infants are born in the United States with orofacial clefts on an average day, or 7500 every year.  With a cleft palate, one is unable to stop airflow through the nose using normal mechanisms; cleft palate speech therefore contains sounds with air leaking out of the nose, referred to as ""nasal escape.""  In the world of trial-and-error that governs speech learning, the child uses the only tool s/he has available to keep air from escaping out of the nose; s/he holds it back at the level of the glottis or larynx.  This mechanism is used by the normal voice to make a hard 'g' as in 'go.'  The child uses this ""glottal stop"" as a substitute for a variety of sounds that s/he cannot create normally.  Cleft palate speech thus becomes a collection of sounds characterized by glottal stops and inappropriate nasal escape; these anomalous articulation patterns are usually referred to as compensatory articulation disorder (CAD), and they severely affect speech intelligibility.  For this reason, corrective surgery is commonly performed around 10-12 months of age, with the goal of providing a more normal anatomical framework by the time the child begins practicing speech.  The repaired palate continues, however, to be variably impaired by the less-than-normal muscle bulk typical of cleft palates and by the stiffness of normal post-surgical scar tissue.  Over perhaps one year following surgical repair, palatal function spontaneously improves to the point where in the majority of children it is adequate to selectively prevent nasal escape.  Speech therapy after surgery to correct CAD begins at the age of two years and often continues for many years.  Correcting cleft speech is important for the child's future ability to live independently and to participate fully in society.  Despite the documented benefits, it is a challenge for speech pathologists to train children in proper speech production at an early age when the likelihood of success is highest, because young children are typically less cooperative, sometimes do not fully comprehend what they are being asked to do, and are often unwilling to do unrewarding speech homework, typically under the guidance of inexperienced parents who are unable to assess subtle progress (or lack thereof).  The PI's goal in this research is to understand the best strategy for helping children with corrected cleft palate produce normalized speech, and to facilitate this process through games that children can use at home with minimal help from parents while allowing data relating to the child's progress to be delivered to speech pathologists in real time.  Project outcomes will especially benefit children from underserved populations.  A cleft speech corpus will benefit researchers working on speech recognition algorithms for cleft speech detection, and new speech engine algorithms will benefit speech therapy at large.  The work will also spur development of a new research focus at UCSC and UCD in human-centered games for health and healthy living.<br/><br/>A major scientific contribution of this project will be a deeper understanding of the determining characteristics of children with cleft palate and how these relate to the phonetic and phonological rule causes of cleft speech.  The technological contributions will be the games and speech engines that are procedurally generated to support in-home and independently administered speech therapy for children with corrected cleft palate, and an algorithm that enables longitudinal voice data curation and analysis to be carried out in real time over the intervention history for every participant as well as across participants.  Methodological contribution will include a measure of error rates when a speech recognition system is designed to pick up cleft-specific mispronunciations, and a method for conducting participatory design of games for speech therapy involving computer scientists, engineers, developmental psychologists, speech and language pathologists, plastic surgeons, and children with corrected cleft palate and their parents."
"1648329","SBIR Phase I:  Touch-free Voice Trigger for Power Constrained Systems","IIP","SBIR Phase I","12/15/2016","12/14/2016","Kan Li","FL","pulsics, LLC","Standard Grant","Peter Atherton","08/31/2017","$225,000.00","","mail.kan@gmail.com","107 SW 140th Terrace #1","Newberry","FL","326693367","8082718600","ENG","5371","5371, 8033","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project will be to bring positive impactful changes to the economy, society, and scientific/technological understanding through intelligent voice-enabled technology. Commercially, the introduction of an intelligent, ultra-low power, embedded voice trigger will capture an important portion of the $113 billion global voice and speech recognition market, contributing to its expansion and that of related markets such as wearables and Internet of Things (IoT). The long-term potential impact is broad and encompasses the key players in the voice recognition ecosystem such as the mobile phone industry, semiconductor companies, chip designers, technology providers and system integrators, who will be enabled to innovate with the proposed platform. Societally, the day-to-day lives of users will be made easier by empowering them through the faster and natural use of speech commands to access device functionality and to retrieve processed information in real time. Furthermore, by performing onboard speech processing, the proposed technology alleviates many societal issues and challenges surrounding privacy and data protection with cloud-based solutions.<br/><br/>This Small Business Innovation Research Phase I project aims to develop an always-on, touch-free, embedded voice trigger with ultra-low power consumption for deployment in smart products and IoT systems. This novel speech processing technology transforms analog speech signals into biologically-inspired data-efficient binary pulse trains and processes them using adaptive state models. These models can be deployed in a network of reprogrammable automata with high accuracy using only the timing information of the digital pulses produced by the user's voice. Compared with conventional speech features, pulse trains, as used in the human auditory system, are highly robust to noise and lead to extremely small footprint hardware implementations, which are ideally suited for low power on-chip processing. Furthermore, by processing only ones and zeros with automata, the proposed approach requires no arithmetic, enabling real-time, complex voice processing capability that would otherwise be achieved at prohibitive costs or be relegated to the cloud. This keyword spotting technology is pre-configurable, customized for each user, and can be used in cellphones or similar portable devices with no network connection required after calibration. It offers increased security and privacy to access portable device features by eliminating the need to send potentially sensitive data continuously online."
"1608140","US-German Data Sharing Proposal: CRCNS Data Sharing: REvealing SPONtaneous Speech Processes in Electrocorticography (RESPONSE)","IIS","CRCNS-Computation Neuroscience, IntgStrat Undst Neurl&Cogn Sys","08/01/2016","08/08/2016","Dean Krusienski","VA","Old Dominion University Research Foundation","Standard Grant","uri hasson","02/28/2019","$552,301.00","Jerry Shih","djkrusienski@vcu.edu","4111 Monarch Way","Norfolk","VA","235082561","7576834293","CSE","7327, 8624","7327, 8089, 8091","$0.00","The uniquely human capability to produce speech enables swift communication of abstract and substantive information. Currently, nearly two million people in the United States, and far more worldwide, suffer from significant speech production deficits as a result of severe neuromuscular impairments due to injury or disease. In extreme cases, individuals may be unable to speak at all. These individuals would greatly benefit from a device that could alleviate speech deficits and enable them to communicate more naturally and effectively. This project will explore aspects of decoding a user's intended speech directly from the electrical activity of the brain and converting it to synthesized speech that could be played through a loudspeaker in real-time to emulate natural speaking from thought. In particular, this project will uniquely focus on decoding continuous, spontaneous speech processes to achieve more natural and practical communication device for the severely disabled.<br/><br/>The complex dynamics of brain activity and the fundamental processing units of continuous speech production and perception are largely unknown, and such dynamics make it challenging to investigate these speech processes with traditional neuroimaging techniques. Electrocorticography (ECoG) measures electrical activity directly from the brain surface and covers an area large enough to provide insights about widespread networks for speech production and understanding, while simultaneously providing localized information for decoding nuanced aspects of the underlying speech processes. Thus, ECoG is instrumental and unparalleled for investigating the detailed spatiotemporal dynamics of speech. The research team's prior work has shown for the first time the detailed spatiotemporal progression of brain activity during prompted continuous speech, and that the team's Brain-to-text system can model phonemes and decode words. However, in pursuit of the ultimate objective of developing a natural speech neuroprosthetic for the severely disabled, research must move beyond studying prompted and isolated aspects of speech. This project will extend the research team's prior experiments to investigate the neural processes of spontaneous and imagined speech production. In conjunction with in-depth analysis of the recorded neural signals, the researchers will apply customized ECoG-based automatic speech recognition (ASR) techniques to facilitate the analysis of the large amount of phones occurring in continuous speech. Ultimately, the project aims to define fundamental units of continuous speech production and understanding, illustrate functional differences between these units, and demonstrate that representations of spontaneous speech can be synthesized directly from the neural recordings. A companion project is being funded by the Federal Ministry of Education and Research, Germany (BMBF)"
"1632582","SBIR Phase II:  Automated Public Speaking Assessment","IIP","SBIR Phase II","08/15/2016","05/29/2019","Debra Cancro","MD","VoiceVibes, Inc.","Standard Grant","Rajesh Mehta","06/30/2020","$1,009,443.00","","debra@myvoicevibes.com","7224 Shub Farm Rd.","Marriottsville","MD","211041171","4107461696","ENG","5373","118E, 165E, 169E, 5373, 8031, 8032, 8039, 8240","$0.00","This Phase II project aims to develop software to automatically assess public speaking skills and prepare students with better oral communications skills necessary to perform job tasks. Oral expression is the most highly valued ability throughout the economy and ranks as the second most highly-valued skill for high-wage, high-growth, high-skill occupations. Approximately 4.5 million college students take a basic communications course each year, however, as class sizes get larger and online learning becomes more common, public speaking instruction becomes increasingly difficult.  Practice and feedback are essential aspects of these courses, yet it is a struggle for teachers to find enough time to sufficiently interact with students. This SBIR project aims to develop the key concepts of automated public speaking assessment such that a student?s vocal delivery can be objectively measured and presented in a manner that creates an independent, personalized learning experience. Unlike traditional methods of public speaking assessment, the proposed system can be available at any time, provide objective feedback and track student practice and improvement. The proposed Software-as-a-Service is projected to generate $16 Million in revenue over five years and create more than 25 high-paying, US-based jobs. <br/><br/><br/>This Small Business Innovative Research (SBIR) Phase II project proposes to develop an automated assessment system for public speaking that determines how a speaker would be perceived by an audience.  Automated assessment for speech has already occurred in spoken language proficiency, which leverages Automated Speech Recognition (ASR) and semantic analysis. Automated voice assessment has also been utilized in lie detection and emotion detection, which focus on autonomic responses in the user?s voice, such as when stress affects the vocal cords. The hypothesis behind this SBIR project is that speakers can consciously use and modify non-semantic speech behaviors to produce more desirable listener perceptions. Automatically linking listener perception to speech behaviors represents a novel direction in automated assessment for speech. The Phase II objective is to develop software sufficient for automated public speaking assessment such that a student?s vocal delivery can be objectively measured and presented in a manner that creates an independent, personalized learning experience. Voice analytics capability investigated in Phase I will be enhanced and developed into a cloud-based service which helps students practice, track, and improve their public speaking habits."
"1625680","Automated Large-Scale Phonetic Analysis: DASS Pilot","BCS","Linguistics","08/15/2016","08/11/2016","William Kretzschmar","GA","University of Georgia Research Foundation Inc","Standard Grant","Joan Maling","03/31/2021","$377,295.00","Margaret Renwick","kretzsch@uga.edu","310 East Campus Rd","ATHENS","GA","306021589","7065425939","SBE","1311","1311, 9178, 9179, SMET","$0.00","Generalizations about language contained in dictionaries and grammars hide the extensive variation in the way that speakers actually use language. However, modern technology now makes it possible to use automated means to extract variation in pronunciation from spoken interviews. This research project uses available software to process sixty-four interviews with speakers from Florida, Georgia, Tennessee, Alabama, Mississippi, Louisiana, Arkansas, and Texas recorded from 1968-1983. These interviews constitute a geographic and social sample of speakers across the Gulf States. All of the transcriptions of the interviews, the vowel pronunciation data, and the visualizations will be presented on the website of the Linguistic Atlas Project. Detailed data on actual speaker variation addresses the industrial methods currently used for speech recognition and speech synthesis.<br/><br/>This project will be the first large-scale test of the complex systems model against acoustic phonetic data. The legacy interviews consist of over 200 Gb of files containing 372 hours of digital audio interviews. In the first stage of the research, vowel pronunciations will be extracted from a list of seventy-eight different words that were elicitation targets in the interviews, plus additional words found to occur frequently in the interviews such as color terms, up to a total of three hundred words. The resulting data set will have approximately 22,500 vowel tokens per interview, nearly 1,500,000 tokens across the data set, a very large corpus of data on Southern American English. The second stage of the project will create visualizations of these tokens to determine the dimensions of variation in the realization of vowels per speaker, social category, and geographic area. The science of complex systems will be employed as a model in the analysis, which predicts that the wide range of realizations that occurs in the groups under analysis will be self-organized into nonlinear distributional patterns. The extraction and display of the full range of vowel variation has the potential to improve industrial methods used for both speech recognition and speech synthesis, as it offers a detailed view of actual variation for speakers and groups rather than assuming a consistent or ?average? realization of vowels."
"1549808","STTR Phase I:  Commercializing an intelligent reading tutor for mobile devices","IIP","STTR PHASE I","01/01/2016","12/07/2015","John Carney","VA","Carney Labs, LLC","Standard Grant","Ben Schrag","05/31/2017","$224,609.00","Gwendolyn Cartledge","john.carney@mari.com","100 N. Pitt St.","Alexandria","VA","223143134","7039696800","ENG","1505","118E, 1505, 8031, 8032, 8039, 9177","$0.00","This STTR Phase I project will commercialize a reading intelligent tutor and take it from a university-based research environment to a commercially viable product.  Phase I will concentrate on the research objectives of finalizing technical innovations and testing and hardening the software for commercial use. A unique feature of this intelligent tutor is its content authoring capability that provides teachers and support staffs the ability to create reading material to best meet the needs of their learners without having to know anything about programming, linguistics or speech recognition.  Another innovation of this tutor is the ability to provide error correction while reading and promoting fluency. The initial impact of this project is the ability to improve literacy for children using an intelligent tutor that's commercially available on today's mobile devices and easy to use and author personalized content. The broader impact is to leverage and design the software for adult learners. Literacy itself is more important than ever in today's global, high-tech economy. The proposed research of this STTR project will address literacy at all ages using an environment that's personalized, accessible, mobile, and adapted for all users.<br/><br/>This Small Business Technology Transfer Phase I project will focus on the commercialization of a research based reading intelligent tutor.  The two technical hurdles are user interface (UI) and providing fluent speech recognition.  One of the major difficulties in developing content that relies on the computer to provide coaching is providing the needed pronunciation information for each word. A crucial success is the ability for the reading tutor to allow the teachers and others to create content for the system without having to know anything about programming, linguistics or speech recognition. Another crucial success is the ability to allow the teachers to specify custom word pronunciations for out of dictionary words using brackets and the normal 26 characters of the English alphabet. There are not any reading fluency programs in the commercial market that combine this technical process with a ""teacher friendly"" user interface. This innovation enables teachers to truly create and add relevant reading materials to the reading intelligent tutor for their learners. The goal of the research in this proposal is to address the technical hurdles and prepare the reading intelligent tutoring software for commercialization across multiple platforms and mobile devices."
"1723215","SHF: Large: Collaborative Research: Exploiting the Naturalness of Software","CCF","Information Technology Researc, Software & Hardware Foundation","07/01/2016","05/08/2020","Tien Nguyen","TX","University of Texas at Dallas","Continuing Grant","Sol Greenspan","06/30/2021","$260,709.00","","nguyen.n.tien@gmail.com","800 W. Campbell Rd., AD15","Richardson","TX","750803021","9728832313","CSE","1640, 7798","7925, 7944","$0.00","This inter-disciplinary project has its roots in Natural Language (NL) processing. Languages such as English allow intricate, lovely and complex constructions; yet, everyday, ``natural? speech and writing is simple, prosaic, and repetitive, and thus amenable to statistical modeling. Once large NL corpora became available, computational muscle and algorithmic insight led to rapid advances in the statistical modeling of natural utterances, and revolutionized tasks such as translation, speech recognition, text summarization, etc.  While programming languages, like NL, are flexible and powerful, in theory allowing a great variety of complex programs to be written, we find that ``natural? programs that people actually write are regular, repetitive and predictable. This project will use statistical models to capture and exploit this regularity to create a new generation of software engineering tools to achieve transformative improvements in software quality and productivity. <br/> <br/>The project will exploit language modeling techniques to capture the regularity in natural programs at the lexical, syntactic, and semantic levels. Statistical modeling will also be used to capture alignment regularities in ``bilingual? corpora such as code with comments, or explanatory text (e.g., Stackoverflow) and in systems developed on two platforms such as Java and C#.  These statistical models will help drive novel, data-driven approaches for applications such as code suggestion and completion, and assistive devices for programmers with movement or visual challenges. These models will also be exploited to correct simple errors in programs. Models of bilingual data will used to build code summarization and code retrieval tools, as well as tools for porting across platforms. Finally, this project will create a large, curated corpus of  software, and code analysis products, as well as a corpus of alignments within software bilingual corpora, to help create and nurture a research community in this area."
"1563098","CIF: Medium: Collaborative Research: Learning in High Dimensions: From Theory to Data and Back","CCF","Comm & Information Foundations","07/01/2016","09/13/2018","David Tse","CA","Stanford University","Continuing Grant","Phillip Regalia","06/30/2021","$597,629.00","","dntse@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","7797","7797, 7924, 7936","$0.00","Statistical-modeling is the cornerstone of analyzing modern data sets, and using observed data to learn the underlying statistical model is a crucial part of most data analysis tasks. However, with the success of data utilization came a vast increase in its complexity as expressed in complex models, numerous parameters, and high dimensional features. This research project studies problems in learning such high-dimensional models, both in theory and in practice with actual datasets in cutting-edge applications. <br/><br/>Learning high-dimensional models efficiently, both in terms of computation and in terms of the use of the data, is an important challenge. The research characterizes the fundamental limits on the sample and computational complexity of several key distribution learning problems, as well as the associated optimal learning algorithms that achieve the limits. The learning problems underpin important tasks such as clustering, multiple testing of hypothesis and information measure estimation. The new algorithms and new methodologies developed are evaluated and applied on real data from three specific applications: 1) denoising of high throughput transcriptomic data; 2) analysis of omics data for personalized medicine; 3) ecological population studies. While these applications are useful on their own right, there will also be many other potential applications in fields such as speech recognition, topic modeling, character recognition, neuroscience, etc."
"1564355","CIF: Medium: Collaborative Research: Learning in High Dimensions: From Theory to Data and Back","CCF","Comm & Information Foundations","07/01/2016","06/29/2016","Alon Orlitsky","CA","University of California-San Diego","Continuing Grant","Phillip Regalia","06/30/2020","$297,994.00","","alon@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7797","7797, 7924, 7936","$0.00","Statistical-modeling is the cornerstone of analyzing modern data sets, and using observed data to learn the underlying statistical model is a crucial part of most data analysis tasks. However, with the success of data utilization came a vast increase in its complexity as expressed in complex models, numerous parameters, and high dimensional features. This research project studies problems in learning such high-dimensional models, both in theory and in practice with actual datasets in cutting-edge applications.<br/><br/>Learning high-dimensional models efficiently, both in terms of computation and in terms of the use of the data, is an important challenge. The research characterizes the fundamental limits on the sample and computational complexity of several key distribution learning problems, as well as the associated optimal learning algorithms that achieve the limits. The learning problems underpin important tasks such as clustering, multiple testing of hypothesis and information measure estimation. The new algorithms and new methodologies developed are evaluated and applied on real data from three specific applications: 1) denoising of high throughput transcriptomic data; 2) analysis of omics data for personalized medicine; 3) ecological population studies.  While these applications are useful on their own right, there will also be many other potential applications in fields such as speech recognition, topic modeling, character recognition, neuroscience, etc."
"1701433","A telescopic algorithm for two-dimensional hidden Markov models with application to genetic studies","DMS","NIGMS","07/01/2016","06/28/2018","Xiang-Yang Lou","AR","Arkansas Childrens Hospital Research Institute","Continuing Grant","Junping Wang","04/30/2020","$1,492,518.00","","zjulxy219@yahoo.com","13 Children's Way","Little Rock","AR","722023510","5013642469","MPS","8047","4075, 9150","$0.00","Family-based genetic studies use a number of statistical techniques to understand how genetic information flows from parent to offspring. The investigators will develop computationally efficient algorithms for identifying genetic information from family data using a technique known as a Hidden Markov model. This model has demonstrated considerable success in a broad range of scientific disciplines, including areas such as speech recognition in telephone conversations and face identification from sequences of images. When the complexity of the problem increases, computational algorithms confront challenges with finding the solution of the model in realistic and practical time frames. The investigators' research on novel computational and statistical approaches will provide efficient algorithms, and software tools, of broad scientific relevance. In their work the tools developed will yield a more powerful approach for finding genes underlying complex diseases. The collaborative team will also directly apply their techniques to forestry genetic data describing a multi-year plant-breeding program.  Educationally, the trainees involved will be integrated into a vibrant interdisciplinary research team, gaining exposure to techniques for the solution of real world problems. <br/><br/>Multi-dimensional Markov processes are ubiquitous in the real world. Dependencies in interacting particle systems, images, videos, digitized documents, and gene transmission are all examples of multi-dimensional Markov processes. A Markov process is a random process (i.e., a statistical phenomenon in which the possible outcomes of a sequence of events or variables vary) in which the prediction of the future state is made just using the information of the current state, independent of both past history and unknown future states. This idea generalizes for higher dimensions.  For example, gene transmission from parent to child is a two-dimensional Markov process, the determination of the future state, the child, depends on the neighbors in two dimensions, the parents.  Most cases of interest, however, are hidden Markov processes, in which the current state of a variable cannot be observed, but must be inferred by consideration of all possible future states given all possible current states.  As the number of quantities to be determined increases, the computational demand in both computer memory and computing time increases dramatically. To apply the concept, therefore, to many important real applications requires the development of novel and powerful computational algorithms.  This project is focused on the design of a novel software package, specifically designed for the solution of hidden Markov models. The techniques developed will be relevant for the solution of problems from a broad array of disciplines, not only for wider family based genetic studies but also areas such as text analysis from images. Thus, this project will increase the computational ability to solve real-world problems across many engineering, geosciences and biological disciplines, with commensurate potential positive societal impact. In this case an interesting application arises in plant breeding through the collaboration with experts in forestry science."
"1618336","RI: Small: Using Automatically Generated Paraphrases and Discriminative ASR Training to Author Robust Question-Answering Dialogue Systems","IIS","Robust Intelligence","09/01/2016","05/15/2018","Michael White","OH","Ohio State University","Standard Grant","Tatiana Korelsky","08/31/2021","$458,000.00","Eric Fosler-Lussier","mwhite@ling.ohio-state.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","7495","7495, 7923, 9251","$0.00","Question-answering (QA) dialogue systems are useful in a broad range of contexts where the primary role of a virtual character is to answer questions posed by the human user. In QA-based dialogue systems, the primary interpretation task can be framed as matching a user's question against a set of questions anticipated by the content author.  This project investigates for the first time how methods for automatically paraphrasing anticipated questions can aid authors in establishing a large set of expected question variants, making it possible to dramatically enhance interpretation robustness for both chatted and spoken language.  By evaluating the project with an existing virtual patient dialogue system and new virtual guide for Columbus, Ohio's COSI (Center of Science and Industry) science museum, the project will enhance medical education and provide an inspirational example of science in action to the children who attend the museum.  It also promises to enhance the effectiveness of short answer scoring in educational software and commercial QA systems for frequently asked questions.<br/><br/><br/>The proposed approach is the first to explore the potential of advanced automatic paraphrasing techniques to enhance the robustness of interpretation in an easy-to-author QA dialogue system.  By employing paraphrasing at content authoring time, it becomes possible to take advantage of and make explicit the author's knowledge of the space of functionally equivalent questions, potentially leading to dramatic improvements in interpretation accuracy; furthermore, doing so makes it possible to set up an effective, task-relevant discrimination space for Automatic Speech Recognition (ASR) training.  To generate paraphrases, the project uses the grammar-based surface realizer OpenCCG for lexico-syntactic alternations together with broad coverage resources, vector space models of word meaning and multiword alignments. To train discriminative ASR models that make a difference in question interpretation, generated paraphrases are incorporated into the semantic error rate estimation.  Using data collected from medical students and museum visitors, the project assesses the approach via its impact on interpretation accuracy and qualitative measures of usability."
"1600325","EAGER: Early Stage Research on Automatically Identifying Instructional Moves in Mathematics","DRL","ECR-EHR Core Research","06/15/2016","06/06/2016","Tamara Sumner","CO","University of Colorado at Boulder","Standard Grant","John Cherniavsky","05/31/2018","$299,928.00","Wayne Ward, William Penuel","sumner@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","EHR","7980","7916, 8244, 8817","$0.00","This is an Early-concept Grant for Exploratory Research research project to develop automated tools to aid in the development of mathematics teaching expertise in preservice teachers. Current research on preservice mathematics teacher instruction relies on observing preservice teachers interacting with students and recording their mathematical interactions to provide guidance and advice as how the preservice teachers can improve their teaching. This research requires highly trained observers and highly trained analysts to record and interpret the student teacher verbal interactions in order to give teachers feed back in how to improve their instruction. The aim of this project is to automate the observation, recording, and interpretation of student-teacher interactions. This would result in more effective research on instructional strategies for the preservice teachers and ultimately lead to changes in teacher professional development when feedback only available in research environments becomes feasible for all preservice teacher professional development. <br/><br/>The project will use as an initial basis the observation toolkit Accountablity Talk for providing teachers with both formative and summative feedback on their instruction. Automatic speech recognition and natural language technologies will be used to record and interpret student teacher verbal interactions. The results of this research have the potential to democratize preservice mathematics teacher professional development and, over time, provide insight into teacher learning that can result in restructuring teacher learning environment to make them more effective in developing high quality mathematics teachers."
"1605228","A Shared Autonomy Approach to Robotic Arm Assistance with Daily Activities","CBET","Disability & Rehab Engineering","09/01/2016","09/09/2019","Patricio Vela","GA","Georgia Tech Research Corporation","Standard Grant","Aleksandr Simonian","08/31/2020","$298,503.00","Maysam Ghovanloo","pvela@ece.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","ENG","5342","","$0.00","Persons with high-level paralysis rely on the care of others and the modification of their environment for accomplishing the activities of daily living (ADL). Due to the degree of support required, paralysis is costly to provide care for. Recovering the ability to manipulate through technological means would lead to significant improvements to the quality of life for the user, and would reduce the long-term economic impact of paralysis and contribute to society by freeing up caregiver time and labor (usually a family-member or friend). To that end, the project proposes to design and validate a wheelchair-mounted robotic-arm with an augmented reality interface for enabling non-tactile human-robot interaction. Importantly, the design will involve a multi-modal interface approach, which includes a recently developed tongue drive interface, a head orientation sensor, and a speech recognition system. User-centric and participatory design methods will ensure that the engineered system will be seen favorably by persons with tetraplegia. The award also supports fundamental research into the design of collaborative human-robot assistive devices, including the interface design and the underlying robot vision and planning algorithms. The research contribution includes improved understanding on how to effectively coordinate the higher level reasoning and thought processes of humans with the autonomous operation capabilities and limitations of current robotic arms. The social significance of the robotics application will be capitalized to create engaging educational and outreach activities in promotion of engineering mathematics.<br/><br/>The long-term goal is to transform the lives of wheelchair bound persons with limited to no manipulation abilities by enabling them to independently perform the activities of daily living (ADL) irrespective of their environment. Technology meeting the varied needs of the ADL tends to have high control complexity, which impedes adoption. It is essential for these technologies to request high-level (guiding) commands rather than low-level control signals, and to request feedback in a manner compatible with the user's conception of the world. The research goal is to engineer and validate a wheelchair-fitted robotic-arm with an augmented reality interface and a multi-modal user interface for coupling the human command and intent control loop with the robotic-arm decision and control loop.  The novelty of the proposed system is that it is a shared control and a shared sensing assistive technology. The coupled system requires human input to overcome the perceptual limitations of robot vision, and employs robotic planning and manipulation to overcome the physical limitations of the user. By involving the human for scene interpretation and by providing ego-centric information to the robot arm, the coupled system minimizes and simplifies user input during complex manipulation tasks. The associated research objectives are to (I) engineer a human-robot augmented reality interface system with coupled feedback to the two systems (human and robot) for communicating intent and requesting high-level user feedback for complex manipulation tasks, (II) identify the manipulation assistance needs and user interface design through a participatory design process, (III) evaluate the user interface with respect to the desired command and control objectives, and (IV) assess the impact of the operational system with respect to task performance and cognitive burden. Engineering the unique shared control and shared sensing system and achieving the proposed research objectives would transform the way that assistive robotic technologies couple the user to technology. Incorporating the research into existing education and outreach activities would promote engineering and robotics to students and prepare them for participation in an increasingly automated world."
"1631562","Doctoral Mentoring Consortium at the 25th International Joint Conference on Artificial Intelligence (IJCAI 2016)","IIS","ROBUST INTELLIGENCE","04/01/2016","04/19/2016","ERIC EATON","PA","University of Pennsylvania","Standard Grant","James Donlon","03/31/2017","$15,000.00","","eeaton@seas.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","CSE","7495","7495, 7556","$0.00","The purpose of this project is to support the participation of a select group of US-based PhD students in the doctoral consortium at the 2016 International Joint Conference on Artificial Intelligence (IJCAI). IJCAI is a premier conference in Artificial Intelligence (AI), drawing wide domestic and international participation, including many top researchers. In particular, the doctoral consortium seeks to broaden the participation of underrepresented groups in this field of research, and we expect to have diverse participation in terms of gender, ethnic background, academic institution, and geographic location. In addition to technical exposure, this experience will give students the opportunity to interact with a broad range of researchers and educators involved in the AI field. This will help the students to become members of the international community of scholars, and to develop into the global scientists that are needed for the future. In the longer term, having engaged and informed scientists will help advance science and high-tech industry in the US.<br/><br/>The wide variety and significance of the topics typically presented at IJCAI, in both the technical program and the doctoral consortium, will provide a great opportunity for students to share, exchange information, and learn from each other and from the best in the field. Participation in a conference is a great motivator for graduate students, and we expect the IJCAI doctoral consortium will generate additional long-term interest among graduate students in pursuing a research-oriented career in artificial intelligence.  Doctoral consortium participants will also have the opportunity to receive feedback on their doctoral research from established scientists who will serve as mentors throughout the proceedings."
"1650295","CAP: The Seventh Symposium on Educational Advances in Artificial Intelligence (EAAI-17)","IIS","Cyberlearn & Future Learn Tech","09/01/2016","09/02/2016","ERIC EATON","CA","Association for the Advancement of Artificial Intelligence","Standard Grant","Tatiana Korelsky","08/31/2018","$22,600.00","","eeaton@seas.upenn.edu","2275 E BAYSHORE RD STE 160","East Palo Alto","CA","943033224","6503283123","CSE","8020","7556, 8045, 8055","$0.00","The Cyberlearning and Future Learning Technologies Program funds efforts that will help envision the next generation of learning technologies and advance what we know about how people learn in technology-rich environments. Cyberlearning CAP projects build capacity for research and development in the field of cyberlearning by improving technical infrastructure, human capital, and in other ways. The Symposium on Educational Advances in Artificial Intelligence, affiliated with the AAAI (Association for the Advancement of Artificial Intelligence) conference in San Francisco in February 2017 is a premier venue for sharing knowledge about how to use AI in education and how to teach about AI. This grant helps support participation of US students and teachers in the conference, as well as early career faculty and organizers of REU (Research Experiences for Undergraduates) sites. <br/><br/>The symposium includes a variety of activities that advance the field and train the next generation of AI researchers and investigators. Peer reviewed papers and keynote addresses are complemented by sessions on model classroom assignments in teaching AI, and a competition for undergrads to create innovative tasks for students learning robotics. A discussion group will share best practices among REU site coordinators. Tracks will consider outreach and ethics in teaching AI. Broader impacts beyond the participants at the symposium are supported by the materials produced and shared among the community, such as the database of open educational materials, and the outputs of the student competition."
"1636932","I-Corps: Artificial Intelligence for Analysis Of Visual Art","IIP","I-Corps","09/01/2016","08/15/2016","Ahmed Elgammal","NJ","Rutgers University New Brunswick","Standard Grant","Lydia Mcclure","10/31/2016","$19,201.00","","elgammal@cs.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","ENG","8023","","$0.00","This I-Corps project utilizes artificial intelligence (AI) principles to detect abnormalities in images - in particular, it focuses on ways to enable visual information extraction.<br/><br/>Large collections of art and other images exist in digitized form but there are few automated methods for processing and indexing this data. This project facilitates a wide range of applications from education to online markets because the technology focuses on automated analysis of art and other images based on algorithmic methodologies in artificial intelligence. For example, the project's computational models might facilitate indexing large-scale collections of art and extracting useful information from this massive visual data. In this context, knowledge can be useful to online and traditional galleries, collectors, educators, authenticators and others in making informed decision regarding artwork and artist.<br/><br/>While the AI techniques are applicable to a broad range of images, the I-Corps team initially developed prototype computational models for analyzing artworks based on the visual information extracted from paintings. The computational models enable the system to perform perceptual and cognition tasks on digitized art for classification of paintings. AI algorithm can be specialized or tuned and optimized for different domains of fine-art analysis. While initially focusing on customer discovery within artistic contexts, the technology has the potential to be applicable in many different contexts. The I-Corps experience will help the team to better understand the market scope."
"1624673","III: Student Travel to Workshop on Intelligent Systems for Supporting Distributed Human Teamwork","IIS","Info Integration & Informatics","03/15/2016","02/25/2016","Barbara Grosz","CA","Association for the Advancement of Artificial Intelligence","Standard Grant","Sylvia Spengler","02/28/2017","$15,000.00","","grosz@eecs.harvard.edu","2275 E BAYSHORE RD STE 160","East Palo Alto","CA","943033224","6503283123","CSE","7364","7364, 7556","$0.00","This award will support the travel and housing expenses of students selected to participate in the AAAI Spring Symposium on Intelligent Systems for Supporting Distributed Human Teamwork, which will be held on March 21-23, 2016, in Palo Alto, California. The symposium is part of the annual AAAI Spring Symposia series. The symposium will bring together researchers studying teamwork in different fields, including artificial intelligence, human-computer interaction, and the social sciences. For student participants, this experience will provide a unique opportunity to interact with researchers from multiple disciplines and learn about research in different fields. Students will receive feedback on their own research and will work closely with senior researchers and other students to shape a new research agenda for the development of intelligent systems for supporting human teamwork. The symposium will be a starting point for forming a new cross-disciplinary community of researchers who are interested in the development of intelligent systems for supporting teamwork. New approaches and systems for supporting teamwork are needed for supporting increasingly more complex distributed teamwork in such areas as healthcare, education and disaster relief."
"1618783","RI: Small: Effective Preference Reasoning over Combinatorial Domains: Principles, Problems, Algorithms, and Implementations","IIS","Robust Intelligence","07/01/2016","12/22/2017","Miroslaw Truszczynski","KY","University of Kentucky Research Foundation","Standard Grant","Rebecca Hwa","06/30/2020","$457,594.00","","mirek@cs.uky.edu","109 Kinkead Hall","Lexington","KY","405260001","8592579420","CSE","7495","7495, 7923, 9150, 9251","$0.00","Preferences are fundamental attributes of human reasoning and decision making. They appear whenever a choice between alternatives is to be made. Understanding and automating preference reasoning is a major problem of artificial intelligence, especially important for the design of autonomous intelligent decision support systems. If there are few alternatives, preferences between them can be represented explicitly and preference reasoning is typically easy. However, in practice the number of alternatives facing the decision maker can be daunting in many cases.  In such cases, modeling and representing preferences of the decision maker, and automating preference reasoning based on the model are challenging. To respond to the challenge, the project will study principles and properties of preference aggregation and optimization over large domains of alternatives, and algorithms to support preference reasoning tasks; will develop methods for preference learning and approximation in support of building preference models; and will implement software for effective preference modeling and reasoning. Areas such as knowledge representation, computational social choice, and constraint solving embodied by answer-set programming and satisfiability testing will inform these studies. The project will result in a theoretical and algorithmic framework for preference reasoning over combinatorial domains, in software tools for effective preference reasoning, and in methods to integrate them into artificial intelligence decision support systems that are becoming pervasive in industrial, scientific and governmental applications. <br/><br/>The project will assume that the space of alternatives is modeled by a combinatorial domain, where alternatives are represented in terms of values of attributes relevant to decision making. While combinatorial domains are exponentially large in the number of attributes, the sets of values of individual attributes are typically small. This opens a possibility of expressing preferences over elements in a combinatorial domain in terms of preferences on attribute values and relations between the attributes. This is the setting for the project, with preference trees, CP-nets and answer set optimization programs as formal representations of preferences over combinatorial domains. The project will focus on preference aggregation and preference optimization. Finding optimal and near-optimal alternatives, finding collections of optimal or near-optimal alternatives that are in some sense diverse (or similar), and aggregating preferences that are only partially known are some examples of specific problems we will consider. As building manually preference models over large domains is infeasible, the project will study methods to learn preference models (for instance, preference trees), and develop methods for model approximation (different models have varying computational properties, and close approximations of ``hard'' models with ``easy'' ones may prove effective for reasoning with the former).  Finally, the project will develop a software suite for several key preference reasoning tasks. The implementation will exploit advances in answer-set programming and satisfiability. The resulting software will be systematically evaluated on benchmarks coming from or motivated by practical applications."
"1640830","Support for Young Researchers to attend the  2016 Intelligent Tutoring Systems Conference","IIS","Cyberlearn & Future Learn Tech","06/01/2016","07/06/2016","Beverly Woolf","MA","University of Massachusetts Amherst","Standard Grant","Tatiana Korelsky","05/31/2019","$20,000.00","","bev@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","8020","7556, 8045","$0.00","The Cyberlearning and Future Learning Technologies Program funds efforts that will help envision the next generation of learning technologies and advance what we know about how people learn in technology-rich environments. Cyberlearning CAP projects build capacity for research and development in the field of Cyberlearning by improving technical infrastructure, human capital, and in other ways. The Intelligent Tutoring Systems (ITS) 2016 conference offers a rare professional opportunity for interdisciplinary students to converge and present cutting-edge research from the fields of artificial intelligence (AI), learning science, computer science, cognitive and learning sciences, psychology, and educational technology. <br/><br/>This project supports travel for selected advanced graduate students to attend the doctoral consortium at the ITS 2016 conference in Croatia.  The conference has a special Young Research Track within the main conference for advanced graduate students. In this track a special session supports students to share their research with papers, posters, tutorials, workshops, and informal interactions with accomplished researchers. Students present their research ideas and receive feedback from researchers in the ITS community. An important goal of the doctoral consortium is to build the new generation of researchers in the forward-looking technical area of intelligent tutoring systems."
"1600043","Robotics Activities at Association for the Advancement of Artificial Intelligence (AAAI) 2016","IIS","Robust Intelligence","02/01/2016","11/18/2015","George Konidaris","CA","Association for the Advancement of Artificial Intelligence","Standard Grant","Weng-keen Wong","07/31/2016","$17,500.00","","gdk@cs.duke.edu","2275 E BAYSHORE RD STE 160","East Palo Alto","CA","943033224","6503283123","CSE","7495","7495, 7556","$0.00","Over the last few years, the AI and Robotics communities have begun to realize that the proliferation of relatively cheap but sophisticated robot hardware, and the development of robust and reliable robotics techniques for performing low-level control and perception.  These developments have created a demand for integrative AI and robotics research to fulfill the potential of today's robots. The project aims at fostering cross-fertilization between these research fields through a series of activities at the AAAI-16 conference that will be held in Phoenix, Arizona from February 12th to 17th, 2016.<br/><br/>The project will include the following activities at the AAAI-16 conference: (1) participation of PhD students interested in integrative AI and Robotics research, (2) presentations by researchers showcasing current problems and solutions in robotics research, (3) a ""spotlight"" presentation by a young researcher showcasing new and exciting research in the field of robotics, (4) a robotics exhibition, and (5) an open house day, open for the general public."
"1635253","Improved Human-Computer Interaction for Design of Complex Systems","CMMI","ESD-Eng & Systems Design","09/01/2016","06/19/2018","Guy Hoffman","NY","Cornell University","Standard Grant","Kathryn Jablokow","08/31/2019","$299,999.00","Daniel Selva Valero, So-Yeon Yoon, Guy Hoffman","hoffman@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","ENG","1464","067E, 068E, 073E","$0.00","Future design of complex systems, for example, the design of aircraft, buildings, and transportation systems, will be done collaboratively by mixed teams of humans and artificial intelligence (AI) systems. The success of this collaboration depends on the degree to which the humans and the AI systems can effectively communicate with each other their goals, intentions and plans of actions. This award seeks to improve how humans and AI systems communicate when designing such systems. In particular, the investigators will give intelligent design agents the ability to provide explanations of their actions and suggestions to humans through both verbal and non-verbal communication channels. It also investigates how the AI system can use robotic body language to improve the collaborative design process. This has potential to have an impact on many aspects of our society which engage in system design, including architecture, medicine, urban planning, industrial design, and business management. Additionally, the research project provides opportunities for education and outreach during its execution.<br/><br/>The research objective of this award is to enable mixed-initiative human-computer design of complex systems by giving intelligent design agents self-explaining abilities, modeling human-computer joint design as a collaborative activity, and leveraging the use of non-verbal channels and embodied interaction to improve human-machine communication in design. Starting from a model of design tools as intelligent agents, and based on the knowledge generated by the Human-Robot Collaboration literature, we will lay out the foundations of a new paradigm for engineering design based on mixed-initiative human-agent collaboration. The four pillars of this new paradigm are: (a) the coordination and meshing of shared plans and intentions between humans and design agents, and their resolution into individual agent plans and actions; (b) the dynamic allocation of roles and gradation of autonomy; (c) the reasoning about, generation, and maintenance of shared attention and common ground; and (d) the integration of verbal and nonverbal channels in communication about agent beliefs, intentions, and goals. Controlled experiments with human subjects will be used to test the effectiveness of the new framework and algorithms. If successful, this research can radically improve the quality of the designs and reduce the resources spent during the design process of complex engineering systems of national importance and high value to society such as systems-of-systems for weather forecasting, climate monitoring, disaster relief, or intelligence, surveillance, and reconnaissance. This research will also have broader impacts into areas outside of engineering that engage in system design activities, including architecture, medicine, urban planning, industrial design, and business management."
"1630047","Symposium on Combinatorial Search, SoCS-2016","IIS","Robust Intelligence","07/01/2016","06/17/2016","Richard Korf","CA","University of California-Los Angeles","Standard Grant","James Donlon","06/30/2019","$7,000.00","","korf@cs.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","7495","7495, 7556","$0.00","This grant is to provide partial support for U.S.-based graduate and undergraduate students to attend the Ninth Symposium on Combinatorial Search (SoCS-2016), a scientific conference to be held at Tarrytown, NY from July 6-8, 2016. Combinatorial search is an area of artificial intelligence that deals with systematic trial-and-error exploration of a very large number of alternative solutions to a problem.   NSF funding is crucial to support students who would otherwise not be able to attend the symposium.  Attending such meetings and presenting their research is an important part of the professional development of students, addressing a critical shortage of highly-skilled computer scientists in the U.S.<br/><br/>SOCS brings together researchers in heuristic search and combinatorial optimization from all areas of artificial intelligence, planning, robotics, constraint programming, operations research, and bioinformatics.  The intellectual merit of this activity stems from bringing together in one place at one time researchers from otherwise diverse areas of computer science that both advance the state of the art in heuristic search and/or combinatorial optimization, and also use these tools in their research.  The broader impacts come from cross-fertilization of different fields that advance and/or use these tools, by promoting research in this area by providing a small intimate meeting on these topics, a forum for presenting such work, and archival proceedings for publishing work in this area.  These latter goals are instrumental to training new researchers in this area."
"1650204","EAGER: Lexicalized Reasoning","IIS","Robust Intelligence","09/01/2016","08/30/2016","Christopher Geib","PA","Drexel University","Standard Grant","James Donlon","08/31/2017","$69,474.00","","cwg33@drexel.edu","1505 Race St, 10th Floor","Philadelphia","PA","191021119","2158955849","CSE","7495","7495, 7916","$0.00","Many problems in artificial intelligence form natural pairs of a recognition and generation problem.  For example, parsing natural language sentences, and generating new natural language sentence are natural inverses.  Similarly, recognizing the plans being executed by others and building our own plans seem intimately intertwined. Unfortunately research on methods to solve these problems has often required specialized representations and algorithms.  As a result, unified models of intelligence remain elusive and the results from one problem area do not translate to another.  This project is part of a research program based on the idea that unified models of intelligence require unifying representations and algorithms across research problems.  In particular, in the last ten years, Steedman's Combinatory Categorial Grammars (CCGs), which had already been shown to be successful in both natural language parsing and generation, have been shown to be effective for both directing probabilistic plan recognition and planning.  This gives us good evidence for seeing CCGs as a unifying representation.  However, this success immediately raises the question where do such grammars come from, and can they be learned?  The main goal of this project is to begin to answer this question.<br/><br/>Natural language researchers have proposed a number of methods for learning CCGs for natural languages. However, none of these methods have been applied to the learning of grammars for plans.  Therefore, this project is the first exploration of these algorithms for learning grammars to direct plan recognition and planning.  Specifically, the focus of this work will be to modify a recently published algorithm for learning natural language CCGs to learn plan grammars.  We will use input datasets generated from the publicly available International Planning Competition domains and attempt to learn grammars to both recognize future instance of plans and to direct the construction of future plan instances.  We will evaluate the effectiveness of the learned plan CCGs using traditional metrics from language grammar learning.  In addition, since these CCGs are intended specifically to direct planning and plan recognition, we will measure the effectiveness of the resulting CCG at these problems using the traditional metrics from these research areas.  For planning this includes runtime and length of plan, and for plan recognition accuracy, runtime, and mean time to recognition."
"1554626","CAREER: Smart Protection for DC Power Systems: Distributed and Proactive Approach","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","03/01/2016","06/15/2020","Jae-Do Park","CO","University of Colorado at Denver","Standard Grant","Anil Pahwa","02/28/2021","$515,000.00","","jaedo.park@ucdenver.edu","MS F428, AMC Bldg 500","Aurora","CO","800452570","3037240090","ENG","7607","1045, 155E, 9102, 9251","$0.00","The goal of this CAREER project is to provide intelligent solutions for the protection of DC power systems, which is one of the major barriers that prevent the wide usage of DC power systems. DC power systems have received renewed attention due to a number of advantages they offer such as easy renewable energy resource integration, high-efficiency long distance transmission, and simpler interface with power electronics converters. Applications including microgrids with distributed generators, bulk power transmission, and low-voltage distribution systems have been investigated. Compared to the control aspects of DC systems that have made sizable progress, system protection has always been a challenge. The research in this project will address protection issues for DC power systems, which will contribute to strengthen this important aspect of DC power systems and can have impact in various applications. The outcomes of this research will be disseminated through presentations and publications and can be readily utilized in conjunction with other aspects of distributed and/or sustainable energy applications using DC power to generate synergistic effects on overall system reliability improvement. Furthermore, this CAREER project will academically and financially support graduate students and nurture them to prepare them as part of the next generation of multidisciplinary researchers and engineers. The project will also provide ample research opportunities to undergraduate students.<br/><br/>While the advantages of DC power systems are great, the protection aspects of these systems have posed many challenges such as effectively breaking a DC arc, autonomously locating a fault within a microgrid, DC protective equipment, and certainly the lack of standards, guidelines and experience. Although protection technology for AC systems has a long history and maturity, it is difficult to directly apply these to DC systems because of the non-alternating nature and fast dynamics of DC power. Hence, the protection of DC power systems needs to take a fundamentally different approach than that of existing AC technologies. Furthermore, complex modern power systems need more intelligence for their operation. In this CAREER project, a distributed and proactive approach will be taken as the project aims to provide a protection framework with swift fault current interruption, accurate fault location, and distributed decision-making based on various system parameters. The protection framework development will contain integrated components for fast responses and compound decisions based on multi-dimensional intelligence. The successful execution of this CAREER project will contribute to the implementation of modern power systems that take advantage of DC power. The activities will result in multidisciplinary research as well as educational impacts, and will provide interesting and useful applications of advanced artificial intelligence, control and communication technologies to the power and energy system area."
"1611894","Twenty-First AAAI/SIGAI Doctoral Consortium","IIS","Robust Intelligence","01/01/2016","12/16/2015","David Roberts","CA","Association for the Advancement of Artificial Intelligence","Standard Grant","Weng-keen Wong","06/30/2016","$17,610.00","","robertsd@csc.ncsu.edu","2275 E BAYSHORE RD STE 160","East Palo Alto","CA","943033224","6503283123","CSE","7495","7495, 7556","$0.00","The AAAI Doctoral Consortium (DC) brings together established researchers and mid-career Ph.D. students for a two-day workshop where students and faculty interact in a range of settings to provide both formal and informal career and research mentoring. As a means of encouraging young and upcoming researchers in Artificial Intelligence, the AAAI Doctoral Consortium has been proven to be a relatively inexpensive and extremely effective model. Many of the current reviewers, mentors, organizers, and panelists were once student participants in prior AAAI DCs.<br/><br/>The Intellectual Merit of the AAAI Doctoral Consortium lies in the unique opportunity for junior researchers to gain high quality feedback on academic and social issues relevant to their career, from senior researchers within their community. As part of the DC program, mentors and students interact in meetings, panels, working lunches, and during oral presentations and discussions. For many students, this is the only opportunity to receive focused input from researchers other than members of their dissertation committee at their home institution. Students not only gain the experience of giving a talk on their thesis research and receiving specific feedback on their work, they also are given the opportunity to discuss more personal issues, such as balancing work and family when  pursuing a research career after completion of their Ph.D. studies. <br/>"
"1565979","CRII: CPS: Towards an Intelligent Low-Altitude UAS Traffic Management System","CNS","CRII CISE Research Initiation","05/15/2016","07/26/2016","Peng Wei","IA","Iowa State University","Standard Grant","David Corman","04/30/2019","$174,998.00","","pwei@gwu.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","026Y","8228, 9150","$0.00","The objective of this project is to provide theoretical foundations for cyber-physical systems to support the increasing autonomy in the presence of other manned/unmanned air traffic. Currently the United States air transportation is facing significant challenges due to the rapid evolution of increasingly autonomous systems such as unmanned aircraft systems (UAS) and their expanding presence. However, there has been little scientific investigation on the cyber-physical systems that supports unmanned aircraft operations operating in the presence of other air vehicles. This research project explores novel strategies of coordinating and managing the UAS traffic to ensure low-altitude airspace safety and efficiency in near future. This will also help to catalyze additional cyber-physical systems research in related areas including aviation infrastructures, navigation and surveillance devices, and unmanned aerial vehicle technologies. The investigator will work with Iowa State University Extension and Outreach Office to promote UAS applications in precision agriculture, aviation safety and responsible uses of UAS.<br/><br/>The theoretical aspect of the research will leverage interdisciplinary methodologies in the fields of optimization, artificial intelligence, and control. The project designs algorithms, implements software and demonstrates proof-of-concept using low-altitude UAS traffic management as a challenge application area. This project establishes an unmanned air system traffic simulation platform, which can be used to validate new methods and tools, and enable collaborations with government, industrial and academic partners. The research outcomes are expected to provide a framework to investigate the feasibility, safety, efficiency and potential benefits of the increasing autonomy in civil aviation."
"1627861","Doctoral Dissertation Research:   Designing Voice Analysis Technologies for Mental Health Applications in the United States","BCS","DDRI Cult Anthro","08/01/2016","08/01/2016","Graham Jones","MA","Massachusetts Institute of Technology","Standard Grant","Jeffrey Mantz","01/31/2018","$28,796.00","Beth Semel","gmj@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","SBE","7605","1390, 9179","$0.00","This project, which trains a graduate student in methods of conducting empirically-grounded scientific research, asks how new, artificial intelligence (AI)-enabled technologies reshape not only the future of mental health care in the United States, but also basic assumptions about the relationship between language, mind, and brain. This research explores these questions through an ethnographic study of interdisciplinary research teams at three U.S. universities that are seeking to develop computer-assisted speech analysis technologies for mental health applications. In the research teams that this study focuses on, neuroscientists, psychiatrists, psychologists and engineers are working together to develop technology that can be used to diagnose and track mental illness by analyzing the formal, acoustic properties of speech (such as pitch, timbre, intonation, and speed), bypassing its semantic content (what the words mean) altogether. The project will have implications for the mental health researchers themselves as they move into uncharted ethical domains in regards to privacy, surveillance, and the increased diagnostic reliance on experimental technologies.<br/><br/>Beth Semel, under the supervision of Dr. Graham Jones at the Massachusetts Institute of Technology, explores how experts across disciplinary boundaries collaborate to design, develop and test artificial intelligence-enabled technologies when they appear to hold very different assumptions about the relationship between language and inner, psychological states. The researcher hypothesizes that collaborations surrounding the development of these technologies not only enact fundamental tensions within dominant views about how language works, but also reflect a re-working of claims of authority and expertise within U.S. mental health care. Increasingly, mental health researchers are eschewing traditional techniques of psychiatric diagnosis, which depend upon patients' subjective, verbal accounts of their psychological states and clinicians' observational and interpretive skills. Instead, they are enlisting the expertise of computer engineers who use AI techniques of pattern recognition to decipher the biomedical significance of behavioral symptoms. Using ethnographic participant observation, the researcher will collect data about how psychiatrists, psychologists, neuroscientists and engineers work together to design, test, and develop voice analysis technologies. Focusing on teams situated at the confluence of academic, commercial, and military arenas, this study explores the variety of ways in which mental illness is conceptualized in terms of scientific, public health, and national security concerns. By exploring how listening practices can shape assumptions about speech, and how the production of new listening techniques and technologies can reshape such assumptions, this research contributes to ongoing debates in linguistic anthropology about how culture affects understandings of the way language works."
"1619344","RI: Small: Harnessing the Power of Constraint Propagation by Controlling Consistency Levels and Synthesizing Constraints","IIS","Robust Intelligence","07/01/2016","04/13/2018","Berthe Choueiry","NE","University of Nebraska-Lincoln","Standard Grant","James Donlon","06/30/2020","$486,000.00","","choueiry@cse.unl.edu","151 Prem S. Paul Research Center","Lincoln","NE","685031435","4024723171","CSE","7495","7495, 7923, 9150, 9251","$0.00","Decision and optimization problems raise many challenges in daily life as well as in business, engineering, and science.  Solving them ""well"" and ""quickly"" directly benefits our life-styles and economy, and allows us to effectively manage our resources.   Many scientific disciplines have focused on formalizing and solving such difficult combinatorial problems.   Constraint Processing (CP), a subfield of Artificial Intelligence, has focused on the transparent modeling of the restrictions (i.e., constraints) that govern those problems and create their complexity.  By examining how constraints interact among themselves and how interactions propagate along the constraints, CP has developed generic mechanisms to solve a problem by ruling out forbidden decisions and combinations of decisions, gradually eliciting the acceptable solutions and the optimal choices.  To this end, CP has formalized in terms of consistency properties how constraints interact among themselves and with the possible options.   Thus, the identification of such properties and the design of algorithms for enforcing them are at the heart of CP, and constitute perhaps what best distinguishes this area from other scientific disciplines concerned with the same computational problems.  The goal of this project is to understand the tradeoffs between the effectiveness and the cost of a variety of consistency algorithms, and to control how to interleave them dynamically during problem solving to solve difficult problems and reduce computational cost.<br/><br/>In particular, the research will garner the most computational benefits from consistency-based techniques by dynamically (a) selecting the right level of consistency to enforce at any point during search, and (b) synthesizing new constraints over opportunistically chosen variables.  The proposed activities extend on recent practical algorithms for higher-levels consistency, and contribute to the progress of the research in fundamental aspects of CP.  The developed methods will benefit other types of graphical models such as games as well as the next generation of commercial and public-domain constraint solvers. The insight gained from this research will improve the scope and content of the introductory and advanced courses on CP both at the undergraduate and graduate levels.   The various research tasks will allow the mentoring and training of young engineers and scientists to understand the roots of complexity in problem solving and to learn how to overcome it in practice."
"1548781","SBIR Phase I: A Novel Experiential Learning Toy Teaching Robotics and Automation to K-12 Students","IIP","SMALL BUSINESS PHASE I","01/01/2016","06/09/2016","Woo Ho Lee","TX","TechComb LLC","Standard Grant","Glenn H. Larsen","12/31/2016","$179,999.00","","wooholee@icloud.com","8220 Snow Goose Way","Fort Worth","TX","761182004","9728416758","ENG","5371","079E, 5371, 8031, 8032, 8039","$0.00","This SBIR Phase I project is to raise awareness and enthusiasm for Science, Technology, Engineering, and Mathematics (STEM) education among young children through experiential learning enabled by novel and interactive robotic toys. U.S. Department of Labor's recent survey reports that only 5% of U.S. workers are employed in science and engineering fields, yet they are responsible for more than 50% of the sustained economic expansion. Over the past few decades the U.S. contribution to the number of scientists and engineers worldwide has reduced from 40% to 15%. Interestingly, there is a high interest among young students for the STEM fields but approximately 40% of them end up switching to non-STEM careers by the time they graduate from college - according to the President's Council of Advisors on Science and Technology. This project brings a radical change in this trend by aptly stimulating the curiosity and challenging the intelligence, the key catalysts for continued interest, in K-12 students. Through innovative and intelligent hardware and software modules, this project delivers an enhanced learning experience on next generation robotics and automation technologies in the classroom as well as at home, leading towards a much richer and secure prospect for the future tax payers.<br/><br/>This SBIR Phase I project develops an interactive and reconfigurable robotics-learning system comprised of smart positioning modules and interconnects. The key innovations include: a zero nuts-and-bolts architecture for quick and easy system setup, distributed artificial intelligence for interactive plug-n-play operations, and personalized gaming and learning feature to engage different age groups, experience levels, and study areas. This gaming and learning system aids in understanding various concepts of new age manufacturing, shaped by rapidly evolving economic constraints, including but not limited to: challenges in flexible/lean manufacturing for low cost and rapid turnaround production; effects of source agnostic automation, where robots/tools from different manufacturers are combined, on overall manufacturing metrics; and difficulties associated with robot cooperative control with limited feedback in complex operations. Such knowledge, gained through hands-on learning, better prepares the future scientists and engineers to not only adapt in the highly demanding manufacturing environment of the future but also explore innovative solutions to complex technical problems. This project envisions to significantly surpass the learning experience offered by contemporary educational toys, including passive building sets, jigsaw puzzles, rule-based interactive toys and so on, by delivering a highly immersed, actively encouraging, and truly exploratory experience for the makers of tomorrow."
"1551866","CompCog: The edge of the lexicon: Productive knowledge and direct experience in the acquisition and processing of multiword expressions","BCS","Linguistics, Perception, Action & Cognition, Robust Intelligence","08/15/2016","08/22/2016","Roger Levy","MA","Massachusetts Institute of Technology","Standard Grant","Tyler Kendall","01/31/2021","$329,233.00","","rplevy@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","SBE","1311, 7252, 7495","1311, 7252, 7495, 9179","$0.00","Language is the most discrete, measurable cultural record of the human mind, and is uniquely expressive among the communicative systems found in nature. Every day we comprehend hundreds of sentences that we hear or read but have never encountered before, and we produce hundreds more. Yet our success at these many acts of communication belies the difficulty of the task: language is rife with ambiguity, our attention is limited, our environments may be noisy, and we often have incomplete information about the shared knowledge and beliefs of the people we engage with. This ability, unique to our species, poses profound challenges for our scientific understanding of the capabilities of the human mind.  Deepening our understanding of these capabilities requires a combination of ideas and methods from linguistics, psychology, and computer science. Advances in this area help lay the groundwork for improvements in natural language technologies such as document summarization, paraphrasing, question answering, and machine translation, and in better identification, diagnosis, and treatment of language disorders.<br/><br/>Within this broader research enterprise, this project focuses on the ""edge of the lexicon"", elucidating the conditions under which a linguistic expression begins to get stored in the mind of the native speaker who uses it, and the consequences of the expression being stored as a holistic unit. Native speakers know both productive rules that license and allow interpretation of phrases and sentences that they have never before encountered and a rich inventory of lexical items that can be combined through these productive rules. Many of these lexical items are individual words, but there is evidence that specific, frequent multi-word expressions, such as ""meat and potatoes"" or ""large majority"" may also get stored in the lexicon. This project combines artificial intelligence-based computational models, large linguistic datasets, and controlled psychological experimentation to explore the edge of the lexicon, probing how direct experience with specific multi-word expressions leads to their being stored in one's mental lexicon, how such storage is reconciled with productive knowledge in language comprehension and production, and how these expressions emerge and change over time."
"1648897","A Conference on Humans, Machines and the Future of Work","IIS","INFORMATION TECHNOLOGY RESEARC, ROBUST INTELLIGENCE","11/01/2016","07/21/2016","Moshe Vardi","TX","William Marsh Rice University","Standard Grant","James Donlon","10/31/2017","$25,000.00","","vardi@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","CSE","1640, 7495","7495, 7556","$0.00","This project supports a conference on ""Humans, Machines, and the Future of Work"" to be held at Rice University, in Houston, TX. The conference will focus on issues created by the impact of information technology on labor markets over the next 25 years, addressing questions such as: What advances in artificial intelligence, robotics and automation are expected over the next 25 years? What will be the impact of these advances on job creation, job destruction, and wages in the labor market? What skills are required for the job market of the future? How can education prepare workers for that job market? What educational changes are needed? What economic and social policies are required to integrate people whose skills do not match the needs of future labor markets? How can social mobility in such an environment be preserved and increased? The conference will feature 16 renowned speakers and panelists from academia, industry and leading think tanks with expertise in technology, economics, social sciences, and the humanities. The goal of the conference is to start a conversation between many academic disciplines on the future of work in order to make this topic a subject of ongoing academic inquiry, as well as a subject of public policy discussion.<br/><br/>The current understanding of the Information Technology Revolution is somewhat similar to the 1970s' understanding of global warming. Facts are known with some level of certainty.  Computers are eliminating some jobs involving structured tasks in manufacturing, clerical work, and some other mid-skill occupations.  At the same time,  computers are creating new jobs in many other occupations, particularly for technically skilled people. Beyond these facts lies a broad landscape of speculation.  Not much is known about the Information Technology Revolution's net effect on employment and wages. Equally important, not much is known about the speed at which the revolution is proceeding.  To the extent this uncertainty can be reduced, it will require a joint effort by computer scientists, economists, sociologists, psychologists, and others. Addressing these issues is an important national challenge."
"1611742","Teaching Critical Thinking Skills in Science with sInvestigator","DUE","IUSE","10/01/2016","10/06/2016","Gheorghe Tecuci","VA","George Mason University","Standard Grant","R.  Hovis","09/30/2020","$300,000.00","James Trefil, Dorin Marcu, Mihai Boicu, Nancy Holincheck","tecuci@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","EHR","1998","8209, 8244, 9178","$0.00","A team of experts from computer science, artificial intelligence, science teaching and pedagogy, systems architecture, software engineering, knowledge engineering and human computer interaction at George Mason University (GMU) is developing a cognitive assistance tool, ""sInvestigator,"" to help students acquire critical thinking skills in addressing scientific problems. The sInvestigator cognitive assistant is a significant technological innovation, based upon a computational theory of reasoning in science. It incorporates a substantial amount of general knowledge about scientific reasoning with evidence guiding and helping the student with the scientific inquiry process. The tool is novel and operates on multiple computer platforms. It is designed for students to use at home and in the lab and supports their acquisition of the core competency of evidence-based reasoning. The resulting theory is extendable to all STEM disciplines both undergraduate and K-12. Broadening participation is achieved through workshops having the capacity of 50-70 participants. Recruiting targets community colleges and minority serving institutions to attract women and underrepresented groups to the workshops organized in collaboration with the GMU Center for Teaching and Faculty Excellence. Evidence based reasoning is at the core of problem solving and decision making not only in STEM disciplines but also law, intelligence analysis, forensics, medicine, history, archeology and other domains. The developed educational materials together with sInvestigator exercises are widely distributed. Information on how teachers and researchers can freely obtain the sInvestigator are posted on GMU's Learning Agents Center website.<br/><br/>The three-year project is first piloted in two Honors courses enrolling 30-40 students. Working in teams, students are guided to approach a scientific problem as ceaseless discovery of evidence, hypothesis and arguments. New knowledge is added by answering research questions that explore improvements in student perception of science process skills and gains in student content knowledge, as measured by course assessments. Students experience numerous opportunities to exercise imagination and creativity and acquire critical scientific practices, particularly: (1) Asking questions; (2) Constructing explanations; (3) Engaging in argument from evidence; and (4) Obtaining, evaluating and communicating explanations (NRC, 2012 p.3). The use of sInvestigator is explored in a sequence of courses to learn what works and what does not work and incrementally evolve the theory and the approach while developing and testing case studies. The mixed-methods evaluation is mostly formative with a focus on understanding students' collaborative experiences with sInvestigator through surveys and interviews. Modification of assessment tools is made based upon evaluation which allows for analysis of student intrinsic motivation, career motivation, self-determination, self-efficacy and grade motivation. In the final year a quasi-experimental research design will be used to evaluate the impact of sInvestigator in a general science course with 250-300 students in two course sections. The design allows for a comparison group on student achievement scores using course assessments, a Science Motivation Questionnaire and a Science Process skills inventory."
"1623094","EXP: Collaborative Research: Extracting Salient Scenarios from Interaction Logs (ESSIL)","IIS","Cyberlearn & Future Learn Tech","09/01/2016","06/24/2020","Leilah Lyons","NY","New York Hall of Science","Standard Grant","Amy Baylor","06/30/2021","$55,000.00","","llyons@nyscience.org","47-01 111TH STREET","Corona","NY","113682950","7185959173","CSE","8020","8045, 8841","$0.00","The Extracting Salient Scenarios from Interaction Logs (ESSIL) project proposes to develop a new type of educational technology to support students' learning about complex systems from their participation in a multi-person immersive simulation.  Many important challenges we face today as a society -- including responding to climate change, managing global economies, city planning, disease outbreaks -- are ""complex systems"" problems, meaning that important phenomena in each (for instance trends in weather, stock bubbles, traffic jams, disease transmission) result not from a single cause, but because many small causes combine together. Participating in a simulation has the potential to help students understand the principles of complex systems, but because different principles surface depending on how each simulation unfolds, it can be difficult for teachers to adjust their lesson plans on the fly to highlight the principles that emerge in a given simulation run. To address this challenge, ESSIL will develop methods to create ""automatic salient recaps,"" as a way to help learners and their teachers make better sense of simulations. These recaps, which will be automatically generated, provide a story of ""what happened"" in the simulation in a way that both helps students remember their experience and reveals important scientific principles. Teachers and other facilitators will use these recaps, along with an accompanying discussion guide, to support productive learning conversations about the scientific principles incorporated in a simulation. The recaps will be developed for a large-scale immersive simulation installed at the New York Hall of Science (NYSCI), potentially improving the educational experience of thousands of daily visitors.  The capabilities developed to produce them have widespread applicability, because logs of student interactions are routinely produced by many educational systems.  The project is supported by the Cyberlearning and Future Learning Technologies Program, which funds efforts that will help envision the next generation of learning technologies and advance what we know about how people learn in technology-rich environments. Cyberlearning Exploration (EXP) Projects explore the viability of new kinds of learning technologies by designing and building new kinds of learning technologies and studying their possibilities for fostering learning and challenges to using them effectively. <br/> <br/>The immersive simulation context for the project is Connected Worlds, an embodied, multi-person ecology simulation at NYSCI, with pedagogical goals around sustainability and systems thinking. Using logs from groups of students interacting with Connected Worlds, ESSIL will construct selective recaps of their experience that both are personally salient to them (by including memorable details of their experience) and have explanatory coherence (to enable their discussion of important interconnections in the simulation's underlying model). Artificial Intelligence-based methods will be developed to 1) identify salient changes in the state of the simulation during  student interaction and 2) construct qualitative models of causal chains that could have led to these changes. These qualitative models will be used to generate salient recaps and discussion guides based on them, which will be provided to teachers whose classes are visiting NYSCI. The effectiveness of the innovation will be investigated by comparing visiting students' conversations with and without ESSIL-generated discussion supports and by interrogating their resulting models of the Connected Worlds system through concept maps."
"1622831","Doctoral Consortium for the 2016 Learning Analytics and Knowledge Conference","IIS","Cyberlearn & Future Learn Tech","06/01/2016","06/03/2016","Stephanie Teasley","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Tatiana Korelsky","05/31/2018","$24,930.00","Bodong Chen","steasley@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","8020","7556, 8045","$0.00","Using data analytics has revolutionized many academic disciplines, such as Astrophysics and Biology. In addition, it has changed the commercial world and critical services like healthcare. The field of education research is also beginning this change. The Society for Learning Analytics Research (SoLAR) is a key player in effecting this change. This project will support a Doctoral Consortium at the SoLAR 2016 conference - the 6th Annual International Learning Analytics and Knowledge (LAK) Conference, to be held in Edinborough, UK in 2016. Participating in the consortium will prepare current doctoral students from the diverse research backgrounds that make up the interdisciplinary field of Learning Analytics (LA), including computer science, information science, psychology, communication, education, artificial intelligence, and cognitive science. It will also allow them to network with each other and with professors and practitioners who are currently engaged in learning analytics research and related work to ensure the continued development of this community. These activities are critical and timely. The upcoming generation of LA researchers will play a vital role in realizing the potential of using data to improve outcomes for elementary, secondary, and post-secondary students in learning, motivation, and perseverance.<br/><br/>Learning Analytics is an interdisciplinary field whose goal is to advance and apply knowledge about learning sciences and education to improve all aspects of learning. LA methods include data visualization, data mining, data science, and mixed methods approaches combining qualitative and quantitative methods (e.g., interviews and back-end, clickstream data analysis). Capacity building is a central concern within the LA community. SoLAR has historically addressed these needs, in part, through specialized workshops held in conjunction with the Society's major conference. The Doctoral Consortium workshops host PhD students who are grappling with their dissertation research. This grant provides travel support to US scholars selected through a competitive application process to participate in this event. They present their work for feedback both in the context of workshop, where they receive advice from a panel of expert mentors, and in a poster session in which they interact with the full conference audience. In addition, their work is published in the proceedings of the conference."
"1623124","EXP: Collaborative Research: Extracting Salient Scenarios from Interaction Logs (ESSIL)","IIS","Cyberlearn & Future Learn Tech","09/01/2016","08/26/2016","Barbara Grosz","MA","Harvard University","Standard Grant","Amy Baylor","02/29/2020","$170,000.00","","grosz@eecs.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","8020","8045, 8841","$0.00","The Extracting Salient Scenarios from Interaction Logs (ESSIL) project proposes to develop a new type of educational technology to support students' learning about complex systems from their participation in a multi-person immersive simulation.  Many important challenges we face today as a society -- including responding to climate change, managing global economies, city planning, disease outbreaks -- are ""complex systems"" problems, meaning that important phenomena in each (for instance trends in weather, stock bubbles, traffic jams, disease transmission) result not from a single cause, but because many small causes combine together. Participating in a simulation has the potential to help students understand the principles of complex systems, but because different principles surface depending on how each simulation unfolds, it can be difficult for teachers to adjust their lesson plans on the fly to highlight the principles that emerge in a given simulation run. To address this challenge, ESSIL will develop methods to create ""automatic salient recaps,"" as a way to help learners and their teachers make better sense of simulations. These recaps, which will be automatically generated, provide a story of ""what happened"" in the simulation in a way that both helps students remember their experience and reveals important scientific principles. Teachers and other facilitators will use these recaps, along with an accompanying discussion guide, to support productive learning conversations about the scientific principles incorporated in a simulation. The recaps will be developed for a large-scale immersive simulation installed at the New York Hall of Science (NYSCI), potentially improving the educational experience of thousands of daily visitors.  The capabilities developed to produce them have widespread applicability, because logs of student interactions are routinely produced by many educational systems.  The project is supported by the Cyberlearning and Future Learning Technologies Program, which funds efforts that will help envision the next generation of learning technologies and advance what we know about how people learn in technology-rich environments. Cyberlearning Exploration (EXP) Projects explore the viability of new kinds of learning technologies by designing and building new kinds of learning technologies and studying their possibilities for fostering learning and challenges to using them effectively. <br/> <br/>The immersive simulation context for the project is Connected Worlds, an embodied, multi-person ecology simulation at NYSCI, with pedagogical goals around sustainability and systems thinking. Using logs from groups of students interacting with Connected Worlds, ESSIL will construct selective recaps of their experience that both are personally salient to them (by including memorable details of their experience) and have explanatory coherence (to enable their discussion of important interconnections in the simulation's underlying model). Artificial Intelligence-based methods will be developed to 1) identify salient changes in the state of the simulation during  student interaction and 2) construct qualitative models of causal chains that could have led to these changes. These qualitative models will be used to generate salient recaps and discussion guides based on them, which will be provided to teachers whose classes are visiting NYSCI. The effectiveness of the innovation will be investigated by comparing visiting students' conversations with and without ESSIL-generated discussion supports and by interrogating their resulting models of the Connected Worlds system through concept maps."
"1632236","WORKSHOP: The Pioneers Workshop at the 2016 ACM/IEEE International Conference on Human-Robot Interaction","IIS","HCC-Human-Centered Computing","03/01/2016","02/19/2016","Maja Mataric","CA","University of Southern California","Standard Grant","Ephraim Glinert","02/28/2017","$33,552.00","","mataric@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7367","7367, 7556","$0.00","This is funding to support a Pioneers Workshop (doctoral consortium) of approximately 31 graduate students (16 of whom are from the United States and therefore  eligible for funding) from diverse research communities (e.g., computer science and engineering, psychology, cognitive science, robotics, human factors, human-computer interaction design, and communications), along with distinguished research faculty. The event will take place on March 7, 2016, immediately preceding the 11th International Conference on Human Robot Interaction (HRI 2016), to be held March 8-10 in Christchurch, New Zealand, and which is jointly sponsored by ACM and IEEE. HRI is a single-track, highly selective annual international conference that seeks to showcase the very best inter- and multi-disciplinary research in human-robot interaction with roots in diverse fields including social psychology, cognitive science, HCI, human factors, artificial intelligence, robotics, organizational behavior, anthropology and many more, and to this end the conference invites broad participation. The theme of HRI 2016 is ""Natural Interaction"" which seeks contributions from a broad set of perspectives, including technical, design, methodological, behavioral, and theoretical, that advance fundamental and applied knowledge and methods in human-robot interaction, with the goal of enabling human-robot interaction through new technical advances, novel robot designs, new guidelines for design, and advanced methods for understanding and evaluating interaction. More information about the conference is available online at http://humanrobotinteraction.org/2016/. This workshop will afford a unique opportunity for the best of the next generation of researchers in human-robot interaction to be exposed to and discuss current and relevant topics as they are being studied in several different research communities (including but not limited to computer science and engineering, psychology, robotics, human factors and ergonomics, and HCI). This is important for the field, because it has been recognized that transformative advances in research in this fledgling area can only come through the melding of cross-disciplinary knowledge and multinational perspectives. Participants will be encouraged to create a social network both among themselves and with senior researchers at a critical stage in their professional development, to form collaborative relationships, and to generate new research questions to be addressed during the coming years. Participants will also gain leadership and service experience, as the workshop is largely student organized and student led. The PI has expressed her strong commitment to recruiting women and members from under-represented groups. To further ensure diversity the event organizers will consider an applicant's potential to offer a fresh perspective and point of view with respect to HRI, will recruit students who are just beginning their graduate degree programs in addition to students who are further along in their degrees, and will strive to limit the number of participants accepted from a particular institution to at most two.<br/><br/>The Pioneers Workshop is designed to complement the conference, by providing a forum for students and recent graduates in the field of HRI to share their current research with their peers and a panel of senior researchers in a setting that is less formal and more interactive than the main conference. During the workshop, participants will talk about the important upcoming research themes in the field, encouraging the formation of collaborative relationships across disciplines and geographic boundaries. To these ends, the workshop format will encompass a variety of activities including three keynotes, a distinguished panel session, and breakout sessions. To start the day, all workshop attendees will briefly introduce themselves and their interests. Following the opening keynote, approximately half of the participants will present 3-minute overviews of their work, leading into an interactive poster session. This will enable all participants to share their research and receive feedback from students and senior researchers in an informal setting. The workshop organizers will facilitate the post-presentation discussion and will encourage participants to ask questions of their peers during the interactive break and poster session. After lunch, the remaining workshop participants will give their 3-minute overviews, followed by presentation of their posters during a second interactive poster session. Senior researchers (in addition to those on the panel) will be invited to attend the student presentations and poster sessions in order to provide feedback to participants, and workshop participants will be invited to present their posters during the main poster session of the HRI conference as well. The conversations between the panel and participants will continue over lunch and during dinner."
"1554123","CAREER: Locally Adaptive Nonparametric Estimation for the Modern Age - New Insights, Extensions, and Inference Tools","DMS","STATISTICS, Division Co-Funding: CAREER","07/01/2016","07/21/2020","Ryan Tibshirani","PA","Carnegie-Mellon University","Continuing Grant","Gabor Szekely","06/30/2021","$400,000.00","","ryantibs@cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","MPS","1269, 8048","1045","$0.00","Nonparametric modeling---which means, roughly, flexible modeling of smooth trends without specific assumptions about their form or shape---finds diverse applications in many areas such as epidemiology, astrophysics, finance, and artificial intelligence. It is also a field ripe for modern statistical development, since nonparametric models are in a sense even more appealing in the ""big data"" era, as it is precisely in data-rich settings that the increased flexibility of these models will begin to show real rewards in terms of statistical accuracy. The proposed work will develop nonparametric methods (and affiliated software) that will be useful to data scientists who model smooth, nonlinear trends in areas like those mentioned above, as well as many others. A specific scientific emphasis will be the forecasting of influenza and dengue fever. Such forecasts will help policy makers design and implement more effective countermeasures towards these diseases. The proposal puts forward two main ideas for educational training, closely related to the research aims to be pursued. The first is a set of short videos on nonparametric smoothing, intended as supplements to an undergraduate level course called Advanced Methods for Data Analysis. They will be integrated with an interactive quiz system, and will be made freely available (on YouTube) so that others outside the class may watch too. The second idea is a statistical computation training group, for PhD students from Statistics and Computer Science.<br/><br/>""Locally adaptive"" nonparametric methods offer more fine-grained flexibility than traditional nonparametric methods, in that they can simultaneously represent different amounts of smoothness at different parts of the function domain. Currently, locally adaptive nonparametric methods are not often used in big, modern data sets, likely because of their computational inefficiency, and the general inavailability of locally adaptive methods in many modern problem settings. The proposed work seeks to change this, and to push the state of the art in modern locally adaptive nonparametric estimation. The research aims are to: deepen the theoretical understanding of existing locally adaptive methods for univariate problems; efficiently scale these methods and extend these theories to problems where data are collected in high dimensions and over graphs; and develop inferential tools for all of these locally adaptive procedures. The specific contributions will be balanced between the theoretical (statistical theories that describe the underpinnings of the methods in question) and computational (practical algorithms that describe implementation of these methods at scale) perspectives. A final more applied research aim is to use the proposed methods to improve and extend a forecasting system for major epidemics such as influenza and dengue fever."
"1554783","CAREER: Phase Transitions in Some Discrete Random Models and Mixing of Markov Chains","DMS","PROBABILITY, Division Co-Funding: CAREER, EPSCoR Co-Funding","07/01/2016","09/04/2019","Nayantara Bhatnagar","DE","University of Delaware","Continuing Grant","Tomek Bartoszynski","06/30/2021","$327,624.00","","banerjee.naya@gmail.com","210 Hullihen Hall","Newark","DE","197160099","3028312136","MPS","1263, 8048, 9150","1045, 9150","$0.00","Water turning into ice at its freezing point or the magnetization of iron are examples of phase transitions in physical systems. At the transition point, the properties of the system such as the volume or heat capacity may change discontinuously. The aim of our research is to study phase transitions in mathematical models using probabilistic tools in the following three directions. (1) The longest increasing subsequence (LIS) of a permutation is the length of a maximal subsequence of the permutation in which the elements increase. How long is the LIS for a uniformly random permutation? This question has been studied in connection with practical applications such as sorting sequences, disk drive scheduling and airplane boarding times. The mathematical study of the LIS has revealed deep and unexpected connections of the problem with areas such as the theory of random matrices, analytic combinatorics and random polymer models.  The proposed research aims to study the LIS when the permutation is drawn from certain non-uniform distributions and associated phase transitions. (2) Many computational problems can be phrased as constraint satisfaction problems (CSPs) where one wants to find a solution to a number of variables with a set of constraints imposed on them. CSPs were first studied in computer science motivated by applications to artificial intelligence. To study the difficulty of finding solutions in typical rather than worst case scenarios, researchers study random CSPs. Using sophisticated heuristics, physicists have made detailed predictions about the location and nature of phase transitions in random CSPs. The accuracy of these heuristic predictions motivates the importance of discovering the rigorous mathematical foundations of these techniques. (3) Interacting particle processes are used to model large, randomly evolving interacting systems of agents that arise in the natural sciences including in physics and in biology. The exclusion and interchange random walks are examples of such interacting particle processes. In the symmetric case the long term mixing behavior of the random walk and the nature of phase transitions is well studied. The goal of this research is to understand the mixing properties of natural asymmetric and weighted versions of these processes. While achieving these three goals, the principal investigator will create exciting research opportunities for graduate and undergraduate students in probability, mentoring programs with the goal of retention of women in mathematics, and the development of online curricular material.<br/><br/>The main aim of this project is to develop new theory and analysis for phase transitions in certain discrete probabilistic models. The first problem is to study the limiting distribution of the LIS in non-uniformly random permutations by way of analyzing the fluctuations of the LIS as the parameter of the distribution is varied. The distribution is known to be Gaussian in one regime of the parameter and Tracy-Widom in another and we aim to study this transition. The second problem is to study the condensation and clustering transitions in random CSPs such as the hardcore model on random graphs. The research aims to identify the location of the reconstruction threshold more precisely in these models and to explore the connection to the clustering transition. Finally, the proposal will consider Markov processes such as asymmetric exclusion and interchange and attempt to relate the mixing times and spectral gaps of these processes to the corresponding quantities for a single particle and to understand the cutoff phenomenon for these processes."
"1561655","Collaborative Research: Big Data from Small Groups: Learning Analytics and Adaptive Support in Game-based Collaborative Learning","DRL","ECR-EHR Core Research","10/01/2016","09/17/2018","James Lester","NC","North Carolina State University","Continuing Grant","Gregg Solomon","09/30/2021","$991,511.00","","lester@csc.ncsu.edu","2701 Sullivan DR STE 240","Raleigh","NC","276950001","9195152444","EHR","7980","8244","$0.00","This is a research project supporting a new model of Computer Supported Collaborative Learning (CSCL) that combines the advantages of game based learning with problem based learning. Good game based learning environments combine rich scenarios with engaging activities to serendipitously provide student learning. These learning environments also provide an opportunity for players to collaborate in reaching their game goals. Good problem based learning environments provide support for the solution of complex and ill-structured problems. The combination of these two types of learning environments promise to provide the engagement and richness of game based learning with the support environment to engage students in authentic science. Both of these environments are computer based so the actions and interactions of the students and teachers are captured for analysis. Applying learning analytics to the captured data provides information on student learning for the teacher, provides learning information to the student for self-reflection and improved learning, and provides information for the system designer to improve the effectiveness of the new CSCL environment.<br/><br/>The scientific problem domain is environmental science for middle school students. The CSCL environment is a  game based learning environment that incorporates problem based learning. The interaction between the CSCL environment and the student is enhanced by the collection of data on the student based on cognitive, affective, and metacognitive states that are inferred using artificial intelligence technologies. Specific strategies are employed to help students construct explanagions, reason effectively, and become self-directed learners. Key outcomes of the project include a model of collaborative scaffolding for game based learning that is usable in classrooms to help students learn STEM content and learning analytics designed to support the teacher in the roles of guide and collaborator. A goal of the project is wide dissemination of the CSCL system."
"1561486","Collaborative Research:  Big Data from Small Groups: Learning Analytics and Adaptive Support in Game-based Collaborative Learning","DRL","ECR-EHR Core Research","10/01/2016","09/18/2018","Cindy Hmelo-Silver","IN","Indiana University","Continuing Grant","Gregg Solomon","09/30/2021","$981,552.00","Krista Glazewski","chmelosi@indiana.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","EHR","7980","8244","$0.00","This is a research project supporting a new model of Computer Supported Collaborative Learning (CSCL) that combines the advantages of game based learning with problem based learning. Good game based learning environments combine rich scenarios with engaging activities to serendipitously provide student learning. These learning environments also provide an opportunity for players to collaborate in reaching their game goals. Good problem based learning environments provide support for the solution of complex and ill-structured problems. The combination of these two types of learning environments promises to provide the engagement and richness of game based learning with the support environment to engage students in authentic science. Both of these environments are computer based so the actions and interactions of the students and teachers are captured for analysis. Applying learning analytics to the captured data provides information on student learning for the teacher, provides learning information to the student for self-reflection and improved learning, and provides information for the system designer to improve the effectiveness of the new CSCL environment.<br/><br/>The scientific problem domain is environmental science for middle school students. The CSCL environment is a  game based learning environment that incorporates problem based learning. The interaction between the CSCL environment and the student is enhanced by the collection of data on the student based on cognitive, affective, and metacognitive states that are inferred using artificial intelligence technologies. Specific strategies are employed to help students construct explanagions, reason effectively, and become self-directed learners. Key outcomes of the project include a model of collaborative scaffolding for game based learning that is usable in classrooms to help students learn STEM content and learning analytics designed to support the teacher in the roles of guide and collaborator. A goal of the project is wide dissemination of the CSCL system."
"1627906","Bounded Cognition in Decision-Making","SES","ECONOMICS","07/01/2016","07/07/2016","Jawwad Noor","MA","Trustees of Boston University","Standard Grant","Kwabena Gyimah-Brempong","06/30/2018","$219,723.00","","jnoor@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","SBE","1320","","$0.00","At the heart of decision-making lie cognitive processes through which an understanding of the decision context is acquired and decision-relevant evaluations made. These processes can be cognitively costly and in particular may be bounded by limited cognitive resources. The PIs propose to analyze the behavior that might arise from such bounded cognition and to subsequently construct choice models. This would provide a path for possible applications of cognitive science in economics. Cognitive science is a multi-disciplinary field spanning psychology, neuroscience, artificial intelligence, linguistics, etc. and the proposal is a step in the direction of exploring the richness of its possible connections with economics.<br/><br/><br/>    As a first step, the PIs propose to study an agent in an intertemporal choice context who evaluates any given consumption stream by drawing on limited cognitive resources to think about the consequences of the stream for his future selves. They posit that the optimality of the use of cognitive resources should be reflected behaviorally in greater patience when dealing with larger stakes. Moreover, the existence of capacity constraints should lead to non-separabilities in the evaluation of consumption streams. They will proceed to find a utility representation for such behaviors that is interpretable in terms of bounded cognition. The project makes use of the approach of axiomatic decision theory."
"1549522","STTR Phase I:  Independent Science Learning through Serious Games with Expert Avatars and Complementary Stories","IIP","STTR PHASE I","01/01/2016","12/10/2016","Peter Solomon","CT","THEBEAMER LLC","Standard Grant","Ben Schrag","06/30/2017","$269,946.00","Sonny Kirkley, Judy Vesel","prsolomon@comcast.net","87 Church St","East Hartford","CT","061083720","8602125071","ENG","1505","079E, 1505, 163E, 8031, 8032, 8039","$0.00","This STTR Phase I project  will develop and test a learning platform to facilitate independent, personal and enjoyable science education. The platform combines an engaging book, whose characters employ a fictional virtual environment to solve a mystery, with a complementary computer-based virtual environment to support games, explorations, and interviews with Expert Avatars (XAs), such as Albert Einstein and Henrietta Leavitt, that appear in the book. The goal is to increase the interest in science for children in grades 5-8, where there is a shortage of science teachers, and where educational tools must compete with the high level of technology to which young people are attracted, like games with talking avatars and virtual personal assistants to answer their questions. Increasing students' interest in science careers can help fill the projected worker shortage for high tech industries and provide desperately needed science teachers. Filling these jobs is expected to increase America's GDP (and tax revenues) and high tech exports. This project will develop the initial offering in a projected series to facilitate independent learning of science, mathematics, engineering, history, the arts and other subjects through interesting stories and exciting virtual environments populated with XAs for the leaders in their fields.<br/><br/>The project will build on the successful application in education of computer-based virtual environments and games by adding two important features. First, the digital content will be complemented by the book, whose characters use a fictional virtual environment to solve the same STARDUST MYSTERY posed in the game. What is STARDUST? Where, when and how did it form? How did it get into George Washington and from him to you? The book provides an introduction, user's guide, scaffolding and sales channel for the game. Second, the virtual environment and game, developed with Unity 3-D, will be populated by XAs for some of the great minds in science. The game will support virtual explorations, such as a trip back through time to the Big Bang and virtual visits with the XAs. The XA's will be backed by an artificial intelligence (AI) system to support a conversation where students' can get answers to questions about the XA's contributions, their lives and their period in history. Responses will be generated using customized knowledge bases for each XA and a hybrid, cloud-based AI system.  The project will include a formative evaluation of the game combined with book excerpts to assess the benefits of, and receptivity to, the game, book and XA combination."
"1654651","HCC: Small: Collaborative Research: Integrating Cognitive and Computational Models of Narrative","IIS","HCC-Human-Centered Computing","05/16/2016","09/09/2016","Robert Young","UT","University of Utah","Continuing Grant","William Bainbridge","07/31/2017","$115,313.00","","young@cs.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","7367","7367, 7923","$0.00","The primary objective of this research is to develop new, cognitively informed computational models of the generation of narrative that is told within three-dimensional virtual environments.  Motivated by theoretic models of narrative structure and psychological models of narrative comprehension, techniques will be developed for creating accounts of sequences of events and the techniques needed to convey them to users. These techniques will use these models to search for narratives that are at once coherent and effective at communicating the underlying event structure.  The project will explore how computational models of the mental processes performed by people when experiencing film or machinima can inform an automatic process used to generate the films themselves.  Extensive empirical studies will provide a comprehensive evaluation of the effectiveness of the models.<br/><br/>The research program has three major thrusts: (1) Integrating generative models of character plans with narrative theoretic structural models to create storylines that reflect both rich character goal structures and recognizable narrative elements. (2) Developing methods for shot sequence selection that build on pragmatic models from linguistic communication to effectively convey characters' plans and goals. (3) Developing and then evaluating a system that integrates these parts to search for narratives that are both coherent and effective.<br/><br/>The project will contribute to the infrastructure of science and education by training new researchers (graduate research assistants) in an area that is broadly multidisciplinary (computer science, cognitive science and psychology). These new researchers will gain from the project a unique integrated view of the contributing disciplines.  Team members will participate in the dissemination of results through journal articles and presentations at national and international conferences on creativity, artificial intelligence, human-computer interaction and psychology. It is expected that the work will have a significant impact on the theory and understanding of creativity, particularly in the context of narrative, serving as a foundation for a new generation of tools that support the creative process."
"1623091","EXP: Collaborative Research: Extracting Salient Scenarios from Interaction Logs (ESSIL)","IIS","Cyberlearn & Future Learn Tech","09/01/2016","06/22/2020","Andee Rubin","MA","TERC Inc","Standard Grant","Amy Baylor","12/31/2020","$324,979.00","","andee_rubin@terc.edu","2067 Massachusetts Avenue","Cambridge","MA","021401339","6178739600","CSE","8020","8045, 8841","$0.00","The Extracting Salient Scenarios from Interaction Logs (ESSIL) project proposes to develop a new type of educational technology to support students' learning about complex systems from their participation in a multi-person immersive simulation.  Many important challenges we face today as a society -- including responding to climate change, managing global economies, city planning, disease outbreaks -- are ""complex systems"" problems, meaning that important phenomena in each (for instance trends in weather, stock bubbles, traffic jams, disease transmission) result not from a single cause, but because many small causes combine together. Participating in a simulation has the potential to help students understand the principles of complex systems, but because different principles surface depending on how each simulation unfolds, it can be difficult for teachers to adjust their lesson plans on the fly to highlight the principles that emerge in a given simulation run. To address this challenge, ESSIL will develop methods to create ""automatic salient recaps,"" as a way to help learners and their teachers make better sense of simulations. These recaps, which will be automatically generated, provide a story of ""what happened"" in the simulation in a way that both helps students remember their experience and reveals important scientific principles. Teachers and other facilitators will use these recaps, along with an accompanying discussion guide, to support productive learning conversations about the scientific principles incorporated in a simulation. The recaps will be developed for a large-scale immersive simulation installed at the New York Hall of Science (NYSCI), potentially improving the educational experience of thousands of daily visitors.  The capabilities developed to produce them have widespread applicability, because logs of student interactions are routinely produced by many educational systems.  The project is supported by the Cyberlearning and Future Learning Technologies Program, which funds efforts that will help envision the next generation of learning technologies and advance what we know about how people learn in technology-rich environments. Cyberlearning Exploration (EXP) Projects explore the viability of new kinds of learning technologies by designing and building new kinds of learning technologies and studying their possibilities for fostering learning and challenges to using them effectively. <br/> <br/>The immersive simulation context for the project is Connected Worlds, an embodied, multi-person ecology simulation at NYSCI, with pedagogical goals around sustainability and systems thinking. Using logs from groups of students interacting with Connected Worlds, ESSIL will construct selective recaps of their experience that both are personally salient to them (by including memorable details of their experience) and have explanatory coherence (to enable their discussion of important interconnections in the simulation's underlying model). Artificial Intelligence-based methods will be developed to 1) identify salient changes in the state of the simulation during  student interaction and 2) construct qualitative models of causal chains that could have led to these changes. These qualitative models will be used to generate salient recaps and discussion guides based on them, which will be provided to teachers whose classes are visiting NYSCI. The effectiveness of the innovation will be investigated by comparing visiting students' conversations with and without ESSIL-generated discussion supports and by interrogating their resulting models of the Connected Worlds system through concept maps."
"1636848","BD Spokes: SPOKE: SOUTH: Collaborative:  Using Big Data for Environmental Sustainability:  Big Data + AI Technology = Accessible, Usable, Useful Knowledge!","OAC","BD Spokes -Big Data Regional I, Information Technology Researc","10/01/2016","09/07/2017","Ashok Goel","GA","Georgia Tech Research Corporation","Standard Grant","Beth Plale","09/30/2020","$870,445.00","","ashok.goel@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","024Y, 1640","028Z, 043Z, 7433, 8083, 9251","$0.00","Protecting the environment is among the biggest challenges facing our society. As the effects of environmental degradation, global warming and climate change continue to grow, there is an increasingly urgent and critical need for research and education in biological diversity, ecological modeling and environmental sustainability. On one hand, professional and citizen scientists need ready access to large-scale biological, ecological and environmental data for modeling, simulation and analysis. On the other, college teachers and students in biology and ecology need to access large-scale data in a form meaningful to them. The various audiences will engage with big data in different ways and so a variety of knowledge-building tools are needed. This project brings together two dozen scientists from a dozen institutions in academia, government and industry to address the problem of translating big data into meaningful knowledge in support of research and education in environmental sustainability.  <br/><br/>Encyclopedia of Life  (EOL) is the world's largest database of biological species and other biodiversity information. EOL also works closely with scores of other biodiversity datasets such as BISON, GBIF, and OBIS. This project seeks to make EOL and related biodiversity data sources accessible, usable, and useful, by integrating extant artificial intelligence tools for information extraction, modeling and simulation, and question answering. The focus of this project will be on the data engineering required for this integration and construction of a resulting EOL+ system. The project team will provide access to EOL+ such that users can build their own tools and services on top of EOL+. The team will work with the NSF South Big Data Hub to organize yearly workshops for building and supporting a community of users of EOL+. Professional and citizen scientists, and teachers and students alike, will be able to access EOL+ through NSF's South Big Data Hub webportal, and use it for modeling and analysis, explanation and prediction, as well as education and workforce development in biological diversity, ecological modeling and environmental sustainability."
"1634627","Actor-Critic-Like Stochastic Adaptive Search Algorithms for Simulation Optimization","CMMI","OE Operations Engineering","09/01/2016","08/02/2016","Jiaqiao Hu","NY","SUNY at Stony Brook","Standard Grant","Georgia-Ann Klutke","08/31/2019","$199,923.00","","jiaqiao.hu.1@stonybrook.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","ENG","006Y","071E, 072E, 073E, 077E, 078E","$0.00","Many systems arising in applications from engineering design, manufacturing, and health care require the use of simulation optimization techniques to improve their performance. However, despite significant progress in recent years, simulation optimization remains an area with many theoretical and practical challenges. This research project aims to expand the current knowledge in this field by investigating a novel approach that integrates theories and tools from reinforcement learning (a subarea of artificial intelligence) within a class of adaptive search algorithms called the model-based methods to solve simulation optimization problems. Because of the generality of these methodologies, the resulting techniques will have broad applicability in a wide array of industry and science sectors. In particular, through collaboration with power engineers, the developed algorithms will be tested and applied to voltage control problems in electric power systems, potentially benefiting both utility companies and energy consumers. The research plan will be closely integrated with the education and training of students in engineering by incorporating new developments into the graduate courses the investigator teaches and recruiting  female and underrepresented minority students to the project.<br/><br/>The goal of this research is to advance theoretical underpinnings of new model-based algorithms that can be orders of magnitude more efficient than the state-of-the-art. This will be accomplished by exploring the connections between model-based methods and policy gradient-based reinforcement-learning algorithms. Specifically, the investigator will examine how to use the insights from actor-critic algorithms in the reinforcement learning framework to effectively reduce the sampling variance of model-based methods. If successful, the approach will integrate function approximation techniques within a model-based optimization setting to provide algorithms with low-variance performance estimates in searching for improved solutions. This research may change the manner in which these algorithms are implemented and applied, leading to faster and more efficient algorithms for solving a broad class of optimization problems, especially in settings that require expensive function evaluations or simulations for performance estimation."
"1557945","Functional Analyses of the Neural Circuits Underlying Vocal Production in Xenopus Laevis","IOS","Activation","04/01/2016","02/05/2019","Ayako Yamaguchi","UT","University of Utah","Standard Grant","Sridhar Raghavachari","03/31/2020","$642,643.00","","a.yamaguchi@utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","BIO","7713","1096, 9150, 9178, 9179","$0.00","The most salient output of brain function is behavior.  However, how the nervous system produces behavior is not well understood, largely because most of the neural pathways underlying behavior are complicated.  In this research project, vocal behavior of African clawed frogs is used as a model because their vocal neural pathways are simple and straight forward, and the pathways in action can be studied using techniques that were previously developed in the PI's laboratory.  In addition to their simplicity and accessibility, the frog vocal pathways provide a unique opportunity to study how female and male brains function differently; male and female frogs produce sex-specific vocalizations during the breeding season, and the injection of male-specific hormones into an adult female results in male-like vocalizations within thirteen weeks.  In this study, the focus is placed on one group of cells that are known to play a critical role in the operation of the pathways.  A variety of experimental techniques will be used to understand where these neurons are, how these neurons function, and how they respond to male-specific hormones.  The results of the study will not only provide us with the understanding of how behaviors are generated in the two sexes, but also provide us with an insight into how human brains generates rhythmic activity such as alpha and gamma waves, many of which are known to underlie cognitive processes, and known to be disrupted in diseased states.  <br/><br/>A fundamentally important question in neuroscience is how neural networks function to generate motor programs that underlie behavior.  Although analyses of a complete neural network that generates behavior is a formidable task, the relative simplicity of the Xenopus vocal network combined with the development of the fictive preparation (a ""singing brain in a dish"" preparation) and the application of behavioral, electrophysiological, anatomical, and newly developed optogenetic techniques allows detailed investigation of the dynamic organization of brain in action.  The results of the proposed study will not only provide insight into the structure, function, and plasticity of the rhythm-generating neural network at the cellular levels, but also allow us to understand the logic of how a feedback loop should be engineered into a network to generate stable rhythms.  Rhythmic neuronal activity is not limited to motor systems, but is prevalent across the entire CNS and is considered to underlie important functions such as perception and cognition.  Thus, understanding the biophysical principles that govern rhythm generation using a simple neural network has a potential to elucidate mechanisms underlying neuronal oscillations in general.  On a technical level, successful application of optogenetic tools to the Xenopus fictive preparation in vitro fills an important gap between research efforts conducted on genetic vs non-genetic model organisms.  There are many non-genetic model organisms that present unique questions.  The ability to express genetically encoded tools in non-model organisms represents a revolutionary change in the field of comparative neuroscience."
"1643098","The 22nd International Conference on Computing in High Energy and Nuclear Physics, CHEP 2016; October 10-14, 2016 in Marriott Marquis, San Francisco","PHY","COMPUTATIONAL PHYSICS","08/01/2016","07/29/2016","Lauren Tompkins","CA","Stanford University","Standard Grant","Bogdan Mihaila","07/31/2017","$10,000.00","","laurenat@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","MPS","7244","7433, 7569, 8084","$0.00","This project will allow for partial support for students participating in the 22nd International Conference on Computing in High Energy and Nuclear Physics, CHEP 2016, that will be held in San Francisco, October 10-14, 2016. CHEP is a global conference with participation by leaders in physics, computing, and large-scale data analysis from the top universities and laboratories around the world. The CHEP speakers will address challenges in computing, networking and software for the world's leading data-intensive science experiments. The participation of students in this conference is important for training purposes and future workforce development activities. <br/><br/>CHEP 2016 is organized jointly by Stanford University, SLAC National Accelerator Laboratory (SLAC) and Lawrence Berkeley National Laboratory (LBNL). Approximately 600 computing experts and scientists are expected to attend CHEP 2016 with strong participation from the science programs at CERN, Fermilab and other major experimental facilities. CHEP 2016 will exploit this environment to stress the increasing importance of the computing-related connections between high-energy and nuclear physics and data-intensive astronomy, X-ray science and computational science. The program spans a wide range of subjects including trigger, data acquisition and control systems, reconstruction and data analysis algorithms, data processing workflows and computing models, artificial intelligence, network systems and security, and many additional topics."
"1629395","XPS: EXPL: Hippogriff: Efficient Heterogeneous Servers for Data Centers and Cloud Services","CCF","Exploiting Parallel&Scalabilty","10/01/2016","09/08/2016","Steven Swanson","CA","University of California-San Diego","Standard Grant","Marilyn McClure","09/30/2020","$300,000.00","","swanson@cs.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","8283","","$0.00","The growing importance of artificial intelligence, network services, and cloud storage drives the demand of building powerful computer systems that can perform many operation at once.  Building computers with different kinds of computing processors (i.e., heterogeneous processing) is an effective way to achieve this goal. However, this approach also creates new problems that can negate some of the benefits it provides.  In particular, using different processors for different tasks requires moving data between those processors.  This movement takes time and can cancel out saving heterogeneous processing provides. This project is addressing this problem in heterogeneous computing systems by making the movement of data between different processors more efficient.  This improved efficiency leads directly to benefits for applications of scientific and commercial importance.<br/><br/>Much of the cost of data movement heterogeneous computing systems stems from the entrenched central processing unit (CPU)-centric programming model.  This project is revisiting the design of the application interface, system software and hardware components to remove CPUs and main memory from the critical path of moving data.  The project provides an efficient programming model that allows the system software stack to automatically and efficiently setup the data movements between heterogeneous processors. We are applying the system to large-scale database systems, massive parallel programming systems like Spark and MapReduce as well as scientific computing that power important daily applications and research projects."
"1636859","BD Spokes: Spoke: South: Collaborative: Using Big Data for Environmental Sustainability: Big Data + AI Technology = Accessible, Usable, Useful Data!","OAC","BD Spokes -Big Data Regional I, Information Technology Researc","10/01/2016","08/21/2017","Jennifer Hammock","VA","Smithsonian Institution","Standard Grant","Beth Plale","09/30/2020","$294,320.00","","hammockj@si.edu","Office of Sponsored Projects","Arlington","VA","222023709","2026337110","CSE","024Y, 1640","028Z, 043Z, 7433, 8083","$0.00","Protecting the environment is among the biggest challenges facing our society. As the effects of environmental degradation, global warming and climate change continue to grow, there is an increasingly urgent and critical need for research and education in biological diversity, ecological modeling and environmental sustainability. On one hand, professional and citizen scientists need ready access to large-scale biological, ecological and environmental data for modeling, simulation and analysis. On the other, college teachers and students in biology and ecology need to access large-scale data in a form meaningful to them. The various audiences will engage with big data in different ways and so a variety of knowledge-building tools are needed. This project brings together two dozen scientists from a dozen institutions in academia, government and industry to address the problem of translating big data into meaningful knowledge in support of research and education in environmental sustainability.  <br/><br/>Encyclopedia of Life  (EOL) is the world's largest database of biological species and other biodiversity information. EOL also works closely with scores of other biodiversity datasets such as BISON, GBIF, and OBIS. This project seeks to make EOL and related biodiversity data sources accessible, usable, and useful, by integrating extant artificial intelligence tools for information extraction, modeling and simulation, and question answering. The focus of this project will be on the data engineering required for this integration and construction of a resulting EOL+ system. The project team will provide access to EOL+ such that users can build their own tools and services on top of EOL+. The team will work with the NSF South Big Data Hub to organize yearly workshops for building and supporting a community of users of EOL+. Professional and citizen scientists, and teachers and students alike, will be able to access EOL+ through NSF's South Big Data Hub webportal, and use it for modeling and analysis, explanation and prediction, as well as education and workforce development in biological diversity, ecological modeling and environmental sustainability."
"1624035","I-Corps: Mined Semantic Analysis Commercialization Research","IIP","I-Corps","02/15/2016","02/16/2016","Wlodek Zadrozny","NC","University of North Carolina at Charlotte","Standard Grant","Steven Konsek","07/31/2017","$50,000.00","","wzadrozn@uncc.edu","9201 University City Boulevard","CHARLOTTE","NC","282230001","7046871888","ENG","8023","","$0.00","This I-Corps Teams project will help evaluate the commercial feasibility of improving knowledge work with Mined Semantic Analysis (MSA) software. A recent report on Artificial Intelligence, estimated a $2 trillion impact on knowledge work from machine automation/augmentation. As a solution that offers highly usable access to relevant domain knowledge, thereby enabling this economic shift, MSA potentially addresses millions of users. Applications include 1) Innovation management (technical document search) 2) Biomedical research (literature search) and 3) Business management (market/competitive research. MSA's concept based approach improves user interface visual prioritization, rapid comprehension and facile navigation to refine search. The software enables practical application of knowledge from billions of hours of human effort, thereby reducing the gap between novice and expert users.<br/><br/>The project aims to validate product market fit of MSA software through customer exploration and MVP prototype development. Building on work done over the past several months, the goal of the interviews is to identify specific market problems that the technology is capable of solving in a demonstrably better way. The I-Corps team will test high potential areas such as: Intellectual property research and analytics, Legal discovery, Investment/Consulting, Enterprise search, Knowledge management, Biomedical literature and Healthcare information search. The team's other project goal is to prototype core functionality to align with key customer performance measures. This will include quick experiments to gauge modes of user interaction and experience, visualization, and some preliminary performance testing."
"1615475","SHF: Small: Cross-Platform Solutions for Pruning and Accelerating Neural Network Models","CCF","Software & Hardware Foundation","07/01/2016","07/13/2016","Hai Li","PA","University of Pittsburgh","Standard Grant","Sankar Basu","06/30/2017","$450,000.00","Yiran Chen","hai.li@duke.edu","300 Murdoch Building","Pittsburgh","PA","152603203","4126247400","CSE","7798","7923, 7945, 8089, 8091","$0.00","Deep neural networks (DNNs) have achieved remarkable success in many applications because of their powerful capability for data processing. The objective of this project is to investigate a software-hardware co-design methodology for DNN acceleration that can be applied to both traditional von Neumann and emerging neuromorphic architectures. The project fits into the general area of ""brain-inspired"" energy efficient computing paradigms that has been of much recent interest. The investigators are also active in various outreach and educational activities that include curricular development, engagement of minority/underrepresented students in research. Undergraduate and graduate students involved in this research will also be trained for the next-generation computer engineering and semiconductor industry workforce.<br/><br/>From a more technical standpoint, a novel neural network sparsification process is to be explored to preserve the state-of-the-art accuracy, while establishing hardware-friendly models of neural network computations.  The result is expected to lead to a holistic methodology composed of neural network model sparsification, hardware acceleration, and an integrated software/hardware co-design.  The project also benefits big data research and industry at large by inspiring an interactive design philosophy between the design of learning algorithms and the corresponding computational platforms for system performance and scalability enhancement."
"1631586","NCS-FO: Collaborative Research: Flexible Rule-Based Categorization in Neural Circuits and Neural Network Models","BCS","IntgStrat Undst Neurl&Cogn Sys","09/01/2016","08/17/2016","Xiao-Jing Wang","NY","New York University","Standard Grant","Kurt Thoroughman","08/31/2019","$414,233.00","","xjwang@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","SBE","8624","7298, 8089, 8091, 8551","$0.00","Categorization is the brain's ability to recognize the meaning of objects and events in our environment, and is an essential cognitive process underlying decision making. Categorical decisions are often flexible, and depend on the demands on the task at hand. The current project aims to understand the brain mechanisms which underlie flexible categorical decision making, as well as computational algorithms for making such decisions my artificially intelligent systems. Experiments will record from ensembles of cortical neurons during flexible categorization tasks. Computational modeling work will train recurrent neural networks to perform the same flexible categorization tasks used in the experiments, with parameters of the model inspired by the experimental data. This will result in a greater understanding of the neural mechanisms underlying categorization and decision making, as well as improvements in computational algorithms for flexible categorization by artificially intelligent systems. The broader impacts of the project include substantial training opportunities for undergraduates, Ph.D. students, and postdoctoral researchers in both experimental and computational approaches to flexible decision making. The project will also generate new experimental data and computational tools that will be shared with the broader scientific community. <br/><br/>This project combines multi-channel neurophysiological recordings and neural circuit modeling to investigate the neural circuit mechanisms of flexibility and generalization in visual categorization. The project leverages a collaboration by the researchers that has proven fruitful in our previous joint research on category learning. The focus of the present project is on flexible task switching between discrimination and categorization, and between categorization rules, in the behavioral, experimental, and computational work. The task paradigms will also directly test the 'exemplar model' of categorization from cognitive psychology, linking behavioral models to neural circuit processes. The project will develop a novel modeling framework, based on training recurrent neural networks to learn to perform multiple tasks. This approach offers a potentially powerful data analysis tool and conceptualization of neural circuit computation in terms of neural population trajectories in a high-dimensional state space, and this perspective is urgently needed to analyze simultaneous recording from many single neurons during performance of complex cognitive tasks, a major thread of modern Data-Intensive Neuroscience and Cognitive Science."
"1631571","NCS-FO: Collaborative Research: Flexible Rule-Based Categorization in Neural Circuits and Neural Network Models","BCS","IntgStrat Undst Neurl&Cogn Sys","09/01/2016","08/17/2016","David Freedman","IL","University of Chicago","Standard Grant","Kurt Thoroughman","08/31/2019","$565,183.00","","dfreedman@uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","SBE","8624","7298, 8089, 8091, 8551","$0.00","Categorization is the brain's ability to recognize the meaning of objects and events in our environment, and is an essential cognitive process underlying decision making. Categorical decisions are often flexible, and depend on the demands on the task at hand. The current project aims to understand the brain mechanisms which underlie flexible categorical decision making, as well as computational algorithms for making such decisions my artificially intelligent systems. Experiments will record from ensembles of cortical neurons during flexible categorization tasks. Computational modeling work will train recurrent neural networks to perform the same flexible categorization tasks used in the experiments, with parameters of the model inspired by the experimental data. This will result in a greater understanding of the neural mechanisms underlying categorization and decision making, as well as improvements in computational algorithms for flexible categorization by artificially intelligent systems. The broader impacts of the project include substantial training opportunities for undergraduates, Ph.D. students, and postdoctoral researchers in both experimental and computational approaches to flexible decision making. The project will also generate new experimental data and computational tools that will be shared with the broader scientific community.<br/><br/>This project combines multi-channel neurophysiological recordings and neural circuit modeling to investigate the neural circuit mechanisms of flexibility and generalization in visual categorization. The project leverages a collaboration by the researchers that has proven fruitful in our previous joint research on category learning. The focus of the present project is on flexible task switching between discrimination and categorization, and between categorization rules, in the behavioral, experimental, and computational work. The task paradigms will also directly test the 'exemplar model' of categorization from cognitive psychology, linking behavioral models to neural circuit processes. The project will develop a novel modeling framework, based on training recurrent neural networks to learn to perform multiple tasks. This approach offers a potentially powerful data analysis tool and conceptualization of neural circuit computation in terms of neural population trajectories in a high-dimensional state space, and this perspective is urgently needed to analyze simultaneous recording from many single neurons during performance of complex cognitive tasks, a major thread of modern Data-Intensive Neuroscience and Cognitive Science."
"1648751","STTR Phase I:  Solar Irradiance Microforecasting","IIP","STTR PHASE I","12/15/2016","12/01/2016","Narayanan Sankar","NC","Microgrid Labs Inc.","Standard Grant","Muralidharan S. Nair","04/30/2018","$224,999.00","Thomas Caudell","sankar@microgridlabs.com","903 Grogans Mill Drive","Cary","NC","275197175","9199854723","ENG","1505","1505, 4080, 8034, 8035, HPCC","$0.00","The broader impact/commercial potential of this project to develop short term Solar irradiance forecasting, will be to support very large deployment of Solar photovoltaic (SPV) generation capacity, by reducing the cost of mitigating cloud caused fluctuation of SPV electricity generation. This increased SPV system deployment will reduce the amount of base load and peaking generation from greenhouse gas causing, and water consuming fossil fuel generators. Such forecasting will enable development of pre-&#8208; mitigation strategies instead of post mitigation using electrical storage systems. Prior studies indicate that this will result in the reduction by up to a factor of five, of the input/output requirements of the electrical storage system used in the pre-&#8208;mitigation scenario, compared to the post mitigation scenario. These benefits will be seen with grid-&#8208;tied, micro-&#8208;grid and off-&#8208;grid SPV systems. This opens commercial opportunities for introducing intelligent sensors and control systems to reduce bulk electrical storage. The technology areas used in this project include sensors, 3D printing, neural network based learning systems, embedded computers and cloud computing. The market sectors that will see a positive impact include all demographics as consumers, and manufacturers of SPV modules and SPV balance of system suppliers.<br/><br/>This Small Business Technology Transfer (STTR) Phase I project  addresses  the  problem  of  mitigating cloud movement induced fluctuation in the output of SPV systems. The research objectives of Phase I are (a) prototype a whole sky imager that provides sufficient circumsolar image discrimination, to drive a neural network based learning system ? this will require  development  of  a  3D-&#8208;printed  mounting system for a whole sky sensor, and interface to a cloud connected, local single board computer, (b) develop and optimize Image Acquisition, Compositing, Analysis, and Forecasting Algorithms to provide 15-&#8208;500 second forecasts of Solar irradiance, and (c) deploy imager + software prototypes to evaluate real live sky imagery in multiple locations with different weather patterns, by gathering data to ?train? the neural network. It is anticipated that this evaluation and analysis of prototype performance will continue in subsequent phases, to obtain high confidence results. The anticipated results of the research in Phase I  are (i) refinement of the image capture system to produce ?good? imagery, (ii) development  of procedures  to  tune  neural  network  learning  system  towards  obtaining  high  confidence  forecasts, and (iii) understanding of performance requirements of local single board computer."
"1611279","Resonant Tunnel Field Effect Transistors Based on Vertical 2D Crystal Heterostructures","ECCS","EPMD-ElectrnPhoton&MagnDevices","10/01/2016","05/01/2018","Wenjuan Zhu","IL","University of Illinois at Urbana-Champaign","Standard Grant","Lawrence Goldberg","09/30/2019","$378,000.00","","wjzhu@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","ENG","1517","100E, 9251","$0.00","The proposed interdisciplinary research combines advances in two dimensional (2D) materials, brain-inspired network computing, and resonant tunnel diodes (RTDs), to create a new device concept: 2D resonant tunnel field-effect-transistor (RTFET).  RTFETs based on 2D crystals will solve the lattice matching/dislocation issues of III-V semiconductor based RTDs. The strong negative differential resistance and high frequency response in these devices will be used to perform ""comparison"" functions, similar to synapses in neurons, for image recognition applications. RTFET-based synapses can potentially reduce energy consumption by more than ten times compared to the circuits based on traditional complementary metal-oxide-semiconductor (CMOS). The ultra-high operating frequencies of RTFETs will enable their applications in high speed wireless communication, THz imaging, and spectroscopy. In addition, RTFETs can also serve as sensitive vehicles to probe 2D/2D interfaces, providing scientific insights on the out-of-plane transport in 2D heterostructures. This research project will not only advance the knowledge of quantum tunneling in 2D crystals, resonant tunneling devices, and brain-like circuits, but also have direct technology impact on image recognition, non-traditional computing, and high frequency wireless communications. The integration of the proposed research with after-school programs in elementary schools, new courses for undergraduate/graduate students, and recruiting/retaining women students, will foster students' interest in nanotechnology while broadening their knowledge base, thus having a positive enduring impact on the education of a world-class and diverse science and technology workforce.<br/><br/>The objective of this project is to understand the interlayer resonant tunneling in 2D vertical heterostructures and demonstrate high speed RTFETs with pronounced negative differential resistance at room temperatures for neurosynaptic image recognition applications. The PI will pursue the following four thrusts: (1) fabrication of a variety of 2D vertical heterostructures, including black phosphorus/boron nitride/black phosphorus, tungsten diselenide/molybdenum disulfide/tungsten diselenide heterostructures, with precise control of rotation angles and layer numbers; (2) evaluate the impact of rotation angle, band offsets, tunneling barrier thickness, and interface qualities, on the resonant tunneling currents in the vertical 2D heterostructures; (3) demonstrate high speed RTFETs with pronounced negative differential resistance at room temperatures; (4) demonstrate synapses based on 2D RTFETs, as basic elements in neurosynaptic chips for image recognition applications. The successful execution of this project will expand the knowledge of resonant quantum tunneling in 2D heterostructures, resonant tunneling devices, and neural-network computing. More specifically, this research will elucidate the effect of rotation angle alignment, band extrema location, bandgap, and band offsets on the interlayer resonant tunneling in RTFETs. This will also provide insight into inelastic tunneling and scattering due to interface traps and defects in RTFETs. The fundamental understanding of the resonant tunneling in 2D heterostructures gained in this research project will enable a new class of novel nanoscale electronic devices based on out-of-plane transport instead of the traditional in-plane transport of 2D crystals. This research will also pave the way for new functional devices beyond CMOS and provide an experimental demonstration of neural-network circuits based on RTFETs. The negative differential resistance and fast response in RTFETs can potentially enable a new computing paradigm beyond the traditional von Neumann architecture."
"1612924","On Statistical Modeling and Parameter Estimation for High Dimensional Systems","DMS","STATISTICS","09/01/2016","08/23/2016","Faming Liang","FL","University of Florida","Standard Grant","Yong Zeng","01/31/2018","$150,000.00","","fmliang@purdue.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","MPS","1269","","$0.00","The dramatic improvements in data collection and acquisition technologies over the last decades have enabled scientists to collect massive amounts of high-dimensional data that allow for monitoring and studying of complex systems.  Due to their intrinsic nature, many of the high-dimensional datasets, such as omics and genome-wide association study (GWAS) data, have a much smaller sample size compared to the dimension (referred to as the small-n-large-P problem). Current research on statistical modeling of small-n-large-P data focuses on linear and generalized linear models. However, these approaches are often not adequate for modeling complex systems, and estimation of the model parameters is challenging.  This project addresses two fundamental problems, statistical modeling and parameter estimation, toward a valid statistical analysis of high-dimensional data.  Successful completion of this project will generate hands-on tools for statistical inference of high-dimensional complex systems, which can benefit researchers in many areas of science and technology.  In particular, the proposed applications to biomedical studies will lead to accurate tools for detecting biomarkers associated with disease processes and tailoring optimal therapy for individual patients with complex diseases. The research results will be disseminated to the statistical and biomedical communities, via collaboration, conference presentations, books, and articles to be published in academic journals. The project will also have significant impact on education through the involvement of graduate students in the project, and incorporation of results into undergraduate and graduate courses.  In addition, the R package developed under this project will provide a valuable tool for statistical analysis of high-dimensional data.<br/><br/>The current approach to modeling small-n-large-P data focuses on linear and generalized linear models, and casts the problem as variable selection by imposing a sparsity constraint on parameter values.  Although these models have many advantages, such as simplicity and computational efficiency, estimation of the parameters is still a challenging problem.  While regularization is often used in these situations, it can perform poorly when the sample size is small and the variables are highly correlated.  Two new methods are proposed to address these concerns, namely, Bayesian neural network (BNN) and blockwise coordinate consistency (BCC).  The BNN method works by first fitting the data with a feed-forward neural network, conducting variable selection through network structure selection under a Bayesian framework, and resolving the associated computational difficulty via parallel computing. Compared to existing methods, BNN can lead to much more precise selection of relevant variables and outcome prediction for high-dimensional nonlinear systems.  The BCC method works by maximizing a new objective function, the expectation of the log-likelihood function, using a cyclic algorithm and iteratively finding consistent estimates for each block of parameters conditional on the current estimates of the other parameters.  The BCC method reduces the high-dimensional parameter estimation problem to a series of low-dimensional parameter estimation problems.  The preliminary results indicate that BCC can provide a drastic improvement in both parameter estimation and variable selection over regularization methods. The validity of the proposed methods will be rigorously studied and applied to biomarker discovery, precision medicine, and joint estimation of the regression coefficients and precision matrix for high-dimensional multivariate regression."
"1629989","CI-NEW: Multilingual FrameNet: A Resource Enabling Cross-Lingual Research for the Natural Language Processing Community","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","08/15/2016","08/09/2016","Collin Baker","CA","International Computer Science Institute","Standard Grant","Tatiana Korelsky","09/30/2020","$607,574.00","","collinb@icsi.berkeley.edu","1947 CENTER ST STE 600","Berkeley","CA","947044115","5106662900","CSE","7359","7359, 9251","$0.00","The FrameNet lexical semantic database records the meanings of words (and multi-word expressions) in everyday English; it is a sort of ""super dictionary"" that is both human-readable and machine-readable.  This database is based on the fact that individual words can evoke and entire situation in our minds, complete with roles for people and things that participate in the situation.  For example, the word hire evokes the situation of Employment, with roles for the Employer, the Employee, the Position, etc.;  both the word vengeance and the expression get back at evoke Revenge, with the roles Avenger, Injured party, Injury, Offender, and Punishment.  These situations are called semantic frames, and the project is guided by the theory of Frame Semantics, developed by the late Prof. Charles J. Fillmore of UC Berkeley.  The FrameNet lexical database currently includes descriptions of more than 1,000 semantic frames, more than 13,000 senses of words and expressions (called Lexical units), and more than 200,000 manually annotated examples which show how the various roles are expressed by different parts of a sentence.<br/>The FrameNet database is widely used in natural language processing; it helps engineers create software to analyze written texts into semantic frames and participants, so that computers can reason about the situations described. Thousands of researchers and companies are already using such software for applications such as automatic analysis of reports from combat or natural disaster situations, understanding financial news reports, recognizing expressions of opinion on blogs and product websites, and searching clinical records and medical research reports. <br/><br/>Although the frames were mainly created for English, most of them have been shown to be useful for other languages as well, and researchers around the world are now creating FrameNet databases for many other languages.  The Multilingual FrameNet project will align the databases for different languages, both at the level of semantic frames and at the level of lexical units.  The aligned database will help to improve applications such as foreign language teaching, cross-linguistic information retrieval, and machine translation.  The new project also includes setting up a website and software so that teachers and students everywhere can participate in the project by adding to English FrameNet, creating a more complete and more useful FrameNet for its many users."
"1556874","LTREB Renewal: Ecological Dynamics in an Experimentally-Tractable Natural Ecosystem","DEB","POP & COMMUNITY ECOL PROG, LONG-TERM RSCH IN ENVIR BIO","03/15/2016","03/15/2016","John Wootton","IL","University of Chicago","Standard Grant","Douglas Levey","02/28/2021","$448,495.00","","twootton@uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","BIO","1182, 1196","1196","$0.00","A central goal for ecology is to document if and how the environment is changing, to determine the causes of these changes, and to predict what the consequences of these changes will be to ecological systems. This is a challenge because of the complex network of connections among the living organisms and the non-living parts of ecosystems.  Mathematical models are essential tools to keep track of these ecological interactions and to predict how they will respond to environmental changes. However, models need to be linked to data from nature. Two major challenges in developing predictive models of environmental change are 1) collecting sufficient data on how interactions among a complete set of species and environmental factors change over time, and (2) rigorously testing model predictions with experiments. This study will combine a quarter-century long series of data on 100+ species and relevant environmental variables in the rocky shoreline of Tatoosh Island in Washington state, with a long-term field experiment that mimics the extinction of a key species, the California mussel. The long term data will be applied to several different modeling approaches and predictions from these models will subsequently be tested with the long-term field experiment. The research will identify the most promising modeling approaches for making ecological prediction, and make them available to ecosystem managers and policy makers interested in the consequences of environmental impacts such as species extinction and global change. The comprehensive data series also will be made available to other scientists to be used as a platform for additional studies.  This project will also engage undergraduate students in field research, data management, mathematical modeling, and in communicating with the public, managers, and policy makers. Furthermore, because the challenge of understanding networks of species interactions is shared with other scientific disciplines that deal with complex networks, project results will be of general value in other disciplines. <br/><br/>The researcher will conduct annual surveys of replicated permanent plots for plants and animals on the shoreline in two ways: 1) by documenting the species identities under 2,600 fixed points over a 5-year period and generating annual transition probabilities among species, and 2) by generating abundance estimates in permanent 60 x 60 cm census plots.  Fifteen experimental plots will be maintained by selectively removing individuals of Mytilus californianus when they appear, leaving all other species undisturbed.  Environmental data will be collected every 30 minutes using a submersible data logger and a land-based weather station. Water chemistry, including critical nutrients, will be monitored.  These data will be analyzed in several ways, including 1) parameterizing transition-based models (Markov chain models, spatially-explicit cellular automata) with environmental dependencies, 2) parameterizing multi-species population dynamic models from plot counts, 3) applying multi-spatial cross-convergent mapping and testing whether it accurately detects key species known to have strong causal effects from independent experiments, 4) applying neural network models and testing their predictions about the consequences of species extinction, and 5) testing whether there is a relationship between the variability of a species' abundance through time and its importance to the ecosystem as assessed by independent experiments. The community modeling projects enabled by the rich long term data sets have a strong potential to advance our understanding of mechanisms underlying community dynamics and their response to environmental change."
"1619394","RI: Small: Recognizing Implicit Personal States in Natural Language","IIS","Robust Intelligence","07/01/2016","06/30/2016","Ellen Riloff","UT","University of Utah","Standard Grant","Tatiana Korelsky","06/30/2019","$300,000.00","","riloff@cs.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","7495","7495, 7923, 9150","$0.00","Narrative texts and personal conversations typically revolve around situations that people find themselves in. Current natural language systems extract only the literal meaning of events, failing to recognize how people are impacted by them.  For example, if a man says that he has been laid off or diagnosed with cancer, then a system should understand that he is in a negative situation. If a woman says that she just graduated from college or has been promoted at work, then a system should understand that she is in a positive situation. This project develops technology to automatically identify situations that positively or negatively impact people. Affective knowledge is essential for applications such as sentiment and social media analysis, for example to recognize at-risk individuals experiencing adverse situations that may make them a danger to themselves or others.<br/><br/>This research develops natural language processing technology to recognize implicit affective states associated with events.  Events and states are represented as situation frames and automatically harvested from large text corpora.  Implicating situations are learned from blogs using semi-supervised label propagation with context graphs to propagate positive/negative evidence based on topic clustering, event-event co-occurrence, and discourse relations. Implicating situations are learned from tweets using bootstrapping methods applied to affective and sarcastic tweets. Each affective situation is automatically assigned a polarity and connotative strength. This research advances human language technology toward fundamentally deeper language understanding about affective states, motivations, and goals for narrative text and conversational dialogue."
"1562364","RI: Medium: Broad-Coverage Semantic Parsing: Linguistic Representation Learning from Crowd-Scale Data","IIS","Robust Intelligence","09/01/2016","06/17/2019","Noah Smith","WA","University of Washington","Continuing Grant","D.  Langendoen","08/31/2021","$1,203,556.00","Luke Zettlemoyer","nasmith@cs.cmu.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7495","7495, 7924, 9251","$0.00","Automated understanding of text is a capability that will advance a wide range of language technologies, including information extraction, question answering, opinion analysis, and translation between languages.  Such technologies have been in demand in the intelligence and defense communities for many years, and they now underlie many commercially available information-management tools.  This project develops robust algorithms that understand natural language expressions by mapping them to formal representations of their meaning, a technique known as semantic parsing.  For semantic parsing to be employed in technologies like those listed above, it needs to overcome the fundamental challenge of broad coverage, the ability to handle any text input, in multiple languages.  This project meets this challenge by creating new methods for gathering large repositories of semantically annotated data at greatly reduced cost; these are then used to train much more accurate broad-coverage parsing models.  The results of this project include open-source implementations, high-quality annotated corpora on an unprecedented scale, and reusable distributed semantic representations for use by the community of natural language processing researchers and practitioners. <br/><br/>The goal of broad-coverage semantic parsing can only be achieved by simultaneously focusing on new, large scale sources of data with semantically meaningful annotations and new learning algorithms for inducing models with the representational capacity to make full use of such data.  For scalable data collection, this project introduces new techniques that rely on two key complementary insights: (1) any reader who understands a text can answer questions about it, and (2) questions can be constructed whose answers probe any aspect of semantics that need to be recovered.  These observations allow designing new data collection techniques that reduce the burden of semantic annotation by providing simple questions and answers about texts.  This QA-style annotation can be done for any text in any language, given only native speakers, bypassing the significant effort that currently goes into defining detailed annotation standards.  It also allows gathering new datasets on a much larger scale, and for more diverse text types, than ever before.  In addition, the project develops new representation learning techniques that tie together a wide range of semantic annotation styles, including the new crowdsourced ones, in a multitask learning setup.  Continuous representations (e.g., of word types) provide a powerful way to allow sharing of statistical strength across a large vocabulary, many of whose elements are sparsely observed.  While past work has emphasized learning word embeddings, this project employs a shared continuous space (""framespace"") that can capture abstract frames and roles used in predicate-argument (and logical) semantics.  The usefulness of these representations depends on the tasks they are trained to perform, and using multiple related tasks can lead to benefits on all of them, by sharing of statistical strength across task-specific representations, across elements of the semantic lexicon, and even across languages."
"1619433","RI: Small: Speedup Learning for Online Planning Under Uncertainty","IIS","Robust Intelligence","09/01/2016","06/20/2016","Alan Fern","OR","Oregon State University","Standard Grant","Jie Yang","08/31/2020","$450,000.00","Prasad Tadepalli","afern@eecs.oregonstate.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","CSE","7495","7495, 7923","$0.00","Many complex stochastic planning domains such as logistics,<br/>emergency response, resilient power grids, and robotics require the<br/>ability to make high-quality decisions under tight time<br/>constraints. This project addresses the need for high-quality, but<br/>computationally efficient, decision making via new theory and<br/>algorithms for speedup learning, which will enable planners to<br/>learn to speedup their performance based on prior planning<br/>experience. This speedup-learning approach is loosely inspired by<br/>the fact that humans routinely learn to speedup their reasoning<br/>processes with experience, without sacrificing decision quality.<br/>Similarly, through speedup learning, an inefficient planner that<br/>produces high-quality decisions will be transformed into a much<br/>faster planner with little loss in decision quality.<br/><br/>The project involves advancing speedup learning for online planning<br/>under uncertainty on four fronts. First, the speedup-learning<br/>problem is formalized by introducing the canonical problem of<br/>Primitive Speedup Learning (PSL) and studying how PSL can be used<br/>to solve various speedup objectives. Second, a novel online<br/>planning framework, which subsumes many existing frameworks and<br/>enables many potential speedup opportunities, is being designed and<br/>developed. Third, the project is producing new speedup learning<br/>algorithms for the new framework, which learn various types of<br/>knowledge and that can exploit deep neural network (DNN)<br/>techniques. Finally, the research is producing extensive empirical<br/>evaluations including applications to the important problems of<br/>power grid control, municipal emergency response, and benchmark<br/>planning domains. The project has the potential for significant broader impact on<br/>applications where time-sensitive decisions must be made within<br/>stochastic environments. It will directly contribute to advances in<br/>two applications in particular: remedial action control in<br/>electrical grids to minimize cascading power outages, and planning<br/>for municipal emergencies such as fire and rescue operations in<br/>cities. The project will also serve to advance graduate education<br/>through research assistantships and undergraduate education through<br/>summer and academic term research experiences for undergraduates. A<br/>special topics graduate course will be taught on the area of<br/>planning and learning at Oregon State University and all course<br/>materials will be open access."
"1629033","CI-New: Collaborative Research: COVE-Computer Vision Exchange for Data, Annotations and Tools","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","08/15/2016","08/08/2016","Walter Scheirer","IN","University of Notre Dame","Standard Grant","Jie Yang","07/31/2020","$200,686.00","","wscheire@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","CSE","7359","7359","$0.00","The project provides discoverability, low overhead for use, reproducibility of research, and persistence for computer vision data.  The project is hence setting a direction toward which the computer vision community can collectively work in creating a dataset infrastructure that allows for transparency across individual datasets and annotations, experimental benchmarks with community-set corpora and metrics, and a web-based infrastructure to cultivate continued development of computer vision datasets.  The availability of such an infrastructure, which is named COVE: Computer Vision Exchange of Data, Annotations and Tools, impacts the computer vision and related communities to develop next generation robust intelligence capabilities that have great potential to positively impact society. The project is integrated with education by supporting graduate and undergraduate students, and reaches middle school students through outreach activities.<br/><br/>The project is establishing COVE, a centralized community-run infrastructure to support the exchange of data and annotations as well as the software tools to manipulate them.  The infrastructure is web-based open-source, and provides open access to its contents. Stewardship over the contents are managed by the Investigators initially and subsequently through elected members of the computer vision community. There are two salient components of the infrastructure. First, a curation infrastructure facilitates back-end storage, querying, data annotation and curation tools, to support it.  To curate the federated data set, COVE uses widely known open-source tools like Python, Bootstrap and Postgresql.  For curation of new annotations to incorporate into the exchange, the project relies heavily on crowd-sourcing.  Second, a usage infrastructure, e.g., data structures and software enables widespread and easy use by researchers and practitioners.  The project develops APIs to allow for easy programmable access to the federated data sets and tools through common software interfaces like Matlab and OpenCV."
"1628987","CI-NEW: Collaborative Research: COVE-Computer Vision Exchange for Data, Annotations and Tools","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","08/15/2016","08/08/2016","Jason Corso","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Jie Yang","07/31/2021","$343,000.00","","jjcorso@eecs.umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7359","7359","$0.00","The project provides discoverability, low overhead for use, reproducibility of research, and persistence for computer vision data.  The project is hence setting a direction toward which the computer vision community can collectively work in creating a dataset infrastructure that allows for transparency across individual datasets and annotations, experimental benchmarks with community-set corpora and metrics, and a web-based infrastructure to cultivate continued development of computer vision datasets.  The availability of such an infrastructure, which is named COVE: Computer Vision Exchange of Data, Annotations and Tools, impacts the computer vision and related communities to develop next generation robust intelligence capabilities that have great potential to positively impact society. The project is integrated with education by supporting graduate and undergraduate students, and reaches middle school students through outreach activities.<br/><br/>The project is establishing COVE, a centralized community-run infrastructure to support the exchange of data and annotations as well as the software tools to manipulate them.  The infrastructure is web-based open-source, and provides open access to its contents. Stewardship over the contents are managed by the Investigators initially and subsequently through elected members of the computer vision community. There are two salient components of the infrastructure. First, a curation infrastructure facilitates back-end storage, querying, data annotation and curation tools, to support it.  To curate the federated data set, COVE uses widely known open-source tools like Python, Bootstrap and Postgresql.  For curation of new annotations to incorporate into the exchange, the project relies heavily on crowd-sourcing.  Second, a usage infrastructure, e.g., data structures and software enables widespread and easy use by researchers and practitioners.  The project develops APIs to allow for easy programmable access to the federated data sets and tools through common software interfaces like Matlab and OpenCV."
"1630019","Group Travel Grant for the Doctoral Consortium of the IEEE Conference on Computer Vision and Pattern Recognition","IIS","Information Technology Researc, Robust Intelligence","06/01/2016","03/23/2016","Adriana Kovashka","PA","University of Pittsburgh","Standard Grant","Jie Yang","05/31/2017","$17,050.00","","kovashka@cs.pitt.edu","300 Murdoch Building","Pittsburgh","PA","152603203","4126247400","CSE","1640, 7495","1640, 7495, 7556","$0.00","This grant partially supports the participation of students from US institutions in the Doctoral Consortium at the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2016. CVPR is the premier annual international conference in computer vision with about 2000 senior, junior, and student participants. The goal of the Doctoral Consortium is to highlight the work of senior PhD students who are within six months of receiving their degrees (including recent graduates), and to give these students an opportunity to discuss their research and career options with faculty and researchers who have relevant expertise and experience. The Doctoral Consortium supports the career development of some of the brightest junior researchers in computer vision, contributes to the research community in general by drawing attention to an important aspect of graduate student development, potentially increases the number of active researchers and educators in STEM, and ensures that the computer vision community, through its recent graduates, makes fast advances in solving problems. <br/><br/>NSF support covers some of the costs for 20 selected US-based graduate students to attend the CVPR 2016 conference and the Doctoral Consortium associated with it. Participants and recipients of travel support are selected by the 2016 CVPR organizing committee, with the logistics managed by the PI. The Doctoral Consortium provides opportunities for graduate students to receive advice on their research work and career plans from experts from different institutions, and with potentially different perspectives, is often not available internally at one's own institution. This year's Doctoral Consortium features two new components which aim to address comments from last year's participants. These components are a panel discussion and more interaction with mentors. The Doctoral Consortium helps graduating students in computer vision find the best career options for their talents and achieve their potential, thus benefiting the computer vision field and society as a whole."
"1629700","CI-NEW: Collaborative Research: COVE-Computer Vision Exchange for Data, Annotations and Tools","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","08/15/2016","08/08/2016","Kate Saenko","MA","Trustees of Boston University","Standard Grant","Jie Yang","07/31/2020","$206,000.00","","saenko@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","7359","7359","$0.00","The project provides discoverability, low overhead for use, reproducibility of research, and persistence for computer vision data.  The project is hence setting a direction toward which the computer vision community can collectively work in creating a dataset infrastructure that allows for transparency across individual datasets and annotations, experimental benchmarks with community-set corpora and metrics, and a web-based infrastructure to cultivate continued development of computer vision datasets.  The availability of such an infrastructure, which is named COVE: Computer Vision Exchange of Data, Annotations and Tools, impacts the computer vision and related communities to develop next generation robust intelligence capabilities that have great potential to positively impact society. The project is integrated with education by supporting graduate and undergraduate students, and reaches middle school students through outreach activities.<br/><br/>The project is establishing COVE, a centralized community-run infrastructure to support the exchange of data and annotations as well as the software tools to manipulate them.  The infrastructure is web-based open-source, and provides open access to its contents. Stewardship over the contents are managed by the Investigators initially and subsequently through elected members of the computer vision community. There are two salient components of the infrastructure. First, a curation infrastructure facilitates back-end storage, querying, data annotation and curation tools, to support it.  To curate the federated data set, COVE uses widely known open-source tools like Python, Bootstrap and Postgresql.  For curation of new annotations to incorporate into the exchange, the project relies heavily on crowd-sourcing.  Second, a usage infrastructure, e.g., data structures and software enables widespread and easy use by researchers and practitioners.  The project develops APIs to allow for easy programmable access to the federated data sets and tools through common software interfaces like Matlab and OpenCV."
"1542439","RET Site: Research Experiences for Teachers in Computer Vision and Bio-Medical Imaging","CNS","RES EXP FOR TEACHERS(RET)-SITE","01/01/2016","08/11/2015","Mubarak Shah","FL","The University of Central Florida Board of Trustees","Standard Grant","Allyson Kennedy","12/31/2019","$600,000.00","Niels da Vitoria Lobo","shah@eecs.ucf.edu","4000 CNTRL FLORIDA BLVD","Orlando","FL","328168005","4078230387","CSE","1359","1359, 7434","$0.00","This funding establishes a new Research Experience for Teachers (RET) Site at the University of Central Florida (UCF). The primary objective of this RET site is to engage high school teachers in summer research experiences in computer vision and bio-medical engineering.  The teachers will develop classroom modules and materials which will be implemented in their classrooms during the academic year.  The UCF RET team has an outstanding track record of achievements in computer vision research, education, outreach, and K-12 educational initiatives, including hosting the longest continually active CISE Research Experiences for Undergraduates Site. The team plans to provide research and professional development opportunities to high school teachers from the greater Orlando area. They particularly focus on teachers teaching Pre-Calculus, AP Calculus, AP Computer Science, AP Statistics, AP Physics, and AP or IB Biology, and the teachers interested in promoting Science Fair activities in their schools. The project will develop a community of teachers who are passionate about computer vision and imaging and who can translated this excitement to their students through engaging, high-quality inquiry learning experiences.<br/><br/>The project is anchored by a research area this is compelling and exciting for teachers and K-12 students and by an exceptional faculty team that has demonstrated expertise in both research and K-12 outreach.  The site features projects that are teacher accessible as well as connected to current research and practice.  The RET site includes sound evaluation and dissemination plans that may provide models for integrating computing and computer vision fundamentals and research into advanced high school subjects, filling an important disciplinary gap.   The project will provide professional development to teachers from a local metropolitan region. Teachers will develop and hone their research, communication, and presentation skills, all of which are essential to their professional growth and success. The project will build the technical capacity of teachers so they are capable of developing and implementing new, exciting inquiry-based learning activities at their schools. The project team will disseminate the project materials through the TeachEngineering digital library and build a local Curriculum Web that will be shared with a wider audience of teachers and annual workshops. The team will also disseminate project results through high-quality journals and professional meetings."
"1644606","EAGER: Converting Print Dictionaries to Machine-Interpretable Format","IIS","ROBUST INTELLIGENCE, DEL","09/15/2016","06/28/2016","Michael Maxwell","MD","University of Maryland College Park","Standard Grant","Donald T. Langendoen","02/28/2018","$74,816.00","","mmaxwell@casl.umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","7495, 7719","7495, 7719, 7916","$0.00","A dictionary documents the building blocks of a language -- its words and idiomatic phrases, with descriptions of their pronunciations, grammatical properties, meanings and uses -- and is an essential component of language documentation, together with a reference grammar and transcribed texts and recordings. Until recently, dictionaries were compiled and organized by hand, entered into some kind of typesetting system, and finally rendered in print form for use by scholars and language learners. Contemporary dictionaries are now compiled and organized electronically so that the information they contain can be used not only to produce stand-alone print artifacts, but also be integrated with the other components to ensure greater accuracy of the documentation as a whole, enable updates to be produced at regular intervals, and support the development of natural-language processing tools for the languages that are documented in this way. The goal of this exploratory project is to develop methods for machines to understand the implicit structure of the hundreds of extant print dictionaries of endangered and other low-resource languages as a critical first step in enabling their documentation to be of maximal usefulness to future generations.<br/><br/><br/>Print dictionaries use ordering, typeface and other formatting conventions to indicate the intended structure of dictionary entries. The first task of this project is to use optical character reading (OCR) software to convert those entries to machine-interpretable form so as to preserve the original formatting. The second is to develop software to convert the corrected OCR output into structured, machine-interpretable archive-standard formats. Because print formats vary widely across dictionaries, human intervention is required to inform the software about how to translate the implicit representations for a particular dictionary's entries into explicit ones. But such manual annotation is only required for a small part of the dictionary, as the formatting conventions are consistent across all of its entries, and once learned can be used to identify and correct errors and inconsistencies, and enable automated editing tasks like updating orthographies. The tool will be developed, tested and evaluated using print dictionaries of two indigenous languages of Latin America that were produced in the latter part of the twentieth century. This project is jointly supported by the Documenting Endangered Languages Program in the Behavioral and Cognitive Sciences Division and by the Robust Intelligence Program in the Information and Intelligent Systems Division."
"1618227","RI: Small: Collaborative Research: Structured Inference for Low-Level Vision","IIS","Robust Intelligence","07/01/2016","06/07/2016","Todd Zickler","MA","Harvard University","Standard Grant","Jie Yang","06/30/2020","$305,000.00","","zickler@eecs.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","7495","7495, 7923","$0.00","Vision is a valuable sensing modality because it is versatile. It lets humans navigate through unfamiliar environments, discover assets, grasp and manipulate tools, react to projectiles, track targets through clutter, interpret body language, and recognize familiar objects and people. This versatility stems from low-level visual processes that somehow produce, from ambiguous retinal measurements, useful intermediate representations of depth, surface orientation, motion, and other intrinsic scene properties. This project establishes a mathematical and computational foundation for similar low-level processing in machines. The key challenge it addresses is how to usefully encode and exploit the fact that, visually, the world exhibits substantial intrinsic structure. By advancing understanding of low-level vision in machines, this project makes progress toward computer vision systems that can compare to vision in humans, in terms of accuracy, reliability, speed, and power-efficiency.<br/><br/>This research revisits low-level vision, and develops a comprehensive framework that possesses a common abstraction for information from different optical cues; the ability to encode scene structure across large regions and at multiple scales; implementation as parallel and distributed processing; and large-scale end-to-end learnability. The project approaches low-level vision as a structured prediction task, with ambiguous local predictions from many overlapping receptive fields being combined to produce a consistent global scene map that spans the visual field. The structured prediction models are different from those used for categorical tasks such as semantic segmentation, because they are specifically designed to accommodate the distinctive requirements and properties of low-level vision: continuous-valued output spaces; ambiguities that may form equiprobable manifolds; extreme scale variations; and global scene maps with higher-order piecewise smoothness. By strengthening the computational foundations of low-level vision, this project strives to enable many kinds of vision systems that are more efficient and more versatile, and it strives to have impacts across the breadth of computer vision."
"1618021","RI: Small: Collaborative Research: Structured Inference for Low-Level Vision","IIS","ROBUST INTELLIGENCE","07/01/2016","06/07/2016","Ayan Chakrabarti","IL","Toyota Technological Institute at Chicago","Standard Grant","Jie Yang","01/31/2018","$194,612.00","","ayan@wustl.edu","6045 S Kenwood Ave","Chicago","IL","606372803","7738340409","CSE","7495","7495, 7923","$0.00","Vision is a valuable sensing modality because it is versatile. It lets humans navigate through unfamiliar environments, discover assets, grasp and manipulate tools, react to projectiles, track targets through clutter, interpret body language, and recognize familiar objects and people. This versatility stems from low-level visual processes that somehow produce, from ambiguous retinal measurements, useful intermediate representations of depth, surface orientation, motion, and other intrinsic scene properties. This project establishes a mathematical and computational foundation for similar low-level processing in machines. The key challenge it addresses is how to usefully encode and exploit the fact that, visually, the world exhibits substantial intrinsic structure. By advancing understanding of low-level vision in machines, this project makes progress toward computer vision systems that can compare to vision in humans, in terms of accuracy, reliability, speed, and power-efficiency.<br/><br/>This research revisits low-level vision, and develops a comprehensive framework that possesses a common abstraction for information from different optical cues; the ability to encode scene structure across large regions and at multiple scales; implementation as parallel and distributed processing; and large-scale end-to-end learnability. The project approaches low-level vision as a structured prediction task, with ambiguous local predictions from many overlapping receptive fields being combined to produce a consistent global scene map that spans the visual field. The structured prediction models are different from those used for categorical tasks such as semantic segmentation, because they are specifically designed to accommodate the distinctive requirements and properties of low-level vision: continuous-valued output spaces; ambiguities that may form equiprobable manifolds; extreme scale variations; and global scene maps with higher-order piecewise smoothness. By strengthening the computational foundations of low-level vision, this project strives to enable many kinds of vision systems that are more efficient and more versatile, and it strives to have impacts across the breadth of computer vision."
"1611295","CCSS: Programmable Mixed-Signal Vision Sensor for Continuous Mobile Vision","ECCS","CCSS-Comms Circuits & Sens Sys","07/01/2016","06/02/2016","Lin Zhong","TX","William Marsh Rice University","Standard Grant","Lawrence Goldberg","06/30/2019","$350,000.00","","lin.zhong@yale.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","ENG","7564","153E","$0.00","The emergence of wearable devices has made it possible for computers to continuously interpret the user environment, or continuous mobile vision. It can extend a user's memory and attention, not only enabling previously impossible, personalized services but also assisting people with vision or attention impairment in an unprecedented way.  While modern devices are capable of capturing and interpreting what their users see, they face a daunting physical barrier: energy efficiency. For example, performing continuous vision workloads drains the battery of Google Glass in about 40 minutes. While process technology and system-level optimization techniques may continue to improve the energy efficiency of digital circuits, a recent measurement study has pointed to a fundamental bottleneck to energy efficiency of computer vision: the image sensor, especially its analog readout circuitry.  The goal of this project is to tackle this bottleneck by designing, prototyping and evaluating a novel vision sensor architecture along with its optimization framework. By targeting computer vision, this vision sensor architecture radically departs from existing image sensor designs that are optimized for photography. Instead of producing high-quality images, it outputs application-specific features by judiciously shifting processing into the analog domain. In doing so, it promises better efficiency by orders of magnitudes for computer vision workloads and relieves the privacy concern with continuous mobile vision.<br/><br/>This project will pursue two complementary, interrelated directions toward the above goal: First, the mixed-signal vision sensor design must provide sufficient programmability under constrained complexity in the analog domain. The project will exploit a novel hardware architecture that cyclically reuses analog modules for a programmable dataflow. This architecture will employ a novel column-based topology that exploits data locality to reduce interconnect complexity for data access. The project will also investigate hardware mechanisms that allow programmable tradeoffs between efficiency and accuracy of processing in the analog domain. Second, vision workloads must be carefully partitioned into analog and digital stages given the sensor architecture. The project shall provide an optimization framework that leverages accurate energy models and noise tolerance of vision workload. The project will further contribute novel use cases of the proposed mixed-signal vision sensor with data privacy, energy consumption, and task performance being the possible optimization constraints."
"1663755","EAGER: Enabling Discovery and Scientific Collaboration on Human Memory via the Web-Based Atlas and Tissue Bank for Patient H.M.'s Brain","BCS","COGNEURO","08/22/2016","10/14/2016","Jacopo Annese","VA","George Mason University","Standard Grant","Uri Hasson","08/31/2017","$300,000.00","","drjannese@gmail.com","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","SBE","1699","1699, 7916, 8089, 8091","$0.00","Knowledge of a specific neural network supporting memory function in the human brain stems from the case of patient H.M. who, in 1953 underwent an experimental medial temporal lobectomy in the hope of reducing the frequency and severity of his epileptic seizures. The operation was successful in that respect, but it unexpectedly left him incapable of creating new memories. For more than five decades, H.M. participated in hundreds of experiments and his case was discussed in thousands of scientific publications. His brain contained the clues to understand how memory works; however, determining with precision which structures were damaged was not possible because even the latest neuroimaging could not clearly resolve the anatomy of the temporal lobes. With support from the National Science Foundation, Dr. Jacopo Annese will complete an 'open source', web-based microscopic atlas of H.M.'s brain which was donated to science post-mortem. The tools-embedded atlas will support the creation of teaching curricula that will expose students to raw neuroimaging data from multiple modalities, cutting edge brain mapping algorithms, web-based exploration tools, all within the clinical and biographical context of H.M. as an individual. The cyber infrastructure created through this project is expected to enable discovery neuroscience by participants world wide.  <br/><br/>Specifically, Dr. Annese and his research team will (1) provide a dedicated support infrastructure to maintain and manage the web atlas for H.M.'s brain; (2) significantly increase the accuracy of the atlas by increasing the number of digitized histological slices to achieve 1 mm per slice interval (from 3mm interval); (3) acquire and deliver image stacks to enable remote quantitative studies; (4) implement new web tools to enable the handling of remote request and curation of results from different laboratories; (5) convert the images into formats that can be 3-D printed using consumer products.   Such cyber infrastructure will make the valuable H.M. data available for new retrospective studies that may further change our current view of how memory is established in the human brain and enable quantitative analyses at the cellular level using a 'virtual microscope'.  The resulting atlas will be used by researchers worldwide to re-interpret, based on clear anatomical evidence, the results from hundreds of neuropsychological exams conducted when H.M. was alive."
"1548308","SBIR Phase I:  Software application for conversion of text-heavy documents into interactive diagram-based documents","IIP","SMALL BUSINESS PHASE I","01/01/2016","11/17/2015","Ray Emrani","CA","VizCommunication","Standard Grant","Ben Schrag","12/31/2016","$150,000.00","","ray@vizcommunication.com","735 Marzella Ave.","Los Angeles","CA","900492043","3109633919","ENG","5371","5371, 8031, 8032, 8039","$0.00","This SBIR Phase I project addresses reading and learning difficulties faced by many learners in school, college, and the workplace. The goal of the project is to build a web application that helps individuals and organizations convert text-heavy documents (such as online textbooks, manuals, and courses) into interactive diagram-based documents. The greatest potential benefit will be for students who have difficulty learning from long paragraphs of text ?including visual learners, English Language Learners, and students with dyslexia or ADHD. The users of the app will be consumers of educational content (students and online learners) and content creators (authors, publishers, test prep companies, and corporate training departments). The theoretical foundation of this proposed research comes from studies that show the learning benefits of graphic organizers in a wide range of disciplines, including science, engineering, humanities, and social studies. This proposal will support the nation's need for a skilled workforce by helping to increase the number of students who graduate from high school, college, and post-secondary vocational programs and by improving the learning outcomes for employer training and retraining programs to meet changing business needs. The commercial impact is expected to be strong, generating income for tax revenue and creating jobs. <br/><br/>The technical innovation of the project is a methodology that provides a well-defined sequence of steps for semi-automatic conversion of complex text-based documents to interactive diagram-based documents. Fields of knowledge to be drawn upon to build the web application include instructional design, information visualization, text mining, and natural language processing. The major technical hurdles to be addressed are accurate identification and extraction of content from a source document, displaying the content using interactive visualization techniques, and integration of the converter with the Company's prototypes of a viewer and an authoring tool (built using HTML5, CSS3, and JavaScript). One way that the methodology lowers the risks involved in the conversion process is to separate the text-mining steps (relatively low risk) from the natural language processing steps (relatively high risk). If successful, the converter will dramatically reduce the time and cost for individuals and organizations to convert existing educational content into documents that are more accessible for diverse learners. The scope of this proposal is limited to the conversion of digital textbooks that are available as open educational resources. Future research will build on this methodology to convert other types of documents (e.g. manuals and online courses) and file formats."
"1551589","INT: Collaborative Research: Detecting, Predicting and Remediating Student Affect and Grit Using Computer Vision","IIS","Cyberlearn & Future Learn Tech","09/01/2016","06/10/2020","Beverly Woolf","MA","University of Massachusetts Amherst","Standard Grant","Amy Baylor","08/31/2021","$1,023,389.00","Tom Murray","bev@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","8020","8045, 8233, 9251","$0.00","The Cyberlearning and Future Learning Technologies Program funds efforts that support envisioning the future of learning technologies and advance what we know about how people learn in technology-rich environments.  Integration (INT) projects refine and study emerging genres of learning technologies that have already undergone several years of iterative refinement in the context of rigorous research on how people learn with such technologies; INT projects contribute to our understanding of how the prototype tools might generalize to a larger category of learning technologies. This INT project integrates prior work from two well-developed NSF-sponsored projects on (i) advanced computer vision and (ii) affect detection in intelligent tutoring systems.  The latter work in particular developed instruments to detect student emotion (interest, confusion, frustration and boredom) and showed that when a computer tutor responded to negative student affect, learning performance improved. The current project will expand this focus beyond emotion to attempt to also detect persistence, self-efficacy, and the trait called 'grit.' The project will measure the impact of these constructs on student learning and explore whether the grit trait (a persistent tendency towards sustained initiative and interest) can be improved and whether and how it depends on other recently experienced emotions. The technological innovation enabling this research into the genre of broadly affectively aware instruction is Smartutors, a tool that uses advanced computer vision techniques to view a student's gaze, hand gestures, head, and face to increase the ""bandwidth"" for automatically detecting their affect. One goal is to reorient students to more productive attitudes once waning attention is recognized.<br/><br/>This research team brings together a unique blend of leading interdisciplinary researchers in computer vision; adaptive education technology and computer science; mathematics education; learning companions; and meta-cognition, emotion, self-efficacy and motivation. Nine experiments will provide valuable data to extend and validate existing models of grit and emotion. In particular, the team will gather fine-grained data on grit, assess the impact of tutor interventions in real-time, and contribute thereby to a theory of grit. Visual data of student behavior will be integrated with advanced analytics of log data of students' actions based on the behavior of over 10,000 prior students (e.g., hint requests, topic mastery) to provide individualized guidance and tutor responses in a timely fashion. This will allow the researchers to measure the impact of interventions on student performance and attitude, and it will uncover how grit levels relate to emotion and what impact emotions and grit combined have on overall student initiative. By identifying interventions that are sensitive to individual differences, this research will refine theories of motivation and emotion and will reveal principles about how to respond to student grit and affect, especially when attention and persistence begin to wane. To ensure classroom success, the PIs will evaluate Smartutors with 1,600 students and explore its transferability by testing it in a more difficult mathematics domainwith older students."
"1551590","INT: Collaborative Research: Detecting, Predicting and Remediating Student Affect and Grit Using Computer Vision","IIS","Cyberlearn & Future Learn Tech","09/01/2016","05/16/2019","John Magee","MA","Clark University","Standard Grant","Amy Baylor","08/31/2021","$166,720.00","","jmagee@clarku.edu","950 MAIN ST","WORCESTER","MA","016101400","5084213835","CSE","8020","063Z, 8045, 8233, 9251","$0.00","The Cyberlearning and Future Learning Technologies Program funds efforts that support envisioning the future of learning technologies and advance what we know about how people learn in technology-rich environments.  Integration (INT) projects refine and study emerging genres of learning technologies that have already undergone several years of iterative refinement in the context of rigorous research on how people learn with such technologies; INT projects contribute to our understanding of how the prototype tools might generalize to a larger category of learning technologies. This INT project integrates prior work from two well-developed NSF-sponsored projects on (i) advanced computer vision and (ii) affect detection in intelligent tutoring systems.  The latter work in particular developed instruments to detect student emotion (interest, confusion, frustration and boredom) and showed that when a computer tutor responded to negative student affect, learning performance improved. The current project will expand this focus beyond emotion to attempt to also detect persistence, self-efficacy, and the trait called 'grit.' The project will measure the impact of these constructs on student learning and explore whether the grit trait (a persistent tendency towards sustained initiative and interest) can be improved and whether and how it depends on other recently experienced emotions. The technological innovation enabling this research into the genre of broadly affectively aware instruction is Smartutors, a tool that uses advanced computer vision techniques to view a student's gaze, hand gestures, head, and face to increase the ""bandwidth"" for automatically detecting their affect. One goal is to reorient students to more productive attitudes once waning attention is recognized.<br/><br/>This research team brings together a unique blend of leading interdisciplinary researchers in computer vision; adaptive education technology and computer science; mathematics education; learning companions; and meta-cognition, emotion, self-efficacy and motivation. Nine experiments will provide valuable data to extend and validate existing models of grit and emotion. In particular, the team will gather fine-grained data on grit, assess the impact of tutor interventions in real-time, and contribute thereby to a theory of grit. Visual data of student behavior will be integrated with advanced analytics of log data of students' actions based on the behavior of over 10,000 prior students (e.g., hint requests, topic mastery) to provide individualized guidance and tutor responses in a timely fashion. This will allow the researchers to measure the impact of interventions on student performance and attitude, and it will uncover how grit levels relate to emotion and what impact emotions and grit combined have on overall student initiative. By identifying interventions that are sensitive to individual differences, this research will refine theories of motivation and emotion and will reveal principles about how to respond to student grit and affect, especially when attention and persistence begin to wane. To ensure classroom success, the PIs will evaluate Smartutors with 1,600 students and explore its transferability by testing it in a more difficult mathematics domainwith older students."
"1551572","INT:  Collaborative Research: Detecting, Predicting and Remediating Student Affect and Grit Using Computer Vision","IIS","Cyberlearn & Future Learn Tech","09/01/2016","08/26/2016","Margrit Betke","MA","Trustees of Boston University","Standard Grant","Amy Baylor","08/31/2021","$614,990.00","Stan Sclaroff","betke@cs.bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","8020","8045, 8233","$0.00","The Cyberlearning and Future Learning Technologies Program funds efforts that support envisioning the future of learning technologies and advance what we know about how people learn in technology-rich environments.  Integration (INT) projects refine and study emerging genres of learning technologies that have already undergone several years of iterative refinement in the context of rigorous research on how people learn with such technologies; INT projects contribute to our understanding of how the prototype tools might generalize to a larger category of learning technologies. This INT project integrates prior work from two well-developed NSF-sponsored projects on (i) advanced computer vision and (ii) affect detection in intelligent tutoring systems.  The latter work in particular developed instruments to detect student emotion (interest, confusion, frustration and boredom) and showed that when a computer tutor responded to negative student affect, learning performance improved. The current project will expand this focus beyond emotion to attempt to also detect persistence, self-efficacy, and the trait called 'grit.' The project will measure the impact of these constructs on student learning and explore whether the grit trait (a persistent tendency towards sustained initiative and interest) can be improved and whether and how it depends on other recently experienced emotions. The technological innovation enabling this research into the genre of broadly affectively aware instruction is Smartutors, a tool that uses advanced computer vision techniques to view a student's gaze, hand gestures, head, and face to increase the ""bandwidth"" for automatically detecting their affect. One goal is to reorient students to more productive attitudes once waning attention is recognized.<br/><br/>This research team brings together a unique blend of leading interdisciplinary researchers in computer vision; adaptive education technology and computer science; mathematics education; learning companions; and meta-cognition, emotion, self-efficacy and motivation. Nine experiments will provide valuable data to extend and validate existing models of grit and emotion. In particular, the team will gather fine-grained data on grit, assess the impact of tutor interventions in real-time, and contribute thereby to a theory of grit. Visual data of student behavior will be integrated with advanced analytics of log data of students' actions based on the behavior of over 10,000 prior students (e.g., hint requests, topic mastery) to provide individualized guidance and tutor responses in a timely fashion. This will allow the researchers to measure the impact of interventions on student performance and attitude, and it will uncover how grit levels relate to emotion and what impact emotions and grit combined have on overall student initiative. By identifying interventions that are sensitive to individual differences, this research will refine theories of motivation and emotion and will reveal principles about how to respond to student grit and affect, especially when attention and persistence begin to wane. To ensure classroom success, the PIs will evaluate Smartutors with 1,600 students and explore its transferability by testing it in a more difficult mathematics domainwith older students."
"1551594","INT: Collaborative Research: Detecting, Predicting and Remediating Student Affect and Grit Using Computer Vision","IIS","Cyberlearn & Future Learn Tech","09/01/2016","11/06/2018","Ivon Arroyo","MA","Worcester Polytechnic Institute","Standard Grant","Amy Baylor","08/31/2021","$759,887.00","Jacob Whitehill","iarroyo@wpi.edu","100 INSTITUTE RD","WORCESTER","MA","016092247","5088315000","CSE","8020","7218, 8045, 8233","$0.00","The Cyberlearning and Future Learning Technologies Program funds efforts that support envisioning the future of learning technologies and advance what we know about how people learn in technology-rich environments.  Integration (INT) projects refine and study emerging genres of learning technologies that have already undergone several years of iterative refinement in the context of rigorous research on how people learn with such technologies; INT projects contribute to our understanding of how the prototype tools might generalize to a larger category of learning technologies. This INT project integrates prior work from two well-developed NSF-sponsored projects on (i) advanced computer vision and (ii) affect detection in intelligent tutoring systems.  The latter work in particular developed instruments to detect student emotion (interest, confusion, frustration and boredom) and showed that when a computer tutor responded to negative student affect, learning performance improved. The current project will expand this focus beyond emotion to attempt to also detect persistence, self-efficacy, and the trait called 'grit.' The project will measure the impact of these constructs on student learning and explore whether the grit trait (a persistent tendency towards sustained initiative and interest) can be improved and whether and how it depends on other recently experienced emotions. The technological innovation enabling this research into the genre of broadly affectively aware instruction is Smartutors, a tool that uses advanced computer vision techniques to view a student's gaze, hand gestures, head, and face to increase the ""bandwidth"" for automatically detecting their affect. One goal is to reorient students to more productive attitudes once waning attention is recognized.<br/><br/>This research team brings together a unique blend of leading interdisciplinary researchers in computer vision; adaptive education technology and computer science; mathematics education; learning companions; and meta-cognition, emotion, self-efficacy and motivation. Nine experiments will provide valuable data to extend and validate existing models of grit and emotion. In particular, the team will gather fine-grained data on grit, assess the impact of tutor interventions in real-time, and contribute thereby to a theory of grit. Visual data of student behavior will be integrated with advanced analytics of log data of students' actions based on the behavior of over 10,000 prior students (e.g., hint requests, topic mastery) to provide individualized guidance and tutor responses in a timely fashion. This will allow the researchers to measure the impact of interventions on student performance and attitude, and it will uncover how grit levels relate to emotion and what impact emotions and grit combined have on overall student initiative. By identifying interventions that are sensitive to individual differences, this research will refine theories of motivation and emotion and will reveal principles about how to respond to student grit and affect, especially when attention and persistence begin to wane. To ensure classroom success, the PIs will evaluate Smartutors with 1,600 students and explore its transferability by testing it in a more difficult mathematics domainwith older students."
"1617953","RI: Small: Supervised Descent Method and its Applications to Computer Vision (and Beyond)","IIS","Robust Intelligence","09/01/2016","06/07/2016","Fernando De la Torre","PA","Carnegie-Mellon University","Standard Grant","Jie Yang","08/31/2021","$449,715.00","","ftorre@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7495","7495, 7923","$0.00","This project develops a fast optimization strategy for continuous and possibly combinatorial optimization problems. Optimization is a fundamental problem in many scientific disciplines from biology, physics and statistics to computer graphics and computer vision. This project focuses on solving 2 dimensional (2D) and 3 dimensional (3D) image alignment problems in computer vision. Solving the correspondence between 2D and 3D images is an open research problem and it is a fundamental component in most computer vision systems for medical imaging, surveillance, advanced driver assistance systems, mobile robots, augmented reality, object recognition and aerial video exploration, among other applications. The project integrates the research with education, and involves undergraduate/graduate students in the research.<br/> <br/>This research addresses two main issues with second order descent methods in optimization: (1) the objective function to optimize might not be analytically differentiable and numerical approximations are impractical, and (2) the Hessian might be large and not positive definite. The research team advocates the concept of learning generic descent maps (i.e., average ""descent directions"") in a supervised manner. Using generic descent maps, the research team derives a practical algorithm, Supervised Descent Method (SDM), for minimizing Nonlinear Least Squares (NLS) problems with continuous parameters. During training, SDM learns a sequence of decent maps that minimize the NLS objective. During testing, these learned descent maps are used to minimize the NLS objective without requiring computation of the expensive Jacobian or Hessian. Beyond NLS, the research team explores the use of SDM to optimize combinatorial optimization problems such as finding 2D and 3D correspondences in images and point-clouds."
"1626008","MRI: Development of an Observatory for Quantitative Analysis of Collective Behavior in Animals","CNS","Major Research Instrumentation","10/01/2016","09/16/2016","Kostas Daniilidis","PA","University of Pennsylvania","Standard Grant","Rita Rodriguez","09/30/2019","$339,174.00","Daniel Lee, Marc F. Schmidt, Danielle Bassett, Jianbo Shi","kostas@cis.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","CSE","1189","1189","$0.00","This project, developing a new instrument to enable an accurate quantitative analysis of the movement of animals and vocal expressions in real world scenes, aims to facilitate innovative research in the study of animal behavior and neuroscience in complex realistic environments. While much progress has been made investigating brain mechanisms of behavior, these have been limited primarily to studying individual subjects in relatively simple settings. For many social species, including humans, understanding neurobiological processes within the confines of these more complex environments is critical because their brains have evolved to perceive and evaluate signals within a social context. Indeed, today's advances in video capture hardware and storage and in algorithms in computer vision and network science make this facilitation with animals possible. Past work has relied on subjective and time-consuming observations from video streams, which suffer from imprecision, low dimensionality, and the limitations of the expert analyst's sensory discriminability. This instrument will not only automate the process of detecting behaviors but also provide an exact numeric characterization in time and space for each individual in the social group. While not explicitly part of the instrument, the quantitative description provided by our system will allow the ability to correlate social context with neural measurements, a task that may only be accomplished when sufficient spatiotemporal precision has been achieved.<br/><br/>The instrument enables research in the behavioral and neural sciences and development of novel algorithms in computer vision and network theory. In the behavioral sciences, the instrumentation allows the generation of network models of social behavior in small groups of animals or humans that can be used to ask questions that can range from how the dynamics of the networks influence sexual selection, reproductive success, and even health messaging to how vocal decision making in individuals gives rise to social dominance hierarchies. In the neural sciences, the precise spatio-temporal information the system would provide can be used to evaluate the neural bases of sensory processing and behavioral decision under precisely defined social contexts. Sensory responses to a given vocal stimulus, for example, can be evaluated by the context in which the animal heard the stimulus and both his and the sender's prior behavioral history in the group. In computer vision, we propose novel approaches for the calibration of multiple cameras ""in the wild"", the combination of appearance and geometry for the extraction of exact 3D pose and body parts from video, the learning of attentional focus among animals in a group, and the estimation of sound source and the classification of vocalizations. New approaches will be used on hierarchical discovery of behaviors in graphs, the incorporation of interactions beyond the pairwise level with simplicial complices, and a novel theory of graph dynamics for the temporal evolution of social behavior. The instrumentation benefits behavioral and neural scientists. Therefore, the code and algorithms developed will be open-source so that the scientific community can extend them based on the application. The proposed work also impacts computer vision and network science because the fundamental algorithms designed should advance the state of the art. For performance evaluation of other computer vision algorithms, established datasets will be employed."
"1647200","US Ignite: Focus Area 1: Predictable Wireless Networking and Collaborative 3D Reconstruction for Real-Time Augmented Vision","CNS","CISE Research Resources","10/01/2016","09/01/2016","Hongwei Zhang","MI","Wayne State University","Standard Grant","John Brassil","03/31/2018","$600,000.00","Jing Hua, Jayanthi Rao, Anthony Holt","hongwei@iastate.edu","5057 Woodward","Detroit","MI","482023622","3135772424","CSE","2890","015Z","$0.00","Eliminating the line-of-sight constraint of human vision and machine vision, the developed network systems foundations of predictable wireless networked and 3D reconstruction will enable 'see-through vision' which will transform the ways humans and engineered systems interact with environments and thus have far-reaching impact on domains such as road transportation, public safety, and disaster response. This project develops the network systems foundation for a vehicle equipped with sensors and an augmented reality display to indicate the presence of other nearby vehicles hidden by obstacles.  In collaboration with Wayne State University (WSU) police and Ford Research and leveraging the WSU living lab and the OpenXC open-source platform for connected vehicles, the project will take an integrated approach to the research, deployment, and dissemination of the wireless network systems for see-through vision. This project proposes a cross-layer framework for addressing physical-domain uncertainties and the interdependencies between wireless networking and 3D reconstruction, and it develops novel algorithms for predictable wireless networking and real-time wireless networked 3D reconstruction. Using the developed network system, this project will develop a see-through vision application for human-driving. The wireless networked see-through vision system will be deployed in the WSU police patrol vehicles, and the project team will outreach to the Detroit and State of Michigan police as well as open-source communities for broad adoption and deployment of the see-through vision system.<br/><br/>With the bold objective of eliminating the line-of-sight constraint of human vision and machine vision, this project addresses wireless networking and 3D reconstruction challenges in a holistic cross-layer framework. By integrating research investigation with systems development and deployment, this project will make the following significant contributions: 1) Effectively leveraging multi-scale physical structures of traffic flows, the multi-scale approach to resource management in vehicular wireless networks not only ensures predictable vehicular wireless networking, it also transforms fundamental challenges of vehicular networks to ones similar to those of mostly-immobile networks, thus enabling the exploration of fundamental, generically-applicable principles and mechanisms for predictable wireless networking; 2) the multi-scale approach to joint scheduling, channel assignment, power control, and rate control enables predictable control of per-packet transmission reliability in the presence of fast-varying network and environmental conditions such as wireless channel attenuation, internal and external interference, data traffic dynamics, and vehicle mobility; 3) the real-time scheduling algorithm enables controllable exploration of real-time capacity regions for system-level optimization; 4) the collaborative 3D reconstruction model integrates visual sensors in a divide-and-conquer fashion, and it enhances the capability of networked vision as well as its robustness to physical uncertainties; 5) the co-design of collaborative 3D reconstruction and wireless networking permits adaptive communication capacity allocation to optimize the quality of 3D reconstruction; 6) the attention-aware see-through vision application creates a new research field of vision augmentation by uniquely integrating computer vision and computer graphics research and by proposing a practical solution for displaying augmented 3D vision."
"1612580","Workshop on Ontology Learning for Behavioral Knowledge Embeddedness","IIS","Smart and Connected Health","07/01/2016","06/17/2016","Kai Larsen","CO","University of Colorado at Boulder","Standard Grant","Wendy Nilsen","06/30/2018","$49,038.00","Michael Paul","kai.larsen@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","8018","7556, 8018","$0.00","Title: IIS: Workshop on Ontology Learning for Behavioral Knowledge Embeddedness<br/>PI: Kai Larsen  <br/><br/>The Workshop on Ontology Learning for Behavioral Knowledge Embeddedness is designed to bring together experts in natural language technologies, computer and behavioral science to develop semantic ontology using two methods: 1) an initial data extraction of relationships between theories and constructs using natural language technologies; and 2) bringing together a multidisciplinary team to clarify the conceptualizations. Such an ontology can be used to clarify the conceptualizations and vocabulary that underlie knowledge and allow the field to grow on a shared vocabulary. The workshop will yield a transdisciplinary knowledge base that will enable researchers to better search, integrate, and understand past behavioral research. <br/><br/>Current research in behavior relies on a plethora of theories and constructs. Past research has suggested that there is significant overlap in these terms and that analysis of extant behavioral theories may provide a much-needed ontology of behavioral knowledge-embeddedness and -integration, itself the necessary requirement to clarify the structure of knowledge within the large set of behavioral theories in existence. Without such an ontology, clarifying the conceptualizations and vocabulary that underlie knowledge is not possible. Beyond developing an ontology, a transdisciplinary knowledge base will enable researchers to better utilize past research. The major goals of this proposed workshop are to 1) evaluate the current state of disparate ontological foundations for behavioral theory efforts, 2) facilitate the development of shared understanding of ontology development and learning from experts in the fields of natural language processing, visualization, and computational ontologies, and 3) advance the rigor of theory-integrative work towards ontology-worthy efforts."
"1624782","I/UCRC: University of Florida Planning Grant: I/UCRC for Big Learning","CNS","IUCRC-Indust-Univ Coop Res Ctr","07/01/2016","08/10/2016","Xiaolin Li","FL","University of Florida","Standard Grant","Dmitri Perkins","12/31/2017","$15,000.00","Jose Principe","andyli@ece.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","CSE","5761","5761","$0.00","This project proposes to establish the NSF I/UCR Center for Big Learning (CBL). The mission of CBL is to pioneer in large-scale deep learning algorithms, systems, and applications through unified and coordinated efforts in the CBL consortium. The vision of CBL is to create intelligence enablers towards intelligence-driven society. With the explosive big data generated from natural systems, engineered systems, and human activities, we need intelligent algorithms and systems to facilitate our decision making with distilled insights automatically at scale. The proposed CBL center is a timely initiative as our society moves towards intelligence-enabled world of opportunities. The CBL consortium is expected to become the magnet of deep learning research and applications and attract leading researchers, enthusiastic entrepreneurs, IT and industry giants working together on accomplishing the promising mission and vision. This planning grant will lead to a successful proposal for the establishment of the NSF I/UCR Center for Big Learning with a solid consortium across multiple campuses and a large number of industry partners. <br/><br/>CBL has the following broader impacts. (1) Making significant contributions and impacts to the deep learning community on pioneering research and applications to address a broad spectrum of real-world challenges. (2) Making significant contributions and impacts to promote products and services of industry in general and our members in particular. (3) Making significant contributions and impacts to the urgently-needed education of our next-generation talents with real-world settings and world-class mentors from both academia and industry. (4) Our meetings, forums, conferences, and planned training sessions will greatly promote and broaden the research and materialization of DL. <br/><br/><br/><br/>The proposed project aims to establish the NSF I/UCR Center for Big Learning (CBL). With dramatic breakthroughs in multiple modalities of challenges (e.g., image, video, speech, text, and Q&A), the renaissance of machine intelligence is looming.The mission of CBL is to pioneer in large-scale deep learning (DL)  algorithms, systems, and applications through unified and coordinated efforts in the CBL consortium via fusion of broad expertise from our large number of faculty members, students, and industry partners. The vision of CBL is to create intelligence enablers towards intelligence-driven society. CBL possesses the pioneering intellectual merit in the following key research themes. (1) Novel algorithms. This theme focuses on novel DL algorithms and architectures, such as deep architecture, complex deep neural networks, brain-inspired components, optimization, deep reinforcement learning, and unsupervised learning. (2) Novel systems. We propose novel architectures, resource management, and software frameworks for enabling large-scale DL platforms and applications on desktops, mobiles, clusters, and clouds. (3) Novel applications in health, mobile/IoT, and surveillance. During the planning phase, we will establish a solid center strategic plan, marketing plan, and the CBL consortium that consists of four academic sites and a large number of industrial members. <br/><br/>CBL has the following broader impacts. (1) Making significant contributions and impacts to the deep learning community on pioneering research and applications to address a broad spectrum of real-world challenges. (2) Making significant contributions and impacts to promote products and services of industry in general and our members in particular. (3) Making significant contributions and impacts to the urgently-needed education of our next-generation talents with real-world settings and world-class mentors from both academia and industry. (4) Our meetings, forums, conferences, and planned training sessions will greatly promote and broaden the research and materialization of DL."
"1702440","III: Small: Collaborative Research: Automatically Generating Contextually-Relevant Visualizations","IIS","Info Integration & Informatics","08/01/2016","01/10/2017","Brent Hecht","IL","Northwestern University","Standard Grant","Maria Zemankova","08/31/2018","$92,858.00","","bhecht@northwestern.edu","750 N. Lake Shore Drive","Chicago","IL","606114579","3125037955","CSE","7364","7364, 7453, 7923","$0.00","Information visualizations, such as charts and maps, can greatly enhance news articles by adding context, helping the reader understand complex facts, aiding in decision making, and making information more memorable. Unfortunately, creating good news visualizations is a difficult and labor-intensive task that involves numerous complex decisions. A designer must identify data relevant to the article, clean the data, generate the visualization (a complex process on its own), and provide annotations to connect the article and visualization. While some design guidelines have been developed, many decisions are based on designer intuition, a process that is not scalable to the thousands of news articles that are published every day. This project seeks to build intelligent tools to help designers more quickly create good news visualizations and to develop systems that generate news visualizations autonomously. This research project will enhance citizen understanding of complex information in the news and improve numerical, graphical, and geography literacy. Additionally, the research will provide support for new job categories (e.g., data scientists, computational journalists, data analysts, etc.) and existing companies (e.g., online media, search engines, etc.) in their evolution to new interactive platforms. The research results will be integrated into a broad set of widely accessible educational materials for a variety of courses (visualization, spatial computing, and text analysis) and will serve as research and practical training for undergraduates, graduates, and professionals. <br/><br/>Providing a scalable solution to automatically generating contextually-relevant visualizations requires the understanding and encoding of the design process. Specifically, the goals of this project are (a) identifying the decision process of visualization designers, (b) creating automated components that operationalize these decisions including text processing, searching through a wide range of heterogeneous data sources and datasets (e.g. census data, stock market data, government macroeconomic data), and automatic visualization construction and annotation, and (c) ranking of the visualizations based on well-known quantitative metrics from information retrieval and information visualization such as relevance, expressiveness, and effectiveness. By extracting key comparisons from an article's text through the use of natural language processing and using existing visualization-article pairs as an evaluation corpus, the system will ensure that relevant datasets are found and that the selected visual forms preserve and enhance the information conveyed in the article. For example, the system will automatically create thematic maps for geospatial comparisons of population change in the U.S. and time series for longitudinal comparisons of company financial results. Although the focus of this work in on the news domain, the research can be extended to other application areas including textbooks, internal company reports, and more generally, to any texts that implicitly or explicitly correspond to quantitative data. Further information, including source code, demos, papers, and datasets, are available at the project homepage (http://txt2vis.cond.org)."
"1654137","EAGER: Improving Protocol Vulnerability Discovery via Semantic Interpretation of Textual Specifications","CNS","Secure &Trustworthy Cyberspace","09/01/2016","08/16/2016","Cristina Nita-Rotaru","MA","Northeastern University","Standard Grant","Phillip Regalia","08/31/2017","$160,000.00","Dan Goldwasser","c.nitarotaru@neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","8060","7434, 7916, 9102","$0.00","Two methods used for vulnerability discovery in network protocols are testing and a semi-automated technique called model checking.  Testing and model checking implementations of network protocols is a tedious and time-consuming task, where significant manual effort goes into designing test cases and protocol property specifications. Both approaches require detailed and structured information about the tested protocols, in the form of messages, state machine, invariants, etc. Most of the time this information is derived manually by people with different levels of expertise. The process can be made more effective and less expensive by leveraging documentation and specification about these protocols and available in text format. Automatically analyzing the information available in documentations in the form of textual specification will open new avenues not only for improving vulnerability finding for network protocols, but for software design in general. <br/><br/>This project combines expertise from natural language processing and network security to create and build a framework for vulnerability discovery in network protocols, by leveraging semantic interpretation of textual specification, automated attack generation and injection, and property model checking for software implementations.  The framework consists of two phases, a knowledge building phase and a vulnerability finding phase. In the knowledge building phase,  semantic interpretation natural language processing techniques is applied to structured text (protocol specifications and documentation) and unstructured text (blogs, forums, and bug reports) to learn structured information about protocols such as: message formats, protocol state machine, constraints, etc.  In the second phase, the information learned in the knowledge phase is applied to two mechanisms for vulnerability finding, the first uses the structured protocol information to create and inject attacks, and the second uses the same information to derive protocol requirements and use them to model check finite state machines extracted from protocol implementations."
"1618810","RI: Small: CompCog: Pique: A Cognitive Model of Curiosity for Personalizing Sequences of Learning Resources","IIS","Robust Intelligence","06/15/2016","05/22/2018","Kazjon Grace","NC","University of North Carolina at Charlotte","Standard Grant","James Donlon","12/31/2020","$449,419.00","Mary Lou Maher","k.grace@uncc.edu","9201 University City Boulevard","CHARLOTTE","NC","282230001","7046871888","CSE","7495","7495, 7923","$0.00","One of the most significant challenges in education is to simultaneously provide personalization and scale.  How can each learner in an online class of hundreds or thousands be provided with knowledge and challenges that suit them personally?  This project will develop an AI system for personalized learning that is inspired by cognitive models of curiosity, creativity and intrinsic motivation. Pique (short for the ""Personalized Curiosity Engine"") is based on understanding what makes an individual learner curious, and then recommending resources that will stimulate their curiosity.  Pique's cognitive model of its learner uses natural language processing techniques to figure out what sequences of resources will be familiar enough to be accessible, but sufficiently new to not be boring.  <br/><br/>Pique is a novel cognitive system drawing on technologies from intelligent tutoring systems, computational creativity, and natural language processing.  Its key contribution is combining a cognitive model of curiosity with educational recommender systems. We will evaluate the effectiveness of Pique first with simulations and then with students at a large comprehensive public university.  Evaluation will take the form of a comparison between the full Pique system and a modified version with its cognitive model of curiosity disabled.  This will enable us to determine whether recommending resources that are simultaneously curiosity-stimulating and fit to the task is more effective than recommending resources that are just fit to the task. Given the interdisciplinary nature of this research we will disseminate our results broadly, including to the educational technology, cognitive systems, information retrieval, and computational creativity communities."
"1633036","BIGDATA: IA: Hype Cycles of Scientific Innovation","IIS","S&CC: Smart & Connected Commun, Big Data Science &Engineering","09/01/2016","09/08/2016","Daniel McFarland","CA","Stanford University","Standard Grant","Sara Kiesler","08/31/2020","$1,253,468.00","Daniel Jurafsky","mcfarland@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","033Y, 8083","042Z, 7433, 8033, 8083","$0.00","Scientific discovery is a collective process based on collaboration, assessment and consensus. But the enormous expansion of scientific research makes it difficult to tell which intellectual efforts forge collective advances.  Better models for identifying and tracking scientific movements from the vast collections of articles, books, grants, and patents that compromise scientific and academic work is crucial to improving our nation's ability to make advances in science and industry, helping policy makers, funding agencies, and venture capitalists as well as scientists and scholars themselves.  Our project studies innovation by collecting and analyzing texts in vast collections of scientific and other scholarly articles, books, grants and patents. By looking at the subtle patterns of language and how they change over time, we can describe and predict where and when collective trends of knowledge innovation emerge and decline; which scientific ideas and movements result in translational knowledge key to industry and health care; and what the key drivers are for such collective intellectual movements.  This work is helping identify where the potential is greatest for innovation, which fields are most primed and receptive to the arrival of new discoveries, and the times and places where resources have the greatest influence.<br/><br/>The project is based on a large dataset of texts the researchers have compiled on US research activity from 1990-2016, including scientific articles, grants, and patents, and by disambiguating and linking mentions of individual people across these datasets.  This project uses topic modeling and other natural language processing algorithms to identify distinct intellectual movements in these corpora by drawing on the sub-languages that characterize them, also applying methods to validate and evaluate these movements. This allows the researchers to model the trajectories of movements over time with tools like latent growth mixture modeling and k-spectral clustering, identifying the mechanisms associated with each movement, how they rise and fall over time, and how they translate into industrial or health applications. For example, the project studies how the trajectory of intellectual movements is a function of their environment and competing research efforts, and show the ways in which it depends on the timing and magnitude of key resources (e.g., money, recognition, new recruits and trainees, social networks of support, or the coherence of the knowledge itself). In such a fashion, this work is an important first step in unraveling the recipe for innovation as a collective, episodic process."
"1661374","CAREER: Visual Question Answering (VQA)","IIS","Robust Intelligence","10/01/2016","05/07/2020","Devi Parikh","GA","Georgia Tech Research Corporation","Continuing Grant","Jie Yang","07/31/2021","$517,022.00","","parikh@vt.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7495","1045, 7495","$0.00","This project addresses the problem of Visual Question Answering (VQA). Given an image and a free-form natural language question about the image (e.g., ""What kind of store is this?"", ""How many people are waiting in the queue?"", ""Is it safe to cross the street?""), the machine's task is to automatically produce a concise, accurate, free-form, natural language answer (""bakery"", ""5"", ""Yes""). VQA is directly applicable to a variety of applications of high societal impact that involve humans eliciting situationally-relevant information from visual data; where humans and machines must collaborate to extract information from pictures. Examples include aiding visually-impaired users in understanding their surroundings, analysts in making decisions based on large quantities of surveillance, and interacting with a robot. This project has the potential to fundamentally improve the way visually-impaired users live their daily lives, and revolutionize how society at large interacts with visual data. <br/><br/>This research enables that VQA represents not a single narrowly-defined problem (e.g., image classification) but rather a rich spectrum of semantic scene understanding problems and associated research directions. Each question in VQA may lie at a different point on this spectrum: from questions that directly map to existing well-studied computer-vision problems (""What is this room called?"" = indoor scene recognition) all the way to questions that require an integrated approach of vision (scene), language (semantics), and reasoning (understanding) over a knowledge base (""Does the pizza in the back row next to the bottle of Coke seem vegetarian?""). Consequently, this work maps to a sequence of waypoints along this spectrum. Motivated by addressing VQA from a variety of perspectives, this research program is generating new datasets, knowledge, and techniques in (i) pure computer vision (ii) integrating vision + language (iii) integrating vision + language + common sense (iv) building interpretable models and (v) combining a portfolio of methods. In addition, novel contributions are being made to (a) training the machine to be curious and actively ask questions to learn (b) using VQA as a modality to learn more about the visual world than what existing annotation modalities allow and (c) training the machine to know what it knows and what it does not."
"1635885","The Impacts of Narratives-based Risk Communication on Hazard Preparedness","CMMI","HDBE-Humans, Disasters, and th","09/15/2016","08/24/2016","Elizabeth Shanahan","MT","Montana State University","Standard Grant","Walter Peacock","08/31/2020","$549,983.00","Eric Raile, Jamie McEvoy, Clemente Izurieta, Geoffrey Poole","shanahan@montana.edu","309 MONTANA HALL","BOZEMAN","MT","597172470","4069942381","ENG","1638","041E, 042E, 043E, 9102, 9150","$0.00","Local hazard preparedness is vital to avoid disaster in the face of extreme events. Whereas conventional risk communication relies on scientific information to affect hazard preparedness, such technical information is often ineffectively assimilated into people's risk perceptions and decisions. Hazard preparedness is largely shaped by factors such as cultural values, cognitive biases, affect, knowledge, information, and experiences, all of which are communicated through stories that people construct and recount to one another. This research will test the effects of an innovative narrative-based risk communication strategy that locates science hazard information in locally produced hazard narratives. Effectively connecting scientific information to individual risk perceptions and decisions through co-produced risk narratives potentially offers an innovative way to improve hazard preparedness that could translate across hazard issues more broadly. With the project focus on flooding events, this research will draw upon expertise in social behavioral sciences, hydrology, and computer science. <br/>   <br/>This interdisciplinary research effort focuses on testing whether a co-produced, narrative-based risk communication approach is more effective than conventional risk communication at improving hazard preparedness (defined as risk perception and decisions). The first objective is to develop co-produced risk narratives that are both scientifically accurate and locally relevant. Using a community-based participatory research (CBPR) approach, baseline data of flood risk narratives will be collected from river communities to ascertain narrative elements as identified in the Narrative Policy Framework (NPF). Narrative elements include subsets of narrative structure (e.g., use of characters, plot) and narrative content (who is cast in the roles of hero, villain, victim). Whereas narrative structures are stable, content varies across narratives. The characters cast in the narrative and their associated actions are formative in constructing different notions of reality and consequent decisions. Natural Language Processing computational techniques will then be used to identify key narrative content from the CBPR data to obtain the best set of combinations of narrative structure and narrative content situated in local language and images. Subsequently, researchers will quantify, explain, and depict sources of hydrologic uncertainty (data, model, and natural uncertainty) associated with flood frequency analysis in 100-year flood maps for each community. This resulting information will be embedded into the algorithmically enhanced CBPR-based risk narratives to create locally relevant and scientifically accurate flood risk narratives. These risk narratives will be returned to the CBPR groups for adjustment and validation. The second objective focuses on testing the effects of these co-produced risk narratives as narrative treatments on hazard preparedness (i.e., risk perception and intended decisions) with an experimental survey design across the larger population in these river communities. Differently constructed co-produced narratives (i.e., hero-focused, victim-focused, hero & victim focused narratives) will be used as treatments to test the extent to which they influence hazard preparedness in contrast to a non-narrative science statement and a control condition of no treatment. The findings are expected to provide insight into the power of narratives in communicating hazard risk and affecting hazard preparedness. The outcomes are expected to be useful for those local and federal entities involved in hazard preparedness strategies."
"1641340","CAREER: Representing, Understanding, and Enhancing Scenes at the Internet-Scale","IIS","GRAPHICS & VISUALIZATION","03/01/2016","05/16/2016","James Hays","GA","Georgia Tech Research Corporation","Continuing grant","Ephraim P. Glinert","12/31/2017","$95,303.00","","hays@gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7453","1045, 7453","$0.00","CAREER: Understanding, Representing, and Enhancing Scenes at the Internet-scale<br/><br/>Photography has an enormous impact on society -- it is our primary visual history and a medium for storytelling, entertainment, and art. But our visual world is extraordinarily complex which makes it difficult for computer vision to understand photos and for computer graphics to synthesize visual content. However, the emergence of Internet-scale photo collections in recent years enables new research directions. We use scene-based representations to leverage Internet-scale data. Scenes (places or environments) are the context in which all other visual phenomena exist and it seems possible to brute-force the space of scenes -- with millions of scenes, we find qualitatively similar scenes and create massively data-driven algorithms with capabilities that are complementary to typical bottom-up graphics and vision pipelines. The underlying principle of this study is that joint investigations of scene representations and large image databases will advance the state-of-the-art in graphics and vision. <br/><br/>First, we are investigating detail synthesis tasks which alleviate camera shake, motion blur, defocus, atmospheric scattering, or low resolution. Scene representations are robust enough to find matching scenes in Internet-scale photo collections even in the presence of dramatic blurring. These matching scenes provide a context-specific statistical model which can be used to insert convincing texture and object detail. Second, we are studying attribute-based representations of scenes. We use crowdsourcing to discover attributes and build large databases for the community. Attributes are a powerful intermediate representation for the next generation of big data imaging research which can have broad societal impact through applications such as robotics, security, assistance to vision-impaired, and vehicle safety. The investigators also are developing a new introductory course for Brown students to explore big data computing across scientific disciplines and are creating an online community for visual computing education to benefit students interested in photography and programming."
"1552377","CAREER: Visual Question Answering (VQA)","IIS","Robust Intelligence","08/01/2016","12/31/2015","Devi Parikh","VA","Virginia Polytechnic Institute and State University","Continuing grant","Jie Yang","01/31/2017","$104,201.00","","parikh@vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","CSE","7495","1045, 7495","$0.00","This project addresses the problem of Visual Question Answering (VQA). Given an image and a free-form natural language question about the image (e.g., ""What kind of store is this?"", ""How many people are waiting in the queue?"", ""Is it safe to cross the street?""), the machine's task is to automatically produce a concise, accurate, free-form, natural language answer (""bakery"", ""5"", ""Yes""). VQA is directly applicable to a variety of applications of high societal impact that involve humans eliciting situationally-relevant information from visual data; where humans and machines must collaborate to extract information from pictures. Examples include aiding visually-impaired users in understanding their surroundings, analysts in making decisions based on large quantities of surveillance, and interacting with a robot. This project has the potential to fundamentally improve the way visually-impaired users live their daily lives, and revolutionize how society at large interacts with visual data. <br/><br/>This research enables that VQA represents not a single narrowly-defined problem (e.g., image classification) but rather a rich spectrum of semantic scene understanding problems and associated research directions. Each question in VQA may lie at a different point on this spectrum: from questions that directly map to existing well-studied computer-vision problems (""What is this room called?"" = indoor scene recognition) all the way to questions that require an integrated approach of vision (scene), language (semantics), and reasoning (understanding) over a knowledge base (""Does the pizza in the back row next to the bottle of Coke seem vegetarian?""). Consequently, this work maps to a sequence of waypoints along this spectrum. Motivated by addressing VQA from a variety of perspectives, this research program is generating new datasets, knowledge, and techniques in (i) pure computer vision (ii) integrating vision + language (iii) integrating vision + language + common sense (iv) building interpretable models and (v) combining a portfolio of methods. In addition, novel contributions are being made to (a) training the machine to be curious and actively ask questions to learn (b) using VQA as a modality to learn more about the visual world than what existing annotation modalities allow and (c) training the machine to know what it knows and what it does not."
"1549697","STTR Phase I:  SunDIAL: Slot DIscovery And Linking","IIP","STTR PHASE I","01/01/2016","02/08/2017","James Kukla","MD","RedShred","Standard Grant","Peter Atherton","08/31/2017","$247,500.00","Zareen Syed","jmk@redshred.com","5520 Research Park Drive","Baltimore","MD","212284851","4438046865","ENG","1505","1505, 163E, 8032","$0.00","The broader impact/commercial potential of this Small Business Technology Transfer (STTR) Phase I project results from it improving the accessibility of documents and providing a powerful tool for consumers and businesses to enable routing, categorizing, and understanding of documents. By producing summaries of opportunity documents for business developers, SunDIAL will accelerate pipeline and business development decisions for businesses of all sizes through automatically generated, SaaS-delivered structured summaries. SunDIAL will advance the ability of practical information extraction systems by generating summaries from any text document. Other broader impacts and benefits include furthering the technology to empower individuals to get the gist quickly of formal documents such as consumer credit contracts, health insurance policies, complex industry request for proposals, grants and other difficult-to-read documents. A successful outcome will allow users to quickly assess documents through infobox displays, triage documents on mobile devices, write rules to sort or filter them automatically, and review key information before ever opening the document.<br/><br/>This Small Business Technology Transfer (STTR) Phase I project will discover unsupervised and unrestricted slots and fillers (attributes and values) to construct structured summaries based on keywords and hidden patterns in document collections. SunDIAL goes beyond the state of the art by not requiring a manually crafted catalogue of slots and complements supervised approaches by discovering new slots. While conventional Natural Language Processing (NLP) approaches are effective on well-formed sentences, the techniques described here are effective for semi-structured content such as section headers, lists and tables. Furthermore, NLP based approaches for fact extraction and text summarization are primarily lexical, requiring further processing for disambiguating and linking to unique entities and concepts in a knowledge base. SunDIAL further advances the state of the art by eliminating these steps as it identifies keywords and links to knowledge base concepts as a first step in the discovery process. Linking concepts to a knowledge base provides the additional advantage that the terms can be explicitly mapped to semantic concepts in other ontologies. SunDIAL will lead to advances in knowledge discovery, and advance the methodologies for information retrieval, information extraction, slot filling, and knowledge-base population."
"1650900","EAGER: The Virtual Assistant Health Coach: Summarization and Assessment of Goal-Setting Dialogues","IIS","Smart and Connected Health","09/01/2016","05/03/2017","Brian Ziebart","IL","University of Illinois at Chicago","Standard Grant","Tatiana Korelsky","08/31/2018","$307,925.00","Ben Gerber, Lisa Sharp, Barbara DiEugenio, Bing Liu","bziebart@uic.edu","809 S. Marshfield Avenue","Chicago","IL","606124305","3129962862","CSE","8018","7916, 8018, 9251","$0.00","Health coaching is an effective process for improving poor health behaviors by providing education on health-related topics, setting personalized and realizable health-related goals, monitoring and encouraging progress towards those goals, and sequencing or refining a progression of health goals over time. Though useful, its highly personalized and labor-intensive nature makes the cost of effective health coaching prohibitive for many underserved populations that could benefit significantly from it. This EArly Grant for Exploratory Research (EAGER) seeks to conduct an exploratory investigation of the technical feasibility of creating a virtual health coaching system that learns from expert demonstration to interact with patients via the smart message system (SMS). The project develops two key initial components: a health goal summarization component that extracts details of proposed and agreed upon health goals from SMS dialogues between participant and health coach; and a health goal assessment component that estimates the suitability of proposed health goals in terms of the five dimensions of goal setting used by the coaching experts: Specificity, Measurability, Attainability, Relevance, and Timeliness (SMART).<br/><br/>This EAGER project develops techniques to interpret short, telegraphic, and often ungrammatical SMS messages and to extract details of health goals for improving physical activities established in those messages using natural language processing and structured prediction methods. It expands sentiment analysis methods to identify both noun and non-noun targets of emotion and uses these analyses in combination with inverse optimal control methods to assess the suitability of the sequence of proposed health goals over the course of the dialogue in terms of each of the five SMART dimensions. Evaluation of the developed methods is planned using a collected corpus of SMS-based communications between a health coach and participants annotated with tags relating to semantics, sentiments, and health goals. Successful development of these capabilities represent an important first step for realizing a virtual health coaching system that is able to provide personalized health coaching benefits of comparable quality to a human health coach."
"1622628","III: Travel Fellowships for Students from U.S. Universities to Attend ISWC 2016","IIS","Info Integration & Informatics","03/01/2016","03/01/2016","Amit Sheth","OH","Wright State University","Standard Grant","nan zhang","02/28/2017","$20,000.00","","amit@sc.edu","3640 Colonel Glenn Highway","Dayton","OH","454350001","9377752425","CSE","7364","7364, 7556","$0.00","This National Science Foundation award will fund Student Travel Fellowships for US students attending the 15th International Semantic Web Conference (ISWC 2016).  The conference will be held on October 17-21, 2016 in Kobe, Japan.  ISWC is the premier international forum for state-of-the-art research on all aspects of the Semantic Web - the next generation World Wide Web.  The Student Fellowships will help cover the travel costs for US students, making it possible for them to attend the conference and discuss and disseminate their work.  They will also provide an opportunity for them to interact with future national and international scientific collaborators.<br/><br/>The International Semantic Web Conference, which is now in its fifteenth year, is an interdisciplinary conference that includes work on:  Data Management, Natural Language Processing, Knowledge Representation and Reasoning, Ontologies and Ontology Languages, Semantic Web Engineering, Linked Data, User Interfaces and Applications.  It regularly has several hundred attendees.  In addition to the main technical tracks, the conference includes a variety of events that provide opportunities for deeper interaction amongst researchers at different institutions, at different stages of their research careers, and researchers who are interested in many different aspects of Semantic Web Research.  Students benefit from the Doctoral Consortium - a full day event where students can get critical, but encouraging, feedback on their work from senior members of the community.  They also benefit from the career mentoring lunch, where experienced members of the community from both academia and industry answer questions in an informal setting. <br/><br/>For further information see the conference web site at: http://iswc2016.semanticweb.org/"
"1742780","CAP: Advancing Technology and Practice for Learning Reading and Writing Skills in Secondary Science Education","IIS","Cyberlearn & Future Learn Tech","07/01/2016","04/23/2017","Rebecca Passonneau","PA","Pennsylvania State Univ University Park","Standard Grant","Tatiana Korelsky","06/30/2018","$23,264.00","","rjp49@cse.psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","8020","8045, 8055","$0.00","The proposed workshop brings together researchers and educators to discuss how to advance technology and practice to better promote secondary school science literacy. There is a clear need for new directions in science literacy. Over the past decade, the National Center for Education Statistics has consistently identified poor reading and writing skills as a serious problem, one which undoubtedly contributes to low science achievement. In addition, written and oral communication skills have been identified as a significant dimension of science practice and education. Secondary school, the focus of this workshop, is a critical period when students first experience a separation between subject matter and literacy. The resulting disciplinary silos may create discontinuities in students' learning of both science content and literacy skills. It may be advantageous for instruction in science and the English language arts to be brought closer together at the secondary level, and to facilitate this through digital learning environments. <br/><br/>Secondary science education has benefited from a range of digital technologies, including computer supported collaborative learning systems (CSCL) and intelligent tutoring systems (ITS).  The hypothesis motivating the workshop is that results from a decade of work in computer-based learning for science and inquiry can be applied to science literacy. One precondition is appropriate application of automated techniques to analyze the texts that students read and write, so as to provide students with tailored feedback in an online setting. A second precondition is to apply insights from the psychology of education on the acquisition of general literacy skills, and argumentation skills in particular, to foster evidence-based communication. The workshop includes researchers from the three critical areas of computer-based learning environments, natural language processing, and the psychology of education. It has three main goals. The first is to share results, datasets and demonstration systems in order to identify the potential for collaborations that would both deepen the analysis of existing data, and identify criteria for the collection of novel datasets that specifically address science literacy. The second workshop goal is to foster collaborations among researchers in the three disciplines, with participation from practitioners, to study the potential impact on students' learning of science literacy skills.  The third workshop goal is to identify mechanisms to foster an interdisciplinary community that will continue to investigate science literacy."
"1633509","Doctoral Dissertation Research: The Academic Cartography of Sugar Sweetened Beverages: Scientific and Technical Interdisciplinarity","SMA","SciSIP-DDRIG","09/01/2016","07/06/2016","Monica Gaughan","AZ","Arizona State University","Standard Grant","Cassidy Sugimoto","08/31/2019","$7,414.00","Lexi White","monica.gaughan@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","SBE","7009","7626, 9179","$0.00","This research investigates the interaction between scientific and legal academic publications to understand interdisciplinary communication and illuminate how science can inform and influence government policy through legal academic papers.  While many researchers have studied patterns in scientific articles, little research contemplates how legal academic articles fit into the publication enterprise.  Legal publications are housed in different academic databases, and are the product of a different publication structure primarily curated by law students.  Unlike scientific articles, many legal articles specifically target statutory, regulatory, and judicial policy.  These articles can and do influence government policy.  Additionally, articles on the same topic may discuss that topic in vastly different ways in legal versus scientific papers due to a difference in training and academic culture. <br/> <br/>This research looks at one specific topic 'sugar sweetened beverages' that is currently trending in both scientific and academic publications to serve as a case study.  It examines the network patterns of how scientific and legal articles cite one another across two academic publication databases (SCOPUS and LexisNexis).  In addition to looking at the citation patterns, this research also looks at the textual content of these articles using a natural language processing technique and creates a second network that maps the articles based on their content similarity.  By looking at the differences between the citation patterns and the textual content similarity of the articles, this research can identify papers and authors that straddle the boundaries between science and the law.  These boundary-spanning authors will then be interviewed about their research process.  These citation and content network maps, along with qualitative data about key interdisciplinary authors will explore a key area of interaction between scientific researchers and legal researchers. A better understanding of these relationships can lead to better communication between scientists, legal scholars, and even policy makers with improved impacts for scientific research."
"1637108","RIDIR: Collaborative Research: Computational and Historical Resources on Nations and Organizations for the Social Sciences (CHRONOS)","SMA","Data Infrastructure","09/01/2016","08/11/2016","Arthur Spirling","NY","New York University","Standard Grant","John Yellen","08/31/2019","$269,127.00","","arthur.spirling@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","SBE","8294","7433","$0.00","This project will collect, process, and analyze millions of U.S. government records concerning international relations, develop tools to explore these records, and make all of them available on a single website with an Application Programming Interface. The project will demonstrate how computational techniques can aid both qualitative and quantitative social science research on a range of areas of major public interest, expanding knowledge about terrorism, intelligence, international trade and aid. Among its broader impacts, it will improve the infrastructure available for multidisciplinary research and teaching, and also give citizens, journalists, and civil society organizations much better access to information about international relations. Participating student researchers will both learn about -- and contribute to -- practical methods to keep government transparent and accountable in the age of ""big data.""<br/><br/>The exponential growth in digitized or ""born digital"" documents will make such methods increasingly important in years to come. To meet the challenge, the team will draw on new work in Natural Language Processing, customize existing tools that turn text into data, and develop new tools for use with historical documents. They will employ Named Entity Recognition techniques to extract names of people, countries, and organizations, enabling users to track the absolute and relative frequency of mentions in large digital archives. Through Topic Modeling, they will summarize the thematic content and show how the most important subjects change over time. And Social Network extraction will allow them to reveal the informal relationships that shape policy from day-to-day. By bringing together all of this quantitative declassified data in a single platform, the project will advance work by political scientists as well as scholars of communications and social networks who seek to understand agenda-setting, power, and influence within and between organizations. The platform will enable researchers to test fundamental questions in IR theory, such as whether policymakers generally speak to one another in terms of ""state security,"" as realists insist, or ""international norms,"" as liberalism assumes. It will allow users to shift between different levels of analysis, from an aggregate view of whole archives, to filtered subsets of metadata, to the specific words in one sentence that produced a single data point. It will bring together quantitative and qualitative approaches to research in international relations, and make both more transparent, rigorous, and replicable."
"1566526","CRII: SaTC: Re-Envisioning Contextual Services and Mobile Privacy in the Era of Deep Learning","CNS","CRII CISE Research Initiation","07/01/2016","05/19/2016","Ting Wang","PA","Lehigh University","Standard Grant","Wei-Shinn Ku","06/30/2019","$168,683.00","","tbw5359@psu.edu","Alumni Building 27","Bethlehem","PA","180153005","6107583021","CSE","026Y","025Z, 8228","$0.00","Deep Learning (DL)-powered personalization holds great promise to fundamentally transform the way people live, work and travel, but poses high risk to people's individual privacy. This project will address the privacy risks arising in DL-powered contextual mobile services by developing solutions that facilitate the use of personal information while maintaining explicit user control over use of the information. The developed learning methods will enable learning from mobile devices in a manner flexible enough to enable current and future DL-powered contextual services, while maintaining explicit user control over how that information is used by third-party service providers.<br/><br/>This research will design and implement PADLOCK, a Privacy-Aware Deep Learning Of Contextual Knowledge engine. PADLOCK executes DL computation over users' personal data in a sandbox environment, while performing lightweight static and runtime analysis to ensure that mobile apps comply with users' privacy policies. The design of PADLOCK explores the tradeoff among privacy protection, communication cost, system overhead and service quality, providing solutions with different provable privacy and efficiency features for a wide range of contextual mobile services.  For further information see the project web site at: http://x-machine.github.io/project/padlock"
"1619123","CHS: Small: Translating Compilers for Visual Computing in Dynamic Languages","CCF","Software & Hardware Foundation","09/01/2016","12/12/2017","Baishakhi Ray","VA","University of Virginia Main Campus","Standard Grant","Anindya Banerjee","07/31/2019","$450,000.00","Westley Weimer, Baishakhi Ray","rayb@cs.columbia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","7798","7923, 7943","$0.00","This collaborative project is developing technologies to enable students, scientists, and other non-expert developers to use computer languages that facilitate rapid prototyping, and yet still automatically convert such programs to have high performance. In this research, the PI and co-PIs focus on programs that operate over visual data, such as programs in computer graphics, computer vision, and visualization. Visual data is important because visual datasets are rapidly growing in size, due to the use of cell-phone cameras, photo and video sharing online, and in scientific and medical imaging. The intellectual merits are that specialized program optimizations are being developed specifically for visual computing and for languages that enable rapid prototyping, alongside techniques that allow the computer to automatically search through different candidate optimizations and choose the fastest one. The project's broader significance and importance are that it will make the writing of computer programs that operate over visual datasets more accessible to novice programmers, make visual computing more accessible to a broader audience, permit faster research and development over visual programs, and make such programs themselves be more efficient.<br/><br/>More specifically, this research program is producing translating compilers that are specialized to handle programs that compute over visual data. The group led by the PI is researching new compilers that translate code from dynamic languages into highly efficient code in a target language. Dynamic languages are defined as those with a very dynamic run-time model, for example, MATLAB, Python, and Javascript. The target language is a language such as C that permits implementation of highly efficient programs. This research framework incorporates ideas from compilers, graphics, computer vision, visual perception, and formal and natural languages. The research will make a number of key intellectual contributions. First, new domain-specific translations and optimizations for visual computing will be formalized into manual rules that can be applied to any input program. Second, the team will research a novel approach of automatically learning translations, instead of using manually-coded rules. This can take the form of learning translation ""suggestions"" from humans, who can interactively suggest better output code. Third, a new search process based on offline auto-tuning will be used to select the translations that result in the fastest program. The success of the project will be verified against a comprehensive test suite of programs from computer vision and graphics."
"1601044","Research Initiation Award: Integrating Image and Text Information for   Biomedical Literature-Based Cross and  Multimodal Retrieval","HRD","Hist Black Colleges and Univ","06/01/2016","05/02/2016","Md Rahman","MD","Morgan State University","Standard Grant","Claudia Rankins","12/31/2020","$297,843.00","","md.rahman@morgan.edu","1700 East Cold Spring Lane","Baltimore","MD","212510002","4438853200","EHR","1594","9178","$0.00","The Historically Black Colleges and Universities-Undergraduate Program (HBCU-UP) Research Initiation Awards (RIAs) provide support to STEM junior faculty at HBCUs who are starting to build a research program, as well as for mid-career faculty who may have returned to the faculty ranks after holding an administrative post or who needs to redirect and rebuild a research program. Faculty members may pursue research at their home institution, at an NSF-funded Center, at a research intensive institution or at a national laboratory. The RIA projects are expected to help further the faculty member's research capability and effectiveness, to improve research and teaching at his or her home institution, and to involve undergraduate students in research experiences. With support from the National Science Foundation, Morgan State University will conduct research in information retrieval using search strategies based on techniques from image processing as well as natural language processing.  This would enable public access to both visual information and take away messages from journal articles.  This project will provide valuable research experience and mentorship for several minority undergraduate students at Morgan State University.  In addition, the project will help Morgan State University build its research capacity and enhance the educational and research experiences of their undergraduate students. <br/><br/>Within the larger goal of expanding queries for information retrieval, the project will 1) use a crowdsourcing based approach to perform large scale manual annotation of visual regions of interest (ROIs) by pairing automatically detected ROIs to concepts occurring in a brief caption, 2) use a feature learning approach to extract discriminative features from ROIs and automatically map the ROIs to concepts in an existing textual ontology, such as RadLex, 3) aided by a visual ontology, consider the semantic relations between the visual words when assessing the distance between images described with the bag-of-visual-words feature representation scheme, 4) in addition to cross modal search by mapping image regions to concepts in ontology, perform multimodal search by fusing weighted text and image features generated by a multi-response linear regression (MLR)-based meta-learner in a classification-driven task-specific manner, and 5) evaluate the retrieval techniques using benchmark and realistic datasets by participating in the yearly ImageCLEF retrieval evaluation campaign.  The labeled set of biomedical images with annotated regions of interest will be made available to the research community."
"1637159","RIDIR: Collaborative Research: Computational and Historical Resources on Nations and Organizations for the Social Sciences (CHRONOS)","SMA","Data Infrastructure","09/01/2016","09/11/2017","Robert Jervis","NY","Columbia University","Standard Grant","John Yellen","08/31/2020","$480,488.00","Owen Rambow, Matthew Connelly","rlj1@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","SBE","8294","7433","$0.00","This project will collect, process, and analyze millions of U.S. government records concerning international relations, develop tools to explore these records, and make all of them available on a single website with an Application Programming Interface. The project will demonstrate how computational techniques can aid both qualitative and quantitative social science research on a range of areas of major public interest, expanding knowledge about terrorism, intelligence, international trade and aid. Among its broader impacts, it will improve the infrastructure available for multidisciplinary research and teaching, and also give citizens, journalists, and civil society organizations much better access to information about international relations. Participating student researchers will both learn about -- and contribute to -- practical methods to keep government transparent and accountable in the age of ""big data.""<br/><br/>The exponential growth in digitized or ""born digital"" documents will make such methods increasingly important in years to come. To meet the challenge, the team will draw on new work in Natural Language Processing, customize existing tools that turn text into data, and develop new tools for use with historical documents. They will employ Named Entity Recognition techniques to extract names of people, countries, and organizations, enabling users to track the absolute and relative frequency of mentions in large digital archives. Through Topic Modeling, they will summarize the thematic content and show how the most important subjects change over time. And Social Network extraction will allow them to reveal the informal relationships that shape policy from day-to-day. By bringing together all of this quantitative declassified data in a single platform, the project will advance work by political scientists as well as scholars of communications and social networks who seek to understand agenda-setting, power, and influence within and between organizations. The platform will enable researchers to test fundamental questions in IR theory, such as whether policymakers generally speak to one another in terms of ""state security,"" as realists insist, or ""international norms,"" as liberalism assumes. It will allow users to shift between different levels of analysis, from an aggregate view of whole archives, to filtered subsets of metadata, to the specific words in one sentence that produced a single data point. It will bring together quantitative and qualitative approaches to research in international relations, and make both more transparent, rigorous, and replicable."
"1632266","SBIR Phase II:  Large-scale Creative Thinking Assessment for the Workforce","IIP","SMALL BUSINESS PHASE II","08/01/2016","10/12/2017","Farzad Eskafi","CA","Sparkting","Standard Grant","Rajesh Mehta","07/31/2018","$760,000.00","","feskafi@gmail.com","1501 ROSE ST STE 203","Berkeley","CA","947031008","5108212610","ENG","5373","5373, 8031, 8032, 8039, 8240","$0.00","This SBIR phase II project is focused on quantifying and developing one's creative thinking abilities.  In more details, this project will develop modules that measures one's domain-base and subject-base creativity and validate the modules using statistical validation techniques. Creative talent will remain the great differentiator in the coming world, and the organizations that thrive there will do so because they have the right potentials.  Using the semantics-based psychometric approach, this project designs exercises and models that assess one's creativity. This project will help employers in acquiring creative employees and also train and educate current employees in ways by which they can become more creative.  The market is vast, encompassing potentially all employers who hire and manage workers with symbol-analytic skills.  According to the 2010 U.S Census, there are over 143M people employed in the workforce. This project will expand to various sectors of the workforce using strategic partnerships with distribution channels in workforce education companies, and talent/human capital management companies.   <br/><br/><br/>Currently, creative thinking strategies and assessments are taught by experts and consultants in the form of workshops.  Such semantic creativity assessment is a manual, time-consuming and expensive process.  This project creates a set of open-ended exercises and evaluates each exercise based on the four creative-thinking dimensions: 1) Originality:  Original thinking capacity and ability to generate novel and out-of-the-box solutions; 2) Fluency: Ideation capacity and ability to push past the first set of known responses; 3)Flexibility: Divergent-thinking capacity and ability to think non-linearly; and 4) Elaboration: Detailed-oriented and ability to provide intrinsic details about each possible solution and response. This project automates such testing by using advances in semantic-based psychometric modelling, natural language processing (NLP), semantic networks from computational linguistics and computational power for statistical mining of large corpora.  Due to ease of scalability, this solution is not limited to any one country or region. The solution can be deployed world-wide. Currently, this project has gathered data from 161 different countries around the world.  This project is the first step towards automation of open-ended exercises in various fields and contexts such as situational judgement assessments, emotional intelligence and motivational assessments."
"1541545","Collaborative Research: Building a Comprehensive Evolutionary History of Flagellate Plants","DEB","GoLife, Systematics & Biodiversity Sci","01/01/2016","01/30/2020","Matt von Konrat","IL","Field Museum of Natural History","Standard Grant","Katharina Dittmar","12/31/2020","$64,721.00","Eve Gaus","mvonkonrat@fieldmuseum.org","1400 S LAKE SHORE DR","CHICAGO","IL","606052827","3126657240","BIO","6133, 7374","6133, 7218, 9169, 9178, 9251, EGCH","$0.00","For the first ~300 million years of plant life on land, Earth's flora consisted entirely of flagellate plants, which today include approximately 30,000 species of bryophytes, lycophytes, ferns, and gymnosperms. Numerous major innovations, including stomata, vascular tissue, roots and leaves, woody stems, and seeds, evolved first in flagellate plant ancestors. The flagellate plants not only provide a window to the early evolution of these critical features, but are represented today by vibrant and diverse lineages that contribute substantially to global ecology, particularly via contributions to global carbon and nitrogen cycles and offering critical habitats for biodiversity. This research project will analyze molecular genetic variation among the 18,000 available flagellate plant species in order to create a species-level phylogeny (evolutionary tree of species relationships) for the entire group. Phylogenetic trees generated will be linked to fossil data, a comprehensive dataset of plant traits, and available data on species occurrence on the planet. The results of this project will inform investigations into the origins of features characteristic of flowering plants and provide a platform for student training and outreach.<br/><br/>This project will improve our understanding of the history and relationships of the flagellate plants by using new sequencing technologies to examine 500 nuclear loci and to produce a species-level phylogeny for these taxa that is linked to and integrated with an immense and varied amount of data on fossils, phenomic characters, and geospatial distributions. Phenotypic datasets will utilize a natural language processing tool which will be enhanced and tested to handle large datasets and the geospatial data will allow exploration of world-wide patterns of diversity and endemism. All data will be fully integrated into ongoing projects such as Open Tree of Life and Next Generation Phenomics. Education experts will develop an online educational tool for training the next generation of biodiversity scientists by providing an accessible framework for using the project data in university classrooms while promoting evidence-based teaching practices. A MicroPlants citizen science project will promote scientific literacy and plant awareness in the general public, through museums and schools."
"1541509","Collaborative Research: Building a Comprehensive Evolutionary History of Flagellate Plants","DEB","GoLife","01/01/2016","08/27/2015","Hong Cui","AZ","University of Arizona","Standard Grant","Katharina Dittmar","12/31/2019","$53,198.00","","hongcui@email.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","BIO","6133","6133, 9169, EGCH","$0.00","For the first ~300 million years of plant life on land, Earth's flora consisted entirely of flagellate plants, which today include approximately 30,000 species of bryophytes, lycophytes, ferns, and gymnosperms. Numerous major innovations, including stomata, vascular tissue, roots and leaves, woody stems, and seeds, evolved first in flagellate plant ancestors. The flagellate plants not only provide a window to the early evolution of these critical features, but are represented today by vibrant and diverse lineages that contribute substantially to global ecology, particularly via contributions to global carbon and nitrogen cycles and offering critical habitats for biodiversity. This research project will analyze molecular genetic variation among the 18,000 available flagellate plant species in order to create a species-level phylogeny (evolutionary tree of species relationships) for the entire group. Phylogenetic trees generated will be linked to fossil data, a comprehensive dataset of plant traits, and available data on species occurrence on the planet. The results of this project will inform investigations into the origins of features characteristic of flowering plants and provide a platform for student training and outreach.<br/><br/>This project will improve our understanding of the history and relationships of the flagellate plants by using new sequencing technologies to examine 500 nuclear loci and to produce a species-level phylogeny for these taxa that is linked to and integrated with an immense and varied amount of data on fossils, phenomic characters, and geospatial distributions. Phenotypic datasets will utilize a natural language processing tool which will be enhanced and tested to handle large datasets and the geospatial data will allow exploration of world-wide patterns of diversity and endemism. All data will be fully integrated into ongoing projects such as Open Tree of Life and Next Generation Phenomics. Education experts will develop an online educational tool for training the next generation of biodiversity scientists by providing an accessible framework for using the project data in university classrooms while promoting evidence-based teaching practices. A MicroPlants citizen science project will promote scientific literacy and plant awareness in the general public, through museums and schools."
"1739012","EXP: Linguistic Analysis and a Hybrid Human-Automatic Coach for Improving Math Identity","IIS","Cyberlearn & Future Learn Tech","09/01/2016","12/14/2017","Jaclyn Ocumpaugh","PA","University of Pennsylvania","Standard Grant","Maria Zemankova","08/31/2020","$556,047.00","Ryan Baker, Scott Crossley, Victor Kostyuk, Matthew Labrum","jlocumpaugh@gmail.com","Research Services","Philadelphia","PA","191046205","2158987293","CSE","8020","8045, 8841, 9251","$0.00","The Cyberlearning and Future Learning Technologies Program funds efforts that support envisioning the future of learning technologies and advance what we know about how people learn in technology-rich environments. Cyberlearning Exploration (EXP) Projects explore the viability of new kinds of learning technologies by designing and building new kinds of learning technologies and studying their possibilities for fostering learning and challenges to using them effectively. This project addresses the effect of students' social identity on learning, an important factor in math and science education. Specifically, it will advance the scientific understanding of math identity (i.e. ""I'm (not) a math person"") by studying the over 100,000 diverse students who use Reasoning Mind, a blended learning system for K-8 mathematics with demonstrated results. Reasoning Mind supports math identity with an innovative design that allows students to email an animated character (aka the Genie) and receive a human-crafted response. This study will show how math identity manifests and changes during students' use of Reasoning Mind in order to inform software designers and classroom teachers on best practices for encouraging math identity. This will have the broader impact of strengthening our nation's ability to supply science and technology fields with a well-trained workforce.<br/><br/>This study examines two components of math identity: self-efficacy and interest in mathematics. Both self-efficacy and interest can be enhanced by curricula that are individualized to appropriately challenge each student (a strength of educational technology), but social stereotypes (i.e. ""Girls aren't good in math"") may decrease math identity or otherwise interfere with its development. This study investigates how educational technology can reverse these trends. In the first phase of this study, a combination of survey methods, Natural Language Processing (NLP), and Educational Data Mining (EDM) techniques will be used to identify how poor and/or changing math identity emerges in the linguistic patterns of student interaction with GenieMail (as well as in other parts of Reasoning Mind). These findings will then be used to enhance GenieMail and other instructional interactions with the Reasoning Mind system by creating a hybrid human/AI system, with the goal of improving math identity across diverse populations of students."
"1651060","RAPID: Collaborative Research: Employees' Response to OPM Data Breaches: Decision Making in the Context of Anxiety and Fatigue","SES","Secure &Trustworthy Cyberspace","07/01/2016","11/15/2016","H. Raghav Rao","TX","University of Texas at San Antonio","Standard Grant","Sara Kiesler","08/31/2017","$33,312.00","Rohit Valecha","hr.rao@utsa.edu","One UTSA Circle","San Antonio","TX","782491644","2104584340","SBE","8060","7434, 7914","$0.00","Nontechnical Description<br/>According to recent reports in the press, the Office of Personnel Management (OPM) was hit hard in two recent cyber-attacks (OPM 2015). In April 2015, OPM discovered that personal data (e.g., Social Security Numbers, full name, and birth date) of 4.2 million current and former Federal government employees had been stolen (referred to as personnel records incident hereafter). Later in June 2015, OPM discovered that around 21.5 million employees - current and former Federal employees and contractors - were affected as their personal information such as Social Security Numbers, fingerprints, and background investigation records were compromised (referred to as background investigation records incident hereafter). Unlike other information, sensitive data such as the background investigation records, which include personal histories, relationships, and biometrics, reveal employees' personal lives are difficult to be re-issued. Typical protection such as a few months of credit monitoring may be insufficient in protecting victims from determined attackers.  To date, little is known about how and why people decide and act in the aftermath of breaches involving their personal data. In particular, the role of data breach fatigue, manifested by insensitivity to data breaches and low estimate of fraud loss, in affecting people's decisions and actions is unknown. The existing research has also been silent on employees' decision making and reactions in response to data breaches. To fill this research gap, in this proposal, we plan to conduct a study that reveals the key decision factors, response actions, and the potential effect of data breach fatigue in the context of anxiety over the possible outcomes of the breach. Findings of the study will help in understanding employee reactions towards data breaches. New knowledge will help industry and policy makers develop intervention strategies that avert the effect of breach fatigue<br/><br/>Technical Description<br/>This proposal will explore the crucial issues that influence employees' responses in the context of the recent two OPM data breach incidents. This proposed research will compare these two different incidents and their impacts on different types of victims, employees who receive notification of the personnel records incident (now), employees who receive notification of both incidents (future), and employees who only receive notification of the background investigation records incident (future). We will also survey employees who have not received any notification, as a control group. In addition to self-reported data through surveys, we shall extend this study by capturing organic Twitter messages related to the two breach incidents in the respective time periods in 2015 to study how people coped with breach incidents. Utilizing natural language processing, we intend to (1) explore patterns of discourses associated with the data breach fatigue, (2) extract coping mechanisms from the discourses, and (3) compare coping mechanisms of employees, identified from the survey and those derived from the data mining."
"1655215","EAGER:   New Graph and CSP Algorithms Based on Spectral and SDP Techniques","CCF","Algorithmic Foundations","09/01/2016","08/10/2016","Luca Trevisan","CA","University of California-Berkeley","Standard Grant","Tracy Kimbrel","08/31/2018","$100,000.00","","luca@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","7796","7916, 7926, 7927","$0.00","Techniques from linear algebra have been successful in the development of algorithms for combinatorial problems, especially graph problems,  with applications in data analysis, natural language processing, and network science, among others. Such algorithms are known as ""spectral"" algorithms. Google's PageRank algorithm is an example of a spectral algorithm.<br/><br/>""Semidefinite programming"" (abbreviated SDP) is a technical tool that subsumes spectral algorithms and which has led to several rigorous results and some preliminary practical algorithmic applications. There is reason to believe that further progress could break through certain technical barriers and provide the solution to long-standing open problems. This project explores various approaches that could lead to such breakthroughs. The PI will also continue his ongoing expository work, which has made recent results in this area more accessible to  a wider audience. Results from this project have the potential for a broad impact in pure mathematics and the theory of optimization; such a ""mathematical technology transfer"" will be facilitated by the fact that this project will overlap with a special program on Pseudorandomness at the Simons Institute for the Theory of Computing, co-organized by the PI,  and a special program on Optimization also at the Simons Institute, that the PI will be a participant in. Both programs will host  scholars from areas outside theoretical computer science whose interests overlap with the potential outcomes of this project.<br/><br/>The PI will investigate promising applications of semidefinite programming to graph partitioning problems, to constraint satisfaction problems, and to  the Unique Games Conjecture, including  the possibility of Lasserre hierarchy SDP relaxations to refute the Unique Games Conjecture, a core open problem in computational complexity theory.  In order to avoid or break known barriers to progress, the PI will investigate the power of Lasserre hiearchy relaxations to approximate the sparsest cut problem in special classes of graphs, and the power of Lasserre hierarchy SDP relaxations for constraint satisfaction problems. Spectral methods, which are a special case of Semidefinite Programming, will be used to analyze distributed algorithms, and new types of graph sparsifiers. Outcomes of the project may have applications in computational complexity theory, the theory of pseudorandomness, graph theory, data analysis, and  network science."
"1541506","Collaborative Research: Building a Comprehensive Evolutionary History of Flagellate Plants","DEB","GoLife, Systematics & Biodiversity Sci","01/01/2016","04/19/2019","Emily Sessa","FL","University of Florida","Continuing Grant","Katharina Dittmar","12/31/2020","$2,275,080.00","John Burleigh, Stuart McDaniel, Emily Sessa, Pasha Antonenko, Ellen Davis","emilysessa@ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","BIO","6133, 7374","6133, 7218, 9169, 9178, 9251, EGCH","$0.00","For the first ~300 million years of plant life on land, Earth's flora consisted entirely of flagellate plants, which today include approximately 30,000 species of bryophytes, lycophytes, ferns, and gymnosperms. Numerous major innovations, including stomata, vascular tissue, roots and leaves, woody stems, and seeds, evolved first in flagellate plant ancestors. The flagellate plants not only provide a window to the early evolution of these critical features, but are represented today by vibrant and diverse lineages that contribute substantially to global ecology, particularly via contributions to global carbon and nitrogen cycles and offering critical habitats for biodiversity. This research project will analyze molecular genetic variation among the 18,000 available flagellate plant species in order to create a species-level phylogeny (evolutionary tree of species relationships) for the entire group. Phylogenetic trees generated will be linked to fossil data, a comprehensive dataset of plant traits, and available data on species occurrence on the planet. The results of this project will inform investigations into the origins of features characteristic of flowering plants and provide a platform for student training and outreach.<br/><br/>This project will improve our understanding of the history and relationships of the flagellate plants by using new sequencing technologies to examine 500 nuclear loci and to produce a species-level phylogeny for these taxa that is linked to and integrated with an immense and varied amount of data on fossils, phenomic characters, and geospatial distributions. Phenotypic datasets will utilize a natural language processing tool which will be enhanced and tested to handle large datasets and the geospatial data will allow exploration of world-wide patterns of diversity and endemism. All data will be fully integrated into ongoing projects such as Open Tree of Life and Next Generation Phenomics. Education experts will develop an online educational tool for training the next generation of biodiversity scientists by providing an accessible framework for using the project data in university classrooms while promoting evidence-based teaching practices. A MicroPlants citizen science project will promote scientific literacy and plant awareness in the general public, through museums and schools."
"1560193","REU Site: CAAR: Combinatorics and Algorithms Applied to Real Problems","CCF","RSCH EXPER FOR UNDERGRAD SITES","03/01/2016","01/27/2016","William Gasarch","MD","University of Maryland College Park","Standard Grant","Rahul Shah","02/28/2019","$375,000.00","Samir Khuller","gasarch@cs.umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","1139","9250","$0.00","This REU site project to be hosted at University of Maryland College Park emphasizes combining theory with practice. The students will benefit from a strong faculty team. Building on the past experience, the project is expected to result in strong undergraduate research leading to successful research/industry careers for the students in the future. Outreach activities with high schools will attract high school students to this research. The site plans to recruit under-represented groups and also students from non-research universities and HBCUs. <br/> <br/>The projects proposed not only form cutting-edge theory research but are also relevant to practice and have high impact applications. The projects considered are in applied area of Big Data Analytics, Information Extraction in Natural Language Processing, Fair Division, Pricing over Social Networks, Secure Communication and Public Health. The approach of getting students interested in theory, starting from applied implementation has an added side benefit that it would train students in sharp algorithmic implementation and experimentation skills, which are deemed in high regard by leading Computer Science companies."
"1648034","SBIR Phase I:  An Automated Tool to Measure and Predict Substance Use Recovery","IIP","SMALL BUSINESS PHASE I","12/15/2016","12/11/2016","Lisa McLaughlin","MI","Workit Health","Standard Grant","Jesus Soriano Molla","11/30/2017","$225,000.00","","lisa@workithealth.com","150 S. Fifth Ave","Ann Arbor","MI","481040000","7347303747","ENG","5371","5371, 8018, 8023, 8032, 8042","$0.00","The broader impact of this Small Business Innovation Research (SBIR) Phase I project is to improve access to high-quality, responsive, and evidence-based treatments for risky users of alcohol and other drugs. Current standards of care are cost-prohibitive, time-intensive and overly reliant on community support groups lacking a scientific basis or a mechanism for accountability. The research proposed here will add to an interactive online platform providing tools and treatment for individuals seeking to change their relationship to substances. The technological innovation supported by this research will result in a graphical meter that provides users of the platform with a simple-to-interpret interface. This tool is designed to strengthen motivation, enhance capacity for self-assessment, and allow for the better allocation of care provision resources. This development will give the proposing organization a major competitive advantage as it contributes to the growing field of digitally-provided behavioral healthcare products. Employers, insurers, and consumers have communicated a clear demand for new methods of treating individuals with substance use disorders, and this technology will equip the proposing organization to meet that demand. <br/><br/>The proposed project addresses the ""black box"" of recovery from risky use of alcohol and other drugs. Ample research demonstrates that behavioral interventions can be effective ways to mitigate risk for individuals with unhealthy use of alcohol and other drugs. Monitoring, assessing, and quantifying that recovery, however, remains out of reach for behavioral interventions delivered in the current standard-of-care. Instead of present-moment substance use recovery, the typical measure remains previous substance consumption. By statistically analyzing a range of available data from users of an online substance use recovery platform, this research will identify the data most correlated with successful recovery from substance use. Regression modeling and bivariate correlations will explore the relationships between different data within the platform and several validated outcome measures already integrated into the intervention. This research will use discourse analysis and natural language processing to identify and categorize user-submitted content to identify linguistic elements related to substance use recovery or risk of relapse. The principal outcome will be a holistic measure of recovery from substance use aggregating multiple instruments and domains of substance health. It will be developed into a graphical interface that communicates to both users and other behavioral healthcare stakeholders in-the-moment recovery from risky substance use."
"1546459","BIGDATA: Collaborative Research: F: Nomadic Algorithms for Machine Learning in the Cloud","IIS","Big Data Science &Engineering","01/01/2016","07/08/2019","Vishwanathan Swaminathan","CA","University of California-Santa Cruz","Standard Grant","Sylvia Spengler","12/31/2019","$596,326.00","","vishy@ucsc.edu","1156 High Street","Santa Cruz","CA","950641077","8314595278","CSE","8083","7433, 8083","$0.00","With an ever increasing ability to collect and archive data, massive data sets are becoming increasingly common. These data sets are often too big to fit into the main memory of a single computer, and so there is a great need for developing scalable and sophisticated machine learning methods for their analysis. In particular, one has to devise strategies to distribute the computation across multiple machines. However, stochastic optimization  and inference algorithms that are so effective for large-scale machine learning appear to be inherently sequential.<br/><br/>The main research goal of this project is to develop a novel ""nomadic"" framework that overcomes this barrier.  This will be done by showing that many modern machine learning problems have a certain ""double separability"" property. The aim is to exploit this property to develop convergent,  asynchronous, distributed, and fault tolerant algorithms that are well-suited for achieving high performance on commodity hardware that is prevalent on today's cloud computing platforms. In particular, over a four year period, the following will be developed: (i) parallel stochastic optimization algorithms for the multi-machine cloud computing setting, (ii) theoretical guarantees of convergence, (iii) open source code under a permissive license, (iv) application of these techniques to a variety of problem domains such as topic models and mixture models. In addition, a cohort of students who can transfer their skills to both industry and academia will be trained, and a graduate level course on scalable machine learning will be developed.<br/> <br/>The proposed research will enable practitioners in different application areas to quickly solve their big data problems.  The results of the project will be disseminated widely through papers and open source software. Course material will be developed for the education of students in the area of Scalable Machine Learning, and the course will be co-taught at UCSC and UT Austin. The project will recruit women and minority students."
"1630365","33rd International Conference on Machine Learning (ICML 2016)","IIS","Robust Intelligence","06/01/2016","06/29/2016","John Cunningham","NY","Columbia University","Standard Grant","Weng-keen Wong","05/31/2017","$30,000.00","","jpc2181@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","7495","7495, 7556","$0.00","The International Conference on Machine Learning (ICML) is considered to be the premier conference in Machine Learning, both from an educational and scientific standpoint. ICML encompasses topics on all facets of Machine Learning, and solicits papers on problem areas, research topics, learning paradigms, and approaches to evaluation. It is a key forum for exchange of ideas in the Machine Learning community, and features scientific poster sessions, educational tutorials, forward-looking workshops, and invited talks.   This project offers awards to students to help defray their travel costs, promoting a broader societal reach for the conference.  Promoting education and participation in cutting edge data science is central to the mission of the NSF, and this project and its awards speak directly to that mission.<br/><br/>ICML 2016 will be held in New York City. We have estimated itemized costs based on previous ICML conferences.  Students funded via this project will be selected by a peer review process based on their financial needs, alignment of research areas, and participation in the conference.  These awards will offer the opportunity to network with experts in the area and gain valuable insights into the cutting edge of Machine Learning research. This will positively impact the depth and breadth of their research as well as quality of their dissertations.  Long-term career benefits should also result, as ICML has a number of industrial sponsors who set up booths and discuss Machine Learning-oriented job opportunities within their own companies.  The ICML 2016 scholarship program follows the successful scholarship programs from the last several years. The requested funds will be used exclusively to help defray the travel and registration costs of students attending the conference. These student scholarships are very important for encouraging student participation in this premier conference and for shaping the future of the field as a whole."
"1618631","TWC: Small: A Moving Target Approach to Enhancing Machine Learning-Based Malware Defense","CNS","Secure &Trustworthy Cyberspace","09/01/2016","08/29/2016","Guanhua Yan","NY","SUNY at Binghamton","Standard Grant","Wei-Shinn Ku","08/31/2021","$349,906.00","","ghyan@binghamton.edu","4400 VESTAL PKWY E","BINGHAMTON","NY","139026000","6077776136","CSE","8060","7434, 7923","$0.00","The ever-growing malware threats call for effective, yet efficient, mitigation techniques. Machine learning offers a promising solution to malware defense due to the scalability and automation that it brings. Machine learning techniques are however not a panacea for advanced malware attacks where cyber criminals can carefully craft malware features to evade detection. The root cause of such attacks can be attributed to the passive nature of existing machine learning-based malware defense systems. <br/><br/>Our project aims to enhance these systems with a novel moving target strategy. Our method differs from traditional methods that use feature selection schemes to search for a static subset of features for malware detection or classification. In contrast, our approach dynamically changes the features used to train a classification model for predicting future malware attacks. To prevent adversarial correlation analysis, our method uses a random walk technique to ensure that features used across different predictive models have low mutual information. We further study the effectiveness of the moving target approach in enhancing machine learning-based malware defense under various evasion strategies by malware attackers. If successful, this research will introduce a promising new strategy for mitigating the ever-growing malware threats."
"1625397","Addressing Ubiquitous STEM Gender Performance Differences","DUE","IUSE","10/01/2016","09/04/2018","Timothy McKay","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Andrea Nixon","09/30/2020","$1,932,221.00","","tamckay@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","EHR","1998","8209, 9178","$0.00","This is a wide reaching project that combines strands of a number of NSF priorities and earlier NSF-supported projects, including ""Writing to Learn,"" WIDER (institutional transformation), INCLUDES (broadening participation), Cyber learning, and Big Data Analytics. Using big data analytics, it has been quantitatively established that female students taking introductory STEM lecture courses at 10 major public research universities show evidence of stereotype threat, which lowers their course performance by 5% to 10% and discourages some from continuing as STEM majors. The goal of the project is to continue to refine and employ a digital mentoring/ treatment tool that has undergone pilot stage testing to deliver up to three types of treatments to countervail the presence of stereotype threat. This tool has the potential to be customized for each student. It was developed with early stage support from an earlier NSF/ DUE grant. This tool will use a writing-to-learn tool being developed under an NSF/IUSE grant. The writing tool is building a comprehensive writing system, combining peer review tools with a natural language processing toolkit designed to provide actionable information about student responses to the digital mentoring tool. This project is expected to have a substantial direct impact on female STEM students and has good prospects for encouraging more of them to remain in STEM as their primary college major. If the tool proves to be effective in eliminating gender performance differences in STEM courses, it is likely to be employed in a growing number of other research universities, thereby multiplying its impact. <br/><br/>The project itself will work with 5,000 distinct undergraduate women in a randomized treatment design to test the efficacy of basic treatment and to test the improved efficacy of individualizing the treatment for each student. Because all of the interactions with students will take place through the digital mentoring/ treatment tool framework, the project will have complete control over what each student experiences, along with a comprehensive ""treatment record"" of what each student encountered and did within the system, including the writing they did in response to the intervention prompts. The ability to control and record personalized treatment for every individual allows the treatment to be assessed through randomized trials. This approach has been the key to the development of sequential multiple assignment randomized trials in digital health coaching, for example."
"1553146","CAREER: Adaptable, Intelligible, and Actionable Models: Increasing the Utility of Machine Learning in Clinical Care","IIS","Info Integration & Informatics","02/01/2016","01/29/2019","Jenna Wiens","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Sylvia Spengler","01/31/2021","$502,839.00","","Wiensj@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7364","1045, 7364","$0.00","In recent years, the availability of clinically relevant datasets has grown enormously. A good understanding of how to organize, process, and transform these data into actionable knowledge is crucial. This research aims to unlock the potential of these data through the exploration of new fundamental research directions and approaches in machine learning. Targeting patients identified as high-risk by through computational data-driven models could reduce the burden of disease in a cost-effective manner. While machine learning opportunities in medicine continue to grow, there have been relatively few successes regarding translation to practice. Clinicians still base the bulk of their daily decisions on relatively small amounts of patient-specific data. The technical contributions made here will enable the meaningful use of complex medical data. Beyond the long-term societal impact, this work will provide valuable student training through research projects related to the proposed objectives. Targeted outreach activities that focus on the societal impacts of computational research will attract a diverse set of graduate students to the field. In addition, this work will help lay the foundation for a new project-based course focusing on applications of machine learning in clinical care. As the field continues to grow, such courses will become critical for equipping the next generation of students with the required tools and insights. Finally, critical inter-departmental collaborations between computer science and engineering and medicine will grow as a result of this work, leading to the enrichment of both fields. <br/><br/>The primary research objective of this proposal is to increase the utility of machine learning in clinical care, through the exploration of new fundamental research directions and approaches in ML. For data-driven predictive models to become widely and safely adopted in clinical care, there remain several key research challenges that the ML community must address: poor adaptability to complex unexpected changes in patient populations and clinical protocols, insufficient intelligibility of accurate but uninterpretable models, and absence of actionability, with accuracy overcoming actionability. The PI proposes the development of new transfer learning techniques for learning robust and adaptable models in a wide range of scenarios. Experiments and evaluations with large-scale clinical datasets will offer insight into how these data change over time, and a better understanding of when and how models should adapt.   Clinical decision models and software are seldom incorporated into practice because they are either black-box or the output (while accurate) does not offer any insight into how to act. One way to increase the intelligibility of models is to focus on building clinically meaningful features. Another way to increase intelligibility is through sparsity. The PI will investigate feature engineering/selection methods for learning useful abstractions that automatically leverage expert knowledge and for learning models based on actionable features. The PI will explore structured regularization techniques to select modifiable features. To gain a better understanding of how different actions affect patient risk, the PI will address the limitations of causal inference in the context of high-dimensional observational datasets. This research will yield methods for producing clinically meaningful inputs, and methods for jointly optimizing sparsity and actionability.  The proposed work will yield novel techniques for extracting and building adaptable, intelligible, and actionable models from patient data. An emphasis on adaptable solutions will ensure that such techniques can be safely adopted long-term. The study of techniques for dealing with the inherent heterogeneity of the data (e.g., different patient populations from across multiple sites) will not only increase the utility of the data but will lead to more general advances in the field of transfer learning. A focus on intelligibility - a quality that is often overlooked by the machine learning community - promises to increase the utility of such models, since clinicians are more likely to adopt a model they can check and understand. Prioritizing actionable models will yield new strategies for causal analysis in high-dimensional observational settings. This, in turn, will enable the generation of new hypotheses regarding causal relationships in clinical medicine."
"1623730","EXP: Linguistic Analysis and a Hybrid Human-Automatic Coach for Improving Math Identity","IIS","Cyberlearn & Future Learn Tech","09/01/2016","08/18/2016","Jaclyn Ocumpaugh","NY","Teachers College, Columbia University","Standard Grant","Maria Zemankova","05/31/2017","$540,047.00","Ryan Baker, Scott Crossley, Leigh Mingle, Victor Kostyuk","jlocumpaugh@gmail.com","525 West 120th Street","New York","NY","100276625","2126783000","CSE","8020","8045, 8841","$0.00","The Cyberlearning and Future Learning Technologies Program funds efforts that support envisioning the future of learning technologies and advance what we know about how people learn in technology-rich environments. Cyberlearning Exploration (EXP) Projects explore the viability of new kinds of learning technologies by designing and building new kinds of learning technologies and studying their possibilities for fostering learning and challenges to using them effectively. This project addresses the effect of students' social identity on learning, an important factor in math and science education. Specifically, it will advance the scientific understanding of math identity (i.e. ""I'm (not) a math person"") by studying the over 100,000 diverse students who use Reasoning Mind, a blended learning system for K-8 mathematics with demonstrated results. Reasoning Mind supports math identity with an innovative design that allows students to email an animated character (aka the Genie) and receive a human-crafted response. This study will show how math identity manifests and changes during students' use of Reasoning Mind in order to inform software designers and classroom teachers on best practices for encouraging math identity. This will have the broader impact of strengthening our nation's ability to supply science and technology fields with a well-trained workforce.<br/><br/>This study examines two components of math identity: self-efficacy and interest in mathematics. Both self-efficacy and interest can be enhanced by curricula that are individualized to appropriately challenge each student (a strength of educational technology), but social stereotypes (i.e. ""Girls aren't good in math"") may decrease math identity or otherwise interfere with its development. This study investigates how educational technology can reverse these trends. In the first phase of this study, a combination of survey methods, Natural Language Processing (NLP), and Educational Data Mining (EDM) techniques will be used to identify how poor and/or changing math identity emerges in the linguistic patterns of student interaction with GenieMail (as well as in other parts of Reasoning Mind). These findings will then be used to enhance GenieMail and other instructional interactions with the Reasoning Mind system by creating a hybrid human/AI system, with the goal of improving math identity across diverse populations of students."
"1624770","I/UCRC for Advanced Electronics through Machine Learning (CAEML)","CNS","Special Projects - CNS, IUCRC-Indust-Univ Coop Res Ctr","08/01/2016","03/31/2020","Paul Franzon","NC","North Carolina State University","Continuing Grant","Behrooz Shirazi","07/31/2021","$774,000.00","Elyse Rosenbaum","paulf@ncsu.edu","2701 Sullivan DR STE 240","Raleigh","NC","276950001","9195152444","CSE","1714, 5761","1800, 5761, 9251","$0.00","The semiconductor industry is perennially one of America's top exporters. Worldwide semiconductor sales for 2014 reached $335.8 billion, and the number of U.S. jobs in this sector was estimated to be around 250,000 in 2013. More broadly, the U.S. tech industry, which depends on semiconductor innovation to spur new products and applications, is itself estimated to represent no less than 5.7% of the entire U.S. private sector workforce (at nearly 6.5 million jobs), and with a tech industry payroll of $654 billion in 2014, it accounted for over 11% of all U.S. private sector payroll. Yet despite its success, the industry must continue to innovate if the U.S. is to retain global leadership in this highly competitive area. The complexity of modern microelectronic products necessitates the use of computer tools to formulate and verify product designs prior to manufacturing. When a product doesn't operate as intended or suffers early failures, this can often be attributed to inadequacy of the models used during the design process. In fact, the shortcomings of existing approaches for system component modeling have become a serious impediment to continued innovation.<br/><br/>The Center for Advanced Electronics through Machine Learning (CAEML) proposes to create machine-learning algorithms to derive models used for electronic design automation with the objective of enabling fast, accurate design of microelectronic circuits and systems. Success will make it much easier and cheaper to optimize a system design, allowing the industry to produce lower-power and lower-cost electronic systems without sacrificing functionality. The eventual result will be significant growth in capabilities that will drive innovation throughout the electronics industry, leading to new devices and applications, continued entrepreneurial leadership, and economic growth.<br/><br/>While achieving those goals, CAEML will also focus on diversifying the undergraduate engineering student body and improving the undergraduate experience. Students from groups traditionally underrepresented in engineering will be targeted for recruitment as undergraduate research assistants. Member companies will provide internships and mentors for participating students, and the diverse graduate and undergraduate student researchers in CAEML will receive hands-on multidisciplinary education. CAEML will also participate in all three site universities? existing avenues for student and faculty engagement with local youth. In particular, university-based summer camps are a tried and tested method of making high-school students familiar with and comfortable on our campuses. NCSU runs engineering summer camps for high-school students, with the Electrical and Computer Engineering department providing an autonomous robotics workshop. CAEML undergraduate and graduate students can serve as counselors or instructors for camps; the CAEML team proposes to develop new activities and workshops for high-school campers on all three sites' campuses. In addition, the MISO program (Maximizing the Impact of STEM Outreach) at NCSU is a campus-wide program funded by NSF that provides an innovative approach to evaluation across NC State's K-12 STEM education outreach programs<br/><br/><br/><br/>The Center for Advanced Electronics through Machine Learning (CAEML) will create machine-learning algorithms to derive models used for electronic design automation, with the objective of enabling fast, accurate design of microelectronic circuits and systems. The electronics industry's continued ability to innovate requires the creation of optimization methodologies that result in low-power integrated systems that meet performance specifications, despite being composed of components whose characteristics exhibit variability and that operate in different physical or signal domains. Today, shortcomings in accuracy and comprehensiveness of component-level behavioral models impede the advancement of computer-aided electronic system design optimization. The model accuracy also impacts system verification. Ultimately, the proper functionality of an electronic system is verified through testing of a representative sample. However, modern electronic systems are so complex that it is unthinkable to bring one to the manufacturing stage without first verifying its operation using simulation. Today, simulation generally does not ensure that an integrated circuit or electronic system will pass qualification testing the first time, and failures are often attributed to insufficiency of the simulation models. With an improved modeling capability, one could achieve better design efficiency, and also perform design optimization. For system simulation, behavioral models of the components' terminal responses are desired for both computational tractability and protection of intellectual property. Despite many years of significant effort by the electronic design automation community, there is not a general, systematic method to generate accurate and comprehensive behavioral models, in part because of the nonlinear, complex, and multi-port nature of the components being modeled.<br/><br/>CAEML will pioneer the use of machine-learning methods to extract behavioral models of electronic components and subsystems from simulation waveforms and/or measurement data. The Center will make 2 primary contributions to the field of machine learning: it will demonstrate the application of machine learning to electronics modeling, and develop the entire machine-learning pipeline. Historically, machine-learning theorists have focused on the model learning and evaluation tasks, but CAEML will focus on end-to-end performance of the pipeline, including data acquisition, selection and filtering, as well as cost function specification. CAEML will develop a methodology to use prior knowledge, i.e., physical constraints and the domain knowledge provided by designers, to speed up the learning process. Novel methods of incorporating component variability, including that due to semiconductor process variations, will be developed. The intended end-users are electronic design automation (EDA) tool developers, IC design houses, and system design and manufacturing companies.<br/><br/>CAEML consists of 3 sites: Illinois, Georgia Tech, and NC State. The scope of research at each site encompasses both algorithm development and the application of the derived models to a variety of IC and system design tasks. Investigators at all 3 university sites have unique skills and expertise while sharing interests in electronic design automation, IC design, system-level signal integrity, and power distribution. To leverage the cross-campus expertise, many of the Center's proposed projects involve investigators from more than one site. The NCSU investigators have special expertise in RFIC and digital design, process design kits, and circuit-level surrogate modeling. All three sites have strong research records in the fields of signal integrity analysis and electronic design automation."
"1566289","CRII: CHS: Facilitating Consumption and Re-expression of Scientific Information in a Journalism Context","IIS","","06/15/2016","05/30/2017","Jessica Hullman","WA","University of Washington","Continuing grant","William Bainbridge","09/30/2018","$174,633.00","","jhullman@northwestern.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","026y","7367, 8228","$0.00","This research will result in software tools that help journalists, scientists, and news readers write and read about scientific findings in more understandable ways. Though it is important that non-scientific audiences are able understand new advances in technological, biological, genetic, and other scientific fields, much of science writing is difficult to understand due to the use of jargon. One way to make scientific findings easier to understand is to re-express them using simpler terms, such as using the term ""butterfly"" instead of ""lepidtoperan."" By writing software to analyze collections of scientific publications and news articles, we will discover more understandable expressions for scientific terms, and develop user interface tools that allow an author or a reader to access these terms while they write or read about science. In addition to supporting science communication through the news, we expect our tools will help scientists create and share more broadly understandable summaries of their work.<br/><br/>This work will contribute new software-based techniques to facilitate lexical simplification of scientific terminology, which draws upon and contributes to the fields of natural language processing and human-computer interaction. The technical objective of this project is to investigate how to enable effective re-expressions of scientific terms in tools through: 1) identifying strategies that human authors like science journalists use to re-express complex scientific content, and 2) developing user interfaces and algorithms that present simplifications to an author like a journalist as she writes about science for a general audience, or to a reader who lacks domain expertise. To build the simplification algorithms that will be used in our interactive tools, we will first learn mappings between scientific and simpler terms from corpora that include scientific writing such as journal publications in PLOS or PubMed as well as simpler summaries of this information written by the scientists or written by news journalists. We will apply word embeddings and other word association methods to such corpora to determine the effectiveness of different methods for learning simplifications. We will apply these simplifications in mixed initiative authoring and reading interfaces that enhance the capabilities of a human author or reader by recommending and enabling easy application of simplifications in various ways."
"1555244","CAREER:  Semiparametric and Machine Learning Approaches to Big Data Challenges in Precision Medicine","DMS","STATISTICS, Division Co-Funding: CAREER","07/01/2016","07/03/2020","Rui Song","NC","North Carolina State University","Continuing Grant","Gabor Szekely","06/30/2021","$400,000.00","","rsong@ncsu.edu","2701 Sullivan DR STE 240","Raleigh","NC","276950001","9195152444","MPS","1269, 8048","1045","$0.00","In the era of Big Data, the goal of better patient outcomes, coupled with lower cost and burden has generated tremendous interest in precision (or personalized, individualized) medicine, which is defined as treatments targeted to the needs of individual patients on the basis of genetic, biomarker, phenotypic, or psychosocial characteristics that distinguish a given patient from other patients with similar clinical presentations (Jameson and Longo, 2015).Precision medicine can be operationalized using individual's health-related metrics and environmental factors to discover individualized treatment regimes (ITRs); methodology for such discovery is an emerging field of statistics. The proposed methods are expected to bring a great impact to accelerate the discovery of new personalized treatment strategies. Therefore the proposed work is directly related with the White House Precision Medicine Initiative(https://www.whitehouse.gov/precision-medicine) as a research effort to revolutionize how to improve health and treat disease. The proposed methods are also general enough to be applied to a variety of data sources including clinical, biomarker, economic and financial data. If successful, the projects will greatly enhance the acquisition and analysis of large-scale data for the scientific and engineering communities.<br/><br/>The main objective of this proposal is to develop cutting-edge semiparametric methods and machine learning tools to realize the promise of precision medicine. Specifically, the PI aims to: develop flexible and efficient methods for discovering optimal ITRs (Aim 1); develop a general class of optimal ITRs (Aim 2); develop optimal ITRs with high-dimensional data (Aim 3); and develop optimal ITRs under population heterogeneity (Aim 4). The proposed work contributes to both semiparametric inference and machine learning fields. Machine learning methods have rarely been studied for doubly robust estimation and optimal ITRs with high-dimensional data. The theoretical developments including driving nonasymptotic distribution, risk bounds, new empirical process technical tools are challenging. The methodologies to be developed in this project will be fundamentally important and generally applicable for studying semiparametric models in high-dimensional setting. Using semiparametric and machine learning methods for precision medicine is an emerging novel area. The integration of research and education is a key aspect of this project. New courses on statistical learning and semiparametric inference will be developed. These courses will broaden the areas of specialized training in a department that has a strong history of attracting under represented groups. The PI is expecting to stimulate interests from a diverse group of researchers in numerous fields. The PI will also reach out to the K-12 education levels by training high school teachers."
"1644441","CM: Machine-Learning Driven Decision Support in Design for Manufacturability","CMMI","CM - Cybermanufacturing System, AM-Advanced Manufacturing, SSA-Special Studies & Analysis, ESD-Eng & Systems Design, Special Initiatives","09/01/2016","05/04/2020","Adarsh Krishnamurthy","IA","Iowa State University","Standard Grant","Bruce Kramer","08/31/2021","$491,175.00","Soumik Sarkar","adarsh@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","ENG","018Y, 088Y, 1385, 1464, 1642","016Z, 067E, 082E, 083E, 091Z, 116E, 9178, 9231, 9251","$0.00","Traditional design and manufacturing relies on the experience and training of the designer to create a component with manufacturable features. However, even after careful design, the as-manufactured part might differ from the as-designed part. In addition, the inclusion of certain features might significantly increase the manufacturing cost. For example, the inclusion of a thin feature might necessitate the use of complex jigs or fixtures to prevent the flexing of the part during machining, which increases manufacturing time and cost. This problem is also encountered in additive manufacturing, where there is no body of knowledge regarding design rules that will reduce manufacturing defects. This project aims to address this challenge by developing computer-aided design tools that can identify difficult-to-manufacture features using machine learning. The process of identification of the source of infeasibility in manufacturing in a complex part is a challenging task, even for an experienced designer. Therefore, the use of machine learning could potentially play a critical role by detecting non-intuitive patterns from examples of feasible and infeasible parts, and identifying the source of infeasibility. The results of the machine-learning framework will be used to build a decision support framework that can interactively identify manufacturability concerns during the design process and present design modifications interactively to the designer. Finally, the multidisciplinary components of the project will be integrated into a larger educational effort to offer students a solid foundation in the critical interdisciplinary area of cyber-enabled manufacturing.<br/><br/>The objective of this project is to create a design for manufacturability tool that uses machine learning to identify difficult to machine or manufacture features in a computer-aided design model and suggest changes to the non-manufacturable features. The novelty of this research is the use of machine learning in a computer-aided design and manufacturing environment, making it accessible to designers using a familiar design interface. The research team will develop tools for loading existing models of parts and performing virtual machining simulations to create a digital voxelized representation of the as-manufactured part. The original as-designed part will also be converted to a voxelized representation that will be suitable for machine learning. The machine-learning framework will be trained using multiple machining simulations and will classify feasible and infeasible designs by learning from positive and negative examples. Furthermore, the machine-learning framework will be used to present alternative feasible designs to the designer."
"1553365","CAREER: Accurate Electrochemical Barriers Accelerated by Machine-learning","CBET","Catalysis","01/01/2016","06/11/2020","Andrew Peterson","RI","Brown University","Standard Grant","Robert McCabe","12/31/2020","$573,860.00","","Andrew_Peterson@brown.edu","BOX 1929","Providence","RI","029129002","4018632777","ENG","1401","1045, 9150, 9251","$0.00","Abstract (Peterson;1553365)<br/><br/>The study will promote advances in theoretical understanding of electrocatalysis as accelerated by machine-learning tools.  The resulting understanding will aid the development of related technologies such as solar-fuel devices, batteries, fuel cells and electrolyzers - all of relevance to renewable energy and the commercialization of sustainable technologies.  The research will also provide educational opportunities to students at various levels in both the U.S. and in rural Kenya, and will facilitate introduction of high-fidelity, accelerated atomistic calculations across a broad research community via the principal investigator's publicly available code, ""Amp"".<br/><br/>Electronic structure theory has revolutionized heterogeneous catalyst design in recent years, however it has had greater challenges in electrocatalysis due to the difficulty of calculating transition state energy barriers (which often dictate catalytic performance).  This study will provide the first systematic study of such potential-dependent electrocatalytic barriers across a range of catalytic materials and adsorbed reactants, thus facilitating the discovery of new materials and electrocatalytic processes.  The transition-state calculations will be enabled by the development of unique, atom-centered, machine-learning tools that dramatically accelerate atomistic calculations while matching the accuracy of computationally-intensive (and often exceedingly time-consuming) ""ab initio"" calculations.  The PI's Amp software modularizes atom-centered machine learning, and will be used to accelerate the search of potential energy surfaces for local minima, transition states, and global minima.  Moreover, the acceleration provided by machine learning will also facilitate the introduction of complicated phenomena associated with the electrochemical environment such as solvent effects and large unit cells of typical materials.  More broadly, the project will provide information that will be used by the PI as teaching tools to convey reaction visualization to students ranging from local high school and Brown undergraduate and graduate students to students at a new science and technology university (JOOUST) in rural Kenya.  Moreover, continued development of Amp, with new shared applications, will accelerate materials discovery across a broad range of applications related to energy and the environment."
"1629161","EXP: Assessing 'Complex Epistemic Performance' in Online Learning Environments","IIS","S-STEM-Schlr Sci Tech Eng&Math, Cyberlearn & Future Learn Tech","09/01/2016","09/02/2016","William Cope","IL","University of Illinois at Urbana-Champaign","Standard Grant","John Cherniavsky","08/31/2019","$549,811.00","ChengXiang Zhai, Duncan Ferguson, Willem Els","billcope@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","1536, 8020","8045, 8244, 8841","$0.00","The Cyberlearning and Future Learning Technologies Program funds efforts that support envisioning the future of learning technologies and advance what we know about how people learn in technology-rich environments. Cyberlearning Exploration (EXP) Projects design and build new kinds of learning technologies in order to explore their viability, to understand the challenges to using them effectively, and to study their potential for fostering learning.  This project will develop online software tools to assess and offer feedback to learners communicating complex scientific or technical information. ""Complex epistemic performance"" here refers to knowledge representations in reports or case studies which involve not only facts and theoretical concepts that might produce correct answers but also arguments, interpretations and conclusions that are matters of disciplinary or professional judgment. Development and testing of these tools will take place using clinical case studies in medicine and veterinary medicine. Medical students will write analyses of specific cases of sick people and animals, marshaling evidence and making diagnoses based on this evidence. Peers will offer ""second opinions"", followed by revision. Students will receive a combination of human feedback and machine feedback.<br/><br/>The principal technical innovation in this project will be the development and testing of machine learning algorithms that offer useful feedback to learners and support instructor assessment. The software will analyze student-created cases in relation to the case objectives and a clinical evaluation rubric. It will compare review text item by item to the rubric criteria. The rubric-criterion ratings and overall ratings assigned by users (students and instructors) will train the software via a process of supervised machine learning. In this way, the software will be able to make progressively more accurate assessments of new texts, as well as evaluate the quality of peer reviews. The software will also use unsupervised machine learning techniques, highlighting as-yet unclassified patterns that may warrant investigation -- in other words, it will ask students and instructors to interpret patterns of response that may be of interest but which they may not have noticed. One key research outcome of this project will be whether and how machine-supported and machine-mediated formative assessment processes improve learning outcomes in science and related professions, as exemplified in university-level medical education.  The algorithms developed in this project could also have broader applicability in a wide range of areas of scientific and technical endeavor."
"1723891","CAREER: Supervised Learning for Incomplete and Uncertain Data","IIS","Info Integration & Informatics","10/01/2016","02/01/2019","Alina Zare","FL","University of Florida","Continuing Grant","Sylvia Spengler","04/30/2021","$327,359.00","","azare@ece.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","CSE","7364","1045, 7364, 9102, 9251","$0.00","This CAREER project will advance the state of the art in supervised machine learning to allow for incomplete, uncertain and unspecific label information.  Supervised machine learning algorithms produce desired outputs for given input data by learning from example training data.  The methods generally rely on completely and accurately labeled training data to drive the learning algorithm. However, many applications are plagued with labels that are incomplete, uncertain, and unspecific (lack precision).  Current techniques do not adequately handle such data.<br/><br/>For example, analysis of satellite imagery to identify the content of each pixel is often conducted by coupling unsupervised learning methods (that do not rely on labeled training data) with manual exploration.  This is time-consuming, error-prone, and expensive.  Imagine, instead, easy-to-use tools that could understand the content of each pixel in satellite imagery.  Extremely large amounts of road map data (for example from Google Maps or OpenStreetMap) and social media information (for example geo-tagged photographs, video clips, and social networking posts) are continually collected and stored.  These data could be used as sparsely-labeled training data (with varying degrees of specificity and uncertainty) to guide understanding of satellite imagery.<br/><br/>Although the data is available, algorithms have yet to be developed to combine these data sources and identify the content of pixels in satellite images.  This work will advance this and other potential applications of machine learning where incomplete, uncertain and unspecific labels in training data challenge the development of effective machine learning algorithms.  <br/><br/>This CAREER project will achieve these advances through the following research objectives:<br/>(1) Investigate and develop a mathematical framework and associated algorithms for Multiple Instance Function Learning that addresses linear and non-linear classification and regression problems with varying levels and types of sparsity, uncertainty, and specificity in training labels.<br/><br/>(2) Study and apply the proposed framework and algorithms towards the fusion of satellite imagery, road map data and social media for global scene understanding. <br/><br/>This research will be conducted in conjunction with integrated education and outreach activities.  In particular, an interactive web application will be developed to provide an avenue for introducing concepts from machine learning and remote sensing to the public for dissemination and outreach. This interactive web application will also be used, along with additional hands-on activities, to introduce high school students to machine learning and remote sensing concepts during an annual summer engineering camp held at the University of Missouri in Columbia, MO.  Paired with the web application will be a research website in which data, code, publications and presentations will be shared with the research community. Furthermore, undergraduate and graduate research assistants will be trained in the areas of machine learning and remote sensing. Finally, relevant research topics will be introduced in the PI's undergraduate and graduate courses."
"1624811","I/UCRC: Center for Advanced Electronics through Machine Learning (CAEML)","CNS","Special Projects - CNS, IUCRC-Indust-Univ Coop Res Ctr","08/01/2016","09/17/2019","Elyse Rosenbaum","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Behrooz Shirazi","07/31/2021","$761,200.00","","elyse@uiuc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","1714, 5761","5761, 9251","$0.00","The semiconductor industry is perennially one of America's top exporters. Worldwide semiconductor sales for 2014 reached $335.8 billion, and the number of U.S. jobs in this sector was estimated to be around 250,000 in 2013. More broadly, the U.S. tech industry, which depends on semiconductor innovation to spur new products and applications, is itself estimated to represent no less than 5.7% of the entire U.S. private sector workforce (at nearly 6.5 million jobs), and with a tech industry payroll of $654 billion in 2014, it accounted for over 11% of all U.S. private sector payroll. Yet despite its success, the industry must continue to innovate if the U.S. is to retain global leadership in this highly competitive area. The complexity of modern microelectronic products necessitates the use of computer tools to formulate and verify product designs prior to manufacturing. When a product doesn't operate as intended or suffers early failures, this can often be attributed to inadequacy of the models used during the design process. In fact, the shortcomings of existing approaches for system component modeling have become a serious impediment to continued innovation. <br/><br/>The Center for Advanced Electronics through Machine Learning (CAEML) proposes to create machine-learning algorithms to derive models used for electronic design automation with the objective of enabling fast, accurate design of microelectronic circuits and systems. Success will make it much easier and cheaper to optimize a system design, allowing the industry to produce lower-power and lower-cost electronic systems without sacrificing functionality. The eventual result will be significant growth in capabilities that will drive innovation throughout the electronics industry, leading to new devices and applications, continued entrepreneurial leadership, and economic growth. <br/><br/>While achieving those goals, CAEML will also focus on diversifying the undergraduate engineering student body and improving the undergraduate experience. Students from groups traditionally underrepresented in engineering will be targeted for recruitment as undergraduate research assistants. Member companies will provide internships and mentors for participating students, and the diverse graduate and undergraduate student researchers in CAEML will receive hands-on multidisciplinary education. CAEML will also participate in all three site universities' existing avenues for student and faculty engagement with local youth. In particular, university-based summer camps are a tried and tested method of making high-school students familiar with and comfortable on our campuses. The Girls' Adventures in Mathematics, Engineering, and Science (GAMES) summer camp program at the University of Illinois at Urbana-Champaign (""Illinois"") brings high-school girls to campus for a week of hands-on engineering activities and camaraderie. The engineering content for many of the GAMES camps, including the one on electrical engineering, is developed by engineering faculty. CAEML undergraduate and graduate students can serve as counselors or instructors for camps; the CAEML team proposes to develop new activities and workshops for high-school campers on all three sites' campuses. In addition, the Beginning Teacher STEM Conference at Illinois brings 150 teachers who have just completed their first year in the classroom to the Urbana-Champaign campus for 2 days to deepen their knowledge of STEM fields and try out activities for use in their classrooms; several of the sessions are taught by College of Engineering faculty including those affiliated with CAEML. <br/><br/><br/><br/>The Center for Advanced Electronics through Machine Learning (CAEML) will create machine-learning algorithms to derive models used for electronic design automation, with the objective of enabling fast, accurate design of microelectronic circuits and systems. The electronics industry's continued ability to innovate requires the creation of optimization methodologies that result in low-power integrated systems that meet performance specifications, despite being composed of components whose characteristics exhibit variability and that operate in different physical or signal domains. Today, shortcomings in accuracy and comprehensiveness of component-level behavioral models impede the advancement of computer-aided electronic system design optimization. The model accuracy also impacts system verification. Ultimately, the proper functionality of an electronic system is verified through testing of a representative sample. However, modern electronic systems are so complex that it is unthinkable to bring one to the manufacturing stage without first verifying its operation using simulation. Today, simulation generally does not ensure that an integrated circuit or electronic system will pass qualification testing the first time, and failures are often attributed to insufficiency of the simulation models. With an improved modeling capability, one could achieve better design efficiency, and also perform design optimization. For system simulation, behavioral models of the components' terminal responses are desired for both computational tractability and protection of intellectual property. Despite many years of significant effort by the electronic design automation community, there is not a general, systematic method to generate accurate and comprehensive behavioral models, in part because of the nonlinear, complex, and multi-port nature of the components being modeled.<br/><br/>CAEML will pioneer the use of machine-learning methods to extract behavioral models of electronic components and subsystems from simulation waveforms and/or measurement data. The Center will make 2 primary contributions to the field of machine learning: it will demonstrate the application of machine learning to electronics modeling, and develop the entire machine-learning pipeline. Historically, machine-learning theorists have focused on the model learning and evaluation tasks, but CAEML will focus on end-to-end performance of the pipeline, including data acquisition, selection and filtering, as well as cost function specification. CAEML will develop a methodology to use prior knowledge, i.e., physical constraints and the domain knowledge provided by designers, to speed up the learning process. Novel methods of incorporating component variability, including that due to semiconductor process variations, will be developed. The intended end-users are electronic design automation (EDA) tool developers, IC design houses, and system design and manufacturing companies.<br/><br/>CAEML consists of 3 sites: Illinois, Georgia Tech, and NC State. The scope of research at each site encompasses both algorithm development and the application of the derived models to a variety of IC and system design tasks. Investigators at all 3 university sites have unique skills and expertise while sharing interests in electronic design automation, IC design, system-level signal integrity, and power distribution. To leverage the cross-campus expertise, many of the Center's proposed projects involve investigators from more than one site. The Illinois investigators have special expertise in computational electromagnetics, electrostatic discharge (ESD), and optimization; they bring capabilities in areas such as circuit design for ESD-induced error detection, computationally-efficient stochastic electromagnetic field simulation, reduced-order modeling and behavioral modeling of electrical/electromagnetic circuits and systems, and multi-domain physics modeling in the presence of uncertainty and variability. All three sites have strong research records in the fields of signal integrity analysis and electronic design automation. Excellent computational resources are available at Illinois for the proposed work; the necessary test and measurement equipment is also available, including a system-level ESD test-bed."
"1563887","RI: III: Medium: Scalable Machine Learning for Automating Scientific Discovery in Astrophysics","IIS","Robust Intelligence","06/15/2016","09/14/2017","Barnabas Poczos","PA","Carnegie-Mellon University","Continuing Grant","Rebecca Hwa","05/31/2020","$1,099,889.00","Andrew Wilson, Rachel Mandelbaum, Eric Xing","bapoczos@andrew.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7495","7364, 7495, 7924","$0.00","The purpose of this work is to i) develop and validate new, efficient machine learning methods for making inferences and predictions in a massively parallel and distributed way on large-scale complex data sets coming from upcoming sky surveys, and ii) help answer important fundamental questions in cosmology and astrophysics using those new methods.  Theoretical properties of these algorithms will also be investigated.  The proposed cosmology and astrophysics applications will include a) building a probabilistic model for light intensity signals from stars, b) evolving the matter density of the Universe at a speed much higher than the traditional method of N-body simulations, and c) creating ""mock catalogs"" with all commonly observable galaxy properties. The methods that are developed will have far broader applicability than the examples listed here, both for other problems in astrophysics and problems in completely different domains (e.g. bioinformatics, climatology, social sciences), where complex scientific simulations require large-scale learning methods. The software developed in this work (including documentation, examples, and case studies) will be made publicly available. The PIs will also include the results in their course materials for graduate and undergraduate students.<br/><br/><br/>The aim of this proposal is to develop new machine learning methods that can work directly on large-scale, high-dimensional functions and continuous distributions as inputs or outputs in a regression problem, and can process large-scale scientific data in a massively parallel distributed way. Important theoretical properties, such as computational efficiency, sample complexity, generalization accuracy, consistency, lower and upper bounds on the convergence rates will also be investigated. Gaussian processes (GPs) are among the most popular nonparametric Bayesian function approximation methods. However, the standard GP methods are limited to at most a few thousands data points, and not applicable for large datasets. Kernel learning for GPs is an even more challenging problem. The question of how to scale up GP kernel learning methods for large datasets will be addressed as part of this project. Using the machine learning methods developed in this proposal, the following cosmology and astrophysics problems will be investigated: a) Scalable Gaussian processes with spectral mixture kernels will be used to build a probabilistic generative model for light intensity signals from stars to extract fundamental properties such as density profiles. b) New machine learning algorithms will be used to evolve the matter density of the Universe at a speed much higher than the traditional method of N-body simulations. This will enable a completely new way of generating a large number of cosmological simulations in order to compare the cosmological observations to our understanding of the Universe. c) Simulated galaxy catalogs are a powerful tool for testing cosmological analysis methods, since the cosmological parameters in the simulation are known and thus our ability to recover them can be tested perfectly. For a single cosmological simulation, the properties and alignments of galaxies are not fully determined, but rather must be added probabilistically to the dark matter distribution as an extra layer of modeling typically with many parameters.  The new machine learning tools developed in this proposal will be used to make ""mock catalogs"" with all commonly observable galaxy properties."
"1624731","I/UCRC: Center for Advanced Electronics through Machine Learning (CAEML)","CNS","IUCRC-Indust-Univ Coop Res Ctr","08/01/2016","09/18/2019","Madhavan Swaminathan","GA","Georgia Tech Research Corporation","Continuing Grant","Behrooz Shirazi","07/31/2021","$750,000.00","","madhavan.swaminathan@ece.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","5761","5761","$0.00","The semiconductor industry is perennially one of America's top exporters. Worldwide semiconductor <br/>sales for 2014 reached $335.8 billion, and the number of U.S. jobs in this sector was estimated to be <br/>around 250,000 in 2013. More broadly, the U.S. tech industry, which depends on semiconductor <br/>innovation to spur new products and applications, is itself estimated to represent no less than 5.7% of the <br/>entire U.S. private sector workforce (at nearly 6.5 million jobs), and with a tech industry payroll of $654 <br/>billion in 2014, it accounted for over 11% of all U.S. private sector payroll. Yet despite its success, the <br/>industry must continue to innovate if the U.S. is to retain global leadership in this highly competitive area. <br/>The complexity of modern microelectronic products necessitates the use of computer tools to formulate <br/>and verify product designs prior to manufacturing. When a product doesn't operate as intended or suffers <br/>early failures, this can often be attributed to inadequacy of the models used during the design process. In <br/>fact, the shortcomings of existing approaches for system component modeling have become a serious <br/>impediment to continued innovation.<br/><br/>The Center for Advanced Electronics through Machine Learning (CAEML) proposes to create <br/>machine-learning algorithms to derive models used for electronic design automation with the objective of <br/>enabling fast, accurate design of microelectronic circuits and systems. Success will make it much easier <br/>and cheaper to optimize a system design, allowing the industry to produce lower-power and lower-cost <br/>electronic systems without sacrificing functionality. The eventual result will be significant growth in <br/>capabilities that will drive innovation throughout the electronics industry, leading to new devices and <br/>applications, continued entrepreneurial leadership, and economic growth.<br/><br/>While achieving those goals, CAEML will also focus on diversifying the undergraduate engineering <br/>student body and improving the undergraduate experience. Students from groups traditionally <br/>underrepresented in engineering will be targeted for recruitment as undergraduate research assistants. <br/>Member companies will provide internships and mentors for participating students, and the diverse <br/>graduate and undergraduate student researchers in CAEML will receive hands-on multidisciplinary <br/>education. CAEML will also participate in all three site universities' existing avenues for student and <br/>faculty engagement with local youth. In particular, university-based summer camps are a tried and tested <br/>method of making high-school students familiar with and comfortable on our campuses. The HOT DAYS <br/>@ Georgia Tech (https://www.ece.gatech.edu/outreach/hot-days)  camp is a week-long summer program <br/>designed to introduce high-school students to electrical and computer engineering concepts through <br/>various half-day modules including building a computer, working with robots, using music synthesis <br/>technology, building simple digital logic circuits and constructing a speaker from common household <br/>items. Additional modules covering CAEML research areas will be developed and incorporated into the <br/>camp's schedule. CAEML undergraduate and graduate students can serve as counselors or instructors <br/>for camps. Georgia Tech's Center for Education Integrating Science, Mathematics, and Computing <br/>(CEISMC) hosts a variety of camps and programs for K-12 teachers, as well as students. CAEML faculty <br/>at Georgia Tech will participate in that effort as well as the NSF-funded Summer Teacher Experience in <br/>Packaging, Utilizing Physics (STEP-UP) Program (https://www.ece.gatech.edu/outreach/step-up-<br/>program), which is an eight-week research experience for metro Atlanta high-school physics teachers.<br/><br/><br/><br/>The Center for Advanced Electronics through Machine Learning (CAEML) will create machine-<br/>learning algorithms to derive models used for electronic design automation, with the objective of enabling <br/>fast, accurate design of microelectronic circuits and systems. The electronics industry's continued ability <br/>to innovate requires the creation of optimization methodologies that result in low-power integrated <br/>systems that meet performance specifications, despite being composed of components whose <br/>characteristics exhibit variability and that operate in different physical or signal domains. Today, <br/>shortcomings in accuracy and comprehensiveness of component-level behavioral models impede the <br/>advancement of computer-aided electronic system design optimization. The model accuracy also impacts <br/>system verification. Ultimately, the proper functionality of an electronic system is verified through testing <br/>of a representative sample. However, modern electronic systems are so complex that it is unthinkable to <br/>bring one to the manufacturing stage without first verifying its operation using simulation. Today, <br/>simulation generally does not ensure that an integrated circuit or electronic system will pass qualification <br/>testing the first time, and failures are often attributed to insufficiency of the simulation models. With an <br/>improved modeling capability, one could achieve better design efficiency, and also perform design <br/>optimization. For system simulation, behavioral models of the components' terminal responses are <br/>desired for both computational tractability and protection of intellectual property. Despite many years of <br/>significant effort by the electronic design automation community, there is not a general, systematic <br/>method to generate accurate and comprehensive behavioral models, in part because of the nonlinear, <br/>complex, and multi-port nature of the components being modeled.<br/><br/>CAEML will pioneer the use of machine-learning methods to extract behavioral models of electronic <br/>components and subsystems from simulation waveforms and/or measurement data. The Center will make <br/>2 primary contributions to the field of machine learning: it will demonstrate the application of machine <br/>learning to electronics modeling, and develop the entire machine-learning pipeline. Historically, machine-<br/>learning theorists have focused on the model learning and evaluation tasks, but CAEML will focus on <br/>end-to-end performance of the pipeline, including data acquisition, selection and filtering, as well as cost <br/>function specification. CAEML will develop a methodology to use prior knowledge, i.e., physical <br/>constraints and the domain knowledge provided by designers, to speed up the learning process. Novel <br/>methods of incorporating component variability, including that due to semiconductor process variations, <br/>will be developed. The intended end-users are electronic design automation (EDA) tool developers, IC <br/>design houses, and system design and manufacturing companies.<br/><br/>CAEML consists of 3 sites: Illinois, Georgia Tech, and NC State. The scope of research at each site <br/>encompasses both algorithm development and the application of the derived models to a variety of IC <br/>and system design tasks. Investigators at all 3 university sites have unique skills and expertise while <br/>sharing interests in electronic design automation, IC design, system-level signal integrity, and power <br/>distribution. To leverage the cross-campus expertise, many of the Center's proposed projects involve <br/>investigators from more than one site. The Georgia Tech investigators have special expertise in <br/>advanced IC packaging, power integrity, multi-physics simulation, computational electromagnetics, neural <br/>networks, optimization and system integration. All three sites have strong research records in the fields of <br/>signal integrity analysis and electronic design automation. Excellent computational resources are <br/>available at Georgia Tech for the proposed work including extensive measurement and fabrication <br/>facilities."
"1564055","TWC: Medium: Collaborative: Efficient Repair of Learning Systems via Machine Unlearning","CNS","Secure &Trustworthy Cyberspace","09/01/2016","03/01/2016","Junfeng Yang","NY","Columbia University","Standard Grant","Wei-Shinn Ku","08/31/2021","$600,068.00","","junfeng@cs.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","8060","7434, 7924","$0.00","Today individuals and organizations leverage machine learning systems to adjust room temperature, provide recommendations, detect malware, predict earthquakes, forecast weather, maneuver vehicles, and turn Big Data into insights. Unfortunately, these systems are prone to a variety of malicious attacks with potentially disastrous consequences. For example, an attacker might trick an Intrusion Detection System into ignoring the warning signs of a future attack by injecting carefully crafted samples into the training set for the machine learning model (i.e., ""polluting"" the model). This project is creating an approach to machine unlearning and the necessary algorithms, techniques, and systems to efficiently and effectively repair a learning system after it has been compromised. Machine unlearning provides a last resort against various attacks on learning systems, and is complementary to other existing defenses.  <br/><br/>The key insight in machine unlearning is that most learning systems can be converted into a form that can be updated incrementally without costly retraining from scratch. For instance, several common learning techniques (e.g., naive Bayesian classifier) can be converted to the non-adaptive statistical query learning form, which depends only on a constant number of summations, each of which is a sum of some efficiently computable transformation of the training data samples. To repair a compromised learning system in this form, operators add or remove the affected training sample and re-compute the trained model by updating a constant number of summations. This approach yields huge speedup -- the asymptotic speedup over retraining is equal to the size of the training set. With unlearning, operators can efficiently correct a polluted learning system by removing the injected sample from the training set, strengthen an evaded learning system by adding evasive samples to the training set, and prevent system inference attacks by forgetting samples stolen by the attacker so that no future attacks can infer anything about the samples."
"1546452","BIGDATA: Collaborative Research: F: Nomadic Algorithms for Machine Learning in the Cloud","IIS","Big Data Science &Engineering","01/01/2016","09/14/2015","Inderjit Dhillon","TX","University of Texas at Austin","Standard Grant","Sylvia Spengler","12/31/2020","$610,432.00","","inderjit@cs.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","8083","7433, 8083","$0.00","With an ever increasing ability to collect and archive data, massive data sets are becoming increasingly common. These data sets are often too big to fit into the main memory of a single computer, and so there is a great need for developing scalable and sophisticated machine learning methods for their analysis. In particular, one has to devise strategies to distribute the computation across multiple machines. However, stochastic optimization  and inference algorithms that are so effective for large-scale machine learning appear to be inherently sequential.<br/><br/>The main research goal of this project is to develop a novel ""nomadic"" framework that overcomes this barrier.  This will be done by showing that many modern machine learning problems have a certain ""double separability"" property. The aim is to exploit this property to develop convergent,  asynchronous, distributed, and fault tolerant algorithms that are well-suited for achieving high performance on commodity hardware that is prevalent on today's cloud computing platforms. In particular, over a four year period, the following will be developed: (i) parallel stochastic optimization algorithms for the multi-machine cloud computing setting, (ii) theoretical guarantees of convergence, (iii) open source code under a permissive license, (iv) application of these techniques to a variety of problem domains such as topic models and mixture models. In addition, a cohort of students who can transfer their skills to both industry and academia will be trained, and a graduate level course on scalable machine learning will be developed.<br/> <br/>The proposed research will enable practitioners in different application areas to quickly solve their big data problems.  The results of the project will be disseminated widely through papers and open source software. Course material will be developed for the education of students in the area of Scalable Machine Learning, and the course will be co-taught at UCSC and UT Austin. The project will recruit women and minority students."
"1622320","CDS&E: Collaborative Research: Machine Learning on Dynamical Systems via Topological Features","DMS","CDS&E-MSS","09/01/2016","06/27/2016","Elizabeth Munch","NY","SUNY at Albany","Standard Grant","Yong Zeng","12/31/2017","$101,672.00","","muncheli@egr.msu.edu","1400 WASHINGTON AVE MSC 100A","Albany","NY","122220100","5184374974","MPS","8069","8083, 9263","$0.00","Objects whose state changes over time, known as dynamical systems, describe a large number of natural and engineered processes; therefore, developing a deeper understanding of their behavior is of great importance.  While sometimes it is possible to derive mathematical models that describe the evolution of a dynamical system, these models are almost always an abstraction of the physical system and, therefore, have a limited ability to predict how the system will change in time.  Further, when the system under investigation is large or too complicated with several factors influencing its behavior, it may simply be impossible to describe the system with the corresponding descriptive equations.  Consequently, in the absence of adequate analytical models it becomes necessary to instrument the dynamical system with sensors and use the resulting data to understand its characteristics.  Specifically, the change in the state of a dynamic system is often governed by an underlying skeleton that gives the overall behavior a shape, and thus the shape of the skeleton directly governs the system behavior.  Most of the time, this shape of the underlying skeleton is unknown and can be easily masked by the complicated and rich system signals.  The emergent field of topological data analysis (TDA), a branch of mathematics that quantifies the shape of data, is capable of revealing information that is invisible to other existing methods by providing a high level X-ray of the skeleton governing the dynamics.  However, the information-rich structures provided by TDA still need to be interpreted in order to classify the dynamics and predict future outcomes.  To accomplish this, the principal investigators will leverage ideas from machine learning, a field of study that investigates algorithms that can learn from the data and use the acquired knowledge for classification and prediction.  However, the mathematical theory that elucidates how machine learning can operate on the features extracted using TDA currently does not exist.  Hence, this work will develop the necessary, novel mathematical and computational tools at the intersection of topological data analysis (TDA), dynamical systems, and machine learning.<br/><br/>The principal investigators seek to understand and formulate the foundations of machine learning when the important features of a dynamical system are summarized by descriptors generated with topological data analysis (TDA).  Although these signatures provide an information-rich structure for the evolution of the dynamics, current literature has only been utilizing a fraction of the available information in order to identify, predict, and classify different dynamic behavior.  One of the current impediments to further exploring the relationship between TDA and dynamical systems is the lack of machine learning theory that can operate on these structures. Therefore, the success of our effort will lead to (1) the establishment of a novel, general, and robust machine learning framework for studying dynamic signals via topological signatures, (2) better understanding of the relationship between TDA and dynamical systems via the use of these methods on real and synthetic data, and (3) the integration of the new knowledge into the investigators' educational programs, which will provide timely training of well-equipped next generation scientists and engineers."
"1622293","CDS&E: Collaborative Research: Machine Learning on Dynamical Systems via Topological Features","DMS","CDS&E-MSS","09/01/2016","06/27/2016","Firas Khasawneh","NY","SUNY Polytechnic Institute","Standard Grant","Yong Zeng","10/31/2017","$93,111.00","","khasawn3@msu.edu","257 Fuller Rd.","Albany","NY","122033603","5184378689","MPS","8069","8083, 9263","$0.00","Objects whose state changes over time, known as dynamical systems, describe a large number of natural and engineered processes; therefore, developing a deeper understanding of their behavior is of great importance.  While sometimes it is possible to derive mathematical models that describe the evolution of a dynamical system, these models are almost always an abstraction of the physical system and, therefore, have a limited ability to predict how the system will change in time.  Further, when the system under investigation is large or too complicated with several factors influencing its behavior, it may simply be impossible to describe the system with the corresponding descriptive equations.  Consequently, in the absence of adequate analytical models it becomes necessary to instrument the dynamical system with sensors and use the resulting data to understand its characteristics.  Specifically, the change in the state of a dynamic system is often governed by an underlying skeleton that gives the overall behavior a shape, and thus the shape of the skeleton directly governs the system behavior.  Most of the time, this shape of the underlying skeleton is unknown and can be easily masked by the complicated and rich system signals.  The emergent field of topological data analysis (TDA), a branch of mathematics that quantifies the shape of data, is capable of revealing information that is invisible to other existing methods by providing a high level X-ray of the skeleton governing the dynamics.  However, the information-rich structures provided by TDA still need to be interpreted in order to classify the dynamics and predict future outcomes.  To accomplish this, the principal investigators will leverage ideas from machine learning, a field of study that investigates algorithms that can learn from the data and use the acquired knowledge for classification and prediction.  However, the mathematical theory that elucidates how machine learning can operate on the features extracted using TDA currently does not exist.  Hence, this work will develop the necessary, novel mathematical and computational tools at the intersection of topological data analysis (TDA), dynamical systems, and machine learning.<br/><br/>The principal investigators seek to understand and formulate the foundations of machine learning when the important features of a dynamical system are summarized by descriptors generated with topological data analysis (TDA).  Although these signatures provide an information-rich structure for the evolution of the dynamics, current literature has only been utilizing a fraction of the available information in order to identify, predict, and classify different dynamic behavior.  One of the current impediments to further exploring the relationship between TDA and dynamical systems is the lack of machine learning theory that can operate on these structures. Therefore, the success of our effort will lead to (1) the establishment of a novel, general, and robust machine learning framework for studying dynamic signals via topological signatures, (2) better understanding of the relationship between TDA and dynamical systems via the use of these methods on real and synthetic data, and (3) the integration of the new knowledge into the investigators' educational programs, which will provide timely training of well-equipped next generation scientists and engineers."
"1553116","CAREER: Learning and Using Models of Geo-Temporal Appearance","IIS","Robust Intelligence, EPSCoR Co-Funding","07/01/2016","03/14/2019","Nathan Jacobs","KY","University of Kentucky Research Foundation","Standard Grant","Jie Yang","06/30/2021","$523,426.00","","jacobs@cs.uky.edu","109 Kinkead Hall","Lexington","KY","405260001","8592579420","CSE","7495, 9150","1045, 7495, 9150, 9251","$0.00","Billions of geotagged and time-stamped images are publicly available via the Internet, providing a rich record of the appearance of people, places, and things across the globe. These images are a largely untapped resource that could be used to improve our understanding of the world and how it changes over time. This project develops automated methods of extracting useful information from this imagery and fusing it into high-resolution global models that capture geo-temporal trends. Once the trends have been captured, these models are used to improve performance on computer vision tasks and make geotagged imagery a usable and navigable resource for education and research in other disciplines. The project includes an education and outreach component that brings real-world problems to computer science (CS) students, mentors students across the educational spectrum, and makes the research accessible to the public.<br/><br/>This project develops computer vision technologies to capture spatial and temporal appearance trends and is organized into four main research thrusts: (1) investigating novel methods for extracting information from Internet imagery using weakly supervised learning, (2) developing techniques that integrate ground-level imagery with aerial and satellite data to model the expected image appearance anywhere in the world at any time, (3) evaluating methods for using such models to improve the performance of computer vision algorithms, and (4) automatically creating visual representations that make it possible for novice users to explore the learned geo-temporal trends via the Internet.<br/><br/>Project webpage: http://geotemporal.csr.uky.edu"
"1649706","RI: Small: Workshop for Women in Machine Learning","IIS","Information Technology Researc, Robust Intelligence","11/01/2016","08/29/2016","Finale Doshi-Velez","MA","Harvard University","Standard Grant","Rebecca Hwa","10/31/2019","$49,000.00","","finale@seas.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","1640, 7495","1640, 7495, 7556","$0.00","This grant supports the annual workshop for Women in Machine Learning (WiML). WiML brings together female researchers in industry and academia, postdoctoral fellows, and graduate students from the machine learning community to exchange research ideas and build mentoring and networking relationships. The one-day workshop is especially beneficial for junior graduate students, giving them a supportive environment in which to present their research (in many cases, for the first time) and enabling them to meet peers and more senior researchers in the field of machine learning. The networking opportunities provided by the workshop have also helped senior graduate students find jobs following graduation. This workshop will foster collaboration within the machine learning community. Established researchers will expose workshop participants about cutting-edge ideas from diverse areas of machine learning. Students will present their own research and receive valuable feedback from both senior researchers and their peers. By enabling women at all stages of their careers in machine learning to exchange research ideas and form new relationships, we expect that new connections and research collaborations will be established, thereby advancing the state-of-the-art of the field. <br/><br/> This workshop will provide a forum for female graduate students, postdoctoral fellows, junior and senior faculty, and industry and government research scientists to exchange research ideas and establish networking and mentoring relationships. Undergraduates, particularly those who are interested in pursuing graduate school or industry positions in machine learning, are also welcome to attend. Bringing together women from different stages of their careers gives established researchers the opportunity to act as mentors, and enables junior women to find female role models working in the field of machine learning. The workshop will also benefit the wider machine learning community: The WiML website includes a directory of over 400 women for organizations looking for female invited speakers.  Co-locating with a major machine learning conference (a) enhances the visibility of the participants in he broader community and (b) facilitates travel for WiML attendees to stay on for the main conference."
"1619098","TWC: Small: Automatic Techniques for Evaluating and Hardening Machine Learning Classifiers in the Presence of Adversaries","CNS","Secure &Trustworthy Cyberspace","09/01/2016","04/11/2018","Yanjun Qi","VA","University of Virginia Main Campus","Standard Grant","Wei-Shinn Ku","08/31/2019","$494,884.00","David Evans, Westley Weimer","yq2h@virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","8060","7434, 7923, 9102","$0.00","New security exploits emerge far faster than manual analysts can analyze them, driving growing interest in automated machine learning tools for computer security. Classifiers based on machine learning algorithms have shown promising results for many security tasks including malware classification and network intrusion detection, but classic machine learning algorithms are not designed to operate in the presence of adversaries. Intelligent and adaptive adversaries may actively manipulate the information they present in attempts to evade a trained classifier, leading to a competition between the designers of learning systems and attackers who wish to evade them. This project is developing automated techniques for predicting how well classifiers will resist the evasions of adversaries, along with general methods to automatically harden machine-learning classifiers against adversarial evasion attacks.<br/><br/>At the junction between machine learning and computer security, this project involves two main tasks: (1) developing a framework that can automatically assess the robustness of a classifier by using evolutionary techniques to simulate an adversary's efforts to evade that classifier; and (2) improving the robustness of classifiers by developing generic machine learning architectures that employ randomized models and co-evolution to automatically harden machine-learning classifiers against adversaries. Our system aims to allow a classifier designer to understand how the classification performance of a model degrades under evasion attacks, enabling better-informed and more secure design choices. The framework is general and scalable, and takes advantage of the latest advances in machine learning and computer security."
"1622678","SCH: INT: Anesthesiology Control Tower: Forecasting Algorithms to Support Treatment (ACTFAST)","IIS","Smart and Connected Health","09/01/2016","08/22/2016","Michael Avidan","MO","Washington University","Standard Grant","Wendy Nilsen","08/31/2019","$589,998.00","Yixin Chen","avidanm@anest.wustl.edu","CAMPUS BOX 1054","Saint Louis","MO","631304862","3147474134","CSE","8018","8018, 8062","$0.00","There is a significant public health concern in the United States regarding major complications and death following surgery. Forty million Americans undergo surgery yearly. Approximately five percent die within a year of their operation, and roughly ten percent suffer major in-hospital morbidity (e.g., stroke, heart attack, pneumonia, renal failure, wound infection). Early recognition of risk and appropriate management could often prevent or modify these adverse outcomes. We believe that this represents an exciting scientific opportunity. Modern intraoperative monitoring yields a wealth of data from thousands of operating rooms across the US.  Integration and real-time analysis of these data streams has the potential to revolutionize perioperative care. We propose to develop, validate and assess machine-learning, forecasting algorithms that predict adverse outcomes for individual patients. The forecasting algorithms will be based on data derived from various sources, including the patient's electronic medical record, the array of physiological monitors in the operating room, and evidence-based scientific literature. These algorithms will facilitate patient-specific clinical decision support, utilizing an innovative approach of an anesthesiology control tower. The anesthesiology control tower for the operating room will be conceptually similar to an air traffic control tower for a busy airport. We are optimistic that the ambitious goal of developing forecasting algorithms for individual surgical patients can be realized, since members of our team have already developed and validated similar forecasting algorithms for critically ill hospital patients. Modifiable risk factors for adverse events could be detected early during surgery, allowing targeted interventions and preventive measures. The ACTFAST study will provide important information on the potential utility of incorporating forecasting algorithms into routine surgical care, including in under-resourced healthcare settings. This project will also yield important educational benefits. There will be tremendous learning for the students who help to develop and validate the forecasting algorithms. Furthermore, the control tower concept is a disruptive educational innovation, which will equip anesthesiology trainees with a new ability to provide simultaneous care to multiple surgical patients. It is notoriously difficult to construct high fidelity scientific models for individual humans, as we are complex biological systems. Ultimately, the success of this ambitious project, which engages interdisciplinary perspectives and applies sophisticated forecasting algorithms to clinical decision support, will have substantial scientific and clinical impact.<br/><br/>Modern intraoperative monitoring yields a wealth of data from thousands of operating rooms across the United States.  Integration and real-time analysis of these data streams has the potential to revolutionize perioperative care. The objective for this investigation is to exploit our experience in running innovative machine learning algorithms, including filtering and outcome-related models, in order to build forecasting algorithms tailored to individual surgical patients. Our central hypothesis is that with sufficient knowledge of patient characteristics coupled with repeated, high-fidelity time series data from the intraoperative electronic medical record, advanced models can be constructed for individual patients that will forecast risk for adverse postoperative outcomes. First, using a training dataset, we will apply data mining and machine-learning methods that we have previously validated to extract useful information from electronic health records and real-time physiological variables. We will develop algorithms using hybrid-learning techniques to combine the strength of non-parametric (generative) models, such as histogram and kernel density estimation, and parametric (discriminative) models, such as support vector machines, logistic regressions, and kernel machines to improve predictions of adverse perioperative outcomes. The goal is to deliver superior prediction quality, with good interpretability and high computational efficiency, that supports fast processing of big data. Second, using a testing dataset, we will validate the predictive accuracy of the developed algorithms, by determining the reliability in forecasting adverse outcomes. The developed algorithms will be tested for accuracy of their predictive performance. These evaluation methods include hold-out sampling, cross-validation, and bootstrap sampling. After being trained and tested, the performance of the developed algorithms will be additionally validated prospectively (out-of-sample performance), using standard measures of accuracy, precision and robustness. We envision that the developed algorithms will facilitate patient-specific clinical decision support, utilizing an innovative approach of an anesthesiology control tower, which will be conceptually similar to an air traffic control tower for a busy airport. The main contributions of this project will include: (1) new machine-learning algorithms for forecasting perioperative adverse events from heterogeneous, multi-scale, and high-dimensional data streams; (2) a clinical decision support system that identifies prognostic factors and suggests interventions based on novel feature ranking algorithms; and (3) a transformative approach to the education of the anesthesiology team and the paradigm of perioperative care. Successfully advancing real-time analytic methods and modeling for individual surgical patients, using heterogeneous and sparse data, would have tremendous scientific and societal impact."
"1622301","CDS&E: Collaborative Research: Machine Learning on Dynamical Systems via Topological Features","DMS","CDS&E-MSS","09/01/2016","06/27/2016","Jose Perea","MI","Michigan State University","Standard Grant","Christopher Stark","08/31/2019","$105,000.00","","joperea@msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","MPS","8069","8083, 9263","$0.00","Objects whose state changes over time, known as dynamical systems, describe a large number of natural and engineered processes; therefore, developing a deeper understanding of their behavior is of great importance.  While sometimes it is possible to derive mathematical models that describe the evolution of a dynamical system, these models are almost always an abstraction of the physical system and, therefore, have a limited ability to predict how the system will change in time.  Further, when the system under investigation is large or too complicated with several factors influencing its behavior, it may simply be impossible to describe the system with the corresponding descriptive equations.  Consequently, in the absence of adequate analytical models it becomes necessary to instrument the dynamical system with sensors and use the resulting data to understand its characteristics.  Specifically, the change in the state of a dynamic system is often governed by an underlying skeleton that gives the overall behavior a shape, and thus the shape of the skeleton directly governs the system behavior.  Most of the time, this shape of the underlying skeleton is unknown and can be easily masked by the complicated and rich system signals.  The emergent field of topological data analysis (TDA), a branch of mathematics that quantifies the shape of data, is capable of revealing information that is invisible to other existing methods by providing a high level X-ray of the skeleton governing the dynamics.  However, the information-rich structures provided by TDA still need to be interpreted in order to classify the dynamics and predict future outcomes.  To accomplish this, the principal investigators will leverage ideas from machine learning, a field of study that investigates algorithms that can learn from the data and use the acquired knowledge for classification and prediction.  However, the mathematical theory that elucidates how machine learning can operate on the features extracted using TDA currently does not exist.  Hence, this work will develop the necessary, novel mathematical and computational tools at the intersection of topological data analysis (TDA), dynamical systems, and machine learning.<br/><br/>The principal investigators seek to understand and formulate the foundations of machine learning when the important features of a dynamical system are summarized by descriptors generated with topological data analysis (TDA).  Although these signatures provide an information-rich structure for the evolution of the dynamics, current literature has only been utilizing a fraction of the available information in order to identify, predict, and classify different dynamic behavior.  One of the current impediments to further exploring the relationship between TDA and dynamical systems is the lack of machine learning theory that can operate on these structures. Therefore, the success of our effort will lead to (1) the establishment of a novel, general, and robust machine learning framework for studying dynamic signals via topological signatures, (2) better understanding of the relationship between TDA and dynamical systems via the use of these methods on real and synthetic data, and (3) the integration of the new knowledge into the investigators' educational programs, which will provide timely training of well-equipped next generation scientists and engineers."
"1563843","TWC: Medium: Collaborative: Efficient Repair of Learning Systems via Machine Unlearning","CNS","Special Projects - CNS, Secure &Trustworthy Cyberspace","09/01/2016","03/02/2017","Yinzhi Cao","PA","Lehigh University","Standard Grant","Dan Cosley","10/31/2018","$615,931.00","","ycao43@jhu.edu","Alumni Building 27","Bethlehem","PA","180153005","6107583021","CSE","1714, 8060","025Z, 7434, 7924, 9178, 9251","$0.00","Today individuals and organizations leverage machine learning systems to adjust room temperature, provide recommendations, detect malware, predict earthquakes, forecast weather, maneuver vehicles, and turn Big Data into insights. Unfortunately, these systems are prone to a variety of malicious attacks with potentially disastrous consequences. For example, an attacker might trick an Intrusion Detection System into ignoring the warning signs of a future attack by injecting carefully crafted samples into the training set for the machine learning model (i.e., ""polluting"" the model). This project is creating an approach to machine unlearning and the necessary algorithms, techniques, and systems to efficiently and effectively repair a learning system after it has been compromised. Machine unlearning provides a last resort against various attacks on learning systems, and is complementary to other existing defenses.  <br/><br/>The key insight in machine unlearning is that most learning systems can be converted into a form that can be updated incrementally without costly retraining from scratch. For instance, several common learning techniques (e.g., naive Bayesian classifier) can be converted to the non-adaptive statistical query learning form, which depends only on a constant number of summations, each of which is a sum of some efficiently computable transformation of the training data samples. To repair a compromised learning system in this form, operators add or remove the affected training sample and re-compute the trained model by updating a constant number of summations. This approach yields huge speedup -- the asymptotic speedup over retraining is equal to the size of the training set. With unlearning, operators can efficiently correct a polluted learning system by removing the injected sample from the training set, strengthen an evaded learning system by adding evasive samples to the training set, and prevent system inference attacks by forgetting samples stolen by the attacker so that no future attacks can infer anything about the samples."
"1640633","PFI: AIR - TT:  Vision-based ergonomic risk assessment of working postures","IIP","GOALI-Grnt Opp Acad Lia wIndus, Accelerating Innovation Rsrch","10/01/2016","08/29/2018","SangHyun Lee","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Jesus Soriano Molla","12/31/2018","$251,711.00","James Mallon","shdpm@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","ENG","1504, 8019","019Z, 116E, 8019, 9251","$0.00","This PFI: AIR Technology Translation project focuses on translating fundamental research findings in computer vision-based posture classification and pose estimation to fill the need for rapid and easy postural ergonomic risk assessment. This is applicable for occupational workers involved in physically demanding labor with awkward working postures, and who consequently suffer from work-related musculoskeletal disorders (WMSDs). The translated technology is important because it enables practitioners to identify potential WMSD risks in advance of actual injury, thus minimizing the potential injury risks. Given the massive number of WMSD cases (e.g., over 360,000 cases per year) and huge associated workers' compensation costs (e.g., over $14.2 billion per year) in the U.S., this research contributes to the development and maintenance of a safe, healthy, and productive workforce, which will have an ameliorative impact on society. The project will result in a fully-functioning software prototype of vision-based ergonomic risk assessment, which has the following unique features: rapid (e.g., near real-time to process video frames), easy (e.g., no training to assess ergonomic risks is needed), and automated (e.g., no need for manual risk assessment). These features allow practitioners to save a significant amount of time and effort on ergonomic risk assessment, when compared to the current manual observation-based risk assessment in the ergonomic industry. <br/><br/>This project addresses the following technology gaps as it translates from research discovery toward commercial application. First, a tracking-based pose estimation, which is a fundamental component to estimate posture severity, provides fast processing of video frames but needs to be robust without target loss, which can happen due to frequent occlusions and environmental noises in real worksites. In this research, the tracking-based approach will be integrated with a detection-based pose estimation along with the adoptions of different features like spatial information and reference frames from just processed ones. Once the prototype is developed, it will be tested in diverse real worksites such as manufacturing assembly lines to ensure its robustness and usability. In addition, personnel involved in this project, including graduate and undergraduate students, will receive entrepreneurship experience through interaction with ergonomic and business experts and potential customers (e.g., customer interaction, market study, and field validation) as well as engagement in the activities offered from the University of Michigan Center for Entrepreneurship (e.g., entrepreneurship classes and workshops). <br/><br/>The project engages Humantech, the largest ergonomic consulting firm in North America, UAW (United Automobile, Aerospace and Agricultural Implement workers of America), and a venture capitalist, to provide real-world test environment and guide commercialization aspects in this technology translation effort from research findings toward commercial reality."
"1546305","BIGDATA: Collaborative Research: IA: Quantifying Plankton Diversity with Taxonomy and Attribute Based Classifiers of Underwater Microscope Images","IIS","ADVANCES IN BIO INFORMATICS, Big Data Science &Engineering","10/01/2016","09/14/2016","Nuno Vasconcelos","CA","University of California-San Diego","Standard Grant","Elizabeth Blood","09/30/2019","$283,372.00","","nuno@ece.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","1165, 8083","7433, 8083","$0.00","Plankton play an essential role in the global ecosystem, forming the base of marine food webs, linking the atmosphere to the deep ocean, and regulating a myriad of ecologically and climatologically important processes. Despite their importance, however, the technology to assess abundances and distributions of plankton has been limited. Changes in abundances of individual species are particularly poorly resolved; this includes the harmful algal blooms that have profound economic, societal, and ecosystem effects in many coastal systems. Traditional tools such as nets and bottles can destroy fragile organisms during sampling. Underwater microscopes, on the other hand, allow observation of the organisms undisturbed, and in their natural setting. New underwater microscopes are generating many thousands of high-resolution images of individual plankton each day. Before these images can be used for scientific analyses, the imaged organisms must be identified and classified. However, the vast number of images generated by such microscopes has led to a serious bottleneck: identification and classification of the images takes an impossibly long time for individuals to accomplish. Fortunately, advances in computer vision science have shown great promise in accurately performing such classification tasks. The main goal of this award is to explore and develop computer vision approaches for plankton image classification. A team of instrumentation specialists, an ocean ecologist, and a computer scientist, including two graduate students and one post doctoral student, will formulate, implement, and test methods to advance the goal of efficient and accurate automated plankton image classification. The advances made in this award will enable both improved classification algorithms in computer science, and vast new data streams for plankton ecology.<br/><br/>Plankton form the base of marine food webs, link the atmosphere to the deep ocean, and regulate  global biogeochemical cycles. Plankton are often studied either through bulk measures, or by manual enumeration of individual taxa. Novel underwater microscope systems such as the Scripps Plankton Camera System (SPCS) are generating tens of thousands of images of individual plankton daily. However, without accurate annotation of the images, the potential science is limited. This project will explore the use of many-layer, deep Convolutional Neural Nets (CNN) as automated computer recognition methods; these techniques hold promise for classifying the nearly one trillion underwater microscope images that have been collected by a variety of research groups around the globe. The primary source of images will be a pair of microscopes that have been operating for 2 years from the Scripps Inst. of Oceanography's pier, yielding 200 million regions of interest. The project will build a large data base of training sets using a novel approach: a bench-top imaging system that is capable of rapidly producing thousands of annotated images showing organisms in all orientations and configurations identical to that in the field. Based on these automatically collected training sets, and hand annotation of in situ images from experts, a deep  (many layer) CNN will embed taxonomic and attribute constraints, and will be used to classify the organisms imaged. With success, this massive, growing, taxonomically classified dataset will enable unprecedented, transformative, taxon-specific explorations of the dynamics of the planktonic ecosystem on time scales from hours to decades."
"1546351","BIGDATA: Collaborative Research: IA: Quantifying Plankton Diversity with Taxonomy and Attribute Based Classifiers of Underwater Microscope Images","IIS","ADVANCES IN BIO INFORMATICS, BIOLOGICAL OCEANOGRAPHY, EarthCube, Big Data Science &Engineering","10/01/2016","09/14/2016","Jules Jaffe","CA","University of California-San Diego Scripps Inst of Oceanography","Standard Grant","Elizabeth Blood","09/30/2020","$916,113.00","Peter Franks","jjaffe@ucsd.edu","8602 La Jolla Shores Dr","LA JOLLA","CA","920930210","8585341293","CSE","1165, 1650, 8074, 8083","7433, 8083","$0.00","Plankton play an essential role in the global ecosystem, forming the base of marine food webs, linking the atmosphere to the deep ocean, and regulating a myriad of ecologically and climatologically important processes. Despite their importance, however, the technology to assess abundances and distributions of plankton has been limited. Changes in abundances of individual species are particularly poorly resolved; this includes the harmful algal blooms that have profound economic, societal, and ecosystem effects in many coastal systems. Traditional tools such as nets and bottles can destroy fragile organisms during sampling. Underwater microscopes, on the other hand, allow observation of the organisms undisturbed, and in their natural setting. New underwater microscopes are generating many thousands of high-resolution images of individual plankton each day. Before these images can be used for scientific analyses, the imaged organisms must be identified and classified. However, the vast number of images generated by such microscopes has led to a serious bottleneck: identification and classification of the images takes an impossibly long time for individuals to accomplish. Fortunately, advances in computer vision science have shown great promise in accurately performing such classification tasks. The main goal of this award is to explore and develop computer vision approaches for plankton image classification. A team of instrumentation specialists, an ocean ecologist, and a computer scientist, including two graduate students and one post doctoral student, will formulate, implement, and test methods to advance the goal of efficient and accurate automated plankton image classification. The advances made in this award will enable both improved classification algorithms in computer science, and vast new data streams for plankton ecology.<br/><br/>Plankton form the base of marine food webs, link the atmosphere to the deep ocean, and regulate  global biogeochemical cycles. Plankton are often studied either through bulk measures, or by manual enumeration of individual taxa. Novel underwater microscope systems such as the Scripps Plankton Camera System (SPCS) are generating tens of thousands of images of individual plankton daily. However, without accurate annotation of the images, the potential science is limited. This project will explore the use of many-layer, deep Convolutional Neural Nets (CNN) as automated computer recognition methods; these techniques hold promise for classifying the nearly one trillion underwater microscope images that have been collected by a variety of research groups around the globe. The primary source of images will be a pair of microscopes that have been operating for 2 years from the Scripps Inst. of Oceanography's pier, yielding 200 million regions of interest. The project will build a large data base of training sets using a novel approach: a bench-top imaging system that is capable of rapidly producing thousands of annotated images showing organisms in all orientations and configurations identical to that in the field. Based on these automatically collected training sets, and hand annotation of in situ images from experts, a deep  (many layer) CNN will embed taxonomic and attribute constraints, and will be used to classify the organisms imaged. With success, this massive, growing, taxonomically classified dataset will enable unprecedented, transformative, taxon-specific explorations of the dynamics of the planktonic ecosystem on time scales from hours to decades."
"1617333","CHS: Small: Shape Processing with Deep Architectures","IIS","HCC-Human-Centered Computing","08/01/2016","07/31/2017","Evangelos Kalogerakis","MA","University of Massachusetts Amherst","Continuing Grant","Ephraim Glinert","07/31/2020","$499,935.00","","kalo@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","7367","7367, 7923","$0.00","Digital representations of three-dimensional shapes are becoming an integral part of many scientific and engineering fields.  And 3D printers are becoming increasingly popular for transforming these digital representations into real objects for industrial or home use as tools, mechanical components, or household items.  Virtual environments for simulation, education, and entertainment require large numbers of diverse 3D models to maintain realism and operability.  In computer vision, several recent object recognition algorithms are trained on large synthetic datasets originating from 3D shape repositories.  All these fields and several others increasingly depend on the availability of digital 3D shapes.  However, it is still a challenge to develop tools that allow users to easily produce new 3D shapes, or even to retrieve and process existing ones from online repositories.  Current 3D modeling tools often require laborious user interaction via low-level selection and editing commands, while existing search engines for retrieving 3D shapes largely depend on manually entered tags and hand-tuned feature representations which results in unsatisfactory retrieval performance.  The PI's goal in this research is to create algorithms based on ""deep"" architectures that automatically learn from data how to reliably analyze and synthesize 3D shapes that are optimized for 3D shape processing and synthesis performance (so that, for example, when these shapes are 3D printed they retain desirable physical properties such as reduced mechanical stress).  Project outcomes will be released as open source code and will have broad impact not only on computer graphics and computer vision but also on industry, computer-aided design and mechanical engineering pipelines, and architectural engineering software for buildings and indoor environments.<br/><br/>This research will make three major contributions in terms of intellectual merit: 1) Deep architectures and algorithms for learning 3D shape feature representations optimized for retrieval and processing performance; the algorithms will be trained on massive 2D image datasets as well as 3D model repositories, and the learned representations will be used for accurate shape categorization, segmentation, correspondence, style analysis, and texturing.  2) Deep architectures and algorithms for learning to reliably generate new 3D shapes based on intuitive user input, such as sketches and textual descriptions.  3) Algorithms for learning to optimize the underlying geometry of 3D shapes such that they acquire desired physical properties for 3D printing and manufacturing."
"1618398","TWC: Small: Evidence of Presence for Intelligent Vehicles using Environment-Based Security","CNS","Secure &Trustworthy Cyberspace","08/15/2016","08/08/2016","Chiu Tan","PA","Temple University","Standard Grant","Phillip Regalia","07/31/2020","$416,938.00","Haibin Ling, Jie Wu","cctan@temple.edu","1801 N. Broad Street","Philadelphia","PA","191226003","2157077547","CSE","8060","7434, 7923","$0.00","Emerging intelligent automobiles will be able to harness advance on-car sensors to support new applications such as pollution detection, road condition monitoring, and traffic control. All these applications require the ability to verify both the location and the time of a reading. This project involves the design of verification methods that make use of environment factors, such as the presence of light and shadows and the measured wireless signal strength, instead of conventional public key infrastructure-based methods, in order to verify when and where data was collected.  This new environment-based paradigm is resilient against insider attacks, easier to deploy, and protects the individual car owner's privacy.  This research will help in realizing applications arising from the increasing presence of smarter vehicles on our roads which can benefit the public's wellbeing. The project also incorporates outreach components for high school and college students through the organization of science competitions, national undergraduate workshops, and summer research camp activities.  <br/><br/>This project is the first effort to use the wireless communication and video recording abilities of modern automobiles to capture natural environment characteristics to securely verify spatial-temporal claims in a vehicular network setting. This is an interdisciplinary research effort combining wireless networking and computer vision to address vehicular network security. The main project goals are: (1) explore new vision analytic algorithms in order to identify the location and time images are captured by an automobile camera;  (2) research new algorithms for identifying optimal roadside unit locations for location disambiguation to support wireless spatial-temporal verification; (3) develop new techniques for utilizing encounters with public vehicles for verifying spatial-temporal claims; (4) developing a fusion framework to combine wireless measurements, vehicular encounters, and visual images for spatial-temporal verification; and (5) perform realistic experiments on real roads and vehicular test bed to collect image and wireless datasets and evaluate the research. These datasets are of interest to both wireless networking and computer vision research communities, and will be made available to the public."
"1523893","NSF Postdoctoral Fellowship in Biology FY 2015","DBI","Biology Postdoctoral Research","01/01/2016","08/31/2015","Michael Harvey","LA","Harvey                  Michael        G","Fellowship Award","Amanda Simcox","12/31/2017","$138,000.00","","","","Baton Rouge","LA","708081472","","BIO","8049","7137, 9150","$0.00","This action funds an NSF Postdoctoral Research Fellowship in Biology for FY 2015, Research Using Biological Collections. The fellowship supports a research and training plan for the Fellow to take transformative approaches to grand challenges in biology that employ biological collections in highly innovative ways.  The title of the research plan for this fellowship to Michael Harvey is ""Assessing the importance of plumage divergence within species for the evolution of plumage diversity across birds."" The host institution for this fellowship is the University of Michigan, and the sponsoring scientist is Dr. Daniel Rabosky<br/><br/>The research focuses on connecting variation among individuals within species to differences across species. Individuals within a species often differ greatly in genetic make-up (genotype) as well as appearance (phenotype). Some of these differences are associated with geography such that individuals from one region are readily distinguishable from those from another region. It is unclear, however, what the implications of this geographic variation are over longer evolutionary timescales. Do species with greater variation tend to produce more species or last longer than species with limited variation? The fellowship research is assessing whether the pattern and rate of phenotypic divergence within bird species predicts patterns or rates of phenotypic diversification across species. Using digital photography and measurements from computer vision, data on phenotypic variation in birds are being gathered from museum specimens at the Museum of Zoology at the University of Michigan and other museums as needed. Estimates are then made of evolutionary history using genetic data also gathered from collections to model the history of phenotype evolution both within and across species. These studies reveal what correlations exist between phenotypic divergence within species and phenotypic diversification among species and represent one of the first comparative analysis of phenotypic divergence within and across species at a large scale.  Then it can be determined what types of variation within a species have lasting impacts on the evolutionary trajectory of its descendants.<br/><br/>Training goals include gaining expertise in using digital imagery and computer vision to quantify phenotypes. Educational outreach to undergraduate students involves them in using museum collections, conducting high-throughput digital photography, training computer vision algorithms, and learning genetic techniques."
"1618717","AF: Small: New classes of optimization methods for nonconvex large scale machine learning models.","CCF","COMPUTATIONAL MATHEMATICS, Algorithmic Foundations","09/01/2016","06/12/2019","Frank Curtis","PA","Lehigh University","Standard Grant","Tracy Kimbrel","08/31/2020","$499,143.00","Katya Scheinberg, Frank Curtis, Martin Takac","fec309@lehigh.edu","Alumni Building 27","Bethlehem","PA","180153005","6107583021","CSE","1271, 7796","7923, 7929, 9263","$0.00","Intelligent systems that say, recommend music or movies based on past interests, or recognize faces or handwriting based on labeled samples, often learn from examples using ""supervised learning.""  The system tries to find a prediction function: a combination of feature values of the song, movie, image, or pen movements, that, on known inputs, produces score values that agree with known preferences. Some combinations may add with simple positive or negative weight parameters (The more guitar the better, or I really don't want accordion), while others can be more complex (nether too loud nor too soft).  If parameters for such a function can be found, then it can be hoped that, on a new input, the function will be a good approximation for the preference. <br/><br/>In scientific computing, there are many optimization techniques used to find the best parameters.  The type called ""gradient methods"" is like a group hike that gets caught in the hills after dark; the members want to go downhill to return to the valley quickly, but take small steps so as not to trip. With a little light, the group can discover more about its vicinity to 1) suggest the best direction, 2) take longer steps without tripping, or 3) send different members in different directions so that someone finds the best way. When there are many parameters (not just latitude and longitude) there are many more directions to step.  Simple combinations define simple (aka convex) valleys, and many optimization-based learning methods (including support vector machines (SVM),  least squares, and logistic regression) have been effectively applied to find the best parameters.  More complex combinations that sometime lead to better learning, may define non-convex valleys, so the known methods may get stuck in dips or have to take very small steps -- they often lack theoretical convergence guarantees and do not always work well in practice. <br/><br/>This project will explore non-convex optimization for machine learning with three techniques that are analogous to the hikers? use of the light: <br/>First, new techniques will be explored for exploiting approximate second-order derivatives within stochastic methods, which is expected to improve performance over stochastic gradient methods, avoid convergence to saddle points, and improve complexity guarantees over first-order approaches. Compared to other such techniques that have been proposed, these approaches will be unique as they will be set within trust-region frameworks, the exploration of which represents the second component of the project. Known for decades to offer improved performance for nonconvex optimization, trust region algorithms have not fully been explored for machine learning, and we believe that, when combined with second-order information, dramatic improvements (both theoretically and practically) can be achieved. Finally, for such methods to be efficient in large-scale settings, one needs to offer techniques for solving trust region subproblems in situations when all data might not be stored on a single computer. To address this,  parallel and distributed optimization techniques will be developed for solving trust region subproblems and related problems.  The three PIs work together with about a dozen students at Lehigh; their website is one way they disseminate research papers, software, and news of weekly activities. <br/><br/>This project is funded jointly by NSF CISE CCF Algorithmic Foundations, and NSF MPS DMS Computational Mathematics."
"1553340","CAREER: Toward A Machine Learning Framework for the Internet of Things","CNS","CSR-Computer Systems Research","01/01/2016","02/24/2020","Stacy Patterson","NY","Rensselaer Polytechnic Institute","Continuing Grant","Marilyn McClure","12/31/2020","$618,661.00","","sep@cs.rpi.edu","110 8TH ST","Troy","NY","121803522","5182766000","CSE","7354","1045, 7354","$0.00","This project will develop a new paradigm and tools for machine learning that can cope with the massive-scale, geographically distributed data in the Internet of Things. The key innovation is to use devices and computing power within the Internet of Things network itself to perform data analysis in a scalable, reliable fashion.<br/><br/>The Internet of Things describes a network of devices, from RFID tags, to smart thermostats, to light bulbs, that can sense and communicate information. It is predicted that by 2020, there will be 25 to 50 billion devices in the Internet of Things. This massive network and the data it generates will enable new applications in a wide range of critical domains including environmental management, smart infrastructure, and healthcare. To achieve this vision, it is crucial to be able to quickly analyze and learn from the massive amount of generated data.  Current approaches for big data analytics require full data transfer to a platform with large computational power, such as the cloud. Given the projected explosion in the number of devices and the resulting data generation rate, this is not feasible. <br/><br/>  The proposed research integrates tools and theory from machine learning, distributed computing, and networked systems in three main thrusts that include; a computational framework that provides an abstraction for algorithm design and implementation that is flexible enough to support a wide collection of machine learning methods, a framework implementation that provides a stable platform for algorithm developers by masking device heterogeneity, devices failures, and the network dynamics of the Internet of Things, and development and implementation of techniques to adapt the network and computation to support algorithm execution with performance guarantees.<br/><br/>The framework developed in this project will facilitate rapid development and deployment of Internet of Things applications. A significant contribution will be an open source software implementation of the framework allowing others with limited network expertise to develop their own applications for the Internet of Things. The project also includes robust educational and outreach components including graduate and undergraduate research, curriculum development, and activities to promote and support the participation of women in computer science."
"1619078","RI: Small: Modeling and Learning Visual Similarities Under Adverse Visual Conditions","IIS","Robust Intelligence","09/01/2016","07/27/2016","Ying Wu","IL","Northwestern University","Standard Grant","Jie Yang","08/31/2021","$440,000.00","","yingwu@eecs.northwestern.edu","750 N. Lake Shore Drive","Chicago","IL","606114579","3125037955","CSE","7495","7495, 7923","$0.00","In many emerging applications such as autonomous/assisted driving, intelligent video surveillance, and rescue robots, the performances of visual sensing and analytics are largely jeopardized by various adverse visual conditions in complex unconstrained environments, e.g., bad weather and illumination conditions. This project studies how and to what extend such adverse visual conditions can be coped with. It will advance and enrich the fundamental research of computer vision, and bring significant impact on developing ""all-weather""computer vision systems that benefit security/safety, autonomous driving, and robotics. The project contributes to education through curriculum development, student training, and knowledge dissemination. It also includes interactions with K-12 students for participation and research opportunities.  <br/><br/>This research seeks innovative solution to overcome adverse visual conditions for visual sensing and analytics. It explores a unified approach that avoids explicit image restoration that is in general computationally demanding. It is focused on learning the ""alignment"" between the two image spaces under adverse and normal conditions, rather than learn everything from scratch. Acting on low-quality data directly without image restoration, this research leads to innovative and computationally efficient solutions to handle adverse visual conditions. Visual restoration can also be done as by-products, and the same approach also provides a general solution to target attribute estimation. The research is focused on: (1) constructing a principled model, called space alignment that models and learns visual similarity, and its theoretical foundation, (2) developing new effective visual matching and tracking approaches based on learning the appropriate visual similarity under various adverse visual condition, (3) investigating visual attribute estimation and identification via learning reconstruction-based visual regression, and (4) developing effective and efficient tools and prototype systems for visual detection, identification, tracking and recognition."
"1565328","CHS: Large: Collaborative Research: Computational Science for Improving Assessment of Executive Function in Children","IIS","HCC-Human-Centered Computing","10/01/2016","06/18/2020","Fillia Makedon","TX","University of Texas at Arlington","Standard Grant","Ephraim Glinert","09/30/2021","$1,371,495.00","Vassilis Athitsos","makedon@cse.uta.edu","701 S Nedderman Dr, Box 19145","Arlington","TX","760190145","8172722105","CSE","7367","7367, 7925, 9251","$0.00","The identification of cognitive impairments in early childhood provides the best opportunity for successful remedial intervention, because brain plasticity diminishes with age.  Attention deficit hyperactivity disorder (ADHD) is a psychiatric neurodevelopmental disorder that is very hard to diagnose or tell apart from other disorders.  Symptoms include inattention, hyperactivity, or acting impulsively, all of which often result in poor performance in school and persist later in life.  In this project, an interdisciplinary team of computer and neurocognitive scientists will develop and implement transformative computational approaches to evaluate the cognitive profiles of young children and to address these issues.  The project will take advantage of both physical and computer based exercises already in place in 300 schools in the United States and involving thousands of children, many of whom have been diagnosed with ADHD or other learning disabilities.  Project outcomes will have important implications for a child's success in school, self-image, and future employment and community functioning.  The PIs will discover new knowledge about the role of physical exercise in cognitive training, including correlations between individual metrics and degree of improvement over time.  They will identify important new metrics and correlations currently unknown to cognitive scientists, which will have broad impact on other application domains as well.  And the PIs will develop an interdisciplinary course on computational cognitive science and one on user interfaces for neurocognitive experts.<br/><br/>The research will involve four thrusts.  The PIs will devise new human motion analysis and computer vision algorithms that can automatically assess embodied cognition during structured physical activities, and which will constitute a breakthrough in improving the accuracy and efficiency of cognitive assessments of young children.  Intelligent mining techniques will be used to discover new knowledge about the role of physical exercise in cognitive training and to find correlations between individual metrics and degree of improvement over time.  A methodology will be developed using advanced multimodal sensing to collect and process huge amounts of evidence based assessment data with intelligent mechanisms that learn about a child's executive function capabilities and help uncover possible causes of cognitive dysfunctions.  And a closed loop cognitive assessment system will be designed and implemented to understand and monitor a child's progress over time and provide recommendations and decision support to cognitive experts so they can make better treatment decisions."
"1620135","Multiscale computational methods in kinetic theory and optimal transport","DMS","COMPUTATIONAL MATHEMATICS","07/01/2016","06/19/2018","Li Wang","NY","SUNY at Buffalo","Continuing Grant","Leland Jameson","11/30/2018","$200,000.00","","wangli1985@gmail.com","520 Lee Entrance","Buffalo","NY","142282567","7166452634","MPS","1271","9263","$0.00","Kinetic equations with multiple scales arise in diverse applications such as rarefied gas dynamics, plasma physics, semiconductors, and biology; they often introduce severe numerical challenges due to the stiffness that comes from the small scales. Optimal transport plays a fundamental role in image registration, video restoration, urban transport, kinetic theory and many others. However, numerical methods for it have not reached their full capacity to meet the most demanding practical applications. This project aims at both advancing the multiscale computational methods - particularly the asymptotic preserving (AP) schemes - in new prospects for kinetic equations including multi-stage and fractional asymptotic limit, and developing fast parallelizable algorithms for optimal transport via advanced optimization technique. <br/><br/>Specifically, the following topics will be investigated in this project: (1) theoretically study the AP schemes for semiconductor Boltzmann equation with two-scale collisions at a deeper depth and generalize them to implicit/high order schemes and to capture the hierarchy of macroscopic models; (2) extend the AP scheme for kinetic equation with fractional diffusion limit to a broader scope including anisotropic scattering, degenerate collision, and Levy-Fokker-Planck interaction (applications to nonclassical photon transport in clouds will be addressed); (3) develop efficient algorithms for optimal transport problems and conduct convergence analysis and apply it to practical problems especially for human crowd dynamics in panic situations. With increasing interest in multiscale kinetic equations and optimal transport, the computational methods developed here will impact beyond the particular applications in this proposal. The dynamics of electron transport in semiconductor devices are one of the main concerns in physics and engineering; the developed methods from this proposal will be equally applicable in a broader context such as gas discharges and multi-group radiative transfer. Nonclassical transport that leads to a fractional diffusion has attracted much attention in plasma physics and economy; it has now been applied in climate science to model the photon transport in clouds as well as in criminology to model the hotspots in residential burglaries. Optimal transport has become a useful tool in image processing, urban transport, computer vision and etc; the development of fast parallelizable algorithms will substantially advance these areas and the application in modeling human crowds is crucial for better preparation of safe mass events."
"1619177","CHS: Small: Collaborative Research: Optimizing the Human-Machine System for Citizen Science","IIS","HCC-Human-Centered Computing, Unallocated Program Costs","07/01/2016","09/22/2016","Lucy Fortson","MN","University of Minnesota-Twin Cities","Continuing Grant","William Bainbridge","06/30/2019","$359,560.00","Claudia Neuhauser","fortson@physics.umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","7367, 9199","7367, 7923","$0.00","This research aims to improve the efficiency, accuracy, and usability of online systems supporting citizen science, in which communities organized around serious scientific research projects combine the contributions of amateurs and professionals.  In order to respond most efficiently to the increasing data deluge across multiple domains, citizen science platforms need to be more dynamic and complex - incorporating intelligent task assignment and machine learning strategies. Systems that make use of both human and machine intelligence are of interest to scientists from a wide range of disciplines. Whether viewed as social machines or as active learning systems in which progressive input from humans improves machine learning, these hybrid systems exhibit complex behavior which needs to be understood for effective system design. For example, machine learning researchers have concentrated on using the large training sets produced by citizen science projects in order to train algorithms that are later applied to a full dataset. Yet this serial processing may not be the most efficient use of the human or machine effort. The main research goal of this project is to investigate how the overall efficiency of the combined human-machine system is impacted by the separate components and their related properties and what the implications are for either human or machine classifiers or both. This process will test the hypothesis that improved overall efficiency will actually reduce the load on expert human classifiers instead of, as currently required, needing larger expert training sets for machines.<br/> <br/>This project will investigate the dynamic combination of human and machine classifiers, gaining for the first time knowledge of how load can be optimally shared in a real, flexible citizen science platform. This research effort will be supported by building and deploying software modules on the existing Zooniverse infrastructure, the world-leading platform for online citizen science.  It will  (1) carry out efficient and dynamic task assignment, distinguishing in near-real time between experienced and inexperienced, and between skilled and less skilled classifiers; and (2) combine human and machine classifications dynamically, periodically training automatic classification routines on the increasing volume of training data produced by volunteers. This new software will then be utilized in a novel ""cascade filtering"" mode that reduces complex classification problems into a series of single binary tasks.  The software developed in this project will provide domain scientists and social machine researchers who wish to exploit the new infrastructure with a fully flexible suite of functions appropriate to the needs defined by their specific problems."
"1619071","CHS: Small: Collaborative Research: Optimizing the Human-Machine System for Citizen Science","IIS","HCC-Human-Centered Computing","07/01/2016","06/30/2017","Laura Trouille","IL","Adler Planetarium","Continuing Grant","William Bainbridge","06/30/2019","$139,914.00","","ltrouille@adlerplanetarium.org","1300 S. Lake Shore Drive","Chicago","IL","606052403","3123220325","CSE","7367","7367, 7923","$0.00","This research aims to improve the efficiency, accuracy, and usability of online systems supporting citizen science, in which communities organized around serious scientific research projects combine the contributions of amateurs and professionals.  In order to respond most efficiently to the increasing data deluge across multiple domains, citizen science platforms need to be more dynamic and complex - incorporating intelligent task assignment and machine learning strategies. Systems that make use of both human and machine intelligence are of interest to scientists from a wide range of disciplines. Whether viewed as social machines or as active learning systems in which progressive input from humans improves machine learning, these hybrid systems exhibit complex behavior which needs to be understood for effective system design. For example, machine learning researchers have concentrated on using the large training sets produced by citizen science projects in order to train algorithms that are later applied to a full dataset. Yet this serial processing may not be the most efficient use of the human or machine effort. The main research goal of this project is to investigate how the overall efficiency of the combined human-machine system is impacted by the separate components and their related properties and what the implications are for either human or machine classifiers or both. This process will test the hypothesis that improved overall efficiency will actually reduce the load on expert human classifiers instead of, as currently required, needing larger expert training sets for machines.<br/> <br/>This project will investigate the dynamic combination of human and machine classifiers, gaining for the first time knowledge of how load can be optimally shared in a real, flexible citizen science platform. This research effort will be supported by building and deploying software modules on the existing Zooniverse infrastructure, the world-leading platform for online citizen science.  It will  (1) carry out efficient and dynamic task assignment, distinguishing in near-real time between experienced and inexperienced, and between skilled and less skilled classifiers; and (2) combine human and machine classifications dynamically, periodically training automatic classification routines on the increasing volume of training data produced by volunteers. This new software will then be utilized in a novel ""cascade filtering"" mode that reduces complex classification problems into a series of single binary tasks.  The software developed in this project will provide domain scientists and social machine researchers who wish to exploit the new infrastructure with a fully flexible suite of functions appropriate to the needs defined by their specific problems."
"1553376","CAREER: Machine Learning-Based Approaches Toward Combatting Abusive Behavior in Online Communities","IIS","HCC-Human-Centered Computing","02/01/2016","02/07/2017","Eric Gilbert","GA","Georgia Tech Research Corporation","Continuing Grant","William Bainbridge","07/31/2018","$191,125.00","","eegg@umich.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7367","1045, 7367","$0.00","This research aims to computationally model abusive online behavior to build tools that help counter it, with the goal of making the Internet a more welcoming place.  Since its earliest days, flaming, trolling, harassment and abuse have plagued the Internet. This project will lay bare the structure of online abuse over many types of online conversations, a major step forward for the study of computer-mediated communication. This will result from modeling abuse with statistical machine learning algorithms as a function of theoretically inspired, sociolinguistic variables, and will entail new technical and methodological advances. This work will enable a transformative new class of automated and semi-automated applications that depend on computationally generated abuse predictions. The education and outreach plan is deeply tied to the research activities, and focuses on scaling-up the research's broader impacts.  A public application programming interface (API) will enable developers and online community managers around the world to integrate into their own sites the defenses against abuse developed by this research.<br/><br/>The work will consist of two major phases. In the first, the research will develop a deep understanding of abusive online behavior via statistical machine learning techniques. Specifically, the work will appropriate theories from social science and linguistics to inform the creation of features for robust statistical machine learning algorithms to predict abuse. These proposed abuse models will enable a brand new, transformative class of mixed-initiative artifacts capable of intervening in social media and online communities. In the second phase, this project will explore this newly enabled class of artifacts by building, deploying and evaluating sociotechnical tools for combatting abuse. Specifically, it will explore two classes of tools that use the abuse predictions: shields and moderator tools. The first, shields, will proactively block inbound abuse from reaching people. The second class of tools, moderator tools, will flag and triage abuse for community moderators."
"1648542","SBIR Phase I:  Predicting Healthcare Fraud, Waste and Abuse by Automatically Discovering Social Networks in Health Insurance Claims Data through Machine Learning","IIP","SMALL BUSINESS PHASE I","12/01/2016","11/30/2016","Armand Prieditis","CA","Albeado, Inc","Standard Grant","Jesus Soriano Molla","11/30/2017","$224,516.00","","armandprie@gmail.com","5201 Great America Parkway","Santa Clara","CA","950541157","4083167831","ENG","5371","5371, 8018, 8023, 8032, 8042","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project will pave the way for new types of social network analysis to detect anomalies, which could lead to more accurate and faster identification of Fraud, Waste, and Abuse (FWA), key opinion leaders (i.e., influentials), and market segments.  Medicare and other healthcare providers lose hundreds of millions of dollars to FWA.  This research proposes using a novel way to discover and combine relationships between entities (e.g., doctors) with information about the entities (e.g., prescription history) using machine learning.  The goal is to reduce a claims investigator's workload while maintaining high accuracy in detecting FWA.  In short, the results of this research will not only improve FWA detection efficiency, but enable detecting new types of FWA.  Societal impact includes reduced costs to the taxpayer for government supported programs such as Medicare through better FWA detection. More broadly, the system could be used to find terrorist and crime networks, detect possible opioid or substance abuse epidemic cohorts, under-medication, over-medication, and even incorrect medications.<br/><br/>The proposed project will apply a novel machine learning method to solve the Fraud, Waste, and Abuse (FWA) problem in health insurance. The technical problem is how to combine relations between entities such as doctors with information about doctors (e.g., a doctor's prescription history). This project advances the state of the art by developing a new way to automatically discover those relations and then combining those relations with the information about doctors through machine learning, thus vastly improving prediction accuracy. The method uses relation information to fill in the gaps of entity information alone and vice versa. It is believed that this method will hugely improve the ability to detect FWA. The goal is to achieve a 50% true positive rate in a database of fraud-convicted doctors published monthly by the government.  The scope of the project involves analyzing several different types of health insurance claims formats (e.g., Medicare) and producing a fraud score, which then others can use. The anticipated results include a fraud score for most doctors in the U.S. (at least those who deal with Medicare), APIs to these scores, and an interactive visual system that claims investigators can use to reduce their workload while accurately identifying FWA."
"1620390","RUI:  Efficient Algorithms for Compressed Sensing and Matrix Completion","DMS","COMPUTATIONAL MATHEMATICS","12/15/2016","12/13/2016","Jeffrey Blanchard","IA","Grinnell College","Standard Grant","Leland Jameson","11/30/2019","$116,309.00","","blanchaj@grinnell.edu","1121 Park Street","Grinnell","IA","501121690","6412694983","MPS","1271","9229, 9263","$0.00","Traditionally, a signal is measured by acquiring every component in the signal and then compressing the signal with an appropriate computational algorithm. For example, digital cameras capture an image with a huge number of pixels and then a compression scheme such as JPEG is used to reduce the size of the digital image for storage or dissemination. In many applications, the costs and challenges associated with acquiring measurements are considerable. In compressed sensing and matrix completion, the measurement process is altered in order to drastically reduce the number of measurements, but the signal reconstruction process is necessarily more difficult. Compressed sensing and matrix completion transfer the workload from the measurement process to computational resources dedicated to the signal reconstruction. Typical applications include compressive radar, geophysical data analysis, medical imaging, and computer vision. This project will take a holistic approach to data acquisition and algorithm development for compressed sensing and matrix completion where theoretical guarantees often rely on computationally expensive subroutines and apply to computationally burdensome measurement processes. Increased efficiency can be achieved through sparse measurement operators, relaxed subroutine requirements in iterative greedy algorithms, and the implementation of these algorithms on computation accelerating hardware.<br/><br/>Compressed sensing combines the acts of signal acquisition and compression into a single operation. Computationally efficient algorithms then produce accurate approximations to sparse signals by exploiting the underlying simplicity that the signal has relatively few important components. Matrix completion similarly exploits the simplicity of the target matrix having only a few independent columns; in other words, one recovers a low rank matrix from a limited number of measurements. While leading greedy algorithms for compressed sensing and matrix completion have theoretical guarantees defining the number of measurements required for accurately recovering the underlying low dimensional signal, these guarantees require many more measurements than practical for applications. Furthermore, many of the algorithms employ theoretically useful but computationally expensive subroutines. Observed performance of more computationally efficient measurement operators encourages the adoption of techniques in practice that lack worst case, uniform guarantees for acquisition and reconstruction. This project seeks to balance the competing desires for theoretical guarantees and fast, efficient algorithms. The project will pursue theoretically viable algorithms which are also practically useful and provide solutions to linear inverse problems in reasonable amounts of computational effort including power, time, and affordable hardware. At the same time, establishing empirical performance characteristics for computationally efficient measurement operators and recovery algorithms which lack precise guarantees will help guide practitioners and theorists in future research. To provide near real time solutions to these computationally intensive algorithms, the project will also further accelerate computation by designing and disseminating algorithm implementations which exploit the massively parallel computations available on high performance computing graphics processing units."
"1729209","CPS/Synergy/Collaborative Research: Safe and Efficient Cyber-Physical Operation System for Construction Equipment","CMMI","CPS-Cyber-Physical Systems","09/01/2016","06/09/2017","Chinemelu Anumba","FL","University of Florida","Standard Grant","Bruce Kramer","12/31/2020","$290,325.00","","anumba@ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","ENG","7918","030E, 034E, 5188, 7339, 7918, 8235","$0.00","Equipment operation represents one of the most dangerous tasks on a construction sites and accidents related to such operation often result in death and property damage on the construction site and the surrounding area. Such accidents can also cause considerable delays and disruption, and negatively impact the efficiency of operations. This award will conduct research to improve the safety and efficiency of cranes by integrating advances in robotics, computer vision, and construction management. It will create tools for quick and easy planning of crane operations and incorporate them into a safe and efficient system that can monitor a crane's environment and provide control feedback to the crane and the operator. Resulting gains in safety and efficiency wil reduce fatal and non-fatal crane accidents. Partnerships with industry will also ensure that these advances have a positive impact on  construction practice, and can be extended broadly to smart infrastructure, intelligent manufacturing, surveillance, traffic monitoring, and other application areas. The research will involve undergraduates and includes outreach to K-12 students.<br/><br/>The work is driven by the hypothesis that the monitoring and control of cranes can be performed autonomously using robotics and computer vision algorithms, and that detailed and continuous monitoring and control feedback can lead to improved planning and simulation of equipment operations. It will particularly focus on developing methods for (a) planning construction operations while accounting for safety hazards through simulation; (b) estimating and providing analytics on the state of the equipment; (c) monitoring equipment surrounding the crane operating environment, including detection of safety hazards, and proximity analysis to dynamic resources including materials, equipment, and workers; (d) controlling crane stability in real-time; and (e) providing feedback to the user and equipment operators in a ""transparent cockpit"" using visual and haptic cues. It will address the underlying research challenges by improving the efficiency and reliability of planning through failure effects analysis and creating methods for contact state estimation and equilibrium analysis; improving monitoring through model-driven and real-time 3D reconstruction techniques, context-driven object recognition, and forecasting motion trajectories of objects; enhancing reliability of control through dynamic crane models, measures of instability, and algorithms for finding optimal controls; and, finally, improving efficiency of feedback loops through methods for providing visual and haptic cues."
"1544999","CPS/Synergy/Collaborative Research: Safe and Efficient Cyber-Physical Operation System for Construction Equipment","CMMI","CPS-Cyber-Physical Systems","01/01/2016","08/21/2015","Mani Golparvar-Fard","IL","University of Illinois at Urbana-Champaign","Standard Grant","Bruce Kramer","06/30/2020","$325,000.00","Timothy Bretl","mgolpar@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","ENG","7918","030E, 034E, 5188, 7339, 7918, 8235","$0.00","Equipment operation represents one of the most dangerous tasks on a construction sites and accidents related to such operation often result in death and property damage on the construction site and the surrounding area. Such accidents can also cause considerable delays and disruption, and negatively impact the efficiency of operations. This award will conduct research to improve the safety and efficiency of cranes by integrating advances in robotics, computer vision, and construction management. It will create tools for quick and easy planning of crane operations and incorporate them into a safe and efficient system that can monitor a crane's environment and provide control feedback to the crane and the operator. Resulting gains in safety and efficiency wil reduce fatal and non-fatal crane accidents. Partnerships with industry will also ensure that these advances have a positive impact on  construction practice, and can be extended broadly to smart infrastructure, intelligent manufacturing, surveillance, traffic monitoring, and other application areas. The research will involve undergraduates and includes outreach to K-12 students.<br/><br/>The work is driven by the hypothesis that the monitoring and control of cranes can be performed autonomously using robotics and computer vision algorithms, and that detailed and continuous monitoring and control feedback can lead to improved planning and simulation of equipment operations. It will particularly focus on developing methods for (a) planning construction operations while accounting for safety hazards through simulation; (b) estimating and providing analytics on the state of the equipment; (c) monitoring equipment surrounding the crane operating environment, including detection of safety hazards, and proximity analysis to dynamic resources including materials, equipment, and workers; (d) controlling crane stability in real-time; and (e) providing feedback to the user and equipment operators in a ""transparent cockpit"" using visual and haptic cues. It will address the underlying research challenges by improving the efficiency and reliability of planning through failure effects analysis and creating methods for contact state estimation and equilibrium analysis; improving monitoring through model-driven and real-time 3D reconstruction techniques, context-driven object recognition, and forecasting motion trajectories of objects; enhancing reliability of control through dynamic crane models, measures of instability, and algorithms for finding optimal controls; and, finally, improving efficiency of feedback loops through methods for providing visual and haptic cues."
"1544973","CPS/Synergy/Collaborative Research: Safe and Efficient Cyber-Physical Operation System for Construction Equipment","CMMI","CYBER-PHYSICAL SYSTEMS (CPS)","01/01/2016","08/21/2015","Chinemelu Anumba","PA","Pennsylvania State Univ University Park","Standard Grant","Bruce Kramer","04/30/2017","$325,000.00","John Messner","anumba@ufl.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","ENG","7918","030E, 034E, 5188, 7339, 7918, 8235","$0.00","Equipment operation represents one of the most dangerous tasks on a construction sites and accidents related to such operation often result in death and property damage on the construction site and the surrounding area. Such accidents can also cause considerable delays and disruption, and negatively impact the efficiency of operations. This award will conduct research to improve the safety and efficiency of cranes by integrating advances in robotics, computer vision, and construction management. It will create tools for quick and easy planning of crane operations and incorporate them into a safe and efficient system that can monitor a crane's environment and provide control feedback to the crane and the operator. Resulting gains in safety and efficiency wil reduce fatal and non-fatal crane accidents. Partnerships with industry will also ensure that these advances have a positive impact on  construction practice, and can be extended broadly to smart infrastructure, intelligent manufacturing, surveillance, traffic monitoring, and other application areas. The research will involve undergraduates and includes outreach to K-12 students.<br/><br/>The work is driven by the hypothesis that the monitoring and control of cranes can be performed autonomously using robotics and computer vision algorithms, and that detailed and continuous monitoring and control feedback can lead to improved planning and simulation of equipment operations. It will particularly focus on developing methods for (a) planning construction operations while accounting for safety hazards through simulation; (b) estimating and providing analytics on the state of the equipment; (c) monitoring equipment surrounding the crane operating environment, including detection of safety hazards, and proximity analysis to dynamic resources including materials, equipment, and workers; (d) controlling crane stability in real-time; and (e) providing feedback to the user and equipment operators in a ""transparent cockpit"" using visual and haptic cues. It will address the underlying research challenges by improving the efficiency and reliability of planning through failure effects analysis and creating methods for contact state estimation and equilibrium analysis; improving monitoring through model-driven and real-time 3D reconstruction techniques, context-driven object recognition, and forecasting motion trajectories of objects; enhancing reliability of control through dynamic crane models, measures of instability, and algorithms for finding optimal controls; and, finally, improving efficiency of feedback loops through methods for providing visual and haptic cues."
"1565310","CHS: Large: Collaborative Research: Computational Science for Improving Assessment of Executive Function in Children","IIS","HCC-Human-Centered Computing","10/01/2016","05/31/2018","Morris Bell","CT","Yale University","Standard Grant","Ephraim Glinert","09/30/2021","$1,221,294.00","Bruce Wexler","morris.bell@yale.edu","Office of Sponsored Projects","New Haven","CT","065208327","2037854689","CSE","7367","7367, 7925, 9251","$0.00","The identification of cognitive impairments in early childhood provides the best opportunity for successful remedial intervention, because brain plasticity diminishes with age.  Attention deficit hyperactivity disorder (ADHD) is a psychiatric neurodevelopmental disorder that is very hard to diagnose or tell apart from other disorders.  Symptoms include inattention, hyperactivity, or acting impulsively, all of which often result in poor performance in school and persist later in life.  In this project, an interdisciplinary team of computer and neurocognitive scientists will develop and implement transformative computational approaches to evaluate the cognitive profiles of young children and to address these issues.  The project will take advantage of both physical and computer based exercises already in place in 300 schools in the United States and involving thousands of children, many of whom have been diagnosed with ADHD or other learning disabilities.  Project outcomes will have important implications for a child's success in school, self-image, and future employment and community functioning.  The PIs will discover new knowledge about the role of physical exercise in cognitive training, including correlations between individual metrics and degree of improvement over time.  They will identify important new metrics and correlations currently unknown to cognitive scientists, which will have broad impact on other application domains as well.  And the PIs will develop an interdisciplinary course on computational cognitive science and one on user interfaces for neurocognitive experts.<br/><br/>The research will involve four thrusts.  The PIs will devise new human motion analysis and computer vision algorithms that can automatically assess embodied cognition during structured physical activities, and which will constitute a breakthrough in improving the accuracy and efficiency of cognitive assessments of young children.  Intelligent mining techniques will be used to discover new knowledge about the role of physical exercise in cognitive training and to find correlations between individual metrics and degree of improvement over time.  A methodology will be developed using advanced multimodal sensing to collect and process huge amounts of evidence based assessment data with intelligent mechanisms that learn about a child's executive function capabilities and help uncover possible causes of cognitive dysfunctions.  And a closed loop cognitive assessment system will be designed and implemented to understand and monitor a child's progress over time and provide recommendations and decision support to cognitive experts so they can make better treatment decisions."
"1552706","CAREER: Robot Learning from Motor-Impaired Instructors and Task Partners","IIS","Robust Intelligence","02/01/2016","02/11/2020","Brenna Argall","IL","Rehabilitation Institute of Chicago","Continuing Grant","Erion Plaku","01/31/2021","$525,000.00","","brenna.argall@northwestern.edu","355 East Erie Street","Chicago","IL","606113167","3122385195","CSE","7495","1045, 7495","$0.00","Assistive machines - like power wheelchairs, robotic arms, exoskeletons, prostheses - are vital for enabling independence for people with severe motor impairments. However, there exists a paradox, where often the more severe a person's impairment, the less able they are to operate these very machines which might improve their quality of life. Here robotics technologies have the potential to transform the field of human health and rehabilitation: by turning the machine into a robot, that can operate itself autonomously and share the control burden. It will be crucial that these robots adapt to the human user's unique preferences and abilities, and how both change over time is crucial for achieving widespread adoption and acceptance, and especially if attached to a human's body to provide physical assistance. <br/><br/>There has been limited study of robot learning from non-experts, and the domain of motor-impaired teachers is even more challenging: their control signals are noisy (due to artifacts in the motor signal) and sparse (if providing motor commands is more effort), and filtered through an interface. Rather than treat these constraints as limitations, the proposed work hypothesizes that such constraints become advantageous for machine learning algorithms that exploit unique characteristics (like problem-space sparsity) of control and feedback signals from motor-impaired humans. The work develops multiple novel machine learning algorithmic techniques, (1) that reason explicitly about the control interface and how it interacts with the full robot control space; (2) that derive information about the human's control patterns and task requirements, from variability in the human's teleoperation commands; and (3) which include the design of adaptation cues informed by reward- and example-based feedback from motor-impaired teachers. The proposed work also performs subject studies with motor-impaired end-users operating multiple robotic platforms, both to explore this problem space and assess the functionality and user acceptance of the contributed algorithmic techniques."
"1664720","BIGDATA: F: DKA: Collaborative Research: High-Dimensional Statistical Machine Learning for Spatio-Temporal Climate Data","IIS","Big Data Science &Engineering","08/22/2016","10/25/2016","Pradeep Ravikumar","PA","Carnegie-Mellon University","Standard Grant","Maria Zemankova","08/31/2019","$319,968.00","","pradeepr@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","8083","7433, 8083","$0.00","While statistical machine learning has seen major advances over the past two decades, rigorous approaches for high-dimensional spatio-temporal scientific data analysis have not received as much attention.  On the other hand, several core scientific areas, including climate science, ecology, environmental sciences, and neuroscience, are generating increasing amounts of high-resolution spatio-temporal data.  It is vital to develop rigorous machine learning approaches for such complex high-dimensional spatio-temporal data in order for key scientific breakthroughs in these areas in the next few decades.  The project contributes to these endeavors by focusing on two key technical and scientific areas: spatio-temporal big data analysis and climate science.  The project systematically develops the statistical machine learning foundations for the analysis of large scale complex high-dimensional spatio-temporal data, and applies such advances to problems arising in climate science, where the total amount of data is set to cross an Exabyte (1 Exabyte = 1000 Petabytes) soon. <br/><br/>The technical work in the project has three broad and interacting components: structured probabilistic graphical models for spatio-temporal data analysis, generalized graphical models for multivariate heavy tailed distributions, and physics-guided models with a richer class of structural constraints and capturing multi-scale phenomena.  The project applies these technical advances to climate science, by generating climate projections at high-resolutions.  Currently, the lack of requisite spatial resolution of current climate models makes automatic assessments of impacts, adaptation and vulnerability (IAV) difficult for a variety of sectors, including urban planning, freshwater resources, food security, energy, transportation systems, human health, and coastal systems."
"1654873","Workshop on Geometry for Signal Processing and Machine Learning","CCF","Comm & Information Foundations","08/15/2016","09/06/2016","Ali Pezeshki","CO","Colorado State University","Standard Grant","Phillip Regalia","07/31/2018","$79,824.00","","pezeshki@engr.colostate.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","CSE","7797","7556, 7936","$0.00","This proposal seeks NSF support for a three-day workshop on Geometry for Signal Processing and Machine Learning to be held October 13?15, 2016 in Estes Park, Colorado. The workshop is being organized by Ali Pezeshki, Colorado State University (PI of this proposal), Doug Cochran, Arizona State University, and Louis Scharf, Colorado State University. <br/><br/>The workshop will explore applications that impel manifold modeling, and develop a framework for solving inference problems on manifolds. Specific objectives of the workshop include<br/><br/>? identifying challenging problems of signal processing and machine learning that lie outside the scope of subspace methods, <br/>? identifying models and methods from modern geometry that address these problems, <br/>? identifying a set of open research questions that would inform NSF?s continuing and future investments in signal processing and machine learning. <br/><br/>The long-term objective is to develop a theory as broad in its scope and as precise in its methodologies as modern statistical signal processing. Such a theory would extend linear subspace models and augment statistical reasoning with geometrical reasoning."
"1605867","Atomically dispersed amorphous catalysts: ab initio computational tools for a new frontier","CBET","Catalysis","08/01/2016","07/31/2016","Baron Peters","CA","University of California-Santa Barbara","Standard Grant","Robert McCabe","07/31/2021","$301,866.00","","baronp@engineering.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","ENG","1401","","$0.00","The project aims to develop and apply computational methods to understand the catalytic properties of isolated metal atom sites on amorphous support materials.  The computational results will be compared to experimental data on the Phillips Petroleum ethylene polymerization catalyst which has been a workhorse industrial catalyst for 60 years despite longstanding questions about the active sites and the mechanism.  The results will not only provide information specific to potential improvements in the Phillips catalyst, but will improve theoretical tools for understanding a broad class of catalysts where the activity is dominated by a small fraction of highly active metal sites on the amorphous support.  Related educational and outreach programs will be offered to students at all levels, including a game to engage high school students in scientific pursuits.<br/><br/>The study will develop computational techniques to identify the specific properties that make certain catalytic sites highly active relative to others among an ensemble of isolated metal sites on an amorphous support material.  The work specifically focuses on chromium supported on amorphous silica (the Phillips catalyst) for which a broad body of characterization data is available.  The computational approach will combine machine learning techniques and rare events methods for analyzing the distribution of sites to predict relationships between structure and activity.  Specifically, machine learning methods trained by ab initio calculations will learn how activity is related to the local structural environments of the isolated chromium species on the silica surface.  Non-Boltzmann sampling techniques will ensure that rare but important sites with unusually high activities (low activation energies) are adequately sampled to ensure accurate site-averaged kinetic properties.  The combined approach provides a new ""importance learning"" strategy that can be broadly used to build models of active site distributions, identify critical characteristics of highly active sites, and engineer better atomically dispersed catalysts on amorphous supports.  Broader educational and outreach contributions include the development and public sharing of ""plug-ins"" that support the importance learning approach, virtual reality visualization of the machine learning tools, and a related game for high school students in which they will have an opportunity to compete with the machine learning algorithm to design highly active catalytic sites."
"1643614","EAGER:   Income Learning:   A New Model for Behavior-Analysis-Inspired Learning from Human Feedback","IIS","ROBUST INTELLIGENCE","08/15/2016","08/08/2016","Matthew Taylor","WA","Washington State University","Standard Grant","James Donlon","07/31/2017","$70,000.00","","taylorm@eecs.wsu.edu","280 Lighty","PULLMAN","WA","991641060","5093359661","CSE","7495","7495, 7916","$0.00","As virtual agents and physical robots become more common, there is an increasing number of complex tasks they can usefully perform to assist humans. These tasks are typically formalized as sequential decision tasks, where robots and agents perceive states, take actions, and receive a reward feedback signal. In practice, there is a critical need to learn directly from human users if such machines are to accomplish tasks outside of those pre-specified by the original developments. Machine reinforcement learning (RL), a paradigm often used for solving sequential decision making tasks, was originally developed with inspiration from animal learning research from the applied behavior analysis (ABA) community. Existing RL approaches operationalize a limited set of ABA principles effectively; however, there are additional principles and properties from ABA research that are not well encapsulated in the existing RL formalisms, and that are likely sources of new inspiration for designing more effective RL techniques capable of learning from human teachers. This project will (1) take combine principles from ABA and RL to produce algorithms that can learn more effectively from humans, (2) evaluate these algorithms in both virtual agents and on robot platforms, and (3) investigate whether and how non-expert humans can construct sequences of tasks of increasing difficulty, similar to how expert animal trainers shape tasks. Insights from these user studies will be leveraged to further improve our algorithms' abilities to learn from human trainers. Once successful, this project will make critical progress towards allowing non-technical users to be able to teach virtual and physical agents to perform complex tasks in a natural setting, familiar to many from previous experience in training household pets.<br/><br/>This project is a part of a larger effort between Washington State University (WSU), North Carolina State University, and Brown University. The WSU effort will focus on implementing the proposed family of machine learning algorithms, called Income Learning (I-Learning). As these algorithms are co-developed by the three universities, WSU will design user studies to evaluate when and how the principles behind I-Learning allow it to outperform other existing algorithms at learning from human feedback. WSU will primarily focus on 1) virtual agents, allowing test learning via crowdsourcing, as well as testing on 2) physical robots and study if embodiment changes user's perceptions and actions, or the algorithms' learning efficacy. Additionally, WSU will investigate 3) human curricula design. Expert trainers can shape the behavior of animals, increasing task complexity over time, so that the animals can learn a sequence of tasks much faster than if they trained directly on the final, difficult task. WSU will run user studies on crowdsourcing platforms to better understand how non-expert humans design curricula for machine learning algorithms in sequential decision tasks, and investigate how these design decisions can inform algorithm design."
"1556110","SBIR Phase II:  A Novel Non-Invasive Intracranial Pressure Monitoring Method","IIP","SBIR Phase II","03/01/2016","12/10/2016","Robert Hamilton","CA","Neural Analytics","Standard Grant","Henry Ahn","02/28/2018","$753,756.00","","robert@neuralanalytics.com","2440 S. Sepulveda Blvd","Los Angeles","CA","900641744","8183174999","ENG","5373","124E, 5345, 5373, 8038, 8042, 8240","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project will be to improve the quality and decrease the high costs associated with treating patients who suffer severe traumatic brain injuries. This project aims to develop an accurate, affordable (<$100 per use) and non-invasive device to monitor a patient's intracranial pressure following head injury. Increased intracranial pressure can result in poor health outcomes including long-term disability or death, if left untreated. However, the only available method to monitor intracranial pressure is expensive (~$10,000 per patient) and requires neurosurgery. The lack of a method to accurately screen patients to determine who needs surgery results in misdiagnoses and incorrect treatment in about 46% of patients among an estimated 50,000 patients in the US alone, and hundreds of thousands more globally. Successful commercialization of product is expected to result in savings in the range $250 million ever year to the US healthcare system.<br/><br/>The proposed project will develop a medical device to accurately display a patient's intracranial pressure non-invasively and for use outside of the neurocritical care unit. The core technological approach of the proposed work is the analysis of blood flow velocity waveforms using advanced signal processing methods in a machine-learning framework. The machine-learning framework allows experience-based learning utilizing prior, established databases of waveforms that have been well-characterized. Three new machine-learning paradigms that utilize the shape features of the blood flow velocity waveforms will be utilized to progressively increase accuracy of intracranial pressure estimation. The first will establish a basic estimate using shape features of individual waveform pulses, considered independent of neighboring pulses. Subsequently, clinically established features of the waveform will be utilized to learn causal changes in the shape features resulting from changes in intracranial pressure. Finally, the shape features in successive pulses will be used as a sequence to machine-learn the intracranial pressure estimate. Together, these will enable increased accuracy in estimation. All of the methods proposed in this program are entirely novel. This approach allows for real time monitoring at an affordable price point that is within current reimbursement limits for ultrasonography procedures."
"1623605","EXP: Modeling Perceptual Fluency with Visual Representations in an Intelligent Tutoring System for Undergraduate Chemistry","IIS","S-STEM-Schlr Sci Tech Eng&Math, Cyberlearn & Future Learn Tech","09/01/2016","08/30/2016","Martina Rau","WI","University of Wisconsin-Madison","Standard Grant","Maria Zemankova","08/31/2020","$540,396.00","Xiaojin Zhu, Robert Nowak","marau@wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","1536, 8020","8045, 8244, 8841","$0.00","The Cyberlearning and Future Learning Technologies Program funds efforts that support envisioning the future of learning technologies and advance what we know about how people learn in technology-rich environments. Cyberlearning Exploration (EXP) Projects design and build new kinds of learning technologies in order to explore their viability, to understand the challenges to using them effectively, and to study their potential for fostering learning. This EXP project aims to help students become visually fluent with visual representations (similar to becoming fluent in a second language). Instructors often use visuals to help students learn (e.g., pie charts of fractions, or ball-and-stick models of chemical molecules) and assume that students can quickly discern relevant information (e.g., whether or not two visuals show the same chemical) once that visual representation has been introduced. But comprehension is not the same as fluency -- students still expend significant mental effort and time interpreting even visuals that they understand conceptually, and the resulting cognitive load can cause them to miss other important information that instructors are imparting. To help improve student fluency with visuals, a series of experiments with undergraduate students and chemistry professors will investigate which visual features they pay attention to and use sophisticated statistical methods to devise example sequences that will most efficiently help students learn to pay attention to relevant visual features. Based on this research, the project team will develop a visual fluency training that will be incorporated into an existing, successful online learning technology for chemistry. The potential educational impact will not be limited to chemistry instruction: given the pervasiveness of visual representations in STEM fields and the number of students who struggle with rapid processing of those visuals, the products of this research could be integrated into other educational technologies.<br/><br/>The PIs will develop a methodology for cognitive modeling of perceptual learning processes that can create adaptive support for perceptual learning tasks. The research will combine machine learning with educational psychology experiments using an Intelligent Tutoring System (ITS) for undergraduate chemistry. In Phase 1, metric learning will assess which visual features of representations novice students and chemistry experts focus on. Applying metric learning to a novice-expert experiment will establish a skill model of student perceptions and perceptual learning goals for the ITS. In Phase 2, the team will use machine learning to develop a cognitive model of perceptual learning. The team will conduct a chemistry learning experiment and apply machine learning to test cognitive models. In Phase 3, the team will use the cognitive model to reverse-engineer optimal sequences of perceptual learning tasks. An experiment will evaluate the effectiveness of these sequences, and the team will build on this analysis to create an adaptive version of perceptual learning tasks. A final experiment will evaluate whether incorporating adaptive perceptual learning tasks with conceptually focused instruction enhances learning.  Because educational technologies have traditionally focused on explicit learning processes that lead to conceptual competencies, they cannot currently assess the implicit learning processes that lead to perceptual fluency.  Combining educational psychology, cognitive science, and machine learning will yield new cognitive models that could transform the adaptive capabilities of educational technologies to support such perceptual fluency as well as other implicit forms of learning. The project will also yield next-generation computational algorithms to model human similarity judgments and to use adaptive surveying to collect data on perceptual judgments more efficiently."
"1749241","CIF: Small: Collaborative Research: Inference of Information Measures on Large Alphabets: Fundamental Limits, Fast Algorithims, and Applications","CCF","Comm & Information Foundations","08/01/2016","08/31/2017","Yihong Wu","CT","Yale University","Standard Grant","Phillip Regalia","08/31/2019","$206,012.00","","yihong.wu@yale.edu","Office of Sponsored Projects","New Haven","CT","065208327","2037854689","CSE","7797","7923, 7935","$0.00","A key task in information theory is to characterize fundamental performance limits in compression, communication, and more general operational problems involving the storage, transmission and processing of information. Such characterizations are usually in terms of information measures, among the most fundamental of which are the Shannon entropy and the mutual information. In addition to their prominent operational roles in the traditional realms of information theory, information measures have found numerous applications in many statistical modeling and machine learning tasks. Various modern data-analytic applications deal with data sets naturally viewed as samples from a probability distribution over a large domain. Due to the typically large alphabet size and resource constraints, the practitioner contends with the difficulty of undersampling in applications ranging from corpus linguistics to neuroscience. One of the main goals of this project is the development of a general theory based on a new set of mathematical tools that will facilitate the construction and analysis of optimal estimation of information measures on large alphabets. The other major facet of this project is the incorporation of the new theoretical methodologies into machine learning algorithms, thereby significantly impacting current real-world learning practices.  Successful completion of this project will result in enabling technologies and practical schemes - in applications ranging from analysis of neural response data to learning graphical models - that are provably much closer to attaining the fundamental performance limits than existing ones. The findings of this project will enrich existing big data-analytic curricula.  A new course dedicated to high-dimensional statistical inference that addresses estimation for large-alphabet data in depth will be created and offered. Workshops on the themes and findings of this project will be organized and held at Stanford and UIUC. <br/><br/>A comprehensive approximation-theoretic approach to estimating functionals of distributions on large alphabets will be developed via computationally efficient procedures based on best polynomial approximation, with provable essential optimality guarantees. Rooted in the high-dimensional statistics literature, our key observation is that while estimating the distribution itself requires the sample size to scale linearly with the alphabet size, it is possible to accurately estimate functionals of the distribution, such as entropy or mutual information, with sub-linear sample complexity. This requires going beyond the conventional wisdom by developing more sophisticated approaches than maximal likelihood (?plug-in?) estimation. The other major facet of this project is translating the new theoretical methodologies into highly scalable and efficient machine learning algorithms, thereby significantly impacting current real-world learning practices and significantly boosting the performance in several of the most prevalent machine learning applications, such as learning graphical models, that rely on mutual information estimation."
"1549713","STTR Phase I:  Developing a Machine Learning Platform for Workplace Wellness Programs","IIP","STTR PHASE I","01/01/2016","12/23/2016","Chang-Jiang Zheng","MN","ZBH ENTERPRISES, LLC","Standard Grant","Jesus Soriano Molla","06/30/2017","$250,000.00","Rui Kuang","drcj@mripathways.com","36 NATHAN LN N","Minneapolis","MN","554416306","6123566423","ENG","1505","1505, 163E, 8018, 8032, 8042","$0.00","The broader impact/commercial potential of this Small Business Technology Transfer (STTR) Phase I project promotes a promising new approach to improve employee wellness with the potential for wider application beyond the workplace. This improved approach combines both innovative clinical diagnostics with computer assisted learning to guide recommendations. It offers the potential to curb epidemic chronic disease through the use of efficient as well as scalable operating models. Adopting this data-driven approach is expected to improve not only participating employees? health but also reduce health care costs. Increased productivity which follows from improved employee health can also be expected. Accordingly, better health and economic savings to both corporate America and working Americans are realistic objectives of this project. Employers and the general public therefore are expected to benefit from this initiative.<br/><br/>The proposed project combines novel as well as economic use of clinical diagnostics, strengthened by computer learning, to effectively improve workplace wellness. Conventional wellness strategies are presently both inefficient as well as ineffective in addressing the growing epidemic of chronic diseases. These chronic diseases include but are not limited to: diabetes, obesity, high blood pressure and cardiovascular diseases. The principal objective of this project is the integration of various clinical findings in prevention of chronic disease enhanced by machine learning. This unique approach aids in the development of scalable, workplace wellness programs which are effective and affordable. The novel platform continuously learns from all participants? interventional results and intelligently discovers the best preventive intervention plan for each participant. The platform innovatively adopts low cost quantitative diagnostic imagery to track key health parameters clinically correlated to wellness. Computer aided modeling includes dynamic physiological models (i.e. current symptoms, lifestyle measurements, past medical history, MRI scans and laboratory tests) to clinically evaluate the course of chronic disease along with various prevention strategies for intervention. Measures of clinical outcome enhanced by machine learning identify important biomarkers to develop intervention recommendations for participating individuals."
"1527747","NRI: Collaborative Research: Multimodal Brain Computer Interface for Human-Robot Interaction","IIS","NRI-National Robotics Initiati","05/15/2016","05/27/2016","Peter Allen","NY","Columbia University","Standard Grant","Kenneth Whang","04/30/2020","$736,552.00","Paul Sajda","allen@cs.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","8013","8086, 8089","$0.00","Human Robot Interaction (HRI) is research that is a key component in making robots part of our everyday life. Current interface modalities such as video, keyboard, tactile, audio, and speech can all contribute to an HRI interface. However, an emerging area is the use of Brain-Computer Interfaces (BCI) for communication and information exchange between humans and robots. BCIs can provide another channel of communication with more direct access to physiological changes in the brain. BCIs vary widely in their capabilities, particularly with respect to spatial resolution, temporal resolution and noise. This project is aimed at exploring the use of multimodal BCIs for HRI. Multimodal BCIs, also referred to as hybrid BCIs (hBCI), have been shown to improve performance over single modality interfaces. This project is focused on using a novel suite of sensors (Electroencephalography (EEG), eye-tracking, pupillary size, computer vision, and functional Near Infrared Spectroscopy (fNIRS)) to improve current HRI systems. Each of these sensing modalities can reinforce and complement each other, and when used together, can address a major shortcoming of current BCIs which is the determination of the user state or situational awareness (SA). SA is a necessary component of any complex interaction between agents, as each agent has its own expectations and assumptions about the environment. Traditional BCI systems have difficulty recognizing state and context, and accordingly can become confusing and unreliable. This project will develop techniques to recognize state from multiple modalities, and will also allow the robot and human to learn about each other's state and expectations using the hBCI we are developing. The goal is to build a usable hBCI for real physical robot environments, with noise, real-time constraints, and added complexity.<br/><br/>The technical contributions of this project include:<br/>1. Characterization of a novel hBCI interface for visual recognition and labeling tasks with real physical data and environments.<br/>2. Integration of fNIRS sensing with EEG and other modalities in human robot interaction tasks. We will test our ability in the temporal domain to determine at what timescale we can correctly classify movement components that would predict a correct (rewarding) trial or non-rewarding/incorrect movement.<br/>3. Analysis and validation of the hBCI in complex robotic tele-operation tasks with human subject operators such as open door, grasp object on table, pick up item off floor etc.<br/>4. Use of hBCI to characterize human/robot state and create a learning method to recognize state over time.<br/>5. Use of augmented reality for HRI decision making.<br/>6. Further develop hBCI for tracking cognitive states related to reward, motivation, attention and value.<br/>A new class of HRI interfaces will be developed that can expand the ability of humans to work with robots; promote the use and acceptance of robot agent systems in everyday life; expand the use of hBCIs in areas other than robotics for human-machine interaction; further the development of hBCIs as our system will be tapping into reward modulated activity that will be used via reinforcement learning to autonomously update the learning machinery; and bridge the educational divide between Engineering/Computer Science and Neuroscience."
"1616340","An optimization-based framework for deconvolution: theoretical guarantees and practical algorithms","DMS","APPLIED MATHEMATICS","08/01/2016","07/25/2016","Carlos Fernandez Granda","NY","New York University","Standard Grant","Victor Roytburd","07/31/2019","$184,481.00","","cfg3@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","MPS","1266","","$0.00","Deconvolution is an inverse problem that consists of teasing apart the contributions of different signal sources in data. While these problems are common across the applied sciences, this project will focus on three examples. In neuroscience, recordings of neuron activity using extracellular electrodes measure the action potentials or spikes of adjacent cells. Spike sorting, or equivalently multi-waveform deconvolution, is the problem of identifying the signals corresponding to each cell and deconvolving them to reveal the separate spiking patterns. Super-resolution fluorescence microscopy allows one to obtain images or videos of complex cell structures at high resolution from low-resolution data. In computer vision, blurred pictures, such as ones taken from a cellphone, are well approximated by the convolution of a sharp image with a motion-blur kernel. The aim of this project is to develop and analyze algorithms to tackle these problems, with special emphasis on adapting these methods so that they can be applied efficiently to large amounts of data. <br/><br/>Most recent literature on underdetermined linear inverse problems under sparsity constraints focuses on randomized sensing schemes, which do not allow to model convolution problems such as super-resolution, spike sorting in neuroscience, or blind deconvolution in computer vision. The main goal of this project is to develop optimization-based methods for deterministic deconvolution problems, as well as to derive theoretical guarantees on their performance and their robustness to noise. This will require developing novel proof techniques, which do not rely on probabilistic tools, to characterize the conditioning of convolution operators and the optimality conditions of L1-norm minimization problems. Building upon these optimization-based methods, practical algorithms will be designed to operate in big-data regimes where it is not computationally tractable to apply sophisticated processing uniformly across the data."
"1636544","Supporting English Learners in STEM Subjects","DRL","ITEST-Inov Tech Exp Stu & Teac, Discovery Research K-12","09/01/2016","07/05/2019","Heidi Schweingruber","DC","National Academy of Sciences","Standard Grant","Robert Ochsendorf","02/29/2020","$1,122,149.00","","hschweingruber@nas.edu","500 FIFTH STREET NW","Washington","DC","200012721","2023342254","EHR","7227, 7645","","$0.00","The expectation that all students, including English language learners (ELLs), achieve high academic standards has become even more evident and complex to date as a result of several key factors. First, as the school-aged population continues to grow more racially, ethnically, and linguistically diverse, ELLs can now be found in virtually every school in the nation. Second, the science and mathematics education landscape has changed significantly resulting from the new visions in these fields, and the challenges posed by the new academic standards for all students. Third, the need to integrate new knowledge and perspectives from the language arts with knowledge from science and mathematics learning, instruction, and assessment has surfaced as a critical component of the potential strategies to be employed in addressing ELLs' current science, technology, engineering, and mathematics (STEM) education situation from pre-K-12 grades. The key challenges today include both enabling educators to better support this student subpopulation, as well as increasing the number and quality of research activities focused on how best to support ELLs' success in these subjects. In response to this challenge, the Board on Science Education (BOSE) of the National Academies of Sciences will conduct a consensus study focused on identifying instructional practices and professional development approaches for teachers, as well as the policies that are needed to support ELLs' accomplishments  in science and mathematics education. The study will synthesize a wide range of research literatures relevant to improving ELLs' STEM learning, and provide a comprehensive understanding of how best to simultaneously support English language development and deep learning in the context of new and more challenging standards in science and mathematics. The study will also provide a framework for future research that can help to identify the most relevant and pressing questions for the field, as well as increase the number and quality of proposed research activities focused on ELLs in STEM. <br/><br/>To conduct the consensus study, BOSE will convene a multidisciplinary committee of experts who will synthesize the most relevant research on related subjects. The committee will include professionals in the fields of science and mathematics education, curriculum development, learning and instruction, linguistics, and assessment to address key sets of research questions: (1) Based on research-informed and field-tested models, strategies, and approaches, what are promising approaches to support ELLs (including ELLs with disabilities) in learning STEM? Given the diversity within the ELLs' population, what has worked, for whom, and under what conditions? What can be learned from these models and what additional research is needed to understand what makes them effective? What commonly used approaches may be less effective?; (2) What is the role of teachers in supporting the success of ELLs in STEM? What is known about the biases teachers may bring to their classrooms with ELLs and how these can be effectively addressed? What kinds of curriculum, professional development experiences, and assessment are needed in order for STEM teachers to improve their support for ELLs in STEM?; (3) How can assessments in STEM (both formative and summative) be designed to reflect the new content standards and to be appropriate for ELLs? What assessment accommodations might need to be considered?; (4) How do policies and practices at the national, state, and local level constrain or facilitate efforts to better support ELLs in STEM (including policies related to identification of students)? What kinds of changes in policy and practice are needed?; and (5) What are the gaps in the current research base and what are the key directions for research, both short-term and long-term? The committee will work over a 30-month period to synthesize relevant research literature and prepare a final consensus report, including results, conclusions, and recommendations. The study will address an issue of national importance and will inform future research on challenges directly related to ELLs, diversity, and equity in STEM education. This issue is particularly relevant to programs such as Discovery Research K-12 that supports efforts that reflect the needs of the increasingly diverse population, and Innovative Technology Experiences for Students and Teachers, which supports strategies for recruiting and selecting participants from identified groups currently underrepresented in STEM professions, careers, and education pathways. The report will target a broad audience of stakeholders, including teachers, school district administrators, researchers, congressional staff, and federal agencies that fund educational research and set policies related to ELLs."
"1615612","III: Small: Robust Algorithms for Multi-Task Learning of Spatio-Temporal Data","IIS","Info Integration & Informatics","08/01/2016","06/20/2016","Pang-Ning Tan","MI","Michigan State University","Standard Grant","Wei Ding","07/31/2020","$499,891.00","Lifeng Luo","ptan@cse.msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","CSE","7364","7364, 7923","$0.00","Recent years have witnessed an explosive growth of data collected from different geographical locations at different time points, known as spatio-temporal data. Spatio-temporal data are generated from a wide array of sensing technology and through scientific simulations. Analysis of the big spatio-temporal data is crucial as it supports key applications of national importance such as environmental sustainability, healthcare, and national security. However, in order to realize its full benefits, there are major analytical and computational challenges that must be overcome. This interdisciplinary research seeks to overcome some of the challenges by developing innovative prediction algorithms. As proof of concept, the algorithms will be incorporated into an ensemble prediction system developed by one of the principal investigators to enhance the nation's seasonal drought forecast capability. Benchmark data sets will be created and made available to other researchers for evaluating their algorithms. The project web page (http://www.cse.msu.edu/~ptan/project/mtl) will be used to disseminate experimental results, published papers, and software developed in this project. The graduate students who participate in the project will be trained to conduct cutting edge multidisciplinary research as next-generation data scientists.<br/><br/>There are many spatio-temporal prediction tasks that involve solving multiple related sub-problems. Rather than designing the prediction model for each sub-problem independently, it would be desirable to solve the prediction tasks jointly by exploiting their spatio-temporal autocorrelations, thus making it a natural fit for applying a multi-task learning paradigm. This proposal centers around the following four key contributions to address the challenges of applying multi-task learning to large-scale spatio-temporal data. First, a space-efficient online multi-task learning algorithm, with theoretically proven convergence rates, will be developed to handle the massive volume of data. Second, a deep learning framework will be developed to extract salient spatio-temporal features for multi-task predictions. Third, novel multi-task learning algorithms will be developed to deal with multi-scale variables, which are becoming increasingly prevalent with the proliferation of high-resolution sensors and simulation data generated at increasingly finer resolutions. Finally, the proposed algorithms will be applied to various spatio-temporal domains, including climate and environmental sciences. Overall the proposed research will advance current state-of-the-art by developing innovative prediction algorithms that consider the inherent spatial and temporal variability of the data and provide solutions with high predictive accuracy and learning efficiency."
"1613378","Doctoral Dissertation Research:  Guenon face patterns and the maintenance of primate reproductive isolation","BCS","Bio Anthro DDRI","07/15/2016","07/15/2016","James Higham","NY","New York University","Standard Grant","Rebecca Ferrell","06/30/2019","$22,611.00","Sandra Winters","jhigham@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","SBE","7608","1392, 9179, SMET","$0.00","Reproduction and mate choice are key components of evolution because they affect how species are formed and maintained. In primates, there are several examples of closely-related primate species living in overlapping geographic ranges. How do these animals distinguish between members of their own versus other species when looking for mates?  This doctoral dissertation project is an investigation of the role of face patterns and behavioral displays in maintaining reproductive isolation in the guenons, a group of forest monkey species in Africa that exhibit an extraordinary variety of colorful face patterns. Innovative methods will be used to measure and analyze the monkeys' appearance and their behavior related to members of their own species and closely-related species. Computer code and methodological details will be made freely available, and the project will support education and diversity in science through undergraduate training and K-12 science education outreach. The investigators will also be involved in scientific and biodiversity conservation outreach programs at their research site, and the project will foster international collaborations between research and conservation organizations in the USA and Nigeria.   <br/><br/>Identifying appropriate mates is potentially challenging for closely-related species with overlapping geographic ranges. One mechanism for preventing matings between different species is the evolution of signals that advertise species membership and that are used in mate choice to maintain reproductive isolation. Previous research has demonstrated that guenons can be classified by species based on images of their faces, and that facial distinctiveness increases with the degree of range overlap. However, the behavioral mechanisms by which a system of facial distinctiveness and associative mate choice might operate remain unclear. In this project, researchers will assess if and how guenons discriminate between species, and how variation in face patterns and behavioral displays influences mating decisions. To answer this question, researchers will: (1) identify regions of guenon faces critically important for species classification using computer vision algorithms; (2) assess guenon visual biases for same versus different species faces using a discrimination task with live guenons; (3) quantify behavioral displays and investigate their relationship with facial morphology using computer vision algorithms; and (4) document the ways in which guenon morphology and behavior are used in the wild using behavioral observations. This research will help to clarify the extent to which guenon face patterns function as signals of species membership, and how guenon morphologies and behaviors interact to influence their mating decisions. Ultimately, the results of this project will help us understand the relationship between phenotypic diversity and reproductive isolation."
"1621689","SBIR Phase I:  Machine Vision for Content-based Video Marketing Analytics","IIP","SBIR Phase I","07/01/2016","06/24/2016","Samuel Anthony","MA","Perceptive Automata, Inc.","Standard Grant","Peter Atherton","09/30/2017","$224,632.00","","santhony@perceptiveautomata.com","1 Broadway 5th Fl","Cambridge","MA","021421190","6172991296","ENG","5371","5371, 8032","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is to protect consumer privacy while continuing to enable the ad-supported Internet model. Current tracking-based consumer targeting approaches inherently erode consumer privacy, surreptitiously tracking users across many different web sites in an effort to gather demographic and behavioral data. On the flip side of the coin, marketers need to collect such data to successfully reach their audiences, and the revenue that marketers pour into advertising online has become an essential component of the economics of the internet. Today, this delicate balance of competing pros and cons is further threatened by the rise of ad-blocking software, which erodes the value of internet ad placement. The video marketing analytics capability developed in this project will limit marketers' need for invasive consumer data, while improving consumer experience. In the commercial realm, marketers would value the opportunity to target their ads in the most emotionally consonant, least disruptive, and most engaging manner possible. This technology will provide marketers with the capability to watch millions of videos algorithmically, thus enabling a more streamlined and customized viewer experience than has ever before been possible on television or on the Internet. <br/><br/>This Small Business Innovation Research Phase I project seeks to develop commercial applications for Perceptual Annotation, a technology developed with NSF funding that allows detailed measurements of human performance to be infused into a machine learning process, allowing the machine learner to both perform better and to perform in a way that is more consistent with humans. By adding this new category of human-derived supervisory signal into a machine learning process, the proposers have demonstrated that it is possible to significantly boost machine vision performance, allowing machines to generalize better to new, previously unseen images. While the company's technology has been rigorously validated on large-scale ""in the wild"" academic datasets, a major technical drive in the proposed SBIR Phase I activities will be to shift the company's efforts to the analysis of ""live,"" enormous, and ever-expanding data sets such as online videos. A second major drive of the proposed Phase I work will be the construction of ""second stage"" machine learning models that take perceptual-annotation-based machine ratings as an input and output actionable marketing decisions."
"1642345","Collaborative Research: SI2-SSE: High-Performance Workflow Primitives for Image Registration and Segmentation","OAC","Software Institutes","10/01/2016","09/08/2016","Gregory Sharp","MA","Massachusetts General Hospital","Standard Grant","Stefan Robila","09/30/2020","$110,000.00","","gcsharp@partners.org","Research Management","Somerville","MA","021451446","8572821670","CSE","8004","026Z, 7433, 8004, 8005","$0.00","Image registration and segmentation are vital enabling technologies for addressing many complex, data driven problems. Examples include individualized medical treatment where disease progression is monitored by analyzing MRI, CT, or ultrasound images over time; identifying anatomical structures in medical images; recognizing objects and people in video footage; and extracting imageable biometrics such as fingerprints, faces, and the iris. Images and videos can now be easily acquired at a rate that far surpasses our capacity to perform advanced image analysis.  For this reason, advanced registration and segmentation algorithms are not routinely used for many large-scale and time sensitive applications because they require more processing time than is available. This project will remedy this situation by developing a high-performance software package for image registration and segmentation, suitable to be run on massively parallel processors, and building a strong user and developer base around it. All software developed through the project will be open source and licensed under the MIT License. Improvements in processing speed achieved by the proposed platform will have significant impact in disciplines such as computer vision, digital forensics, and biomedical image analysis. Finally, the project team is committed to the diversity mission of Drexel University and will reach out to under-represented groups when recruiting graduate students for this project. Selected research tasks will be integrated within existing courses and curriculum will be developed for new experiential programs stemming from this effort.<br/><br/>The overall goal of this project is to develop a high-performance, many-core CPU and GPU accelerated algorithmic software package for attacking classes of problems that depend on solutions to data-dense inverse problems such as registration, segmentation, tomography, and parameter estimation. The specific technical approach involves developing algorithmic primitives required by a broad class of inference and analysis based workflows. Probabilistic primitives for building generative, discriminative, and conditional random field classification models will be implemented with emphasis on object segmentation.  Specialized registration operators will be developed for spline and voxel-driven algorithms. These primitives will be developed within the single instruction multiple data paradigm which utilizes many-core processing architectures via OpenMP, CUDA, and OpenCL. The workflow will be supplemented by a graphical user interface (GUI), providing a feature rich studio of tools that expose high-performance primitives to scientists visually and intuitively. The platform architecture will be designed as a distributed system service targeting locally administered scientific computing clusters where the number of compute nodes will be able to scale with load requirements. The GUI and the computational core may either run in a distributed client-server configuration or together locally on a single high performance workstation. Emphasis will be placed on documentation and video/written tutorials necessary for adoption. The project team will use an open software development model to build a strong user base comprising both novice users as well as researchers with the need to implement new algorithms on top of a stable software infrastructure. It is expected that the availability of this tool and its source code will catalyze an increase in quantitative image analysis spanning across research disciplines."
"1636865","BD Spokes: SPOKE: MIDWEST: Digital Agriculture - Unmanned Aircraft Systems, Plant Sciences and Education","OAC","BD Spokes -Big Data Regional I, International Research Collab","10/01/2016","10/16/2018","Travis Desell","ND","University of North Dakota Main Campus","Standard Grant","Alejandro Suarez","09/30/2020","$995,739.00","Joe Colletti, Gregory Monaco, Richard Barnhart, William McGimpsey, Jennifer Clarke, Travis Desell","tjdvse@rit.edu","264 Centennial Dr Stop 7306","Grand Forks","ND","582027306","7017774151","CSE","024Y, 7298","028Z, 7433, 8083, 9150","$0.00","The Digital Agriculture Spoke of the Midwest Big Data Hub seeks to organize academic, industrial, and governmental sectors around the development of policies and best practices for data science and Big Data applications in agriculture, with a particular focus on automating the Big Data lifecycle for unmanned aircraft systems (UAS) and for plant sciences, phenomics, and genomics. This effort is necessitated by the projected growth in the global population (9.5 billion people by 2050), which will require the global agricultural workforce to produce 70% more food than our farmers do today. Historically, agricultural revolutions in cultivation, social organization, and industrialization have provided the means to increase food production. However, future revolutions must leverage the advantages provided by the modern information society. This project will serve as a catalyst for this data-driven revolution, which will be broad and societal in nature and address the triple-bottom line of being economically viable, socially acceptable, and environmentally sensible. Whereas the initial focus areas are specific, the resulting best practices and partnership-building will translate to and enable other areas such as remote sensing systems and farm management techniques.  An expected outcome is improved and efficient use of UAS, imaging, and genomics in agricultural sciences, ultimately leading to a more sustainable global food and nutrition system. Coordination of these activities will be enhanced through a Digital Agriculture open web portal of data science resources, designed to integrate existing information silos, facilitate collaboration, and contribute to workforce development. Educational activities and tools will be leveraged from pre-existing traineeship programs and collaborative entities, and broadened with newly developed annual workshops. Special issue-teams of academic, industrial, and governmental representatives will be used to conduct deep-learning analysis of project educational activities to identify and refine mechanisms for broadening and diversifying participation. Through these efforts, the collaboration will improve access to data assets, train a workforce with relevant skills and expertise, and will contribute to solving the Sustainable Global Food and Nutrition Security challenge.<br/><br/>This project will focus on two knowledge domains important to agriculture, UAS, and Plant Sciences. These two themes of Intellectual Merit will be melded with cross-cutting activities designed to improve the management, accessibility, automation, and value of the lifecycle for data that are generated by multiple, high-throughput sensor and measurement platforms in contexts related to agriculture and agriculture production. Best practices for transport, storage, dissemination, and analysis of Big Data will be translatable and scalable to other areas such as farm management systems and precision agriculture, and will enable the access to and use of valuable data assets related to UAS and plant sciences, thereby accelerating progress toward sustainable agricultural production. Many of the ideas and methods developed under this project and the partnership-building activities that link multiple public institutions and private entities will be transferable to other disciplines that require Big Data, such as transportation, health sciences, and food, energy, and water, and will therefore generate innovation and discovery from many and complex data resources. One aspect of these partnerships is the desire to build a workforce with strong data science skillsets. To accomplish this, project activities include participation by undergraduate, graduate, and early career scientists in annual meetings, Zoom events, and webinars. Interested participants from the academic, industrial, and governmental sectors will be supported and encouraged to engage in cutting-edge research and development areas such as direct data collection of plant features by UAS, biological feature extraction through image analysis, Big Data processing pipelines, and techniques for data management and sharing. Diversity of innovation related to UAS and Plant Sciences will be encouraged through a suite of issue teams who analyze in-person and web-based trainings, goal-oriented Meetups, and conference events for diversity using deep learning techniques. These modalities for deep learning were selected for their scalability and improved access by underrepresented groups. The project has a heavy emphasis on workforce training and best practices. Workshops and webinars, including hackathons and datathons, will help both students and people already in the workforce expand their professional development."
"1642380","Collaborative Research: SI2-SSE: High-Performance Workflow Primitives for Image Registration and Segmentation","OAC","Software Institutes","10/01/2016","09/08/2016","James Shackleford","PA","Drexel University","Standard Grant","Stefan Robila","09/30/2020","$390,000.00","Nagarajan Kandasamy","shack@drexel.edu","1505 Race St, 10th Floor","Philadelphia","PA","191021119","2158955849","CSE","8004","026Z, 7433, 8004, 8005","$0.00","Image registration and segmentation are vital enabling technologies for addressing many complex, data driven problems. Examples include individualized medical treatment where disease progression is monitored by analyzing MRI, CT, or ultrasound images over time; identifying anatomical structures in medical images; recognizing objects and people in video footage; and extracting imageable biometrics such as fingerprints, faces, and the iris. Images and videos can now be easily acquired at a rate that far surpasses our capacity to perform advanced image analysis.  For this reason, advanced registration and segmentation algorithms are not routinely used for many large-scale and time sensitive applications because they require more processing time than is available. This project will remedy this situation by developing a high-performance software package for image registration and segmentation, suitable to be run on massively parallel processors, and building a strong user and developer base around it. All software developed through the project will be open source and licensed under the MIT License. Improvements in processing speed achieved by the proposed platform will have significant impact in disciplines such as computer vision, digital forensics, and biomedical image analysis. Finally, the project team is committed to the diversity mission of Drexel University and will reach out to under-represented groups when recruiting graduate students for this project. Selected research tasks will be integrated within existing courses and curriculum will be developed for new experiential programs stemming from this effort.<br/><br/>The overall goal of this project is to develop a high-performance, many-core CPU and GPU accelerated algorithmic software package for attacking classes of problems that depend on solutions to data-dense inverse problems such as registration, segmentation, tomography, and parameter estimation. The specific technical approach involves developing algorithmic primitives required by a broad class of inference and analysis based workflows. Probabilistic primitives for building generative, discriminative, and conditional random field classification models will be implemented with emphasis on object segmentation.  Specialized registration operators will be developed for spline and voxel-driven algorithms. These primitives will be developed within the single instruction multiple data paradigm which utilizes many-core processing architectures via OpenMP, CUDA, and OpenCL. The workflow will be supplemented by a graphical user interface (GUI), providing a feature rich studio of tools that expose high-performance primitives to scientists visually and intuitively. The platform architecture will be designed as a distributed system service targeting locally administered scientific computing clusters where the number of compute nodes will be able to scale with load requirements. The GUI and the computational core may either run in a distributed client-server configuration or together locally on a single high performance workstation. Emphasis will be placed on documentation and video/written tutorials necessary for adoption. The project team will use an open software development model to build a strong user base comprising both novice users as well as researchers with the need to implement new algorithms on top of a stable software infrastructure. It is expected that the availability of this tool and its source code will catalyze an increase in quantitative image analysis spanning across research disciplines."
"1566270","CRII: RI: Automatically Understanding the Messages and Goals of Visual Media","IIS","CRII CISE Research Initiation, Robust Intelligence","06/01/2016","03/08/2018","Adriana Kovashka","PA","University of Pittsburgh","Standard Grant","Jie Yang","05/31/2019","$182,590.00","","kovashka@cs.pitt.edu","300 Murdoch Building","Pittsburgh","PA","152603203","4126247400","CSE","026Y, 7495","7495, 8228, 9251","$0.00","This project develops technologies to interpret the visual rhetoric of images. The project advances computer vision through novel solutions to the novel problem of decoding the visual messages in advertisements and artistic photographs, and thus brings computer vision closer to its goal of being able to automatically understand visual content. From a practical standpoint, understanding visual rhetoric can be used to produce image descriptions for the visually impaired that align with how a human would label these images, and thus give them access to the rich content shown in newspapers or on TV. This project is tightly integrated with education. The work is interdisciplinary and can attract undergraduate students to the research from different fields. <br/><br/>This research focuses on three media understanding tasks: (1) understanding the persuasive messages conveyed by artistic images and the strategies that those images use to convey their message; (2) exposing a photographer's bias towards their subject, e.g., determining whether a photograph portrays its subject in a positive or negative light; and (3) predicting what part of an artistic photograph a viewer might find most captivating or poignant. To enable decoding of artistic images, a large dataset is collected and annotated with a number of artistic properties and persuasion techniques that are intended for human understanding, then methods are developed to model visual symbolism in artistic images, as well as adapt positive/negative effect methods from sentiment analysis. To predict the photographer's bias towards a subject, a dataset of historical and modern portrayals of minorities and foreigners is collected, then an algorithm is created that reasons about body language and 3D layout and composition of the photo. To predict poignance, eyetracking data on a set of artistic images from famous photographers is collected, then semantic and connotation conflicts between the objects in the photographs are analyzed."
"1527558","NRI: Collaborative Research: Multimodal Brain Computer Interface for Human-Robot Interaction","IIS","NRI-National Robotics Initiati","05/15/2016","05/27/2016","Joseph Francis","TX","University of Houston","Standard Grant","Kenneth Whang","04/30/2021","$308,077.00","","jtfranci@Central.UH.EDU","4800 Calhoun Boulevard","Houston","TX","772042015","7137435773","CSE","8013","8086, 8089","$0.00","Human Robot Interaction (HRI) is research that is a key component in making robots part of our everyday life. Current interface modalities such as video, keyboard, tactile, audio, and speech can all contribute to an HRI interface. However, an emerging area is the use of Brain-Computer Interfaces (BCI) for communication and information exchange between humans and robots. BCIs can provide another channel of communication with more direct access to physiological changes in the brain. BCIs vary widely in their capabilities, particularly with respect to spatial resolution, temporal resolution and noise. This project is aimed at exploring the use of multimodal BCIs for HRI. Multimodal BCIs, also referred to as hybrid BCIs (hBCI), have been shown to improve performance over single modality interfaces. This project is focused on using a novel suite of sensors (Electroencephalography (EEG), eye-tracking, pupillary size, computer vision, and functional Near Infrared Spectroscopy (fNIRS)) to improve current HRI systems. Each of these sensing modalities can reinforce and complement each other, and when used together, can address a major shortcoming of current BCIs which is the determination of the user state or situational awareness (SA). SA is a necessary component of any complex interaction between agents, as each agent has its own expectations and assumptions about the environment. Traditional BCI systems have difficulty recognizing state and context, and accordingly can become confusing and unreliable. This project will develop techniques to recognize state from multiple modalities, and will also allow the robot and human to learn about each other's state and expectations using the hBCI we are developing. The goal is to build a usable hBCI for real physical robot environments, with noise, real-time constraints, and added complexity.<br/><br/>The technical contributions of this project include:<br/>1. Characterization of a novel hBCI interface for visual recognition and labeling tasks with real physical data and environments.<br/>2. Integration of fNIRS sensing with EEG and other modalities in human robot interaction tasks. We will test our ability in the temporal domain to determine at what timescale we can correctly classify movement components that would predict a correct (rewarding) trial or non-rewarding/incorrect movement.<br/>3. Analysis and validation of the hBCI in complex robotic tele-operation tasks with human subject operators such as open door, grasp object on table, pick up item off floor etc.<br/>4. Use of hBCI to characterize human/robot state and create a learning method to recognize state over time.<br/>5. Use of augmented reality for HRI decision making.<br/>6. Further develop hBCI for tracking cognitive states related to reward, motivation, attention and value.<br/>A new class of HRI interfaces will be developed that can expand the ability of humans to work with robots; promote the use and acceptance of robot agent systems in everyday life; expand the use of hBCIs in areas other than robotics for human-machine interaction; further the development of hBCIs as our system will be tapping into reward modulated activity that will be used via reinforcement learning to autonomously update the learning machinery; and bridge the educational divide between Engineering and Neuroscience."
"1614309","EAPSI: Applying Machine Learning Techniques to Predict Task High Performance Computing Performance on a Variety of Execution Platforms","OISE","EAPSI","06/15/2016","07/05/2016","Jonathan Grossman","TX","Grossman                Jonathan       M","Fellowship Award","Anne Emig","05/31/2017","$5,400.00","","","","Houston","TX","770303282","","O/D","7316","5921, 5978, 7316","$0.00","Ten years ago, the highest performing computing systems in the world were homogeneous and specialized: they were composed of a single processor architecture and they executed primarily scientific workloads. Today, we see much more diversity in both the hardware platforms and applications. Many of the world's largest computing clusters contain multiple processor architectures and execute a wider variety of application workloads applications in science, data analytics, genomics, and medical imaging. The choice of software execution platforms has expanded. This increased complexity in multiple dimensions makes efficiently scheduling workloads on high-performance computing (HPC) systems more challenging. Even worse, our ability to reason about the behavior of automated scheduling systems usually diminishes as system complexity increases. This project takes steps to address these problems by experimenting with machine learning techniques for predicting task performance on a variety of execution platforms, including the Java Virtual Machine (JVM), native CPU threads, and native GPU threads. This research will be conducted under the mentorship of Professor Hironori Kasahara, Director of the Advanced Multicore Processor Research Institute at Waseda University in Tokyo, Japan. The work has the potential to positively impact present and future high-performance computing applications in both industry and research las that run on heterogeneous platforms. <br/><br/>The project will focus on the development of a novel framework for automatic platform selection in the area of heterogeneous systems, as well as an understanding of how efficient, accurate, and analyzable modern techniques are at offline and online performance model training. Our proposed approach is hybrid, using both one-time offline training of performance models as well as continuous online training to produce a predictive performance model. We will construct and open source a general-purpose automatic platform selection framework to validate our approach. Rather than focusing on evaluating a single technique, we will use this framework to perform a comprehensive survey of proposed machine learning techniques for automatic platform selection to understand the tradeoffs of each in terms of performance, accuracy, and analyzability.<br/><br/>This award under the East Asia and Pacific Summer Institutes program supports summer research by a U.S. graduate student and is jointly funded by NSF and the Japan Society for the Promotion of Science."
"1625879","CI-P: Planning a Community Benchmarking Infrastructure for Bidirectional Reflectance Distribution Functions","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","08/01/2016","08/01/2016","Pieter Peers","VA","College of William and Mary","Standard Grant","Wendy Nilsen","01/31/2018","$100,000.00","","ppeers@cs.wm.edu","Office of Sponsored Programs","Williamsburg","VA","231878795","7572213966","CSE","7359","7359","$0.00","This project will support the planning and building of an initial testbed application for evaluating material models that describe how light is reflected  from surfaces in computer graphics research.  After talking with interested researchers in the community, the lead investigator will build an application that allows anyone to submit and download three important types of data for doing this research: (1) reference implementations of material models, (2) measures of how well the predicted light reflections of a model match real examples, and (3) examples of measured reflections from physical materials to compare against.  When people submit new material model implementations, the application will automatically run all of the models it contains using all of the measures and measured examples available.  This is an important advance because currently there is no good way for researchers to compare their work against others', which slows down progress in both research and graphics applications; the testbed will make it much easier to compare new ideas to old ones, as well as to understand which models work best in which situations.  This will advance research in a number of areas related to computer graphics, as well as directly benefiting society through improving graphics in real applications including virtual reality, animation, and data visualization. <br/><br/>To do this, the lead investigator will create a repository with an initial set of reference algorithms, datasets, fitting functions, and evaluation metrics based on existing research around bidirectional reflectance distribution functions (BRDFs).  Other researchers will be able to contribute to the repository through an Application Programming Interface (API) that allows for easy addition of new items; it will then dynamically report on run all-against-all comparisons with existing items in the repository.  During this planning grant the lead investigator will develop the initial infrastructure both to demonstrate a proof-of-concept and to present a concrete artifact that can jump start the discussion of how to make the repository more useful to the broader research community.  Although starting with researchers most directly involved with BRDFs, the community will eventually include people from a number of related research areas, including non-linear optimization, human perception, rendering, photorealistic visualization, and computer vision."
"1613054","A Geometric Approach to Bayesian Modeling and Inference with the Nonparametric Fisher-Rao Metric","DMS","STATISTICS","09/15/2016","09/06/2016","Sebastian Kurtek","OH","Ohio State University","Standard Grant","Gabor Szekely","08/31/2020","$120,000.00","Karthik Bharath","kurtek.1@osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","MPS","1269","","$0.00","Bayesian modeling and inference are commonly used statistical approaches to the analyses of complex high-dimensional data from many scientific fields including computer vision, biology, biometrics, bioinformatics and medicine. This research project is concerned with developing geometry-based, computationally efficient and scalable tools for Bayesian modeling of such datasets that have high potential for revealing novel insights. An example is a Bayesian model for statistical analysis of tumor heterogeneity in cancer with the possibility for improved disease characterization and new treatment approaches. The novelty and potential for high impact of this project come from the utility of an area of mathematics called differential geometry in the study of Bayesian statistical models and inferences. <br/><br/>While much progress has been made in the area of Bayesian modeling and inference both in terms of theory and computation, little attention has been given to studying the underlying geometry of such models. In this project, the PIs focus on developing a practical, unified Riemannian-geometric framework for three main problems: (1) Bayesian sensitivity analysis, (2) geometric variational inference, and (3) geometric nonparametric prior construction; these problems culminate in a fourth one of Bayesian density estimation, wherein the tools described in the first three can be used with obvious advantages. For a Bayesian model with prior, sampling and posterior densities, the geometric properties of the model are investigated and exploited through a square-root transformation, under which the nonlinear manifold of probability densities endowed with the nonparametric Fisher-Rao metric simplifies to the positive orthant of the unit sphere endowed with the Euclidean metric. Because the geometry of the sphere is well-known, important tools for analysis (e.g., exponential and inverse exponential maps, parallel transport, geodesics) are available in closed-form. As a result, this framework is versatile computationally and applicable to parametric, semiparametric and nonparametric Bayesian models. More importantly, it provides a formal mathematical background for defining distances between densities and developing geometrically calibrated measures. Thus, the two main contributions of this project are the development of (1) metric-based inferential methods for Bayesian models that may permit a more intuitive explanation of prior and posterior beliefs, and (2) a geometric quantification of various aspects of posterior inference through intrinsic analysis on the space of all probability densities."
"1565235","AF: Large: Collaborative Research: Algebraic Proof Systems, Convexity, and Algorithms","CCF","Special Projects - CCF, Algorithmic Foundations","05/01/2016","09/09/2019","Pablo Parrilo","MA","Massachusetts Institute of Technology","Continuing Grant","A. Funda Ergun","04/30/2021","$2,135,000.00","Jonathan Kelner, Ankur Moitra","Parrilo@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","2878, 7796","7925, 7926, 7927","$0.00","This project tackles some of the central questions in algorithms, optimization, and the theory of machine learning, through the lens of the ""Sum of Squares"" algorithmic framework. In particular, it will allow us to understand what classes of functions can be efficiently minimized, and what computational resources are needed to do so. If successful, this will significantly advance our understanding in all these key areas, produce new practical algorithmic methodologies, as well as build new connections with other fields, including quantum information theory, statistical physics, extremal graph theory and more.<br/><br/>This collaborative grant will foster new interactions between intellectual communities that have had relatively little interaction so far, and the PIs will organize workshops, courses, and other events that bring these communities together. The students and postdocs trained will gain a uniquely broad view of the landscape of these areas.<br/><br/>The PIs propose a unified approach to the development and analysis of convex proof systems that include and generalize the ""Sum of Squares"" (SoS) method. Despite considerable recent progress, understanding SoS?s performance seems to be out-of-reach for most current techniques.  Significant progress in this area requires the synthesis of ideas and techniques from different domains, including theoretical computer science, optimization, algebraic geometry, quantum information theory and machine learning. The research plans include both theory-building and problem-solving aspects, with the ultimate goal of obtaining a complete understanding of the SoS method and related proof systems, as well as their algorithmic implications.<br/><br/>Research efforts will be directed along several thrusts: Unique Games and related problems (Small Set Expansion, Max Cut, Sparsest Cut), analysis of average-case problems (e.g., Planted Clique), applications to Machine Learning (sparse PCA, dictionary learning), algorithmic speedups of SoS, and connections to math and physics (e.g., quantum entanglement, p-spin glasses, extremal graph theory and algebraic geometry). While the main focus is on theoretical aspects, this project is also concerned with effective computational methods, and the outcomes may yield novel practical techniques for machine learning and optimization.<br/><br/>Other key features of this proposal include its strong integration with curriculum development, undergraduate research projects, and training the next wave of graduate students and postdocs and equipping them with the necessary tools to work across these areas."
"1565264","AF: Large: Collaborative Research: Algebraic Proof Systems, Convexity, and Algorithms","CCF","Algorithmic Foundations","05/01/2016","06/25/2020","Boaz Barak","MA","Harvard University","Continuing Grant","A. Funda Ergun","04/30/2021","$865,000.00","","b@boazbarak.org","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","7796","7925, 7926, 7927","$0.00","This project tackles some of the central questions in algorithms, optimization, and the theory of machine learning, through the lens of the ""Sum of Squares"" algorithmic framework. In particular, it will allow us to understand what classes of functions can be efficiently minimized, and what computational resources are needed to do so. If successful, this will significantly advance our understanding in all these key areas, produce new practical algorithmic methodologies, as well as build new connections with other fields, including quantum information theory, statistical physics, extremal graph theory and more.<br/><br/>This collaborative grant will foster new interactions between intellectual communities that have had relatively little interaction so far, and the PIs will organize workshops, courses, and other events that bring these communities together. The students and postdocs trained will gain a uniquely broad view of the landscape of these areas.<br/><br/>The PIs propose a unified approach to the development and analysis of convex proof systems that include and generalize the ""Sum of Squares"" (SoS) method. Despite considerable recent progress, understanding SoS?s performance seems to be out-of-reach for most current techniques. Significant progress in this area requires the synthesis of ideas and techniques from different domains, including theoretical computer science, optimization, algebraic geometry, quantum information theory and machine learning. The research plans include both theory-building and problem-solving aspects, with the ultimate goal of obtaining a complete understanding of the SoS method and related proof systems, as well as their algorithmic implications.<br/><br/>Research efforts will be directed along several thrusts: Unique Games and related problems (Small Set Expansion, Max Cut, Sparsest Cut), analysis of average-case problems (e.g., Planted Clique), applications to Machine Learning (sparse PCA, dictionary learning), algorithmic speedups of SoS, and connections to math and physics (e.g., quantum entanglement, p-spin glasses, extremal graph theory and algebraic geometry). While the main focus is on theoretical aspects, this project is also concerned with effective computational methods, and the outcomes may yield novel practical techniques for machine learning and optimization.<br/><br/>Other key features of this proposal include its strong integration with curriculum development, undergraduate research projects, and training the next wave of graduate students and postdocs and equipping them with the necessary tools to work across these areas."
"1544244","Research Initiation: Investigating Engineering Students Habits of Mind: A Case Study Approach","EEC","EngEd-Engineering Education","01/01/2016","09/11/2015","Mireille Boutin","IN","Purdue University","Standard Grant","Julie Martin","12/31/2018","$150,000.00","Alejandra Magana-de-Leon","mboutin@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","ENG","1340","110E","$0.00","The problems facing engineers in the 21st century are highly complex, involving often competing requirements related to available technology, societal needs, environmental considerations, and others. Fostering the ability of students to solve these complex problems requires going beyond teaching technical skills to consider the way engineers think. This project is an interdisciplinary study that aims to characterize undergraduate engineering students' ""habits of mind"", which are modes of thinking required for engineering students to become effective problem solvers capable of transferring such skills to new contexts. An example of habit of mind is a willingness to make mistakes while trying to solve a problem, an attitude that allows engineers to successfully attack problems that were previously unsolved. Students who have these habits of mind will be prepared to address complex issues such as those described in the NAE Grand Challenges.<br/><br/>The project will leverage an education website called Rhea (www.projectrhea.org) along with data acquired as part of an engineering course to identify how students experience scientific habits of mind as they engage in problem solving. To this end, qualitative and quantitative research methods enhanced with machine learning techniques will be combined. This will be accomplished through an interdisciplinary partnership in which a boundary spanning research program to identify and validate novel research methods and formative and summative assessment mechanisms will be initiated. The efforts will center on enhancing qualitative and quantitative educational research and assessment methods with machine learning techniques such as automatic data clustering. The rationale for this project is that its successful completion will (a) enable the research team to develop collaborative inter-disciplinary efforts where the PI Boutin will develop expertise in educational research methods and the co-PI Magana will develop expertise in machine learning; (b) provide a context for an exploratory study to be used as a baseline to apply for future funding to establish a program in engineering education research methods and assessment; (c) address challenges in cultivating a culture of lifelong learning among professional and future engineers via scientific habits of mind in an engineering context; and (d) develop new methods to characterize and measure different aspects of professional formation processes in engineering education. The long term vision is to develop an interdisciplinary partnership in the field of machine-learning-assisted education research."
"1563113","SHF: Medium: PRISM: Platform for Rapid Investigation of efficient Scientific-computing & Machine-learning","CCF","Software & Hardware Foundation","08/01/2016","07/18/2017","Oyekunle Olukotun","CA","Stanford University","Standard Grant","Almadena Chtchelkanova","07/31/2021","$960,000.00","Jack Poulson","kunle@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","7798","7924, 7941, 7942","$0.00","Today's systems demand acceleration in processing and learning using massive datasets. Unfortunately, because of poor energy scaling and power limits, performance and power improvements due to technology scaling and instruction level parallelism in general-purpose processors have ended. It is well known that full custom, application-specific hardware accelerators can provide orders-of-magnitude improvements in energy/op for a variety of application domains. Therefore, there is a special interest in systems that can optimize and accelerate the building blocks of machine learning and data science routines. Many of these building blocks share the same characteristics as building blocks of high performance computing kernels working on matrices. <br/><br/>Such application specific solutions rely on joint optimization of algorithms and the hardware, but cost hundreds of millions of dollars. PRISM (Platform for Rapid Investigation of efficient Scientific- computing and Machine-learning accelerators) is proposed to amortize these costs. PRISM enables application designers to get rapid feedback about both the available parallelism and locality of their algorithm, and the efficiency of the resulting application/hardware design. PRISM platform consists of two coupled tools that incorporate design knowledge at both the hardware and algorithm level. This knowledge enables the tool to give application designers the ability to quickly evaluate the performance of their applications on the proposed/existing hardware, without the application designer needing to be an expert at hardware or algorithms. This platform will leverage tools created from the team's prior research. <br/><br/>Initially, these tools will be used to create an efficient solution for each application, followed by a comparison of the resulting hardware designs. The possibility of creating platforms that span multiple classes of algorithms can then be explored. Finally, a comparison of these new architectures to existing heterogeneous architectures with GPUs and FPGAs will be made, to gain understanding about what modifications are necessary for these architectures to achieve higher levels of efficiency when supporting these classes of algorithms. The work on key applications will lead to better insight about the computation and communication intrinsic to these computations, and provide algorithms for these applications that will be effective on conventional and new architectures."
"1637479","NRI: Collaborative Research: Experiential Learning for Robots: From Physics to Actions to Tasks","IIS","NRI-National Robotics Initiati","10/01/2016","05/15/2018","Dieter Fox","WA","University of Washington","Standard Grant","James Donlon","09/30/2020","$760,000.00","Ali Farhadi","fox@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","8013","8086, 9251","$0.00","Recent advances in machine learning coupled with unprecedented archives of labeled data are advancing machine perception at a remarkable rate. However, applying these advances to robotics has not advanced as quickly because learning for robotics requires both active interaction with the physical world, and the ability to generalize over a variety of task contexts. This project addresses this knowledge gap through the development of new learning methods to produce experience-based models of physics. In this approach, an object or category specific model of physics is learned directly from perceptual data rather than deploying general-purpose physical simulation methods. These physical models will support both direct control of action - for example pouring a liquid into a container, and the learning of the physical effects of sequences of actions - for example planning to handle fluids in a laboratory. More generally, these methods will provide a means for robots to learn how to handle fluids, soft materials, and other complex physical phenomena.<br/><br/>The proposed experiential learning framework will build on recent advances in deep neural networks. The key problem is to learn the mappings between raw perceptual and control data via a low-dimensional implicit physics space representing a perception-based physical model of how an object acts in the environment. Three directions will be investigated: 1) the development of experiential physics models for object interaction and fluid flow that have strong predictive capabilities, 2) creating mappings directly from experiential models to control of actions such as pouring or moving an object, 3) the assembly of local experience-based controllers into complex tasks from interactive demonstration. Additionally, the project will develop unique data sets that include physical models, simulations, data components, and learned components that other groups can access and build on to enable comparative research similar to what has emerged in machine perception."
"1638429","BIGDATA: Small: DA: Collaborative Research: Real Time Observation Analysis for Healthcare Applications via Automatic Adaptation to Hardware Limitations","IIS","Big Data Science &Engineering","03/01/2016","04/18/2016","Rong Jin","PA","Carnegie-Mellon University","Standard Grant","Sylvia Spengler","06/30/2017","$97,799.00","","rongjin@cse.msu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","8083","7433, 7923, 8083","$0.00","This research seeks to develop novel machine learning algorithms that enable real-time video and sensor data analysis on large data streams given limited computational resources. The work focuses on healthcare as an application domain where real-time video analysis can prevent user-errors in operating medical devices or provide immediate alerts to caregivers about dangerous situations.  The research will develop algorithms to automatically adapt data analysis approaches to maximize accuracy of analysis within a short time period despite limited available computing resources. Today's healthcare environment is significantly more technologically sophisticated than ever before. Many medical devices are now frequently used in patient's homes, ranging from simple equipment such as canes and wheelchairs to sophisticated items such as glucose meters, ambulatory infusion pumps and laptop-sized ventilators. The rapidly growing home health industry raises new safety concerns about devices being used inappropriately in the home setting. The proposed research is designed to reduce medical device related use-errors by developing computational algorithms that perform real-time video analysis and alert the patient or caregiver when medical devices are not used appropriately. The real-time video and sensor data analysis is also critical to the healthcare systems that monitor the activities of the elderly or those with disabilities in order to allow a caregiver to react immediately to an incident. <br/><br/>New machine learning theories and algorithms will automatically adapt to hardware limitations, with the aim to learn from a large number of training examples, a prediction function that (i) is sufficiently accurate in making effective predictions and (ii) can be run efficiently on a specified computer system to deliver time critical results. Three types of prediction models are studied to address the problem of automatic hardware adaptation, including a vector-based model, a matrix-based model, and a prediction model based on a function from a Reproducing Kernel Hilbert Space (RKHS).  A general framework and multiple optimization techniques are being developed to learn accurate prediction models that match limited memory and computational capacity. The new learning algorithms will be evaluated in several medical scenarios through real-time prediction of a patient's activities from observations in the large video archives collected by several healthcare related projects.  The intellectual merit of the proposed work is in bridging the gap between the high complexity of a prediction model and limited computational resources, a scenario that is encountered in many application domains besides healthcare. The proposed research in machine learning algorithms and theories will make it possible to run complicated prediction algorithms on big data within the limitation of a given computing infrastructure. The developed techniques for automatic hardware adaptation will be applied to a large dataset of continuous video and sensor recordings for medically-critical activity recognition.  The project's broader impacts include providing medical experts with algorithms and tools supporting novel approaches to analyzing observational data in their quest to recognize and characterize human behavior. Surveillance systems with continuous observations will be able to categorize salient events with co-located, limited hardware. Researchers with complex data from continuous streams will be able to explore their domains with greater accuracy within constrained time using their available computing resources. Similarly, large archives can be exploited as rapidly as possible with limited hardware."
"1625378","ChANgE Chem Lab: Cognitive Apprenticeship for Engineers in Chem Lab","DUE","IUSE","09/01/2016","08/23/2016","Kent Crippen","FL","University of Florida","Standard Grant","Tom Higgins","08/31/2021","$599,333.00","Philip Brucat, CHANG-YU WU, Maria Korolev","kcrippen@coe.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","EHR","1998","8209, 9178","$0.00","ChANge Chem Lab will improve the quality of STEM education by transforming the laboratory curriculum of undergraduate general chemistry for engineers. This builds on the success of a prior NSF-funded TUES project (DUE-1245068). The curriculum will provide a rich context grounded in the Grand Challenges for Engineering, ensuring a relevant and engaging experience for students. Development activities will include inquiry-based laboratory experiments and support materials that emulate and make explicit an engineer's way of thinking, learning, and collaborating. Student success will be supported by design structures that engage deep learning strategies and which embody our understanding of effective learning in the laboratory. This approach has been shown to support the retention of underrepresented student groups.  <br/><br/>ChANgE Chem Lab will generate further evidence regarding how to utilize the potential of laboratory experiments to improve student learning and broaden participation in STEM. This will be achieved by building upon the intellectual merit of prior work (DUE-1245068), which produced a validated model of project-based learning that supports the retention of underrepresented students. Cognitive apprenticeships will provide the theoretical framework to develop six new engineering design projects for the undergraduate general chemistry lab. Each project will include three weeks of inquiry-based experiments and support materials. The project will directly involve more than 600 students, 15 teaching assistants, and five instructors. This approach has been shown to increase the retention of all students, and this project will address the low retention rate of college for students majoring in engineering during the first two-years and broaden participation."
"1637949","NRI: Collaborative Research: Experiential Learning for Robots: From Physics to Actions to Tasks","IIS","NRI-National Robotics Initiati","10/01/2016","08/17/2016","Gregory Hager","MD","Johns Hopkins University","Standard Grant","James Donlon","08/31/2020","$648,000.00","Marin Kobilarov","hager@cs.jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","CSE","8013","8086","$0.00","Recent advances in machine learning coupled with unprecedented archives of labeled data are advancing machine perception at a remarkable rate. However, applying these advances to robotics has not advanced as quickly because learning for robotics requires both active interaction with the physical world, and the ability to generalize over a variety of task contexts. This project addresses this knowledge gap through the development of new learning methods to produce experience-based models of physics. In this approach, an object or category specific model of physics is learned directly from perceptual data rather than deploying general-purpose physical simulation methods. These physical models will support both direct control of action - for example pouring a liquid into a container, and the learning of the physical effects of sequences of actions - for example planning to handle fluids in a laboratory. More generally, these methods will provide a means for robots to learn how to handle fluids, soft materials, and other complex physical phenomena.<br/><br/>    The proposed experiential learning framework will build on recent advances in deep neural networks. The key problem is to learn the mappings between raw perceptual and control data via a low-dimensional implicit physics space representing a perception-based physical model of how an object acts in the environment. Three directions will be investigated: 1) the development of experiential physics models for object interaction and fluid flow that have strong predictive capabilities, 2) creating mappings directly from experiential models to control of actions such as pouring or moving an object, 3) the assembly of local experience-based controllers into complex tasks from interactive demonstration. Additionally, the project will develop unique data sets that include physical models, simulations, data components, and learned components that other groups can access and build on to enable comparative research similar to what has emerged in machine perception."
"1565596","CRII: III: Integrating Domain Knowledge via Interactive Multi-Task Learning","IIS","","05/01/2016","04/20/2017","Jiayu Zhou","MI","Michigan State University","Continuing grant","Sylvia Spengler","03/31/2019","$174,883.00","","dearjiayu@gmail.com","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","CSE","026y","7364, 8228","$0.00","The ever increasing availability of data has attracted a huge amount of effort building machine learning models from the data to unleash its hidden power. One ubiquitous finding about these machine learning tasks is that in most real-world applications the learning tasks are closely related to each other. Moreover, human experts in many domains can usually provide indispensable domain knowledge describing how these models are related. Maximally exploiting such knowledge is critical in building high quality machine learning models. This project will develop effective and efficient interactive algorithms and tools (including open source software) to enable knowledge discovery by integrating domain knowledge of task relatedness from human experts. The algorithms and tools developed in this project will directly impact biomedical informatics as they will be used to build disease progression models. The educational component of this project includes developing a new curriculum that incorporates research into the classroom and provides students from under-represented groups with opportunities to participate in research.<br/><br/>Leveraging task relatedness, multi-task learning (MTL) simultaneously learns all related learning tasks and performs knowledge transfer among the tasks to improve the quality of models from all the tasks. Although there are numerous studies for MTL that assume different types of task relatedness, limited progress has been made in incorporating domain knowledge in MTL. This project will advance MTL by: (1) developing algorithms for knowledge aware multi-task feature learning which exploit domain knowledge of features to guide the selection of joint features from the learning tasks; (2) developing algorithms for knowledge aware multi-task relationship learning which utilize domain knowledge of tasks to guide the learning of task relationships; and (3) developing efficient and scalable optimization algorithms to facilitate effective interactive visualization. For further information see the project web page: http://jiayuzhou.github.io/projects/crii"
"1649208","EAGER: IIS: Empowering Probabilistic Reasoning with Random Projections","IIS","Robust Intelligence","09/01/2016","08/09/2016","Stefano Ermon","CA","Stanford University","Standard Grant","Kenneth Whang","08/31/2017","$90,000.00","","ermon@cs.stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","7495","7495, 7916","$0.00","Autonomous agents such as self-driving cars are required to act intelligently and adaptively in increasingly complex and uncertain real-world environments. To cope with the uncertainty and ambiguity of real world domains, AI systems rely heavily on statistical approaches. To make sensible decisions under uncertainty, agents need to reason probabilistically about their environments. Probabilistic reasoning, however, is known to be computationally very difficult in the worst case. While significant progress has been made over the past decades, many complex problems remain out of reach. This project aims to develop a new family of algorithms for reasoning under uncertainty. These novel techniques have the potential to provide more efficient algorithms for decision-making, learning and inference with improved theoretical guarantees on the accuracy. These techniques will be applicable in a wide range of domains, including medical diagnosis, information extraction, computer vision, and robotics.<br/><br/>This research project will develop a new family of algorithms for reasoning under uncertainty based on random projections. Random projections have played a key role in scaling up data mining and database systems. While drastically reducing computational cost, they also provide principled approximations. This research will explore the use of random projections based on universal hashing schemes in the context of probabilistic reasoning. The project will develop new techniques for learning and decision making under uncertainty problems. Specifically, new frameworks and algorithms with improved theoretical guarantees and practical performance will be developed. In order to provide efficient reasoning algorithms, the use of random projections will be considered in combination with a range of existing techniques, including modern optimization, variational, and sampling methods. A key focus will be to develop practical techniques and scale-up to real-world domains.  The techniques developed will be made available to both academia and industry through open-source software. Educational and outreach efforts will include the involvement of undergraduate students undertaking independent research projects."
"1632154","PFI:BIC MAKERPAD: Cognitively Intuitive Shape-Modeling and Design Interface enabling a Distributed Personalized Fabrication Network.","IIP","CM - Cybermanufacturing System, PFI-Partnrships for Innovation","09/01/2016","08/29/2016","Karthik Ramani","IN","Purdue University","Standard Grant","Jesus Soriano Molla","08/31/2021","$1,000,000.00","Ananth Iyer, Sanjay Rao","ramani@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","ENG","018Y, 1662","1662","$0.00","Currently product design and manufacturing are accessible only to enterprises and professionals such as engineers and designers. Everyone has ideas but only a select few can bring them to fabrication. This is because of lack of intuitive and easy-to-use design tools, detailed knowledge required to make prototypes, help needed for users with difficulties, and access to fabrication facilities. This gap could be seen as a waste of untapped human creative resources and economic potential to the Nation. MAKERPAD will bridge the gaps between the need for design tools for personalized production and the rapid growth of 3D fabrication technologies by providing intuitive tools for designing and access to remote fabrication resources. Using the cloud, MAKERPAD will enable personalized fabrication technology easily accessible and available to makers and students.<br/><br/>MAKERPAD is enabled by (a) 3D mixed and augmented reality, (b) cloud-powered algorithms (shape modeling, computer vision, human computer interaction), and (c) service models supporting 3D scanning, searching and printing. Using the affordances created by depth sensing cameras that are being embedded into both mobile and intuitive fabrication oriented design modeling, smart algorithms will be enabled to work real-time over the cloud. MAKERPAD will develop and use the human-centric cognitively intuitive design interfaces, digital modeling in the virtual cloud world, thus enabling connectivity to a personalized fabrication network. Using a new robotic toy platform as well as accessories as the use case, the service model design will appropriately support consumers at various levels of capabilities. The workflow at the user and system level will be designed in a seamless fashion for the users to obtain laser cut and 3D printed models of their designs. If successful, this system will support different levels of service, quality and sophistication of digital designs and physical models depending on the user requirements, usage context, budget and time constraints. The smart system will dynamically configure the appropriate technologies, providers and approaches, such as Do-It-Yourself (DIY) as well as concierge services, to deliver the optimal level of solution to the user.<br/><br/>This project is a collaboration between Purdue University (Mechanical, Electrical and Computer Engineering, and Business School) and primary implementation partner gesture interface technology ZeroUI (San Jose, CA, small business). Secondary partners for test-beds include a museum (Imagination Station, Lafayette, IN, non-profit) and gifted education research institute (GERI, Purdue University). Other partners include augmented reality - Meta (Portola Valley, CA, medium business); and depth sensing camera - Leap (San Francisco, CA, medium business)."
"1639792","EAGER: Asynchronous Event Models for State-Topology Co-Evolution of Temporal Networks","IIS","Info Integration & Informatics","07/15/2016","11/30/2018","Duen Horng Chau","GA","Georgia Tech Research Corporation","Standard Grant","Wei Ding","06/30/2019","$200,000.00","Le Song","polo@gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7364","7364, 7916","$0.00","The purpose of this research project is to develop probabilistic models and the related machine learning algorithms for modeling network evolution and dynamics. The research lays theoretic foundations and provides practical tools for scientists to control networks in order to achieve desirable outcomes. Although the research is widely applicable, the research team primarily considers two application areas: social networks and P2P microfinance. In social networks, this project brings practical values to the Internet industry by better understanding and modeling of user behaviors and their impacts on social ties and social group formation. For P2P microfinance, this project has the potential to better engage not-for-profit lenders and thus to help small business in developing countries. Furthermore, the research provides materials and contents for both undergraduate and graduate education and helps students develop interdisciplinary mindsets and tools needed to tackle real-world problems.  <br/> <br/>This proposed research aims to develop machine learning theory and algorithms for networked asynchronous and interdependent event streams arising from modern applications. The researchers especially emphasize methodology that can handle temporal networks when the underlying network structures are undergoing substantial changes. One major theme of the proposal is the modeling of the interplay between network node dynamics and network topology dynamics, or network co-evolution. The researchers propose a novel framework based on multivariate point processes for modeling and analyzing event data. The methods significantly expand the application area of conventional machine learning techniques. One example is to answer the question ``who will do what and when'', which is critical to event sequence modeling in network data analysis where traditional machine learning algorithms are difficult to apply."
"1609394","RET Site:  Physically and Biologically Inspired Computational Models and Systems","CNS","RES EXP FOR TEACHERS(RET)-SITE","02/15/2016","02/25/2016","Michael Niemier","IN","University of Notre Dame","Standard Grant","Harriet Taylor","01/31/2021","$597,047.00","Wolfgang Porod","mniemier@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","CSE","1359","1359","$0.00","This award establishes a new Research Experiences for Teachers (RET) Site at Notre Dame University.  The site focuses on engaging local high school teachers in summer research projects that explore computational models inspired by physical and biological systems.  The RET Site will develop a strong partnership between Notre Dame and schools in the greater South Bend area, including public, private, and parochial schools.  The site will recruit cohorts of high school teachers in the areas of Science, Technology, Engineering and Mathematics (STEM) who will work on compelling research projects with faculty mentors and research teams in the summer and then will implement related inquiry-based learning modules and activities in their classrooms during the academic year. The interdisciplinary nature of the research theme will provide a fertile ground for developing creative and appealing high school lessons and teaching activities in biology, physics, math, and other engineering disciplines that align with state and national curricular standards. <br/><br/>RET Site teachers will attend a 7-week summer institute to participate in cutting-edge research projects with mentoring from engineering faculty who lead current research programs in the three areas of emphasis.  The first area includes deep learning algorithms that solve problems such as language understanding and a behavior prediction most commonly using deep convolutional neural networks.   These data-hungry and computationally intensive methods tend to produce models that are far more accurate than their conventional counterparts.  The second thread involves non-Boolean/non-von Neumann neuromorphic hardware that processes data in an analog fashion. Systems that function like this could be inspired by the human retina which operates in a continuous fashion and processes continuous input flows with real spatial-temporal dynamics.  The third thread encompasses computation models that are directly inspired by physics.  This includes areas such as optically inspired computing models involving elements such as spin waves, magnetic lenses, and coupled oscillators to process information.  These projects will provide true in-depth learning about computation, waves, and optics which are fundamental to the high school physics curriculum. Working with PIs and faculty mentors, teachers will develop innovative, standards-compliant curriculum modules and participate in a number of professional development activities. The teacher-created modules and lessons will be disseminated local through a RET handbook and on the Notre Dame RET website as well as at the annual workshop and disseminated through TeachEngineering.org, a nationally recognized repository for searchable, standards-based engineering curricula.   Extensive follow-up activities are planned throughout the academic year to ensure the translation of lab experiences into classroom practice, and to foster and strengthen long-term partnership between engineering faculty and the local school districts. A third-party professional program evaluator will track and evaluate the program and provide feedback for improvement."
"1604984","Accelerating Multimetallic Catalyst Design for Electrochemical CO2 Reduction using Quantum Chemical Modeling and Machine Learning","CBET","Catalysis, Chemical Catalysis","07/01/2016","06/20/2019","Hongliang Xin","VA","Virginia Polytechnic Institute and State University","Standard Grant","Robert McCabe","06/30/2020","$398,942.00","Luke Achenie","hxin@vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","ENG","1401, 6884","8396, 8398, 8399, 8607, 8650, 9251","$0.00","1604984<br/>Xin, Hongliang<br/><br/>The proposed work is a computational study aimed at identifying novel multimetallic nanomaterials for the efficient electrochemical conversion of carbon dioxide (CO2) to value-added chemicals and fuels. This has the dual benefit of reducing the emissions of the greenhouse gas CO2 and moving closer to a sustainable energy future based on a closed loop carbon cycle fueled by a combination of solar energy and electrochemical conversion processes. <br/><br/>Prior research has demonstrated that copper (Cu) nanocubes exhibit remarkable selectivity towards carbon-carbon bond formation, but with electrical efficiency too low to be commercially viable. The study is based on the hypothesis that multimetallic nanocubes consisting of precisely mixed, low-cost metals can convert CO2 to useful chemicals and fuels at higher efficiency and selectivity than the Cu nanocubes alone. The researchers bring together expertise in density functional theory calculations and ab initio molecular dynamics - aided by advanced machine-learning algorithms - to predict materials combinations that lower the over-potential for electrochemical reduction of CO2 to ethylene and ethanol. The research is based on a three-step approach that first unravels the active site and reaction mechanism of CO2 reduction on Cu nanocubes, then creates predictive models linking nanoparticle composition and structure to the surface reactivity by machine-learning models, and lastly, develops an integrated framework for accelerating catalyst discovery. The broader impact of the work will be enhanced through educational outreach activities and open-source access to the tools developed during the course of the project."
"1643056","EAGER: Application-driven Data Precision Selection Methods","CCF","Software & Hardware Foundation","08/01/2016","07/21/2016","Ganesh Gopalakrishnan","UT","University of Utah","Standard Grant","Almadena Chtchelkanova","07/31/2018","$299,970.00","Mary Hall, Zvonimir Rakamaric, Hari Sundar, Vivek Srikumar","ganesh@cs.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","7798","7916, 7942, 8206","$0.00","Numerical algorithms used in Cyber Physical Systems, decision-making systems, financial processing, and other HPC applications that use real numbers are prone to introduce computational errors because of a well-known reason: real numbers do not exist in computers, and we must use floating-point data types to approximate such computations. As data movement costs energy, the lowest precision of floating-point data must be allocated without compromising the computational integrity. This project implements methods to reduce the amount of energy consumed by numerical computations running on computing devices at all scales including supercomputers for scientific research all the  way to embedded and mobile devices finding uses in many walks of real life including medical devices and robots.  A key thrust of the work is to perform energy reduction through reduced transfers between computing units. The project studies how the number of bits used to represent data introduce errors in computations, and whether these errors affect the correctness of results.<br/><br/>The PIs propose to develop new formal methods tools to automatically estimate error bounds, develop auto-tuning compilers to carefully select precision, and build new superoptimizers to generate more efficient code. These new technologies will be applied to improve software in the domains of machine learning and high-performance computing. The PIs shall develop suitable criteria for errors in high performance computing systems and machine learning systems. They will develop tools that allocate precision optimally while staying within the bounds of acceptable answers. Their tools will be released to a community of researchers interested in working toward exascale computing, and deploying machine learning applications in safety-critical devices. This work represents a synergistic combination of PI skills ranging through high performance computing, machine learning, formal methods, and compiler technologies."
"1659949","From Research to Practice and Practice to Research in the Era of Cyber Physical Systems","CBET","Proc Sys, Reac Eng & Mol Therm","12/01/2016","11/16/2016","Erik Ydstie","PA","Carnegie-Mellon University","Standard Grant","Triantafillos Mountziaris","11/30/2017","$40,425.00","","ydstie@cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","ENG","1403","7556","$0.00","1659949 - Ydstie<br/><br/>This grant is to provide partial support for US-based young faculty and graduate students to attend the 2017 Foundations of Computer Aided Process Operations (FOCAPO) / Chemical Process Control (CPC) Conference that will be held in Tucson, Arizona, from January 8 to 12, 2017. The FOCAPO/CPC Conference has a focus on examining where the field of process control and operations is heading in relation to Big Data, Cyber Physical Systems, the Internet of Things, Optimization, and Machine Learning. The organizing committee, which consists of distinguished researchers with a broad range of skills in process systems engineering, will provide for a vibrant and diverse atmosphere that fosters exchange of ideas on research and education. It is anticipated that approximately 15 US-based young faculty and 30 graduate students will receive partial travel support to attend this conference.<br/><br/>Intellectual Merit:<br/>The continuing and rapid development of computer hardware and software systems enables advances in the areas of Cyber-Physical Systems, the Internet of Things, Big Data, and Data Analytics by integrating virtual and physical entities through information networks. Furthermore, ubiquitous computation, almost infinite data storage capabilities due to the migration from dedicated servers to the ?cloud?, and new algorithms for optimization and machine learning offer new opportunities for research and development in the area of process systems engineering and control<br/>The Conference will bring together researchers from academia and industry working in the area of process systems engineering.  The following topics will be explored:<br/>1. The Internet of Things, Big Data, Communication, Machine Learning and Large Scale Computations across the domains of control and process operations.<br/>2. Emerging technologies, areas of research and non-traditional application domains.<br/>3. Integration of strategic, tactical, and operational decisions through modeling, optimization, and control in dynamic systems with uncertainty and stochastic perturbations.<br/>4. Education and workforce development in a rapidly changing global environment.<br/>5. Future directions for industry implementation and academic research.<br/>The conference will provide a unique forum for academic researchers and industrial practitioners to assess the current status of research in this field and discuss future directions. <br/><br/>Broader Impacts:<br/>The US-based conference participants supported by this grant will have the opportunity to network with leading researchers in the field of Chemical Process Systems Engineering and Control, and to exchange ideas about the current status and future directions in research and education. They will benefit from state-of-the-art reviews and presentations by leading experts and by debating controversial points with their peers. The Conference will provide an opportunity for interaction and cooperation among industrial and academic researchers. This will be especially beneficial to the young researchers and could positively influence their professional development. The Conference will also include workshops on Model Predictive Control, Process Operations, and Machine Learning."
"1526234","RI: Small: Bounded Distortion Models for Articulated and Deformable Object Recognition","IIS","Robust Intelligence","01/01/2016","07/21/2016","David Jacobs","MD","University of Maryland College Park","Continuing Grant","Jie Yang","12/31/2020","$435,618.00","","djacobs@cs.umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","7495","7495, 7923","$0.00","This project develops technologies for understanding shapes and parts of a person or animal and how these relate to their surface appearance. By building models that capture the variations in shape and pose of humans and animals, it becomes possible to understand the way that a person or animal's appearance changes as its arms and legs move or its head turns.  These models can then be aligned with images, assisting in the recognition of figures and the determination of their pose. Understanding human pose and activity is a fundamental problem in computer vision with a host of interesting applications in surveillance, video retrieval, and automated video annotation. Automated systems that can identify the species of animals can form the basis for automated field guides that can be used in education and studies of biodiversity. <br/><br/>This research develops new algorithms for matching image features and registering 3D models with bounded distortion mappings. The research team models people and animals using a skeleton capturing their articulations, along with a deformable skin model and an appearance model encoded by classifiers that can identify body parts of an animal. Given an image, the system computes an optimal bounded distortion transformation to register the model with the image. The system identifies both the pose and shape change of the person or animal with respect to the model and provides a way to rank possible detections of the model. The research team explores the problem of identifying the species of animals. The research team further applies algorithms to determine the pose of humans in images."
"1618648","AF:III: small: Convex optimization for protein-protein interaction network alignment","CCF","Algorithmic Foundations","07/01/2016","06/28/2016","Jinbo Xu","IL","Toyota Technological Institute at Chicago","Standard Grant","Mitra Basu","06/30/2020","$299,994.00","Qixing Huang","j3xu@ttic.edu","6045 S Kenwood Ave","Chicago","IL","606372803","7738340409","CSE","7796","7923, 7931","$0.00","High-throughput experimental techniques have been producing a large amount of protein-protein interaction (PPI) data. Comparative analysis (e.g., alignment) of PPI networks greatly benefits the understanding of evolutionary relationship among species, helps identify functional modules and provides information for protein function annotations. The research goal of this proposal is to study optimization methods that can align PPI networks much more accurately than existing methods. This proposal will apply several elegant and powerful optimization techniques to understand the mathematical structure of the problem and develop efficient alignment algorithms. This proposal will also develop software implementing the proposed algorithms. <br/><br/>The proposed algorithms will be implemented as both a standalone program and Cytoscape plugin so that they can be easily used by biologists. The resultant software and plugin shall benefit a broad range of biological and biomedical applications, such as protein functional annotation, understanding of disease processes, design of novel diagnostics and drugs, and precision medicine. The research results will be disseminated to the optimization, computer vision/graphics and biology communities through a variety of venues. The source code will be released so that it can be useful to other network analysis researchers who want to adapt the code for their own research projects and to other optimization method researchers who want to work on biological network analysis. This project will train a few PhD students and summer interns, who will receive training in the intersection of optimization techniques, network biology and programming. Undergraduate and underrepresented students will be recruited through our summer intern program, CRA-W and collaborators. The research results will be integrated into course materials and used in an Illinois online bioinformatics program that has trained many underrepresented students. <br/><br/>This proposal will study a novel convex optimization algorithm for the alignment of two or multiple PPI networks. This convex method distinguishes itself from the widely-used seed-and-extension or progressive alignment strategy in that it simultaneously aligns all the input networks and proteins while the latter methods use a greedy strategy to build an alignment. A greedy strategy may introduce alignment errors at an early stage that cannot be fixed later, but this convex method can avoid this. Due to its simultaneous alignment strategy, this convex method shall detect many more proteins that are functionally conserved across all input PPI networks than existing methods and produce more accurate pairwise alignments of multiple networks. This proposal will also study a few methods to speed up the proposed convex alignment method, by making use of special topology properties of PPI networks and exploring low-rank representation of proteins. Finally, this proposal will implement the proposed algorithms as a standalone software package and Cytoscape plugin to greatly facilitate the application of comparative network analysis to biological and biomedical science discovery."
"1554178","CAREER:Open-Source Data Analytics for Distribution Systems Management and Operations","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","02/01/2016","01/06/2016","Ram Rajagopal","CA","Stanford University","Standard Grant","Radhakisan Baheti","01/31/2021","$500,000.00","","ramr@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","ENG","7607","1045, 155E","$0.00","Distribution system operations (DSO) are designed to maintain reliability in the presence of predictable variability. Future distribution systems will operate in a dramatically different environment with deep penetration of distributed energy resources (e.g. solar, EVs, storage, smart loads) and widespread adoption of novel devices for network management resulting in increased variability. Unless DSO can be adapted to these conditions, performance and revenues of future utilities will be severely impaired. How to adapt Future systems will generate a wealth of data from consumers, line sensors and network equipment. Utilizing this data for learning, prediction and resource coordination is challenging and not fully understood. This proposal seeks to connect ""bits to watts"" utilizing modern data analytics in order to enable scalable and cost effective DSO for future distribution networks. In particular the proposal explores new approaches in machine learning, optimization and behavior learning and their applications in power systems. The methods will be implemented in a software platform: Visualization and Insight for Demand Operations and Management (VISDOM). The research component of the proposal will enable emissions reductions and massive scaling of the management of behind the meter resources. It contributes to the budding smart grid data analytics industry expected to reach a $6 billion market size by 2020. The education component of the proposal will create a novel curriculum and online education in data thinking to prepare the data analytics workforce of the future. <br/><br/>The project will make use of large spatial and temporal data sets from industry and utilities to explore new approaches in machine learning, stochastic control & optimization and behavioral economics to address problems in power systems. The central problems that will be addressed are: (i) Build an adaptive consumer behavior learning framework that scales to large numbers of consumers; (ii) Investigate probabilistic demand forecasting and pricing methods at multiple scales ranging from individual residential consumers to communities; (iii) Develop a novel network reconstruction and monitoring framework to learn the power distribution network from data; (iv) Create data and simulation driven placement and coordination mechanisms for residential demand-side resources; and (v) Utilize an interactive platform that engages consumers in real-time to develop novel randomized trial approaches and apply it to innovative behavioral programs. Impacts such as increasing the value of consumer demand flexibility by more than 50% are expected. The resulting methods will be made available in open-source in the VISDOM platform.  VISDOM can support a thriving community of academics and industry partners that experiment with demand side management. Currently, every project develops non-transparent and limited analysis mechanisms that consume time and resources. More broadly, the time-series data based approaches developed in this proposal are applicable to other fields such as marketing, healthcare and e-commerce. The education component will advance concepts from data thinking into power systems. The proposed curriculum includes a new hands-on course in data analytics for energy systems for undergraduate and masters students; online adult education courses directed at utility professionals and a broader audience and a K12 experimental practicum prepared with high school teachers visiting the PI's lab in a summer program. In addition, a smart grid seminar involving distinguished speakers from academia and industry will be supported and made available online."
"1562438","Collaborative Research: Living Building Information Model (BIM): A Layered Approach for Automatic and Continuous Built Environment Model Update","CMMI","GOALI-Grnt Opp Acad Lia wIndus, CIS-Civil Infrastructure Syst","09/01/2016","04/30/2020","Fernanda Leite","TX","University of Texas at Austin","Standard Grant","Yueyue Fan","08/31/2021","$276,000.00","","fernanda.leite@austin.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","ENG","1504, 1631","019Z, 029E, 036E, 039E, 116E, 9102, 9178, 9231, 9251","$0.00","Infrastructure and buildings are designed to have long life cycles - on the order of decades. Many buildings in the world are still in operation after centuries amid numerous renovation efforts. This long operational phase represents the majority of a building's lifecycle, yet the information regarding maintenance and renovation is rarely kept up to date. Building Information Models (BIMs) can alleviate this data shortage by centrally storing this data. However, even with a BIM, building updates are not kept due to the difficulty of continuous manual updates over a building's lifetime. This research will create a method to automatically update a BIM by exploiting recent advancements in machine vision. This automation process can systematically and continuously analyze the built environment, detecting changes from a previous assessment. It can distill and update the critical building information with minimal human error and effort. This research will result in a fundamental change in construction record keeping and benefit building operators by enabling maintenance and renovation activities to more easily be planned throughout a building's lifetime. The findings of this work will be integrated into undergraduate and graduate educational modules. Videos created for YouTube will also be developed to attract high school and underrepresented persons to a career in civil engineering.<br/><br/>The project will generate contextual-data relationships for use by an active illumination range camera-based, machine-vision system for automatically updating a BIM database with construction changes and leveraging metadata distilled from BIM object-oriented database models, the Computer-Aided Facilities Management (CAFM) database, and expert knowledge input by human operators. The project will tackle several intellectual challenges.  A primary challenge resides in the machine vision component of the work, which will need to provide additional meta-data beyond a 3D geometric model of an object, pushing the boundary of machine vision for the context of civil engineering systems. Another challenge is to extend data modeling capabilities for the built environment. Specifically, capabilities to translate meta-data from machine vision to identify and obtain meaningful contextual data that is specific for the objects will be created. A specific logical component, the Contextual Decision Maker (CDM), will also be developed to merge meta-data from multiple data sources. Finally, the entire system will be tested in an indoor renovation project to provide a realistic machine-learning process that will grow more robust over time."
"1562515","Collaborative Research: Living Building Information Model (BIM): A Layered Approach for Automatic and Continuous Built Environment Model Update","CMMI","CIS-Civil Infrastructure Syst","09/01/2016","02/29/2016","Liang Chung Lo","PA","Drexel University","Standard Grant","Yueyue Fan","06/30/2020","$199,924.00","Ko Nishino","james.lo@drexel.edu","1505 Race St, 10th Floor","Philadelphia","PA","191021119","2158955849","ENG","1631","029E, 036E, 039E","$0.00","Infrastructure and buildings are designed to have long life cycles - on the order of decades. Many buildings in the world are still in operation after centuries amid numerous renovation efforts. This long operational phase represents the majority of a building's lifecycle, yet the information regarding maintenance and renovation is rarely kept up to date. Building Information Models (BIMs) can alleviate this data shortage by centrally storing this data. However, even with a BIM, building updates are not kept due to the difficulty of continuous manual updates over a building's lifetime. This project will create a method to automatically update a BIM by exploiting recent advancements in machine vision. This automation process can systematically and continuously analyze the built environment, detecting changes from a previous assessment. It can distill and update the critical building information with minimal human error and effort. This research will result in a fundamental change in construction record keeping and benefit building operators by enabling maintenance and renovation activities to more easily be planned throughout a building's lifetime. The findings of this work will be integrated into undergraduate and graduate educational modules. Videos created for YouTube will also be developed to attract high school and underrepresented persons to a career in civil engineering.<br/><br/>The project will generate contextual-data relationships for use by an active illumination range camera-based, machine-vision system for automatically updating a BIM database with construction changes and leveraging metadata distilled from BIM object-oriented database models, the Computer-Aided Facilities Management (CAFM) database, and expert knowledge input by human operators. The project will tackle several intellectual challenges.  A primary challenge resides in the machine vision component of the work, which will need to provide additional meta-data beyond a 3D geometric model of an object, pushing the boundary of machine vision for the context of civil engineering systems. Another challenge is to extend data modeling capabilities for the built environment. Specifically, capabilities to translate meta-data from machine vision to identify and obtain meaningful contextual data that is specific for the objects will be created. A specific logical component, the Contextual Decision Maker (CDM), will also be developed to merge meta-data from multiple data sources. Finally, the entire system will be tested in an indoor renovation project to provide a realistic machine-learning process that will grow more robust over time."
"1623251","Workshop on Scenarios for Brain-Inspired Cognitive Assistants, San Jose CA, May 12-13th.","ECCS","COMMS, CIRCUITS & SENS SYS, ENG NNI SPECIAL STUDIES","07/15/2016","07/19/2016","Jonathan Candelaria","NC","Semiconductor Research Corporation","Standard Grant","Dominique M. Dagenais","03/31/2017","$39,493.00","","Jonathan.Candelaria@src.org","4819 Emperor Blvd.","Durham","NC","277035420","9199419400","ENG","7564, 7681","7556","$0.00","Proposal 1623251<br/><br/>Title: Workshop on Scenario for Brain-Inspired Cognitive Assistants<br/><br/>PI: Jonathan Candelaria<br/> <br/>Institution:  Semiconductor Research Corporation<br/><br/>Date & Location:  San Jose CA, May 12-13th.<br/>Workshop Goal: Explore research opportunities towards a goal of creating cognitive intelligent assistance to improve human productivity and overall quality of life. <br/><br/>Non-Technical:<br/><br/>Today we are at the cusp of a new era of computing. In the first era, machines were designed and built to greatly accelerate the performance of basic arithmetic calculations compared to human ""computers"" In the second era, these machines, by then called ""computers"" themselves, were programmed to solve highly complex problem sets, as well as enable global communication networks. We are now beginning to explore the use of machines to expand upon and augment human cognitive capabilities. We envision ""machine intelligence"" acting as an interface between humans and their environments, providing insight and guidance for problems that cannot be handled by the unaided mind or by computers alone. How to optimize this collaborative interaction between humans and these ""intelligent"" machines is an open research question. In order to create the foundation for future intelligent systems that can most effectively and efficiently assist individuals, businesses and society at large, it is essential that this research question be addressed.<br/><br/>Technical:<br/><br/>This workshop is designed to discuss and study progress toward developing ""intelligent"" machines. It can serve as ""helpful assistants"" to improve human productivity and overall quality of life. The workshop on ""Brain-Inspired Cognitive Assistance""<br/>is proposed for San Jose CA, May 12-13th.  The goal of this workshop is to formulate scenarios for developing novel architectures for intelligent, energy efficient, brain-inspired perception, computing, decision and management in the future 10-15 years. Various concepts show promise, including cellular automaton assembly, static and dynamic neural networking, deep machine learning, etc. as well as a wide variety of social robotics principles, genetic software, bio-inspired convergence of interaction principles, predictive cognition, natural language and complex pattern recognition algorithms and methodologies."
"1543958","BREAD PHENO:  High-Throughput Phenotyping with Smart Phones. #phenoApps","IOS","Plant Genome Research Resource, BM Gates Foundation","08/01/2016","07/10/2018","Jesse Poland","KS","Kansas State University","Continuing Grant","Diane Okamuro","07/31/2020","$1,632,824.00","Ernest Mwebaze, Mitchell Neilsen, Michael Gore, Bruce Gooch","jpoland@ksu.edu","2 FAIRCHILD HALL","Manhattan","KS","665061100","7855326804","BIO","7577, 8288","1329, 7577, 8288, 9109, 9150, 9179, BIOT","$0.00","Food and nutritional security will be a grand challenge in the coming decades.  The global population is expected to increase to over 9 billion and food demand will grow by more than 50%.  Currently, there are 2 billion people worldwide living in poverty, mostly relying on subsistence agriculture in developing countries.  While poverty and food insecurity is a complex issue, the development of improved climate-resilient, high yielding and nutritious plant varieties is a critical part of improving food security, increasing income and economic welfare.  To address this challenge, innovative approaches are needed to speed up the development of improved plant varieties.  In plant breeding and genetics, precise measurements of plant characteristics are needed to accurately determine the effect of important genes and to identify and select the most promising candidate plant varieties.  There has been limited technology development in this area, particularly for traits measured in field trials where most measurements are still taken and recorded by hand.  This project will develop mobile applications (apps) for measuring plant traits that can be deployed on inexpensive and readily available mobile devices.  Initial testing and deployment through collaboration with cassava and wheat breeders will enable rapid dissemination and broad usability.  Middle-school and high-school students will also be engaged to test and use the apps to explore plant growth and measure plant traits.  Equipping thousands of plant breeders around the world with tools for rapid measurement and analysis of important plant traits will provide the foundation for accelerated development of improved plant varieties that will ultimately result in increased productivity, food security, nutrition and income of smallholder farmers and their families in developing countries. <br/><br/>Over the past decade, the availability of genomic data has exploded while the methods to collect phenotypes have made minimal advancements.  This has led to a dramatic imbalance in data sets connecting genotype to phenotype and highlights phenotyping as the remaining major bottleneck in plant breeding programs.  This project will advance the field of 3D graphics and modeling, data mining and deep learning through integration of simultaneous ground truth phenotypic measurements and imaging with mobile technology.  Building on the success of Field Book (www.wheatgenetics.org/field-book), user-friendly mobile apps for field-based high-throughput phenotyping (HTP) will be developed and deployed.  This project will converge novel advances in image processing and machine vision to deliver mobile apps through established breeder networks.  Novel image analysis algorithms will be developed to model and extract plant phenotypes.  A robust development pipeline will be assisted by 1) real-time field testing through breeding collaborators around the world and 2) middle-school and high-school students using the apps to explore plant growth and quantitative differences under genetic control.  To ensure both immediate, broad deployment and functionality on a diverse set of crops, breeder networks for cassava and wheat will be engaged, providing a diverse set of target plant phenotypes, environments, breeding programs and working cultures. By combining data from research programs with ground truth breeder knowledge, this project will lay the foundation for collecting training sets that can subsequently be used to extract and quantify complex phenotypes using deep learning. Open-source apps for smartphones and tablets will consist of both software and documentation so that users will be able to understand how to use the apps. Apps will be distributed through online app stores (Windows Store, iTunes App Store, Google Play), through project websites, and via collaborative plant breeding networks. The resulting source code will be hosted in a public GitHub repository with a GNU General Public License (GPL) open-source license."
"1633259","BIGDATA: Collaborative Research: IA: BirdVox: Automating Acoustic Monitoring of Migrating Bird Species","IIS","Big Data Science &Engineering","10/01/2016","09/13/2016","Juan Bello","NY","New York University","Standard Grant","Sylvia Spengler","09/30/2020","$612,383.00","","jpbello@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","8083","7433, 8083, 9102","$0.00","Current bioacoustic monitoring of natural environments requires processing by humans to extract information content from recordings. Thus human processing creates a fundamental bottleneck in which data collection far outpaces capabilities to extract relevant and desired information. Bioacoustic research on automatic species classification in natural environments can be broadly divided into two groups: distinguishing a predefined set of known species from audio clips and extracting species as events that occur in a continuous audio stream. Both classification techniques have their specific problems--many of the data used distinguishing predefined species are recorded under ""studio"" conditions and not extensible to natural conditions, while processing of continuous audio streams generate many false positives. <br/><br/>To overcome these challenges we will take a multi-tiered approach:  Analyzing a data set consisting of full-night recordings from 10 recording units over 100 nights. Building a web-enabled software to engage citizen scientists to identify the flight calls, providing us with a large and extensive model training dataset.  Developing novel convolutional deep-learning networks, which are well suited for analysis of complex auditory scenes.  Visualizing patterns detected and classified flight calls in space and time to produce novel information about the bird migration.  Comparing model-generated acoustic data with radar, video, and direct visual citizen science datasets to produce the most comprehensive accounts of nocturnal bird migration possible.  The combination of domain knowledge in bird vocalizations, engaging citizen scientists to allow development of large well annotated training datasets, and taking a novel deep-learning approach, will finally resolve the machine classification of acoustic signals in natural environments."
"1621828","SBIR Phase I:  Innovative visual search and similarity for decor, apparel, and style","IIP","SMALL BUSINESS PHASE I","07/01/2016","06/22/2016","Sean Bell","NY","Grokstyle LLC","Standard Grant","Peter Atherton","06/30/2017","$225,000.00","","founders@grokstyle.com","29 WEDGEWOOD DR","Ithaca","NY","148501064","6072806026","ENG","5371","5371, 8032","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is to develop and commercialize visual search for fine-grained recognition of products and style in interior decor and apparel.  The technology will help the broader public find items that may be difficult to search for using traditional text-based search.  In many markets (home decor, fashion, etc.), customers seek products that have unique visual appearances that cannot easily be expressed with a text-based search. This project will develop visual search tools for the home decor and apparel markets. <br/><br/>This Small Business Innovation Research (SBIR) Phase I project will develop software based on deep learning for product and apparel recognition and style recognition. Our prior prototype uses deep learning to recognize specific products from ""regular"" photographs taken by customers, where the challenge is that these regular photos of products can have many different backgrounds, sizes, orientations, or lighting when compared to the iconic product image, and the product could be significantly occluded by clutter in the scene.  The goal of this project is to generalize the work to achieve broad applicability through four major objectives: Generalizing the settings and product categorization and taxonomy to support a broad range of customers and product types (Objective 1); semi-automatic detection of products in scene images to scale to large photo collections (Objective 2); refining the trained models for fine-grained matches to meet customer needs (Objective 3); and deploying the system live to companies (Objective 4)."
"1723379","AitF:  FULL: Collaborative Research:   PEARL: Perceptual Adaptive Representation Learning in the Wild","CCF","Algorithms in the Field","09/01/2016","03/14/2017","Kate Saenko","MA","Trustees of Boston University","Standard Grant","A. Funda Ergun","08/31/2020","$173,754.00","","saenko@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","7239","012Z","$0.00","Vast amounts of digitized images and videos are now commonly available, and the advent of search engines has further facilitated their access. This has created an exceptional opportunity for the application of machine learning techniques to model human visual perception. However, the data often does not conform to the core assumption of machine learning that training and test images are drawn from exactly the same distribution, or ""domain."" In practice, the training and test distributions are often somewhat dissimilar, and distributions may even drift with time. For example, a ""dog"" detector trained on Flickr may be tested on images from a wearable camera, where dogs are seen in different viewpoints and lighting conditions. The problem of compensating for these changes--the domain adaptation problem--must therefore be addressed both in theory and in practice for algorithms to be effective. This problem is not just a second-order effect and its solution does not constitute a small increase in performance.  Ignoring it can lead to dramatically poor results for algorithms ""in the field.""<br/><br/>This project will develop a core suite of theory and algorithms for PErceptual Adaptive Representation Learning (PEARL), which, when given a new task domain, and previous experience with related tasks and domains, will provide a learning architecture likely to achieve optimal generalization on the new task. We expect PEARL to have a significant impact on the research community by providing a much-needed theoretical and computational framework that takes steps toward unifying the subfields of domain adaptation theory and domain adaptation practice. Our theoretical and practical advancements will impact many application areas by allowing the use of pre-trained perceptual models (visual and otherwise) in new situations and across space and time. For example, in mobile technology and robotics, PEARL will help personal assistants and robots better adapt their perceptual interfaces to individual users and particular situated environments.  At the core of this project are three main research thrusts: 1) making theoretical advances for domain adaptation by developing generalized discrepancy distance minimization; 2) using the theoretical guarantees of generalized discrepancy distance to develop algorithms for key adaptation scenarios of deep perceptual representation learning, domain adaptation with active learning, and time-dependent adaptation; 3) advancing the theory and developing algorithms for the multiple-source adaptation scenario. In addition to our core aims, we plan to implement our algorithms within a scalable open-source framework, and evaluate our algorithms on large-scale visual data sets."
"1634676","Learning Algorithms for Dynamic Inventory and Pricing Optimization Problems","CMMI","OE Operations Engineering","08/01/2016","07/28/2016","Xiuli Chao","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Georgia-Ann Klutke","07/31/2020","$300,000.00","","xchao@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","ENG","006Y","071E, 072E, 073E, 077E, 078E","$0.00","Learning algorithms aim to solve dynamic optimization problems in which the decision maker has limited or no prior information about either a part of or the entire system structure. Indeed, in many applications, the system is so complex that it may not be possible to lay out an exact theoretical model with all system parameters known in advance. In these settings, the decision maker needs to learn such information during the decision making process, e.g., by extracting information from the collected data, to design algorithms for improved system performance. This view of optimization as a dynamic learning process has become prominent in recent years and has led to some promising results. This research will develop efficient data-driven learning algorithms for dynamic operations optimization problems in supply chain management. It will be accomplished by incorporating and extending ideas and techniques from machine learning and stochastic optimization, and the effectiveness of the algorithms will be measured by regret, defined as its average loss (increment) in profit (cost) per unit time compared with a clairvoyant who has complete information about the underlying system structure. This project involves several disciplines such as manufacturing, computing, operations research, and business analytics, and the multidisciplinary approach will encourage participation from under-represented groups and positively impact graduate and undergraduate education.<br/><br/> <br/>Efficient data-driven algorithms will be developed for several classes of dynamic operations optimization problems, including multi-product dynamic inventory control with stockout substitutions, multi-product pricing and inventory control under customer choice models, inventory and pricing optimization under changing and seasonal environments, dynamic optimization in competitive environments, dynamic inventory control and pricing with reference point effect, and dynamic joint operations and marketing decision making. The research integrates cutting-edge knowledge and ideas from various areas, such as statistics, game theory, machine learning, operations research, and behavioral sciences, and it will lead to efficient learning algorithms that perform well both theoretically and empirically. With the increasing availability of data in companies, the research from this project will help them better utilize data for intelligent pricing and inventory decisions, and increase revenue and minimize cost."
"1607260","Rigidity Phenomena in Geometry and Dynamics","DMS","GEOMETRIC ANALYSIS","09/01/2016","08/23/2016","Ralf Spatzier","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Christopher Stark","08/31/2020","$322,000.00","","spatzier@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","MPS","1265","9251","$0.00","Dynamical systems and ergodic theory investigate the evolution of a physical or mathematical system over time, such as turbulence in a fluid flow or changing planetary systems. New ideas and concepts such as information, entropy, chaos and fractals have changed our understanding of the world. Dynamics and ergodic theory provide excellent mathematical tools, and have a strong impact on the sciences and engineering. Symbolic dynamics for example has been instrumental in developing efficient and safe codes for computer science. Tools and ideas from smooth dynamics are used as far afield as cell biology and meteorology. Geometry is a highly developed and ancient field in mathematics of amazing vigor.  It studies curves, surfaces and their higher dimensional analogues, their shapes, shortest paths, and maps between such spaces.  Differential geometry had its roots in cartography, starting with Gauss in the nineteenth century. It is closely linked with physics and other sciences and applied areas such as computer vision. Geometry and dynamics are closely related. Indeed, important dynamical systems come from geometry, and vice versa geometry provides tools to study dynamical systems. One main goal of this project studies when two dynamical systems commute, i.e. when one system is unaffected by the changes brought on by the other. Alternatively, these are systems with unexpected symmetries Important examples of such systems arise from geometry when the space contains many flat subspaces.  Group theory finally enters both dynamics and geometry by studying the group of symmetries of a geometry or dynamical situation, or by investigating the dynamical and geometric behavior of the group of symmetries acting on a space.<br/><br/>This project centers on problems between dynamical systems, group theory and geometry. There are two main goals: First, establish exponential mixing properties for several different systems in dynamics, in particular frame flows from Riemannian geometry and solenoids coming from noninvertible systems. The principal investigator (PI) will draw tools from dynamics, geometry and number theory to accomplish these goals. Second, prove rigidity properties in geometry and in dynamical systems, in particular when the system and spaces in question are ""higher rank"", e.g. when spaces have flat subspaces or the dynamics has nontrivially commuting elements. Such systems appear naturally in seemingly quite separate areas, for example in number theory or in studying the spectrum of the Laplacian. The investigator will work on rigidity properties of actions of higher rank abelian and semi-simple Lie groups and their lattices striving to classify such systems under suitable geometric or dynamical hypotheses.  The PI will employ tools from geometry, dynamics, Lie groups, and specifically exponential mixing properties.  The PI will also investigate discrete faithful representations of hyperbolic groups in p-adic Lie groups, equilibrium states for partially hyperbolic dynamical systems and spherical higher rank in Riemannian geometry."
"1620016","Multiscale Weak Galerkin Methods for Flows in Highly Heterogeneous Media","DMS","COMPUTATIONAL MATHEMATICS","08/01/2016","07/30/2018","Xiu Ye","AR","University of Arkansas Little Rock","Continuing Grant","Leland Jameson","07/31/2021","$216,522.00","","xxye@ualr.edu","2801 South University","Little Rock","AR","722041000","5015698474","MPS","1271","8396, 8611, 9150, 9263","$0.00","Fluid flow in porous media is important in many areas, including oil extraction and recovery, environmental protection, energy conservation, and the design and operation of fuel cells, solar cells, and batteries. Development of accurate, efficient, and reliable numerical schemes to simulate such fluid flow has received considerable attention in mathematics and engineering communities over the past decade.  However, mathematical modeling and numerical simulation of fluid flows in heterogeneous media and realistic settings remain a challenge. Much of the difficulty in porous media flow simulations is due to the involvement of different length scales, from macroscopic scale to microscopic scale. This research project aims to develop accurate, efficient, and reliable numerical algorithms for flows in porous media.  Weak Galerkin finite element methods (WGFEMs) will be developed for flows in highly heterogeneous domains, porous media, and complex flows in heterogeneous media. The methods under development are anticipated to significantly advance the utility of numerical analysis for realistic scientific and engineering applications.  Graduate students are involved in the project.<br/><br/>This project aims to develop new weak Galerkin (WG) finite element methods (FEMs) with excellent flexibility in element construction and mesh generation, suited to dealing with heterogeneous physical parameters. Additionally, it is envisioned that the new multiscale WGFEMs will be applicable in other fields, such as structural analysis, electromagnetic wave scattering, image processing, and computer vision. Collaboration with petroleum industry partners is planned in this research project."
"1617999","CIF: Small: Collaborative Research: Geometrical and Statistical Modeling of Space-Time symmetries for Human Action Analysis and Retraining","CCF","Comm & Information Foundations","07/15/2016","07/08/2016","Pavan Turaga","AZ","Arizona State University","Standard Grant","Phillip Regalia","06/30/2020","$280,000.00","","pavan.turaga@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","7797","7923, 7936","$0.00","This interdisciplinary research aims to advance current understanding and utilization of space-time symmetries in analyzing human movements, using fundamental tools from engineering, geometry, and statistics. Broader applications of this research include home or workplace-based self-reflection of daily activities, promotion of higher efficiency of human movements, and long-term management and/or prevention of movement disorders. While the need for comprehensively and statistically analyzing human kinematics is well chronicled, the current measures are often limited to simplistic quantities such as speeds and acceleration profiles of individual limbs. This project will focus on both spatial and temporal symmetries of limb movements, full body shapes, and complete dynamical actions, for assessment of movements ranging from daily activities to physiotherapeutic exercises.<br/> <br/>Symmetry has been used in the past, in clinical biomechanics, but in a limited way. This project will develop a comprehensive theory, built on fundamental tools from differential geometry and statistical analysis of geometric objects, to represent, quantify, analyze, and classify motions according to their level of symmetry. The specific forms of symmetry will include spatial reflection, temporal reflection, and space-time glide symmetries. This formulation will incorporate data from various sensing modalities and features, including point trajectories and stick figures from motion capture systems, to shape silhouettes and dynamic textures obtained from video sensors. The project outcomes also include the development of a real-time media-system for movement re-training and reflection of common actions, such as sitting to standing (STS). The proposal brings together a strong and inter-disciplinary team of researchers with expertise in computer vision and action recognition (Turaga), differential geometry and statistics (Srivastava), and somatics and kinesiology (Coleman)."
"1657939","CAREER: Transforming data analysis via new algorithms for feature extraction","CCF","Algorithmic Foundations","06/01/2016","06/29/2018","Luis Rademacher","CA","University of California-Davis","Continuing Grant","A. Funda Ergun","06/30/2021","$329,094.00","","lrademac@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","CSE","7796","1045, 7926","$0.00","Analysis and exploration of data, including classification, inference, and retrieval, are ubiquitous tasks in science and applied fields. Given any such task, a fundamental paradigm is the extraction of features that are relevant. In the design of algorithms for the analysis and exploration of data, feature extraction techniques act as basic building blocks or primitives that can be combined to model complex behavior. Some of the fundamental feature extraction tools include Principal Component Analysis (PCA), Independent Component Analysis (ICA), and half-space-based learning and classification. Data rarely satisfy the precise assumptions of these models and feature extraction tools, and combining these tools amplifies errors. This motivates the challenging task of designing new algorithms that are robust against noise and that can be combined as building blocks while keeping the error propagation under control.<br/><br/>The proposed work will:<br/>(1) Raise ICA from a very successful practical tool to an algorithmic primitive with strong theoretical guarantees and applicability to a rich family of problems beyond independence.<br/>(2) Find reasonable assumptions and algorithms that allow efficient learning of intersections of half-spaces.<br/>(3) Systematically study the following well-motivated refinement of PCA known as the subset selection problem. This refinement aims to select relevant features among the given features of the input data, unlike PCA, which creates new and possibly artificial features.<br/><br/>New feature extraction algorithms enhance the toolbox available to researchers in data-intensive fields such as biology, signal processing and computer vision. They also enable improved data analysis by practitioners in security, marketing, business and government processes, and essentially any field that involves the analysis of feature-rich data. The proposed work includes the implementation of the more practical algorithms.<br/><br/>Education and outreach aspects of this project include the mentoring of young researchers, the design of a new course for graduate and undergraduate students incorporating some of the PI's research, and the involvement of pre-college students and local communities into science and research."
"1645047","EAGER: Active Citizen Engagement to Enable Lifecycle Management of Infrastructure Systems","CMMI","Special Initiatives","09/01/2016","07/28/2016","Shirley Dyke","IN","Purdue University","Standard Grant","Robin Dillon-Merrill","08/31/2018","$100,000.00","","sdyke@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","ENG","1642","043Z, 1638, 7916, 9102","$0.00","Flaws and defects in structures evolve gradually over a structure's lifetime, and potential degradation must be evaluated periodically to make sound management decisions and set repair priorities. Human observation, still the predominant mechanism for such evaluation, is time-consuming and costly. Citizen science and crowdsourcing provide opportunities to collect large numbers of photos of certain structures, from many perspectives, at frequent intervals, and under many conditions. Such visual data reach across both time and space, enabling a detailed record of deterioration over time. This EArly-concept Grant for Exploratory Research (EAGER) project will exploit the latest knowledge in computer vision to automate many tasks related to lifecycle structural evaluation and management, reducing both lifecycle cost and risk. The methodology developed within this project will also launch new opportunities for structural engineers seeking to exploit data science to address a broad range of structural engineering problems. A streaming video demonstrating the methodology on our target structure will be developed for broad dissemination and student engagement. <br/><br/>Everyday images from citizens, not trained as engineers, are very different than the types of images than engineers capture. Large portions of these images have information irrelevant for engineering purposes and automated processing of these images would generate faulty conclusions. Furthermore, these images are collected from random locations and perspectives, and lack scale and orientation information. However, these barriers can be overcome. This project will incorporate essential knowledge about the structural evaluation process to enable the use of these images for engineering purposes. Geometric relationships between each of the query images and the model of the target structure will be computed by matching their local features. Automatic localization of each of these images with respect to the target structure will be performed, and relevant portion of the images, called the region of interest, will be extracted for structural evaluation by human or machine. Experimental validation will be performed using images collected from active engaged citizens through social media. A quantitative evaluation of the capabilities of the methodology will be performed, targeting especially vulnerable regions of a structure. This project will overcome the inherent challenges in using visual data from citizen scientists, facilitating a transformation in how we perform lifecycle structural evaluation."
"1600447","The Positive Grassmannian:  Applications and Generalizations","DMS","OFFICE OF MULTIDISCIPLINARY AC, Combinatorics","07/01/2016","06/14/2018","Lauren Williams","CA","University of California-Berkeley","Continuing Grant","Tomek Bartoszynski","06/30/2021","$376,965.00","","williams@math.harvard.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1253, 7970","1515","$0.00","This project explores structures that lie at the intersection of combinatorics, representation theory, statistical physics, and integrable systems, with potential significant impact on several fields, including applications to shallow water waves, translation in protein synthesis, and scattering amplitudes in supersymmetric Yang-Mills theory.  The central structures in the mathematics of the project are Grassmannians, which parameterize subspaces of vector spaces and are ubiquitous in mathematics, appearing variously as projective spaces in projective geometry, compact smooth manifolds in differential geometry, and as a scheme in algebraic geometry. Grassmannians play an important role for spatial recognition in computer vision, in coding and communication theory, in studying shallow water waves in physics, and in the computation of scattering amplitudes of subatomic particles. This project explores features of remarkably rich subsets of real Grassmannians called totally positive and totally non-negative Grassmannians. These structures constitute refinements and extensions of the classical theory of positive definite and positive semi-definite matrices and representation theoretic work in the context of Lie Theory, and their structure will be studied from topological, representation theoretic, and combinatorial points of view. This project also seeks to increase the visibility of women mathematicians via a series of lectures at the University of California, Berkeley given by distinguished women. <br/><br/>More concretely, this project concerns several interrelated questions surrounding the positive Grassmannian, Macdonald-Koornwinder polynomials, and the asymmetric exclusion process. In particular, the research will investigate: a new polytopal manifestation of mirror symmetry for flag varieties; the topology of the positive Grassmannian; the structure of soliton solutions to the Kadomtsev?Petviashvili equation coming from the Grassmannian; the combinatorics of the amplituhedron, a new generalization of the positive Grassmannian; and combinatorial formulas for Macdonald-Koornwinder polynomials and asymmetric simple exclusion process probabilities using rhombic tableaux."
"1615597","III: Small: Collaborative Research: Structured Methods for Multi-Task Learning","IIS","Info Integration & Informatics","08/01/2016","07/21/2016","Jiayu Zhou","MI","Michigan State University","Standard Grant","Wei Ding","07/31/2020","$250,050.00","","dearjiayu@gmail.com","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","CSE","7364","7364, 7923","$0.00","The ability of human to learn from and transfer knowledge across related learning tasks enables us to grasp complex concepts from only a few examples. For instance, a three-year old child is able to discriminate chairs from tables without having been exposed to hundreds of different examples. In contrast, computer learning programs typically require training on a large number of examples in order to achieve similar levels of recognition. This prompts the study of multi-task learning in which multiple related tasks are learned simultaneously, thereby facilitating inter-task knowledge transfer. However, most multi-task learning studies are restricted to problems with well-defined tasks and structures. This project aims at developing algorithms and tools (including open source software) to attack problems that are not traditionally treated, but can potentially be reformulated and solved more effectively by multi-task learning. This allows a broad class of challenging machine learning problems to benefit from multi-task learning techniques. This project also develops a new curriculum that incorporates the proposed research into the classroom. In addition, this project will allow the PIs to continue the ongoing efforts of actively recruiting and advising students from under-represented groups.<br/><br/>To achieve these goals, this project focuses on an innovative, integrated research and education plan that includes the following components: (1) providing principled guidelines for reformulating problems into the multi-task learning formalism; (2) developing robust and clustered multi-task learning models to identify and prevent false interactions among unrelated tasks; (3) developing sparsity-inducing multi-task learning models to capture richly structured task interactions; (4) developing high-order multi-task learning models to capture task relatedness from interactions between features; and (5) investigating computational algorithms and theoretical properties of multi-task learning. The outcome of this project includes the capabilities of reformulating diverse machine learning problems into the multi-task learning framework and providing radically new ways to attack challenging problems that cannot be solved effectively by traditional methods. The systematic study of multi-task learning in this project is expected to generate novel reformulations, structured mathematical models, efficient optimization algorithms, and principled theoretical analyses, which will lead to significant practical and theoretical advances in multi-task learning."
"1612867","Nonparametric estimation of integral curves and surfaces","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS","08/15/2016","07/06/2018","Lyudmila Sakhanenko","MI","Michigan State University","Standard Grant","Gabor Szekely","08/31/2019","$204,316.00","David Zhu","sakhanen@msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","MPS","1253, 1269","1515, 8091","$0.00","Integral curves are reasonable models for a variety of natural structures that arise in brain imaging and in atmospheric sciences. Natural structures in brain imaging include the axonal fibers connecting neurons. Natural structures in atmospheric sciences include isolines of barometric pressure, storm fronts, and jet streams. Generally, the data are points along the curves and are corrupted by noise. The goal of this research is statistical estimation of these curves. These researchers will investigate a detailed list of issues pertinent to statistical integral curve estimation, which will deepen and widen our understanding of the natural structures. The PIs will validate their statistical methodology by analyzing images on both diseased and healthy brains. The reliable statistical estimation of fibers will include the assessment of uncertainty in the images used for diagnostic procedures in diseases such as Alzheimer's disease, multiple sclerosis, and brain tumors. It will also enhance the imaging tools needed for planning imaging-guided neurosurgeries. In meteorology the correct estimation of these curves can enhance existing weather maps. Finally, the PIs will capitalize on the similarities between modeling curves and modeling surfaces and investigate the statistical estimation of surfaces, which serve as natural models for axonal fiber bundles in brain imaging and iso-surfaces in digitized images used by many sciences.<br/> <br/>Integral curves are solutions of differential equations where the governing vector or tensor fields are observed directly or indirectly and perturbed by noise. Several directions of their statistical analysis are planned, including a fully nonparametric diffusion function model, simultaneous confidence bands for integral curves, adaptive estimation, comparison of images of the same brain obtained via different procedures, model reduction based on tensor's order, and a unified approach to nonparametric estimation of manifolds, where integral curves and surfaces serve as particular cases. Specifically, the PIs plan to provide an end-user with enhanced images that contain not only estimated integral curves, but also simultaneous confidence bands that show the quality of the estimation in a uniform way, and moreover all the tuning parameters would be calculated from the data only. Upon completion of the proposed research, a practitioner would be armed with a procedure of systematic comparison of statistical properties of different tractography algorithms. Finally, the PIs plan to extend their methodology from 1D curves to 2D surfaces. The basis of this research is a synergy of empirical processes theory, Gaussian processes theory, the theory of ordinary differential equations, perturbation theory for tensors, numerical analysis, computer vision, and computational statistics."
"1615035","III: Small: Collaborative Research: Structured Methods for Multi-Task Learning","IIS","Info Integration & Informatics","08/01/2016","07/21/2016","Shuiwang Ji","WA","Washington State University","Standard Grant","Aidong Zhang","01/31/2019","$246,883.00","","sji@tamu.edu","280 Lighty","PULLMAN","WA","991641060","5093359661","CSE","7364","7364, 7923","$0.00","The ability of human to learn from and transfer knowledge across related learning tasks enables us to grasp complex concepts from only a few examples. For instance, a three-year old child is able to discriminate chairs from tables without having been exposed to hundreds of different examples. In contrast, computer learning programs typically require training on a large number of examples in order to achieve similar levels of recognition. This prompts the study of multi-task learning in which multiple related tasks are learned simultaneously, thereby facilitating inter-task knowledge transfer. However, most multi-task learning studies are restricted to problems with well-defined tasks and structures. This project aims at developing algorithms and tools (including open source software) to attack problems that are not traditionally treated, but can potentially be reformulated and solved more effectively by multi-task learning. This allows a broad class of challenging machine learning problems to benefit from multi-task learning techniques. This project also develops a new curriculum that incorporates the proposed research into classroom. In addition, this project will allow the PIs to continue the ongoing efforts of actively recruiting and advising students from under-represented groups. <br/><br/>To achieve these goals, this project focuses on an innovative, integrated research and education plan that includes the following components: (1) providing principled guidelines for reformulating problems into the multi-task learning formalism; (2) developing robust and clustered multi-task learning models to identify and prevent false interactions among unrelated tasks; (3) developing sparsity-inducing multi-task learning models to capture richly structured task interactions; (4) developing high-order multi-task learning models to capture task relatedness from interactions between features; and (5) investigating computational algorithms and theoretical properties of multi-task learning. The outcome of this project includes the capabilities of reformulating diverse machine learning problems into the multi-task learning framework and providing radically new ways to attack challenging problems that cannot be solved effectively by traditional methods. The systematic study of multi-task learning in this project is expected to generate novel reformulations, structured mathematical models, efficient optimization algorithms, and principled theoretical analyses, which will lead to significant practical and theoretical advances in multi-task learning."
"1650080","EAGER: Novel sampling algorithms for scaling up spectral methods for unsupervised learning","IIS","Robust Intelligence","08/15/2016","08/23/2016","Claire Monteleoni","DC","George Washington University","Standard Grant","Weng-keen Wong","07/31/2018","$90,000.00","","cmontel@colorado.edu","1922 F Street NW","Washington","DC","200520086","2029940728","CSE","7495","7495, 7916","$0.00","In the era of big data, unsupervised learning has become increasingly important. At a high-level, unsupervised learning serves to reduce the data size, while capturing its important underlying structure. For a powerful and widely-used family of unsupervised learning techniques (those based on spectral methods), scaling up to large data sets poses significant computational challenges. This research project will develop extremely simple and lightweight sampling techniques for scaling up this family of unsupervised learning methods. Since big data is ubiquitous, these research advances are likely to be transformative to a range of fields. This project will benefit society through the research team's ongoing collaborations in climate science, agriculture, and finance. The team will also continue to engage the computer science community in this endeavor, by training students, developing tutorials, and broadening the participation of women and minorities in computing.<br/><br/>This project will advance machine learning research by scaling up spectral methods for the analysis of large data sets. While spectral methods for the unsupervised learning tasks of clustering and embedding have found wide success in a variety of practical applications, scaling them up to large data sets poses significant computational challenges. In particular, the storage and computation needed to handle the affinity matrix (a matrix of pairwise similarities between data points) can be prohibitive. An approach that has found promise is to instead approximate this matrix in some sense. The goal of this project is to provide simple approximation techniques that manage the tradeoff between their space and time complexity vs. the quality of the approximation. The proposed approach involves sampling techniques that address this goal by exploiting latent structure in a data set, in order to minimize the amount of information that needs to be stored to (approximately) represent it. This leads to techniques that speed up the computation and reduce the memory requirements of spectral methods, while simultaneously providing better approximations. The project will also continue the team's momentum on leveraging advances in machine learning for data-driven discovery."
"1660128","WiFiUS: Collaborative Research: Sequential Inference and Learning for Agile Spectrum Use","CNS","Special Projects - CNS, Special Projects - CCF","09/01/2016","09/08/2016","Lifeng Lai","CA","University of California-Davis","Standard Grant","Monisha Ghosh","02/28/2018","$85,988.00","","lflai@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","CSE","1714, 2878","7363, 8229","$0.00","A key imperative to expanding future wireless services is to overcome the spectral crunch. At present, static allocation and rigid regulation lead to under utilization of available spectral resources. Flexible spectrum use aims at exploiting under-utilized spectrum. Available spectrum opportunities may be non-contiguous, scattered over a large bandwidth, and are available locally and for a limited period of time due to the highly dynamic nature of wireless transmissions. This fuels the need to understand how to discover, assess and utilize the time-frequency-location varying spectral resources efficiently and with minimal delay. Moreover, it is critical to access identified idle spectrum in an agile manner.<br/><br/>This project will design sequential inference and learning algorithms for agile spectrum access when the state of the spectrum varies rapidly. The key advantage of sequential algorithms, as compared to block-wise algorithms, is that they typically lead to significantly reduced decision delays. The overarching goal of this project is to design sequential inference and learning algorithms for agile spectrum utilization. In particular, this project will employ advanced sequential inference and learning methods for the following three interconnected yet increasingly sophisticated and demanding tasks: 1) to employ sequential reinforcement learning and sequential inference algorithms to design sensing policies for rapid spectrum opportunities discovery; 2) to design sequential algorithms for fast and accurate spectrum quality assessment; and 3) to build, maintain and exploit an interference map of the area where our network operates and represent it as a spatial potential field. The proposed research is expected to make substantial contributions to both applications and theory. On the application level, the proposed research has the potential to substantially improve spectral efficiency by introducing novel tools from sequential analysis, machine learning and statistical inference for the design of spectrum discovery, assessment and exploitation policies. On the theoretical level, the proposed project will advance the state of the art in sequential analysis and contribute new approaches to the general methodological base for optimal stopping, control and machine learning problems. Furthermore, new methods and theory of modeling and exploiting knowledge of interference using spatial potential fields, sequential statistics and advanced propagation modeling will be developed."
"1617397","CIF: Small: Collaborative Research: Geometrical and Statistical Modeling of Space-Time symmetries for Human Action Analysis and Retraining","CCF","Comm & Information Foundations","07/15/2016","07/08/2016","Anuj Srivastava","FL","Florida State University","Standard Grant","Phillip Regalia","06/30/2020","$216,883.00","","anuj@stat.fsu.edu","874 Traditions Way, 3rd Floor","TALLAHASSEE","FL","323064166","8506445260","CSE","7797","7923, 7936","$0.00","This interdisciplinary research aims to advance current understanding and utilization of space-time symmetries in analyzing human movements, using fundamental tools from engineering, geometry, and statistics. Broader applications of this research include home or workplace-based self-reflection of daily activities, promotion of higher efficiency of human movements, and long-term management and/or prevention of movement disorders. While the need for comprehensively and statistically analyzing human kinematics is well chronicled, the current measures are often limited to simplistic quantities such as speeds and acceleration profiles of individual limbs. This project will focus on both spatial and temporal symmetries of limb movements, full body shapes, and complete dynamical actions, for assessment of movements ranging from daily activities to physiotherapeutic exercises. <br/><br/>Symmetry has been used in the past, in clinical biomechanics, but in a limited way. This project will develop a comprehensive theory, built on fundamental tools from differential geometry and statistical analysis of geometric objects, to represent, quantify, analyze, and classify motions according to their level of symmetry. The specific forms of symmetry will include spatial reflection, temporal reflection, and space-time glide symmetries. This formulation will incorporate data from various sensing modalities and features, including point trajectories and stick figures from motion capture systems, to shape silhouettes and dynamic textures obtained from video sensors. The project outcomes also include the development of a real-time media-system for movement re-training and reflection of common actions, such as sitting to standing (STS). The proposal brings together a strong and inter-disciplinary team of researchers with expertise in computer vision and action recognition (Turaga), differential geometry and statistics (Srivastava), and somatics and kinesiology (Coleman)."
"1641042","EAGER: Additive Parts-based Data Representation with Nonnegative Sparse Autoencoders","ECCS","CCSS-Comms Circuits & Sens Sys","09/01/2016","07/28/2016","Jacek Zurada","KY","University of Louisville Research Foundation Inc","Standard Grant","Lawrence Goldberg","08/31/2019","$233,235.00","Tamer Inanc","jmzura02@louisville.edu","Atria Support Center","Louisville","KY","402021959","5028523788","ENG","7564","7916","$0.00","One of the long-standing open problems of computational learning is its inability to produce solutions that are intuitively understandable.  Because of the limited transparency, most computed predictions are not easily justifiable by humans who need to render final decisions. This project addresses this shortcoming of machine learning predictions by investigating a class of machine learning algorithms that mimics natural processing and leads to more interpretable representations of data. Such processing decomposes visual patterns or other data into parts through unsupervised learning and produces non-negative parts only.  Since this approach allows only additive recombination of parts to reconstruct the original data, it mimics natural processing in human perception and cognition. The project advances novel data representation beyond standard computational learning approaches.  Tests are conducted with planar images or tabulated data that describe specific domain of interest. The tests objectives are to produce useful and understandable features, logic rules or verbal explanations in lower dimensional space. <br/><br/>This work aims at evaluating how rich data can be explored in order to be better understood. The novel paradigm is to generate non-negative, sparse and localized features and receptive fields within hierarchies of features. This is achieved through autoencoder-based transformations of visual images or of typical non-negative data matrices.  The novel autoencoders are constrained to have non-negative weights. Specific conditions to be tested include pooling, rectifying-type activation functions of neurons and select norms of activations sparsity. The project advances the following transformational challenges at the intersection of computational and human systems: (1) Representation of data with non-negative encodings only, (2)  Complexity of layers and of receptive filters (more simpler filters vs. fewer complex filters), (3) Choice of the number of layers, also in the context of hyper-parameter tuning, (4) Pooling for non-negative processing,  (5) Connections between the autoencoder-based learning and related biological evidence, and finally (6) Ability to generate explanations or understandable rules for select domains"
"1625677","MRI: Acquisition of Equipment to Establish Big Data Analytics Infrastructure for Research and Education","CNS","Major Research Instrumentation, EPSCoR Co-Funding","10/01/2016","09/20/2019","Kazem Taghva","NV","University of Nevada Las Vegas","Standard Grant","Rita Rodriguez","09/30/2020","$198,752.00","Laxmi Gewali, Bing Zhang, Ge Lin, Kazem Taghva","kazem.taghva@unlv.edu","4505 MARYLAND PARKWAY","Las Vegas","NV","891549900","7028951357","CSE","1189, 9150","1189, 9150","$0.00","This project, acquiring an integrated high performance computing instrument for big data research and education, aims to fill the gap for big data analytics at a minority-serving institution in an EPSCoR state. The ability to perform network big data research and education adds assets to various basic research activities within the institution. The instrument will also become a critical asset for research groups currently involved in big data analysis and management. It strengthens educational training activities by enabling innovative cross disciplinary courses and promoting our education outreach activity with K-12 teachers and students. <br/><br/>The instrumentation would initially support the following projects addressing issues of national priority critical for training graduate students in big data analytics: 1. Data-Bridge system development and deployment; 2. Community detection framework in large networks; and 3. Deep learning framework for big data analytics. These investigators are among the first few to systematically study sociometric systems for long tail science data collection to develop DataBridge indexing mechanism for scientific datasets. As scientific datasets by themselves provide very sparse information content for searching and discovery, the DataBridge will provide a rich set of tools for mining information and context. The research on community detection in large networks is expected to lead to new insights into community detection of large social and information networks, and new theoretical models for understanding such a community and algorithms to harness them. The research on deep learning library for big data analytics lays out a basic research program for tracking one of the fundamental challenges of developing the Actionable Intelligence Discovery and Exploitation (AIDE)."
"1564955","ABI Development: Developing RaptorX Web Portal for Protein Structure and Functional Study","DBI","ADVANCES IN BIO INFORMATICS","07/15/2016","07/20/2016","Jinbo Xu","IL","Toyota Technological Institute at Chicago","Standard Grant","Peter McCartney","06/30/2020","$557,028.00","","j3xu@ttic.edu","6045 S Kenwood Ave","Chicago","IL","606372803","7738340409","BIO","1165","","$0.00","Proteins play fundamental roles in all biological processes. Complete description of protein structures and functions is a fundamental step towards understanding biological life and has various applications. Millions of protein sequences are available, but a majority of them have no experimentally-solved structures and functions. This project aims to greatly improve RaptorX, a fully-automated web server for computational prediction of protein structure and function, with the goal to deliver a long-term sustainable web portal to facilitate transformative research in biology. This web portal shall benefit a broad range of biological and biomedical applications, such as genome annotation, understanding of disease processes, drug design, precision medicine and even biomaterial and bio-energy development. The results will be disseminated to the broader community through a variety of venues: web servers, standalone software, publications and talks. Since late in 2011, RaptorX has served >25,000 worldwide users including middle- and high-school students. The standalone programs have been downloaded by >1500 worldwide users. After this project is fulfilled, RaptorX will contribute much more to the broader community. Students involved in this project will receive training in the intersection of computer science, molecular biology, biophysics, and biochemistry. Undergraduate and underrepresented students will be recruited through summer intern programs and collaborators. The research results will be integrated into course materials and used in the Illinois online bioinformatics program. <br/><br/>The RaptorX web server was originally developed for only template-based protein modeling. This project will transform RaptorX by first developing a few novel and powerful deep learning (e.g., Deep Conditional Convolutional Neural Fields) and structure learning (e.g., group graphical lasso) methods to significantly improve the accuracy of protein structure and functional prediction and then conducting an efficient implementation. The resultant RaptorX will be able to perform much more accurate prediction of protein secondary and tertiary structure, solvent accessibility and disordered regions, and the quality of a theoretical protein 3D model (in the absence of natives). This project will also expand the RaptorX server to perform contact prediction and contact-assisted protein folding for proteins without good templates. The RaptorX web server is available at http://raptorx.uchicago.edu, from which users can also download the standalone programs."
"1618806","RI: Small: Building Strong Geometric Priors for Total Scene Understanding","IIS","Robust Intelligence","07/15/2016","07/13/2016","Charless Fowlkes","CA","University of California-Irvine","Standard Grant","Jie Yang","06/30/2020","$377,407.00","","fowlkes@ics.uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","CSE","7495","7495, 7923","$0.00","This project is exploring how capabilities for geometric image understanding can change the way people approach the problem of automatically interpreting the semantic content of individual photos or videos. By developing algorithms for accurately localizing cameras from images that integrate other sources of geo-spatial data, such as 3D models of buildings and maps of urban areas, the project aims to significantly improve the ability of computer vision systems to understand image content. Utilizing strong prior information for scene understanding has a wide range of important practical applications.  An assistive robot providing elderly care in a home should leverage knowledge of the appearance and location of objects in its immediate environment while adapting to changes on multiple time scales (a coffee cup sitting on the table moves much more frequently than the table itself).  A network of self-driving cars could benefit significantly from dynamically updated urban maps built from the stream of data collected by the cars and other cameras (e.g., adapting behavior to a temporary lane closure that changes typical car and pedestrian traffic patterns).  The project involves students in research spanning a range of traditional disciplines and is engaging a wider audience across the UC Irvine campus in understanding and applying these technologies to novel social and scientific applications.<br/><br/>This research investigates an alternate approach in which scene priors (including affordances and semantic attributes) are represented in 3D geo-spatial model coordinates rather than in 2D image space. Incorporating geometric context into scene understanding has largely been pursued under very weak prior assumptions on scene geometry and camera pose. Importantly, the research allows for direct integration of non-visual data such as GIS maps. The project is developing the appropriate algorithms and datasets to integrating such data along with a continual stream of images to produce a strong, temporally-evolving (4D) scene prior that can improve accuracy of camera pose estimation, monocular geometry, object detection and semantic segmentation."
"1633206","BIGDATA: Collaborative Research: IA: BirdVox: Automating Acoustic Monitoring of Migrating Bird Species","IIS","Info Integration & Informatics, Big Data Science &Engineering","10/01/2016","05/08/2017","Steven Kelling","NY","Cornell University","Continuing Grant","Sylvia Spengler","09/30/2020","$963,018.00","Andrew Farnsworth, Holger Klinck","stk2@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7364, 8083","7433, 8083, 9251","$0.00","Current bioacoustic monitoring of natural environments requires processing by humans to extract information content from recordings. Thus human processing creates a fundamental bottleneck in which data collection far outpaces capabilities to extract relevant and desired information. Bioacoustic research on automatic species classification in natural environments can be broadly divided into two groups: distinguishing a predefined set of known species from audio clips and extracting species as events that occur in a continuous audio stream. Both classification techniques have their specific problems--many of the data used distinguishing predefined species are recorded under ""studio"" conditions and not extensible to natural conditions, while processing of continuous audio streams generate many false positives. <br/><br/>To overcome these challenges we will take a multi-tiered approach:  Analyzing a data set consisting of full-night recordings from 10 recording units over 100 nights. Building a web-enabled software to engage citizen scientists to identify the flight calls, providing us with a large and extensive model training dataset.  Developing novel convolutional deep-learning networks, which are well suited for analysis of complex auditory scenes.  Visualizing patterns detected and classified flight calls in space and time to produce novel information about the bird migration.  Comparing model-generated acoustic data with radar, video, and direct visual citizen science datasets to produce the most comprehensive accounts of nocturnal bird migration possible.  The combination of domain knowledge in bird vocalizations, engaging citizen scientists to allow development of large well annotated training datasets, and taking a novel deep-learning approach, will finally resolve the machine classification of acoustic signals in natural environments."
"1632116","PFI:BIC- A Smart Service System for Traffic Incident Management Enabled by Large-data Innovations (TIMELI)","IIP","PFI-Partnrships for Innovation, IUCRC-Indust-Univ Coop Res Ctr","09/01/2016","10/21/2016","Anuj Sharma","IA","Iowa State University","Standard Grant","Jesus Soriano Molla","08/31/2020","$1,000,000.00","Soumik Sarkar, Neal Hawkins, Srikanta Tirthapura, Stephen Gilbert","anujs@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","ENG","1662, 5761","1662, 9150","$0.00","The Federal Highway Administration estimates that a quarter of the congestion on U.S. roads is due to traffic incidents such as a crash, an overturned truck, or stalled vehicles. Congestion costs the commercial trucking industry $9.2 billion annually, and incidents have been shown to increase the risk of secondary crashes by 2.8 percent with every minute of congestion. To address these economic and safety issues, traffic incident management (TIM) centers, typically operated by state departments of transportation (DOT), monitor roadways for traffic incidents, coordinate incident response, and provide traffic management and control to minimize the impacts of traffic incidents. This research will develop a new TIM system, called TIMELI (Traffic Incident Management Enabled by Large-data Innovations), that has greatly enhanced capabilities for incident risk assessment and response over current products. Software-based intelligent transportation systems (ITS) that are currently available are limited to very basic controls and do not provide comprehensive or dynamic decision support. These systems display streams of traffic data on a map and rely on technicians to input a control action. To address these limitations, this smart system aims to be a more effective data-driven TIM that provides user-centric information visualization and improved analytics and machine learning. Use of the system by state DOTs can reduce the duration and impacts of incidents and improve the safety of motorists, crash victims, and emergency responders. Use will also reduce the TIM technician fatigue and reduce their turnover rates. <br/><br/>The goal and outcome of TIMELI is to use emerging large-scale data analytics to reduce the number of road incidents through proactive traffic control and to minimize the impact of individual incidents that do occur through early detection, response, and traffic management and control. This will be achieved using end-to-end machine learning for situational awareness, the design and rapid solution of geo-temporally aware traffic models using partial differential equations, stochastic model predictive control, and user-centric advanced visualization techniques for decision assistance. Current technology gaps in data handling and archiving, analysis for decision support, and the design of output formats will be addressed using big data technologies. Multiple large data streams will be ingested and data analytics will be performed for quality assurance and anomaly detection. New algorithmic approaches, machine learning, and a stochastic framework will be used to detect anomalous outliers and implement context-sensitive traffic models. An advanced human machine interface will provide information visualization and decisions recommendations in an intuitive format to minimize any cognitive bottlenecks. The objectives are to develop TIMELI and to integrate it into an existing TIM system. These will be accomplished by the following methods: (1) defining TIM user requirements and identifying bottlenecks in technician tasks using human factors research; (2) developing a prototype that includes a big-data-enabled back-end solution, an analytics engine, and a front-end interface; and (3) conducting testing, evaluation, and integration within Iowa DOT's existing TIM environment. TIMELI's multiple innovations will transform current TIM systems by creating a smart and reliable decision assist system used to monitor traffic conditions in real time, proactively control risk using advisory control, quickly detect traffic incidents, identify the location and potential cause of these incidents, suggest traffic control alternatives, and minimize cognitive bottlenecks for TIM operators. The test bed will be the Center for Transportation Research and Education's fully functional traffic operations lab that is connected to the Iowa DOT's data streams.<br/><br/>This research will contribute to education by involving undergraduate and graduate researchers in Civil Engineering, Electrical and Computer Engineering, Mechanical Engineering and Human Factors Engineering, and will generate real-world data sets that will be used in developing educational material.<br/><br/>The partners in this project are Iowa State University (lead academic institutions), TransCore (a commercial provider of intelligent transportation systems, Des Moines, IA), and Iowa Department of Transportation-DOT (government agency, Ames, IA)."
"1616974","Large Scale Structure Studies on a Value Added Galaxy Catalog","AST","EXTRAGALACTIC ASTRON & COSMOLO","09/01/2016","09/02/2016","Istvan Szapudi","HI","University of Hawaii","Standard Grant","Nigel Sharp","08/31/2021","$578,131.00","","szapudi@ifa.hawaii.edu","2440 Campus Road, Box 368","HONOLULU","HI","968222234","8089567800","MPS","1217","1206, 7433, 7480, 9150","$0.00","New statistical and machine learning techniques will create a better catalog of galaxies.  This catalog will combine information from several existing surveys.  A technique called ""photometric redshifts"" will give speed and distance information from study of the galaxy colors.  The result will be the widest area and deepest catalog yet available.  It will be ideal for comparing to other large area surveys, especially of the Cosmic Microwave Background (CMB).  Those studies will shed light on cosmology and dark energy.  They may even show whether a more exotic theory of gravity is needed.  Education is included through special topic courses.  There are connections to national laboratories.  The methods will be introduced to students from other disciplines.<br/><br/>This project will create a combined value-added galaxy catalog to be called PS1*, for statistical studies of large-scale structure (LSS).  Building on experience developing a star-galaxy separation algorithm for the first PanSTARRS catalog (PS1) using Support Vector Machines (SVM) and training sets from Sloan Digital Sky Survey (SDSS) data, the work will proceed by matching Wide-field Infrared Survey Explorer (WISE) data, PS1, and where available, SDSS and Two Micron All Sky Survey (2MASS) objects.  The SVM algorithm will extend over the output catalog.  The resulting galaxy maps will be several times larger than SDSS, with less stellar contamination than PS1 alone, and will be deeper than WISE or 2MASS.  Previously demonstrated machine learning tools will then be used to estimate photometric redshifts for the combined sample.  The new PS1* will be the widest area, deepest photometric redshift catalog, and will be optimal for cross correlation studies with other wide data sets, such as the CMB, X-ray surveys, the cosmic infrared background, and maps of gravitational lensing.<br/><br/>Specific questions that will be studied include whether CMB anomalies are caused by LSS, whether superstructures that will be found in PS1* have any effect on the CMB, how the map of the Integrated Sachs-Wolfe (ISW) effect to be created from PS1* relates to the CMB map from the Planck satellite, and how well LSS statistics and cosmological parameters can be extracted.  The project will explore novel machine learning techniques for both star-galaxy separation and photometric redshifts and apply state of the art statistical techniques, shedding light on cosmological parameters and dark energy.  It will investigate any connection between LSS and CMB anomalies, and whether this is consistent with ISW or requires a more exotic theory.  The catalog will be publicly available for a variety of cross-correlation studies, and the algorithms and open source software developed will be disseminated widely.  This study promotes the integration of research into teaching through courses, internships, and including cross-disciplinary students."
"1637876","NRI: Enhancing Mapping Capabilities of Underwater Caves using Robotic Assistive Technology","IIS","NRI-National Robotics Initiati, EPSCoR Co-Funding","11/01/2016","08/25/2016","Ioannis Rekleitis","SC","University of South Carolina at Columbia","Standard Grant","Jie Yang","10/31/2019","$526,405.00","","yiannisr@cse.sc.edu","Sponsored Awards Management","COLUMBIA","SC","292080001","8037777093","CSE","8013, 9150","8086, 9150","$0.00","This project develops robotic assistive technologies to improve mapping capabilities of underwater caves. The project enables the practical construction of accurate volumetric models for water-filled caves. The technology of this robotic system can also be deployed on underwater vehicles enabling the autonomous exploration of caves and other underwater structures. The developed techniques can also be used in some applications of aerial and ground vehicles. Collected data from field deployments of the developed sensor are made available to the wider robotic, geological, and speleological research community through public-domain releases in order to further innovation. Furthermore, the data and software, released under an open-source license, enable researchers to test algorithms on computer vision, state estimation, and sensor fusion, in challenging environments. The project integrates research and education through training graduate and undergraduate students and enhancing several graduate and undergraduate courses at the University of South Carolina. The project also engages undergraduate students from Benedict College, a Historically Black College or University (HBCU). The collected data are used in outreach activities to recruit high-school students of the greater Columbia area in STEM education, engaging students and educators, particularly in underserved communities.<br/><br/>This research develops 3D reconstruction algorithms utilizing the environmental characteristic of a cave system. The research team studies robotic technologies for sensor fusion of multiple data streams in a single unit and validates experimentally the developed system via extensive testing in underwater cave explorations in collaboration with expert cave divers. The project introduces robotic technology to the underwater cave explorer community by capitalizing on existing practices in three steps: (a) deploying stereo cameras to be used in conjunction with structured light carried by the divers, (b) developing a bearing-only Cooperative Localization system for accurately recording the skeleton of explored caves; (c) developing a sensor suite that seamlessly integrates inertial measurement unit, sonar, depth, and visual data with state estimation algorithms for the volumetric mapping of the cave. The project enhances underwater cave mapping abilities by increasing: 1) the scale of the area mapped, 2) the safety of the divers by reducing their cognitive load during exploration and 3) the quality of the produced maps."
"1553547","CAREER: Scaling Approximate Inference and Approximation-Aware Learning","IIS","Info Integration & Informatics","04/01/2016","04/18/2017","Wolfgang Gatterbauer","PA","Carnegie-Mellon University","Continuing grant","Aidong Zhang","11/30/2017","$163,512.00","","wolfgang@ccis.neu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7364","1045, 7364","$0.00","The last decade has seen an enormous increase in our ability to gather and manage large amounts of data; business, healthcare, education, economy, science, and almost every aspect of society are accumulating data at unprecedented levels. The basic premise is that by having more data, even if uncertain and of lower quality, we are also able to make better-informed decisions. To make any decisions, we need to perform ""inference"" over the data, i.e. to either draw new conclusions, or to find support for existing hypotheses, thus allowing us to favor one course of action over another. However, general reasoning under uncertainty is highly intractable, and many state-of-the-art systems today perform approximate inference by reverting to sampling. Thus for many modern applications (such as information extraction, knowledge aggregation, question-answering systems, computer vision, and machine intelligence), inference is a key bottleneck, and new methods for tractable approximate inference are needed.<br/><br/>This project addresses the challenge of scaling inference by generalizing two highly scalable approximate inference methods and complementing them with scalable methods for parameter learning that are ""approximation-aware."" Thus, instead of treating the (i) learning and the (ii) inference steps separately, this project uses the approximation methods developed for inference also for learning the model. The research hypothesis is that this approach increases the overall end-to-end prediction accuracy while simultaneously increasing scalability. Concretely, the project develops the theory and a set of scalable algorithms and optimization methods for at least the following four sub-problems: (1) approximating general probabilistic conjunctive queries with standard relational databases; (2) learning the probabilities in uncertain databases based on feedback on rankings of output tuples from general queries; (3) approximating the exact probabilistic inference in undirected graphical models with linearized update equations; and (4) complementing the latter with a robust framework for learning linearized potentials from partially labeled data."
"1622168","SBIR Phase I:  Aggressive Maneuvering of Small Autonomous Robots in Home Environments","IIP","SMALL BUSINESS PHASE I","07/01/2016","06/20/2016","David Jun","IL","Petronics Inc","Standard Grant","Muralidharan S. Nair","05/31/2017","$225,000.00","","davidjun@petronics.io","60 Hazelwood Dr Rm 216","Champaign","IL","618207460","7737443283","ENG","5371","5371, 6840, 8034, 8035, HPCC","$0.00","The broader impact/commercial potential of this project is to enable mobile robots to coexist harmoniously with people in their homes and offices. The market for consumer and office robots is projected to grow 17% annually, seven times faster than the market for manufacturing robots, reaching $1.5B by 2019. An important step toward market growth is creating autonomous robots that are unobtrusive, intelligent, and highly agile. Accomplishing this requires robots to be small enough to stay out of the way and fast enough to elegantly avoid humans and environmental obstacles. Inappropriate noise levels and safety concerns make it unlikely that airborne vehicles will be prevalent in indoor environments, whereas wheeled mobile robots can achieve near-silent operation. Tiny, fast robots are unobtrusive enough to use as a low-cost surveillance tool in home or office security, and portable enough for covertly investigating hostile situations. People with severe disabilities could travel vicariously by combining a virtual reality headset with a telepresence robot. Fast maneuvering robots could be used as a compelling educational or entertainment platform for kids and adults. <br/><br/>This Small Business Innovation Research (SBIR) Phase I project aims to prove the feasibility of enabling small wheeled robots to maneuver aggressively and predictably in varied operating conditions using consumer-affordable hardware components. While this project develops algorithms using a low-cost camera solution for localization, the methodology developed in Phase I will only improve performance as the technology for localization and navigation matures. The key challenges in creating a robot that can quickly navigate varied environments, as demonstrated during rigorous testing of early prototypes in real homes, involve understanding how a small robot moves on varied surfaces in the presence of slip, and correspondingly, how to accurately and efficiently plan predictable maneuvers on these surfaces. Three key Phase I objectives will address these challenges: 1) automatically learning surface models in unknown environments, 2) planning and executing aggressive maneuvers on learned surfaces, and 3) integrating a robot and a low-cost computer vision based localization system for autonomous control."
"1548956","SBIR Phase I: Development of an Intelligent, Emotive, and Perceptive Socially Assistive Robot for Dementia Therapy","IIP","SMALL BUSINESS PHASE I","01/01/2016","11/24/2015","Mohammad Mahoor","CO","Dream Face Technologies, LLC","Standard Grant","Muralidharan S. Nair","08/31/2016","$150,000.00","","mmahoor@dreamfacetech.com","8772 Troon Village PL","Lone Tree","CO","801243135","7202015694","ENG","5371","4080, 5371, 6840, 8035, HPCC","$0.00","The broader impact/commercial potential of this project is the development of a social robot that will provide initial evidence for the feasibility and promise of a new generation of life-like animation-based and emotionally intelligent robots that provide companionship for individuals with cognitive disabilities (e.g. Dementia and AD) and increase their happiness and independence. The application and the potential societal and commercial impact of the project is not limited to aging population and dementia therapy and can be used in a variety of applications such as science, math & reading tutoring in classrooms or homes, language and speech therapy, autism therapy (e.g. special Ed programs in school districts), and even as concierges and front desk receptionists in hospitals, museums, hotels, and restaurants. <br/><br/>This Small Business Innovation Research (SBIR) Phase I project will demonstrate the feasibility of developing and deploying of emotionally intelligent light-projected life-like conversational robots for improving the quality of life, self-efficacy and cognitive abilities of elderly individuals with Alzheimer?s disease (AD) and dementia. Older people with AD and other dementias have significantly more hospital stays, skilled nursing facility stays and home health care visits than other older people. There is thus a critical and growing demand in society and in our healthcare system to find efficient, effective and affordable ways to provide care and improve the quality of life of elderly individuals with dementia. This project will create an innovative, empathic social robot which will engage elderly individuals in assisted living facilities in a variety of activities, including exercises, games, and especially natural spoken dialogs that relevant to their interests and needs. The project extends the capabilities of current social robots through research that integrates computer vision, spoken dialog, character animation and robotic head movement technologies to develop an emotionally intelligent and empathic robotic caregiver and companion. A pilot study will be conducted, in collaboration with health care practitioners and residences of senior living facilities, to study how naturally and effectively the social robot communicates with elderly individuals and engages them in therapeutic activities (e.g. Montessori-based activities and Spaced Retrieval memory care techniques), and whether interacting with the robot over several weeks improves their self-efficacy and sense of well-being."
"1566589","CRII: SHF: Machine-Learning-Based Test Effectiveness Prediction","CCF","CRII CISE Research Initiation","05/15/2016","05/18/2016","Lingming Zhang","TX","University of Texas at Dallas","Standard Grant","Sol Greenspan","04/30/2019","$174,150.00","","lingming.zhang@utdallas.edu","800 W. Campbell Rd., AD15","Richardson","TX","750803021","9728832313","CSE","026Y","7798, 7944, 8228","$0.00","Test effectiveness, which indicates the capability of tests in detecting potential software bugs, is crucial for software testing. More effective tests can detect more potential bugs and thus help prevent economic loss or even physical damage caused by software bugs. Therefore, a huge body of research efforts have been dedicated to test effectiveness evaluation during the past decades. Recently, mutation testing, a powerful methodology that computes the detection rate of artificially injected bugs to measure test effectiveness, is drawing more and more attention from both the academia and industry. Various studies have shown that artificial bugs generated by mutation testing are close to real bugs, demonstrating mutation testing effectiveness in test effectiveness evaluation. However, a major obstacle for mutation testing is the efficiency problem ? mutation testing requires the execution of each artificial buggy version (i.e., mutant) to check whether the test suite can detect that bug, and which is extremely time consuming. Therefore, a light-weight but precise technique for measuring test effectiveness is highly desirable.<br/><br/>The approach is to automatically extract test effectiveness information (e.g., mutation testing results) from various open-source projects  to directly predict the test effectiveness of the current project without any mutant execution. More specifically, the PI proposes to design a general classification framework based on a suite of static and dynamic features collected according to the PIE theory of fault detection. Furthermore, this research will explore judicious applications of advanced program analysis, machine learning, and software mining techniques for more powerful feature collection, more active learning, as well as more comprehensive training data preparation. The proposed approach will result in efficient but precise test effectiveness evaluation for projects developed using various programming languages and test paradigms, which is crucial for high-quality software. Furthermore, the training of the classification models will require to collect various basic testing, analysis, and mining information from a huge number of open-source projects, and thus may also benefit a large variety of software testing/analysis/mining  techniques that explore open-source software repositories."
"1629060","Bilateral Bargaining through the Lens of Big Data","SES","Economics","09/15/2016","04/22/2020","Steven Tadelis","MA","National Bureau of Economic Research Inc","Standard Grant","Nancy Lutz","08/31/2021","$403,036.00","Matthew Backus, Brad Larsen, Matthew Taddy","stadelis@berkeley.edu","1050 Massachusetts Avenue","Cambridge","MA","021385398","6178683900","SBE","1320","1320","$0.00","Bilateral bargaining is pervasive, and has been part of human interaction for millennia. People bargain over retail goods, professional services, salaries, real estate, territorial boundaries, mergers and acquisitions, household chores, and more. For an economic activity that is so pervasive, it has proven to be one that is not easily analyzed, let alone fully understood. Because bargaining parties are uncertain about the bargaining position of their counterpart in bargaining, economic theory suggests that there will be inefficiencies such as unnecessary delays in reaching an agreement or even the complete breakdown of negotiations. This project will explore how these informational frictions affect bargaining and whether they can be mitigated with the use of communication between the parties, which serves as a framework for modeling negotiation. The investigators will explore the communication process in bargaining using modern tools from economic theory, econometrics and machine learning in order to bridge some of the gap between the abundance of theory and the rather slim availability of empirical studies on how people actually bargain. In the process, the investigators will employ and train graduate and undergraduate students who will learn how to exploit and analyze ""big data"" and how to tie the data insights to theoretical notions of bargaining from game theory and economics. The results will shed light on how to facilitate bargaining and create value, which is likely to play an important role with the growing prevalence of online bargaining platforms. <br/><br/>This project is based on access to novel data of millions of bargaining transactions on the eBay.com ""Best Offer"" platform, where sellers offer items at a listed price and invite buyers to engage in alternating, sequential-offer bargaining. The nature of the data -- high-dimensional and sometimes sparse and unstructured -- not only benefits from, but in fact requires the collaboration of methods from economics, statistics and machine learning, which explains the diversity of approaches and the composition of the group of investigators for this project. The investigators use a variety of methods that will exploit the unique and expansive data obtained from eBay.com. A first set of analyses describe the way in which bargaining unfolds between buyer-seller pairs on eBay.com. A second set of analyses explore the communication between agents engaged in bargaining using data from eBay that contains stored messages sent alongside offers for Best Offer listings. The probability of certain outcomes (e.g., a counteroffer from the seller) will be modeled using machine learning techniques, and within the resulting predictive models the investigators will isolate the role of message content. This content effect will provide a descriptive analysis of the importance of communication in bargaining. The last set of analyses explores the role of ""cheap talk"" in bargaining. Based on theoretical models of cheap talk signaling, the investigators propose a set of tests that unveil equilibrium behavior and apply a set of regression discontinuity techniques to verify the ways in which behavior conforms with equilibrium theory predictions."
"1738063","EAGER: Quantifying and Reducing Data Bias in Object Detection Using Physics-based Image Synthesis","IIS","ROBUST INTELLIGENCE","09/01/2016","03/08/2017","Kate Saenko","MA","Trustees of Boston University","Standard Grant","Jie Yang","05/31/2018","$55,074.00","","saenko@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","7495","7495, 7916","$0.00","This project develops improved computer vision methods for automatic recognition of arbitrary objects in images from realistic environments. Object recognition is typically performed by fitting a function that maps an image to likely object locations and labels. Such a function is fitted (trained) on a database of example images along with their human-assigned object locations and labels. This research can result in more accurate visual perception for socially relevant applications, such as robots performing household tasks, assisting the elderly, responding to disasters and quickly learning new manufacturing and service skills. It can also provide a common codebase for the wider community, new dataset challenges for domain adaptation problems, the dissemination of scientific and technical results and associated courseware, and specific outreach to ensure broad participation of underrepresented groups.<br/><br/>The specific research agenda is structured around two aims. The first aim is to establish bounds on the coverage of latent physical factors in datasets needed for human-level performance on arbitrary domains. The study involves both existing datasets and new datasets generated using graphics rendering techniques at various degrees of photorealism. The goal is to develop a theory of the physical complexity of a given dataset and how it affects generalization to real world object recognition tasks, with respect to a given image representation and learning framework. Physical parameters include but are not limited to: 3D shape, surface color, texture, background/scene, camera viewpoint, sensor noise, lighting, specularities and cast shadows. The second research aim is to learn image representations invariant to some of the physical causes of data bias. The goal is to develop model and representation learning methods that are able to learn from a combination of real and non-photorealistic synthetic data, and are resistant to common sources of data bias. The representations include simple edge-based descriptors, and more generally hierarchical representations based on layers of convolution and pooling operations."
"1601659","Digitization TCN: Collaborative Research: Lepidoptera of North America Network: Documenting Diversity in the Largest Clade of Herbivores","DBI","Digitization","07/01/2016","05/04/2016","Nico Franz","AZ","Arizona State University","Standard Grant","Reed Beaman","06/30/2020","$186,570.00","Melody Basham","nico.franz@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","BIO","6895","6895","$0.00","Lepidoptera (butterflies and moths) are one of the most diverse groups of organisms on the planet: worldwide there are approximately 160,000 species, including around 14,300 species in North America. Moths and butterflies are a conspicuous component of terrestrial habitats and one of the most diverse groups of plant-feeding animals worldwide. This group insect includes species of great economic importance. Their juveniles feed on plants useful to humans, including grains, cotton, tobacco, and timber and shade trees. However, many of the adults are beneficial as pollinators and are icons of conservation as evidenced by Monarch butterflies. Given their economic importance and sheer beauty, butterflies and moths are one of the most abundant insect group in museum collections, but only a fraction of the approximately 15 million specimens in non-federal collections have had their specimen label information digitally recorded and accessible to researchers and educators. Of those specimens that have been digitized, fewer than 10% of the North American Lepidoptera species have sufficient, accessible occurrence data to make reliable predictions about habitat use, susceptibility to global change impacts, or other ecologically important interactions. This project will digitize and integrate existing, unconnected collections of lepidopterans to leverage the outstanding potential of this group of organisms for transformative research, training and outreach. <br/><br/>The Lepidoptera of North America Network (LepNet) comprises 26 research collections that will digitize approximately 2 million specimen records and integrate these with over 1 million existing records. LepNet will digitize 43,280 larval vial records with host plant data, making this the first significant digitization of larvae in North American collections. LepNet will produce ca. 82,000 high-quality images of exemplar species covering 60% of North American lepidopteran species. These images will enhance remote identifications and facilitate systematic, ecological, and global change research. In collaboration with Visipedia, LepNet will create LepSnap, a computer vision tool that can provide automated identifications to the species level. Museum volunteers and student researchers equipped with smartphones will image >132,000 additional research-quality images through LepSnap. Up to 5,000 lepidopteran species will be elevated to a ""research ready"" status suitable for complex, data-driven analyses. LepNet will build on the existing data portal (SCAN) in consolidating data on Lepidoptera to the evolution of lepidopteran herbivores in North America. Access to these data will be increased through integration with iDigBio. Data for a broad range of research, including the evolutionary ecology of Lepidoptera and their host plants in the context of global change processes affecting biogeographic distributions will be generated. The LepXPLOR! program will spearhead education and outreach efforts for 67 existing programs, engaging a diverse, nationwide workforce of 400+ students and 3,500+ volunteers. Overall, LepNet will generate a sustainable social-research network dedicated to the creation and maintenance of a digital collection of North American Lepidoptera specimens (http://www.lep-net.org/)."
"1600937","Digitization TCN: Collaborative Research: Lepidoptera of North America Network: Documenting Diversity in the Largest Clade of Herbivores","DBI","Digitization","07/01/2016","05/04/2016","Boris Kondratieff","CO","Colorado State University","Standard Grant","Reed Beaman","06/30/2020","$286,208.00","Paul Opler","Boris.Kondratieff@Colostate.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","BIO","6895","6895","$0.00","Lepidoptera (butterflies and moths) are one of the most diverse groups of organisms on the planet: worldwide there are approximately 160,000 species, including around 14,300 species in North America. Moths and butterflies are a conspicuous component of terrestrial habitats and one of the most diverse groups of plant-feeding animals worldwide. This group insect includes species of great economic importance. Their juveniles feed on plants useful to humans, including grains, cotton, tobacco, and timber and shade trees. However, many of the adults are beneficial as pollinators and are icons of conservation as evidenced by Monarch butterflies. Given their economic importance and sheer beauty, butterflies and moths are one of the most abundant insect group in museum collections, but only a fraction of the approximately 15 million specimens in non-federal collections have had their specimen label information digitally recorded and accessible to researchers and educators. Of those specimens that have been digitized, fewer than 10% of the North American Lepidoptera species have sufficient, accessible occurrence data to make reliable predictions about habitat use, susceptibility to global change impacts, or other ecologically important interactions. This project will digitize and integrate existing, unconnected collections of lepidopterans to leverage the outstanding potential of this group of organisms for transformative research, training and outreach. <br/><br/>The Lepidoptera of North America Network (LepNet) comprises 26 research collections that will digitize approximately 2 million specimen records and integrate these with over 1 million existing records. LepNet will digitize 43,280 larval vial records with host plant data, making this the first significant digitization of larvae in North American collections. LepNet will produce ca. 82,000 high-quality images of exemplar species covering 60% of North American lepidopteran species. These images will enhance remote identifications and facilitate systematic, ecological, and global change research. In collaboration with Visipedia, LepNet will create LepSnap, a computer vision tool that can provide automated identifications to the species level. Museum volunteers and student researchers equipped with smartphones will image >132,000 additional research-quality images through LepSnap. Up to 5,000 lepidopteran species will be elevated to a ""research ready"" status suitable for complex, data-driven analyses. LepNet will build on the existing data portal (SCAN) in consolidating data on Lepidoptera to the evolution of lepidopteran herbivores in North America. Access to these data will be increased through integration with iDigBio. Data for a broad range of research, including the evolutionary ecology of Lepidoptera and their host plants in the context of global change processes affecting biogeographic distributions will be generated. The LepXPLOR! program will spearhead education and outreach efforts for 67 existing programs, engaging a diverse, nationwide workforce of 400+ students and 3,500+ volunteers. Overall, LepNet will generate a sustainable social-research network dedicated to the creation and maintenance of a digital collection of North American Lepidoptera specimens (http://www.lep-net.org/)."
"1600616","Digitization TCN: Collaborative Research: Lepidoptera of North America Network: Documenting Diversity in the Largest Clade of Herbivores","DBI","Digitization","07/01/2016","06/27/2018","Lawrence Gall","CT","Yale University","Continuing Grant","Reed Beaman","06/30/2020","$238,107.00","","lawrence.gall@yale.edu","Office of Sponsored Projects","New Haven","CT","065208327","2037854689","BIO","6895","6895","$0.00","Lepidoptera (butterflies and moths) are one of the most diverse groups of organisms on the planet: worldwide there are approximately 160,000 species, including around 14,300 species in North America. Moths and butterflies are a conspicuous component of terrestrial habitats and one of the most diverse groups of plant-feeding animals worldwide. This group insect includes species of great economic importance. Their juveniles feed on plants useful to humans, including grains, cotton, tobacco, and timber and shade trees. However, many of the adults are beneficial as pollinators and are icons of conservation as evidenced by Monarch butterflies. Given their economic importance and sheer beauty, butterflies and moths are one of the most abundant insect group in museum collections, but only a fraction of the approximately 15 million specimens in non-federal collections have had their specimen label information digitally recorded and accessible to researchers and educators. Of those specimens that have been digitized, fewer than 10% of the North American Lepidoptera species have sufficient, accessible occurrence data to make reliable predictions about habitat use, susceptibility to global change impacts, or other ecologically important interactions. This project will digitize and integrate existing, unconnected collections of lepidopterans to leverage the outstanding potential of this group of organisms for transformative research, training and outreach. <br/><br/>The Lepidoptera of North America Network (LepNet) comprises 26 research collections that will digitize approximately 2 million specimen records and integrate these with over 1 million existing records. LepNet will digitize 43,280 larval vial records with host plant data, making this the first significant digitization of larvae in North American collections. LepNet will produce ca. 82,000 high-quality images of exemplar species covering 60% of North American lepidopteran species. These images will enhance remote identifications and facilitate systematic, ecological, and global change research. In collaboration with Visipedia, LepNet will create LepSnap, a computer vision tool that can provide automated identifications to the species level. Museum volunteers and student researchers equipped with smartphones will image >132,000 additional research-quality images through LepSnap. Up to 5,000 lepidopteran species will be elevated to a ""research ready"" status suitable for complex, data-driven analyses. LepNet will build on the existing data portal (SCAN) in consolidating data on Lepidoptera to the evolution of lepidopteran herbivores in North America. Access to these data will be increased through integration with iDigBio. Data for a broad range of research, including the evolutionary ecology of Lepidoptera and their host plants in the context of global change processes affecting biogeographic distributions will be generated. The LepXPLOR! program will spearhead education and outreach efforts for 67 existing programs, engaging a diverse, nationwide workforce of 400+ students and 3,500+ volunteers. Overall, LepNet will generate a sustainable social-research network dedicated to the creation and maintenance of a digital collection of North American Lepidoptera specimens (http://www.lep-net.org/)."
"1601888","Digitization TCN: Collaborative Research: Lepidoptera of North America Network: Documenting Diversity in the Largest Clade of Herbivores","DBI","Digitization","07/01/2016","05/04/2016","Christopher Marshall","OR","Oregon State University","Standard Grant","Reed Beaman","06/30/2020","$228,549.00","","marshach@science.oregonstate.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","BIO","6895","6895","$0.00","Lepidoptera (butterflies and moths) are one of the most diverse groups of organisms on the planet: worldwide there are approximately 160,000 species, including around 14,300 species in North America. Moths and butterflies are a conspicuous component of terrestrial habitats and one of the most diverse groups of plant-feeding animals worldwide. This group insect includes species of great economic importance. Their juveniles feed on plants useful to humans, including grains, cotton, tobacco, and timber and shade trees. However, many of the adults are beneficial as pollinators and are icons of conservation as evidenced by Monarch butterflies. Given their economic importance and sheer beauty, butterflies and moths are one of the most abundant insect group in museum collections, but only a fraction of the approximately 15 million specimens in non-federal collections have had their specimen label information digitally recorded and accessible to researchers and educators. Of those specimens that have been digitized, fewer than 10% of the North American Lepidoptera species have sufficient, accessible occurrence data to make reliable predictions about habitat use, susceptibility to global change impacts, or other ecologically important interactions. This project will digitize and integrate existing, unconnected collections of lepidopterans to leverage the outstanding potential of this group of organisms for transformative research, training and outreach. <br/><br/>The Lepidoptera of North America Network (LepNet) comprises 26 research collections that will digitize approximately 2 million specimen records and integrate these with over 1 million existing records. LepNet will digitize 43,280 larval vial records with host plant data, making this the first significant digitization of larvae in North American collections. LepNet will produce ca. 82,000 high-quality images of exemplar species covering 60% of North American lepidopteran species. These images will enhance remote identifications and facilitate systematic, ecological, and global change research. In collaboration with Visipedia, LepNet will create LepSnap, a computer vision tool that can provide automated identifications to the species level. Museum volunteers and student researchers equipped with smartphones will image >132,000 additional research-quality images through LepSnap. Up to 5,000 lepidopteran species will be elevated to a ""research ready"" status suitable for complex, data-driven analyses. LepNet will build on the existing data portal (SCAN) in consolidating data on Lepidoptera to the evolution of lepidopteran herbivores in North America. Access to these data will be increased through integration with iDigBio. Data for a broad range of research, including the evolutionary ecology of Lepidoptera and their host plants in the context of global change processes affecting biogeographic distributions will be generated. The LepXPLOR! program will spearhead education and outreach efforts for 67 existing programs, engaging a diverse, nationwide workforce of 400+ students and 3,500+ volunteers. Overall, LepNet will generate a sustainable social-research network dedicated to the creation and maintenance of a digital collection of North American Lepidoptera specimens (http://www.lep-net.org/)."
"1601002","Digitization TCN: Collaborative Research: Lepidoptera of North America Network: Documenting Diversity in the Largest Clade of Herbivores","DBI","Digitization","07/01/2016","05/04/2016","Joseph McHugh","GA","University of Georgia Research Foundation Inc","Standard Grant","Reed Beaman","06/30/2020","$48,623.00","","mchugh.jv@gmail.com","310 East Campus Rd","ATHENS","GA","306021589","7065425939","BIO","6895","6895","$0.00","Lepidoptera (butterflies and moths) are one of the most diverse groups of organisms on the planet: worldwide there are approximately 160,000 species, including around 14,300 species in North America. Moths and butterflies are a conspicuous component of terrestrial habitats and one of the most diverse groups of plant-feeding animals worldwide. This group insect includes species of great economic importance. Their juveniles feed on plants useful to humans, including grains, cotton, tobacco, and timber and shade trees. However, many of the adults are beneficial as pollinators and are icons of conservation as evidenced by Monarch butterflies. Given their economic importance and sheer beauty, butterflies and moths are one of the most abundant insect group in museum collections, but only a fraction of the approximately 15 million specimens in non-federal collections have had their specimen label information digitally recorded and accessible to researchers and educators. Of those specimens that have been digitized, fewer than 10% of the North American Lepidoptera species have sufficient, accessible occurrence data to make reliable predictions about habitat use, susceptibility to global change impacts, or other ecologically important interactions. This project will digitize and integrate existing, unconnected collections of lepidopterans to leverage the outstanding potential of this group of organisms for transformative research, training and outreach. <br/><br/>The Lepidoptera of North America Network (LepNet) comprises 26 research collections that will digitize approximately 2 million specimen records and integrate these with over 1 million existing records. LepNet will digitize 43,280 larval vial records with host plant data, making this the first significant digitization of larvae in North American collections. LepNet will produce ca. 82,000 high-quality images of exemplar species covering 60% of North American lepidopteran species. These images will enhance remote identifications and facilitate systematic, ecological, and global change research. In collaboration with Visipedia, LepNet will create LepSnap, a computer vision tool that can provide automated identifications to the species level. Museum volunteers and student researchers equipped with smartphones will image >132,000 additional research-quality images through LepSnap. Up to 5,000 lepidopteran species will be elevated to a ""research ready"" status suitable for complex, data-driven analyses. LepNet will build on the existing data portal (SCAN) in consolidating data on Lepidoptera to the evolution of lepidopteran herbivores in North America. Access to these data will be increased through integration with iDigBio. Data for a broad range of research, including the evolutionary ecology of Lepidoptera and their host plants in the context of global change processes affecting biogeographic distributions will be generated. The LepXPLOR! program will spearhead education and outreach efforts for 67 existing programs, engaging a diverse, nationwide workforce of 400+ students and 3,500+ volunteers. Overall, LepNet will generate a sustainable social-research network dedicated to the creation and maintenance of a digital collection of North American Lepidoptera specimens (http://www.lep-net.org/)."
"1634726","3D Motion and Swarm Control of Magnetically Propelled Microrobots for in vivo Particulate Drug Delivery","CMMI","Dynamics, Control and System D","09/01/2016","08/02/2016","MinJun Kim","PA","Drexel University","Standard Grant","Jordan Berg","02/28/2017","$289,431.00","","mjkim@lyle.smu.edu","1505 Race St, 10th Floor","Philadelphia","PA","191021119","2158955849","ENG","7569","030E, 034E, 8024","$0.00","This project will demonstrate the use of rotating magnetic fields to propel and steer magnetic microswimmers for medical applications such as drug delivery. Unlike previous work in this area, this project considers swarms of microswimmers instead of single vehicles, and allows fluids with non-ideal behavior characteristic of, for example, mucus. The results will be experimentally validated using a controllable synthetic biofluid. The results will guide future development of control systems for microrobotics, and advance towards practically controllable magnetic microswimmers in vivo. A complimentary outreach program will provide and cultivate a unique, interdisciplinary training environment for K-12, undergraduate and graduate students, exploiting eye-catching microswimmer control, drug delivery, and haptic devices.<br/><br/>The PI has recently demonstrated that achiral magnetic rigid geometries are capable of propulsion when rotated by a magnetic field. This project builds upon that demonstration, by formulating the motion control problem in the setting of stochastic differential equations, in order to create a stochastic control system for 3D motion and swarm control of magnetic microswimmers. Motion control of microswimmers is accomplished with magnetic control and computer vision feedback. Notably, variations in the physical parameters of the individual microswimmers will be leveraged to address uncertainty in the fluid environment. The approach will be used to formulate control and coordination schemes for the motion of a large number of microswimmers in heterogeneous 2D and 3D workspaces, using motion planning and control frameworks that address issues such as controllability and optimality. The results will be experimentally validated in a non-Newtonian fluid with controllable parameters that simulates a biological environment."
"1561814","Statistical Methods for Differential Network Biology with Applications to Aging","DMS","NIGMS","07/01/2016","08/14/2018","Ali Shojaie","WA","University of Washington","Continuing Grant","Pedro Embid","06/30/2021","$1,188,872.00","Daniel Promislow, Mathias Drton","ashojaie@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","MPS","8047","4075","$0.00","Networks are widely used in molecular biology to model interactions among components of biological systems and gain insight into changes in biological mechanism associated with various diseases. Recent evidence suggests that changes in biological networks, including rewiring or disruption of key interactions, may be associated with the development of complex diseases. This research project is motivated by the study of changes in metabolic networks in association with evolution and aging. The project aims to develop new statistical machine learning methods to determine whether evolutionary changes are manifested through changes in how metabolites interact with each other in metabolic pathways. More broadly, the methodologies under development in this project and the accompanying software will provide novel tools for biomedical researchers to infer differential patterns of connectivity in molecular networks associated with complex diseases. <br/><br/>The methodologies under development in this project utilize the framework of graphical models to estimate interactions among components of biological networks and identify changes in such networks associated with evolution and aging. Probabilistic graphical models provide a general framework for modeling interactions among random variables. While recent methodological and theoretical advances have facilitated the applications of graphical models to analysis of high-dimensional biological networks, existing methods are not applicable to heterogeneous and non-Gaussian observations obtained from mass-spectrometry-based metabolomics profiling experiments in complex aging studies. The research project aims to bridge this gap by developing new statistical machine learning methods for learning graphical models from heterogeneous and non-Gaussian observations and inferring changes in graphical models in different subpopulations. In particular, the project will (i) develop a flexible framework for estimation of multiple graphical models from heterogeneous populations with complex structures, (ii) develop an inference framework for detecting differential connectivity in biological networks, and (iii) provide a general framework for estimation of non-Gaussian graphical models, with changes over time or experimental conditions. Together, these tools provide a comprehensive framework for differential network analysis and will advance the current state of statistical machine learning methods for the analysis of high-dimensional biological networks."
"1707498","III: Small: New Machine Learning Approaches for Modeling Time-to-Event Data","IIS","Info Integration & Informatics","08/28/2016","11/07/2016","Chandan Reddy","VA","Virginia Polytechnic Institute and State University","Standard Grant","Amarda Shehu","08/31/2019","$202,631.00","","reddy@cs.vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","CSE","7364","7364, 7923","$0.00","Due to the advancements in recent data collection technologies, different disciplines have attained the ability to not only accumulate a wide variety of data but also to monitor observations over longer periods of time. In many real-world applications, the primary goal of monitoring these observations is to better estimate the time for a particular event of interest to occur. Examples of these events include disease recurrence in healthcare, time to default in finance, device failure in engineering, etc. A major challenge with such time-to-event data is that it is often incomplete; some data instances are either removed or become unobservable over a period of time before the event occurs. Due to this missing piece of information, standard statistical and machine learning tools cannot readily be applied to analyze such data. Survival analysis methods, primarily developed by the statistics community, aim to model time-to-event data and are usually more effective compared to the standard prediction algorithms as they directly model the probability of occurrence of an event in contrast to assigning a nominal label to the data instance. More importantly, they can implicitly handle missing data. However, in many practical scenarios, the missing data challenges are compounded by several other related complexities such as the presence of correlations within the data, temporal dependencies across multiple instances (collected over a period of time), lack of available information from a single source, and difficulty in acquiring sufficient event data in a reasonable amount of time. Such data poses unique challenges to the field of predictive analytics and thus creates opportunities to develop new algorithms to tackle these issues.  This project provides innovative computational methods to assist novel scientific discoveries and bring practical transformational impact to the analysis and exploration of various time-to-event datasets and applications.  The proposed methods are primarily being evaluated in the context of biomedical data, but are applicable to various other forms of time-to-event data that is often seen in other disciplines such as social science, engineering, finance, and economics.<br/><br/>This project builds novel computational and analytical algorithms that can efficiently and accurately capture the underlying predictive patterns in time-to-event data. The project aims at building new algorithms for longitudinal data analysis, integrate multiple sources while building time-to-event models, and predict temporal events with limited amount of training data. Specifically, the research objectives are to develop the following: (i) Latent feature models that can capture the longitudinal dependencies underlying multiple outcomes over a period of time. Multiple independent regression models for various missing data time windows are learned and then unified into a multi-output regression model over the diverse output space using sparsity regularizers. (ii) Multi-source time-to-event models that can effectively integrate multiple sources of information and make predictions by incorporating prior knowledge about the instances and their relationships. (iii) Bayesian methods for early-stage event prediction to tackle the problem of lack of sufficient training data on events at early stages of studies (which is a common problem in such time-to-event data). All the methods proposed in this project are evaluated using real-world biomedical data including high-dimensional genomic data and heterogeneous electronic health records. In addition, the algorithms developed in this project will also be used to tackle the problem of student retention."
"1613152","Computational and Communication Efficient Distributed Statistical Methods with Theoretical Guarantees","DMS","STATISTICS, GOALI-Grnt Opp Acad Lia wIndus","09/01/2016","08/13/2018","Xiaoming Huo","GA","Georgia Tech Research Corporation","Continuing Grant","Gabor Szekely","08/31/2019","$475,000.00","","xiaoming@isye.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","MPS","1269, 1504","019Z","$0.00","In many contemporary data-analysis settings, it is expensive and/or infeasible to assume that the entire data set is available at a central location. In recent works of computational mathematics and machine learning, great strides have been made in distributed optimization and distributed learning (i.e., machine learning). On the other hand, classical statistical methodology, theory, and computation are typically based on the assumption that the entire data are available at a central location; this is a significant shortcoming in modern statistical knowledge. The statistical methodology and theory for distributed inference are underdeveloped. The PI will develop new distributed statistical methods that are computation and communication efficient. He will study the theoretical guarantees of these distributed statistical estimators. The applicability and need of these methods in a wide spectrum of application domains will be explored and demonstrated. This research can have impacts in healthcare, supply chain industries, retail and services, and many more. <br/><br/>Based on recent works in applied mathematics and machine learning, the PI is to explore theory, algorithms, and applications of statistical procedures that are developed for distributed data and aggregated inference (i.e., distributed inference), with considerations on the storage, computational complexity, and statistical properties of the relevant estimators. The project will develop practical models, statistical theory, and computationally efficient and provably correct algorithms that can help scientists to conduct more effective distributed data analysis. Statistical properties of these methods will be thoroughly studied, including analysis of asymptotic properties, simulation studies in finite sample cases, and establishment of effectiveness in some real applications. PhD students will be involved in the research. Course modules will be developed and made available publicly."
"1600218","Accelerated Dynamics of Surface Chemical Reactions","CBET","Proc Sys, Reac Eng & Mol Therm","09/01/2016","08/16/2016","Ramamurthy Ramprasad","CT","University of Connecticut","Standard Grant","Triantafillos Mountziaris","02/28/2018","$300,000.00","","ramprasad@gatech.edu","438 Whitney Road Ext.","Storrs","CT","062691133","8604863622","ENG","1403","7752","$0.00","1600218  PI: Ramprasad    <br/>Institution: University of Connecticut<br/>Title: Accelerated Dynamics of Surface Chemical Reactions<br/><br/>Surface chemical reactions are ubiquitous in natural phenomena and play a key role in several scientific disciplines, such as heterogeneous catalysis, crystal growth, and electrochemistry.  Achieving a molecular-level picture of the mechanisms and dynamical detail of surface chemical reactions though remains a daunting task. Current methods of inquiry, be it empirical or computational, provide us with only a limited view of this complex world. One way of achieving the requisite level of understanding is by the use of molecular dynamics (MD) simulations. These simulations can directly monitor the progression of reactions at surfaces at the molecular level as a function of a variety of relevant conditions. Nevertheless, significant gaps remain in present-day MD capabilities: they are either fast (but not versatile or accurate), e.g., those based on empirical force-fields, or they are versatile and accurate (but not efficient), e.g., those based on quantum mechanical (or ab initio) methods. The proposed work will exploit an adaptive machine learning scheme that can both accelerate ab initio MD simulations as well as create accurate force-fields (at no extra cost) on-the-fly. Timescales previously unreachable using quantum mechanical simulations may be accessed using this new paradigm (conceivably,milliseconds to seconds), while still preserving the fidelity of quantum mechanics.<br/><br/>The primary reason ab-initio MD simulations are slow is largely because of enormous redundancies that permeate present-day paradigms. Energies and forces, the ingredients necessary to perform MD simulations, are evaluated for every configuration that is visited, regardless of whether a new configuration is similar to a previously visited configuration. The basic premise underlying this proposal is that a methodology based on machine learning can be used to eliminate the significant effort involved in predicting atomic forces and energies of revisited or similar states within a local minimum, and at equivalent multiple local minima, thus eliminating an enormous amount of redundancies. Only when a truly new configuration is encountered is an ab initio scheme necessary; otherwise, the inexpensive machine learning algorithm is used to predict atomic forces and energies. In the former instance, the learning algorithm is retrained to include the new information, thus making the prediction scheme adaptive on-the-fly. The specific goals of the proposed research are: (1) The development of stand-alone force-fields just using the data accumulated during ab initio MD simulations; and (2) Application of this development to important model surface science problems, including surface adatom diffusion, surface oxidation, and surface catalytic reactions. Several educational initiatives are also planned, including new course offerings, workshops, online lectures, short courses, and symposia."
"1601957","Digitization TCN: Collaborative Research: Lepidoptera of North America Network: Documenting Diversity in the Largest Clade of Herbivores","DBI","Digitization","07/01/2016","05/04/2016","Jennifer Zaspel","IN","Purdue University","Standard Grant","Reed Beaman","06/30/2021","$114,702.00","","zaspelj@mpm.edu","Young Hall","West Lafayette","IN","479072114","7654941055","BIO","6895","6895","$0.00","Lepidoptera (butterflies and moths) are one of the most diverse groups of organisms on the planet: worldwide there are approximately 160,000 species, including around 14,300 species in North America. Moths and butterflies are a conspicuous component of terrestrial habitats and one of the most diverse groups of plant-feeding animals worldwide. This group insect includes species of great economic importance. Their juveniles feed on plants useful to humans, including grains, cotton, tobacco, and timber and shade trees. However, many of the adults are beneficial as pollinators and are icons of conservation as evidenced by Monarch butterflies. Given their economic importance and sheer beauty, butterflies and moths are one of the most abundant insect group in museum collections, but only a fraction of the approximately 15 million specimens in non-federal collections have had their specimen label information digitally recorded and accessible to researchers and educators. Of those specimens that have been digitized, fewer than 10% of the North American Lepidoptera species have sufficient, accessible occurrence data to make reliable predictions about habitat use, susceptibility to global change impacts, or other ecologically important interactions. This project will digitize and integrate existing, unconnected collections of lepidopterans to leverage the outstanding potential of this group of organisms for transformative research, training and outreach. <br/><br/>The Lepidoptera of North America Network (LepNet) comprises 26 research collections that will digitize approximately 2 million specimen records and integrate these with over 1 million existing records. LepNet will digitize 43,280 larval vial records with host plant data, making this the first significant digitization of larvae in North American collections. LepNet will produce ca. 82,000 high-quality images of exemplar species covering 60% of North American lepidopteran species. These images will enhance remote identifications and facilitate systematic, ecological, and global change research. In collaboration with Visipedia, LepNet will create LepSnap, a computer vision tool that can provide automated identifications to the species level. Museum volunteers and student researchers equipped with smartphones will image >132,000 additional research-quality images through LepSnap. Up to 5,000 lepidopteran species will be elevated to a ""research ready"" status suitable for complex, data-driven analyses. LepNet will build on the existing data portal (SCAN) in consolidating data on Lepidoptera to the evolution of lepidopteran herbivores in North America. Access to these data will be increased through integration with iDigBio. Data for a broad range of research, including the evolutionary ecology of Lepidoptera and their host plants in the context of global change processes affecting biogeographic distributions will be generated. The LepXPLOR! program will spearhead education and outreach efforts for 67 existing programs, engaging a diverse, nationwide workforce of 400+ students and 3,500+ volunteers. Overall, LepNet will generate a sustainable social-research network dedicated to the creation and maintenance of a digital collection of North American Lepidoptera specimens (http://www.lep-net.org/)."
"1712096","3D Motion and Swarm Control of Magnetically Propelled Microrobots for in vivo Particulate Drug Delivery","CMMI","Dynamics, Control and System D","09/01/2016","04/01/2019","MinJun Kim","TX","Southern Methodist University","Standard Grant","Jordan Berg","08/31/2021","$321,431.00","","mjkim@lyle.smu.edu","6425 BOAZ","Dallas","TX","752750302","2147682030","ENG","7569","030E, 034E, 116E, 8024, 9178, 9231, 9251","$0.00","This project will demonstrate the use of rotating magnetic fields to propel and steer magnetic microswimmers for medical applications such as drug delivery. Unlike previous work in this area, this project considers swarms of microswimmers instead of single vehicles, and allows fluids with non-ideal behavior characteristic of, for example, mucus. The results will be experimentally validated using a controllable synthetic biofluid. The results will guide future development of control systems for microrobotics, and advance towards practically controllable magnetic microswimmers in vivo. A complimentary outreach program will provide and cultivate a unique, interdisciplinary training environment for K-12, undergraduate and graduate students, exploiting eye-catching microswimmer control, drug delivery, and haptic devices.<br/><br/>The PI has recently demonstrated that achiral magnetic rigid geometries are capable of propulsion when rotated by a magnetic field. This project builds upon that demonstration, by formulating the motion control problem in the setting of stochastic differential equations, in order to create a stochastic control system for 3D motion and swarm control of magnetic microswimmers. Motion control of microswimmers is accomplished with magnetic control and computer vision feedback. Notably, variations in the physical parameters of the individual microswimmers will be leveraged to address uncertainty in the fluid environment. The approach will be used to formulate control and coordination schemes for the motion of a large number of microswimmers in heterogeneous 2D and 3D workspaces, using motion planning and control frameworks that address issues such as controllability and optimality. The results will be experimentally validated in a non-Newtonian fluid with controllable parameters that simulates a biological environment."
"1623669","I-Corps: Automated Postures Analysis for Ergonomic Risk","IIP","I-Corps","02/01/2016","01/20/2016","SangHyun Lee","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Steven Konsek","07/31/2016","$50,000.00","","shdpm@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","ENG","8023","","$0.00","Workers in industries like manufacturing, construction, retail, health care, and logistics are involved in physically demanding activities, dealing with awkward body postures and repetitive manual handling tasks that result in ergonomic injuries (e.g., musculoskeletal disorders).  For example, ergonomic injuries account for an average of 33% of nonfatal occupational injuries and illnesses in the U.S. These injuries are also associated with high costs (about $50 billion annually in the U.S.) to employers due to absenteeism, lost productivity, and increased health care, disability, and workers' compensation costs. To deal with such injuries, manual observation-based ergonomic assessments (e.g., checklists) have been widely used to identify awkward or/and repetitive working postures. However, manual observation methods are time-consuming, expensive and error prone, which makes them difficult to be easily applied to many workplaces. The need for trained analysts is also an obstacle to promote ergonomic assessments in workplaces. As a result, an effective and easily accessible means for ergonomic assessments is required to detect and minimize the risks of ergonomic injuries in a timely-manner. <br/><br/>The proposed computer vision-based automatic posture analysis approach processes video images of workers taken via ordinary video recording devices like smartphones, tablets, and off-the-shelf camcorders, and consequently evaluates the level of their ergonomic risk they have while performing workplace tasks. The proposed innovation is to make the current ergonomic risk assessment process efficient, affordable, and robust by minimizing time-consuming, expensive, and error prone manual observation. Image sequences of working postures have distinguishable patterns that can be used to differentiate safe and injury-prone postures. By learning these patterns, this  technology automatically identifies awkward postures on video recordings, enabling one to conduct ergonomic assessments in a timely manner without technical sophistication or skill. The proposed technical approach is also flexible and robust enough to deal with complex and crowded work environments. Particularly, this innovative virtual modeling approach to automatically create massive training datasets eliminates cumbersome data collection. In addition, the capability to differentiate different postures and realize rapid pose estimation with mobile devices enables the proposed approach to be applied to diverse ergonomic checklists in many industries. This innovation can provide an exciting path for many industries who suffer ergonomic injuries to reduce their burden of manual observation, ultimately opening a door toward the prevention of ergonomic injuries and the increase of productivity."
"1601124","Digitization TCN: Collaborative Research: Lepidoptera of North America Network: Documenting Diversity in the Largest Clade of Herbivores","DBI","Digitization","07/01/2016","05/04/2016","Naomi Pierce","MA","Harvard University","Standard Grant","Reed Beaman","06/30/2021","$179,204.00","","npierce@oeb.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","BIO","6895","6895","$0.00","Lepidoptera (butterflies and moths) are one of the most diverse groups of organisms on the planet: worldwide there are approximately 160,000 species, including around 14,300 species in North America. Moths and butterflies are a conspicuous component of terrestrial habitats and one of the most diverse groups of plant-feeding animals worldwide. This group insect includes species of great economic importance. Their juveniles feed on plants useful to humans, including grains, cotton, tobacco, and timber and shade trees. However, many of the adults are beneficial as pollinators and are icons of conservation as evidenced by Monarch butterflies. Given their economic importance and sheer beauty, butterflies and moths are one of the most abundant insect group in museum collections, but only a fraction of the approximately 15 million specimens in non-federal collections have had their specimen label information digitally recorded and accessible to researchers and educators. Of those specimens that have been digitized, fewer than 10% of the North American Lepidoptera species have sufficient, accessible occurrence data to make reliable predictions about habitat use, susceptibility to global change impacts, or other ecologically important interactions. This project will digitize and integrate existing, unconnected collections of lepidopterans to leverage the outstanding potential of this group of organisms for transformative research, training and outreach. <br/><br/>The Lepidoptera of North America Network (LepNet) comprises 26 research collections that will digitize approximately 2 million specimen records and integrate these with over 1 million existing records. LepNet will digitize 43,280 larval vial records with host plant data, making this the first significant digitization of larvae in North American collections. LepNet will produce ca. 82,000 high-quality images of exemplar species covering 60% of North American lepidopteran species. These images will enhance remote identifications and facilitate systematic, ecological, and global change research. In collaboration with Visipedia, LepNet will create LepSnap, a computer vision tool that can provide automated identifications to the species level. Museum volunteers and student researchers equipped with smartphones will image >132,000 additional research-quality images through LepSnap. Up to 5,000 lepidopteran species will be elevated to a ""research ready"" status suitable for complex, data-driven analyses. LepNet will build on the existing data portal (SCAN) in consolidating data on Lepidoptera to the evolution of lepidopteran herbivores in North America. Access to these data will be increased through integration with iDigBio. Data for a broad range of research, including the evolutionary ecology of Lepidoptera and their host plants in the context of global change processes affecting biogeographic distributions will be generated. The LepXPLOR! program will spearhead education and outreach efforts for 67 existing programs, engaging a diverse, nationwide workforce of 400+ students and 3,500+ volunteers. Overall, LepNet will generate a sustainable social-research network dedicated to the creation and maintenance of a digital collection of North American Lepidoptera specimens (http://www.lep-net.org/)."
"1600556","Digitization TCN: Collaborative Research: Lepidoptera of North America Network: Documenting Diversity in the Largest Clade of Herbivores","DBI","Digitization","07/01/2016","05/04/2016","Anthony Cognato","MI","Michigan State University","Standard Grant","Reed Beaman","06/30/2021","$226,389.00","","cognato@msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","BIO","6895","6895","$0.00","Lepidoptera (butterflies and moths) are one of the most diverse groups of organisms on the planet: worldwide there are approximately 160,000 species, including around 14,300 species in North America. Moths and butterflies are a conspicuous component of terrestrial habitats and one of the most diverse groups of plant-feeding animals worldwide. This group insect includes species of great economic importance. Their juveniles feed on plants useful to humans, including grains, cotton, tobacco, and timber and shade trees. However, many of the adults are beneficial as pollinators and are icons of conservation as evidenced by Monarch butterflies. Given their economic importance and sheer beauty, butterflies and moths are one of the most abundant insect group in museum collections, but only a fraction of the approximately 15 million specimens in non-federal collections have had their specimen label information digitally recorded and accessible to researchers and educators. Of those specimens that have been digitized, fewer than 10% of the North American Lepidoptera species have sufficient, accessible occurrence data to make reliable predictions about habitat use, susceptibility to global change impacts, or other ecologically important interactions. This project will digitize and integrate existing, unconnected collections of lepidopterans to leverage the outstanding potential of this group of organisms for transformative research, training and outreach. <br/><br/>The Lepidoptera of North America Network (LepNet) comprises 26 research collections that will digitize approximately 2 million specimen records and integrate these with over 1 million existing records. LepNet will digitize 43,280 larval vial records with host plant data, making this the first significant digitization of larvae in North American collections. LepNet will produce ca. 82,000 high-quality images of exemplar species covering 60% of North American lepidopteran species. These images will enhance remote identifications and facilitate systematic, ecological, and global change research. In collaboration with Visipedia, LepNet will create LepSnap, a computer vision tool that can provide automated identifications to the species level. Museum volunteers and student researchers equipped with smartphones will image >132,000 additional research-quality images through LepSnap. Up to 5,000 lepidopteran species will be elevated to a ""research ready"" status suitable for complex, data-driven analyses. LepNet will build on the existing data portal (SCAN) in consolidating data on Lepidoptera to the evolution of lepidopteran herbivores in North America. Access to these data will be increased through integration with iDigBio. Data for a broad range of research, including the evolutionary ecology of Lepidoptera and their host plants in the context of global change processes affecting biogeographic distributions will be generated. The LepXPLOR! program will spearhead education and outreach efforts for 67 existing programs, engaging a diverse, nationwide workforce of 400+ students and 3,500+ volunteers. Overall, LepNet will generate a sustainable social-research network dedicated to the creation and maintenance of a digital collection of North American Lepidoptera specimens (http://www.lep-net.org/)."
"1618931","CSR: Small: A Unified Approach Toward User-specific Improvements of Quality of Experience for Video Streaming","CNS","Special Projects - CNS, CSR-Computer Systems Research","10/01/2016","05/22/2017","Yao Liu","NY","SUNY at Binghamton","Standard Grant","Marilyn McClure","09/30/2020","$504,373.00","","yaoliu@binghamton.edu","4400 VESTAL PKWY E","BINGHAMTON","NY","139026000","6077776136","CSE","1714, 7354","7923, 9251","$0.00","The number of users streaming mobile video over the Internet has increased at an unprecedented rate throughout the past decade. Because video streaming is a mainstay of mobile computing, it is important that the best possible experience is delivered to users. Many mathematical algorithms have been proposed to improve quality in video-streaming-related domains. Typically, the parameters of these algorithms are established based on machine-centric indicators of video quality. Although researchers have attempted to connect these indicators with true user-perceived quality, in practice, there is often a disconnect. This project aims to improve directly-measurable indicators of user satisfaction in mobile video streaming by taking into account both individual user preferences as well as a user's tolerance for less than perfect quality in a specific video. <br/><br/>The proposed research will improve user-specific indicators by connecting problems in video streaming with problems in multi-task learning and collaborative filtering. These machine learning strategies are especially effective for prediction when large amounts of data are available. This effectiveness on large-scale learning fits well into this project's proposed context of improving user-facing indications of satisfaction. These indications include video abandonment, video session times, and collected navigation commands. Unlike user-surveys, these metrics can be collected at large scales through automated tooling in the video player. This research will investigate strategies that combine these large scale measurements with predictions from machine learning approaches toward selecting algorithm parameters that produce the most improvement in user-facing quality. This project will explore such parameter selection strategies in the context of improving user-perceived dynamic adaptive streaming quality. It will also explore such strategies to maintain a fixed level of user-facing quality while reducing mobile display power consumption via backlight scaling. Demonstrations of the approaches produced by this project will be featured in courses at the PI's institution and will be used to draw undergraduate interest toward computer science research."
"1601369","Digitization TCN: Collaborative Research: Lepidoptera of North America Network: Documenting Diversity in the Largest Clade of Herbivores","DBI","Digitization","07/01/2016","07/12/2017","Akito Kawahara","FL","University of Florida","Continuing Grant","Reed Beaman","06/30/2021","$362,839.00","Jaret Daniels","kawahara@flmnh.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","BIO","6895","6895","$0.00","Lepidoptera (butterflies and moths) are one of the most diverse groups of organisms on the planet: worldwide there are approximately 160,000 species, including around 14,300 species in North America. Moths and butterflies are a conspicuous component of terrestrial habitats and one of the most diverse groups of plant-feeding animals worldwide. This group insect includes species of great economic importance. Their juveniles feed on plants useful to humans, including grains, cotton, tobacco, and timber and shade trees. However, many of the adults are beneficial as pollinators and are icons of conservation as evidenced by Monarch butterflies. Given their economic importance and sheer beauty, butterflies and moths are one of the most abundant insect group in museum collections, but only a fraction of the approximately 15 million specimens in non-federal collections have had their specimen label information digitally recorded and accessible to researchers and educators. Of those specimens that have been digitized, fewer than 10% of the North American Lepidoptera species have sufficient, accessible occurrence data to make reliable predictions about habitat use, susceptibility to global change impacts, or other ecologically important interactions. This project will digitize and integrate existing, unconnected collections of lepidopterans to leverage the outstanding potential of this group of organisms for transformative research, training and outreach. <br/><br/>The Lepidoptera of North America Network (LepNet) comprises 26 research collections that will digitize approximately 2 million specimen records and integrate these with over 1 million existing records. LepNet will digitize 43,280 larval vial records with host plant data, making this the first significant digitization of larvae in North American collections. LepNet will produce ca. 82,000 high-quality images of exemplar species covering 60% of North American lepidopteran species. These images will enhance remote identifications and facilitate systematic, ecological, and global change research. In collaboration with Visipedia, LepNet will create LepSnap, a computer vision tool that can provide automated identifications to the species level. Museum volunteers and student researchers equipped with smartphones will image >132,000 additional research-quality images through LepSnap. Up to 5,000 lepidopteran species will be elevated to a ""research ready"" status suitable for complex, data-driven analyses. LepNet will build on the existing data portal (SCAN) in consolidating data on Lepidoptera to the evolution of lepidopteran herbivores in North America. Access to these data will be increased through integration with iDigBio. Data for a broad range of research, including the evolutionary ecology of Lepidoptera and their host plants in the context of global change processes affecting biogeographic distributions will be generated. The LepXPLOR! program will spearhead education and outreach efforts for 67 existing programs, engaging a diverse, nationwide workforce of 400+ students and 3,500+ volunteers. Overall, LepNet will generate a sustainable social-research network dedicated to the creation and maintenance of a digital collection of North American Lepidoptera specimens (http://www.lep-net.org/)."
"1600824","Digitization TCN: Collaborative Research: Lepidoptera of North America Network: Documenting Diversity in the Largest Clade of Herbivores","DBI","Digitization","07/01/2016","05/04/2016","Merrill Peterson","WA","Western Washington University","Standard Grant","Reed Beaman","06/30/2020","$12,132.00","","merrill.peterson@wwu.edu","516 High Street","Bellingham","WA","982259038","3606502884","BIO","6895","6895","$0.00","Lepidoptera (butterflies and moths) are one of the most diverse groups of organisms on the planet: worldwide there are approximately 160,000 species, including around 14,300 species in North America. Moths and butterflies are a conspicuous component of terrestrial habitats and one of the most diverse groups of plant-feeding animals worldwide. This group insect includes species of great economic importance. Their juveniles feed on plants useful to humans, including grains, cotton, tobacco, and timber and shade trees. However, many of the adults are beneficial as pollinators and are icons of conservation as evidenced by Monarch butterflies. Given their economic importance and sheer beauty, butterflies and moths are one of the most abundant insect group in museum collections, but only a fraction of the approximately 15 million specimens in non-federal collections have had their specimen label information digitally recorded and accessible to researchers and educators. Of those specimens that have been digitized, fewer than 10% of the North American Lepidoptera species have sufficient, accessible occurrence data to make reliable predictions about habitat use, susceptibility to global change impacts, or other ecologically important interactions. This project will digitize and integrate existing, unconnected collections of lepidopterans to leverage the outstanding potential of this group of organisms for transformative research, training and outreach. <br/><br/>The Lepidoptera of North America Network (LepNet) comprises 26 research collections that will digitize approximately 2 million specimen records and integrate these with over 1 million existing records. LepNet will digitize 43,280 larval vial records with host plant data, making this the first significant digitization of larvae in North American collections. LepNet will produce ca. 82,000 high-quality images of exemplar species covering 60% of North American lepidopteran species. These images will enhance remote identifications and facilitate systematic, ecological, and global change research. In collaboration with Visipedia, LepNet will create LepSnap, a computer vision tool that can provide automated identifications to the species level. Museum volunteers and student researchers equipped with smartphones will image >132,000 additional research-quality images through LepSnap. Up to 5,000 lepidopteran species will be elevated to a ""research ready"" status suitable for complex, data-driven analyses. LepNet will build on the existing data portal (SCAN) in consolidating data on Lepidoptera to the evolution of lepidopteran herbivores in North America. Access to these data will be increased through integration with iDigBio. Data for a broad range of research, including the evolutionary ecology of Lepidoptera and their host plants in the context of global change processes affecting biogeographic distributions will be generated. The LepXPLOR! program will spearhead education and outreach efforts for 67 existing programs, engaging a diverse, nationwide workforce of 400+ students and 3,500+ volunteers. Overall, LepNet will generate a sustainable social-research network dedicated to the creation and maintenance of a digital collection of North American Lepidoptera specimens (http://www.lep-net.org/)."
"1601461","Digitization TCN: Collaborative Research: Lepidoptera of North America Network: Documenting Diversity in the Largest Clade of Herbivores","DBI","Digitization","07/01/2016","05/04/2016","Robin Thomson","MN","University of Minnesota-Twin Cities","Standard Grant","Reed Beaman","06/30/2020","$97,048.00","Ralph Holzenthal","thom1514@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","BIO","6895","6895","$0.00","Lepidoptera (butterflies and moths) are one of the most diverse groups of organisms on the planet: worldwide there are approximately 160,000 species, including around 14,300 species in North America. Moths and butterflies are a conspicuous component of terrestrial habitats and one of the most diverse groups of plant-feeding animals worldwide. This group insect includes species of great economic importance. Their juveniles feed on plants useful to humans, including grains, cotton, tobacco, and timber and shade trees. However, many of the adults are beneficial as pollinators and are icons of conservation as evidenced by Monarch butterflies. Given their economic importance and sheer beauty, butterflies and moths are one of the most abundant insect group in museum collections, but only a fraction of the approximately 15 million specimens in non-federal collections have had their specimen label information digitally recorded and accessible to researchers and educators. Of those specimens that have been digitized, fewer than 10% of the North American Lepidoptera species have sufficient, accessible occurrence data to make reliable predictions about habitat use, susceptibility to global change impacts, or other ecologically important interactions. This project will digitize and integrate existing, unconnected collections of lepidopterans to leverage the outstanding potential of this group of organisms for transformative research, training and outreach. <br/><br/>The Lepidoptera of North America Network (LepNet) comprises 26 research collections that will digitize approximately 2 million specimen records and integrate these with over 1 million existing records. LepNet will digitize 43,280 larval vial records with host plant data, making this the first significant digitization of larvae in North American collections. LepNet will produce ca. 82,000 high-quality images of exemplar species covering 60% of North American lepidopteran species. These images will enhance remote identifications and facilitate systematic, ecological, and global change research. In collaboration with Visipedia, LepNet will create LepSnap, a computer vision tool that can provide automated identifications to the species level. Museum volunteers and student researchers equipped with smartphones will image >132,000 additional research-quality images through LepSnap. Up to 5,000 lepidopteran species will be elevated to a ""research ready"" status suitable for complex, data-driven analyses. LepNet will build on the existing data portal (SCAN) in consolidating data on Lepidoptera to the evolution of lepidopteran herbivores in North America. Access to these data will be increased through integration with iDigBio. Data for a broad range of research, including the evolutionary ecology of Lepidoptera and their host plants in the context of global change processes affecting biogeographic distributions will be generated. The LepXPLOR! program will spearhead education and outreach efforts for 67 existing programs, engaging a diverse, nationwide workforce of 400+ students and 3,500+ volunteers. Overall, LepNet will generate a sustainable social-research network dedicated to the creation and maintenance of a digital collection of North American Lepidoptera specimens (http://www.lep-net.org/)."
"1602081","Digitization TCN: Collaborative Research: Lepidoptera of North America Network: Documenting Diversity in the Largest Clade of Herbivores","DBI","Digitization","07/01/2016","08/06/2019","Neil Cobb","AZ","Northern Arizona University","Continuing Grant","Reed Beaman","06/30/2021","$677,212.00","Ben Brandt","neil.cobb@nau.edu","ARD Building #56, Suite 240","Flagstaff","AZ","860110001","9285230886","BIO","6895","6895","$0.00","Lepidoptera (butterflies and moths) are one of the most diverse groups of organisms on the planet: worldwide there are approximately 160,000 species, including around 14,300 species in North America. Moths and butterflies are a conspicuous component of terrestrial habitats and one of the most diverse groups of plant-feeding animals worldwide. This group insect includes species of great economic importance. Their juveniles feed on plants useful to humans, including grains, cotton, tobacco, and timber and shade trees. However, many of the adults are beneficial as pollinators and are icons of conservation as evidenced by Monarch butterflies. Given their economic importance and sheer beauty, butterflies and moths are one of the most abundant insect group in museum collections, but only a fraction of the approximately 15 million specimens in non-federal collections have had their specimen label information digitally recorded and accessible to researchers and educators. Of those specimens that have been digitized, fewer than 10% of the North American Lepidoptera species have sufficient, accessible occurrence data to make reliable predictions about habitat use, susceptibility to global change impacts, or other ecologically important interactions. This project will digitize and integrate existing, unconnected collections of lepidopterans to leverage the outstanding potential of this group of organisms for transformative research, training and outreach. <br/><br/>The Lepidoptera of North America Network (LepNet) comprises 26 research collections that will digitize approximately 2 million specimen records and integrate these with over 1 million existing records. LepNet will digitize 43,280 larval vial records with host plant data, making this the first significant digitization of larvae in North American collections. LepNet will produce ca. 82,000 high-quality images of exemplar species covering 60% of North American lepidopteran species. These images will enhance remote identifications and facilitate systematic, ecological, and global change research. In collaboration with Visipedia, LepNet will create LepSnap, a computer vision tool that can provide automated identifications to the species level. Museum volunteers and student researchers equipped with smartphones will image >132,000 additional research-quality images through LepSnap. Up to 5,000 lepidopteran species will be elevated to a ""research ready"" status suitable for complex, data-driven analyses. LepNet will build on the existing data portal (SCAN) in consolidating data on Lepidoptera to the evolution of lepidopteran herbivores in North America. Access to these data will be increased through integration with iDigBio. Data for a broad range of research, including the evolutionary ecology of Lepidoptera and their host plants in the context of global change processes affecting biogeographic distributions will be generated. The LepXPLOR! program will spearhead education and outreach efforts for 67 existing programs, engaging a diverse, nationwide workforce of 400+ students and 3,500+ volunteers. Overall, LepNet will generate a sustainable social-research network dedicated to the creation and maintenance of a digital collection of North American Lepidoptera specimens (http://www.lep-net.org/)."
"1600391","Tools for Positivity in Algebraic Combinatorics","DMS","Combinatorics","07/01/2016","07/12/2018","Jonah Blasiak","PA","Drexel University","Continuing Grant","Tomek Bartoszynski","06/30/2020","$223,985.00","","jblasiak@gmail.com","1505 Race St, 10th Floor","Philadelphia","PA","191021119","2158955849","MPS","7970","","$0.00","Nonnegative integer invariants provide an important way of understanding complicated algebraic or geometric objects. Examples include the degree of a polynomial and the number of holes of a surface. The goal of this project is develop general methods to obtain a detailed understanding of nonnegative integer invariants arising in several different areas of mathematics. This may form the foundation for developments in quantum information theory, knot theory, physics, and signal processing.  In particular, this project offers potential new insights into the tensor decomposition problem, which is essentially the problem of recovering individual signals from a mixture of signals and has applications in medicine, computer vision, chemistry, and fast matrix multiplication.<br/><br/>Positivity problems in algebraic combinatorics ask to find positive combinatorial formulae for nonnegative quantities arising in geometry and representation theory. The goal of this project is to develop tools to solve positivity problems arising in two areas of active research, Macdonald theory and geometric complexity theory. Macdonald polynomials are a two-parameter family of symmetric polynomials, which have ties to many areas including geometry, physics, and knot theory. A major breakthrough in this area came with the proof of the Macdonald positivity conjecture, which showed that important structure coefficients related to Macdonald polynomials are nonnegative. It remains a fundamental open question to give a positive combinatorial interpretation of these coefficients. Geometric complexity theory is an approach to P versus NP and related problems in complexity theory using algebraic geometry and representation theory.  A fundamental problem in representation theory, believed to be important for this approach, is the Kronecker problem, which asks for a positive combinatorial formula for decomposing the tensor product of two irreducible representations of the symmetric group into irreducibles. This project will further develop the theory of noncommutative Schur functions, a powerful tool for solving positivity problems, particularly focusing on applications to Macdonald polynomials and the Kronecker problem."
"1601443","Digitization TCN: Collaborative Research: Lepidoptera of North America Network: Documenting Diversity in the Largest Clade of Herbivores","DBI","Digitization","07/01/2016","05/04/2016","Lynn Kimsey","CA","University of California-Davis","Standard Grant","Reed Beaman","06/30/2020","$232,511.00","","lskimsey@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","BIO","6895","6895","$0.00","Lepidoptera (butterflies and moths) are one of the most diverse groups of organisms on the planet: worldwide there are approximately 160,000 species, including around 14,300 species in North America. Moths and butterflies are a conspicuous component of terrestrial habitats and one of the most diverse groups of plant-feeding animals worldwide. This group insect includes species of great economic importance. Their juveniles feed on plants useful to humans, including grains, cotton, tobacco, and timber and shade trees. However, many of the adults are beneficial as pollinators and are icons of conservation as evidenced by Monarch butterflies. Given their economic importance and sheer beauty, butterflies and moths are one of the most abundant insect group in museum collections, but only a fraction of the approximately 15 million specimens in non-federal collections have had their specimen label information digitally recorded and accessible to researchers and educators. Of those specimens that have been digitized, fewer than 10% of the North American Lepidoptera species have sufficient, accessible occurrence data to make reliable predictions about habitat use, susceptibility to global change impacts, or other ecologically important interactions. This project will digitize and integrate existing, unconnected collections of lepidopterans to leverage the outstanding potential of this group of organisms for transformative research, training and outreach. <br/><br/>The Lepidoptera of North America Network (LepNet) comprises 26 research collections that will digitize approximately 2 million specimen records and integrate these with over 1 million existing records. LepNet will digitize 43,280 larval vial records with host plant data, making this the first significant digitization of larvae in North American collections. LepNet will produce ca. 82,000 high-quality images of exemplar species covering 60% of North American lepidopteran species. These images will enhance remote identifications and facilitate systematic, ecological, and global change research. In collaboration with Visipedia, LepNet will create LepSnap, a computer vision tool that can provide automated identifications to the species level. Museum volunteers and student researchers equipped with smartphones will image >132,000 additional research-quality images through LepSnap. Up to 5,000 lepidopteran species will be elevated to a ""research ready"" status suitable for complex, data-driven analyses. LepNet will build on the existing data portal (SCAN) in consolidating data on Lepidoptera to the evolution of lepidopteran herbivores in North America. Access to these data will be increased through integration with iDigBio. Data for a broad range of research, including the evolutionary ecology of Lepidoptera and their host plants in the context of global change processes affecting biogeographic distributions will be generated. The LepXPLOR! program will spearhead education and outreach efforts for 67 existing programs, engaging a diverse, nationwide workforce of 400+ students and 3,500+ volunteers. Overall, LepNet will generate a sustainable social-research network dedicated to the creation and maintenance of a digital collection of North American Lepidoptera specimens (http://www.lep-net.org/)."
"1600774","Digitization TCN: Collaborative Research: Lepidoptera of North America Network: Documenting Diversity in the Largest Clade of Herbivores","DBI","Digitization","07/01/2016","05/04/2016","Derek Sikes","AK","University of Alaska Fairbanks Campus","Standard Grant","Reed Beaman","12/31/2020","$90,708.00","","dssikes@alaska.edu","West Ridge Research Bldg 008","Fairbanks","AK","997757880","9074747301","BIO","6895","6895","$0.00","Lepidoptera (butterflies and moths) are one of the most diverse groups of organisms on the planet: worldwide there are approximately 160,000 species, including around 14,300 species in North America. Moths and butterflies are a conspicuous component of terrestrial habitats and one of the most diverse groups of plant-feeding animals worldwide. This group insect includes species of great economic importance. Their juveniles feed on plants useful to humans, including grains, cotton, tobacco, and timber and shade trees. However, many of the adults are beneficial as pollinators and are icons of conservation as evidenced by Monarch butterflies. Given their economic importance and sheer beauty, butterflies and moths are one of the most abundant insect group in museum collections, but only a fraction of the approximately 15 million specimens in non-federal collections have had their specimen label information digitally recorded and accessible to researchers and educators. Of those specimens that have been digitized, fewer than 10% of the North American Lepidoptera species have sufficient, accessible occurrence data to make reliable predictions about habitat use, susceptibility to global change impacts, or other ecologically important interactions. This project will digitize and integrate existing, unconnected collections of lepidopterans to leverage the outstanding potential of this group of organisms for transformative research, training and outreach. <br/><br/>The Lepidoptera of North America Network (LepNet) comprises 26 research collections that will digitize approximately 2 million specimen records and integrate these with over 1 million existing records. LepNet will digitize 43,280 larval vial records with host plant data, making this the first significant digitization of larvae in North American collections. LepNet will produce ca. 82,000 high-quality images of exemplar species covering 60% of North American lepidopteran species. These images will enhance remote identifications and facilitate systematic, ecological, and global change research. In collaboration with Visipedia, LepNet will create LepSnap, a computer vision tool that can provide automated identifications to the species level. Museum volunteers and student researchers equipped with smartphones will image >132,000 additional research-quality images through LepSnap. Up to 5,000 lepidopteran species will be elevated to a ""research ready"" status suitable for complex, data-driven analyses. LepNet will build on the existing data portal (SCAN) in consolidating data on Lepidoptera to the evolution of lepidopteran herbivores in North America. Access to these data will be increased through integration with iDigBio. Data for a broad range of research, including the evolutionary ecology of Lepidoptera and their host plants in the context of global change processes affecting biogeographic distributions will be generated. The LepXPLOR! program will spearhead education and outreach efforts for 67 existing programs, engaging a diverse, nationwide workforce of 400+ students and 3,500+ volunteers. Overall, LepNet will generate a sustainable social-research network dedicated to the creation and maintenance of a digital collection of North American Lepidoptera specimens (http://www.lep-net.org/)."
"1637039","Collaborative Research: A Visual System for Autonomous Foraminifera Identification","OCE","OCEAN TECH & INTERDISC COORDIN","08/01/2016","04/22/2016","Edgar Lobaton","NC","North Carolina State University","Standard Grant","Kandace Binkley","07/31/2019","$173,659.00","","edgar.lobaton@ncsu.edu","2701 Sullivan DR STE 240","Raleigh","NC","276950001","9195152444","GEO","1680","7916","$0.00","The goal of this project is to develop an automated system for identification of foraminifera (single-celled organisms with shells). Currently undergraduate workers are often employed to hand pick several thousands of specimens from ocean sediments for each study. This is tedious and time consuming work. By automating the bulk of the identification process, user expertise can be focused on verification and identification of subtle differences.<br/><br/>A visual identification system will be developed in order to automate the identification of target microorganisms. The visual system will incorporate a controllable LED lighting ring used to capture images by illuminating the specimens from several directions, mimicking an important step in the traditional identification process. These images will be used to create a 3D model of the organism in real-time within a second. Computer vision and pattern recognition techniques will be tuned to acceptable recognition rates set by feedback from an expert in paleoceanography who will also provide labeled samples for training and validation. The initial proof of concept study will focus on identifying six species of planktonic foraminifera, and their morphotypes, that are widely used by paleoceanographers."
"1637023","Collaborative Research: A Visual System for Autonomous Foraminifera Identification","OCE","OCEAN TECH & INTERDISC COORDIN","08/01/2016","04/22/2016","Thomas Marchitto","CO","University of Colorado at Boulder","Standard Grant","Kandace Binkley","07/31/2018","$63,713.00","","tom.marchitto@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","GEO","1680","7916","$0.00","The goal of this project is to develop an automated system for identification of foraminifera (single-celled organisms with shells). Currently undergraduate workers are often employed to hand pick several thousands of specimens from ocean sediments for each study. This is tedious and time consuming work. By automating the bulk of the identification process, user expertise can be focused on verification and identification of subtle differences.<br/><br/>A visual identification system will be developed in order to automate the identification of target microorganisms. The visual system will incorporate a controllable LED lighting ring used to capture images by illuminating the specimens from several directions, mimicking an important step in the traditional identification process. These images will be used to create a 3D model of the organism in real-time within a second. Computer vision and pattern recognition techniques will be tuned to acceptable recognition rates set by feedback from an expert in paleoceanography who will also provide labeled samples for training and validation. The initial proof of concept study will focus on identifying six species of planktonic foraminifera, and their morphotypes, that are widely used by paleoceanographers."
"1601275","Digitization TCN: Collaborative Research: Lepidoptera of North America Network: Documenting Diversity in the Largest Clade of Herbivores","DBI","Digitization","07/01/2016","05/04/2016","Frank Krell","CO","Denver Museum of Nature and Science","Standard Grant","Reed Beaman","06/30/2020","$45,681.00","Jeff Stephenson","Frank.Krell@dmns.org","2001 Colorado Boulevard","Denver","CO","802055732","3033708304","BIO","6895","6895","$0.00","Lepidoptera (butterflies and moths) are one of the most diverse groups of organisms on the planet: worldwide there are approximately 160,000 species, including around 14,300 species in North America. Moths and butterflies are a conspicuous component of terrestrial habitats and one of the most diverse groups of plant-feeding animals worldwide. This group insect includes species of great economic importance. Their juveniles feed on plants useful to humans, including grains, cotton, tobacco, and timber and shade trees. However, many of the adults are beneficial as pollinators and are icons of conservation as evidenced by Monarch butterflies. Given their economic importance and sheer beauty, butterflies and moths are one of the most abundant insect group in museum collections, but only a fraction of the approximately 15 million specimens in non-federal collections have had their specimen label information digitally recorded and accessible to researchers and educators. Of those specimens that have been digitized, fewer than 10% of the North American Lepidoptera species have sufficient, accessible occurrence data to make reliable predictions about habitat use, susceptibility to global change impacts, or other ecologically important interactions. This project will digitize and integrate existing, unconnected collections of lepidopterans to leverage the outstanding potential of this group of organisms for transformative research, training and outreach. <br/><br/>The Lepidoptera of North America Network (LepNet) comprises 26 research collections that will digitize approximately 2 million specimen records and integrate these with over 1 million existing records. LepNet will digitize 43,280 larval vial records with host plant data, making this the first significant digitization of larvae in North American collections. LepNet will produce ca. 82,000 high-quality images of exemplar species covering 60% of North American lepidopteran species. These images will enhance remote identifications and facilitate systematic, ecological, and global change research. In collaboration with Visipedia, LepNet will create LepSnap, a computer vision tool that can provide automated identifications to the species level. Museum volunteers and student researchers equipped with smartphones will image >132,000 additional research-quality images through LepSnap. Up to 5,000 lepidopteran species will be elevated to a ""research ready"" status suitable for complex, data-driven analyses. LepNet will build on the existing data portal (SCAN) in consolidating data on Lepidoptera to the evolution of lepidopteran herbivores in North America. Access to these data will be increased through integration with iDigBio. Data for a broad range of research, including the evolutionary ecology of Lepidoptera and their host plants in the context of global change processes affecting biogeographic distributions will be generated. The LepXPLOR! program will spearhead education and outreach efforts for 67 existing programs, engaging a diverse, nationwide workforce of 400+ students and 3,500+ volunteers. Overall, LepNet will generate a sustainable social-research network dedicated to the creation and maintenance of a digital collection of North American Lepidoptera specimens (http://www.lep-net.org/)."
"1601164","Digitization TCN: Collaborative Research: Lepidoptera of North America Network: Documenting Diversity in the Largest Clade of Herbivores","DBI","Digitization","07/01/2016","05/04/2016","Richard Brown","MS","Mississippi State University","Standard Grant","Reed Beaman","06/30/2020","$208,729.00","","moth@ra.msstate.edu","PO Box 6156","MISSISSIPPI STATE","MS","397629662","6623257404","BIO","6895","6895","$0.00","Lepidoptera (butterflies and moths) are one of the most diverse groups of organisms on the planet: worldwide there are approximately 160,000 species, including around 14,300 species in North America. Moths and butterflies are a conspicuous component of terrestrial habitats and one of the most diverse groups of plant-feeding animals worldwide. This group insect includes species of great economic importance. Their juveniles feed on plants useful to humans, including grains, cotton, tobacco, and timber and shade trees. However, many of the adults are beneficial as pollinators and are icons of conservation as evidenced by Monarch butterflies. Given their economic importance and sheer beauty, butterflies and moths are one of the most abundant insect group in museum collections, but only a fraction of the approximately 15 million specimens in non-federal collections have had their specimen label information digitally recorded and accessible to researchers and educators. Of those specimens that have been digitized, fewer than 10% of the North American Lepidoptera species have sufficient, accessible occurrence data to make reliable predictions about habitat use, susceptibility to global change impacts, or other ecologically important interactions. This project will digitize and integrate existing, unconnected collections of lepidopterans to leverage the outstanding potential of this group of organisms for transformative research, training and outreach. <br/><br/>The Lepidoptera of North America Network (LepNet) comprises 26 research collections that will digitize approximately 2 million specimen records and integrate these with over 1 million existing records. LepNet will digitize 43,280 larval vial records with host plant data, making this the first significant digitization of larvae in North American collections. LepNet will produce ca. 82,000 high-quality images of exemplar species covering 60% of North American lepidopteran species. These images will enhance remote identifications and facilitate systematic, ecological, and global change research. In collaboration with Visipedia, LepNet will create LepSnap, a computer vision tool that can provide automated identifications to the species level. Museum volunteers and student researchers equipped with smartphones will image >132,000 additional research-quality images through LepSnap. Up to 5,000 lepidopteran species will be elevated to a ""research ready"" status suitable for complex, data-driven analyses. LepNet will build on the existing data portal (SCAN) in consolidating data on Lepidoptera to the evolution of lepidopteran herbivores in North America. Access to these data will be increased through integration with iDigBio. Data for a broad range of research, including the evolutionary ecology of Lepidoptera and their host plants in the context of global change processes affecting biogeographic distributions will be generated. The LepXPLOR! program will spearhead education and outreach efforts for 67 existing programs, engaging a diverse, nationwide workforce of 400+ students and 3,500+ volunteers. Overall, LepNet will generate a sustainable social-research network dedicated to the creation and maintenance of a digital collection of North American Lepidoptera specimens (http://www.lep-net.org/)."
"1557499","Neural Bases of Song Preference and Reproductive Behavior in a Female Songbird","IOS","STATISTICS, Cross-BIO Activities, MSPA-INTERDISCIPLINARY, Modulation","06/01/2016","05/31/2016","Marc F. Schmidt","PA","University of Pennsylvania","Standard Grant","Edda Thiels","05/31/2020","$800,000.00","Kostas Daniilidis","marcshm@sas.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","BIO","1269, 7275, 7454, 7714","1096, 1228, 8007, 8091, 9178, 9179","$0.00","For many decades, neuroscientists and evolutionary biologists have been interested in the mechanics and function of the songbird's ""song system"": the interconnected neural circuit that connects the higher-order auditory areas in the brain with the motor circuits in the brainstem that drive behavior. This work predominantly has focused on how the song system allows male songbirds to learn and produce song. The role of this circuit in female songbirds, which do not sing, has largely been ignored. Rather than acting as a circuit that generates vocal behavior, this work investigates the hypothesis that the ""song system"" in females serves to organize preferences for males' songs and guides their behavioral reactions to song in the form of a copulation solicitation display that ensures survival of the species. The project capitalize on the robustness, selectivity, and social malleability of the copulatory behavior in the brown-headed cowbird, to investigate how the song system transforms a sensory stimulus (the song) into a motor command that controls a postural response. The project also provides opportunities for undergraduate and graduate students to engage in interdisciplinary research, and it includes science education activities aimed at elementary school children as well as a comprehensive summer course in neuroscience for high school students.<br/><br/>The proposed work integrates disparate fields of science, including neuroscience, behavior, and engineering to provide unique insight into the evolution of neural circuits that control behavior. In the first aim, the investigators use a combination of classic pathway tracing techniques and recently developed transsynaptic tracer (vesicular stomatitis virus) to map the connectivity from the forebrain to the individual muscle groups that are activated during the production of a copulation solicitation display (CSD). In the second aim, the investigators record neural activity in forebrain song control nuclei HVC and RA during the production of CSD in female cowbirds to quantify the nature of the forebrain motor commands that control this highly selective sexual behavior. To evaluate the relationship between recorded neural activity patterns and the behavior, we will use a computer vision approach to quantify the copulatory behavior. In the final aim of the proposal, the investigators record neural responses to song in higher-order auditory forebrain areas (NCM, NIf, CM) within the context of CSD production. These experiments serve to test the hypothesis that these forebrain areas, which have known projections to song control nuclei, encode song valence and provide a direct link between song quality and the females' behavioral response. The neural and behavioral data will be made available at public internet site dedicated to Song Bird Science."
"1523875","NSF Postdoctoral Fellowship in Biology FY 2015","DBI","Broadening Participation of Gr","03/01/2016","08/27/2015","Michael Gil","FL","Gil                     Michael","Fellowship","Amanda Simcox","02/28/2018","$138,000.00","","","","Gainesville","FL","326014387","","BIO","1157","7137","$0.00","This action funds an NSF Postdoctoral Research Fellowship in Biology for FY 2015, Broadening Participation. The fellowship supports a research and training plan in a host laboratory for the Fellow and a plan to broaden participation of groups under-represented in science.  The title of the research plan for this fellowship to Michael Gil is ""Effects of prey social context on predator-prey interactions and community stability in coral reefs."" The host institution for this fellowship is The University of California at Davis, and the sponsoring scientists include Andy Sih (primary), Marissa Baskett, and Mullica Jaroensutasinee (Walailak University, Thailand).<br/><br/>In a variety of ecosystems, prey have been shown to rapidly propagate danger reactions and signals, across species and wide spatial ranges, by incidentally providing information to one another, e.g., by evading a predator. Prey behavior that results from this social information may have important consequences for predator-prey interactions and, ultimately, the structure of the greater community. In coral reefs, where mixed-species prey fishes consume algae that drive coral declines, social information shared among prey could influence global coral reef degradation to alga-dominated states. The fellowship research uses field experimentation, assisted by computer vision technology, combined with mathematical simulation modeling to examine the behavioral and ecological effects of social information in mixed-species reef fish assemblages. This integrated approach is revealing important insights regarding the role of social information in the dynamics and function of animal communities, a topic of general conservation concern that is largely unexplored in most taxa. Further, it elucidates mechanisms underlying community stability in coral reefs, a topic that continues to perplex researchers.<br/><br/>Training goals include emphasis on quantitative methods and the study of animal behavior to formulate and parameterize a mathematical model for predicting the effects of social information on predator, prey, and resource dynamics. Public outreach includes creating micro-narrative films that educate a diverse public audience of non-scientists on core concepts in ecology/animal behavior and on what science and a career therein actually entail. These products are to be distributed through a variety of outlets, including an interactive public science communication website (co-administered with students) and lectures given to high schools with large populations of under-represented students. An international collaboration is being established between UC Davis and Walailak University (Thailand) whose benefits include ongoing ecological monitoring in Thailand and informing conservation efforts in socioeconomically invaluable coral reef ecosystems."
"1552309","CAREER: Learning the Chromatin Network from ChIP-Seq Data","DBI","IIBR: Infrastructure Innovatio, ADVANCES IN BIO INFORMATICS, Unallocated Program Costs","07/01/2016","09/12/2019","Su-In Lee","WA","University of Washington","Continuing Grant","Peter McCartney","06/30/2021","$768,269.00","","suinlee@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","BIO","084Y, 1165, 9199","1045, 1165","$0.00","Each cell of an organism shares the same nuclear DNA sequence (genome), but different types of cells turn on different sets of genes to carry out their unique functions. To correctly turn on and off the needed sets of genes requires coordinated action by several hundred regulatory molecules.  The regulators interact with the genome and with each other: understanding how this happens is one of the most important questions biological researchers are trying to answer. This project will use chromatin immunoprecipitation-sequencing (ChIP-Seq) data, which measures where a particular regulator is located on the genome: this may be zero, one or a few hundred positions out of the billions possible in a genome. Regulators often act together and one may change the action of another, so another part of the proposed work is to discover sites where regulators are co-localized. However, co-localization alone does not show whether regulators interact directly, or indirectly through an intermediate. The overall goal of this project is to develop a computational framework that identifies direct and indirect interactions among regulators, using thousands of ChIP-seq datasets. This framework will develop novel statistical and machine learning techniques to overcome the limitations of existing methods. The techniques that are developed will be applied to answer the following fundamental questions: How do hundreds of regulators interact with each other to regulate the genome? How do these regulator interactions differ across cell types? How do these interactions differ across different species (human, mouse, fly and worm)? The implementation of the new methods will be made publicly available to help many other scientists to study how the human genome works. This project is interdisciplinary in nature and has significant emphasis on interdisciplinary education, through project courses and outreach activities.<br/><br/><br/>Identifying the interactions among chromatin regulators, such as transcription factors and histone modifications, is of paramount importance to understand genome regulation. To infer this network of interactions, this research will compare multiple chromatin immunoprecipitation-sequencing (ChIP-seq) datasets, each measuring genome-wide localization of a chromatin regulator. Co-localization may indicate that two regulators interact directly or indirectly through transitive interactions. To identify direct interactions, the proposal aims to develop novel network inference methods to infer conditional dependence relationships (i.e., correlation not explained via any other variables in the network) among a large number of ChIP-seq datasets. While network inference has become a commonly used analysis tool for other types of data, such as gene expression data, the immense size of the ChIP-seq data sets and the strong redundancies present in the data limit the use of existing network inference methods. To resolve these challenges, this research proposes a novel machine learning (ML) framework to enable network inference from large collections of ChIP-seq data: 1) efficient ML methods to infer the chromatin network based on the entire ENCODE ChIP-seq data that contain redundancies; 2) new ML methods to jointly infer the context-specific chromatin networks and the associated genomic contexts by incorporating other types of genomic data; and 3) new ML methods to learn a conserved chromatin network across species and predict chromatin factor interactions even if the factors are not measured in the species of study. For further information see the project web site at: http://suinlee.cs.washington.edu/projects/chromnet."
"1561155","Collaborative Research: ArguLex -  Applying Automated Analysis to a Learning Progression for Argumentation.","DUE","ECR-EHR Core Research","09/01/2016","08/29/2016","Mary Anne Sydlik","MI","Western Michigan University","Standard Grant","Finbarr Sloane","08/31/2021","$117,986.00","","maryanne.sydlik@wmich.edu","1903 West Michigan Avenue","Kalamazoo","MI","490085200","2693878298","EHR","7980","8244, 8817","$0.00","Argumentation is fundamental to both science and science education.  This perspective is reflected in the Next Generation Science Standards where argumentation is presented as one of eight fundamental science and engineering practices through which students learn both the core ideas of science and the crosscutting concepts of science. Argumentation, however, is not measured well using standard multiple choice items. An alternative that is well suited to measuring argumentation skills is the assessment of student written work; but this approach is both expensive and time consuming. This research project addresses this issue through research into using language technology to automate the scoring of student written work to assess their argumentation skills.<br/><br/>This project applies lexical analysis and machine learning technologies to develop an efficient, valid, reliable and automated measure of middle school students' abilities to engage in scientific argumentation. The project will build upon prior work that developed high quality assessments for a learning progression for argumentation. However, these assessments are time and resource intensive to score. But when used with automated approaches, such assessments will allow measurement of argumentation to be taken to scale with rapid formative feedback, and will be an invaluable resource for STEM teachers, researchers, and teacher educators. The project brings together BSCS researchers who have experience measuring argumentation; Michigan State University's Automated Analysis of Constructed Response research group which provides expertise across a range of scientific disciplines refining analysis for formative educational purposes; Stanford University which provides expertise in learning progressions for argumentation in science; and Western Michigan University which serves as an external evaluator."
"1632935","BIGDATA: Collaborative Research: F: Foundations of Nonconvex Problems in BigData Science and Engineering: Models, Algorithms, and Analysis","IIS","Big Data Science &Engineering","09/01/2016","08/17/2016","Jack Xin","CA","University of California-Irvine","Standard Grant","Victor Roytburd","08/31/2020","$349,999.00","","jxin@math.uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","CSE","8083","7433, 8083","$0.00","In today's digital world, huge amounts of data, i.e., big data, can be found in almost every aspect of scientific research and human activity.  These data need to be managed effectively for reliable prediction and inference to improve decision making.  Statistical learning is an emergent scientific discipline wherein mathematical modeling, computational algorithms, and statistical analysis are jointly employed to address these challenging data management problems.  Invariably, quantitative criteria need to be introduced for the overall learning process in order to gauge the quality of the solutions obtained. This research focuses on two important criteria: data fitness and sparsity representation of the underlying learning model.  Potential applications of the results can be found in computational statistics, compressed sensing, imaging, machine learning, bio-informatics, portfolio selection, and decision making under uncertainty, among many areas involving big data.<br/><br/>Till now, convex optimization has been the dominant methodology for statistical learning in which the two criteria employed are expressed by convex functions either to be optimized and/or set as constraints of the variables being sought.  Recently, non-convex functions of the difference-of-convex (DC) type and the difference-of-convex algorithm (DCA) have been shown to yield superior results in many contexts and serve as the motivation for this project.  The goal is to develop a solid foundation and a unified framework to address many fundamental issues in big data problems in which non-convexity and non-differentiability are present in the optimization problems to be solved. These two non-standard features in computational statistical learning are challenging and their rigorous treatment requires the fusion of expertise from different domains of mathematical sciences.  Technical issues to be investigated will cover the optimality, sparsity, and statistical properties of computable solutions to the non-convex, non-smooth optimization problems arising from statistical learning and its many applications.  Novel algorithms will be developed and tested first on synthetic data sets for preliminary experimentation and then on publicly available data sets for realism; comparisons will be made among different formulations of the learning problems."
"1660819","CRII: CHS: Scaling Up Online Peer Tutoring of Computer Programming","IIS","CRII CISE Research Initiation, HCC-Human-Centered Computing","08/01/2016","10/27/2016","Philip Guo","CA","University of California-San Diego","Continuing Grant","William Bainbridge","08/31/2018","$171,913.00","","pjguo@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","026Y, 7367","7367, 8228","$0.00","This project will develop innovative software to broaden access to free one-on-one tutoring, starting in the domain of computer programming, which is crucial for many kinds of 21st-century jobs. Learning is one of the most important and fundamental lifelong endeavors. A well-educated public is crucial for maintaining a healthy, prosperous, and innovative society. Regardless of subject, one-on-one tutoring is the most personal and effective way to learn. Although recent efforts such as MOOCs (Massive Open Online Courses) are scaling up access to lectures and other educational materials, it is hard to scale up one-on-one tutoring to online settings because there are far more learners in the world than qualified expert tutors. Access to free online tutors will especially benefit lower-income learners, who are more likely to be from rural areas or underrepresented minority groups.<br/><br/>The technical objective of this project is to investigate how to scale up peer tutoring of computer programming through user interfaces and algorithms for matching learners with tutors, hosting tutoring sessions within real-time code visualizations, and reviewing archived sessions. The central idea is to draw from the large pool of learners concurrently accessing online educational resources to serve as peer tutors for one another. Although peers lack the deep expertise of experts, many are effective tutors since they recently learned the material and can empathize better with learners. This project will build upon Online Python Tutor, a Web-based educational tool that the researcher created to visualize the execution of computer code, now one of the most popular websites for learning programming in the Python language. This project's specific aims are to build a peer tutoring system atop the Online Python Tutor website and to use it to study online peer tutoring interactions. The system has two main components: 1.) PythonLive is a real-time tutoring interface for computer programming that enables multiple users to concurrently write, execute, visualize, and chat about code, a single tutor to effectively handle multiple simultaneous tutees, and offline learning by reviewing archived tutoring sessions. 2.) TutorMatch is an interface that uses crowdsourcing and machine learning to connect learners with appropriately-skilled peer tutors in real-time. In sum, this research will contribute new software-based techniques to facilitate peer tutoring of computer programming, which draws upon and contributes to the fields of human-computer interaction, social computing, and computer-supported cooperative learning."
"1652107","III: Small: Integrating Casual Discovery and Feature Selection with Streaming Features","IIS","Info Integration & Informatics","07/01/2016","11/26/2019","Xu Yuan","LA","University of Louisiana at Lafayette","Standard Grant","Wei Ding","05/31/2021","$497,864.00","","c00404431@louisiana.edu","104 E University Ave","Lafayette","LA","705032014","3374825811","CSE","7364","7364, 7923, 9150","$0.00","With the advent of emerging massive datasets in image processing,<br/>biology, finance, and so on, traditional data mining systems<br/>face new challenges to induce knowledge and discover causal<br/>relations in dynamic streaming feature environments, where new<br/>features continuously stream in over time. These challenges include<br/>(1) continuous growth of feature volumes over time, (2) a huge feature<br/>space, even of unknown or infinite size, and (3) not all features<br/>being available before learning begins. These challenges call for a<br/>new learning paradigm with continuously increasing features. In this<br/>project, we take the increasing feature volumes as streaming features,<br/>and the corresponding learning problem is referred to as Online<br/>Learning with Streaming Features (OLSF). Since existing online<br/>learning efforts mostly deal with data with increasing observations<br/>but fixed feature dimensions, OLSF provides a unique chance to unfold<br/>and characterize pattern trends for dynamic systems with streaming<br/>features.<br/><br/>This project aims to address two fundamental issues for OLSF: (1)<br/>causal discovery with sequentially increasing feature dimensions; and<br/>(2) causal relations for feature selection. We design novel methods<br/>and algorithms for causal discovery in OLSF and establish formal connections<br/>between casual discovery and feature selection by investigating the<br/>mutual benefits between them in the context of online stream feature<br/>learning.  To evaluate the proposed research, we conduct empirical<br/>studies on a large body of benchmark datasets, as well as with a<br/>domain-specific real-world case study in personalized news filtering<br/>and summarization where the feature space changes over time.  The<br/>new algorithms and techniques in this project will advance our ability<br/>to discover knowledge from dynamic systems using streaming features<br/>with bounded resources. The spectrum of the methods from the project<br/>will not only enrich our knowledge and understanding of pattern<br/>discovery and machine learning for dynamic systems, but also provide a<br/>new view to capture and characterize dynamic systems from a streaming<br/>feature perspective."
"1561149","Collaborative Research: Applying Automated Analysis to a Learning Progression for Argumentation","DUE","ECR-EHR Core Research","09/01/2016","08/29/2016","Jonathan Osborne","CA","Stanford University","Standard Grant","Finbarr Sloane","06/30/2021","$343,917.00","","osbornej@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","EHR","7980","8244, 8817","$0.00","Argumentation is fundamental to both science and science education.  This perspective is reflected in the Next Generation Science Standards where argumentation is presented as one of eight fundamental science and engineering practices through which students learn both the core ideas of science and the crosscutting concepts of science. Argumentation, however, is not measured well using standard multiple choice items. An alternative that is well suited to measuring argumentation skills is the assessment of student written work; but this approach is both expensive and time consuming. This research project addresses this issue through research into using language technology to automate the scoring of student written work to assess their argumentation skills.<br/><br/>This project applies lexical analysis and machine learning technologies to develop an efficient, valid, reliable and automated measure of middle school students' abilities to engage in scientific argumentation. The project will build upon prior work that developed high quality assessments for a learning progression for argumentation. However, these assessments are time and resource intensive to score. But when used with automated approaches, such assessments will allow measurement of argumentation to be taken to scale with rapid formative feedback, and will be an invaluable resource for STEM teachers, researchers, and teacher educators. The project brings together BSCS researchers who have experience measuring argumentation; Michigan State University's Automated Analysis of Constructed Response research group which provides expertise across a range of scientific disciplines refining analysis for formative educational purposes; Stanford University which provides expertise in learning progressions for argumentation in science; and Western Michigan University which serves as an external evaluator."
"1616112","RI: Small: Learning to Read, Ground, and Reason in Multimodal Text","IIS","Robust Intelligence","09/01/2016","06/10/2016","Hanna Hajishirzi","WA","University of Washington","Standard Grant","James Donlon","08/31/2020","$450,000.00","","hannaneh@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7495","7495, 7923","$0.00","Web data, news, and textbooks offer informative but unstructured multimodal text. The ability to translate multimodal text into a semantic representation that is amenable to further reasoning is a key step toward taming information overload, one of the fundamental problems in modern AI. Designing systems that can understand and use multimodal text requires multiple interconnected components: semantic interpretation, multimodal alignment, knowledge acquisition, and reasoning. Most previous work has focused on a single component in isolation and ignored the high-order crucial interdependencies between these tasks. This proposal aims at building a unified frame&#8232;work for learning to read, ground, and reason in multimodal textbooks.  This&#8232; framework will include three interconnected&#8232; components: context-aware visual and textual interpretation, acquiring and representing knowledge, and reasoning. This work is designed for significant social impact through a broad range of applications including educational and accessibility. The advances in understanding textbooks and question answering could be potentially helpful in designing an automatic personalized tutoring system to educate students about algebra, geometry, and science topics. Advancements in visual interpretation and multimodal knowledge could be beneficial to visually impaired individuals to make the diagrammatic information accessible to them. This project will be instrumental for education, research, and collaborative experience for undergraduate and graduate students including under-represented and minority groups.<br/><br/>The proposed framework is designed to iteratively read multimodal textbooks in context, acquire knowledge, interpret data, update and prune the acquired knowledge, and finally reason about the queries.  A core challenge is to do robust, scalable, context-aware semantic analysis and reasoning on multimodal text. The proposal is organized in three main thrusts that build upon each other toward the complete proposed framework. First, the project proposes a precise reasoning algorithm in narratives in learning to solve algebra word problems.  The proposed algorithm will learn to combine local contextual cues into a novel semantic structure using the global context of the narrative. Second, it proposes to build an automated system for interpreting and reasoning in multimodal text by learning to ground text and diagram into a formal representation and a new reasoning algorithm to solve those problems. Finally, it will construct a novel, principled machine learning framework for knowledge acquisition, interpretation, and reasoning in multimodal texts - science textbooks. The proposed framework will be applied in conversational dialogs and personalized tutoring systems. The key contributions will include a unified framework for learning to read, ground, and reason in multimodal textbooks, new algorithms for joint multi-modal text and diagram interpretation, precise understanding of narratives, gradual knowledge acquisition, and reasoning."
"1615706","RI: Small: Collaborative Research: Batch Learning from Logged Bandit Feedback","IIS","Robust Intelligence","07/01/2016","07/05/2016","Thorsten Joachims","NY","Cornell University","Standard Grant","Rebecca Hwa","06/30/2019","$399,818.00","","tj@cs.cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7495","7495, 7923","$0.00","Log data is one of the most ubiquitous forms of data available, as it can be recorded from a variety of systems (e.g., search engines, recommender systems, ad placement platforms) at little cost. Making huge amounts of log data accessible to learning algorithms provides the potential to acquire knowledge at unprecedented scale. Furthermore, the ability to learn from log data can enable effective machine learning even in systems where manual labeling of training data is not economically viable. Log data, however, provides only partial information -- ""contextual-bandit feedback"" -- limited to the particular actions taken by the system. The feedback for all the other actions the system could have taken is typically not known. This makes learning from log data fundamentally different from traditional supervised learning, where ""correct"" predictions together with a loss function provide full-information feedback.<br/><br/>This project tackles the problem of Batch Learning from Bandit Feedback (BLBF) by developing principled learning methods and algorithms that can be trained with logs containing contextual-bandit feedback. First, the project develops the learning theory of BLBF, especially with respect to understanding the use and design of counterfactual risk estimators for BLBF. Second, the project derives new learning methods for BLBF. Past work has already demonstrated that Conditional Random Fields can be trained in the BLBF setting, and the project derives BLBF analogs of other learning methods as well. Third, the project derives scalable training algorithms for these BLBF methods to enable large-scale applications. And, finally, the project validates the methods with real-world data from operational systems."
"1613950","III: Small: Integrating Casual Discovery and Feature Selection with Streaming Features","IIS","Info Integration & Informatics","07/01/2016","06/24/2016","Xindong Wu","VT","University of Vermont & State Agricultural College","Standard Grant","Aidong Zhang","09/30/2016","$497,864.00","","xwu@louisiana.edu","85 South Prospect Street","Burlington","VT","054050160","8026563660","CSE","7364","7364, 7923, 9150","$0.00","With the advent of emerging massive datasets in image processing,<br/>biology, finance, and so on, traditional data mining systems<br/>face new challenges to induce knowledge and discover causal<br/>relations in dynamic streaming feature environments, where new<br/>features continuously stream in over time. These challenges include<br/>(1) continuous growth of feature volumes over time, (2) a huge feature<br/>space, even of unknown or infinite size, and (3) not all features<br/>being available before learning begins. These challenges call for a<br/>new learning paradigm with continuously increasing features. In this<br/>project, we take the increasing feature volumes as streaming features,<br/>and the corresponding learning problem is referred to as Online<br/>Learning with Streaming Features (OLSF). Since existing online<br/>learning efforts mostly deal with data with increasing observations<br/>but fixed feature dimensions, OLSF provides a unique chance to unfold<br/>and characterize pattern trends for dynamic systems with streaming<br/>features.<br/><br/>This project aims to address two fundamental issues for OLSF: (1)<br/>causal discovery with sequentially increasing feature dimensions; and<br/>(2) causal relations for feature selection. We design novel methods<br/>and algorithms for causal discovery in OLSF and establish formal connections<br/>between casual discovery and feature selection by investigating the<br/>mutual benefits between them in the context of online stream feature<br/>learning.  To evaluate the proposed research, we conduct empirical<br/>studies on a large body of benchmark datasets, as well as with a<br/>domain-specific real-world case study in personalized news filtering<br/>and summarization where the feature space changes over time.  The<br/>new algorithms and techniques in this project will advance our ability<br/>to discover knowledge from dynamic systems using streaming features<br/>with bounded resources. The spectrum of the methods from the project<br/>will not only enrich our knowledge and understanding of pattern<br/>discovery and machine learning for dynamic systems, but also provide a<br/>new view to capture and characterize dynamic systems from a streaming<br/>feature perspective."
"1555811","SBIR Phase II:  Personalization Platform to Enhance Third-Party Children's Content and Improve School Readiness","IIP","SBIR Phase II","02/01/2016","05/08/2017","Prasanna Krishnan","PA","SmartyPAL","Standard Grant","Rajesh Mehta","01/31/2019","$763,317.00","","praskrish@gmail.com","631 Pine street","philadelphia","PA","191064108","6503536724","ENG","5373","116E, 5373, 8031, 8032, 8240, 9102, 9177, 9231, 9251","$0.00","This SBIR Phase II project addresses the issue that while mobile devices hold great promise for education, most of the kids' apps that claim to be educational ultimately amount to little more than brain candy. Furthermore, the few apps that are based on educational research are resource-intensive to develop and therefore have limited opportunity to scale their success. This project is building an educational platform that addresses the problems of both quality and scale in this market. It is creating a content enhancement system designed specifically for creating research-based educational content that allows developers and content studios to build mobile and web apps at a fraction (10%-20%) of the cost of traditional app development. The content built using this system is automatically integrated with analytics and personalization technology that tailors the content to each child based on the child's responses. The personalization technology applies methods from psychometrics and cognitive development to ensure that the content adapts to the child and facilitates learning in an engaging way. The project will deliver a large library of context-based and personalized learning content for preK and primary grades that parents know is based on sound research, and ultimately helps increase school readiness and learning outcomes. Given the growing demand for age-appropriate mobile educational content for children, this project has the potential to generate significant returns as a commercial enterprise and, importantly, through the economic gains resulting from improvements in educational outcomes. It will also give content developers the opportunity to economically develop and deploy new content that uses personalization technology, without having to build this complex technology themselves, thereby applying this innovation at a large scale.<br/><br/>This SBIR Phase II project is building a mobile game-based learning platform to onboard high quality third-party children's educational content and enhance their effectiveness through analytics and personalization. To achieve this, the project is developing two significant technical innovations: (1) a cloud-based personalization engine that powers adaptive learning in varied educational contexts and (2) a content enhancement system to efficiently and economically enrich third-party children's content (in the form of digital assets, books, or videos) with interactive learning games that are powered by the personalization logic. The personalization engine will use machine learning techniques combined with statistical methods of psychometric assessment and principles from cognitive development. It will inconspicuously measure children's interaction with the proposed platform's educational content and dynamically adjust game-play to suit each user's individual skills, interests and educational needs. The proposed model of leveraging and augmenting the efforts of many content developers is a dramatic departure from existing content development paradigms in this space. The goal of the project is to substantially increase the scale and reach of personalized, research-based learning experiences on mobile devices and ultimately maximize the potential of modern technology to deliver dynamic learning experiences."
"1665216","Collaborative Research: Using Data Mining and Observation to derive an enhanced theory of SRL in Science learning environments","DUE","ECR-EHR Core Research","09/01/2016","06/07/2019","Ryan Baker","PA","University of Pennsylvania","Standard Grant","Dawn Rickey","08/31/2021","$862,492.00","Luc Paquette","ryanshaunbaker@gmail.com","Research Services","Philadelphia","PA","191046205","2158987293","EHR","7980","7433, 8244, 9150","$0.00","This project aims to enhance theory and measurement of students' self-regulated learning (SRL) processes during science learning. SRL refers to learning that is guided by metacognition (thinking about one's thinking), strategic action (planning, monitoring, and evaluating personal progress against a standard), and motivation to learn. The project will accomplish this by developing a technology-based framework which leverages human expert judgment and machine learning methods to identify key moments during SRL and analyze these moments in depth. The project uses an existing science learning environment, Betty's Brain, that combines learning-by-modeling with critical thinking and problem solving skills to teach complex science topics. The environment is designed to have the student teach Betty science topics using concept maps (the critical elements of the science) and reading materials provided by the environment. A critical component of this project is to determine when a student using the system needs help. Using SRL as a basis, the additions to Betty's brain will identify key points in the SRL processes of metacognition, strategic action, and motivation. Some of these points can be determined automatically in recognizing key points while others require human intervention to recognize key points and then to determine what actions should be taken to enable the student. This information will be recorded and the experiences then are used   to update the automatic identification of key points.<br/><br/>The project's main intellectual merit is in integrating the power of data mining to rapidly sift through large amounts of data to find key inflection or change points in student reasoning and strategies, with the power of human beings to deeply understand other humans' SRL processes. This measurement framework, with the accompanying detectors of inflection points in students' SRL in online learning, has the potential to transform science learning and teaching in K-12 settings by providing insights into how SRL unfolds during learning through the interactions between affect, engagement, cognition and metacognition. Those insights will be used to extend an existing theory of SRL, increasing its richness, specificity, and predictive power. Self-regulated learning is important to student success, both in K-12 education and during life-long learning afterwards. Better understanding of SRL processes will support the development of computer-based science learning environments, such as Betty's Brain, with the capability to better support students' learning of SRL skills and strategies in science classrooms. By studying these issues within the diverse population of urban students who currently use Betty's Brain, success of the project will increase the  relevance of SRL to the full diversity of America's learners. The project's software will be available through the portal www.teachableagents.org."
"1561159","Collaborative Research: ArguLex -  Applying Automated Analysis to a Learning Progression for Argumentation","DUE","ECR-EHR Core Research","09/01/2016","08/14/2018","Kevin Haudek","MI","Michigan State University","Standard Grant","Finbarr Sloane","08/31/2021","$450,000.00","John Merrill, Kevin Haudek","haudekke@msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","EHR","7980","8244, 8817","$0.00","Argumentation is fundamental to both science and science education.  This perspective is reflected in the Next Generation Science Standards where argumentation is presented as one of eight fundamental science and engineering practices through which students learn both the core ideas of science and the crosscutting concepts of science. Argumentation, however, is not measured well using standard multiple choice items. An alternative that is well suited to measuring argumentation skills is the assessment of student written work; but this approach is both expensive and time consuming. This research project addresses this issue through research into using language technology to automate the scoring of student written work to assess their argumentation skills.<br/><br/>This project applies lexical analysis and machine learning technologies to develop an efficient, valid, reliable and automated measure of middle school students' abilities to engage in scientific argumentation. The project will build upon prior work that developed high quality assessments for a learning progression for argumentation. However, these assessments are time and resource intensive to score. But when used with automated approaches, such assessments will allow measurement of argumentation to be taken to scale with rapid formative feedback, and will be an invaluable resource for STEM teachers, researchers, and teacher educators. The project brings together BSCS researchers who have experience measuring argumentation; Michigan State University's Automated Analysis of Constructed Response research group which provides expertise across a range of scientific disciplines refining analysis for formative educational purposes; Stanford University which provides expertise in learning progressions for argumentation in science; and Western Michigan University which serves as an external evaluator."
"1632951","BIGDATA: Collaborative Research: F: Foundations of Nonconvex Problems in BigData Science and Engineering: Models, Algorithms, and Analysis","IIS","Big Data Science &Engineering","09/01/2016","08/17/2016","Yufeng Liu","NC","University of North Carolina at Chapel Hill","Standard Grant","Victor Roytburd","08/31/2020","$300,000.00","","yfliu@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","8083","7433, 8083","$0.00","In today's digital world, huge amounts of data, i.e., big data, can be found in almost every aspect of scientific research and human activity.  These data need to be managed effectively for reliable prediction and inference to improve decision making.  Statistical learning is an emergent scientific discipline wherein mathematical modeling, computational algorithms, and statistical analysis are jointly employed to address these challenging data management problems.  Invariably, quantitative criteria need to be introduced for the overall learning process in order to gauge the quality of the solutions obtained. This research focuses on two important criteria: data fitness and sparsity representation of the underlying learning model.  Potential applications of the results can be found in computational statistics, compressed sensing, imaging, machine learning, bio-informatics, portfolio selection, and decision making under uncertainty, among many areas involving big data.<br/><br/>Till now, convex optimization has been the dominant methodology for statistical learning in which the two criteria employed are expressed by convex functions either to be optimized and/or set as constraints of the variables being sought.  Recently, non-convex functions of the difference-of-convex (DC) type and the difference-of-convex algorithm (DCA) have been shown to yield superior results in many contexts and serve as the motivation for this project.  The goal is to develop a solid foundation and a unified framework to address many fundamental issues in big data problems in which non-convexity and non-differentiability are present in the optimization problems to be solved. These two non-standard features in computational statistical learning are challenging and their rigorous treatment requires the fusion of expertise from different domains of mathematical sciences.  Technical issues to be investigated will cover the optimality, sparsity, and statistical properties of computable solutions to the non-convex, non-smooth optimization problems arising from statistical learning and its many applications.  Novel algorithms will be developed and tested first on synthetic data sets for preliminary experimentation and then on publicly available data sets for realism; comparisons will be made among different formulations of the learning problems."
"1621940","SBIR Phase I:  A smart behavior modification platform to improve motivation, learning and self-esteem.","IIP","SBIR Phase I","07/01/2016","04/24/2017","Patrick Grimes","DE","eBravium, Inc.","Standard Grant","Benaiah Schrag","08/31/2017","$225,000.00","Patrick Grimes","grimes4patrick@gmail.com","2711 Centerville Rd Ste 400","Wilmington","DE","198081645","7755271791","ENG","5371","5371, 8031, 8032, 8039, 9150","$0.00","This SBIR Phase 1 project will improve motivation, learning and self-esteem of children by incorporating well-established principles from behavioral psychology and gaming science.  The innovation disrupts conventional paradigms by advantageously leveraging the exploding screen time trends of digital games and social media, which are commonly a nemesis to learning and attention span.  Children's interactions with this system will provide insights into their ability to delay gratification, a skill that is critically linked to many positive societal and life outcomes.  Initially, the innovation will address a critical need to offer a personalized approach to accelerate compliance with Common Core Standards (Common Core) related to math fluency and mastery. Once the premise is validated, the new system will be used with the Attention Deficit Hyperactivity Disorder population to improve attention span and reduce impulsivity. Ultimately, the platform will integrate with existing learning management systems administered by schools so a variety of subjects including STEM and Common Core can be featured.  This combination will create a revolutionary education platform that is delivered through smart devices, so different types of learning can occur at home, school or on the go, thereby, making state-of-the art learning technologies readily available to diverse social economic groups.<br/><br/><br/>The technical innovation is a tablet and smartphone-based learning system that uses a neuro-cognitive model of a child's mental state to determine the optimal parameters to maximize learning as well as boost attention span.   Underlying the patent-pending platform are adaptive algorithms that merge established behavioral training principles including those from Precision Teaching and Attention Deficit Hyperactivity Disorder treatments together with the latest trends in gaming science.  Concurrently, the innovation will create behavioral and reward mechanisms intended to increase the level of dopamine, a key neurotransmitter in both the learning and reward systems.  Tangentially, the innovation will accelerate important neurological pathways that can free up working memory and improve self-esteem. A paramount outcome is to create a comprehensive software exchange that can be delivered through all types of smart devices.  The project goal involves writing new software and algorithms, leveraging established software and evaluating the software and algorithms with controlled experiments. The technical hurdles to be addressed include: 1) creating new algorithms for an adaptive math-fact curriculum that improve math skills and attention span, 2) enhancing existing data stack with state-of-art machine learning that maintains higher learning performance and motivation, and 3) demonstrating consistent trends in classroom, tutoring and home settings."
"1618948","III: Small: Collaborative Learning with Incomplete and Noisy Knowledge","IIS","Info Integration & Informatics","08/01/2016","07/11/2016","Quanquan Gu","VA","University of Virginia Main Campus","Standard Grant","Aidong Zhang","12/31/2018","$500,000.00","Hongning Wang","qgu@cs.ucla.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","7364","7364, 7923","$0.00","The accelerated growth of Big Data has created enormous amount of information at the macro level for knowledge discovery. But at the micro level, one can only expect a handful of observations in most individual users. This hinders the exploration of subtle patterns and heterogeneities among distinct users for improving the utility of Big Data analytics at a per-user basis. The objective of this project is to develop a set of algorithmic solutions to perform online learning in a collaborative fashion, where personalized learning solutions actively interact with users for feedback acquisition and collaborate with each other to learn from incomplete and noisy input.  This project amplifies the utility of statistical learning in many important fields, such as healthcare, business intelligence, crowdsourcing, and cyber physical systems, where automated decision models are built on diverse, noisy and heterogeneous supervision. The research activities will be incorporated into teaching materials for student training and education in the areas of information retrieval, machine learning and data mining. <br/><br/>This project consists of three synergistic research thrusts. First, it develops a family of contextual bandit algorithms to perform collaborative online learning over networked users. Dependency among users is estimated and exploited to collaboratively update the individualized bandit parameters. Second, it develops principled solutions to optimize task-specific and general loss functions for online learning, which enables the collaborative learning solutions reach more important real-world applications, such as information retrieval and user behavior modeling. Third, it models and differentiates the reliability of the sources of feedback to optimize the overall online learning effectiveness, which is especially important in the applications such as health informatics, crowdsourcing and cyber physical systems. Expected outcomes of the project include: 1) open source implementations for the developed online learning solutions; and 2) evaluation corpora that will enable researchers to conduct follow-up research in related domains."
"1561150","Collaborative Research: ArguLex--Applying Automated Analysis to a Learning Progression for Argumentation","DUE","ECR-EHR Core Research","09/01/2016","08/29/2016","Christopher Wilson","CO","BSCS Science Learning","Standard Grant","Finbarr Sloane","08/31/2021","$579,408.00","Molly Stuhlsatz","cwilson@bscs.org","5415 Mark Dabling Boulevard","Colorado Springs","CO","809183842","7195315550","EHR","7980","8244, 8817","$0.00","Argumentation is fundamental to both science and science education.  This perspective is reflected in the Next Generation Science Standards where argumentation is presented as one of eight fundamental science and engineering practices through which students learn both the core ideas of science and the crosscutting concepts of science. Argumentation, however, is not measured well using standard multiple choice items. An alternative that is well suited to measuring argumentation skills is the assessment of student written work; but this approach is both expensive and time consuming. This research project addresses this issue through research into using language technology to automate the scoring of student written work to assess their argumentation skills.<br/><br/>This project applies lexical analysis and machine learning technologies to develop an efficient, valid, reliable and automated measure of middle school students' abilities to engage in scientific argumentation. The project will build upon prior work that developed high quality assessments for a learning progression for argumentation. However, these assessments are time and resource intensive to score. But when used with automated approaches, such assessments will allow measurement of argumentation to be taken to scale with rapid formative feedback, and will be an invaluable resource for STEM teachers, researchers, and teacher educators. The project brings together BSCS researchers who have experience measuring argumentation; Michigan State University's Automated Analysis of Constructed Response research group which provides expertise across a range of scientific disciplines refining analysis for formative educational purposes; Stanford University which provides expertise in learning progressions for argumentation in science; and Western Michigan University which serves as an external evaluator."
"1553284","CAREER: Scalable learning with combinatorial structure","IIS","Robust Intelligence","01/15/2016","02/20/2020","Stefanie Jegelka","MA","Massachusetts Institute of Technology","Continuing Grant","Rebecca Hwa","12/31/2020","$493,059.00","","stefje@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7495","1045, 7495","$0.00","Advances in science and technology increasingly rely on inference and prediction from data such as videos, molecules, networks, or sets of purchased goods.  Such data consists of several elements that participate in a collective structure. Effective inference and prediction in turn rely on (i) concise and accurate representations of latent interdependence structure in data; and (ii) fast learning and optimization algorithms that can process modern large data sets. This CAREER project addresses these challenges by building new algorithmic foundations that open a wider set of mathematical tools for practical data analysis. <br/><br/>In particular, this project explores and exploits key structural properties and representations. For example, a wide spectrum of important dependence structures (and consequently numerous learning tasks) are well captured by the ubiquitous combinatorial concept of submodular functions on sets, characterized by the property of diminishing returns. Building on this insight and other new tools, this research develops a suite of scalable optimization procedures with theoretical guarantees, as well as new tools for probabilistic modeling and fast inference. The resulting combinatorial learning methods are deployed in novel applications addressing the development of new materials, reducing  environmental impact, in video analytics, and healthcare. Thereby, the proposed methods foster progress and deliver insights beyond computer science. Parts of this project are integrated into a new advanced graduate class and a new hands-on undergraduate class on data analytics that combines statistical modeling with computation, forming a core part of a new educational program. Undergraduate students are involved in the application part of the research, and selected results will serve to motivate high school students to pursue STEM careers. For the research community, the project includes interdisciplinary workshops and tutorials, and further the confluence of discrete optimization and machine learning. Educational materials, data and code are made publicly available.<br/><br/>The research questions of this project include three main threads:<br/><br/>1) Developing a set of new, scalable optimization techniques with theoretical guarantees for combinatorial learning problems. The algorithms will combine combinatorial and continuous optimization, exploit suitable mathematical properties, relaxations, and compact representations, and will implement new ways to leverage data-dependent properties that distinguish practical cases from the worst case.<br/><br/>2) Extending insights from optimization to probabilistic modeling and inference. This transfer will enable new models and new, fast computational procedures for sampling and probabilistic inference that exploit similar properties as the optimization algorithms.<br/><br/>3) Real-world applications of the new models and algorithms. Via interdisciplinary collaborations, the third thread explores new applications of combinatorial learning methods to video analytics, instruction, healthcare, and materials design."
"1623702","EXP: Exploratory Study on the Adaptive Online Course and its Implication on Synergetic Competency","IIS","S-STEM-Schlr Sci Tech Eng&Math, Cyberlearn & Future Learn Tech","09/01/2016","04/25/2017","Noboru Matsuda","TX","Texas A&M University","Standard Grant","Amy Baylor","08/31/2018","$566,000.00","Larry Johnson, Norman Bier","noboru.matsuda@gmail.com","400 Harvey Mitchell Pkwy South","College Station","TX","778454375","9798626777","CSE","1536, 8020","8045, 8244, 8841, 9251","$0.00","The Cyberlearning and Future Learning Technologies Program funds<br/>efforts that support envisioning the future of learning technologies<br/>and advance what we know about how people learn in technology-rich<br/>environments. Cyberlearning Exploration (EXP) Projects explore the<br/>viability of new kinds of learning technologies by designing and<br/>building new kinds of learning technologies and studying their<br/>possibilities for fostering learning and challenges to using them<br/>effectively. Most online courseware helps teach facts and concepts,<br/>while a different type of online learning software called intelligent<br/>tutoring systems can effectively teach skills in a way that is<br/>tailored to each learner. Unfortunately, these two tools are rarely<br/>integrated because of the expense and specialized expertise required<br/>to create intelligent tutors. This project will close this gap by<br/>building and testing a new scalable technology that will allow<br/>teachers without years of specialized training to author adaptive<br/>online courses that combine the best of both these approaches.  This<br/>scalable cyberlearning platform will provide students with effective<br/>online instruction, provide learning engineers with an efficient<br/>authoring environment to build adaptive online courses, and provide<br/>researchers with a sharable corpus of big learning data that they can<br/>use to develop and refine theories of how students learn in adaptive<br/>online-course learning environments.<br/><br/>This project will build a web-browser-based authoring environment that<br/>supports the creation cognitive tutors and their seamless integration<br/>into online courses and will measure how well the resulting adaptive<br/>online courses promote facets of student learning such as synergetic<br/>competency and engagement. The central hypotheses are: (1) that the<br/>SimStudent technology -- a machine-learning agent that learns<br/>cognitive skills from demonstration -- can be a practical authoring<br/>tool for cognitive tutors that can be easily embedded into online<br/>courses; (2) that this technology can represent a tight connection<br/>between learners' procedural competency and conceptual competency by<br/>combining knowledge-tracing (a standard method used by existing<br/>cognitive tutors) and text-mining (data-mining latent skills from<br/>traditional online course instructions) into an innovative<br/>student-modeling technique; and (3) that adaptive online courses<br/>created with this technique can produce robust student learning by<br/>promoting connections between their procedural and conceptual<br/>understanding (synergetic competency). As part of the overall research<br/>program, the project will: (a) develop a genetic application<br/>programming interface (API) for an existing web-based authoring<br/>technology to build cognitive tutors for online course integration;<br/>(b) develop an adaptive instructional technology as a generic control<br/>mechanism for adaptive online courses; (c) build new adaptive online<br/>courses on Open edX and also convert an existing OLI course into an<br/>adaptive online course; (d) conduct in-vivo studies using the adaptive<br/>online courses to test their effectiveness; (e) test the efficacy of<br/>the proposed adaptive online courses in supporting students to achieve<br/>the aforementioned synergetic competency. Successful completion of the<br/>project will yield the following expected outcomes: (i) a scalable<br/>online course architecture with efficient authoring tools for building<br/>cognitive tutors and integrating them into online courses in order to<br/>make those courses adaptive; (ii) a practical technique to identify<br/>relationships between procedural competency and conceptual competency;<br/>and (iii) an expanded theory of how students learn with the adaptive<br/>online course, and in particular of how students achieve robust<br/>learning with synergetic competency."
"1617332","RI: Small: Collaborative Research: New Directions in Spectral Learning with Applications to Comparative Epigenomics","IIS","ROBUST INTELLIGENCE","07/01/2016","09/07/2018","Kevin Chen","NJ","Rutgers University New Brunswick","Standard Grant","Kenneth Whang","07/31/2018","$81,988.00","","kcchen@biology.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7495","7495, 7923","$0.00","The goal of this project is to design algorithms and statistical tools to build complex probabilistic models from massive quantities of data in a computationally efficient manner. This work is motivated by an important current problem in genomics, namely comparative epigenetics. While every cell in an organism has the same DNA sequence, epigenetic marks on the genome are known to be highly correlated with variation between cells. A pressing question in biology is to compare the epigenetic marks across different cell types to understand these differences. While massive amounts of data has been generated for this purpose, there is a great need for computational tools that can operate on this data and provide biologically meaningful solutions. This work will thus advance the state-of-the-art in the analysis of large complex data sets and advance the field of epigenomics. The broader impact of the work includes organizing workshops and tutorials at machine learning and bioinformatics venues, involving undergraduate students in research, and releasing open source software for the community.<br/><br/>Specifically, this project will focus on spectral learning, which has recently provided principled and computationally efficient methods for learning parameters of probabilistic graphical models. While spectral learning methods are known for some simple latent variable models, a major barrier to realizing the potential of spectral learning in real-world applications is the lack of associated statistical tools such as regularization and hypothesis testing that connect these methods in a principled manner to end-to-end application frameworks. This project proposes to develop such statistical tools by integrating modern spectral learning with the classical statistical literature in econometrics on Generalized Method of Moments. The project proposes to formulate the statistical generalized method of moment procedures for complex graphical models in the context of spectral learning as constrained optimization problems and proposes ways of solving these problems. Finally, the novel algorithms developed will be directly applied to model epigenomics data sets from the ENCODE and Roadmap Epigenomics Projects to yield methods that can operate on the massive quantities of data and provide biologically meaningful solutions. These algorithms and software have the potential to have a widespread impact on the understanding of complex human diseases such as cancer and mental disorders. This will provide a basis for designing therapeutics for these diseases and advance society towards a future of Personalized Medicine."
"1553726","CAREER: Crowdsourcing for Multirobot Coordination","IIS","Robust Intelligence","02/01/2016","02/13/2020","Nora Ayanian","CA","University of Southern California","Continuing Grant","Jie Yang","01/31/2021","$525,000.00","","ayanian@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7495","1045, 7495","$0.00","Teams of humans are exceptionally good at coordination. Teams of robots, however, are extremely clumsy at coordination, requiring extensive communication and computation. Reliance on this infrastructure poses a significant roadblock to bringing robot teams into real-world applications. This project is pursuing an integrated research, education, and outreach approach for developing novel, data-driven algorithms for multi-robot coordination, inspired by human coordination. As cognitive beings that make decisions based on broad context, memory, and sensing, human capabilities are challenging to transfer to robotics. To facilitate this transfer, the project is developing an online crowdsourcing application that tasks participants with creating a global structure, such as a shape. The application constrains participants to robot-like capabilities by limiting available information and actions. The application will provide a faithful representation of the capabilities of distributed teams of robots, and will be used to gain insights into human coordination that can then be transferred to a multi-robot system.<br/><br/>The overarching goal of the proposed work is to develop novel methodologies for multi robot coordination firmly grounded in human collaboration, based on models learned from data collected via a crowdsourced online application. To this end, the research objectives are (1) to explicate the relationship between context (communication and sensing) and outcomes in distributed teams of humans working on tightly coupled tasks using data generated from an online multi-person interface; (2) to identify, using statistical methods, parameters for distributed teams of robots solving similar shared objective problems; (3) to infer, using deep learning architectures, diverse ensembles of coordination models for distributed teams of robots solving tightly coupled problems using the data collected from the crowdsourcing application; and (4) to validate these models by evaluating their success in solving tightly coupled problems using a combination of simulation, hardware, and mixed reality experiments."
"1618690","III: Small: Transfer Learning Within and Across Networks for Collective Classification","IIS","Info Integration & Informatics","07/01/2016","05/18/2020","Jennifer Neville","IN","Purdue University","Standard Grant","Wei Ding","06/30/2021","$495,308.00","","neville@cs.purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7364","7364, 7923","$0.00","Relational machine learning methods can significantly improve the predictive accuracy of models for a range of network domains, from social networks to physical and biological networks. The methods automatically learn network correlation patterns (e.g., in biological networks a pair of interacting proteins are more likely to have the same function than two randomly selected proteins) from observed data and then use them in a collective inference process to propagate predictions throughout the network. The primary assumption in these relational methods is that model parameters estimated from one network are applicable to other networks drawn from the same distribution. However, there has been little work studying the impact of this assumption, and in particular how variability in network structure affects the performance of relational models and collective inference. This project aims to investigate this issue in order to move beyond the implicit assumption that the networks/data are drawn from the same underlying distribution. The research will establish a formal framework for learning across heterogeneous network structures and characterize the impact of network structure on models of attribute correlation. The findings will deepen our understanding of how/when relational model performance generalizes across network datasets and the work will develop new methods to improve generalization.  <br/><br/>More specifically, in this project the PI makes the key observation that templated graphical models are often used in network classification methods. These models are composed of small (i.e., local) model templates that are ''rolled out'' over a heterogeneous network to dynamically construct a larger model with variable structure for estimation and inference. Due to the roll out process, the generalizability of a learned model will depend on the similarity between the networks used for learning and prediction. In this project, the PI will study this issue in greater depth by formalizing relational learning and collective inference as a ''transfer learning'' problem, with the goal of learning a model from one domain and successfully applying it to a different domain. The research will investigate how to best transfer learned knowledge within networks (i.e., from one labeled part of a network to another), and across networks (i.e., from one network in a population to another). The project will develop rigorous statistical methods and advanced computational algorithms to answer this question via four specific aims: (Aim1) formal foundation for assessing transferability within and across networks; (Aim2) generative models of attributed networks for empirical investigation; (Aim3), within-network transfer methods for non-stationary data; and (Aim4) across-network transfer methods using template matching and global smoothing."
"1563078","SHF: Medium: Collaborative Research: From Volume to Velocity: Big Data Analytics in Near-Realtime","CCF","Software & Hardware Foundation","08/01/2016","07/21/2016","Oyekunle Olukotun","CA","Stanford University","Standard Grant","Almadena Chtchelkanova","07/31/2021","$666,665.00","Christopher Re","kunle@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","7798","7924, 7942","$0.00","Most existing techniques and systems for data analytics focus exclusively on the volume side of the common definition of Big Data as volume, velocity and variety. In contrast, there are clear indications that the velocity component will become the dominant requirement in the near future, most significantly, because of the proliferation of mobile devices across the planet. This is compounded by the fact that the freshest data often contains the most valuable information and that users have grown accustomed to data that is deeply analyzed and processed by sophisticated machine learning (ML) techniques, to enable their ""always on"" experience. In most mobile interactions, for example, the physical locations of one or potentially many users play a role, but the system needs to process the actual locations, not the ones from ten minutes ago. Many similar use cases exist in finance, intelligence and other domains. In all of them, the desires for fresh and for highly processed data are in a fundamental tension, as high quality analysis is computationally expensive and often done in large batches. The intellectual merits of this project are to investigate a combination of new ideas to address this challenge, spanning machine learning algorithms, specialized hardware accelerators, domain-specific languages, and compiler technology. The project's broader significance and importance are to pave the way for new kinds of high-velocity big-data analytics, which have the potential to revolutionize the way that people interact with the world.<br/><br/>The project investigates new incremental ML primitives and new algorithms that can trade off speed with precision, but retain provable guarantees. Novel DSLs (domain-specific languages) make such algorithms and techniques available to application developers, and new compilation techniques map DSL programs to specialized accelerators. In particular, the project shows how through these novel compilation techniques, machine learning algorithms can especially benefit from hardware acceleration with FPGAs. Finally, the project investigates new compilation techniques for end-to-end data path optimizations, including conversion of incoming data from external formats into DSL data structures, and transferring data between network interfaces and FPGA accelerators. Tying these new ideas and techniques together, this project will result in an integrated full-stack solution (spanning algorithms, languages, compilers, and architecture) to the problem of achieving high velocity in big data analytics."
"1561278","Characterizing and improving children's block-building skills: Interdisciplinary studies using approaches from cognitive science and computer science","DRL","ECR-EHR Core Research","06/15/2016","07/09/2018","Amy Shelton","MD","Johns Hopkins University","Standard Grant","Gregg Solomon","05/31/2021","$1,449,434.00","Barbara Landau, Gregory Hager","ashelton@jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","EHR","7980","8816, 8817, 9251","$0.00","The ability to understand and reason about the spatial relationships among objects supports children's academic readiness and achievements in math and reading yet, we know little about when they emerge or how to improve them during early development. One of the earliest and most accessible windows into spatial skills is children's block play--building structures with physical blocks. Building with blocks is surprisingly complex, which makes it difficult to characterize in detail how children build structures, why they sometimes struggle, and what can be done to improve their skills. Drawing on traditional observational methods and advancements in computer science, this project creates detailed, robust, and automatic techniques for characterizing children's building behaviors and articulating the different building paths taken by ""novice"" and ""expert"" child builders. Its multidisciplinary approach advances the frontiers of understanding about how people learn, and how they might use their STEM knowledge more effectively. The tools produced by the project will offer new ways to measure spatial skills in formal and informal learning settings, and has the potential to change the way we think about early block-building as a marker for learning. In this way the project reflects NSF's investments in promising developments that build a coherent, cumulative knowledge base, focusing on high-leverage topics.<br/><br/>Spatial skills represent a fundamental aspect of human knowledge, supporting a wide range of cognitive functions including our ability to create and understand 2- and 3-D spatial representations of information. This project focuses on one of the earliest developing yet highly complex spatial skills--block building--which has garnered attention in both cognitive and educational arenas due to its accessibility and adaptability for young children in formal and informal learning contexts. Consistent with the Education and Human Resources Core Research program's mission of supporting fundamental research on learning in STEM that combines theory, techniques, and perspectives from a wide range of disciplines and contexts, the spatial skills coding system developed in this project combines video and motion tracking of children's block building. This will allow researchers to gather and characterize data from much larger numbers of children than ever before, to relate these data on block-building to other academically-relevant skills, and to use the characterization of child ""expert"" performance as instructional input to ""novice"" builders. Its use of traditional cognitive methods along with machine learning techniques to characterize the process of children's block building and how it develops will generate scalable tools that can be used by scientists and educators to characterize, analyze, and promote development of spatial skills in our youngest learners."
"1615679","RI: Small: Collaborative Research: RUI: Batch Learning from Logged Bandit Feedback","IIS","Robust Intelligence","07/01/2016","03/07/2019","Douglas Turnbull","NY","Ithaca College","Standard Grant","Rebecca Hwa","06/30/2020","$132,000.00","","dturnbull@ithaca.edu","953 Danby Road","Ithaca","NY","148507000","6072741206","CSE","7495","7495, 7923, 9251","$0.00","Log data is one of the most ubiquitous forms of data available, as it can be recorded from a variety of systems (e.g., search engines, recommender systems, ad placement platforms) at little cost. Making huge amounts of log data accessible to learning algorithms provides the potential to acquire knowledge at unprecedented scale. Furthermore, the ability to learn from log data can enable effective machine learning even in systems where manual labeling of training data is not economically viable. Log data, however, provides only partial information -- ""contextual-bandit feedback"" -- limited to the particular actions taken by the system. The feedback for all the other actions the system could have taken is typically not known. This makes learning from log data fundamentally different from traditional supervised learning, where ""correct"" predictions together with a loss function provide full-information feedback.<br/><br/>This project tackles the problem of Batch Learning from Bandit Feedback (BLBF) by developing principled learning methods and algorithms that can be trained with logs containing contextual-bandit feedback. First, the project develops the learning theory of BLBF, especially with respect to understanding the use and design of counterfactual risk estimators for BLBF. Second, the project derives new learning methods for BLBF. Past work has already demonstrated that Conditional Random Fields can be trained in the BLBF setting, and the project derives BLBF analogs of other learning methods as well. Third, the project derives scalable training algorithms for these BLBF methods to enable large-scale applications. And, finally, the project validates the methods with real-world data from operational systems."
"1617157","RI: Small: Collaborative Research: New Directions in Spectral Learning with Applications to Comparative Epigenomics","IIS","Robust Intelligence","07/01/2016","06/30/2016","Kamalika Chaudhuri","CA","University of California-San Diego","Standard Grant","Kenneth Whang","06/30/2020","$273,000.00","","kamalika@cs.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7495","7495, 7923","$0.00","The goal of this project is to design algorithms and statistical tools to build complex probabilistic models from massive quantities of data in a computationally efficient manner. This work is motivated by an important current problem in genomics, namely comparative epigenetics. While every cell in an organism has the same DNA sequence, epigenetic marks on the genome are known to be highly correlated with variation between cells. A pressing question in biology is to compare the epigenetic marks across different cell types to understand these differences. While massive amounts of data has been generated for this purpose, there is a great need for computational tools that can operate on this data and provide biologically meaningful solutions. This work will thus advance the state-of-the-art in the analysis of large complex data sets and advance the field of epigenomics. The broader impact of the work includes organizing workshops and tutorials at machine learning and bioinformatics venues, involving undergraduate students in research, and releasing open source software for the community. <br/><br/>Specifically, this project will focus on spectral learning, which has recently provided principled and computationally efficient methods for learning parameters of probabilistic graphical models. While spectral learning methods are known for some simple latent variable models, a major barrier to realizing the potential of spectral learning in real-world applications is the lack of associated statistical tools such as regularization and hypothesis testing that connect these methods in a principled manner to end-to-end application frameworks. This project proposes to develop such statistical tools by integrating modern spectral learning with the classical statistical literature in econometrics on Generalized Method of Moments. The project proposes to formulate the statistical generalized method of moment procedures for complex graphical models in the context of spectral learning as constrained optimization problems and proposes ways of solving these problems. Finally, the novel algorithms developed will be directly applied to model epigenomics data sets from the ENCODE and Roadmap Epigenomics Projects to yield methods that can operate on the massive quantities of data and provide biologically meaningful solutions. These algorithms and software have the potential to have a widespread impact on the understanding of complex human diseases such as cancer and mental disorders. This will provide a basis for designing therapeutics for these diseases and advance society towards a future of Personalized Medicine."
"1628976","DIP: Improving Collaborative Learning in Engineering Classes Through Integrated Tools","IIS","Science of Learning, S-STEM-Schlr Sci Tech Eng&Math, Cyberlearn & Future Learn Tech","09/01/2016","09/07/2016","Emma Mercier","IL","University of Illinois at Urbana-Champaign","Standard Grant","Tatiana Korelsky","08/31/2021","$1,349,576.00","Luc Paquette","mercier@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","004Y, 1536, 8020","1340, 8045, 8244, 8842","$0.00","The Cyberlearning and Future Learning Technologies Program funds efforts that support envisioning the future of learning technologies and advance what we know about how people learn in technology-rich environments. Development and Implementation (DIP) Projects build on proof-of-concept work that shows the possibilities of the proposed new type of learning technology to build and refine a minimally-viable example of their proposed innovation that allows them to understand how such technology should be designed and used in the future and that allows them to answer questions about how people learn, how to foster or assess learning, and/or how to design for learning. This project is focused on the teaching of collaborative problem solving activities in introductory engineering courses and builds on a prior project to design tools for collaborative sketching in these courses.  The project is based on a recognition of the importance of collaborating in engineering, the need for student to learn this skill, the value of collaborative learning tasks for engaging students in authentic problem solving activities, and the difficulty that graduate student teaching assistants (TAs) encounter when trying to teach in this way. There are two parts to the technology innovation. The first part is a set of tools for the teaching assistants, to help them manage the classroom technologies, and to help them understand how to intervene in groups who are struggling with the content or collaborative processes. The second part is a set of tools for the students. Building on the collaborative sketch software previously developed, prompts to support their collaborative processes will be embedded in the software students will use, based on analysis of the logfiles that help determine who needs what prompts when. Research goals include understanding how receiving prompts changes the nature of students' collaborative activity, and how receiving insight into the difficulties students are having helps TAs learn about to foster collaborative learning in their classes.<br/><br/>The PIs are addressing the difficulties encountered implementing collaborative learning activities in engineering courses by designing and studying tools for TAs and students in these classes. Through an iterative design approach, the PIs will design and study tools for TAs to orchestrate the classroom and collaboration activities and to tools for students which support their collaborative problem solving processes. The PIs will investigate the use of learning analytics in evaluating the collaborative practices of students using these tools; in particular, logfiles will be examined for collaborative indicators based on prior research on collaborative processes, then clustered to look for patterns of engagement, and finally used to create regression models of successful collaboration processes using machine learning techniques. Cross-validation of the models will be done with both logfile and video data to avoid overfitting. These insights will be provided to TAs to examine whether such information is helpful in determining how and when to intervene in groups. Findings from the research will provide insight into: 1) The knowledge that TAs need in order to successfully implement collaborative problem solving in undergraduate courses; 2)  Whether TAs can learn more about collaborative problem solving with the support of tools aimed at helping them implement this form of pedagogy; 3) Whether students can learn collaborative problem solving skills through embedded prompts during multi-week collaborative activities and 4) The potential of analytics in determining when and how to reduce the collaboration supports from groups."
"1617969","III: Small: Collaborative Research: Scalable Schema-Based Event Extraction","IIS","Info Integration & Informatics","09/01/2016","08/30/2016","Niranjan Balasubramanian","NY","SUNY at Stony Brook","Standard Grant","Sylvia Spengler","08/31/2019","$391,188.00","","niranjan@cs.stonybrook.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","7364","7364, 7923","$0.00","One of the major bottlenecks in current language understanding algorithms is the lack of commonsense knowledge about how the world works. When we communicate through language, we implicitly assume that the readers will use this common sense knowledge and make the necessary inferences.  Computers, on the other hand, do not have access to this shared common knowledge, and as a result are often unable to understand text well enough to perform important tasks such as question answering. This project will study methods to learn one type of common sense knowledge about event scenarios: the series of events (actions) and the types of entities involved. For example, an arrest scenario typically involves a crime event, and an arrest event, with an arresting agent (say police), a suspect, and possibly a victim of the crime. Language understanding algorithms need to be explicitly told to look for these specific types of events and entities. This approach does not scale to the many possible real world event scenarios. This project will develop machine learning algorithms that automatically acquire this type of knowledge covering a broad range of domains in large text collections. Such algorithms can form the basis of a wide variety of assistive technology that enables public access to information. Examples include the generation of schemas from historical documents to assist students in targeted learning about historical events, or extraction of events and actors involved in current world events from streaming news sources. More generally, access to the event structure of documents will enable better question answering capabilities that, embedded appropriately into search engines, can lead to a more informed public.<br/><br/>The project will pursue three central research thrusts to learning commonsense event schemas. The first thrust develops new probabilistic algorithms for inducing event schemas that represent real-world scenarios (e.g., a Suspect is arrested by Police, pleads to a Judge, and is later convicted). The second thrust will develop new models that extract instances of these learned schemas from text (e.g., John is the Suspect). This project is unique to previous work by formalizing these as separate tasks, and thus enabling deeper research into knowledge learning apart from traditional relation extraction. Finally, the third thrust will standardize potential evaluation frameworks for event schema research. Due to the young nature of this research area, formal evaluation and analysis is inconsistent across previous work. This project will produce the largest and most diverse set of event schemas through crowd-sourcing, enabling consistent and clear evaluation of future models."
"1602337","SCH: EXP: Monitoring Motor Symptoms in Parkinson's Disease with Wearable Devices","IIS","Smart and Connected Health","09/01/2016","08/18/2016","Jessica Hodgins","PA","Carnegie-Mellon University","Standard Grant","Sylvia Spengler","08/31/2020","$678,850.00","Fernando De la Torre","jkh@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","8018","8018, 8061","$0.00","Parkinson's Disease (PD) poses a serious threat to the elderly population, affecting as many as one million Americans. There is no cure, and medications can only provide symptomatic relief. In addition, costs associated with PD, including treatment, social security payments, and lost income from inability to work, is estimated to be nearly $25 billion per year in the United States alone. The current state-of-the-art in PD management suffers from several shortcomings: (1) frequent clinic visits are a major contributor to the high cost of PD treatment and are inconvenient for the patient, especially in a population for which traveling is difficult; (2) inaccurate patient self-reports and 15-20 minute clinic visits are not enough information for doctors to accurately assess their patients, leading to difficulties in monitoring patient symptoms and medication response; and (3) motor function assessments are subjective, making it difficult to monitor disease progression. Furthermore, because they must be performed by a trained clinician, it is infeasible to do frequent motor function assessments. This project aims to promote a paradigm shift in PD management through in-home monitoring using wearable accelerometers and machine learning. Novel algorithms and experimental protocols are developed to allow for robust detection and assessment of PD motor symptoms during daily living environments.  <br/><br/>Specifically, this project develops algorithms for weakly-supervised learning, time series analysis, and personalization of classifiers. In previous studies, data was collected in controlled environments for a short amount of time (1-4 hours) and manually labeled for fully-supervised learning. In contrast, this project collects long-term (several weeks), in-home data where the participants' actions are natural and unscripted. Participants use a cell phone app to label their own data, marking segments of time as containing or not containing the occurrence of a PD motor symptom. Since the exact time of the symptom is unknown, this constitutes weakly-labeled data. This project extends multiple-instance learning algorithms for learning from weakly-labeled data in time series. Additional major technical challenges include detection of subtle motor symptoms and local minima during optimization. To further increase robustness and generalization, this project explores the use of personalization algorithms to learn person-specific models of motor symptoms from unsupervised data. The proposed techniques for weakly-supervised learning and personalization are general, and they can be applied to other human sensing problems."
"1632971","BIGDATA: Collaborative Research: F: Foundations of Nonconvex Problems in BigData Science and Engineering: Models, Algorithms, and Analysis","IIS","Big Data Science &Engineering","09/01/2016","08/17/2016","Jong-Shi Pang","CA","University of Southern California","Standard Grant","Victor Roytburd","08/31/2020","$400,687.00","","jongship@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","8083","7433, 8083","$0.00","In today's digital world, huge amounts of data, i.e., big data, can be found in almost every aspect of scientific research and human activity.  These data need to be managed effectively for reliable prediction and inference to improve decision making.  Statistical learning is an emergent scientific discipline wherein mathematical modeling, computational algorithms, and statistical analysis are jointly employed to address these challenging data management problems.  Invariably, quantitative criteria need to be introduced for the overall learning process in order to gauge the quality of the solutions obtained. This research focuses on two important criteria: data fitness and sparsity representation of the underlying learning model.  Potential applications of the results can be found in computational statistics, compressed sensing, imaging, machine learning, bio-informatics, portfolio selection, and decision making under uncertainty, among many areas involving big data.<br/><br/>Till now, convex optimization has been the dominant methodology for statistical learning in which the two criteria employed are expressed by convex functions either to be optimized and/or set as constraints of the variables being sought.  Recently, non-convex functions of the difference-of-convex (DC) type and the difference-of-convex algorithm (DCA) have been shown to yield superior results in many contexts and serve as the motivation for this project.  The goal is to develop a solid foundation and a unified framework to address many fundamental issues in big data problems in which non-convexity and non-differentiability are present in the optimization problems to be solved. These two non-standard features in computational statistical learning are challenging and their rigorous treatment requires the fusion of expertise from different domains of mathematical sciences.  Technical issues to be investigated will cover the optimality, sparsity, and statistical properties of computable solutions to the non-convex, non-smooth optimization problems arising from statistical learning and its many applications.  Novel algorithms will be developed and tested first on synthetic data sets for preliminary experimentation and then on publicly available data sets for realism; comparisons will be made among different formulations of the learning problems."
"1631436","NCS-FO: Using computational cognitive neuroscience to predict and optimize memory","DRL","ECR-EHR Core Research, IntgStrat Undst Neurl&Cogn Sys","09/01/2016","03/13/2020","Todd Gureckis","NY","New York University","Standard Grant","Betty Tuller","08/31/2020","$954,910.00","Lila Davachi","tg35@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","EHR","7980, 8624","8089, 8091, 8551","$0.00","The last decade has seen an explosion of research concerning the neural processes underlying memory formation and learning. As the basic research in this field becomes more mature, exciting possibilities for application of this knowledge have begun to emerge. This proposal aims to capitalize on these findings by developing assistive learning technologies that may revolutionize the way we teach and train people.  Researchers at New York University will develop automated ""adaptive teaching"" technologies that guide learners through material in an individualized way. The goal is to increase retention or mastery of materials by tailoring instruction to individual learners.  The novel contribution of this project is to combine insights from cognitive neuroscience and machine learning in the design and operation of such technologies.  Successful development of this synergistic research would be a transformative application of neuroscience to our daily lives and may lead to new commercial technologies.  The research will also provide a post-doctoral training opportunity for the next generation of scientists working at the intersection of neuroscience and computer science. The award is from the Integrated Strategies for Understanding Neural and Cognitive Systems program, with funding from the EHR Core Research (ECR) program, which supports fundamental research that advances the research literature on STEM learning, and from the Behavioral and Cognitive Sciences division in the SBE directorate. <br/><br/>The specific scientific goal is to explore novel applications of neuroscience methods (particularly fMRI) to improve how people learn. Neuroscience research has identified robust neural correlates of successful memory formation (e.g., activity in brain areas such as the medial temporal lobe and the hippocampus). The goal of this project is to use these variables to help predict the information needs of learners in an adaptive way. The project design involves scanning an individual's brain during the learning phase of a task. The research team then will identify which materials would benefit from additional study by combining computational models of the time course of learning and forgetting with theories mapping neural activation to successful memory formation. A computer algorithm then selects new materials for learners to re-study in a subsequent session. The goal is to show that generating a training sequence from a computer-based ""neurofeedback"" algorithm can enhance long-term memory retention more than when learners choose for themselves which items to re-study. This is a high-risk but potentially large reward project that merges basic science findings from neuroscience and cognitive science in ways that may transform the way we educate people."
"1561676","Collaborative Research: Using Data Mining and Observation to derive an enhanced theory of SRL in Science learning environments","DUE","ECR-EHR Core Research","09/01/2016","08/17/2016","Gautam Biswas","TN","Vanderbilt University","Standard Grant","Dawn Rickey","08/31/2020","$629,630.00","","gautam.biswas@vanderbilt.edu","Sponsored Programs Administratio","Nashville","TN","372350002","6153222631","EHR","7980","8244, 8817, 9150","$0.00","This project aims to enhance theory and measurement of students' self-regulated learning (SRL) processes during science learning. SRL refers to learning that is guided by metacognition (thinking about one's thinking), strategic action (planning, monitoring, and evaluating personal progress against a standard), and motivation to learn. The project will accomplish this by developing a technology-based framework which leverages human expert judgment and machine learning methods to identify key moments during SRL and analyze these moments in depth. The project uses an existing science learning environment, Betty's Brain, that combines learning-by-modeling with critical thinking and problem solving skills to teach complex science topics. The environment is designed to have the student teach Betty science topics using concept maps (the critical elements of the science) and reading materials provided by the environment. A critical component of this project is to determine when a student using the system needs help. Using SRL as a basis, the additions to Betty's brain will identify key points in the SRL processes of metacognition, strategic action, and motivation. Some of these points can be determined automatically in recognizing key points while others require human intervention to recognize key points and then to determine what actions should be taken to enable the student. This information will be recorded and the experiences then are used   to update the automatic identification of key points.<br/><br/>The project's main intellectual merit is in integrating the power of data mining to rapidly sift through large amounts of data to find key inflection or change points in student reasoning and strategies, with the power of human beings to deeply understand other humans' SRL processes. This measurement framework, with the accompanying detectors of inflection points in students' SRL in online learning, has the potential to transform science learning and teaching in K-12 settings by providing insights into how SRL unfolds during learning through the interactions between affect, engagement, cognition and metacognition. Those insights will be used to extend an existing theory of SRL, increasing its richness, specificity, and predictive power. Self-regulated learning is important to student success, both in K-12 education and during life-long learning afterwards. Better understanding of SRL processes will support the development of computer-based science learning environments, such as Betty's Brain, with the capability to better support students' learning of SRL skills and strategies in science classrooms. By studying these issues within the diverse population of urban students who currently use Betty's Brain, success of the project will increase the  relevance of SRL to the full diversity of America's learners. The project's software will be available through the portal www.teachableagents.org."
"1631871","SBIR Phase II:  An Intelligent Mental Health Therapy System","IIP","SBIR Phase II","10/01/2016","06/12/2019","Sherry Benton","FL","TAO Connect, Inc.","Standard Grant","Alastair Monk","06/30/2021","$1,360,084.00","Parisa Rashidi","sherry.benton@taoconnect.org","747 SW 2nd Avenue STE 258","Gainesville","FL","326016280","3525144094","ENG","5373","165E, 169E, 5373, 8018, 8032, 8042, 8240","$0.00","The broader impact/commercial potential of this Small Business Innovation Research, Phase II project is to help make therapy more consistent with patient preferences, beliefs, and values to maximize engagement in therapy and improve patient outcomes. Therapy for mental health problems is highly effective, yet many patients drop out before getting the full benefit because they are not satisfied or engaged in the therapy. The proposed project involves collecting data on all of patients actions in the online treatment system along with their ratings of each activity and their symptom improvement over time. The research and development team will use this data to create a machine learning system that will make suggestions for best next steps in therapy based on what thousands of other users experienced. This is the intelligent counseling system. It will work very similarly to movie streaming services or online book sellers who recommend movies or books to you based on your past preferences and the preferences of thousands of other users.<br/><br/>The proposed project will develop a feedback and recommendation system based on advanced analytics and machine learning techniques to provide personalized treatments to customize and individualize online mental health treatment, the Intelligent Counseling System (ICS). This personalized system will contain a number of alternative treatment items from several theoretical perspectives, using a variety of patient interactive activities, varying in format, length, pace, and other characteristics. In such a setting, a recommendation system can predict the users' preferences and recommend the subsequent treatment component. In addition, to achieve maximum adherence and to decrease the attrition rate, the platform will enable personalized motivational interventions and supportive messaging. The delivery times and the content of supportive messaging will adapt and vary depending on the projected treatment progress. Our machine learning based system will be trained incrementally as more data becomes available over time, thus it will benefit from improved accuracy over time. We will extract local, semi-local, and global temporal features at multiple temporal resolutions and will use feature selection techniques to identify which factors contribute to the success of treatments for patients, and to predict if a user is improving or is deteriorating. This will result in adaptive motivational messages and recommendation for tailoring treatment in term of important identified treatment features."
"1653624","SCH: INT: Collaborative Research: Diagnostic Driving: Real Time Driver Condition Detection Through Analysis of Driving Behavior","IIS","Smart and Connected Health","07/01/2016","05/15/2018","Yi-Ching Lee","VA","George Mason University","Standard Grant","Wendy Nilsen","08/31/2020","$907,135.00","","ylee65@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","CSE","8018","8018, 8062, 9251","$0.00","The automobile presents a great opportunity for healthcare monitoring. For one, most Americans engage in daily driving, and patient's time spent in vehicles is a missed opportunity to monitor their condition and general wellbeing. The goal of this project is to develop and evaluate technology for automatic in-vehicle monitoring of early symptoms of medical conditions and disrupted medications of patients, and to provide preventive care. Specifically, in this project we will focus on Attention-Deficit/Hyperactivity disorder (ADHD) in teenagers and young adults, a prevalent chronic medical condition which when uncontrolled has the potential for known negative health and quality of life consequences. The approach of using driving behavior to monitor ADHD symptoms could be applied to many other medical conditions (such as diabetes, failing eyesight, intoxication, fatigue or heart attacks) thereby transforming medical management into real-time sensing and management. Identification of all these conditions from driving behavior and alerting the proper agent could transform how we think about health monitoring and result in saved lives and reduced injuries.<br/><br/>The main goal of this project is to leverage the large amounts of health data that can be collected while driving via machine learning, in order to detect subtle changes in behavior due to out-of-control ADHD symptoms that can, for example, indicate the onset of episodes of inattention before they happen. Via lab-based driving simulator as well as on-road studies, the research team will investigate the individualized behaviors and patterns in vehicle control behaviors that are characteristic of ADHD patients under various states of medication usage. The team will develop a machine learning framework based on case-based and context-based reasoning to match the current driving behavior of the patient with previously recorded driving behavior corresponding to different ADHD symptoms. The key machine learning challenge is to define appropriate similarity measures to compare driving behavior that take into account the key distinctive features of ADHD driving behavior identified during our study. The team will evaluate the accuracy with which the proposed approach can identify and distinguish between different out-of-control ADHD symptoms, which are the implications for long-term handling of ADHD patients, via driving simulator experiments as well as using instrumented cars with real patients."
"1566359","CRII: CSR: Multi-View Learning Solutions for Next-Generation Computationally-Autonomous Wearables","CNS","Special Projects - CNS, CSR-Computer Systems Research","05/01/2016","05/08/2017","Hassan Zadeh","WA","Washington State University","Standard Grant","Marilyn McClure","04/30/2018","$191,000.00","","hassan.ghasemzadeh@wsu.edu","280 Lighty","PULLMAN","WA","991641060","5093359661","CSE","1714, 7354","1714, 8228, 9251","$0.00","Wearables have emerged as a revolutionary technology for many new applications in healthcare, fitness, and human-centered Internet-of-Things (IoT). Computational algorithms, including machine learning and signal processing techniques, are often used to extract valuable information from wearable sensor data continuously and in real-time. These algorithms, however, need to be retrained upon any changes in configuration of the system, such as addition/removal of a sensor to/from the network, sensor displacement/misplacement, sensor upgrade, adoption of the system by new users, and changes in physical and behavioral status of the user. Retraining of the computational algorithms requires collecting sufficient amount of labeled training data, a time consuming, labor-intensive, and expensive process that limits scalability and sustainability of wearable technologies. The goal of this research is to enable automatic reconfiguration of the computational algorithms without need for collecting new labeled data. <br/><br/>This proposed research aims to design, develop and validate algorithms and tools for self-configuration of wearables through two overarching research trusts. First, this project investigates synchronous multi-view learning solutions for scenarios where source and target views observe the phenomena of interest simultaneously. In the synchronous learning, direct associations between observations made by the source view and those of the target view are established through context-sensitive learning processes that take the properties of physiological monitoring and human body into account for transfer learning purposes. Second, this research develops asynchronous multi-view learning algorithms to allow for automatic knowledge transfer even in absence of synchronous measurements in the source and target views. The asynchronous learning research devises feature mapping, instance transformation, and data labeling techniques to determine how data instances of the target view are associated with those of the source view while taking into consideration physical and contextual attributes of the user.<br/><br/>This project will potentially result in highly sustainable and scalable wearables capable to self-monitor and self-configure in highly dynamic and uncontrolled environments. The true realization of computationally autonomous wearables will allow for conducting high-precision chronic disease management and contribute to availability of new wearable-based consumer applications. This can lead to the development of products and business around the concept of human-centered IoT and their use in automation of health management and many applications that are currently infeasible."
"1561567","Collaborative Research: Using Data Mining and Observation to derive an enhanced theory of SRL in Science learning environments","DUE","ECR-EHR Core Research","09/01/2016","10/17/2016","Ryan Baker","NY","Teachers College, Columbia University","Standard Grant","John Cherniavsky","01/31/2017","$862,492.00","Luc Paquette","ryanshaunbaker@gmail.com","525 West 120th Street","New York","NY","100276625","2126783000","EHR","7980","8244, 8817, 9150","$0.00","This project aims to enhance theory and measurement of students' self-regulated learning (SRL) processes during science learning. SRL refers to learning that is guided by metacognition (thinking about one's thinking), strategic action (planning, monitoring, and evaluating personal progress against a standard), and motivation to learn. The project will accomplish this by developing a technology-based framework which leverages human expert judgment and machine learning methods to identify key moments during SRL and analyze these moments in depth. The project uses an existing science learning environment, Betty's Brain, that combines learning-by-modeling with critical thinking and problem solving skills to teach complex science topics. The environment is designed to have the student teach Betty science topics using concept maps (the critical elements of the science) and reading materials provided by the environment. A critical component of this project is to determine when a student using the system needs help. Using SRL as a basis, the additions to Betty's brain will identify key points in the SRL processes of metacognition, strategic action, and motivation. Some of these points can be determined automatically in recognizing key points while others require human intervention to recognize key points and then to determine what actions should be taken to enable the student. This information will be recorded and the experiences then are used   to update the automatic identification of key points.<br/><br/>The project's main intellectual merit is in integrating the power of data mining to rapidly sift through large amounts of data to find key inflection or change points in student reasoning and strategies, with the power of human beings to deeply understand other humans' SRL processes. This measurement framework, with the accompanying detectors of inflection points in students' SRL in online learning, has the potential to transform science learning and teaching in K-12 settings by providing insights into how SRL unfolds during learning through the interactions between affect, engagement, cognition and metacognition. Those insights will be used to extend an existing theory of SRL, increasing its richness, specificity, and predictive power. Self-regulated learning is important to student success, both in K-12 education and during life-long learning afterwards. Better understanding of SRL processes will support the development of computer-based science learning environments, such as Betty's Brain, with the capability to better support students' learning of SRL skills and strategies in science classrooms. By studying these issues within the diverse population of urban students who currently use Betty's Brain, success of the project will increase the  relevance of SRL to the full diversity of America's learners. The project's software will be available through the portal www.teachableagents.org."
"1564330","RI: Medium: CompCog: Automated Discovery of Macro-Variables from Raw Spatiotemporal Data","IIS","Robust Intelligence","05/15/2016","09/15/2017","Pietro Perona","CA","California Institute of Technology","Continuing Grant","Rebecca Hwa","04/30/2021","$1,100,000.00","Frederick Eberhardt, Yisong Yue","perona@caltech.edu","1200 E California Blvd","PASADENA","CA","911250600","6263956219","CSE","7495","7495, 7924","$0.00","Observation and careful experimentation provide the basis for scientific inquiry, which in turn guides our understanding of the world and policy decisions. Today, scientific data is collected from a vast array of sensors: satellite images and radar, neuro-imaging, microscopes, body monitoring, socio-economic indicators, to name just a few. While models and theories were traditionally derived via careful handcrafting by domain experts, the new data deluge makes direct human analysis impossible. We need intelligent machines that can process vast amounts of sensory data into interpretable quantities that provide actionable information. This project will develop machines that will be able to learn on their own, purely from experience, produce and test hypotheses on causes and effects in complex dynamic scenes, and better collaborate with human scientists and analysts. For generality, we will develop and test our theory in two different domains. Amongst the immediate benefits of our project are methods for discovering the causal relationship between genes, brains and behavior. <br/><br/><br/>Our objective is to develop theory and practical algorithms for automatically interpreting a dynamic scene containing interacting agents. This will involve automatically identifying the main spatial locations, the objects, the actors, their actions and goals, and their relations to one another. The output is a description of the events, and hypotheses on the actors? goals, cause-effect relationships and likely developments. The key technical questions that we will tackle are how to infer semantically meaningful ""macro"" variables (i.e. agents' role and goals, actions, objects, special locations) directly from raw sensory data (mostly video), how to infer the causal relationships among such variables, and how to adaptively plan new experiments, including collecting feedback from human experts, to resolve ambiguities in the model.  The intellectual merit of our project lies in developing an end-to-end, pixels-to-causes approach to the automatic analysis of dynamic scenes. To this end, we will integrate, build upon, and transcend the capabilities of extant ""low-level"" correlational machine learning and ""high-level"" causal inference approaches, combined with interactive learning approaches to sequential experimental design."
"1564207","SHF: Medium: Collaborative Research: From Volume to Velocity: Big Data Analytics in Near-Realtime","CCF","Software & Hardware Foundation","08/01/2016","07/21/2016","Tiark Rompf","IN","Purdue University","Standard Grant","Almadena Chtchelkanova","07/31/2021","$332,800.00","","tiark@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7798","7924, 7942","$0.00","Most existing techniques and systems for data analytics focus exclusively on the volume side of the common definition of Big Data as volume, velocity and variety. In contrast, there are clear indications that the velocity component will become the dominant requirement in the near future, most significantly because of the proliferation of mobile devices across the planet. This is compounded by the fact that the freshest data often contains the most valuable information and that users have grown accustomed to data that is deeply analyzed and processed by sophisticated machine learning (ML) techniques, to enable their ""always on"" experience. In most mobile interactions, for example, the physical locations of one or potentially many users play a role, but the system needs to process the actual locations, not the ones from ten minutes ago. Many similar use cases exist in finance, intelligence and other domains. In all of them, the desires for fresh and for highly processed data are in a fundamental tension, as high quality analysis is computationally expensive and often done in large batches. The intellectual merits of this project are to investigate a combination of new ideas to address this challenge, spanning machine learning algorithms, specialized hardware accelerators, domain-specific languages, and compiler technology. The project's broader significance and importance are to pave the way for new kinds of high-velocity big-data analytics, which have the potential to revolutionize the way that people interact with the world.<br/><br/>The project investigates new incremental ML primitives and new algorithms that can trade off speed with precision, but retain provable guarantees. Novel DSLs (domain-specific languages) make such algorithms and techniques available to application developers, and new compilation techniques map DSL programs to specialized accelerators. In particular, the project shows how through these novel compilation techniques, machine learning algorithms can especially benefit from hardware acceleration with FPGAs. Finally, the project investigates new compilation techniques for end-to-end data path optimizations, including conversion of incoming data from external formats into DSL data structures, and transferring data between network interfaces and FPGA accelerators. Tying these new ideas and techniques together, this project will result in an integrated full-stack solution (spanning algorithms, languages, compilers, and architecture) to the problem of achieving high velocity in big data analytics."
"1658782","I-Corps: Software tool to assist mental health care providers","IIP","I-Corps","11/01/2016","01/25/2018","Benson Irungu","TX","The University of Texas Health Science Center at Houston","Standard Grant","Pamela McCauley","05/31/2018","$50,000.00","","benson.irungu@uth.tmc.edu","7000 FANNIN ST","Houston","TX","770305400","7135003999","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project will be to leverage advanced computational and machine learning technology to assist psychiatric hospitals in stratifying patients' readmission risk, optimization of post-discharge interventions and promotion of wellness. Healthcare reforms in the United States have emphasized provision of value-based healthcare services as opposed to the traditional 'fee-for-service' approach. A major outcome to be used by regulatory bodies and payers is patient readmission - interpreted as a proxy measure of quality of care provided during hospitalization. Therefore, psychiatric facilities will increasingly be required to pay special attention to patients with a high likelihood of readmission after discharge. This will be a significant departure from the traditional approach and require novel analytical tools able to proactively guide interventions. Computational and machine learning technologies can assist healthcare providers in stratifying patients' readmission risks, optimize post-discharge interventions and resource allocation - translating into better patient outcomes.<br/><br/>This I-Corps project is focused on exploring a commercialization opportunity for a computational and machine learning 'software as a service' platform that analyzes clinical informatics data from patients hospitalized in an inpatient psychiatric facility to predict post-discharge risk of readmission. In addition, the proposed platform consists of an advanced analytics engine that recommends available post-discharge intervention services and community resources to enhance the continuity of care. This platform will leverage on historical electronic health records data and a curated database of available community healthcare and social resources. It will predict readmission risk and optimize patient post-discharge interventions. The I-Corps program will allow the team to understand the customer workflow and potential for product-market fit."
"1649788","EAGER: A Data Flow Approach to Meet the Challenges of Big Data Analytics","HRD","Information Technology Researc, Special Projects - CCF","09/01/2016","08/27/2016","Lei Huang","TX","Prairie View A & M University","Standard Grant","Claudia Rankins","08/31/2020","$299,999.00","","lhuang@pvamu.edu","P.O. Box 519","Prairie View","TX","774460519","9362611689","EHR","1640, 2878","041Z, 7916","$0.00","The National Science Foundation uses the Early-concept Grants for Exploratory Research (EAGER) funding mechanism to support exploratory work in its early stages on untested, but potentially transformative, research ideas or approaches. This EAGER project was awarded as a result of the invitation in the Dear Colleague Letter NSF 16-080 to proposers from Historically Black Colleges and Universities to submit proposals that would strengthen research capacity of faculty at the institution. The project at Prairie View A & M University aims to implement machine learning algorithms on the data flow architecture and to conduct comprehensive performance and energy consumption studies in comparison with those of classical von Neumann computer architectures. The project outcomes can address the challenges modern computers based on the von Neumann architecture are facing to pertaining memory and power walls in the big data era. <br/><br/>The project is the first attempt to implement machine-learning algorithms on the data flow architecture. The results of the project will demonstrate if the data flow model can be used successfully to meet the performance, energy efficiency, and scalability requirements of widely used big data analytics and machine learning applications. Once demonstrated, the work can lead to the design and implementation of better and faster high performance computers and Big Data Analytics applications. In addition, the project will also increase the research capability at Historically Black Colleges and Universities in High Performance Computing, Computer Architecture, and Big Data Analytics.<br/><br/>This EAGER project is funded by the Directorate for Computer and Information Science and Engineering."
"1658794","CAREER:   New Approaches for Ranking in Machine Learning","IIS","Robust Intelligence","07/01/2016","09/06/2017","Cynthia Rudin","NC","Duke University","Continuing grant","Weng-keen Wong","08/31/2018","$480,000.00","","cynthia@cs.duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","7495","1045, 1187","$0.00","In numerous industries, decisions are based on large amounts of data, where a ranked list of possible actions determines how limited resources will be spent. Over the last decade, machine learning algorithms for ranking have been designed to address prioritization problems. These algorithms rank a set of objects according to the probability to possess a certain attribute; for example, we might rank a set of manholes in order of their probability to catch fire next year. However, current algorithms solve ranking problems approximately rather than exactly, and these approximate algorithms can be slow; furthermore they do not take into account many application-specific problems.<br/><br/>The goals of this project include: <br/><br/>I) Finding exact solutions to ranking problems by developing a toolbox of algorithmic techniques based on mixed-integer optimization technology. <br/><br/>II) Finding solutions faster by showing a fundamental equivalence of ranking problems to easier classification problems that can be solved an order of magnitude faster. <br/><br/>III) Developing frameworks for new structured problems. The first framework pertains to ranking problems that have a graph structure that are relevant to the energy domain. The second framework handles a sequential prediction problem arising from recommender systems, with applications also in the medical domain.<br/><br/>Through collaboration with industry, the proposed methods are being applied in several different areas, including the prevention of serious events (fires and explosions) on NYC's electrical grid."
"1637908","NRI: Collaborative Research: Accelerating Robotic Manipulation with Data-Enhanced Contact Mechanics","IIS","NRI-National Robotics Initiati","09/01/2016","07/27/2016","Matthew Mason","PA","Carnegie-Mellon University","Standard Grant","David Miller","08/31/2019","$422,050.00","","matt.mason@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","8013","8086","$0.00","Robotic manipulation depends upon mechanical contact between robot and object.  A better understanding of mechanical contact enables a wider range of more flexible manipulation techniques, which in turn enables the applications of greatest societal benefit such as eldercare, disaster response, or surgery.   This project is developing a broader and more accurate understanding of frictional contact, using a fusion of physics and data.  The project combines recent advances in a physics-based understanding of frictional contact with new machine learning techniques applied to a large corpus of experimental data.  One operation of great interest is manipulation of an object held in the robot gripper, even when the gripper is very simple.  Other operations of interest are handling objects in clutter, and manipulation of flexible objects, such as clothing.<br/><br/>The project is attacking several central challenges: modeling frictional contact, modeling deformation, measuring small motions and interaction forces, gathering large amounts of data, and developing techniques for learning in a closed-loop system.  Parametric and semi-parametric models enable the project to apply engineering models enhanced with observation data, for both planning and control.  New machine learning techniques such as predictive state representations (PSRs) enable identification and modeling of previously hidden state, as well as learning in closed-loop systems.  New infrastructure enables gathering of relevant, precise data, on a large scale.  The project is developing and employing a Robotic Manipulation Arena, with a unique combination of manipulation resources and instrumentation to provide high volumes of high quality experimental data.  The primary outcomes are robust and practical contact models, so that robots can work more dexterously and opportunistically."
"1553086","CAREER: The optimal use of data","CCF","Comm & Information Foundations","02/15/2016","03/25/2020","John Duchi","CA","Stanford University","Continuing Grant","Phillip Regalia","01/31/2021","$497,033.00","","jduchi@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","7797","1045, 7936","$0.00","Modern techniques for data gathering?arising from medicine and bioinformatics, internet applications such as web-search, physics and astronomy, mobile data gathering platforms?have yielded an explosion in the mass and diversity of data. Concurrently, statistics, decision theory, and machine learning have successfully laid a groundwork for answering questions about our world based on analysis of this data. As more information is collected, classical approaches for inference and learning are insufficient, as additional concerns arise?computational resources, privacy considerations, storage limitations, network communication constraints? outside of statistical accuracy. This prompts a basic question: how can multiple criteria be balanced while maintaining statistical performance?<br/><br/>To bring statistics and machine learning into closer contact with other desiderata, this research involves the development of procedures that trade between scarce resources in principled and optimal ways. Such trade-offs have been difficult to characterize, as current tools for providing fundamental limits (such as information theory in communication) do not connect disparate areas. Three concrete sub-areas serve as bases for this research. The investigators study the interplay of computing with learning, estimation, and optimization by connecting notions of computation?such as memory accesses or synchronization in distributed systems?to data analysis tasks. Second, the research investigates adaptive and robust procedures?and associated statistical costs?that will become more important given increasingly long-tailed and messy data. Thirdly, the investigators study privacy in estimation, using information and decision-theoretic tools to characterize the tensions between statistical accuracy and sensitive data disclosures. Combined, these lay the groundwork for a theory on the use of data in the face of constraints, along with a functional and practical understanding of procedures that balance scarce resources against statistical accuracy."
"1637758","NRI: Collaborative Research: Accelerating Robotic Manipulation with Data-Enhanced Contact Mechanics","IIS","NRI-National Robotics Initiati","09/01/2016","07/27/2016","Byron Boots","GA","Georgia Tech Research Corporation","Standard Grant","Jie Yang","08/31/2019","$452,219.00","","bboots@cs.washington.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","8013","8086","$0.00","Robotic manipulation depends upon mechanical contact between robot and object.  A better understanding of mechanical contact enables a wider range of more flexible manipulation techniques, which in turn enables the applications of greatest societal benefit such as eldercare, disaster response, or surgery.   This project is developing a broader and more accurate understanding of frictional contact, using a fusion of physics and data.  The project combines recent advances in a physics-based understanding of frictional contact with new machine learning techniques applied to a large corpus of experimental data.  One operation of great interest is manipulation of an object held in the robot gripper, even when the gripper is very simple.  Other operations of interest are handling objects in clutter, and manipulation of flexible objects, such as clothing.<br/><br/>The project is attacking several central challenges: modeling frictional contact, modeling deformation, measuring small motions and interaction forces, gathering large amounts of data, and developing techniques for learning in a closed-loop system.  Parametric and semi-parametric models enable the project to apply engineering models enhanced with observation data, for both planning and control.  New machine learning techniques such as predictive state representations (PSRs) enable identification and modeling of previously hidden state, as well as learning in closed-loop systems.  New infrastructure enables gathering of relevant, precise data, on a large scale.  The project is developing and employing a Robotic Manipulation Arena, with a unique combination of manipulation resources and instrumentation to provide high volumes of high quality experimental data.  The primary outcomes are robust and practical contact models, so that robots can work more dexterously and opportunistically."
"1616297","AF: Small: Entropy Maximization in Approximation, Learning, and Complexity","CCF","Algorithmic Foundations","09/01/2016","05/17/2016","James Lee","WA","University of Washington","Standard Grant","A. Funda Ergun","08/31/2020","$466,000.00","","jrl@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7796","7923, 7926, 7927, 9251","$0.00","Entropy plays a distinguished role in the world.  The second law of thermodynamics tell us that, in closed systems, entropy always increases; it is maximized at thermodynamic equilibrium.  Given a collection of data, the ""principle of maximum entropy"" asserts that, among all hypothetical probability distributions that agree with the data, the one of maximum entropy best represents the current state of knowledge.<br/><br/>Moreover, if one considers a convex set of probability distributions, the problem of maximizing a strongly concave function (like the Shannon entropy) over this set is computationally tractable and has a unique optimal solution.  This project is concerned with the structure and computational utility of entropy maximizers in algorithm design, machine learning, complexity theory, and related areas of discrete mathematics.  In particular, the project will study the role of entropy maximization in encouraging simplicity in the optimum solution.  This property stands to reason:  The entropy maximizer should intuitively contain only the information implied by the constraints and nothing more.<br/><br/>The scope of the project includes not only classical entropy functionals like the Shannon entropy and Kullback-Leibler divergence, but also the analogous notions for quantum states (von Neumann entropy).  The study of quantum entropy maximizers has far-reaching applications in semi-definite programming and communication complexity.  Moreover, much of the theory extends to other Bregman divergences, and this is particularly relevant for applications in online algorithms where certain smoothed entropy functionals become relevant.  A portion of the project concerns entropy optimality on path spaces.  This perspective provides a novel view of Markov processes on discrete and continuous spaces.  The PI will employ this viewpoint to study rapid mixing of Markov chains, as well smoothing properties of the noise operator on the discrete hypercube (a topic with remarkable applications in complexity theory and hardness of approximation).<br/><br/>Finally, it should be mentioned that iterative algorithms for finding entropy maximizers can be viewed in the framework of entropy-regularized gradient descent; such algorithms are fundamental in machine learning (boosting) and online convex optimization (multiplicative weights update).  This provides a powerful connection to large bodies of work, and a substantial motivation for the project is to create a bridge of ideas and techniques between the two perspectives.<br/><br/>Broader impact of the project includes training of the next generation of scientists, including at the undergraduate level.  This project presents a number of opportunities for undergraduate researchers to contribute in a meaningful and substantial way, while at the same time receiving valuable mentoring and experience as developing scientists."
"1637753","NRI: Collaborative Research: Accelerating Robotic Manipulation with Data-Enhanced Contact Mechanics","IIS","NRI-National Robotics Initiati","09/01/2016","07/27/2016","Alberto Rodriguez Garcia","MA","Massachusetts Institute of Technology","Standard Grant","Jie Yang","08/31/2019","$430,000.00","","albertor@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","8013","8086","$0.00","Robotic manipulation depends upon mechanical contact between robot and object.  A better understanding of mechanical contact enables a wider range of more flexible manipulation techniques, which in turn enables the applications of greatest societal benefit such as eldercare, disaster response, or surgery.   This project is developing a broader and more accurate understanding of frictional contact, using a fusion of physics and data.  The project combines recent advances in a physics-based understanding of frictional contact with new machine learning techniques applied to a large corpus of experimental data.  One operation of great interest is manipulation of an object held in the robot gripper, even when the gripper is very simple.  Other operations of interest are handling objects in clutter, and manipulation of flexible objects, such as clothing.<br/><br/>The project is attacking several central challenges: modeling frictional contact, modeling deformation, measuring small motions and interaction forces, gathering large amounts of data, and developing techniques for learning in a closed-loop system.  Parametric and semi-parametric models enable the project to apply engineering models enhanced with observation data, for both planning and control.  New machine learning techniques such as predictive state representations (PSRs) enable identification and modeling of previously hidden state, as well as learning in closed-loop systems.  New infrastructure enables gathering of relevant, precise data, on a large scale.  The project is developing and employing a Robotic Manipulation Arena, with a unique combination of manipulation resources and instrumentation to provide high volumes of high quality experimental data.  The primary outcomes are robust and practical contact models, so that robots can work more dexterously and opportunistically."
"1619884","Efficient Methods for Large-Scale Self-Concordant Convex Minimization","DMS","COMPUTATIONAL MATHEMATICS","07/01/2016","06/19/2018","Quoc Tran-Dinh","NC","University of North Carolina at Chapel Hill","Continuing Grant","Leland Jameson","06/30/2020","$239,840.00","","quoctd@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","MPS","1271","9263","$0.00","The literature on the formulation, analysis and applications of convex optimization is ever expanding due to its broad applications in signal processing, machine learning, statistics, and other fields of data science. In theory, many convex problems have a well-understood structure, and hence state-of-the-art first order and interior-point methods can obtain high accurate solutions. In practice, however, modern applications present a host of increasingly larger-scale and nonsmooth optimization problems that can render these methods impractical. Fortunately, recent advances in convex optimization offer a surprising new angle to fundamentally re-examine the theory and practice of large-scale convex optimization models in a unified fashion. Successful development of the PI's ideas will have several broad impacts in data analysis and computational science. While existing state-of-the-art approaches focus on certain classes of convex problems, the PI strongly believes that the approach proposed in this project can be expand to cover a wide range of unexploited convex optimization applications. The obtained theory and methods can be specified and customized to solve various problems in different fields, including massive data analysis, machine learning, high-resolution imaging science, operations research, networks, and control. Successful real-world applications and software development can create a major impact to practicians in academic and industry. The PI's broad collaborations are expected to make a significant progress in the application of convex optimization techniques. Several research topics in this project will be integrated into graduate training programs through special topic courses and PhD research directions, whereas undergraduate training activities can also benefit from this research via internship training and interdisciplinary collaborations.<br/><br/>This project focuses on exploiting and generalizing a prominent concept so-called self-concordance to develop new efficient convex optimization techniques to attack two classes of large-scale convex optimization problems, and will be integrated into three work packages (WPs). WP1. Composite self-concordant convex optimization: While existing convex optimization methods essentially rely on the Lipschitz gradient assumption which unfortunately excludes many important applications such as Poisson and graphical learning models, the PI instead focuses on the self-concordance structure and its generalizations. Such a concept is key to the theory of interior-point methods, but has remained unexploited in composite minimization. Grounded in this structure, the PI will develop novel and provable convex optimization algorithms for solving several subclasses of large-scale composite convex problems. He also plans to generalize this self-concordant notion to other subclasses of problems such as logistic loss functions and entropy models to cover a broader range of applications.  WP2. Constrained convex optimization involving self-concordant barriers: Various constrained convex applications are integrated with a self-concordant barrier structure, while other convex constraints often have a ""simple"" structure. Existing general-purpose convex algorithms solve these problems by mainly employing either a standard interior-point method or an augmented Lagrangian framework. The PI alternatively concentrates on exploiting special structures of these problems using the theory of self-concordant barriers and combining them with both the interior-point idea and the proximal framework to develop new and scalable algorithms equipped with a rigorous convergence guarantee, while offering a parallel and distributed implementation. WP3. Implementation and applications: This WP aims at investigating the implementation aspects of the proposed algorithms and upgrading the PI's SCOPT solver. The methods developed in this project will be validated through three concrete real-world applications: image processing involving Poisson models, graphical learning problems, and large-scale max-cut and graph clustering problems."
"1536895","Collaborative Research: Designing Functional Materials with Optimal Learning","CMMI","ESD-Eng & Systems Design","01/01/2016","08/30/2015","Peter Frazier","NY","Cornell University","Standard Grant","Kathryn Jablokow","12/31/2019","$338,798.00","Paulette Clancy","pf98@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","ENG","1464","024E, 067E, 073E, 8021","$0.00","New products and material processing methods often require the identification of novel materials that are stronger, lighter, cheaper, or better in some way.  Searching for new materials with a trial-and-error approach can be expensive and often ineffective.  With this award, new mathematical methods and computer software will be developed to accelerate materials discovery.  The planned approach will narrow the available options to those that are most likely to succeed, making discovery of new materials and processes more reliable and less costly. Demonstration of the approach will be made for materials to be used in flexible organic solar cells, but the methods could also be amenable to materials for use in pharmaceuticals or to food additives.<br/><br/>A new optimal learning approach to materials design is planned that uses advances in Bayesian experimental design and machine learning to predict material properties from previous data and domain expertise, and to intelligently suggest physical and computational experiments that will provide information that is most supportive of discovery.  These new mathematical techniques promise to greatly accelerate materials design, providing better materials more reliably and with less experimental effort.  The approach will be demonstrated in the search for organic semiconductor materials over a set of existing candidates, solvent choices, and processing conditions, and integrate both physical and computational experiments in this search. The test case is an all-organic solar cell system of contorted hexabenzocoronenes (c-HBC), deposited on carbon nanotubes (CNT).  This complex system involves issues including complexation between c-HBC and CNT at different processing conditions, etc., which provide a stringent test of optimal learning and computer simulation methods to predict the processing-structure-function triad. This approach is broadly applicable to a diverse set of materials design problems."
"1544613","CPS: TTP Option: Frontiers:  Collaborative Research: Software Defined Control  for Smart Manufacturing Systems","CNS","Special Projects - CNS, CPS-Cyber-Physical Systems","09/01/2016","08/23/2019","Elaine Shi","NY","Cornell University","Continuing Grant","Ralph Wachter","08/31/2019","$302,820.00","","runting@cs.cmu.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","1714, 7918","7918, 8236, 9102","$0.00","Software-Defined Control (SDC) is a revolutionary methodology for controlling manufacturing systems that uses a global view of the entire manufacturing system, including all of the physical components (machines, robots, and parts to be processed) as well as the cyber components (logic controllers, RFID readers, and networks). As manufacturing systems become more complex and more connected, they become more susceptible to small faults that could cascade into major failures or even cyber-attacks that enter the plant, such as, through the internet.  In this project, models of both the cyber and physical components will be used to predict the expected behavior of the manufacturing system. Since the components of the manufacturing system are tightly coupled in both time and space, such a temporal-physical coupling, together with high-fidelity models of the system, allows any fault or attack that changes the behavior of the system to be detected and classified. Once detected and identified, the system will compute new routes for the physical parts through the plant, thus avoiding the affected locations. These new routes will be directly downloaded to the low-level controllers that communicate with the machines and robots, and will keep production operating (albeit at a reduced level), even in the face of an otherwise catastrophic fault. These algorithms will be inspired by the successful approach of Software-Defined Networking.  Anomaly detection methods will be developed that can ascertain the difference between the expected (modeled) behavior of the system and the observed behavior (from sensors). Anomalies will be detected both at short time-scales, using high-fidelity models, and longer time-scales, using machine learning and statistical-based methods. The detection and classification of anomalies, whether they be random faults or cyber-attacks, will represent a significant contribution, and enable the re-programming of the control systems (through re-routing the parts) to continue production.<br/><br/>The manufacturing industry represents a significant fraction of the US GDP, and each manufacturing plant represents a large capital investment. The ability to keep these plants running in the face of inevitable faults and even malicious attacks can improve productivity -- keeping costs low for both manufacturers and consumers. Importantly, these same algorithms can be used to redefine the production routes (and machine programs) when a new part is introduced, or the desired production volume is changed, to maximize profitability for the manufacturing operation  ."
"1559829","REU Site: Bioinformatics Research and Interdisciplinary Training Experience in Analysis and Interpretation of Information-Rich Biological Data Sets (REU-BRITE)","DBI","RSCH EXPER FOR UNDERGRAD SITES, Cross-BIO Activities","09/15/2016","08/04/2017","Gary Benson","MA","Trustees of Boston University","Continuing Grant","Sally O'Connor","08/31/2019","$292,295.00","","gbenson@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","BIO","1139, 7275","9250","$0.00","This REU Site award to Boston University (BU), located in Boston, MA, will support the training of 8 students for 10 weeks during the summers of 2017-2019. The program will focus on computational and mathematical analysis of bioinformatics data. Research projects will include studying gene regulatory networks, the microbiome, and the linkage between genetic variation and phenotypes. Students will learn to analyze high-throughput sequencing data, gain experience in statistical and machine learning methods, and learn dynamic systems modeling. Full-time research with a Bioinformatics faculty mentor will be supplemented by workshops covering 1) bioinformatics programming, 2) parallel and cloud computing, 3) creating a scientific poster, 4) applying to graduate school, and 5) scientific ethics and responsible conduct of research. Additional activities will include tours of local science facilities, and participation in BU Bioinformatics symposia. A stipend, housing, meal, and travel allowances will be provided. Students will be selected based on relevant coursework in biology, mathematics, and computing, a good academic record, enthusiasm for bioinformatics research, and evidence of involvement in community activities and community service. <br/><br/>It is anticipated that a total of 24 students will be trained in the program with an emphasis on recruiting women, members of groups underrepresented in the sciences, and students from schools with limited research opportunities. Students will learn how research is conducted and many will present posters at national, regional, or BU sponsored undergraduate research conferences. <br/><br/>A common web-based assessment tool used by all REU programs funded by the Division of Biological Infrastructure (Directorate for Biological Sciences) will be used to determine the effectiveness of the training program. A Research Experience Wiki will be used for weekly recording of skills learned. Students will be tracked after the program to determine career paths using BU Bioinformatics LinkedIn and Facebook pages. Students will be asked to respond to an automatic email sent via the NSF reporting system. More information is available at http://www.bu.edu/bioinformatics/research/undergraduate-research, or by contacting the PI (Dr. Gary Benson at gbenson@bu.edu) or the program coordinator (Ms. Caroline Lyman at clyman@bu.edu)."
"1544901","CPS: TTP Option: Frontiers:  Collaborative Research: Software Defined Control  for Smart Manufacturing Systems","CNS","CPS-Cyber-Physical Systems","09/01/2016","08/14/2019","Sibin Mohan","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Sandip Roy","08/31/2021","$1,125,000.00","Sayan Mitra","sibin@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7918","7918, 8235, 8236","$0.00","Software-Defined Control (SDC) is a revolutionary methodology for controlling manufacturing systems that uses a global view of the entire manufacturing system, including all of the physical components (machines, robots, and parts to be processed) as well as the cyber components (logic controllers, RFID readers, and networks). As manufacturing systems become more complex and more connected, they become more susceptible to small faults that could cascade into major failures or even cyber-attacks that enter the plant, such as, through the internet.  In this project, models of both the cyber and physical components will be used to predict the expected behavior of the manufacturing system. Since the components of the manufacturing system are tightly coupled in both time and space, such a temporal-physical coupling, together with high-fidelity models of the system, allows any fault or attack that changes the behavior of the system to be detected and classified. Once detected and identified, the system will compute new routes for the physical parts through the plant, thus avoiding the affected locations. These new routes will be directly downloaded to the low-level controllers that communicate with the machines and robots, and will keep production operating (albeit at a reduced level), even in the face of an otherwise catastrophic fault. These algorithms will be inspired by the successful approach of Software-Defined Networking.  Anomaly detection methods will be developed that can ascertain the difference between the expected (modeled) behavior of the system and the observed behavior (from sensors). Anomalies will be detected both at short time-scales, using high-fidelity models, and longer time-scales, using machine learning and statistical-based methods. The detection and classification of anomalies, whether they be random faults or cyber-attacks, will represent a significant contribution, and enable the re-programming of the control systems (through re-routing the parts) to continue production.<br/><br/>The manufacturing industry represents a significant fraction of the US GDP, and each manufacturing plant represents a large capital investment. The ability to keep these plants running in the face of inevitable faults and even malicious attacks can improve productivity -- keeping costs low for both manufacturers and consumers. Importantly, these same algorithms can be used to redefine the production routes (and machine programs) when a new part is introduced, or the desired production volume is changed, to maximize profitability for the manufacturing operation  ."
"1617583","III: Small: A New Approach to Latent Space Learning with Diversity-Inducing Regularization and Applications to Healthcare Data Analytics","IIS","Info Integration & Informatics","09/01/2016","08/24/2016","Eric Xing","PA","Carnegie-Mellon University","Standard Grant","Sylvia Spengler","08/31/2021","$499,361.00","","epxing@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7364","7364, 7923","$0.00","Latent variable models (LVMs), which extract hidden information, such as topics, themes, or disease patterns, from raw data, play an important role in electronic health record (EHR) management and applications. With the dramatic increase of the volume and complexity of EHR data, current LVMs face several new challenges, including inadequacy in capturing rare patterns existing in only small number of patients in a population (also known as long tail patterns), redundancy amongst patterns being discovered, and low computational efficiency, which all seriously impair the value of EHR data in driving high-quality personalized medicine. There is a critical need in developing new methods to transform conventional LVMs to ones that can circumvent such limitations so that the EHR data can be more effectively and reliably used for healthcare applications. This project addresses this need and develops a new technique known as ""diversity-inducing machine learning models"", which promote rare patterns and condense redundant patterns, at high computational efficiency, to enable more effective pattern discovery and knowledge extraction from complex and heterogeneous (e.g., textual, image, and time series) EHR data. <br/><br/>Specifically, this project contains the following research components: 1. Develop a new regularized LVM learning framework that allows the basis of the latent space to favor a more diversity-inducing geometry and less redundancy, thereby accomplish long-tail pattern coverage and better interpretability for both Euclidean and Hilbert space settings. 2. Develop a diversity-promoting Bayesian LVM learning framework that enables efficient inference of posteriors probability distributions to facilitate quantization of uncertainty and alleviate over fitting. 3. Theoretically analyze the diversity-inducing techniques proposed in 1 and 2 to understand how these techniques affect the generalization errors in supervised LVMs, posterior contraction rate in unsupervised LVMs, and the information geometry of the distributions induced by LVMs. 4. Apply the diversified LVMs to healthcare applications. This project also provides rich opportunities for multi-disciplinary education and research training, at both undergraduate, graduate, and professional levels."
"1544678","CPS: TTP Option: Frontiers:  Collaborative Research: Software Defined Control  for Smart Manufacturing Systems","CNS","CM - Cybermanufacturing System, CPS-Cyber-Physical Systems","09/01/2016","08/29/2019","Dawn Tilbury","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Ralph Wachter","08/31/2021","$2,362,392.00","Zhuoqing Mao, Kira Barton, James Moyne","tilbury@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","018Y, 7918","7918, 8236, 9102","$0.00","Software-Defined Control (SDC) is a revolutionary methodology for controlling manufacturing systems that uses a global view of the entire manufacturing system, including all of the physical components (machines, robots, and parts to be processed) as well as the cyber components (logic controllers, RFID readers, and networks). As manufacturing systems become more complex and more connected, they become more susceptible to small faults that could cascade into major failures or even cyber-attacks that enter the plant, such as, through the internet.  In this project, models of both the cyber and physical components will be used to predict the expected behavior of the manufacturing system. Since the components of the manufacturing system are tightly coupled in both time and space, such a temporal-physical coupling, together with high-fidelity models of the system, allows any fault or attack that changes the behavior of the system to be detected and classified. Once detected and identified, the system will compute new routes for the physical parts through the plant, thus avoiding the affected locations. These new routes will be directly downloaded to the low-level controllers that communicate with the machines and robots, and will keep production operating (albeit at a reduced level), even in the face of an otherwise catastrophic fault. These algorithms will be inspired by the successful approach of Software-Defined Networking.  Anomaly detection methods will be developed that can ascertain the difference between the expected (modeled) behavior of the system and the observed behavior (from sensors). Anomalies will be detected both at short time-scales, using high-fidelity models, and longer time-scales, using machine learning and statistical-based methods. The detection and classification of anomalies, whether they be random faults or cyber-attacks, will represent a significant contribution, and enable the re-programming of the control systems (through re-routing the parts) to continue production.<br/><br/>The manufacturing industry represents a significant fraction of the US GDP, and each manufacturing plant represents a large capital investment. The ability to keep these plants running in the face of inevitable faults and even malicious attacks can improve productivity -- keeping costs low for both manufacturers and consumers. Importantly, these same algorithms can be used to redefine the production routes (and machine programs) when a new part is introduced, or the desired production volume is changed, to maximize profitability for the manufacturing operation  ."
"1634050","Computational neuroimaging of human auditory cortex","BCS","Cognitive Neuroscience","07/15/2016","07/19/2016","Joshua McDermott","MA","Massachusetts Institute of Technology","Standard Grant","Kurt Thoroughman","06/30/2019","$500,000.00","","jhm@MIT.EDU","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","SBE","1699","","$0.00","Just by listening, humans can infer a vast array of things about the world around them: who is talking, whether a window in their house is open or shut, or what their child dropped on the floor in the next room. This ability to derive information from sound is a core component of human intelligence, and is enabled by many stages of neuronal processing extending from the ear into the brain. Although much is known about how the ears convert sound to electrical signals that are sent to the brain, the mechanisms by which the brain mediates our sound recognition abilities remains poorly understood. These gaps in knowledge limit our ability to develop machine systems that can replicate our listening skills (e.g. for use in robots) or to understand the basis of listening difficulties, as in disorders such as dyslexia or auditory processing disorder, or in age-related hearing loss. To gain insight into the neuronal processes that enable auditory recognition, the brain's processing of sound will be studied using fMRI, a technique to non-invasively measure brain activity. The responses measured in the brain will be compared to the numerical responses produced by state-of-the-art computer algorithms for sound recognition. The research will help reveal the principles of human auditory intelligence, with the long-term goals of enabling more effective machine algorithms and treatments for listening disorders. The research will also provide insight into the inner workings of computer audio algorithms, stimulating interaction between engineering, industry, and neuroscience. The project will facilitate other research efforts via the dissemination of new tools for manipulating sound and the creation of audio data sets, and will recruit and train women and underrepresented minorities in computational neuroscience.<br/><br/>Aspects of the structure and function of primary auditory cortex are well established, and there are a variety of proposals for pathways that might extend out of primary auditory cortex. However, we know little about the transformations within the auditory cortex that enable sound recognition, and there are few computational models of how such transformations might occur. The goal of the proposed research is to conduct fMRI experiments that reveal representational transformations within auditory cortex that might contribute to auditory recognition, to use fMRI responses to test existing models of auditory computation, and to develop new models that can account for human abilities and neuronal responses. Functional MRI will be used to characterize cortical responses because it allows measurements from the entire auditory cortex at once, making it possible to compare responses in different regions of the auditory cortex (including those far from the cortical surface), and thus to probe for representational transformations between regions. New models of auditory computation will be developed by leveraging the recent successes of ""deep learning"", and their relevance to the brain will be tested using new synthesis-based methods for model evaluation. The results will help reveal how the auditory cortex mediates robust sound recognition."
"1646998","Conference:  Travel Support for GlobalSIP 2016, To Be Held This Year in Crystal City, VA, December 7-9, 2016","ECCS","COMMS, CIRCUITS & SENS SYS","09/01/2016","12/09/2016","Hang Liu","DC","Catholic University of America","Standard Grant","Akbar Sayeed","08/31/2017","$15,000.00","Phillip Regalia","liuh@cua.edu","620 Michigan Ave.N.E.","Washington","DC","200640001","2026355000","ENG","7564","153E, 7556","$0.00","This award supports student travel to the 2016 IEEE Global Conference on Signal and Information Processing (GlabalSIP). The conference takes place near Washington, DC, in December 2016, and consists of symposia spanning big data, deep learning, 5G communications, medical imaging, secure communications, and distributed optimization, among other emerging areas. The conference provides an unparalleled opportunity for students to take in the latest topics of current research interest. Student participation thus contributes to NSF's mission to promote the next generation of scientists and engineers, working in fields quite relevant to the Engineering Directorate.  <br/><br/>The broader impacts includes broadening participation of women and underrepresented minorities, thereby fostering a diverse pool of next generation researchers in these emerging area. The social program is designed to foster interaction between participants of all symposia, thus offering students opportunities to expose themselves to emerging trends that might not be readily accessible in their home laboratories, alongside interaction with other researchers working in related fields, aiming to promote cross-fertilization of ideas.  The conference will also bestow a Best Student Paper award, to be determined based on judges' notes of all papers and posters presented by students."
"1633130","BIGDATA: IA: Acting on Actionable Intelligence: A Learning Analytics Methodology for Student Success Efficacy Studies","IIS","Project & Program Evaluation","09/01/2016","09/15/2016","Richard Levine","CA","San Diego State University Foundation","Standard Grant","Finbarr Sloane","08/31/2021","$1,096,196.00","Juanjuan Fan, Bernie Dodge","rlevine@mail.sdsu.edu","5250 Campanile Drive","San Diego","CA","921822190","6195945731","CSE","7261","7433, 8083, 8244","$0.00","The research supported by this project will study how instructors, administrators, and education researchers take advantage of rich student and student performance data collected by the university. The data will be used in the development of a new statistical model that will identify students in need of help and the sort of help that they need. The system is built upon statistical models that are used in personalized medicine to determine the best medical interventions for an individual patient. The research will be carried out by an interdisciplinary team from statistics and data science, institutional research, instructional technology, and information technology and they will develop a learning analytics methodology to automate the tasks of data collection and processing, data visualizations and summaries, data analysis, and scientific reporting in student success efficacy studies. As part of this development, the concept of individualized treatment effects is introduced as a method to assess the effectiveness of interventions and/or instructional regimes and provide personalized feedback to students.<br/><br/>More specifically the research goal of the project is to develop and test new statistical methods for analyzing large sets of student data. The data sets to be analyzed and tested arise from administrative student data collected by San Diego State University. Additionally, the research will develop new methods of data cleaning for the student information system and learning management system data collected by the university to make the entire analysis procedures more efficient. The technical contribution is to utilize a new random forest of interaction trees machine learning method that enables the analysis of treatment effects for individuals and for subgroups (e.g., testing the success of a pedagogical or other intervention for both individual students and for specific subgroups of students). The results of the statistical analysis will be displayed as dashboards to report the findings for the assessment of intervention strategies in improving student retention and performance."
"1537011","Collaborative Research:  Designing Functional Materials with Optimal Learning","CMMI","DEMS-Design Eng Material Syst","01/01/2016","08/30/2015","Yueh-Lin (Lynn) Loo","NJ","Princeton University","Standard Grant","Kathryn Jablokow","12/31/2019","$249,999.00","","lloo@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","ENG","8086","024E, 067E, 073E, 8021, 9102","$0.00","New products and material processing methods often require the identification of novel materials that are stronger, lighter, cheaper, or better in some way.  Searching for new materials with a trial-and-error approach can be expensive and often ineffective.  With this award, new mathematical methods and computer software will be developed to accelerate materials discovery.  The planned approach will narrow the available options to those that are most likely to succeed, making discovery of new materials and processes more reliable and less costly. Demonstration of the approach will be made for materials to be used in flexible organic solar cells, but the methods could also be amenable to materials for use in pharmaceuticals or to food additives.<br/><br/>A new optimal learning approach to materials design is planned that uses advances in Bayesian experimental design and machine learning to predict material properties from previous data and domain expertise, and to intelligently suggest physical and computational experiments that will provide information that is most supportive of discovery.  These new mathematical techniques promise to greatly accelerate materials design, providing better materials more reliably and with less experimental effort.  The approach will be demonstrated in the search for organic semiconductor materials over a set of existing candidates, solvent choices, and processing conditions, and integrate both physical and computational experiments in this search. The test case is an all-organic solar cell system of contorted hexabenzocoronenes (c-HBC), deposited on carbon nanotubes (CNT).  This complex system involves issues including complexation between c-HBC and CNT at different processing conditions, etc., which provide a stringent test of optimal learning and computer simulation methods to predict the processing-structure-function triad. This approach is broadly applicable to a diverse set of materials design problems."
"1617408","III: Small: Interactive Construction of Complex Query Models","IIS","Info Integration & Informatics","07/15/2016","07/21/2016","James Allan","MA","University of Massachusetts Amherst","Standard Grant","Wei-Shinn Ku","06/30/2020","$515,994.00","","allan@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","7364","7364, 7923","$0.00","This research program will investigate and implement SearchIE, a search-based approach to information ""extraction."" SearchIE will allow rapid, personalized, situational identification of types of objects or actions in text, where those types are likely to be useful for a complex search task. Modern search engines often provide some mechanism to indicate that a query keyword matches a document only if it occurs in the name of a person or in a location. To make that possible, annotators found and marked a large number of people names (for example) in text, a machine learning algorithm was applied to learn which low-level features are indicative of the name type, and then a resulting classifier for that type is run across the collection of documents. It is then possible to write a query that means ""paris used as a person's name rather than a location.""  Unfortunately, the existing approaches do not serve searchers interested in novel, unanticipated types - for example, names of whaling ships, officers in Queen Victoria's navy, local watering holes. Such examples cannot be handled currently because the classifiers need to be trained and run ahead of time, an expensive data labeling process that is too daunting for many search tasks. Since on-line information gathering almost always starts with search and frequently involves identifying items of interest in the found text, bringing these two together has the potential to change both substantially. The SearchIE approach makes it possible for someone to build personalized extractors contextualized by their topical interests. The result is that the technology can radically improve online searching for lay persons as well as professionals by significantly reducing the time needed to focus queries into relevant information. <br/><br/>It does not appear that the information extraction task has ever been approached directly as a search task. SearchIE is unique in bringing an information retrieval (search) mindset to the extraction problem, providing new capabilities that are either impossible or extremely difficult in the traditional ""annotate then detect"" model of the problem. This project will investigate the fundamental issues raised by the SearchIE approach. What models can best integrate extraction and search in new settings where they can truly happen simultaneously? How can a searcher describe and edit a model for the types of interest? Can an interactively developed model be a springboard into a machine learned model and when is there enough information to do that? Does using topical context to limit the scope of extraction provide the expected accuracy gains using SearchIE's approach? What data structure modifications are needed to fully implement SearchIE so that it is efficient as well as effective? How well does this approach fare on additional standard test collections? Addressing the systems and algorithmic issues are fundamental problems that have the potential to greatly impact both search and extraction. For further information, see the project's web site at http://ciir.cs.umass.edu/research/searchie."
"1553465","CAREER: Interacting Dynamic Bayesian Models for Social Behavior and Reasoning","IIS","Methodology, Measuremt & Stats, Robust Intelligence","03/01/2016","02/29/2016","Katherine Heller","NC","Duke University","Standard Grant","Jie Yang","02/28/2021","$516,016.00","","kheller@gmail.com","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","1333, 7495","1045, 7495","$0.00","This project develops machine learning methods to analyze online data and illuminate social behavior. People spend a great deal of their lives socializing, or interacting with other people. Social interactions are inherently part of most of our activities, and, therefore, understanding social interactions is a fundamental part of understanding human behavior. As the amount of time people spend online rapidly grows, social interactions are increasingly occurring online.  This project uses Dynamic Bayesian models to analyze time series data of social behavior that inherently involves interactions over time. The project provides tools for understanding social behavior and better methods for data analysis. The project trains graduate students and helps high school students learn more about data analysis. The project principal investigator also maintains a strong commitment to actively involving in the organization of Women in Machine Learning Workshop to ensuring the advancement of women in STEM.<br/><br/>This research focuses on understanding the underlying structure of the interactions themselves. The research team elucidates this underlying structure through hierarchically modeling the interactions between multiple dynamic processes. More specifically, the project develops: (1) interacting dynamic Bayesian methods that can be used to model social interactions that jointly capture the temporal dynamics and the linguistic content of interactions between individuals, and discover latent attributes of individuals, such as power and influence, or roles such as bullies and victims; and (2) methods for analyzing epidemiological social network data."
"1649087","EAGER: Using Machine Learning to Increase the Operational Efficiency of Large Distributed Systems","CCF","CSR-Computer Systems Research, Software & Hardware Foundation","09/01/2016","09/01/2016","Evgenia Smirni","VA","College of William and Mary","Standard Grant","Almadena Chtchelkanova","08/31/2019","$299,994.00","","esmirni@cs.wm.edu","Office of Sponsored Programs","Williamsburg","VA","231878795","7572213966","CSE","7354, 7798","7916, 7942","$0.00","Large, distributed systems are nowadays ubiquitous and part of sustainable IT solutions to a broad range of customers and applications.  Data centers in the private or public cloud and high performance computing systems are two examples of complex, highly distributed systems: the former are used by almost everyone on a daily basis, the latter are used by computational scientists for advancing science and engineering.  High availability and reliability of these complex systems are important for the quality of user experience.  Efficient management of such systems contributes to their availability and reliability, and relies on a priori knowledge of the timing of the collective demands of users and a priori knowledge of certain performance measures (e.g., usage, temperature, power) of various systems components.<br/><br/>This  project aims to provide a systematic methodology to improve the operational efficiency of complex, distributed systems by developing neural networks that can efficiently and accurately predict the incoming workload within fine and coarse time scales. Such workload prediction can dramatically improve the operational efficiency of data centers and high performance systems by driving proactive management strategies that specifically aim to enhance reliability.  For datacenters, the focus is on actively reducing performance tickets that are automatically triggered by pro-actively managing virtual machine resizing and migration. For high performance computing systems the focus is on predicting hardware faults to autonomically improve the scheduler's efficiency, direct cooling, and improve performance and memory bandwidth."
"1621798","Asynchronous parallel stochastic frameworks with convergence guarantee for solving large-scale fixed point problems","DMS","COMPUTATIONAL MATHEMATICS, CDS&E-MSS","09/01/2016","08/30/2016","Ming Yan","MI","Michigan State University","Standard Grant","Christopher Stark","08/31/2019","$150,000.00","","yanm@math.msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","MPS","1271, 8069","8083, 9263","$0.00","In the last two decades, the size of data sets in a large number of areas has grown quickly. In many applications of machine learning, there are massive amounts of training data sets and the data sets may be collected and stored at different locations. Learning a model from these data sets imposes high demands for computation, memory, and data transfer on algorithms. Asynchronous parallel algorithms are applied to solve these large-scale problems via high performance computing and reduced communication and idle time. The performance of asynchronous parallel algorithms is improved largely comparing to synchronous parallel algorithms, especially when the number of cores is large. However, theoretical analysis on the convergence and convergence rates of these algorithms still investigation. <br/> <br/>In this proposal, the PI will develop fast and robust generic asynchronous parallel stochastic frameworks with provable convergence for solving large-scale fixed point problems that have applications in a large number of areas. One objective is to develop asynchronous stochastic algorithms for finding a zero point of a random operator, the sum of a random operator and a deterministic operator, and the sum of two random operators and show the convergence of these algorithms. Another objective is to couple coordinate updates into these asynchronous stochastic algorithms and show their convergence. The last objective is to implement these algorithms and develop software to help people without knowledge about parallel computing run asynchronous algorithms. The research in solving fixed point problems is motivated by problems in various computational sciences and engineering, and its development benefits all these fields by providing fast and robust algorithms. Areas impacted by the proposed work include machine learning, optimization, optimal control, statistics, finance, signal and image processing, compressive sensing, as well as other lines of research involving large data sets and distributed data."
"1640081","E2CDA: Type I: EXtremely Energy Efficient Collective ELectronics (EXCEL)","CCF","Energy Efficient Computing: fr, Software & Hardware Foundation","09/01/2016","07/27/2020","Suman Datta","IN","University of Notre Dame","Continuing Grant","Sankar Basu","08/31/2020","$2,865,248.00","Xiaobo Hu, Zoltan Toroczkai, Arijit Raychowdhury, Supratik Guha","sdatta@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","CSE","015Y, 7798","7798, 7945","$0.00","With billions of smart and connected devices, ""data deluge"" is a reality with more than eight zettabytes of data generated in last year alone. The primary focus of this multi-disciplinary research effort is to develop a new paradigm of computing titled Extremely energy efficient Collective Electronics (EXCEL) to enable hardware accelerated data-analytics that can extract information from unlabeled and unstructured data. This research is expected to uncover fundamentally new ways of harnessing coupled dynamical systems for solving computationally hard problems in an energy efficient way. With innovations in novel materials and devices, chip-scale dynamical system implementation, architectural changes and critical benchmarking, EXCEL will lay the foundation for a new non von-Neumann computing paradigm to achieve orders of magnitude improvement in computational energy efficiency. The outreach activities are prioritized around educating future generations of students to adapt to the forthcoming evolution and revolution in information processing systems. The research project is structured to benefit from strong engagement with industry, which will facilitate technology transfer in the future. The participating PIs are committed to developing course modules for both undergraduate and graduate students in the areas of emerging nanotechnologies, unconventional computing, machine learning and computational medicine.<br/><br/>The EXCEL project lays the foundation for a radically different approach to energy efficient information processing by leveraging emergent phenomena in novel devices and dynamics of coupled systems to execute optimization, learning and inference tasks in a collective, cooperative and scalable way. The intellectual foundation of EXCEL rests on the utilization of local information content embedded in the spatio-temporal dynamics of coupled networks (oscillatory and spiking) to perform computation in a massively parallel way. The collective computing paradigm is a timely departure from the traditional model of von-Neumann computing which relies on batch, discrete time, iterative updates (lacking temporal locality) and shared states (lacking spatial locality). EXCEL focuses on developing a complexity theoretic foundation in analog computing. This includes exploration of both continuous and discrete optimization problems as well as stochastic machines for on-line learning. The EXCEL researchers will actively pursue physical hardware demonstration and quantify their advantages over Boolean computers in solving computationally hard problems that are finding ever expanding applications in high-performance data centers, real-time cyber-physical systems and computational medicine."
"1635004","Using Mixed Discrete-Continuum Representations to Characterize the Dynamics of Large Many-Body Dynamics Problems","CMMI","Dynamics, Control and System D","09/01/2016","07/17/2016","Dan Negrut","WI","University of Wisconsin-Madison","Standard Grant","Jordan Berg","08/31/2020","$399,960.00","Radu Serban","negrut@wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","ENG","7569","030E, 031E, 032E, 033E, 034E, 035E, 039E, 040E, 099E, 1059, 7234, 8024","$0.00","The goal of this project is to understand how computer simulation can be used to predict the motion of large systems of bodies interacting with each other through friction and contact. Studying these so-called ""many-body dynamics problems"" has theoretical and practical relevance in several disciplines (physics, chemistry, astronomy, geomechanics), industries (pharmaceuticals, food processing, farming, manufacturing, construction, mining), and engineering applications (additive manufacturing, nanoparticle self-assembly, robotics, ground vehicle mobility). In terms of educational and outreach impact, initiatives undertaken as part of this project will (i) promote the discipline of Computational Science at middle and high-school levels via a ""The Science Behind Video Gaming"" short course and a residential summer program, respectively; (ii) update and expand curricula in two graduate courses on high performance computing and advanced computational dynamics; (iii) expand a biannual advanced computing forum that facilitates transfer of technology; and, (iv) provide training via a Master of Engineering distance learning program for practitioners who need to analyze, process, and solve problems using information generated by the growing use of data collection in engineering design and industrial operations.<br/><br/>The research effort will focus on investigating techniques that facilitate a discrete-continuum dual representation in the simulation of many-body dynamics problems. The fundamental question answered is how should one handle parts of a many-body dynamics problem using a continuum formalism so that the new mixed discrete-continuum representation manages to preserve the dynamics of the original problem? Also, how should a continuum representation be fine-grained into a discrete one, and conversely, what techniques should be used to coarse-grain a discrete representation into a continuous one? Mechanical engineering expertise (dynamics of many-body systems, solid mechanics, plasticity), applied math techniques (meshless methods for solving partial differential equations, optimization methods), and computer science components (machine learning, software engineering) will combine in a coordinated effort to solve the stated problem. In this context, the goal of this project is to (a) establish a systematic methodology for producing rheologies that, when embedded in a continuum mechanics model, produce a solution that is close to that of a large discrete many-body dynamics problem; and (b) use this methodology to understand whether there are rheologies that have a universal attribute; i.e., that are applicable to all, or a large spectrum of, many-body dynamics problems. In this context, a rheology is regarded as a methodology that ties at microscale the dynamics/flow of a continuum to the forces acting on it. The research plan is built around the idea of augmenting physical insights with a machine learning process that uses large amounts of data generated by fully-resolved, many-body dynamics solutions to produce rheology candidates."
"1553728","CAREER: Deciphering the human regulome: omics-based analysis of intergenic genotype-to-trait associations, made accessible and powerful","DBI","ADVANCES IN BIO INFORMATICS, Unallocated Program Costs","04/15/2016","03/09/2020","Stephen Ramsey","OR","Oregon State University","Continuing Grant","Peter McCartney","03/31/2021","$566,688.00","","stephen.ramsey@oregonstate.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","BIO","1165, 9199","1045, 1165","$0.00","This project's research activities will advance the field of bioinformatics by creating a computational method that combines different types of information from genome-wide association studies. Genetic association studies measure sequence differences across an entire genome in order to identify what variants cause the occurrence and variability of traits like height and disease susceptibility. The goal of this project is to develop methods to precisely locate gene regulatory variants, that now are only known to be somewhere in a large region, and use them to understand how traits vary in a population. The methods developed in this research aim to combine a number of types of information, like gene expression levels for cells, measurement of traits in many individuals, and comparisons with traits in other species, in order to identify causal regulatory variants. The project's curriculum development activities will contribute to STEM education by creating and sharing a hands-on workshop on genome bioinformatics in the research area. By integrating research and educational activities students will (i) gain science literacy in the areas of genetics and bioinformatics;  (ii) show how well hands-on methods work in genetics education; and (iii) use already performed genetic association studies to gain new knowledge in biology and in biomedicine.<br/><br/>This research will create and evaluate an integrative machine-learning model for identifying regulatory variants within human intergenic GWAS regions. The model's inputs will include the reference genome, the local DNA 3-D shape, phylogenetic conservation, and transcriptomic and epigenomic measurements. The model's output will be predicted regulatory variants with significance scores. The model will be benchmarked against published methods using ground-truth regulatory variants. The machine-learning model's variant predictions will be incorporated into an open-source, web-based software tool for integrative post-analysis of GWAS data. Compatibility with a cloud-computing framework will position the tool for maximum impact. Through educational activities that are integrated with the project's research activities, Dr. Ramsey will create, evaluate, and disseminate a Genome Bioinformatics Workshop unit for high school educators and students. Participants will learn to use the tool to analyze and explore human GWAS-identified regions for a model trait (height); through this they would be expected to gain a better understanding of the potential of the field of personal genomics. The workshop's materials will be developed within an interdisciplinary workshop incubator consisting of pairs of STEM-underrepresented CS and biology undergraduate summer students. We will create, evaluate, and disseminate the workshop unit in partnership with three outreach programs for STEM-underrepresented students. <br/>Project results will be made available at the project website: lab.saramsey.org/regulome"
"1736497","SHB: Small: Robustly Detecting Clinical Laboratory Errors","IIS","INFORMATION TECHNOLOGY RESEARC","09/01/2016","04/17/2017","Todd Leen","DC","Georgetown University","Standard Grant","Sylvia J. Spengler","09/30/2017","$182,230.00","","tkl13@georgetown.edu","37th & O St N W","Washington","DC","200571789","2026250100","CSE","1640","7923, 8018","$0.00","Hospital clinical laboratory tests are a major source of medical information used to diagnose, treat, and monitor patients.  Such test errors lead to delays, additional clinical evaluation, additional expense, and sometimes to erroneous treatments that increase risk to patients.  One recent study suggests that errors in measured total blood calcium concentration due to instrument mis-calibration alone cost from $60M to $199M annually in the US. However, the vast majority of clinical laboratory errors do not originate in instrument mis-calibration. Clinical laboratory errors affect about 0.5% of samples collected.  Of those, approximately 75% of clinical laboratory test errors originate during sample collection, transport, and storage before samples reach the analysis instruments i.e., the pre-analytic phase.  However the quality control measures standard in hospital clinical test labs only monitor instrument calibration and are therefore completely blind to sample faults introduced in the pre-analytic phase, where most errors originate.  Data derived from patient samples, rather than instrumentation calibration checks, holds the key to detect faults introduced in the pre-analytic phase.  Current methods are either so insensitive to errors that they do not detect sample faults reliably, or they routinely flag normal samples as being faulty.<br/><br/>This project brings together an interdisciplinary team of researchers  from Oregon Health and Science University and Northeastern University with expertise in machine learning, signal processing, and laboratory medicine to develop and apply statistical machine learning technology to reliably detect errors in hospital clinical laboratory tests, using data derived from patient samples.  The primary obstacle to developing reliable statistical detectors for lab errors is the cost of labeling samples combined with the low error rate.  Developing and evaluating any automated error-detection algorithm requires a sufficient number of samples, both faulty and non-faulty. Determining which tests are faulty requires review of the tests and other patient data (e.g. charts) by a clinical lab expert - a time-consuming and economically unfeasible prospect given the low fault rate.  The project addresses this challenge through active learning paradigms used to select, with emphasis on rare classes, subsets of the data for labeling by human experts.  The project focuses on chronic kidney disease because of its medical importance and large data repository at Oregon Health and Science University. This research will provide algorithms for clinical lab error detection that will extend to tests used in other disease entities (for example diabetes and heart failure).  <br/><br/>Ultimately, the error-detection algorithms developed from this research will make their way into clinical laboratory information systems and further into commercialization and thus deployment on a scale significant enough to have widespread positive impact on laboratory costs patient risk.  The project provides cross-disciplinary training in statistical pattern recognition and clinical laboratory science for graduate and undergraduate students. Additional information about the project can be found at:  http://www.bme.ogi.edu/~tleen/LabErrorDetect/."
"1552598","CAREER: Natural User Interfaces for Children","IIS","HCC-Human-Centered Computing","01/01/2016","02/20/2020","Lisa Anthony","FL","University of Florida","Continuing Grant","Ephraim Glinert","12/31/2020","$501,582.00","","lanthony@cise.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","CSE","7367","1045, 7367, 9251","$0.00","Natural user interfaces allow users to interact with technology through modalities like touch, gesture, and motion.  They are a key element in realizing the vision of ubiquitous computing, yet they present challenges with respect to supporting children.  The PI's research on touchscreen interaction for children has found that existing surface gesture recognition algorithms designed, trained, and tested on adult input, and interaction design guidelines developed based on adult interaction patterns, do not apply equally well to children.  For example, if a typical gesture interface is expecting a gesture to be entered as a single stroke, the system will not be able to process the multiple strokes generated by a child, leading to an unsuccessful interaction for that child.  Recognition for whole-body interaction gestures experiences similar challenges; a child is more likely to perform an action or gesture (e.g., ""jump"" or ""wave"") with greater intensity or different motion paths than an adult performing the same gesture.  In this research the PI's goal is to fundamentally advance our understanding of how to design and develop natural user interactions for children.  The research will be carried out in three phases: (1) Data Collection and Analysis: collection of input behaviors from elementary-school aged children in each modality, and analysis for patterns and characteristics of children's input; (2) Recognition and Classification: development of new recognition algorithms attuned to the expected input behavior patterns of children and use of machine learning to evaluate their performance; and (3) Multimodal Interaction: investigation of multimodal input patterns exhibited by children, and validation of new approaches to multimodal synthesis that perform well on children's natural input.  A testbed application will be developed to showcase the findings in an educational domain.  Open-source natural user interaction recognition and synthesis algorithms and clear, practicable design recommendations will be developed and released in both peer-reviewed papers and on the project website for use by researchers and practitioners.<br/><br/>This research will make fundamental advances in our understanding of child-computer interaction with natural user interfaces and develop robust new approaches for recognizing input and recovering from errors.  This work will contribute solutions to interaction design research questions, such as how to best adapt and use these new modalities for children, and machine learning research questions, such as how to develop intelligent multimodal recognition algorithms tailored for children's input.  The knowledge gained and contributions delivered by this research will inform the design of ubiquitous computing for children in learning contexts.  To these ends, the PI will create models of expected input behavior patterns for children in two natural modalities, touchscreen interaction and whole-body interaction, and use these models to develop and adapt intelligent recognition algorithms tailored to process children's input.  Multimodal interaction, or streamlined processing of disparate input from multiple simultaneous and unsynchronized input streams, is also a key component of natural user interfaces.  Like unimodal recognition, traditional approaches to multimodal fusion for adult input may also not apply well to children.  Specifically, the following research questions are targeted: What are the ways children produce interaction behaviors in multimodal natural user input modalities? What interaction design techniques are most effective for children using multimodal natural user input modalities? What new multimodal recognition and fusion algorithms for natural user interaction modalities perform effectively on children's input?  This work will be conducted in partnership with schools and teachers, allowing the research findings to have immediate impact on how real children are using natural user interaction technology in educational contexts.  The project will involve both undergraduate and graduate research assistants, broadening participation in computer science by recruiting women and underrepresented minority students.  Education plans focus on developing a new undergraduate certificate in Human-Centered Computing."
"1700506","CAREER: Smart Sampling and Correlation-Driven Inference for High Dimensional Signals","ECCS","CCSS-Comms Circuits & Sens Sys","07/01/2016","10/20/2016","Piya Pal","CA","University of California-San Diego","Standard Grant","Lawrence Goldberg","12/31/2020","$500,000.00","","pipal@eng.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","ENG","7564","1045, 153E","$0.00","Technological advances have driven modern sensing systems towards generating massive amounts of data, making it increasingly challenging to store, transmit and process such data in a cost effective and reliable manner.  However, the ultimate goal in many information-processing tasks is to infer some parameters of interest, that govern the statistical and physical model of the data. This includes applications ranging from source localization in radar and imaging systems to inferring latent variables in machine learning.  The number of parameters in such problems is much smaller than the acquired volume of data, which leads to the possibility of more intelligent ways of sensing high dimensional signals, that can exploit the statistical model of the signal (with or without invoking sparsity), and the physics of the problem. The objective of this project is to develop a systematic theory of smart sampling and information retrieval algorithms for modern sensing systems that exploit the correlation structure of high dimensional signals to significantly reduce the number of measurements needed for inference.  The proposed research can lead to deployment of fewer sensors (than what is traditionally required), as well as more energy efficient ways to collect and process spatio-temporal data that will positively impact a number of applications across disciplines, such as, high resolution imaging, remote sensing, neural signal processing and wireless communication.  The educational component of this project aims at integrating the research outcomes into innovative teaching platforms such as ''Sense Smarter'', and ''Signals Everywhere'' that will help train the next generation of electrical engineers, and encourage them to pursue careers in STEM fields.<br/><br/>The technical component of the project has three interconnected goals: (i) designing fundamentally new geometries for correlation-aware samplers that exploit the statistical as well as physical signal models, (ii) developing, and analyzing the performance of new correlation driven algorithms to understand fundamental capabilities of correlation-aware samplers, and (iii) exploiting the ideas behind correlation-aware samplers to develop more efficient algorithms for solving bi- and multi-linear problems. Design of these samplers will provide new theoretical insights into properties of quadratic samplers, and will help address fundamental mathematical questions that can be of independent interest.  The samplers also facilitate the development of new inference strategies, and the proposed rigorous theoretical analysis of these algorithms is expected to fundamentally advance our current understanding of the limits of parameter estimation from compressed data. Finally, the ideas behind correlation-aware samplers have strong connections with problems in machine learning such as dictionary learning, and latent variable analysis, and they will foster future research advances in these areas."
"1553954","CAREER: Smart Sampling and Correlation-Driven Inference for High Dimensional Signals","ECCS","CCSS-Comms Circuits & Sens Sys","01/01/2016","01/06/2016","Piya Pal","MD","University of Maryland College Park","Standard Grant","chengshan xiao","11/30/2016","$500,000.00","","pipal@eng.ucsd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","ENG","7564","1045, 153E","$0.00","Technological advances have driven modern sensing systems towards generating massive amounts of data, making it increasingly challenging to store, transmit and process such data in a cost effective and reliable manner.  However, the ultimate goal in many information-processing tasks is to infer some parameters of interest, that govern the statistical and physical model of the data. This includes applications ranging from source localization in radar and imaging systems to inferring latent variables in machine learning.  The number of parameters in such problems is much smaller than the acquired volume of data, which leads to the possibility of more intelligent ways of sensing high dimensional signals, that can exploit the statistical model of the signal (with or without invoking sparsity), and the physics of the problem. The objective of this project is to develop a systematic theory of smart sampling and information retrieval algorithms for modern sensing systems that exploit the correlation structure of high dimensional signals to significantly reduce the number of measurements needed for inference.  The proposed research can lead to deployment of fewer sensors (than what is traditionally required), as well as more energy efficient ways to collect and process spatio-temporal data that will positively impact a number of applications across disciplines, such as, high resolution imaging, remote sensing, neural signal processing and wireless communication.  The educational component of this project aims at integrating the research outcomes into innovative teaching platforms such as ''Sense Smarter'', and ''Signals Everywhere'' that will help train the next generation of electrical engineers, and encourage them to pursue careers in STEM fields.<br/><br/>The technical component of the project has three interconnected goals: (i) designing fundamentally new geometries for correlation-aware samplers that exploit the statistical as well as physical signal models, (ii) developing, and analyzing the performance of new correlation driven algorithms to understand fundamental capabilities of correlation-aware samplers, and (iii) exploiting the ideas behind correlation-aware samplers to develop more efficient algorithms for solving bi- and multi-linear problems. Design of these samplers will provide new theoretical insights into properties of quadratic samplers, and will help address fundamental mathematical questions that can be of independent interest.  The samplers also facilitate the development of new inference strategies, and the proposed rigorous theoretical analysis of these algorithms is expected to fundamentally advance our current understanding of the limits of parameter estimation from compressed data. Finally, the ideas behind correlation-aware samplers have strong connections with problems in machine learning such as dictionary learning, and latent variable analysis, and they will foster future research advances in these areas."
"1617986","CIF: Small: Advancing Adaptive Importance Sampling for Signal Processing","CCF","Comm & Information Foundations","06/15/2016","06/24/2016","Monica Fernandez-Bugallo","NY","SUNY at Stony Brook","Standard Grant","Phillip Regalia","08/31/2020","$498,493.00","","monica.bugallo@stonybrook.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","7797","7923, 7936","$0.00","There are many applications where the interest is in learning about unknowns from observed data. The goals range from predicting future data to learning about scientific or societal truths. Bayesian signal processing allows for explicit incorporation of all available information about an addressed task. It amounts to optimally combining common-sense knowledge and observational evidence. Due to its strength and appeal, Bayesian modeling and analysis has been embraced by all of science and engineering. However, the main current problems are those with large numbers of unknowns (complex systems) and/or large amounts of data (big data). This raises the concern that Bayesian inference may become computationally incapable of handling them because of the sheer size and complexity of the studied systems. The goal of this project is to advance the theory and practice of a class of Bayesian methods, adaptive importance sampling (AIS), for dealing with problems where the numbers of unknowns and/or data are large.<br/><br/>This project focuses on building a novel framework for AIS that will extend its use for Bayesian inference to problems with large amounts of unknowns and/or data. The research involves investigating in greatest detail the intricacies of AIS on several areas including (a) novel schemes for AIS with emphasis on new strategies for adaptive learning and for stable weight computation, and on advanced approaches for dealing with high dimensional models and big data, (b) model selection and machine learning, (c) global optimization, and (d) application to a case-study where understanding the progression of cancer from cancer stem cells is of interest."
"1619308","III: Small: Robust Large-Scale Data Mining for Knowledge Discovery in Depression Thought Records","IIS","Info Integration & Informatics","08/01/2016","07/21/2016","Heng Huang","TX","University of Texas at Arlington","Standard Grant","Aidong Zhang","09/30/2018","$500,000.00","","heng.huang@pitt.edu","701 S Nedderman Dr, Box 19145","Arlington","TX","760190145","8172722105","CSE","7364","7364, 7923","$0.00","This project investigates new robust large-scale data mining and machine learning algorithms to solve critical computational challenges in mining massive depression thought records for cognitive behavior therapy. Depression is rapidly emerging as one of the major problems in our society and is also related to many other health conditions, such as stroke, diabetes, hypertension, HIV/AIDS, etc. Cognitive behavior therapy is the most extensively researched form of psychotherapy for depression, and the depression thought records from patients is the key component of cognitive behavior therapy. However, the process of reviewing and analyzing the depression thought records is extremely time consuming, which inhibits both clinical interviews and the training of new therapists. This project builds a novel data mining system to automatically discover knowledge from depression thought records for assisting therapists in selecting potential interventions and aiding new therapists in their development of cognitive behavior therapy skills. This project will facilitate the development of novel educational tools to enable new courses and enhance current courses. This project engages minority students and under-served populations in research activities to give them a better exposure to cutting-edge science research.<br/> <br/>To effectively and efficiently analyze large-scale depression thought records, this project explores the following research tasks. First, the project develops a robust semi-supervised learning model to categorize logical thinking errors of depression thought records. Second, the project investigates a joint multi-task method to simultaneously recognize the categories of thinking errors and emotions of depression thought records. Third, new multi-label and multi-instance learning is studied for identifying coping activities. Fourth, to analyze the multi-language depression thought records, robust transfer learning methods are developed for cross-language knowledge transfer. Meanwhile, parallel computational algorithms are designed and applied for large-scale depression thought record data mining. These novel data mining algorithms are designed to solve large-scale applications and automate the depression thought record data mining, which holds great promise for smart health."
"1647361","EAGER: Feedback-based Network Optimization for Smart Cities","CMMI","S&CC: Smart & Connected Commun","08/01/2016","07/28/2016","Ilya Safro","SC","Clemson University","Standard Grant","Yueyue Fan","07/31/2019","$151,074.00","Mashrur Chowdhury","isafro@clemson.edu","230 Kappa Street","CLEMSON","SC","296345701","8646562424","ENG","033Y","029E, 036E, 1057, 7916, 9150, CVIS","$0.00","This EArly-concept Grant for Exploratory Research (EAGER) project will focus on human-infrastructure interactions within future smart cities. Specifically, the goal of this project is to develop a conceptual network optimization framework that exploits user feedback from crowd-sourced data. The next-generation ubiquitous traffic sensors, such as fixed traffic detectors, mobile sensors with location-based services (e.g., Google traffic), and traffic active mobile sensors (e.g., Waze), have been used to retrieve large and diverse geo-located and time-stamped data. In addition, data crowd-sourced from social media and mobile applications are increasingly available for understanding human mobility. On the other hand, existing network optimization models in transportation were largely developed without considering human behavior. This project will explore methods to design an electric vehicle wireless charging network of the future. Such a charging network will provide non-stop, in-motion charging particularly suitable for urban environments. For many users, wireless charging will be an opportunistic, emergency charging choice that supplements distributed charging resources at home, the workplace, and at retail facilities. Traditional network optimization models, which implicitly assume that charging demand is distributed and a given, may not result in a user-satisfactory solution in such tasks as finding the best routes that contain wireless charging network  segments. The PIs aim to overcome this deficiency by developing: (i) user feedback-driven network optimization models that explicitly account for user satisfaction, and (ii) fast and scalable optimization methods for real-world, large-scale problem implementations that efficiently and effectively utilize crowd-sourced information. The results of this research will include a ""living lab"" assessment in which students will develop a mobile app to collect feedback about the route with wireless charging network segments. This feedback will be analyzed and incorporated in optimization models.<br/><br/>This approach will be implemented using a combination of large-scale machine learning and optimization solvers. The mobile app will provide basic anonymized information about users, trip types, and route feedback through secured channels. These data and open information about all road segments will be used to explore relevant latent factors and create cost-sensitive support vector machine based classifiers to identify the most suitable segments for wireless charging network and adjust the optimization-based network design. Rather than using traditional, computationally expensive optimization solvers, the PIs will pursue an algebraic multigrid-based approach to cope with large-scale, real-world problems, and leverage their support vector machine solvers."
"1621444","SBIR Phase I:  Predictive Algorithms for Water Point Failure","IIP","SBIR Phase I","07/01/2016","06/20/2016","Evan Thomas","CO","SweetSense Inc.","Standard Grant","Rick Schwerdtfeger","07/31/2017","$224,562.00","","ethomas@colorado.edu","2536 N Gilpin St","Denver","CO","802050000","3035504671","ENG","5371","033E, 152E, 5371, 8033","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is in creating a market for financially sustained and accountable water services in developing countries. The impact of improved water, sanitation, and hygiene on public health is significant, and has the potential to prevent at least 9.1% of the global disease burden and 6.3% of all deaths. Present-day approaches for delivering water services in developing countries typically focus on deploying, maintaining, and monitoring aid-projects for only a few years. Impact is nominally evaluated by implementers (non-profit, private and government alike) directly. However, even when a positive impact is measured, the majority of these environmental service and monitoring interventions are short-term, and measurements may be misleading. For example, a multi-decade project apparently increased access to clean water supplies in rural areas from 58% in 1990 to 91% in 2015. Improved services may be realized through preventative and ""just in time"" maintenance activities, enabled through instrumentation and predictive failure data analysis algorithms. This may, critically, enable zero-interruption in water supply. Intermediate access to water, caused by water point failure, to clean water is known to increase health risks.<br/><br/>This Small Business Innovation Research (SBIR) Phase I project intends to develop predicative machine learning algorithms for water point failures derived from cellular reporting electronic sensors installed on rural water infrastructure in developing countries. The innovation proposed for research in this proposal consists of employing an ensemble of robust machine learning classification techniques, using cross-validation methods to tune model parameters and evaluate performance, in order to develop a data-adaptive system capable of predicting failure well enough in advance to allow preventive maintenance, repair or replacement.  Specifically, we will first examine condition based maintenance. Condition based maintenance has several advantages over time based maintenance, especially the ability to allocate limited maintenance resources where they are needed, instead of spreading maintenance resources evenly, including where they may not be needed. Our proposed Phase 1 SBIR focuses on developing predictive algorithms for water point failures using our existing sensor hardware and applied to existing customers. Our success criteria for a Phase 1 SBIR is a predictive algorithm that can accurately identify water points in near-failure."
"1541069","CRISP Type 2: Collaborative Research: Towards Resilient Smart Cities","OAC","CIS-Civil Infrastructure Syst, Information Technology Researc, CYBERINFRASTRUCTURE, CSR-Computer Systems Research","01/01/2016","06/27/2018","Narayan Mandayam","NJ","Rutgers University New Brunswick","Standard Grant","William Miller","12/31/2020","$926,000.00","Arnold Glass, Janne Lindqvist","narayan@winlab.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","1631, 1640, 7231, 7354","008Z, 029E, 036E, 039E, 7218, 7433, 8004, 9251","$0.00","Realizing the vision of truly smart cities is one of the most pressing technical challenges of the coming decade. The success of this vision requires synergistic integration of cyber-physical critical infrastructures (CIs) such as smart transportation, wireless systems, water networks, and power grids into a unified smart city. Such smart city CIs have significant resource dependence as they share energy, computation, wireless spectrum, users and personnel, and economic investments, and as such are prone to correlated failures due to day-to-day operations, natural disasters, or malicious attacks. Protecting tomorrow's smart cities from such failures requires instilling resiliency into the processes that manage the city's common CI resources. Such processes must be able to adaptively and optimally reallocate smart city resources to recover from failure. The goal of this Critical Resilient Interdependent Infrastructure Systems and Processes (CRISP) collaborative research project is to address this fundamental challenge via a coordinated and interdisciplinary approach that relies on machine learning, operations research, behavioral economics, and cognitive psychology to lay the mathematical foundations of resilient smart cities. The anticipated results will break new ground in the understanding of synergies between multiple cyber-physical infrastructure and resilient resource management thus catalyzing the global deployment of smart cities. This research will yield advances to the areas of resilient systems, cyber-physical systems, security and privacy engineering, game theory, computer and network science, behavioral economics, data analytics, and psychology. The project will involve students from diverse backgrounds across engineering, computer science, economics, and psychology that will be trained on pertinent research issues related to smart cities and resiliency. The project will also contribute to fostering trust between residents and the various technological processes that are fundamental to the operation of a smart city.<br/><br/>This research will introduce a foundational, transformational, analytical framework for leveraging synergies between a city's CIs to yield resilient resource management schemes cognizant of both technological and human factors. By bringing together researchers from interdisciplinary fields, this framework yields several advances: 1) Rigorous mathematical tools for delineating the inter-dependencies between CIs via a complementary mix of novel tools from graph theory, power indices, machine learning, and random spatial models; 2) Resilient resource management mechanisms that advance notions from frameworks such as behavioral game theory to enable optimized management of shared CI resources in face of failures stemming from agents of varying intelligence levels; 3) Behavioral models for characterizing the trust relationships between the residents of a smart city and the CIs; 4) Behavioral studies that provides guidelines on how to influence the users of the CIs in such a way so as to improve the resiliency of the CIs; and 5) Large-scale smart city simulators coupled with realistic experiments that will bridge the gap between theory and practice. The insights from this project will apply to the future scientific cyber-infrastructures that are likely to be interconnected as well as interdependent. The simulator will be a software artifact that would be a useful component of a scientific cyberinfrastructure aimed at understanding (for example) smart cities."
"1659160","I-Corps: Real-Time Update System for Hospitals","IIP","I-Corps","12/01/2016","03/30/2018","Jean Walrand","CA","University of California-Berkeley","Standard Grant","Anita La Salle","10/31/2018","$50,000.00","","wlr@eecs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project is a significant increase in the efficiency of hospital procedure scheduling.  The project analyzes the sources of inefficiencies in the planning, staffing, coordination, and execution of the various phases of surgeries and associated procedures.  Preliminary analysis of hospital data shows that the throughput of surgical units can be increased significantly with the same staffing while reducing the delays and improving the working conditions of the personnel. Delays in obtaining and communicating updates on the status of surgeries and on actions that personnel should perform are major causes of inefficiency, as is the randomness of the duration of tasks. We expect the methodology to be widely applicable to the coordination of teams in services and transportation industries.<br/><br/>This I-Corps project is based on a machine learning approach to the optimization of real-time messaging tuned using actual hospital data. The technical novelty is a formulation of the real-time policies that incorporates the messages between the main actors and enables the use of machine learning to optimize the policies.  The approach combines new parametric models of real-time scheduling, stochastic gradient descent, and infinitesimal perturbation analysis. In this formulation, perturbation analysis computes the gradient of the objective function with respect to the timing of messages and results in an efficient algorithm.  The algorithm discovers the best time to send messages to optimize a combination of operating room efficiency and patient waiting times."
"1628832","I-Corps:  VeriSight CPS: Enhancing the Design and Operation of Cyber-Physical Systems with Verified Insight","IIP","I-Corps","03/01/2016","02/29/2016","Sanjit Seshia","CA","University of California-Berkeley","Standard Grant","Steven Konsek","02/28/2017","$50,000.00","","sseshia@eecs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","ENG","8023","","$0.00","Cyber-Physical Systems (CPS) tightly integrate computation with physical processes. Examples include modern automobiles, medical devices, toys, drones, robots, smart thermostat and HVAC systems and many more. The commercial potential of these systems has been recognized with the growing buzz around the ""Internet of Things (IoT)."" As the design of CPS/IoT systems goes mainstream, from a research, hobbyist, and niche industrial activity to a mainstream, large-scale industry, the design challenges are mounting. A primary challenge is to help designers and users gain better insight into their systems: what behaviors the systems must and must not have, why they exhibit certain desired/undesired behaviors, and how to design and implement them to achieve desired behavior. This project proposes to create a software toolkit, VeriSight CPS, and investigate its capabilities to meet this challenge. VeriSight can be used to answer queries about a system at various phases of the design and operation of a system. Industrial impact of the VeriSight toolkit is anticipated in several areas, including transportation, robotics, and medical devices.<br/><br/>The technical goals of this project are to develop a software toolkit to assist in the specification, design, verification, debugging,optimization, and maintenance of cyber-physical systems, and to investigate its effectiveness in a focused set of applications of high impact. The underlying theory is a novel blend of formal methods for design automation and machine learning. Unlike mainstream machine learning and data analytics techniques, VeriSight CPS involves the use of verification technology to analyze models and to answer queries. The envisioned contributions of the project include a general software architecture for VeriSight CPS applicable to multiple application domains, the investigation of various user interaction models, and an exploration of industrial applications in key CPS domains including transportation, medical devices, and robotics. A key part of this Innovation Corps (I-Corps) project will be to interview potential customers, e.g., those from the categories listed above, and determine which options will be the most fruitful."
"1639675","EarthCube Building Blocks:  Collaborative Proposal: Polar Data Insights and Search Analytics for the Deep and Scientific Web","ICER","Polar Cyberinfrastructure, EarthCube","09/01/2016","09/16/2016","Siri Jodha Khalsa","CO","University of Colorado at Boulder","Standard Grant","Eva Zanzerkia","08/31/2020","$294,999.00","","khalsa@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","GEO","5407, 8074","1079, 7433","$0.00","This project develops an NSF EarthCube Building Block focused on Polar Data Science. The system will build upon work in Information Retrieval and Data Science and upon existing investment from NSF Polar, EarthCube, and from DARPA and NASA in this area. The system will collect, analyze, and make interactive the wealth of textual and scientific Polar data collected to date across the Deep web of scientific information -- scientific journals, multimedia information, scientific data, web pages, etc. The system builds upon fundamental research in text analysis, search, and visualization. Its primary goal is to unlock unstructured scientific data from 90+ data formats and to scale to 10s-100s of millions of records using the NSF XSEDE supercomputing resources. The system will perform information retrieval and machine learning on data crawled from the Polar Deep and Scientific web. Crawling will be informed by science questions crowdsourced through the EarthCube and Polar communities. The project is a collaboration with NSIDC, Ronin Institute, and the broader community including the newly funded Arctic Data Center led by NCEAS, to build our proposed system.<br/><br/>The result of periodic and regular crawling will be a Crawl Data Repository (CDR) of raw textual data e.g., web pages containing richly curated dataset abstract descriptions, news stories tied to datasets, ASCII note files and dataset descriptions, and other textual data available on or pointed to by Polar repositories as well as scientific data (HDF, Grib, NetCDF, Matlab, etc.). The CDR will be made available for historical and future analysis by the broader EarthCube and Polar communities. In addition, an extraction pipeline will generate an Extraction Data Repository (EDR) of machine learning features not previously present (geospatial, temporal, people, places, scientific publications and topics, etc.) that will be the basis of interactive, visual analytics over the Polar data resources. Information collected will assist in answering scientific questions such as these derived from the President?s National Strategy for the Arctic Region. To date, the team has also crowd sourced 30+ questions from the Polar community represented on CRYOLIST https://goo.gl/4dDyIS and will continue to solicit this feedback and use the information collected to aid science as prioritized by the community. They will also engage the community to assist in validating our system. This is not a predictive tool per-se ? though it can help to enable such predictions. Its focus is on building an operational and core capability for textual scientific data analysis, both retrospective, and prospective."
"1558636","Collaborative Research: Inference Methods for Machine Learning and High-Dimensional Data in Policy Evaluation and Structural Economic Models","SES","Economics, Methodology, Measuremt & Stats","05/15/2016","05/03/2016","Christian Hansen","IL","University of Chicago","Standard Grant","Nancy Lutz","04/30/2019","$191,462.00","","christian.hansen@chicagobooth.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","SBE","1320, 1333","1320, 1333","$0.00","Much of empirical economics focuses on estimating and drawing credible inferences about the causal effects of economic policies or about features of underlying economic models such as elasticities.  The type of data that researchers have at their disposal to aid in this task is increasingly rich and complex.  While these increased data resources open up many new opportunities, they also pose additional challenges as researchers must employ data-reduction techniques - for example, techniques from the analysis of ""big data"" - to make analyzing complex data and models feasible and informative, and nave application of such techniques may render conclusions drawn about economic effects invalid.  This research project will establish a general, formal framework to provide guidance about construction of estimation and inference devices coupled with appropriate use of tools from ""big data"" or data-mining that will deliver reliable conclusions about economic objects of interest.  The proposed research will present the methods and corresponding theoretic guarantees to cover a variety of situations encountered in empirical research in economics and the social sciences, offer empirical applications, and provide usable software in statistical packages popular within the social sciences.  The theoretical and empirical work will thus help bridge the gap between social science practice and ""big data"", and will provide methods that will enhance the credibility of the drawn scientific conclusions.  <br/><br/>The proposed research will provide bridges between high-dimensional statistical modeling and applied social science research.  Integrating high-dimensional methods with economically relevant modeling frameworks and targets is important in providing researchers tools which can be used to analyze modern, complex data and provide reliable inferential statements about the objects of interest.  The proposed research will advance the theory of inference following regularization which is a key element to inference in modern, large data sets.  The main goal of this research project is to generalize available results about inference for a low-dimensional target parameter of interest by providing an encompassing framework that will include interesting nonlinear models and estimation procedures such as maximum likelihood and generalized method of moments.  We will also provide an extension to cover cases where the target of interest is function valued, such as when interest is in a set quantile treatment effects across a range of quantile indices.  This advancement will expand the frontier for applications of high-dimensional methods in applications where inference about sets of model parameters is the goal.  This expansion is useful even in low-dimensional models and is likely to become crucial as large, complicated data sets become more readily available.  In addition to providing theoretical results, the research aims to provide illustrative empirical examples and software in both R and Stata for application of these methods."
"1549015","SBIR Phase I: Decoding Obfuscated Text to Find Trafficking Victims","IIP","SMALL BUSINESS PHASE I","01/01/2016","06/29/2016","Andreas Olligschlaeger","PA","Marinus Analytics LLC","Standard Grant","Peter Atherton","12/31/2016","$179,879.00","","olli@marinusanalytics.com","4620 Henry Street","Pittsburgh","PA","152133715","8669452803","ENG","5371","163E, 5371, 8032","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project goes beyond combating sex trafficking in the United States.  The proposed technology finds compounded influence with extension to countries in North America, Asia, and Europe, especially those with sufficient Internet penetration.  This innovation will enable previously impossible information extraction algorithms to be applied to data on trafficking activity.  The United States will lead by example by empowering its law enforcement to find and rescue victims of human trafficking and prosecute their exploiters, showing the world that exploitation will not be tolerated in our society.  The proposed innovation will also increase collaboration across fragmented jurisdictions domestically and internationally, streamlining investigative workflows and enhancing productivity.  The capability to decode Unicode characters into meaningful intelligence is an important and novel innovation impacting other investigations related to black market economies, which are becoming increasingly commonplace.  Many detectives need analogous capabilities in domains including drug, gun, and animal trafficking and the sale of counterfeit goods online (e.g. pharmaceuticals, licensed merchandise) to empower them to find patterns left by perpetrators.  Both international expansion of existing tools and expansion of domains served will multiply the commercial potential of this project.<br/><br/>This Small Business Innovation Research (SBIR) Phase I project will develop new machine learning technology to decode patterns of text used by sex traffickers in online advertisements to evade prosecution. Recently, criminals have developed new ways to avoid law enforcement detection, particularly by use of look-alike Unicode characters and symbols, which cannot be easily translated by a computer for automated search or deeper analysis. There is no existing solution that can comprehensively and accurately decode the obfuscated information. We will apply machine learning methods to train a model to predict the appropriate Latin character translation most likely to be represented by alternative symbols, allowing us to automatically and predictively decode obfuscated text. We will produce a set of algorithms to solve this problem, empowering law enforcement to stay ahead of criminal tactics. Beyond the United States, we will deploy the innovation internationally, as well as into new domains, including but not limited to, drug, gun, and animal trafficking, spam, phishing schemes, and attempts to avoid keyword-based alerting systems. These illicit activities are a significant detriment to both our society and economy, and urgent solutions are needed to give those who combat these activities the tools they need to stay ahead."
"1618563","CSR: Small: Performance and Fairness with Multiple Page Sizes","CNS","Special Projects - CNS, CSR-Computer Systems Research","10/01/2016","06/18/2019","Christopher Rossbach","TX","University of Texas at Austin","Standard Grant","Matt Mutka","09/30/2020","$516,000.00","Emmett Witchel","rossbach@cs.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","1714, 7354","7354, 7923, 9251","$0.00","Modern computing workloads such as machine learning, big data analytics, and scientific computing require enormous memory capacities. The system software that manages memory was designed for much smaller systems, and consequently, it imposes hefty performance and power penalties on modern systems. The time has come to pay this technical debt. This work comprehensively redesigns operating system and virtual machine memory management to accommodate large memories, enabling applications to obtain the performance and efficiency promised by current hardware.<br/><br/>The goal of this research is a set of principles and a framework for the operating system and hypervisor to transparently support small and large memory pages. Large memory pages (e.g., 2MB pages for the popular x86 architecture instead of the standard 4KB) can provide significant performance benefit by dramatically reducing address translation over- heads. However, their support and adoption has been hindered by fundamental management problems arising from fragmentation and poor visibility into memory accesses. Current large page management suffers a variety of pathologies such as memory bloat and unfairness across processes and/or virtual machines, and system administrators generally disable large pages in production systems. A framework that relies on managing contiguity as a first-class resource and on tracking utilization and access frequency of memory pages will enable an OS to coordinate its currently disparate mechanisms, avoid performance pathologies, and enable applications to enjoy the performance benefits of large pages."
"1724008","III: Small: Reconstructing viral population without using a reference genome","IIS","ADVANCES IN BIO INFORMATICS, Information Technology Researc, Cross-BIO Activities, Info Integration & Informatics","07/01/2016","07/15/2020","Raj Acharya","IN","Indiana University","Continuing Grant","Sylvia Spengler","08/31/2021","$428,203.00","","acharya@cse.psu.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","CSE","1165, 1640, 7275, 7364","7364, 7923, 8750","$0.00","Next-generation sequencing (NGS), which allows sampling millions of short DNA sequences from a genome, has revolutionized the field of genomics. One area of particular importance is the reconstruction of genomes (haplotypes) from a viral population, which is a fundamental problem in virology, evolutionary biology, and human health. Though there have been several methods developed to take advantage of NGS data, those are limited to populations for which a reference genome is available. This excludes many important cases, such as RNA viruses or certain HIV/HCV viral populations. In such situations, the haplotypes are sufficiently divergent as to render the reference meaningless. Moreover, most algorithms are not robust in the presence of recombination, which is a common occurrence in many viral populations. The achievement of this project's aims will allow for the full potential of NGS data to be realized in the field of virology. In particular, it will help to propel the understanding of viral population dynamics and give biologists powerful tools to understand disease progression and enable novel treatment and prevention strategies.  The algorithms and software developed will be made freely available for use through software sharing platforms like GitHub or Galaxy. The PIs will offer a strong educational component including (a) graduate and undergraduate classes that use the output of the proposed research, and (b) development of a seminar series. The PIs will (a) train future generations of scientists and engineers to enhance and use bioinformatic/genomic cyber resources; (b) facilitate creative, cyber-enabled boundary-crossing collaborations, including those with industry and international dimensions, to advance the frontiers of science and engineering and broaden participation in STEM fields.<br/><br/>This project?s aim is to develop probabilistic De Bruijn graphs and network flow on such graphs for the reconstruction of viral population when a reference is not available. Given NGS data, the algorithms should determine the number, sequences, and relative frequencies of the haplotypes.  This project's proposed algorithms are based on a unique combination of established techniques (e.g. maximum likelihood, expectation-maximization, clustering, Lander Waterman statistics) with novel propositions for probabilistic De Bruijn graphs, machine learning, and network flows that are of interest in other applications. The PI and Co-PIs have complementary backgrounds in virology, machine learning, network flow, and genome reconstruction problems."
"1616547","Using Automated Classification of Appearance to Reveal the Physics of Galaxy Formation","AST","EXTRAGALACTIC ASTRON & COSMOLO","09/01/2016","07/02/2018","Preethi Nair","AL","University of Alabama Tuscaloosa","Continuing Grant","Nigel Sharp","08/31/2021","$506,505.00","Jeremy Bailin","pnair@ua.edu","801 University Blvd.","Tuscaloosa","AL","354870001","2053485152","MPS","1217","1206, 7480, 9150","$0.00","Galaxies are the basic building blocks of the Universe.  Knowing how they grow is a key challenge of current astronomy, and this history is shown by their appearance.  Unfortunately, right now this work uses only small samples studied by eye.  To sort out the physics needs very large numbers of objects.  Future surveys will provide the data but visual inspection will not be feasible.  This study will apply advanced methods from the field of machine learning.  Comparing real galaxies from space-based and ground-based data to computer models will make major progress in this all-important field.  The lead researcher will work hard to overcome major barriers acting against women and minorities in scientific fields.  This involves several different targeted activities.<br/><br/>Understanding the physical processes responsible for the growth of galaxies is one of the key challenges in extragalactic astronomy.  The assembly history of a galaxy is imprinted in its detailed morphology, but current quantitative classification schemes are only useful for broad binning.  Thus, the comparison of observations with theoretical predictions has only used small samples of visually classified galaxies.  Coping with the large samples needed to disentangle the complex physics involved requires a robust quantitative classification scheme.  This study will implement a promising machine learning algorithm that has proven successful at identifying bars, single-armed or multi-armed galaxies, rings and Hubble type.  The project will: determine a robust quantitative classification and apply it to multiple ground and space based data sets; study the dependence of bars, rings and spiral arms on properties such as mass, star formation, environment and redshift; determine triggering and destruction mechanisms of bars and spiral arms; compare observed trends to those of simulated galaxies; and apply this robust classification scheme to 3D data cubes to compare stellar, gas and kinematic morphologies.  The work for countering the underrepresentation of women and minorities in STEM fields includes more frequent public observing nights, buying a portable planetarium to encourage daytime visits by university and school students, creating planetarium-based lecture-tutorials and interactive demonstrations for teachers and pupils, and arranging discussions with former physics majors about alternate career pathways."
"1549761","STTR Phase I:  Wearable System for Mining Parkinson's Disease Symptom States in an Ambulatory Setting","IIP","STTR Phase I","01/01/2016","12/21/2015","Britta Ulm","PA","Abililife","Standard Grant","Jesus Soriano Molla","05/31/2017","$224,997.00","Dong Huang","Britta@abililife.com","6024 Broad Street Suite 2","Pittsburgh","PA","152063010","3035488523","ENG","1505","1505, 8018, 8038, 8042","$0.00","The broader impact/commercial potential of this Small Business Technology Transfer (STTR) Phase I project is to mitigate fall risk among Parkinson's patients and the elderly, which will potentially save families and patients $34 billion annually in fall-related injuries and rehabilitation. Falling is common among individuals age 65 and over, one in three people fall at least once in a calendar year and Parkinson's patients are twice as likely to fall as their counterparts. It is expected that the proposed technology will provide a holistic view of a patient's health. The real-time data detected by the integrated sensors offers information that consumers and caretakers can use to plan health strategies at home and with their physicians. The proposed technology is expected to fit seamlessly into the lives of consumers so that they benefit from the power of technology without the difficulty utilizing it. Several fall detection systems are currently on the market; however, the two main issues with these systems are compliance and detection. No existing fall prevention devices determine when a patient's risk of falling is elevated. The proposed technology has the potential to enter the personal emergency response systems (PERS) market, which is estimated to grow to $1.86 billion by 2017.<br/><br/>The proposed project addresses consumers' need to monitor and be proactive about their chronic health symptoms, particularly as they relate to falls. The goal of the proposed project is to develop and commercialize a device that predicts when a fall is likely to occur and to provide actionable feedback. We will use machine learning to achieve the proposed research objective by analyzing data collected from sensors embedded in a back brace to develop algorithms that will predict symptom onset and alert Parkinson's patients and caretakers to increased fall risk. The algorithms developed to analyze the data collected from the sensors on the proposed technology are the intellectual merit of this project. The machine learning algorithms will be used to find a correlation between sensor readings and symptoms the individual is experiencing. The anticipated results are that the correlations found in the data will lead to a better understanding of the individual's symptoms, disease progression, and which sensor readings indicate increased fall risk. Understanding the mechanisms that cause individuals with Parkinson's to fall will develop better alert systems and improve fall prevention. The results from data collection and analysis could also lead to better detection of early warning signs of Parkinson's progression."
"1617630","RI: Small: Incremental Sampling-Based Algorithms and Stochastic Optimal Control on Random Graphs","IIS","Robust Intelligence","06/15/2016","07/24/2017","Panagiotis Tsiotras","GA","Georgia Tech Research Corporation","Continuing Grant","Erion Plaku","12/31/2020","$335,757.00","","p.tsiotras@ae.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7495","7495, 7923","$0.00","Autonomous and semi-autonomous vehicles and systems have become indispensable both for civil (fire-fighting, nuclear waste handling, law-enforcement, deep ocean exploration and drilling, weather forecasting, transportation) and military (guided missiles, spacecraft, unmanned drones) applications. Automation, when coupled with information technology, will continue to permeate our society at ever increasing levels. Autonomous systems, which, thus far, have been a crucial component in homeland security applications (e.g., border patrol, persistent monitoring, etc), are now seen as a key factor of empowering people in their daily lives across work, leisure, and domestic tasks. The next generation of autonomous systems will operate and interact with humans in the household or the office. The recent investment of information technology companies such as Amazon and Google in robotics technology is likely to accelerate the adoption of these new technologies by the general public. The safe and reliable operation of all these autonomous systems hinges crucially on their ability to reason and navigate about their environment.  The theory and methodologies developed in this research will make it possible to run highly sophisticated algorithms inside the ""brain"" of these autonomous systems to enable optimal decision-making, thus increasing their reliability, predictability, performance and fail-safe operation. Self-driving vehicles, anthropomorphic robots, aerial drones, manufacturing automation systems, and precision surgical instruments among others, will all benefit from the results of this research.<br/><br/><br/>The proposed research tackles a fundamental problem in the area of motion planning and trajectory generation for robotic and intelligent autonomous systems. A serious bottleneck in solving such problems under limited resource constraints (e.g., computer memory, time) is their high dimensionality that precludes the nave use of discretizing the (continuous) state space. In this research it is proposed to develop new incremental, optimal sampling-based motion planning algorithms with improved convergence rates over existing methods, so as to enable close-to-real-time trajectory generation for autonomous vehicles operating in an uncertain and dynamically changing environment. To achieve this objective, this research will build on recent results and ideas from Rapidly-exploring Random Graphs (RRG), along with relaxation methods borrowed from the areas of Asynchronous Dynamic Programming (ADP) and Machine Learning (ML). Specifically, recent advances from machine learning can be used to address the three main issues hindering the broader applicability of probabilistic sampling based motion planners to a wider variety of problems: collision checking, efficient sampling, and local steering. One main tenet of the proposed research is the exploitation of the inherent parallelism of the proposed algorithms, which -- coupled with the recent advances in multi-core computer architectures and GPUs -- will enable real-time computations."
"1617915","TWC: Small: Collaborative: Practical Hardware-Assisted Always-On Malware Detection","CNS","Special Projects - CNS, Secure &Trustworthy Cyberspace","09/01/2016","05/12/2017","Dmitry Ponomarev","NY","SUNY at Binghamton","Standard Grant","Sandip Kundu","08/31/2020","$283,000.00","Lei Yu","dponomar@binghamton.edu","4400 VESTAL PKWY E","BINGHAMTON","NY","139026000","6077776136","CSE","1714, 8060","025Z, 7434, 7923, 9178, 9251","$0.00","The project explores building support for malware detection in hardware.   Malware detection is challenging and resource intensive, as the number and sophistication of malware increases.  The resource requirements for malware detection limit its use in practice, leaving malware unchecked on many systems.  We use a low level hardware detector to identify malware as a computational anomaly using low level features such as hardware events, instruction mixes and memory address patterns.  Once malware is suspected, we inform a higher level software detection or protection mechanism that can focus its resources only on suspected malware.  The detector uses low complexity machine learning approaches to classify malware from normal programs using implementations that are feasible in hardware.  <br/><br/>The project explores countermeasures based on adversarial machine learning to limit attackers trying to evade detection, develops secure integration between the hardware and software detection, and evaluates implementation tradeoffs. The project contributes a new approach to improve the effectiveness of malware detection and to allow systems to be protected continuously without requiring the large resource investment needed by software monitors.  The project holds the promise of significantly impacting an area of critical national need to help secure systems against the expanding threats of malware.  The principles pursued in the proposal can generalize to different computational environments including mobile phones, clouds, and cyberphysical systems."
"1559894","REU Site: REU Research Experiences and Mentoring in Data-Driven Discovery","OAC","RSCH EXPER FOR UNDERGRAD SITES","04/01/2016","02/29/2016","David Kaeli","MA","Northeastern University","Standard Grant","Alan Sussman","03/31/2020","$359,587.00","","kaeli@ece.neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","1139","9102, 9250","$0.00","Non-Technical:<br/>This REU site will help support NSF's mission to promote the progress of science by providing multi-disciplinary research experience for undergraduates with exciting 10-week summer-based experiences in computer science/engineering laboratories, enabling work on both fundamental and applied data-driven problems, focused on applying machine learning techniques, data analytics, and parallel computing technologies, and preparing them for future scientific career.  The REU students will have the opportunity to utilize state-of-the-art high performance computing resources to tackle big data problems present in a number of problem domains, including Bioengineering, Environmental Engineering and Biomedical Engineering. PI's will leverage the Massachusetts Green High Performance Computing Center located in Holyoke, MA to provide a backdrop for this multi-disciplinary REU Site. The program will provide students with extensive cyber-infrastructure training, equipping program participants with the skillsets to explore the boundaries between math, science, engineering and parallel computing that are truly multi-disciplinary. The REU Site is located at Northeastern University in the heart of Boston.  <br/><br/>Technical:<br/>The REU projects span a wide range of technical fields in Computer Science and Engineering.  Example projects include:  applying machine learning to cluster Chronic Obstructive Pulmonary Disease (COPD) data, data mining Twitter feeds to identify trends in climate change, utilizing hardware accelerators to detect cancerous tumors in 4-D Computed Tomography images, and utilizing regression analysis on gene expressions to understand patterns in retina regeneration.  One novelty of this site is its focus on recruiting efforts, tailoring their research experiences for students that have only completed their freshmen year of their undergraduate education, engaging them early in research on their academic pathway. The REU site works to create a mentoring eco-system at both Northeastern University and the partnering institutions, developing a model that is transferrable to other institutions.  The program builds relationships with six urban community colleges/universities to build a pipeline of students, creating a diverse pool of REU candidates, showing students potential pathways to graduate education and research. The program leverages the CISE REU Toolkit for pre- and post-program surveys, as well as for recruiting. The REU-D3 Site is focused on effecting changes in mentoring attitudes and practices at the host and partnering academic institutions."
"1541105","CRISP Type 2: Collaborative Research: Towards Resilient Smart Cities","OAC","CIS-Civil Infrastructure Syst, Information Technology Researc, CYBERINFRASTRUCTURE","01/01/2016","08/24/2015","Walid Saad","VA","Virginia Polytechnic Institute and State University","Standard Grant","William Miller","12/31/2020","$1,100,000.00","Danfeng Yao, Sheryl Ball, Myra Blanco","walids@vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","CSE","1631, 1640, 7231","008Z, 029E, 036E, 039E","$0.00","Realizing the vision of truly smart cities is one of the most pressing technical challenges of the coming decade. The success of this vision requires synergistic integration of cyber-physical critical infrastructures (CIs) such as smart transportation, wireless systems, water networks, and power grids into a unified smart city. Such smart city CIs have significant resource dependence as they share energy, computation, wireless spectrum, users and personnel, and economic investments, and as such are prone to correlated failures due to day-to-day operations, natural disasters, or malicious attacks. Protecting tomorrow's smart cities from such failures requires instilling resiliency into the processes that manage the city's common CI resources. Such processes must be able to adaptively and optimally reallocate smart city resources to recover from failure. The goal of this Critical Resilient Interdependent Infrastructure Systems and Processes (CRISP) collaborative research project is to address this fundamental challenge via a coordinated and interdisciplinary approach that relies on machine learning, operations research, behavioral economics, and cognitive psychology to lay the mathematical foundations of resilient smart cities. The anticipated results will break new ground in the understanding of synergies between multiple cyber-physical infrastructure and resilient resource management thus catalyzing the global deployment of smart cities. This research will yield advances to the areas of resilient systems, cyber-physical systems, security and privacy engineering, game theory, computer and network science, behavioral economics, data analytics, and psychology. The project will involve students from diverse backgrounds across engineering, computer science, economics, and psychology that will be trained on pertinent research issues related to smart cities and resiliency. The project will also contribute to fostering trust between residents and the various technological processes that are fundamental to the operation of a smart city.<br/><br/>This research will introduce a foundational, transformational, analytical framework for leveraging synergies between a city's CIs to yield resilient resource management schemes cognizant of both technological and human factors. By bringing together researchers from interdisciplinary fields, this framework yields several advances: 1) Rigorous mathematical tools for delineating the inter-dependencies between CIs via a complementary mix of novel tools from graph theory, power indices, machine learning, and random spatial models; 2) Resilient resource management mechanisms that advance notions from frameworks such as behavioral game theory to enable optimized management of shared CI resources in face of failures stemming from agents of varying intelligence levels; 3) Behavioral models for characterizing the trust relationships between the residents of a smart city and the CIs; 4) Behavioral studies that provides guidelines on how to influence the users of the CIs in such a way so as to improve the resiliency of the CIs; and 5) Large-scale smart city simulators coupled with realistic experiments that will bridge the gap between theory and practice. The insights from this project will apply to the future scientific cyber-infrastructures that are likely to be interconnected as well as interdependent. The simulator will be a software artifact that would be a useful component of a scientific cyberinfrastructure aimed at understanding (for example) smart cities."
"1643351","CNS: CSR: Small: Exploiting 3D Memory for Energy-Efficient Memory-Driven Computing","CNS","CSR-Computer Systems Research","10/01/2016","12/21/2017","Viktor Prasanna","CA","University of Southern California","Standard Grant","Marilyn McClure","09/30/2020","$497,764.00","Charalampos Chelmis","prasanna@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7354","7923","$0.00","Semiconductor technology is facing fundamental physical limits creating an increased demand for acceleration of data-intensive applications on architectures that bring memory much closer to reconfigurable compute logic. Three dimensional integrated circuits (3DIC) appear to be the most prominent technology towards memory-driven computing by enabling large amounts of memory stacked in layers to be accessed by a logic unit using high bandwidth vertical interconnects. Software-defined technologies can provide the framework for harnessing the potential breakthrough performance of 3D and other advanced memory technologies in a holistic but dynamic manner, while at the same time hiding their internal complexity. This project focuses on developing a novel software paradigm to perform algorithmic exploration of memory-driven computing on new memory architectures and facilitate the development of massively parallel algorithms for memory-unconstrained computing with the potential for breakthrough performance levels.  <br/><br/>The project will develop Software-Defined 3D Memory (SD3DM) as a transformative layer for memory-driven computing that will not simply virtualize 3D memory but will holistically address the oncoming reality of massive on-chip 3D Memory for accelerating data-intensive applications while jointly optimizing energy consumption. Memory access optimizations will be developed at the algorithm level to meet application performance objectives of throughput, latency, and energy efficiency. Specifically, the optimizations will be designed to fully exploit the characteristics of target architectures by (i) carefully defining application-specific dynamic data layouts, (ii) developing application-specific memory controllers for runtime support, and (iii) designing novel in-memory data permutation mechanisms to accelerate inter-stage communication. Integer Linear Programming (ILP) and Stochastic Programming (SP) based dynamic data layouts that exploit the interlayer pipelining and parallel vault access features of 3D memory for throughput and energy-optimal mapping of data to different memory components will be developed. Data layout algorithms will be developed in in conjunction with application-specific memory controllers to provide maximum pipeline execution efficiency for any given application. <br/><br/>The proposed optimizations will be demonstrated on widely used signal processing and machine learning algorithms with diverse data access and logic use requirements. Successful completion of this project will directly lead to a significant increase in the size of signal processing and machine learning problems that can be solved on emerging 3DIC platforms at speeds that were not possible before. The developed work will potentially influence multiple application domains. The investigators will encourage the participation by women, minorities, and under-represented groups in the project through USC's Minority Opportunities in Research (MORE) Programs."
"1639753","Earthcube Building Blocks: Collaborative Proposal: Polar Data Insights and Search Analytics for the Deep and Scientific Web","ICER","Polar Cyberinfrastructure, EarthCube","09/01/2016","09/16/2016","Chris Mattmann","CA","University of Southern California","Standard Grant","Eva Zanzerkia","08/31/2019","$514,999.00","","mattmann@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","GEO","5407, 8074","7433, 8048","$0.00","This project develops an NSF EarthCube Building Block focused on Polar Data Science. The system will build upon work in Information Retrieval and Data Science and upon existing investment from NSF Polar, EarthCube, and from DARPA and NASA in this area. The system will collect, analyze, and make interactive the wealth of textual and scientific Polar data collected to date across the Deep web of scientific information -- scientific journals, multimedia information, scientific data, web pages, etc. The system builds upon fundamental research in text analysis, search, and visualization. Its primary goal is to unlock unstructured scientific data from 90+ data formats and to scale to 10s-100s of millions of records using the NSF XSEDE supercomputing resources. The system will perform information retrieval and machine learning on data crawled from the Polar Deep and Scientific web. Crawling will be informed by science questions crowdsourced through the EarthCube and Polar communities. The project is a collaboration with NSIDC, Ronin Institute, and the broader community including the newly funded Arctic Data Center led by NCEAS, to build our proposed system.<br/><br/>The result of periodic and regular crawling will be a Crawl Data Repository (CDR) of raw textual data e.g., web pages containing richly curated dataset abstract descriptions, news stories tied to datasets, ASCII note files and dataset descriptions, and other textual data available on or pointed to by Polar repositories as well as scientific data (HDF, Grib, NetCDF, Matlab, etc.). The CDR will be made available for historical and future analysis by the broader EarthCube and Polar communities. In addition, an extraction pipeline will generate an Extraction Data Repository (EDR) of machine learning features not previously present (geospatial, temporal, people, places, scientific publications and topics, etc.) that will be the basis of interactive, visual analytics over the Polar data resources. Information collected will assist in answering scientific questions such as these derived from the President?s National Strategy for the Arctic Region. To date, the team has also crowd sourced 30+ questions from the Polar community represented on CRYOLIST https://goo.gl/4dDyIS and will continue to solicit this feedback and use the information collected to aid science as prioritized by the community. They will also engage the community to assist in validating our system. This is not a predictive tool per-se ? though it can help to enable such predictions. Its focus is on building an operational and core capability for textual scientific data analysis, both retrospective, and prospective."
"1639652","Earthcube Building Blocks: Collaborative Proposal: Polar Data Insights and Search Analytics for the Deep and Scientific Web","ICER","Polar Cyberinfrastructure, EarthCube","09/01/2016","09/16/2016","Ruth Duerr","NJ","Ronin Institute for Independent Scholarship Incorporated","Standard Grant","Eva Zanzerkia","08/31/2019","$129,571.00","","ruth.duerr@ronininstitute.org","127 Haddon Place","Montclair","NJ","070432314","9737072485","GEO","5407, 8074","7433","$0.00","This project develops an NSF EarthCube Building Block focused on Polar Data Science. The system will build upon work in Information Retrieval and Data Science and upon existing investment from NSF Polar, EarthCube, and from DARPA and NASA in this area. The system will collect, analyze, and make interactive the wealth of textual and scientific Polar data collected to date across the Deep web of scientific information -- scientific journals, multimedia information, scientific data, web pages, etc. The system builds upon fundamental research in text analysis, search, and visualization. Its primary goal is to unlock unstructured scientific data from 90+ data formats and to scale to 10s-100s of millions of records using the NSF XSEDE supercomputing resources. The system will perform information retrieval and machine learning on data crawled from the Polar Deep and Scientific web. Crawling will be informed by science questions crowdsourced through the EarthCube and Polar communities. The project is a collaboration with NSIDC, Ronin Institute, and the broader community including the newly funded Arctic Data Center led by NCEAS, to build our proposed system.<br/><br/>The result of periodic and regular crawling will be a Crawl Data Repository (CDR) of raw textual data e.g., web pages containing richly curated dataset abstract descriptions, news stories tied to datasets, ASCII note files and dataset descriptions, and other textual data available on or pointed to by Polar repositories as well as scientific data (HDF, Grib, NetCDF, Matlab, etc.). The CDR will be made available for historical and future analysis by the broader EarthCube and Polar communities. In addition, an extraction pipeline will generate an Extraction Data Repository (EDR) of machine learning features not previously present (geospatial, temporal, people, places, scientific publications and topics, etc.) that will be the basis of interactive, visual analytics over the Polar data resources. Information collected will assist in answering scientific questions such as these derived from the President?s National Strategy for the Arctic Region. To date, the team has also crowd sourced 30+ questions from the Polar community represented on CRYOLIST https://goo.gl/4dDyIS and will continue to solicit this feedback and use the information collected to aid science as prioritized by the community. They will also engage the community to assist in validating our system. This is not a predictive tool per-se ? though it can help to enable such predictions. Its focus is on building an operational and core capability for textual scientific data analysis, both retrospective, and prospective."
"1626825","Computational Techniques for Studying Everyday Multiattribute Choice","SES","Decision, Risk & Mgmt Sci","09/01/2016","07/18/2016","Sudeep Bhatia","PA","University of Pennsylvania","Standard Grant","Jeryl Mumpower","08/31/2019","$394,474.00","LYLE UNGAR","bhatiasu@sas.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","SBE","1321","9179","$0.00","Everyday choice objects, such as food items, movies, clothes, and consumer goods, can be seen as possessing different attributes or features. Although choices between everyday objects involve attending to and evaluating these attributes, the attributes themselves may be complex and not easily observed by researchers. This project attempts to develop computational techniques to uncover the attributes involved in everyday choice by combining insights from machine learning and statistics with existing theories in decision making research. It offers the possibility harnessing developments in machine learning and data science to advance our theoretical understanding of everyday decision making and, in the process, yield broader impacts by informing and improving implementation of and policy toward important decisions like those concerned with health care or retirement planning.<br/><br/>There are two major components to this project. The first is computational, and involves the use of statistical techniques to recover (otherwise unobservable) attribute representations for real-world choice objects from large-scale user-generated internet data. The second major component is empirical, and involves the use of these recovered attributes, combined with existing multi-attribute decision rules, to study multi-attribute choices between various real-world objects. Overall, the project applies the proposed approach to three domains: movie choice, book choice, and food choice, and for each of these domains, attempts to predict choice probabilities, decision times, and judgments of attribute importance in naturalistic decision problems involving movies, books, and food items, given to participants in the laboratory. In a similar manner, this project uses these domains to test whether behavioral effects such as choice set dependence and reference dependence, established using the types of stylized experiments popular in multi-attribute research, also hold when the objects under consideration are naturalistic and are not described using explicit attribute-by-object matrices. Finally this project uses these domains to study decisions in which the choice sets themselves are stored in memory, and are not explicitly presented to decision makers."
"1612961","New High Dimensional Phenomena and Applications","DMS","PROBABILITY, ANALYSIS PROGRAM","06/01/2016","05/21/2018","Sergey Bobkov","MN","University of Minnesota-Twin Cities","Continuing grant","Tomek Bartoszynski","05/31/2019","$199,994.00","","bobkov@math.umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","MPS","1263, 1281","","$0.00","The project's research lies at the crossroads of several themes in mathematics and is focused on the study of probabilistic, geometric, and information-theoretic aspects of concentration of measure and other high-dimensional phenomena. In complex systems where rules of randomness are well understood and sufficiently many underlying events are independent of each other, aggregate behavior at large scales tends to deviate very little from the median behavior. This phenomenon, known as concentration of measure, has been the subject of exciting developments, since concentration tools allow one to analyze many essential properties of rather general systems. Being strongly motivated by challenging purely mathematical questions, this research area also has a wide range of applications in disciplines such as information theory, statistics, computer science, and machine learning, among others. This research project is aimed in particular at a correct understanding of the role of the growing dimension, especially in the problems where high dimension serves as a unifying force. High-dimensional models are useful in practice for instance to understand the entire evolution of a phenomenon in time, not just the governing local rules, which may lead one's low-dimensional intuition astray. The project will have an important impact correcting such misconceptions in mathematics, and will have further impact when these ideas are applied to areas such as statistics and machine learning. The project will also have broader impact on educating undergraduate and graduate students in the mathematical sciences. <br/><br/>The project's themes refer either to long-standing open problems or to challenging questions related to recent developments. More specifically, the investigator plans to develop new concentration tools starting from the spherical concentration phenomenon and its extensions to Grassman and Stiefel manifolds. With new tools, one of the targets of investigation will be the circle of problems related to the K-L-S conjecture of Kannan, Lovasz, and Simonovits. The project will explore refined concentration properties of high dimensional projections of log-concave and more general convex measures; in particular, the work will investigate new integral geometric characteristics of convex measures on Euclidean spaces that are responsible for spectral gap and Cheeger isoperimetric constants. Part of the project is devoted to asymptotic expansions in the central limit theorem for the relative entropy and Fisher information, including Berry-Esseen bounds, and their applications to optimal transport for sums of independent random vectors. The project also deals with information-theoretic inequalities and transport problems about empirical distributions."
"1619322","TWC: Small: Collaborative: Practical Hardware-Assisted Always-On Malware Detection","CNS","Secure &Trustworthy Cyberspace","09/01/2016","07/25/2016","Nael Abu-Ghazaleh","CA","University of California-Riverside","Standard Grant","Sandip Kundu","08/31/2020","$225,000.00","","nael@cs.ucr.edu","Research & Economic Development","RIVERSIDE","CA","925210217","9518275535","CSE","8060","7434, 7923","$0.00","The project explores building support for malware detection in hardware.   Malware detection is challenging and resource intensive, as the number and sophistication of malware increases.  The resource requirements for malware detection limit its use in practice, leaving malware unchecked on many systems.  We use a low level hardware detector to identify malware as a computational anomaly using low level features such as hardware events, instruction mixes and memory address patterns.  Once malware is suspected, we inform a higher level software detection or protection mechanism that can focus its resources only on suspected malware.  The detector uses low complexity machine learning approaches to classify malware from normal programs using implementations that are feasible in hardware.  <br/><br/>The project explores countermeasures based on adversarial machine learning to limit attackers trying to evade detection, develops secure integration between the hardware and software detection, and evaluates implementation tradeoffs. The project contributes a new approach to improve the effectiveness of malware detection and to allow systems to be protected continuously without requiring the large resource investment needed by software monitors.  The project holds the promise of significantly impacting an area of critical national need to help secure systems against the expanding threats of malware.  The principles pursued in the proposal can generalize to different computational environments including mobile phones, clouds, and cyberphysical systems."
"1541108","CRISP Type 2: Collaborative Research: Towards Resilient Smart Cities","OAC","CIS-Civil Infrastructure Syst, Information Technology Researc, CYBERINFRASTRUCTURE, CSR-Computer Systems Research, Software Institutes","01/01/2016","12/14/2017","Arif Sarwat","FL","Florida International University","Standard Grant","William Miller","12/31/2020","$511,999.00","Ismail Guvenc","asarwat@fiu.edu","11200 SW 8TH ST","Miami","FL","331990001","3053482494","CSE","1631, 1640, 7231, 7354, 8004","008Z, 029E, 036E, 039E, 9251","$0.00","Realizing the vision of truly smart cities is one of the most pressing technical challenges of the coming decade. The success of this vision requires synergistic integration of cyber-physical critical infrastructures (CIs) such as smart transportation, wireless systems, water networks, and power grids into a unified smart city. Such smart city CIs have significant resource dependence as they share energy, computation, wireless spectrum, users and personnel, and economic investments, and as such are prone to correlated failures due to day-to-day operations, natural disasters, or malicious attacks. Protecting tomorrow's smart cities from such failures requires instilling resiliency into the processes that manage the city's common CI resources. Such processes must be able to adaptively and optimally reallocate smart city resources to recover from failure. The goal of this Critical Resilient Interdependent Infrastructure Systems and Processes (CRISP) collaborative research project is to address this fundamental challenge via a coordinated and interdisciplinary approach that relies on machine learning, operations research, behavioral economics, and cognitive psychology to lay the mathematical foundations of resilient smart cities. The anticipated results will break new ground in the understanding of synergies between multiple cyber-physical infrastructure and resilient resource management thus catalyzing the global deployment of smart cities. This research will yield advances to the areas of resilient systems, cyber-physical systems, security and privacy engineering, game theory, computer and network science, behavioral economics, data analytics, and psychology. The project will involve students from diverse backgrounds across engineering, computer science, economics, and psychology that will be trained on pertinent research issues related to smart cities and resiliency. The project will also contribute to fostering trust between residents and the various technological processes that are fundamental to the operation of a smart city.<br/><br/>This research will introduce a foundational, transformational, analytical framework for leveraging synergies between a city's CIs to yield resilient resource management schemes cognizant of both technological and human factors. By bringing together researchers from interdisciplinary fields, this framework yields several advances: 1) Rigorous mathematical tools for delineating the inter-dependencies between CIs via a complementary mix of novel tools from graph theory, power indices, machine learning, and random spatial models; 2) Resilient resource management mechanisms that advance notions from frameworks such as behavioral game theory to enable optimized management of shared CI resources in face of failures stemming from agents of varying intelligence levels; 3) Behavioral models for characterizing the trust relationships between the residents of a smart city and the CIs; 4) Behavioral studies that provides guidelines on how to influence the users of the CIs in such a way so as to improve the resiliency of the CIs; and 5) Large-scale smart city simulators coupled with realistic experiments that will bridge the gap between theory and practice. The insights from this project will apply to the future scientific cyber-infrastructures that are likely to be interconnected as well as interdependent. The simulator will be a software artifact that would be a useful component of a scientific cyberinfrastructure aimed at understanding (for example) smart cities."
"1640587","UHDNetCity: User-centered Heterogeneous Data Fusion for Multi-networked City Mobility","CNS","Special Projects - CNS","09/01/2016","08/01/2018","Reza Arghandeh","FL","Florida State University","Standard Grant","David Corman","02/28/2019","$233,123.00","Laura Arpan, Jinghui Hou, Eren Ozguven","reza@caps.fsu.edu","874 Traditions Way, 3rd Floor","TALLAHASSEE","FL","323064166","8506445260","CSE","1714","042Z, 7916","$0.00","As more of the world's cities suffer from congestion, pollution, and energy exploitation, urban mobility remains one of the toughest challenges that cities face as the process of population growth and urbanization continues. So far, the most common approach for urban mobility characterization focuses on vehicle's spatial and temporal positions. However, urban mobility is a multidimensional characteristic of the city life, experienced as tangled layers of interconnected infrastructures and information networks around people and their needs in a spatio-emporal frame. As a result, the study of mobility should go beyond transportation systems, be customer-centered and merged into other physical systems and cyber networks. This Early-concept Grant for Exploratory Research (EAGER) project is motivated by the need to increase the situational awareness in urban mobility and distribute reliable and timely information to city managers and city residents about issues associated with urban mobility. Through successful collaboration, this project aims to develop a new definition of urban mobility with measurable indices to characterize the urban mobility paradigm around citizens integrating transportation networks, electricity networks, and crowdsourced data. This EAGER project is expected to contribute to the team's established and ongoing effort in the Global City Teams Challenge (GCTC) in collaboration with the City of Tallahassee, Florida. The research team has completed the first phase of the GCTC, and this EAGER project will lay the foundation for the second phase by developing a data-driven approach to characterize urban mobility, which integrates collected data from the transportation network, electricity network, weather, air quality and social media within the City of Tallahassee. This approach will put the City of Tallahassee one step closer in their efforts towards being a ""smart city"" by improving the city services through measurable mobility benefits, and enhance the quality of life for residents. This approach will be supported by the active GCTC action cluster including Internet2, EDD Inc., and StanTec companies to support the Tallahassee GCTC efforts.<br/><br/> <br/>The UHDNetCity will be able to bring measurable mobility benefits and improve Tallahassee resident's quality of life in terms of (1) lowering energy consumption by vehicles and infrastructure, (2) reducing congestion, crashes and traveler frustration, (3) improving safety and reliability, and (4) providing a more streamlined, efficient and cost-effective system to operate and maintain city service networks. The UHDNetCity framework combines data fusion, signal processing, and machine learning, to provide a unified mathematical foundation for real-time urban mobility sensing by processing heterogeneous spatio-temporal measurement data and network models. This mathematical framework will lead to bridging the gap between supervised, and semi-supervised machine learning algorithms for urban mobility characterization using hidden data structures in the heterogeneous urban data sources. The UHDNetCity employs a user-driven play-centric design approach to encourage resident's adoption of the urban crowdsourcing dashboards such as DigiTally mobile app developed by the City of Tallahassee and promotes their engagement in the urban mobility management."
"1637092","Next Generation Connected and Smart Cyber Fire Fighter System","CNS","S&CC: Smart & Connected Commun","07/01/2016","02/11/2019","Manel Martinez-Ramon","NM","University of New Mexico","Standard Grant","David Corman","06/30/2019","$199,920.00","Ramiro Jordan, Yin Yang","manel@unm.edu","1700 Lomas Blvd. NE, Suite 2200","Albuquerque","NM","871310001","5052774186","CSE","033Y","7916, 7918, 8083, 9150","$0.00","The goal of this project is to demonstrate that advanced information and sensor technology can improve operational efficiency and increase the security and safety of fire fighters.   Existing firefighting systems will be augmented by exploiting the information capabilities of hardware and software components that can be attached to the existing fire fighter equipment, with minimal physical burden and required training.  The system will provide a model of the emergency scenario that will allow the commander to evaluate possible alternative actions based on their experience and available resources. This situation awareness will be created from the data provided by the fighter gear (microphones, cameras, body and ambient sensors), and will include the estimation of the fighter situation (including fighters' incidents, oxygen reserve or estimated time left to leave the scenario) and the scenario itself (including the presence of victims, evaluation hazardous object or environments, as hot surfaces, toxic gas and others).   This proposal is highly relevant for smart and connected communities.  It addresses problem space of great relevance in emergency operations with a technology solution that faces significant research and operational challenges.  The project engages technical communities, non-profit partners and local government institutions.  It is a cooperation between various departments of the University of New Mexico, in collaboration with the City of Santa Fe and the City of Albuquerque Fire departments and the National Fire Protection Association. <br/><br/>The project integrates a hardware layout that collects the data from each fire fighter on duty with a software engine for extracting data and processing.  Data from infrared cameras, body and ambient sensors' will be interfaced to a communication node to transmit extracted and compressed information using a mesh structure for communications based on software defined radio supporting heterogeneous communication assets, instant deployment and hot reconfiguration, resiliency and recovery abilities.  The software engine will integrate machine learning based feature extraction and prediction methods that will process the ambient data, audio and speech, to sense the fighter's condition, detect relevant keywords whose meaning can be transmitted, or give orders to the system.  Video will be locally processed to extract relevant features (civilians, heat surfaces, hazardous objects and others). Machine learning algorithms will then be used to construct the situational awareness that will be served to the commander and fire fighters, including scenario and fire fighters' situation.  The system will be tested in a variety of operational scenarios to evaluate the potential for transition and application in other emergency domains."
"1559172","Collaborative Research: Inference Methods for Machine Learning and High-Dimensional Data in Policy Evaluation and Structural Economic Models","SES","Economics, Methodology, Measuremt & Stats","05/15/2016","05/03/2016","Victor Chernozhukov","MA","Massachusetts Institute of Technology","Standard Grant","Nancy Lutz","04/30/2019","$246,218.00","","vchern@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","SBE","1320, 1333","1320, 1333","$0.00","Much of empirical economics focuses on estimating and drawing credible inferences about the causal effects of economic policies or about parameters of underlying economic models.  The type of data that researchers use for this task becomes increasingly rich and complex.  While these increased data resources open up many new opportunities, they also pose additional challenges, and nave application of such techniques may render conclusions drawn about economic effects invalid.  This research project will establish a general, formal framework to provide guidance about construction of estimation and inference devices coupled with appropriate use of tools from ""big data"" or data-mining that will deliver reliable conclusions about economic objects of interest.  The proposed research will present the methods and corresponding theoretic guarantees to cover a variety of situations encountered in empirical research in economics and the social sciences, offer empirical applications, and provide usable software in statistical packages popular within the social sciences.  The theoretical and empirical work will thus help bridge the gap between social science practice and ""big data"", and will provide methods that will enhance the credibility of the drawn scientific conclusions.  <br/><br/>The proposed research will provide bridges between high-dimensional statistical modeling and applied social science research.  Integrating high-dimensional methods with economically relevant modeling frameworks and targets is important in providing researchers tools which can be used to analyze modern, complex data and provide reliable inferential statements about the objects of interest.  The proposed research will advance the theory of inference following regularization which is a key element to inference in modern, large data sets.  The main goal of this research project is to generalize available results about inference for a low-dimensional target parameter of interest by providing an encompassing framework that will include interesting nonlinear models and estimation procedures such as maximum likelihood and generalized method of moments.  The investigators will also provide an extension to cover cases where the target of interest is function valued, such as when interest is in a set quantile treatment effects across a range of quantile indices.  This advancement will expand the frontier for applications of high-dimensional methods in applications where inference about sets of model parameters is the goal.  This expansion is useful even in low-dimensional models and is likely to become crucial as large, complicated data sets become more readily available.  In addition to providing theoretical results, the research aims to provide illustrative empirical examples and software in both R and Stata for application of these methods."
"1560276","REU Site: Proactive Health Informatics","IIS","RSCH EXPER FOR UNDERGRAD SITES, Smart and Connected Health","02/15/2016","02/16/2016","Katie Siek","IN","Indiana University","Standard Grant","Wendy Nilsen","01/31/2020","$359,990.00","","ksiek@indiana.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","CSE","1139, 8018","8018, 9250","$0.00","REU Site: Proactive Health Informatics<br/>The Proactive Health (ProHealth) Informatics Research Experience for Undergraduates site at Indiana University Bloomington will provide 10 undergraduate students with the opportunity to conduct research for 10 weeks with world-renown faculty members in the fields of human computer interaction, pervasive computing, machine learning, privacy, and technical policy. At least 5 of the undergraduate students will be from underrepresented groups, including first generation college students, and students who do not have access to research opportunities. The undergraduate students will have the opportunity to: (1) learn how to design and build new mobile or wearable health systems; (2) use the data collected from these systems to create personalized recommendations; (3) investigate possible privacy issues from collecting this data and propose solutions to protect one's privacy; or (4) understand the law implications for collecting, using, and storing this data. These projects can potentially improve the health of the broader United States population. Undergraduate researchers will learn how to: conduct ethical research; document their progress; represent themselves professionally online; prepare for graduate school; network in professional settings; appropriately disseminate one's findings through writing and presentation; and prepare oneself for future career opportunities.<br/>  <br/>The objective of the Proactive Health (ProHealth) Informatics REU Site is to train undergraduates from diverse backgrounds to become the next generation of researchers who design, develop, and evaluate intelligent, pervasive systems that empower lay people to proactively manage their health. Innovations include the design of new mobile or wearable health systems that integrate seamlessly into a person's life while providing appropriately shared, and valuable feedback. To this end, the interdisciplinary ProHealth team will  (1) train undergraduates to conduct cross-cutting computing research; (2) introduce students to graduate education and research career opportunities through preparatory workshops and one-on-one mentoring; (3) increase faculty and graduate student awareness of undergraduate research mentoring through Affinity Research Group model workshops; (4) provide opportunities for students to disseminate research results and faculty to disseminate best practices for mentoring; and (5) inspire middle and high school students to consider computing through REU panels and shadowing opportunities during computing-related summer camps."
"1560229","REU SITE: Data Science Research for Safe, Sustainable and Healthy Communities","IIS","RSCH EXPER FOR UNDERGRAD SITES","05/01/2016","03/09/2016","Elke Rundensteiner","MA","Worcester Polytechnic Institute","Standard Grant","Wendy Nilsen","04/30/2020","$367,500.00","Fatemeh Emdad","rundenst@wpi.edu","100 INSTITUTE RD","WORCESTER","MA","016092247","5088315000","CSE","1139","9250","$0.00","This Research Undergraduate Experience (REU) program provides students the opportunity to perform interdisciplinary research in WPI's Department of Computer Science and in particular in the Data Science program focusing on societal challenges in cyber security, healthcare, and sustainability - all interlinked concerns of critical national importance. Cities in the 21st century, including the City of Worcester and its surrounding communities, are plagued by many urban challenges from pollution to traffic congestion.  This project will drive the vision of empowering citizens and organizations by supporting data-driven policy making. We will provide a unique educational research experience for undergraduate students from under-represented groups by exposing them to projects through which they tackle critical societal challenges via data science. By giving students the opportunity to apply their education toward improving the quality of life for individuals in their community, we will show them the power of data science in the real world. This will pique their interest in pursuing further education in this important STEM discipline, thus contributing to the STEM workforce and the US economic prosperity.<br/> <br/><br/>There are limited undergraduate research opportunities in Data Science despite its recognition as a critical STEM skill set criteria. The goal of our REU program is to fill this gap by engaging underrepresented undergraduate students in data science research in solving the most critical problems faced by our communities: from cyber-security and safety analytics, to data-driven healthcare, and sustainability. REU participants will work in interdisciplinary research teams, mentored by WPI faculty advisors and graduate students, and have ownership of a project for the duration of their summer.  The students will learn about and apply data science techniques from data mining, data bases, machine learning, statistical learning, to business intelligence. To supplement the educational and research skill development of students, a weekly seminar series on professional development covering topics such as research inquiry, data interpretation and visualization, scientific writing, communication skills, to career opportunities afforded by data science will be held.  At the conclusion of the program, students will present their results via an oral presentation in a conference-like format.  By empowering students to solve pressing problems in their communities via data science, students will be compelled to seek advanced degrees and promising careers in data science."
"1560191","REU Site: Big Data Analytics","IIS","RSCH EXPER FOR UNDERGRAD SITES","04/01/2016","01/22/2016","Sanmay Das","MO","Washington University","Standard Grant","Wendy Nilsen","03/31/2019","$359,111.00","Roman Garnett","sanmay@wustl.edu","CAMPUS BOX 1054","Saint Louis","MO","631304862","3147474134","CSE","1139","9150, 9250","$0.00","This new Research Experiences for Undergraduates (REU) site will train students in a computer science approach to data analytics, ranging from machine learning to data management, and develop research skills from primary research engagement and supplementary activities.  Students will: (1) be exposed to the many applications of data analytics across computer science, (2) gain expertise via one specific project, and (3) be equipped with a technical competitive advantage, knowledge, and motivation to pursue graduate study in a STEM field.  They will be deeply involved in projects that span a wide range of interests and have social appeal.  This REU site will build national economic competitiveness by training and engaging a new generation of students, with a specific focus on women and on students from institutions without much access to research in STEM areas.<br/><br/>Big data - the confluence of an ever growing supply of data generated across all fields and disciplines and increasingly powerful computational tools to process and mine these data - is widely acknowledged as one of the biggest opportunities and challenges of the 21st century.  This site will give undergraduates a chance to engage in cutting-edge research in one of many computer science fields that utilizes data analysis.  Each student will work with a single faculty mentor on a specific project.  Students will participate in an intense, hands-on research experience associated with their own summer project, but they will enjoy a cohort community of REU students all working within and learning about data analytics.  Students will learn the ways in which the intelligent use of data can solve difficult problems and improve people's quality of life.  They will complete their time at the site with the skills to further the use of data analysis applied to the field of their choosing."
"1642550","CIF: Small: Collaborative Research: Ordinal Data Compression","CCF","Comm & Information Foundations","03/01/2016","05/20/2016","Arya Mazumdar","MA","University of Massachusetts Amherst","Standard Grant","Phillip Regalia","08/31/2019","$246,331.00","","arya@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","7797","7923, 7935","$0.00","With the emergence of Big Data platforms in social and life sciences, it is becoming of paramount importance to develop efficient lossless and lossy data compression methods catering to the need of such information systems. Although many near-optimal compression methods exist for classical text, image and video data, they tend to perform poorly on data which naturally appears in fragmented or ordered form. This is especially the case for so called ordinal data, arising in crowd-voting, recommender systems, and genome rearrangement studies. There, information is represented with respect to a ?relative,? rather than ?absolute? scale, and the particular constraints of the ordering cannot be properly captured via simple dictionary constructions. This project seeks to improve the operational performance of a number of data management, cloud computing and communication systems by developing theoretical, algorithmic and software solutions for ordinal data compaction.<br/><br/>The main goal of the project is to develop the first general and comprehensive theoretical framework for ordinal compression. In particular, the investigators propose to investigate new distortion measures for ordinal data and rate-distortion functions for lossy ordinal compression; rank aggregation and learning methods for probabilistic ordinal models, used for ordinal clustering and quantization; and smooth compression and compressive computing in the ordinal domain. The proposed analytical framework will also allow for addressing algorithmic challenges arising in the context of compressing complete, partial and weak rankings. The accompanying software solutions are expected to find broad applications in areas as diverse as theoretical computer science (sorting, searching and selection), machine learning (clustering and learning to rank), and gene prioritization and phylogeny (reconstruction of lists of influential genes and ancestral genomes, respectively)."
"1622867","STTR Phase I:  An Agent-based Self-learning Technology for Efficient Building Operations and Automated Participation in Electricity Markets","IIP","STTR PHASE I","07/01/2016","06/20/2016","Manisa Pipattanasomporn","VA","BEMCONTROLS, LLC","Standard Grant","Rick Schwerdtfeger","06/30/2017","$224,673.00","Murat Kuzlu","manisa@bemcontrols.com","1224 PROVIDENCE TER","MC LEAN","VA","221012648","5713039097","ENG","1505","033E, 1505, 152E, 8033","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR)  Phase I project is the capability to enable small- and medium-sized commercial buildings to operate more efficiently and facilitate their participation in electricity markets. It will result in energy consumption reduction in buildings, thus providing savings in electricity bills and lowering carbon footprints, without compromising occupant comfort. The proposed platform also enables revenue generation by allowing buildings to participate in ancillary electricity markets and respond to pricing or demand response signals from electric utilities. By integrating intelligent energy management and automatic demand response features to the traditional building operation, this helps electric utility companies to avoid/defer extensive upgrades of their electrical infrastructures, such as generation, transmission or distribution facilities, with the growing demand for electricity. The proposed software platform demonstrates the benefits of building automation to building owners/operators, and provides them the ability to cross-reference their buildings with best practices in building operations. It also bridges the knowledge transfer process between a small business and an institution of higher education by facilitating both sides to meet, work and exchange experiences with each other.<br/><br/>This Small Business Innovation Research (SBIR) Phase I project is built on an agent-based platform for building automation systems where advanced algorithms enable the software platform to be proactive and help optimize building operations. It can also perform fault detection, diagnostics and predictive maintenance, integrate emerging Internet of Things (IoT) devices and enable buildings to interact with the grid in an intelligent manner. Under the proposed project, a reinforced machine-learning algorithm will be developed that uses historical building energy consumption data and ambient conditions - including occupant preferences - to optimize building operations based on usage patterns of major loads and power sources in each zone of the building. Another major research contribution lies in novel algorithms that enable buildings to transact with the grid. These allow buildings to respond to system stress conditions and electricity price signals by automatically adjusting the operation of major loads and internal generating sources. Overall, the proposed software platform allows a building to optimize its operation, perform peak demand management during grid stress conditions, and participate in ancillary service markets."
"1607280","Collaborative Research: Opening a Quantitative Window into the Mind and Communication of Dolphins","PHY","PHYSICS OF LIVING SYSTEMS","09/01/2016","08/27/2019","Marcelo Magnasco","NY","Rockefeller University","Continuing Grant","Krastan Blagoev","08/31/2021","$447,592.00","","magnasco@rockefeller.edu","1230 YORK AVENUE","New York","NY","100656307","2123278309","MPS","7246","8091","$0.00","The cognitive and communication prowess of the large-brained, highly gregarious bottlenose dolphin is legendary and has captured the interest and imagination of scientists and the public at large. Notably, dolphins are one of a handful of species that are vocal learners; thus the dolphin presents us with a unique and alternative model to songbirds to advance our understanding of the processes that underlie vocal learning and communication in a large-brained mammal showing high social and behavioral complexity. Despite decades of research on dolphin vocal communication, the constituent elements, the sequential and temporal organization of the elements, and the nature of vocal transactions between dolphins remain enigmatic. In order to decipher dolphin communication and further elucidate these issues, the PIs will use observational approaches in the wild and in an aquarium as well as two experimental approaches in an aquarium. A first approach studies how dolphins coordinate their behavior in the face of novelty: How do dolphins synchronize their behavior when requested to present novel behaviors in tandem? How do they succeed in working as a team when confronted with a new challenge? In a second approach, the researchers will provide dolphins with a two-way, interactive touchscreen and audio system. Their proclivity for vocal learning and their other cognitive processes will be probed by giving them choice and control over the interactive system and observing and documenting how they use and respond to the contingencies of their interactions, which shall be obtaining novel acoustic signals temporally paired with objects, interactions or activities. The aquarium studies are particularly conducive to outreach efforts and informal science education since the dolphin-related activities will be conducted in an open amphitheater environment at a leading aquarium. The exhibit has changed from dolphin shows to more educational demonstrations about animal care, behavior, and conservation. The research activities will take place concurrently with visits of the public and field trips of school children and therefore, the research endeavor will be communicated to the public in real time. Virtually all the activities proposed can, with modest effort and cost, be made into an open exhibit at the National Aquarium in Baltimore, instructing visiting schoolchildren on the research process and on collaborative research across scientific boundaries. The field trips to the research sites are organized jointly with undergraduate and graduate courses in field research for Hunter College students and are excellent opportunities to showcase and engage actual cutting edge research to these students. This tightly-knit interdisciplinary collaboration between the labs of animal cognitive psychologist and marine mammal scientist and physicist, computational neuroscientist and bio-acoustician, seeks to and extend high-throughput settings to dolphin communication studies in captivity.<br/><br/>A key intellectual merit of this proposal is to address these fundamental questions in the field of animal cognition and communication through powerful methods drawn from statistical mechanics, machine learning, and signal processing. Central technical problems in ""decoding"" dolphin communication are the difficulty in sound localization and attributing the detected vocalizations to specific individual dolphins, the lack of automated behavioral analysis methods, and the crucial step of creating an integrated picture of attributed vocal and behavioral transactions or ""conversations"" that can be subjected to high-through analysis. Another technical problem is the design and deployment of an advanced two-way interactive system consisting of an underwater touchpad and real-time audio input/output, plus the associated set of interaction interfaces. Finally, this project also addresses important technical problems in the deployment of aerial and aquatic drones for field studies."
"1619212","RI: Small: Collaborative Research: Developing Golden Speakers for Second-Language Pronunciation Training","IIS","Robust Intelligence","09/01/2016","08/10/2016","Ricardo Gutierrez-Osuna","TX","Texas A&M Engineering Experiment Station","Standard Grant","Tatiana Korelsky","08/31/2021","$179,999.00","","rgutier@cs.tamu.edu","400 Harvey Mitchell Pkwy S","College Station","TX","778454645","9798626777","CSE","7495","7495, 7923","$0.00","People who learn a second language (L2) as adults often speak with a persistent foreign accent.  This can make them less intelligible, more subject to discrimination, and less confident when interacting with others. Surprisingly, though, L2 learners rarely receive formal training in pronunciation, in part because effective training must be customized to meet each learner's individual needs.  To address this gap, the investigators propose to develop algorithms to synthesize a personalized ""golden speaker"" for each learner: his or her own voice but with a native accent. The rationale is that, by listening to their own golden speaker, learners can more easily perceive differences between their actual and ideal pronunciations. This work focuses on developing the technology for golden speakers, which the investigators plan to evaluate in the future as a new tool for pronunciation learning systems. As such, this research can benefit a large number of workers in the US who are non-native speakers of English, particularly in higher education, health care and the technology sector. The project also provides opportunities for graduate and undergraduate students to conduct research in a multi-disciplinary team with expertise in signal processing, machine learning, and language acquisition. <br/><br/>Two types of golden-speaker model are proposed. The first type is based on a reformulation of parametric statistical models for voice conversion, where instead of force-aligning source (native) and target (non-native) frames, they are matched based on their phonetic similarity.  Several similarity metrics are proposed, from vocal-tract-length normalization to deep auto-encoders. The second type is based on a sparse representation of speech, which models individual frames as linear combinations of phonetic anchors.  This requires new techniques to transform the constellation of anchors in the L2 speech to match the structure of native anchors (e.g., pairwise distances). Two types of evaluation are proposed for the golden-speaker models: their ability to interpolate phones not included in the learner's inventory, and the accent, intelligibility and comprehensibility of the resulting speech, as rated by native English listeners.  For this purpose, the investigators propose to collect a large speech corpus from multiple Spanish and Korean learners of English and Indian speakers of English, each at different levels of English proficiency."
"1618953","RI: Small: Collaborative Research: Developing Golden Speakers for Second-Language Pronunciation   Training.","IIS","Robust Intelligence","09/01/2016","08/10/2016","John Levis","IA","Iowa State University","Standard Grant","Tatiana Korelsky","08/31/2021","$120,000.00","","jlevis@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","7495","7495, 7923","$0.00","People who learn a second language (L2) as adults often speak with a persistent foreign accent. This can make them less intelligible, more subject to discrimination, and less confident when interacting with others. Surprisingly, though, L2 learners rarely receive formal training in pronunciation, in part because effective training must be customized to meet each learner's individual needs. To address this gap, the investigators propose to develop algorithms to synthesize a personalized ""golden speaker"" for each learner: his or her own voice but with a native accent. The rationale is that, by listening to their own golden speaker, learners can more easily perceive differences between their actual and ideal pronunciations. This work focuses on developing the technology for golden speakers, which the investigators plan to evaluate in the future as a new tool for pronunciation learning systems. As such, this research can benefit a large number of workers in the US who are non-native speakers of English, particularly in higher education, health care and the technology sector. The project also provides opportunities for graduate and undergraduate students to conduct research in a multi-disciplinary team with expertise in signal processing, machine learning, and language acquisition. <br/><br/>Two types of golden-speaker model are proposed. The first type is based on a reformulation of parametric statistical models for voice conversion, where instead of force-aligning source (native) and target (non-native) frames, they are matched based on their phonetic similarity. Several similarity metrics are proposed, from vocal-tract-length normalization to deep auto-encoders. The second type is based on a sparse representation of speech, which models individual frames as linear combinations of phonetic anchors. This requires new techniques to transform the constellation of anchors in the L2 speech to match the structure of native anchors (e.g., pairwise distances). Two types of evaluation are proposed for the golden-speaker models: their ability to interpolate phones not included in the learner's inventory, and the accent, intelligibility and comprehensibility of the resulting speech, as rated by native English listeners. For this purpose, the investigators propose to collect a large speech corpus from multiple Spanish and Korean learners of English and Indian speakers of English, each at different levels of English proficiency."
"1632257","SBIR Phase II:  Using Data Mining to Optimally Customize Therapy for Individuals with Autism","IIP","SBIR Phase II","08/01/2016","07/07/2020","John Nosek","PA","Guiding Technologies Corporation","Standard Grant","Alastair Monk","07/31/2021","$1,016,170.00","","johnnosek@verizon.net","1500 JFK Blvd Suite 1825 2 Penn","Philadelphia","PA","191021710","6096059273","ENG","5373","116E, 165E, 169E, 5373, 7744, 8018, 8032, 8042, 8089, 8091, 8240, 9231, 9251","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project will revolutionize the treatment of individuals with autism. One of every sixty-eight US children has autism (over 1.1 million). The estimated cost of providing Applied Behavior Analysis (ABA) therapy to those who could benefit is $7.5 billion dollars annually. Societal impacts include: 1) more individuals with autism across the globe will receive treatment regimens that will enable them to live more fulfilled lives and reach their full potential; 2) families whose children are good candidates for treatment and receive it will experience reduced stress and better family life; and 3) the additional lifetime cost of not effectively treating children with autism, which is approximately ten-fold the cost of treatment, will be reduced. Because high-quality, contextually rich ABA performance data will be collected for the first time, efforts to apply data analytics will contribute in two important ways: a) patterns may be discerned across individuals with autism to better understand variations in autism and create therapies to target these differences; b) expansion of the frontiers of data mining to provide guidance in real time will contribute to a number of areas within and beyond ABA therapy.<br/><br/>The proposed project will optimize therapy outcomes for individuals with autism by transforming agent-based guiding technology into an adaptive and intelligent ABA therapy assistant for supervisors and instructors. The project pushes the boundaries in providing cost-effective, adaptable, intelligent, real-time guidance and data-collection support to instructors that integrates naturally into the instructional process and is easy to learn and use. ABA therapy experts, supervisors and instructors will verify the analyses and resulting guidance incorporated into the technology.  Advanced theories of usability engineering, including some developed by the project team, will be used to build interfaces that supervisors and instructors can intuit without the need for learning new concepts and syntax. The project will utilize the collected logs from multiple sessions with multiple therapy recipients and multiple therapy providers to uncover hidden patterns and assist supervisors in selecting appropriate therapy steps personalized for the individual with autism. The project will build on a large body of recent work in visualization, machine learning on temporal predictive modeling and sequential pattern mining, including some of the previous results of the project team. Special attention will be paid to the recent work in educational data mining and intelligent tutoring."
"1633124","BIGDATA: IA: Democratizing Massive Fluid Flow Simulations via Open Numerical Laboratories and Applications to Turbulent Flow and Geophysical Modeling","OCE","PHYSICAL OCEANOGRAPHY, EarthCube, Big Data Science &Engineering","10/01/2016","09/13/2016","Charles Meneveau","MD","Johns Hopkins University","Standard Grant","Baris Uz","09/30/2020","$952,570.00","Alexander Szalay, Gregory Eyink, Randal Burns, Tamer Zaki","meneveau@jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","GEO","1610, 8074, 8083","4444, 7433, 8083","$0.00","Computer simulations of turbulent fluid flows are playing an increasingly vital role in engineering applications (e.g. reducing drag forces on vehicles and predicting wind turbine aerodynamic efficiency) and in geophysical sciences (e.g. describing the fate of pollutant dispersion or Lagrangian transport and mixing in the ocean). Simulations consist of discretizing and integrating the partial differential equations governing fluid flow and transport forward in time, providing solutions for physical variables (fields such as velocity and pressure) as function of time and space in the entire domain of interest. Since such simulations generate enormous amounts of data, the prevailing approach has been for researchers to analyze the data ""on the fly"" during the simulation runs while only a small subset of time-steps are stored for subsequent analysis. As a result, often large simulations of the same process must be repeated after new questions arise that were not initially obvious. Many (or even most) breakthrough concepts cannot be anticipated in advance, as they will be motivated in part by output data and must then be tested against it. As a result, there is a need for methods to store entire space-time data from such simulations.  This project develops innovative tools for the efficient creation of open numerical databases that contain massive outputs from computational fluid dynamics simulations used in turbulence research and geophysical transport modeling and makes these available to the entire community. Several of the datasets to be included into the Open Numerical Laboratory will be contributed by external researchers. In addition to enhancing engineering and geophysical fluid mechanics and turbulence research, democratized access to large-scale turbulent flow simulation data will also play a crucial role in education and training for the next generation of researchers. Active learning through new educational modules that allow students to query simulation datasets in unprecedented detail will provide new educational paradigms. More broadly, the lessons learned from this project will be generalizable to many other fields where numerical simulations generate very large datasets that are difficult to access using prevailing approaches. In this way, the project will enhance the scientific and broader impacts of the US high-performance scientific computing infrastructure. <br/><br/>This project will develop innovative tools for the efficient creation of open numerical databases that contain massive outputs from computational fluid dynamics simulations used in turbulence research and geophysical transport modeling. An ingest pipeline to be developed will enable users to transfer data from file systems containing the output of their massive direct numerical simulations, build a database, and serve it to the community for open exploratory data analysis and innovative turbulence and oceanic mixing research. To date, the investigators involved in this project have built an Open Numerical Laboratory focusing on direct numerical simulations (DNS) of canonical turbulent flows, in which the entire space-time data are available to the wider research community. However, the existing datasets are few in number and databases have been created one by one, using methodologies difficult to replicate on a massive scale.  Moreover, emerging Exascale simulations will potentially result in data sets of unprecedented scale (tens to hundreds of PetaBytes). Advanced computer science algorithms will be required to tackle these challenges. This project will (a) develop automated, and scalable data management algorithms to ingest, index and serve very large data sets generated by a wide range of groups, (b) explore novel algorithms using spatio-temporal subsampling combined with online interpolation with re-simulation, yielding large compression factors depending on the subsampling stride, and (c) use machine learning algorithms to identify localized regions of interest in the simulations and save these 4D domains in a database for detailed follow-up analytics. The new databases will include data from (1) the largest channel flow DNS, (2) rotating and stratified turbulence of geophysical interest, (3) a DNS of developing wall boundary layer and (4) detailed ocean circulation models with complex boundary conditions. As part of the innovative domain science applications, data sets will be used to improve turbulence models using data-assimilation concepts, study Lagrangian vortex dynamics, and explore geophysical transport in a regional general circulation model of the North Atlantic Ocean."
"1554140","CAREER: Algorithms and Decision Models for Learning in Health Care Systems","CMMI","OE Operations Engineering, CAREER: FACULTY EARLY CAR DEV","06/01/2016","01/27/2016","Mohsen Bayati","CA","Stanford University","Standard Grant","Georgia-Ann Klutke","05/31/2021","$500,000.00","","bayati@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","ENG","006Y, 1045","071E, 072E, 073E, 077E, 078E, 1045, 8023","$0.00","This Faculty Early Career Development (CAREER) grant aims to build new algorithms and mathematical models for optimizing decisions in healthcare systems. Growing availability of digital health records has created many opportunities for transforming health care delivery facilities such as hospitals, potentially improving their quality and operational efficiency. For example, recent research shows the benefits of guiding decisions in a hospital via statistical predictions. However, predictions are worthless when they cannot be integrated with the organization's workflow. In this regard, new ways of combining predictions with operations research models are needed. This research tackles mathematical barriers to the integration of operations research and statistical modeling. An example of this integration would be building new optimization algorithms that can adapt to patterns of uncertainty in the data at hand. The findings of this research can potentially improve quality of medical care and reduce health care costs. In addition, because this research lies at the intersection of machine learning, medicine, operations research, and statistics, it can help train the next generation of academic scholars (with emphasis on underrepresented groups) on this multidisciplinary area.<br/><br/>A common theme of statistical models in the age of big data is to characterize uncertainty via parametric or non-parametric models in a ""high dimensional"" space. High dimensional space refers to the space of available predictor variables that could contain information about uncertain outcomes in a decision task. Models based on high-dimensional data are particularly useful in healthcare because healthcare systems are complex, with many system-specific features in their patient populations and practice patterns. Such potential applications have led to a large body of recent literature (known by high-dimensional statistics) to deal with algorithmic and statistical challenges of such modeling framework. In contrast, in the operations research literature, the uncertainty is typically restricted to few known probability distributions to make mathematical analysis more tractable. This research aims to relax the aforementioned restrictions by combining ideas from the high-dimensional statistics with the mathematics of operations research, and applying the resulting models to two application areas -- wait time prediction in emergency departments, and personalized administration of new treatments. For example, in settings that can be modeled as multi-armed bandit problems, new theory that can help reduce the uncertainty in decisions by utilizing availability of many predictor variables in addition to the data on past decisions can be useful. Reducing this knowledge gap requires developing a new class of algorithms and asymptotic theory when the number of predictors grows faster than time periods or the number of samples. Another challenge that needs to be addressed is that in statistical setting the samples are usually assumed to be independent, however this assumption fails in decision systems with feedback where current decisions may impact future samples."
"1618244","III: Small: Quantifying Multifaceted Perception Dynamics in Online Social Networks","IIS","Info Integration & Informatics","09/01/2016","04/25/2017","Aron Culotta","IL","Illinois Institute of Technology","Standard Grant","Sylvia Spengler","05/31/2020","$471,992.00","Jennifer Cutler","culotta@cs.iit.edu","10 West 35th Street","Chicago","IL","606163717","3125673035","CSE","7364","7364, 7923","$0.00","Measuring public perceptions and how they change over time is a central problem in marketing, public health, and politics. Traditional measurement methods rely on surveys and focus groups, which can be costly and time-consuming. Online social networks offer an attractive alternative: real-time perceptions can be estimated from public, online activity and compared with an entity's communications to quantify how public messaging affects perception.  While prior algorithmic approaches rely purely on text-based sentiment analysis, this project will develop novel methods based on the insight that an entity's online social connections are indicative of how they are perceived (e.g., ""birds of a feather flock together""). Thus, rather than typical one-dimensional measures of sentiment, the project will instead investigate public perception with respect to multiple characteristics of an entity (e.g., is it seen as pro-environment, pro-health, etc.). A multi-faceted evaluation will be performed to study the phenomenon of ""greenwashing,"" a deceptive marketing practice in which firms market their products or policies as more environmentally friendly than they truly are. This project has the potential to enhance consumer protection by exposing deceptive marketing practices.<br/><br/>The project will develop social network analysis algorithms to assess perception of an entity and also language processing algorithms to quantify the communications of an entity with respect to a perceptual attribute. The approaches to both problems rely on innovative algorithms to measure the strengths of the social and linguistic relations between public entities and exemplar accounts that typify the perceptual attribute of interest. A key advantage of the approach is its minimal requirement of human input, e.g., given only a single keyword like ""environment,"" the approach identifies suitable exemplars and fits linguistic and perceptual models. The project will develop novel machine learning methods for domain adaptation, positive-unlabeled learning, and learning from label proportions in order to fit such models and ensure they are robust to omitted variable bias. The models will be evaluated using public Twitter and Facebook data to quantify the relationship between the perceptions and online communications of brands and other public entities, with a particular focus on identifying cases of greenwashing."
"1553579","CAREER: SentientCache: Rethinking the Cache Abstraction","CNS","Special Projects - CNS, CSR-Computer Systems Research","01/01/2016","02/27/2020","Ymir Vigfusson","GA","Emory University","Continuing Grant","Marilyn McClure","12/31/2020","$552,532.00","","ymir@mathcs.emory.edu","1599 Clifton Rd NE, 4th Floor","Atlanta","GA","303224250","4047272503","CSE","1714, 7354","1045, 7354, 9251","$0.00","The SentientCache project focuses on improving the performance and cost-effectiveness of large-scale distributed systems by automatically learning what data should be memorized.  Inadequate performance of distributed systems, including those supporting major websites, is frustrating and costly. Performance problems are frequently addressed by using dedicated computers, so-called cache servers, for storing answers to common requests to reduce response time while also shielding the databases supporting the system from disruptively high loads.  The SentientCache project aims to revamp the design of modern caches to better utilize server resources. The project will produce theoretically and experimentally validated prototypes of cache systems that factor in external costs and contextual information, and learn from experience.<br/><br/>The SentientCache project will engage graduate and undergraduate students in innovative research through real-world workload analysis, software engineering and large-scale system experimentation.  As well, there is an educational outreach plan to expose undergraduates, high school and K-12 students to research in computer systems with exciting competition components for high-school and undergraduate students to promote computer science careers.<br/><br/>Successful research outcomes from the project should advance the field of computer systems and shift the cache paradigm towards endowing system caches with greater intelligence and interaction with their environments in several ways:  The research will systematically characterize changes in workloads, scale, performance requirements and memory constraints of distributed caches, use advanced machine learning and other prediction techniques within systems research, and better determine how profiling can help to automatically perform cross-cutting performance optimization within large-scale systems."
"1613035","Variable Selection via Inverse Modeling for Detecting Nonlinear Relationships","DMS","STATISTICS","08/01/2016","08/16/2018","Jun Liu","MA","Harvard University","Continuing Grant","Gabor Szekely","07/31/2020","$200,000.00","","jliu@stat.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","MPS","1269","","$0.00","With the ever-growing amount of data in many application areas, effective methods for detecting factors influencing the value of a response variable are in high demand. It is of growing importance to develop methods for detecting variables that exert significant nonlinear response. Inspired by the sliced inverse regression method developed in the early 1990s, the PI proposes a general framework for developing effective variable selection strategies in nonlinear systems of high dimension. The PI will further study theoretical properties of these variable selection algorithms. The proposed theoretical investigation will provide theoretical understanding of limitations of existing dimension-reduction techniques when the dimensionality grows with the sample size. <br/> <br/>With the ever-growing amount of data in many application areas, effective methods for detecting factors that may influence the value of a target quantity of interest (response variable) are in high demand. The problem is termed as ""variable (or feature) selection"" in regression modeling and statistical learning, and is a long-standing problem in statistics and machine learning. The PI focuses here on the detection of factors that may exert nonlinear and/or interactive effects on the response variable. Recent studies from the PI's group reveal that the sliced inverse regression (SIR) and inverse modeling strategies provide a powerful framework for developing effective variable selection strategies in nonlinear systems of high dimension. The PI aims at developing more robust and effective tools for detecting such complex relationships and studying theoretical properties of SIR-based algorithms. The proposed method will also be applicable to do robust variable selection for classification problems. The proposed theoretical investigations will provide (a) theoretical understanding of limitations of existing dimension-reduction techniques when the dimensionality grows with the sample size; (b) guidance on the construction of necessary sparsity conditions that can guarantee consistency of variable selections in ultra-high dimensional nonlinear problems; (c) the optimal convergence rate of that the best possible learning algorithm can achieve in such settings; and (d) theoretical justifications whether the proposed algorithms can achieve or are not far from the optimality."
"1619458","III: Small: Robustness in Social Network Analysis: Models, Inference, and Algorithms","IIS","Info Integration & Informatics","09/01/2016","06/28/2016","David Kempe","CA","University of Southern California","Standard Grant","Maria Zemankova","08/31/2020","$507,996.00","Yan Liu","David.M.Kempe@gmail.com","University Park","Los Angeles","CA","900890001","2137407762","CSE","7364","7364, 7923","$0.00","The burgeoning field of ""Social Network Analysis"" focuses on extracting useful insights from such social network data. Implemented or envisioned applications range from learning about the nature and driving forces behind human interactions, to targeted product or activity recommendations and even homeland security. Contrary to other networks, such as transportation or computer networks, massive uncertainty and noise are practically always associated with social network data: data pertaining to individuals are often not observable, or are observed incorrectly. The primary goal of this project is to understand the risks and implications of such noisy data, and to design network analysis algorithms that are significantly more robust to noise and missing data. Given the importance that mathematical models play in social networks analysis, a closely related thread of the project is to analyze the fit between typical social network models and real-world data, in particular regarding high-level connectivity properties. The project website will be used to disseminate research prototypes and data that are collected as part of the project.<br/><br/>Specifically, three connected research thrusts that integrate the PIs' expertise in machine learning and theoretical computer science will be explored: (1) How well do standard random graph models fit real-world social network data, in particular with regard to expansion and spectral properties? Since the answer likely is ""poorly,"" how well do modifications based on requiring local or global structure remedy this problem? (2) What is the impact of missing observations of diffusion or activation processes on the inferred social networks when learning from some contagious behavior? How can this impact be mitigated by algorithms that take the possibility of missing data into account? (3) If social network data are observed with significant (and possibly non-random) noise, under what conditions can stability of an algorithmic output be ensured? How ""obvious"" does the right answer have to be to not get obscured by noise in the data? Can ""obvious"" answers be found more efficiently? The proposed research has the potential to impact the way in which social network inference and optimization are addressed. The PIs are committed to a suite of activities, among them inclusion of undergraduate students in the proposed research and outreach to local high school students, for broader impacts."
"1566219","CRII: III: Computational Methods to Explore Big Bioassay Data for Better Compound Prioritization","IIS","","05/01/2016","04/20/2017","Xia Ning","IN","Indiana University","Continuing Grant","Sylvia Spengler","11/30/2018","$172,350.00","","ning.104@osu.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","CSE","026y","7364, 8228","$0.00","Bioassay data represent an extremely valuable source of experimental Big Data with rich content that have been substantially produced in the early stages of drug discovery for testing chemical compound bioactivities and identifying promising drug candidates. However, the power of such Big bioassay data has not been fully unleashed, particularly for the purposes of discovering novel knowledge and improving drug development. This is largely due to the fact that the exploration of a much larger space of bioassays has been fundamentally hindered by the less developed ability to identify and utilize the relations across bioassays. In this project, the PI and her team will develop novel computational methods and tools that can effectively explore a wide range of heterogeneous bioassays, identify experimentally unrevealed relations among them, and utilize the novel knowledge derived from them so as to improve compound prioritization. The research will bring scientific impacts and shed light on fully utilizing the existing wealth of Big Data, stimulating knowledge distillation in innovative manners, establishing visionary conceptual hypotheses and developing novel analytical techniques correspondingly. This research aims to solve critical problems in drug discovery through Big Data means, and has a great potential to improve drug candidate identification through accurate compound prioritization, and thus it will have far-reaching economic and societal impacts. <br/><br/>The PI and her team will develop a computational framework to produce better compound ranking for each bioassay. This framework will consist of a local structure learning component and a global structure learning component to discover and leverage the compound ranking within a bioassay and ranking relations across bioassays, respectively. They will also develop new methods to better rank compounds under a combination of criteria. In particular, they will solve compound ranking based on activity and selectivity simultaneously by leveraging ranking difference across bioassays. The research will be innovative, both in terms of employing original computational models and methods into important problems in drug discovery, and in terms of developing unique methodologies and computational techniques for core Computer Science research. For drug discovery, the research will provide novel perspectives and methodologies as to how researchers can utilize the large-scale experimental data to solve important problems in drug discovery. For core Computer Science, the research will contribute a new solution framework and methods spanning the areas of data mining and machine learning. Specifically, the research will lead to novel methods for boosting ranking performance by actively including additional data, incorporating relevant information within a regularized optimization framework, deploying iterative procedures and greedy strategies for large-scale problems with multiple simultaneous tasks, etc. All these methods are generalizable to a variety of other Computer Science applications. For further information see the project web page: http://cs.iupui.edu/~xning/compRank.html"
"1606535","Collaborative Research: Opening a Quantitative Window into the Mind and Communication of Dolphins","PHY","PHYSICS OF LIVING SYSTEMS","09/01/2016","08/27/2019","Diana Reiss","NY","CUNY Hunter College","Continuing Grant","Krastan Blagoev","08/31/2020","$352,403.00","","diana.reiss@hunter.cuny.edu","695 Park Avenue","New York","NY","100655024","2127724020","MPS","7246","8091","$0.00","The cognitive and communication prowess of the large-brained, highly gregarious bottlenose dolphin is legendary and has captured the interest and imagination of scientists and the public at large. Notably, dolphins are one of a handful of species that are vocal learners; thus the dolphin presents us with a unique and alternative model to songbirds to advance our understanding of the processes that underlie vocal learning and communication in a large-brained mammal showing high social and behavioral complexity. Despite decades of research on dolphin vocal communication, the constituent elements, the sequential and temporal organization of the elements, and the nature of vocal transactions between dolphins remain enigmatic. In order to decipher dolphin communication and further elucidate these issues, the PIs will use observational approaches in the wild and in an aquarium as well as two experimental approaches in an aquarium. A first approach studies how dolphins coordinate their behavior in the face of novelty: How do dolphins synchronize their behavior when requested to present novel behaviors in tandem? How do they succeed in working as a team when confronted with a new challenge? In a second approach, the researchers will provide dolphins with a two-way, interactive touchscreen and audio system. Their proclivity for vocal learning and their other cognitive processes will be probed by giving them choice and control over the interactive system and observing and documenting how they use and respond to the contingencies of their interactions, which shall be obtaining novel acoustic signals temporally paired with objects, interactions or activities. The aquarium studies are particularly conducive to outreach efforts and informal science education since the dolphin-related activities will be conducted in an open amphitheater environment at a leading aquarium. The exhibit has changed from dolphin shows to more educational demonstrations about animal care, behavior, and conservation. The research activities will take place concurrently with visits of the public and field trips of school children and therefore, the research endeavor will be communicated to the public in real time. Virtually all the activities proposed can, with modest effort and cost, be made into an open exhibit at the National Aquarium in Baltimore, instructing visiting schoolchildren on the research process and on collaborative research across scientific boundaries. The field trips to the research sites are organized jointly with undergraduate and graduate courses in field research for Hunter College students and are excellent opportunities to showcase and engage actual cutting edge research to these students. This tightly-knit interdisciplinary collaboration between the labs of animal cognitive psychologist and marine mammal scientist and physicist, computational neuroscientist and bio-acoustician, seeks to and extend high-throughput settings to dolphin communication studies in captivity.<br/><br/>A key intellectual merit of this proposal is to address these fundamental questions in the field of animal cognition and communication through powerful methods drawn from statistical mechanics, machine learning, and signal processing. Central technical problems in ""decoding"" dolphin communication are the difficulty in sound localization and attributing the detected vocalizations to specific individual dolphins, the lack of automated behavioral analysis methods, and the crucial step of creating an integrated picture of attributed vocal and behavioral transactions or ""conversations"" that can be subjected to high-through analysis. Another technical problem is the design and deployment of an advanced two-way interactive system consisting of an underwater touchpad and real-time audio input/output, plus the associated set of interaction interfaces. Finally, this project also addresses important technical problems in the deployment of aerial and aquatic drones for field studies."
"1561512","Enabling Cloud-Based Quality-Data Management Systems","CMMI","OE Operations Engineering","07/01/2016","03/11/2016","Shiyu Zhou","WI","University of Wisconsin-Madison","Standard Grant","Georgia-Ann Klutke","06/30/2020","$299,498.00","Xiaojin Zhu","szhou@engr.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","ENG","006Y","071E, 072E, 073E, 076E, 077E, 078E, 8023","$0.00","Cloud-based platforms for accessing, sharing, and visualizing manufacturing-enterprise-level data are becoming available. In a cloud-based quality-data-management system, the quality-characteristics of different devices, products, and facilities are accumulated in a centralized database. These data pertain to multiple machines and multiple facilities, offering opportunities to achieve more effective quality control and productivity improvements. However, most cloud-based platforms are as yet unable to exploit the information contained in such data to make better decisions for production-system control and quality improvement. The objective of this project is to advance a series of methodologies that enable modeling of a large number of quality characteristics, timely change detection, accurate root cause diagnosis, and optimal repair decision-making. The project will also contribute to workforce training by offering students opportunities to engage in interdisciplinary research dealing with manufacturing, computing, sensing, and machine learning. <br/><br/>The reason why cloud-based platforms may not as yet exploit manufacturing-enterprise-level data lies in the dearth of techniques to (1) describe the quality characteristics and their relationships, and (2) make decisions informed by such descriptive models. To enable cloud-based quality-data-management systems of the future, the investigators will first advance methodology needed for a flexible, yet rigorous, hierarchical graphical model, which will describe the inter-relationships among different quality characteristics. The hierarchical structure of the model will enable information sharing across different facilities within an enterprise. Based on this descriptive model, the investigators will next develop methodologies for process monitoring and diagnosis via likelihood based risk-adjustment and Bayesian-factor theory, and for optimal repair decisions via Partially Observable Markov Decision Processes (POMDP) framework. The developed methodologies will be tested on data obtained from an industrial collaborator."
"1565928","CRII: SHF: FSM-Centric Approximate Computing --- A Disciplined Approach","CCF","CRII CISE Research Initiation","03/01/2016","02/16/2016","Zhijia Zhao","CA","University of California-Riverside","Standard Grant","Yuanyuan Yang","02/28/2019","$175,000.00","","zhijia@cs.ucr.edu","Research & Economic Development","RIVERSIDE","CA","925210217","9518275535","CSE","026Y","7798, 7941, 8228","$0.00","This project proposes a new paradigm to enhance computing efficiency --- Finite State Machine (FSM)-centric approximate computing. Approximate computing has shown promise for both reducing energy consumption and improving performance across different applications, especially those in image processing, machine learning and data analytics. To date, approximate computing has been inapplicable to FSM modeling of computations, which has important applications in domains that include biological science, cyber security, data compression, software engineering and hardware design. Growing data volumes and limitations on computer processing power constrain FSM?s efficiency. The establishment of FSM-centric approximate computing will open the door to a new dimension of efficiency optimization for software applications.<br/><br/>This research will take advantage of the synergy between FSM computations and approximate computing --- the inherent error tolerance capability within FSM computations --- to develop a computing platform for exploring approximate FSM computations. The key idea is a quantitative analysis of FSM reliability that captures how errors generated by underlying approximate hardware propagate through FSM transitions. Additionally, this research will also design and implement two complementary approximation schemes --- one relies on the ""inexactness"" of approximate hardware; the other provides pure software approximation and runs on conventional exact hardware. Together these approximation strategies will demonstrate the potential of FSM-centric approximate computing in improving the efficiency of FSM applications."
"1624727","Phase II I/UCRC Pennsylvania State University Site: Center for Health Organization Transformation","IIP","IUCRC-Indust-Univ Coop Res Ctr","10/01/2016","07/11/2019","Conrad Tucker","PA","Pennsylvania State Univ University Park","Continuing Grant","Prakash Balan","09/30/2020","$321,500.00","Christopher DeFlitch, Nilam Ram, Harleah Buck","conradt@andrew.cmu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","ENG","5761","116E, 5761, 8042, 9102, 9251","$0.00","Phase II I/UCRC Pennsylvania State University Site: Center for Health Organization Transformation<br/><br/>The U.S. healthcare system is a global leader in terms of scientific and clinical technology development. However, with expenditures at 17.1% of the GDP, significant gaps in terms of access and quality exist, along with rates of morbidity and mortality that are substantially worse than other industrialized nations. The mission of CHOT is to advance the knowledge and practice of transformational strategies in evidenced-based healthcare management and clinical practice. The development of transformative approaches will necessitate involving partners from all sectors of healthcare including IT providers, systems solutions providers, care providers, insurers, and employers with large healthcare coverage programs. The realization that a ""one-size-fits-all"" approach to healthcare is unsustainable has motivated the modeling, development and deployment of personalized advanced wellness systems (PAWS). PAWS transcends traditional boundaries existing within healthcare systems, ranging from personalized patient care models (from a patient?s perspective), to personalized drug delivery systems (from a healthcare decision maker's perspective). CHOT is the only industry-university cooperative research center focused on innovations in healthcare delivery. The Center's interdisciplinary approach will deliver integrated and broadly applicable solutions and will enhance the education of the next generation of healthcare policy experts, physicians, nurses, and engineers. Undergraduate and graduate students become a part of a high-performing multidisciplinary organization that provides an ecosystem for networking, leadership, and career placement. It is expected that the work of the Center will result in improved health care quality and more effective use of financial and system resources in the health system, while providing immediate decision support to industry. <br/><br/>CHOT has identified four research areas in order to meet the needs of our members as well as the healthcare industry: macro-systems innovation; health quality, safety, access, and efficiency; patient-centered care and population health interventions; electronic health records and information exchange. The underlying theme of personalized advanced wellness systems (PAWS) is achieved through fundamental research in value-based care, time-driven activity based costing, person-centered care, medical device design and integration, emergency department benchmarking, community paramedicine, telehealth and wellness, and precision medicine and precision health. A concerted plan of seven research projects will be supported by research methods in systems engineering. In particular, systems thinking and system dynamics methods will be combined with simulation, data analytics, and machine learning in order to develop models that can inform the prospective assessment of interventions and innovations. In order to bridge gap between what is learned/discovered and what is done in practice/ management, knowledge-to-action (KTA) initiatives will be pursued, with a view toward technology readiness in order to increase the impact of these projects."
"1640867","CIF21 DIBBs: EI: Data Laboratory for Materials Engineering","OAC","DMR SHORT TERM SUPPORT, Data Cyberinfrastructure","09/01/2016","09/23/2016","Venugopal Govindaraju","NY","SUNY at Buffalo","Standard Grant","Amy Walton","08/31/2020","$2,909,772.00","Krishna Rajan, Thomas Furlani, Srirangaraj Setlur","govind@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","1712, 7726","026Z, 7433, 8048, 8400","$0.00","This project directly addresses the goals of the Materials Genome Initiative -- to accelerate the pace of discovery and deployment of advanced material systems.  To obtain insights for the discovery of new materials and to study existing materials, scientists and engineers rely heavily on an ever-growing number of materials research databases and scholarly research publications that date back many decades.  New materials innovation often takes years, sometimes decades, to develop a new material.  The project addresses the challenges through several steps, including automatic extraction of data from relevant electronic publications, storage of the data in formats that support comparison and analysis, development of advanced computational tools to improve the analysis, integration of the tools into a data laboratory to support the discovery of new trends and relationships among materials properties, and to predict new materials with desired properties.<br/><br/>The infrastructure building blocks developed under this project enable researchers to (i) use document processing technologies to process scientific publications and data from scientific databases in materials science to create a knowledge base; (ii) use machine learning technologies to learn from the data in this enhanced knowledge base to address a variety of use cases in materials science and engineering; and (iii) use innovative information retrieval and visualization tools for insightful analysis, facilitating faster discovery of new materials.  The tools will be hosted and disseminated through a web portal built on the HubZero platform, which will also provide users with the ability to query and visualize data, and run simulations and experiments. The data laboratory portal also provides the ability to run simulations and experiments on high-performance computing clusters using the building blocks.   The data laboratory provides a platform for materials informatics, enabling prediction of properties of metal alloys and interaction with the materials discovery and engineering user community.  While the primary target is the interdisciplinary field of materials research, the tools are designed to be domain agnostic as the core technologies can be applied to documents and databases across a broad swath of disciplines to enhance the pace of scientific discoveries.<br/><br/>This award by the Advanced Cyberinfrastructure Division is jointly supported by the NSF Directorate for Mathematical & Physical Sciences (Division of Materials Research)."
"1624503","I/UCRC: Center for Hybrid Multicore Productivity Research (CHMPR) - Rutgers Site","CNS","IUCRC-Indust-Univ Coop Res Ctr","08/01/2016","11/05/2019","Nabil Adam","NJ","Rutgers University Newark","Continuing Grant","Behrooz Shirazi","07/31/2021","$249,974.00","Basit Shafiq, Vijayalakshmi Atluri, Jaideep Vaidya, Soon Chun","adam@adam.rutgers.edu","Blumenthal Hall, Suite 206","Newark","NJ","071021896","9739720283","CSE","5761","043Z, 5761","$0.00","As a research site of the NSF I/UCRC for Hybrid Multicore Productivity Research (CHMPR), the mission of the Rutgers Institute for Data Science, Learning, and Applications (I-DSLA) is to advance research and education in the area of Big data management and analytics. The multidisciplinary research program in collaboration with the industry partners, focuses on addressing research and development challenges in application domains that span healthcare; e-government and e-commerce; and community resilience and public safety. A key objective of this project is to integrate research and education by providing student opportunities to work on real world problems with industry partners. The emphasis on real problems is particularly suited to attracting individuals from diverse backgrounds, broadening participation in computer science and informatics research.<br/><br/><br/>The research agenda of the Rutgers I-DSLA site is complementary and synergistic to that of CHMPR. The research projects at I-DSLA span several themes (data analytics, machine learning, data management, information security and privacy, social media, Web services, and semantic Web) and domains. In the healthcare domain, the multidisciplinary research program involves researchers from the Rutgers New Jersey Medical School Cancer Center, Rutgers I-DSLA, and Rutgers Center for Information Management, Integration, and Connectivity (CIMIC). The program leverages the complementary expertise of the team members to develop data analytics-based approaches that enables discovery of optimal treatment regimens and identification of patient and disease characteristics that permit responses to specific therapeutic combinations and modalities. In the e-government and e-commerce domains, the research work addresses fundamental problems related to secure information sharing, business process composition and management, and privacy-preserving data analysis. In the community resilience and public safety domain, the research work focuses on disaster management, critical infrastructure protection, and smart cities."
"1618679","AF: Small: Harmonic Analysis for Quantum Complexity","CCF","Algorithmic Foundations","06/01/2016","05/17/2016","Ryan O'Donnell","PA","Carnegie-Mellon University","Standard Grant","Dmitri Maslov","05/31/2019","$450,000.00","","odonnell@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7796","7923, 7927, 7928","$0.00","The PI will conduct new research to advance the field of harmonic analysis of Boolean functions.  At a high level, harmonic analysis can be described as the theory of using a particular mathematical tool, the Fourier transform, for detecting, analyzing, and representing patterns in large data sets.  Applying it in the context of Boolean functions refers to the case where the data comes from 0s and 1s, the basic building blocks in computer science.  Harmonic analysis of Boolean functions has proven to be a powerful tool in computer science, with application to theories of machine learning, error-correcting codes, cryptography, privacy, optimization algorithms, and distributed computing.  Part of the aim of this project is to further develop the mathematical and computer science foundations of harmonic analysis of Boolean functions.  Another aim of the project is to expand its scope to include further applications in the field of quantum computation.  If and when quantum computers are built, it is known that they will be able to break cryptographic codes that are secure today; this is fundamentally due to their ability to efficiently perform Fourier transforms on huge data sets.  However the full potential power of quantum computation is not well understood, and building quantum computers remains a major engineering challenge.  The project will investigate these issues, analyzing the advantages of quantum computers over classical ones, and understanding how to more efficiently test components of quantum computers. A final key outcome of the project will be scientific and educational training for computer science graduate students at Carnegie Mellon University, as well as wide dissemination of the research produced.<br/><br/>            At a more technical level, the project has several major-stretch-goals that will be used to guide the research in harmonic analysis of Boolean functions.  Included among these are conjectures of Aaronson and Ambainis concerning the influence of variables on low-degree Boolean functions, and on the decision tree complexity of checking correlation of a function with its Fourier transform.  Proofs of these conjectures would yield new complexity results delimiting the power of quantum computation versus classical computation.  Another major technical goal of the project is to develop the theory of learning and testing an unknown quantum state."
"1566455","CRII: NeTS: Ubiquitous Sensing based Location-aware Driving Safety System","CNS","CRII CISE Research Initiation","07/01/2016","04/11/2016","Yan Wang","NY","SUNY at Binghamton","Standard Grant","Alexander Sprintson","06/30/2019","$175,000.00","","y.wang@temple.edu","4400 VESTAL PKWY E","BINGHAMTON","NY","139026000","6077776136","CSE","026Y","7363, 8228","$0.00","This project exploits mobile sensing and vehicle localization to identify fine-grained abnormal driving behaviors, such as weaving, swerving, and fast U-turns, and further to infer location-aware dangerous vehicular status. Several existing works have tried to detect abnormal driving behaviors by focusing on detecting drivers' status based on pre-deployed infrastructure, such as alcohol sensors, infrared sensors, and cameras. Such approaches incur extra installation cost and are thus difficult to be widely adopted. In order to build pervasive location-aware driving safety systems, this project tries to deploy low power consumption sensing (utilizing mobile devices carried by users in vehicles) and learning techniques based on statistical analysis to localize vehicles and identify fine-grained abnormal driving behaviors. More importantly, the proposed system keeps tracking the drivers' behaviors and determines fine-grained location-related dangerous vehicular status, such as driving on the center line of two-way roads or occupying left lanes for a long time.<br/><br/>This project seeks to conduct a comprehensive study to understand to what extent the current mobile devices can model various real-world driving behaviors and corresponding vehicle dynamics. A new real-time mobile sensing system, which combines real-time mobile sensing and heterogeneous driving environments, is developed to address driving safety concerns. The final results will be the abiding principles of cyber-physical architecture that resolve dynamic impacts of complex environments and provide clear guidelines over Internet of Things (IoTs). Specifically, effective features are investigated from mobile sensor readings that are able to depict each type of abnormal driving behaviors. These features can thus be extracted to localize the vehicles and derive the patterns of abnormal driving behaviors (e.g., weaving, swerving, fast U-turn, and sudden breaks) with the consideration of generic driving scenarios and heterogeneous mobile devices. Techniques based on machine learning are developed to generate a classifier model that could clearly identify fine-grained abnormal driving behaviors. The classifier model will be further utilized as a foundation to devise the location-aware driving safety system, which can track users' driving behaviors and realize location-related dangerous vehicular status in real-time using low-computing-capability mobile devices."
"1637254","EAGER: Collaborative Research: mHABIT - Towards Building a Living Lab for mHealth Analytical and Behavioral Research using Internet of Things","SES","S&CC: Smart & Connected Commun","09/01/2016","08/30/2016","Anindya Ghose","NY","New York University","Standard Grant","Sara Kiesler","08/31/2018","$115,376.00","","aghose@stern.nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","SBE","033Y","042Z, 7916, 9150","$0.00","This project will create a living lab for mHealth Analytical and Behavioral Research using Internet of Things (mHABIT) to build a generalizable infrastructure for new analytical models and a Behavioral Experimentation Platform (BEP) to understand drivers of human health and wellness behavior and lifestyle changes through mobile and sensor technologies. Using an interdisciplinary approach, this project will enhance the understanding of human behavior and interactions with smart technologies in communities. The investigators will leverage test beds with domestic and international partners to advance knowledge towards developing new analytical and experimental methods drawn from econometrics, machine learning, behavioral economics and randomized field experiments. This project will contribute to a scalable prototype technology platform and lead to new solutions for improving user health and wellness, and healthcare efficiency. Overall, this project will integrate advanced Internet-of-Things infrastructures with an instrumented version of the physical world to improve quality of life, health and wellbeing, and sustainability of communities. The methods and infrastructures developed from this project can be easily deployed by healthcare providers to support data collection, analytics, solution and evaluation. The insights from this project will suggest policy implications towards the design of smart community through sustained usage of emerging technology. Moreover, broader impact includes dissemination of research to the public, underrepresented groups, and widespread deployment of the technology.<br/> <br/>The infrastructure developed from this project will collect and analyze large-scale and fine-grained user GPS trajectory data and RFID tracking data, linked with the EHR data, to examine what factors drive users' engagement with mHealth, their interactions with doctors inside and outside the clinical setting, and what changes they make in their personal lifestyle to improve their health outcomes. To evaluate the learning of user health behavior and decision making, the investigators plan to implement a pilot deployment of the BEP in the mHABIT living lab, by partnering with healthcare providers in the US and overseas, to design and implement novel mobile-enabled interventions and evaluate the effectiveness of mHealth technology from a causal perspective."
"1618784","CHS: Small: Early Dyslexia Detection and Support at Scale to Help Students Succeed in School","IIS","HCC-Human-Centered Computing","07/01/2016","06/20/2019","Jeffrey Bigham","PA","Carnegie-Mellon University","Standard Grant","Ephraim Glinert","06/30/2020","$567,999.00","Maria Luz Rello Sanchez","jbigham@cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7367","7218, 7367, 7923, 9251","$0.00","At least 10% of the population has dyslexia, which results in difficulty with reading and writing, and often leads to school failure (40% of those who drop out of school have dyslexia).  If people know they have dyslexia, they can with effort train over time to overcome its negative effects.  Yet even though we know how to detect dyslexia, most children are diagnosed late because current procedures are expensive and require professional oversight.  The PI's goal is for everyone to know as early as possible if they might have dyslexia; his approach to achieving this goal is to make it easy, inexpensive, and even enjoyable to find out.  To these ends, he and his team plan to design personalized game activities based on the detection results to target the cognitive skills with which students need to practice most.  Much of the research into detection and support activities has thus far taken place in the lab; the team plans to extend this work into the real world via wearable devices to help people with dyslexia better read and write in the context of learning activities outside of the classroom, e.g., in museums or at historic sites.  The work will build on the team's Dytective software, which the PI plans to publicly release along with other tools that will be developed and refined as part of this research.  The team will work with community partners like dyslexia organizations and schools, undergraduate and graduate students, and experts from related fields to disseminate their findings as widely as possible, to nurture the development of young researchers in this area, and to integrate their work with other related efforts.  In addition, new course modules on Dyslexia and Language Technology will be added to the Human Factors course at CMU.<br/><br/>Current approaches for detecting dyslexia require either a professional psychologist or expensive brain imaging equipment (and an expert to run it and interpret the results).  The PI's approach to detecting and supporting dyslexia uses a scalable web-based game.  The method relies on human-computer interaction metrics drawn from people playing games designed with a linguistic and empirical understanding of the errors that people with dyslexia tend to make.  Machine learning over this data may allow for the detection of dyslexia much earlier (and at much less expense) than would otherwise be possible.  Although the current research is informed by prior work characterizing the origin of dyslexia and its linguistic manifestation, the intellectual contribution lies instead in understanding how we can use data from game play to detect dyslexia.  This approach, once demonstrated, may generalize to other areas, and the exercises that are found to be most useful in detecting or supporting dyslexia may also inform our basic understanding of dyslexia."
"1560037","REU Site: Software Testing and Analytics","CCF","RSCH EXPER FOR UNDERGRAD SITES","03/15/2016","10/18/2018","Junhua Ding","NC","East Carolina University","Standard Grant","Harriet Taylor","02/29/2020","$359,838.00","M.H. Tabrizi","junhua.ding@unt.edu","Office Research Administration","Greenville","NC","278584353","2523289530","CSE","1139","9250","$0.00","This project will establish a three-year REU site in software testing and analytics at East Carolina University (ECU). It will offer a ten-week research program for ten undergraduate students during summer semesters. The faculty-student interaction as well as interaction among students will take different forms such as meetings, seminars, tutorials, workshop, and field trips. The REU project will allow a diverse pool of undergraduate students to experience cutting-edge research experience that will help them to become self reliant in STEM research. Students will gain valuable research skills that will prepare them for their future fields of study, and their exposure to the research will help them to compete for high technology fields in an innovative job market. The research experience will also motivate them to continue onto graduate studies. The REU project also will provide students an opportunity to collaborate with their faculty mentors and student peers across the nation after the summer program. <br/><br/>The sample research projects cover open research topics in software testing and analytics. Software Testing and Analysis of Scientific Software is to investigate the technique for adequately testing complex scientific software systems. The experimental data generated from the testing will be analyzed with machine learning tools for improving the test efficiency and effectiveness. We expect students will master basic principles of software testing and become skillful in creating test strategies and using tools for testing scientific software. Fault Detection Effectiveness and MC/DC Coverage of Combinatorial Test Cases will investigate the integration of combinatorial testing and MC/DC (modified condition/decision coverage) testing. Studies such as how logical expressions can be effectively tested, sensitivity analysis of different partitions of the input domain and factors that may affect combinatorial-based test generation, and a cross comparison between tests generated using different combinatorial testing algorithms will be conducted. Students will receive rigorous training in software testing and software testing research in this project. Software Analytics for Mobile Domain Specific Language (DSL) Construction will analyze program analysis results for the improvement of the development of DSL, and Guided Test Generation for Web Applications will use program analysis results to derive tests for testing web applications. The two projects will offer students the opportunity to learn the principles, applications and experimental study of program analysis."
"1618629","TWC: Small: Intelligent Malware Detection Utilizing Novel File Relation-Based Features and Resilient Techniques for Adversarial Attacks","CNS","Secure &Trustworthy Cyberspace, EPSCoR Co-Funding","08/15/2016","08/22/2016","Yanfang Ye","WV","West Virginia University Research Corporation","Standard Grant","Sol Greenspan","11/30/2019","$481,693.00","Katerina Goseva-Popstojanova","yanfang.ye@case.edu","P.O. Box 6845","Morgantown","WV","265066845","3042933998","CSE","8060, 9150","7434, 7923, 9102, 9150","$0.00","Malware (e.g., viruses, worms, and Trojans) is software that deliberately fulfills the harmful intent of an attacker. It has been used as a major weapon by the cyber-criminals to launch a wide range of attacks that cause serious damages and significant financial losses to many Internet users. To protect legitimate users from these attacks, the most significant line of defense against malware is anti-malware software products, which predominately use signature-based methods to recognize threats. However, driven by considerable economic benefits, malware attackers are using automated malware development toolkits to quickly write and modify malicious codes that can evade detection by anti-malware products. In order to remain effective, the anti-malware industry calls for much more powerful methods that are capable of protecting the users against new threats and are more difficult to evade. The broader impacts of this work include benefits to the society at large by making cyberspace more secure and resilient to cyber-attacks. The project integrates research with education through curriculum development activities and engages graduate and undergraduate students in research. It is also expected to increase the involvement of underrepresented groups, including minority and women.<br/> <br/>The goal of this project is to design and develop intelligent and resilient solutions against malware attacks. The project is focused on the following research aims: (1) design novel relation-based features (e.g., file co-occurrence, file co-location, and bundled installations) that are more robust and harder to evade in malware detection; (2) design and develop an effective semi-supervised learning framework utilizing both content-based and relation-based features for malware detection; and (3) design and develop resilient techniques against adversarial attacks on machine learning/data mining based models. The techniques developed by this project will create a resilient platform, at both feature and model levels, against adversarial malware attacks. Furthermore, the proposed techniques are designed to be arm race capable, and can be used in other cyber security domains, such as anti-spam, fraud detection, and counter-terrorism. Through this project, a joint computer security lab will be established which aims at creating innovations for intelligent and resilient defenses against malware attacks as well as other cybersecurity threats."
"1650299","CAREER: When Energy Harvesting Meets ""Big Data"": Designing Smart Energy Harvesting Wireless Sensor Networks","ECCS","CCSS-Comms Circuits & Sens Sys","07/01/2016","08/09/2016","Jing Yang","PA","Pennsylvania State Univ University Park","Standard Grant","Lawrence Goldberg","02/28/2021","$481,386.00","","yangjing@psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","ENG","7564","1045, 153E, 9102, 9150","$0.00","In order to build a self-sustainable wireless sensor network, powering sensor nodes with energy harvesting devices becomes a natural and feasible solution, thanks to the recent progress on energy harvesting technology. However, wireless sensor networks usually need to collect and transmit vast amounts of data. Utilizing the random, non-uniform, and scarce harvested energy intelligently to meet the energy demand caused by ""big data"" is extremely challenging. The real-time latency requirement for data delivery makes the problem even more complicated. This project overcomes these challenges through strategically allocating the stochastic energy resources to collect and transmit the most important sensor data, providing a reliable pipeline for data collection, transmission and analysis in energy harvesting sensor networks. The project is expected to have direct impact on the design and wide deployment of energy harvesting wireless sensor networks, with applications in radio spectrum management, environment monitoring, healthcare, surveillance, disaster relief, etc. Furthermore, the proposed work has widespread potential applications far beyond energy harvesting wireless sensor networks, such as automatic residential energy consumption scheduling in smart grid, micro-grid planning with renewable energy inputs, information extraction from astronomical amounts of data collected from large-scale participatory sensing, etc. The proposed project integrates a research agenda with a strong educational component and will serve the following educational purposes: 1) the PI will use the proposed project to stimulate and maintain undergraduate students' interests in engineering via undergraduate research, senior project design and project based learning; 2) the requested funding will be used to train graduate students via curriculum development, proper research mentoring and industry collaboration; and 3) the proposed project will be used to attract potential engineering students through various outreach activities. <br/><br/><br/>The goal of this project is to construct a new paradigm of sensing and transmission schemes in data-intensive energy harvesting wireless sensor networks to intelligently utilize the random, non-uniform, and scarce harvested energy with analytically provable sensing, transmission and inference performance guarantees. Two different but closely coupled approaches are proposed to achieve this goal. One is an energy-driven approach and the other is a data-driven approach. For the energy-driven approach, the statistics of the energy harvesting process are exploited to construct online sensing and transmission schemes. Two main tasks addressed in this research thrust are objective-oriented cooperative sensing scheduling policies to cope with the non-uniform energy supply in large-scale sensor networks, and delay-constrained data transmission schemes to meet the real-time latency requirement. The data-driven approach utilizes the characteristics of the underlying sensing phenomena to adaptively and strategically allocate scarce energy resources for the collection and transmission of the most important sensor data. Two specific tasks in this research thrust include a Gaussian process based framework to systematically utilize the spatial-temporal correlations in sensing fields, and adaptive sensing strategies that exploit the structured sparsity of underlying sensing signals, both under the stochastic energy constraints at sensors. The project promises to build intelligent energy harvesting wireless sensor networks with superb sensing, transmission and inference performances on a solid analytical foundation. The delay-constrained information theoretic analysis for energy harvesting communications will integrate a new set of analytical tools from renewal theory with tools in information theory, and create synergies between them. The proposed data-driven adaptive sensing approach will develop synergies between stochastic queueing control and high-dimensional data analysis. The interdisciplinary nature of the research allows us to utilize techniques from stochastic queueing control, renewal theory, information theory, machine learning and is expected to advance the understanding of those areas."
"1622515","SCH: INT: Collaborative Research: Computer Guided Laparoscopy Training","IIS","Smart and Connected Health","08/01/2016","08/01/2016","Henry Fuchs","NC","University of North Carolina at Chapel Hill","Standard Grant","Wendy Nilsen","07/31/2021","$769,382.00","","fuchs@cs.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","8018","8018, 8062","$0.00","IIS-1622589 SCH: INT: Collaborative Research: Computer Guided Laparoscopy Training<br/><br/><br/>Laparoscopic surgery, when performed by a well-trained surgeon, is a remarkably effective procedure that minimizes complications associated with open incisions, blood loss and post-operative pain. It also reduces recovery time. However, the procedure is more challenging than conventional surgery due to restricted vision, hand-eye coordination problems, limited working space, and lack of tactile sensation. Therefore, effective training and guidance methods are needed to minimize the potential risks inherent in such procedures. The goal of this project is to develop and validate techniques for computer-guided laparoscopic surgical training in a simulated, non-patient based environment. A computer-aided surgical trainer (CAST) will physically guide trainees' instruments during surgical skills practice sessions by utilizing assistive force with augmented reality displays. Guided training will be validated through a pilot experimental study, in which the expertise of computer-guided trainees will be compared to that of instructor-guided trainees. Data such as the time it takes a trainee to execute a particular surgical task, how accurate he or she is, etc., will be collected to analyze task performance precisely and objectively. New scientific methods for motion trajectory planning and path following using assistive force and augmented reality techniques will result from this work. It is anticipated that computer-guided practice will speed up learning and reinforce appropriate techniques, ultimately, leading to better surgical outcomes and improved patient safety. The CAST system should serve as a sophisticated, yet still low-cost, training solution for fundamental medical skills training.  <br/><br/>The specific objectives are a) to refine and implement a memory- and time-efficient hybrid offline-online optimal path planner for computer-guided training of basic laparoscopic skills. In this task, collision-free trajectory planning methods (such as those used in robotics) will be generated by incorporating offline-online hybrid techniques with memory and computational time efficient path repository. Thus, basic laparoscopic tasks can be planned and guided automatically, using haptic force and augmented reality visualization; b) to design and implement an intelligent, adaptive guidance controller for surgical space navigation, where a fuzzy logic and machine learning-based methods will be developed that will take into account trainees' skill levels so that optimal amount of training assistance can be provided in mastering surgical tasks; c) to design and implement visual guidance techniques through augmented reality overlays that provide 'navigational' cues, supplementing force-based control of surgical instruments; and d) to validate guided training through a pilot study. In this task, trainees' performance using computer guidance methods will be compared, using statistical analysis, to that of unguided trainees. The principal investigators will aim to increase the participation of undergraduate students, and in particular of underrepresented groups, through collaboration with the well-established programs at both PIs'  institutions and through sponsorship of senior projects and independent study courses."
"1643544","I-Corps: Adaptable Virtual Reality Visual Spatial Training Technology","IIP","I-Corps","08/01/2016","07/19/2016","Marguerite Moore","MI","Northern Michigan University","Standard Grant","Steven Konsek","07/31/2017","$50,000.00","Jeff Nyquist","mmoore@nmu.edu","1401 Presque Isle Avenue","Marquette","MI","498555301","9062272300","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project is to provide brain training and assessment solutions for a variety of commercial uses through an adaptable virtual reality visual spatial training technology. These include treatments for mild traumatic brain injury, cognitive decline and vision decrements as well as improvements/training in driver safety, academic performance, and police and armed forces performance and safety.  The back-end program analytics will be able to learn from large numbers of users and potentially develop new solutions for novel customer problems. With future incorporation of machine learning, the technology could become highly personalized to each user's unique needs. These future markets represent a growing trend of brain fitness and brain performance products. The technology uniquely partners with the huge emerging markets of virtual reality, micro technology and quantifiable-self industries.<br/><br/>This I-Corps project addresses the need for a technology that continually challenges functional and cognitive brain patterns in a safe environment. This innovation provides a novel way to assess additional brain systems which are more sensitive to brain injury, thus offering more perceptive assessment of mild traumatic brain injury. The technology is a series of tasks controlled by an algorithm that demands the use of the occipital and parietal lobes as well as the attentional networks. The algorithm efficiently determines the user's current ability. It also provides a novel ability to switch from assessment to rehabilitation. New research indicates that the injured brain may not recover as quickly with the traditional ""wait and see"" method compared to a protocol that requires neuronal activity.  A pilot study showed improvements in balance, attention and reaction time in collegiate athletes. Additional research demonstrated improvements in low vision children over several training sessions. This technology challenges the brain to complete familiar tasks in a virtual reality world where the chance of re-injury during the recovery phase is eliminated."
"1632976","NRT-DESE: Network Biology: From Data to Information to Insights","DGE","NSF Research Traineeship (NRT)","09/15/2016","09/09/2016","Michelle Girvan","MD","University of Maryland College Park","Standard Grant","John Weishampel","08/31/2021","$2,959,866.00","Daniel Butts, William Fagan, Amitabh Varshney, Hector Corrada Bravo","girvan@umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","EHR","1997","9179, SMET","$0.00","An urgent issue facing today's researchers in the life sciences is coping with the data explosion resulting from the advent of powerful new technologies. More data does not yield better information without the interdisciplinary tools required for such a transformation. This National Science Foundation Research Traineeship (NRT) award to the University of Maryland, College Park will build an innovative, cross-disciplinary model for graduate education that addresses this challenge by preparing students to pursue a range of STEM careers at the nexus of the computer, physical, and life sciences. Trainees will learn to combine physics-style quantitative modeling with data processing, analysis, and visualization methods from computer science to gain deeper insights into the principles governing living systems. The project anticipates training approximately sixty (60) PhD students, including thirty-five (35) funded trainees, from the physical, computer, and life sciences.<br/><br/>Understanding how data-derived interaction patterns can give insights into complex biological phenomena is the research focus of this program. Through an innovative combination of cross-disciplinary training, collaborative research, and outreach activities, NRT trainees will become experts in the process of transforming raw biological data into useful information from which new biological insights can be inferred. Participants will receive training in four different areas of network analysis: quantitative metrics for biological networks; mechanistic models of biological networks; network statistics and machine learning for biological applications; and visualization techniques for large, complex, biological datasets. This training will provide the foundation for research in one or more of three application areas, covering a wide range of biological scales: biomolecular networks; neuronal networks; and ecological/behavioral networks. Research experiences, interdisciplinary coursework, peer-to-peer tutorials, and internships with partners will provide graduate students with the skills needed to communicate complex scientific ideas to diverse audiences in order to maximize impact. Outreach activities will extend the benefits of the program to undergraduates, middle/high school students, and to the public at large.<br/><br/>The NSF Research Traineeship (NRT) Program is designed to encourage the development and implementation of bold, new potentially transformative models for STEM graduate education training. The Traineeship Track is dedicated to effective training of STEM graduate students in high priority interdisciplinary research areas, through the comprehensive traineeship model that is innovative, evidence-based, and aligned with changing workforce and research needs."
"1637007","EAGER: Collaborative Research: mHABIT - Towards Building a Living Lab for mHealth Analytical and Behavioral Research using Internet of Things","SES","S&CC: Smart & Connected Commun","09/01/2016","08/30/2016","Beibei Li","PA","Carnegie-Mellon University","Standard Grant","Kenneth Land","08/31/2019","$115,375.00","","beibeili@andrew.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","SBE","033Y","042Z, 7916, 9150","$0.00","This project will create a living lab for mHealth Analytical and Behavioral Research using Internet of Things (mHABIT) to build a generalizable infrastructure for new analytical models and a Behavioral Experimentation Platform (BEP) to understand drivers of human health and wellness behavior and lifestyle changes through mobile and sensor technologies. Using an interdisciplinary approach, this project will enhance the understanding of human behavior and interactions with smart technologies in communities. The investigators will leverage test beds with domestic and international partners to advance knowledge towards developing new analytical and experimental methods drawn from econometrics, machine learning, behavioral economics and randomized field experiments. This project will contribute to a scalable prototype technology platform and lead to new solutions for improving user health and wellness, and healthcare efficiency. Overall, this project will integrate advanced Internet-of-Things infrastructures with an instrumented version of the physical world to improve quality of life, health and wellbeing, and sustainability of communities. The methods and infrastructures developed from this project can be easily deployed by healthcare providers to support data collection, analytics, solution and evaluation. The insights from this project will suggest policy implications towards the design of smart community through sustained usage of emerging technology. Moreover, broader impact includes dissemination of research to the public, underrepresented groups, and widespread deployment of the technology.<br/> <br/>The infrastructure developed from this project will collect and analyze large-scale and fine-grained user GPS trajectory data and RFID tracking data, linked with the EHR data, to examine what factors drive users' engagement with mHealth, their interactions with doctors inside and outside the clinical setting, and what changes they make in their personal lifestyle to improve their health outcomes. To evaluate the learning of user health behavior and decision making, the investigators plan to implement a pilot deployment of the BEP in the mHABIT living lab, by partnering with healthcare providers in the US and overseas, to design and implement novel mobile-enabled interventions and evaluate the effectiveness of mHealth technology from a causal perspective."
"1619818","Closing the Duality Gap:  Decomposition of High-Dimensional Nonconvex Optimization","DMS","COMPUTATIONAL MATHEMATICS","09/01/2016","07/12/2017","Mengdi Wang","NJ","Princeton University","Continuing Grant","Leland Jameson","08/31/2019","$200,000.00","","mengdiw@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","MPS","1271","9263","$0.00","In the modern computerized age, nonconvex optimization remains a critical computational challenge. Nonconvexity of an optimization problem implies a combinatorial structure, which often makes the computation problem fundamentally hard. Efficient computation tools with global approximation guarantees are in high demand. The principal investigator will study a class of nonconvex optimization problems that naturally arise from distributed intelligence systems, sparse estimation and data analysis.  The proposed research will contribute new computation tools for data analytics, statistic and machine learning, distributed and parallel computing, and multi-agent intelligence systems. The project will also develop two new courses for both undergraduate and graduate students at Princeton and also will involve undergraduate students in the research project via the Princeton undergraduate summer research program.<br/><br/>This research project aims to tackle an important class of nonconvex problems utilizing their geometric structure via a systematic dualization approach. The result is expected to advance the non-convex optimization theory as well as to provide algorithmic solutions to a large variety of distributed systems. Specifically, the principal investigator plans to study the non-convex duality for a class of non-convex optimization problems that admit a near-separable structure, with extensions to minimax problems and variational inequalities, and to develop computation tools that produce approximate global optimal solutions with complexity guarantees. In addition to the fundamental aspects, the principal investigator aims to investigate practical algorithms tailored to specific problems in high-dimensional structural estimation, sparse learning, and distributed optimization. The theoretical results and new methodology are expected to advance the theory of non-convex optimization as well as to provide algorithmic solutions to a variety of computational challenges."
"1536838","Real-Time Feedback-Enabled Simulation Modeling of Dynamic Construction Processes","CMMI","CIS-Civil Infrastructure Syst","01/01/2016","08/31/2015","Amir Behzadan","FL","The University of Central Florida Board of Trustees","Standard Grant","elise miller-hooks","11/30/2015","$275,000.00","","abehzadan@tamu.edu","4000 CNTRL FLORIDA BLVD","Orlando","FL","328168005","4078230387","ENG","1631","029E, 036E, 039E, 1057, CVIS","$0.00","According to the U.S. Census Bureau, in 2015, the U.S. construction industry will surpass $1 Trillion Dollars in spending. Construction and infrastructure projects consist of interconnected networks of people, equipment, and materials. Most often, finding optimal work strategies, and making timely operational decisions that lead to maximum productivity while minimizing project completion cost and time is not trivial. Unlike manufacturing and industrial systems, construction projects involve dynamic (constantly evolving) layouts, complex resource interactions, uncertainties in workflows and processes, and unforeseen conditions that can result in deviations from plans and unwanted delays. Figures show that only 30 percent of construction projects finish on time and within budget. Therefore, the accuracy and timeliness of operational-level decision-making in construction projects is of utmost importance. This award supports fundamental research to enhance construction decision-making accuracy by reducing uncertainties through the seamless integration of process-level data into decision-making. This will be achieved by building the theoretical foundation and significantly advancing the current state of construction simulation modeling through enabling real-time interaction with a simulation model as the real project evolves, and communicating the simulation output through a feedback loop to steer the events in the real project. Therefore, results from this research will benefit the U.S. economy and the society since it leads to better decision-making which results in reducing waste, rework, cost, time, and ensures safety. The multi-disciplinary nature of this project will help broaden participation of underrepresented and diverse student groups in integrated research and pedagogical activities, and positively impact engineering education.<br/><br/>The proposed knowledge-based simulation modeling framework enables process-level models to autonomously learn from and adapt to ever-changing and evolving construction systems. Process-level knowledge that serves as the input of such simulation models is obtained from ubiquitous sensory data that describe relationships, interactions, and uncertainty attributes of field processes, and enable the generation and maintenance of more accurate simulation models. In doing so, some scientific barriers are yet to be overcome to realize the full accreditation and application of this framework. The research team will design and test methods that draw from data mining, machine learning, forecasting, and control to fill the existing knowledge gaps in capturing and mining complex data and meta-data from equipment and human crew interactions. The resulting process-level knowledge will be rich enough to describe, model, analyze, and project the uncertainties of construction systems at any point in time and consequently help adjust resource allocations and operational scenarios on the job site."
"1602236","Real-Time Feedback-Enabled Simulation Modeling of Dynamic Construction Processes","CMMI","CIS-Civil Infrastructure Syst","01/01/2016","10/29/2015","Amir Behzadan","MO","Missouri State University","Standard Grant","Cynthia Chen","03/31/2018","$275,000.00","","abehzadan@tamu.edu","901 South National","Springfield","MO","658970027","4178365972","ENG","1631","029E, 036E, 039E, 1057, CVIS","$0.00","According to the U.S. Census Bureau, in 2015, the U.S. construction industry will surpass $1 Trillion Dollars in spending. Construction and infrastructure projects consist of interconnected networks of people, equipment, and materials. Most often, finding optimal work strategies, and making timely operational decisions that lead to maximum productivity while minimizing project completion cost and time is not trivial. Unlike manufacturing and industrial systems, construction projects involve dynamic (constantly evolving) layouts, complex resource interactions, uncertainties in workflows and processes, and unforeseen conditions that can result in deviations from plans and unwanted delays. Figures show that only 30 percent of construction projects finish on time and within budget. Therefore, the accuracy and timeliness of operational-level decision-making in construction projects is of utmost importance. This award supports fundamental research to enhance construction decision-making accuracy by reducing uncertainties through the seamless integration of process-level data into decision-making. This will be achieved by building the theoretical foundation and significantly advancing the current state of construction simulation modeling through enabling real-time interaction with a simulation model as the real project evolves, and communicating the simulation output through a feedback loop to steer the events in the real project. Therefore, results from this research will benefit the U.S. economy and the society since it leads to better decision-making which results in reducing waste, rework, cost, time, and ensures safety. The multi-disciplinary nature of this project will help broaden participation of underrepresented and diverse student groups in integrated research and pedagogical activities, and positively impact engineering education.<br/><br/>The knowledge-based simulation modeling framework in this project enables process-level models to autonomously learn from and adapt to ever-changing and evolving construction systems. Process-level knowledge that serves as the input of such simulation models is obtained from ubiquitous sensory data that describe relationships, interactions, and uncertainty attributes of field processes, and enable the generation and maintenance of more accurate simulation models. In doing so, some scientific barriers are yet to be overcome to realize the full accreditation and application of this framework. The research team will design and test methods that draw from data mining, machine learning, forecasting, and control to fill the existing knowledge gaps in capturing and mining complex data and meta-data from equipment and human crew interactions. The resulting process-level knowledge will be rich enough to describe, model, analyze, and project the uncertainties of construction systems at any point in time and consequently help adjust resource allocations and operational scenarios on the job site."
"1546838","TOOLS-PGR: Computational Infrastructure to Enable High-throughput, High-quality Annotations of Compartmentalized Metabolic Networks for Plant Genomes","IOS","Plant Genome Research Project, Cross-BIO Activities","08/15/2016","08/30/2018","Seung Rhee","DC","Carnegie Institution of Washington","Continuing Grant","Gerald Schoenknecht","07/31/2021","$2,193,335.00","Peter Karp","srhee@carnegiescience.edu","1530 P ST NW","WASHINGTON","DC","200051910","2023876400","BIO","1329, 7275","7577, 9109, 9178, 9251, BIOT","$0.00","It has been estimated that agricultural productivity needs to be increased to meet the demands imposed by population growth and climate change. Changing the metabolism of crop species is one way to improve productivity. Thus, increasing our knowledge of plant metabolism can significantly accelerate crop improvement efforts. New DNA sequencing technologies have produced an enormous amount of data. However, it has been difficult to obtain useful metabolic information from those DNA sequences. The plant research community needs efficient tools that can extract information related to metabolism from those DNA sequences. This project will produce the tools and datasets that will be used to systematically characterize the components of metabolism: enzymes, transporters, and pathways. These tools will make it easy to compare the metabolic genetic potential of two or more species, and enable the identification of targets for crop improvement. This project will also offer training opportunities in biochemistry and computer sciences to postdoctoral associates and students. In addition, workshops will be offered at professional meetings to train members of the plant research community on the use of the tools developed by the project. Finally, the tools developed by this project will be made available to the scientific community through a web portal.<br/><br/>Accurate and rapid annotation of metabolic enzymes and transporters from sequenced genomes and their metabolic network reconstructions are essential resources for interpreting the results of 'omics' data systematically and enabling the generation of new hypotheses. This proposal aims to meet these needs by developing a computational pipeline to enable rapid and accurate prediction of genome-scale metabolic complements of any sequenced plant based on the large pool of experimentally characterized information. First, the team will improve the accuracy of enzyme function prediction by adding new classifiers and features to a redesigned machine-learning framework. Additions of new classifiers such as phylogenomics-based function prediction and new features such as conserved protein domain architecture and conserved residues would reduce false positive predictions of proteins that share high sequence similarity with known enzymes but catalyze distinct functions. The team will also develop a new learning based algorithm to predict subcellular locations of enzymes and reactions for any plant species. The algorithm will combine the localization likelihoods of enzymes derived from the experimentally determined localization information of their orthologs and the localization information of the neighboring reactions in the metabolic network to propagate the localization likelihoods among all the reactions in the network. Another new algorithm will be developed to predict transporters and the substrates of transporters. All data generated from this project will be integrated into the PMN databases. In addition, a pipeline will be packaged to enable users to submit their genome sequences online and obtain the prediction results through a web server. Finally, innovative, integrated views of metabolic pathways with gene co-expression, transporters and subcellular compartments will be developed."
"1553288","CAREER:  Structure and Analysis of Low Degree Polynomials","CCF","Algorithmic Foundations","04/01/2016","03/18/2020","Daniel Kane","CA","University of California-San Diego","Continuing Grant","Joseph Maurice Rojas","03/31/2021","$500,000.00","","dakane@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7796","1045, 7926, 7927","$0.00","The primary goal of complexity theory is to design fast algorithms for solving various computational problems and to understand when such algorithms will not exist. In order to do this, a number of sophisticated mathematical tools are often required. One of the more prominent tools used in this way are polynomials. Polynomials are both varied enough to express or approximate a number of objects of interest, and yet are also simple enough that properties of them can be easily understood. Thus, relating properties of the objects in question to properties of polynomials is a well-known technique that shows up in many areas of computer science.<br/><br/>Unfortunately, our understanding of these fundamental objects is far from complete. In fact, there are a number of problems for which our lack of understanding can be seen as a bottleneck for proving further results about problems of interest. This project will focus on attempts to remedy this gap. In particular, the PI plans to follow up on several recent advances in understanding of this area and to attempt to leverage any new results to gain insight into other important questions within computer science. In addition to leading to new algorithms of practical import, the research promises to have potential impacts in other fields such as probability theory and<br/>algebraic geometry.<br/><br/>More specifically, the PI intends to improve upon existing tools for understanding low degree polynomials in many variables. Of particular interest would be work relating to results on the distribution of the values of such a polynomial on random inputs, with particular focus on recent structural results that allow one to decompose arbitrary polynomials in terms of better behaved ones. Having attained such results, the project will continue by making use of these improvements in order to make progress on other important problems in theoretical computer science. In particular, there are a number of specific problems in the areas of explicit pseudorandom generators, machine learning, and circuit complexity for which such technical improvements show promise for providing substantial new results.<br/><br/>In addition to the research component of this project, the PI intends to help provide new educational opportunities particularly with regard to learning mathematical problem solving skills which are a key component of mathematics and computer science research."
"1622589","SCH: INT: Collaborative Research: Computer Guided Laparoscopy Training","IIS","Smart and Connected Health","08/01/2016","08/01/2016","Jerzy Rozenblit","AZ","University of Arizona","Standard Grant","Wendy Nilsen","03/31/2021","$1,118,923.00","Allan Hamilton","jr@ece.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","CSE","8018","8018, 8062","$0.00","Laparoscopic surgery, when performed by a well-trained surgeon, is a remarkably effective procedure that minimizes complications associated with open incisions, blood loss and post-operative pain. It also reduces recovery time. However, the procedure is more challenging than conventional surgery due to restricted vision, hand-eye coordination problems, limited working space, and lack of tactile sensation. Therefore, effective training and guidance methods are needed to minimize the potential risks inherent in such procedures. The goal of this project is to develop and validate techniques for computer-guided laparoscopic surgical training in a simulated, non-patient based environment. A computer-aided surgical trainer (CAST) will physically guide trainees' instruments during surgical skills practice sessions by utilizing assistive force with augmented reality displays. Guided training will be validated through a pilot experimental study, in which the expertise of computer-guided trainees will be compared to that of instructor-guided trainees. Data such as the time it takes a trainee to execute a particular surgical task, how accurate he or she is, etc., will be collected to analyze task performance precisely and objectively. New scientific methods for motion trajectory planning and path following using assistive force and augmented reality techniques will result from this work. It is anticipated that computer-guided practice will speed up learning and reinforce appropriate techniques, ultimately, leading to better surgical outcomes and improved patient safety. The CAST system should serve as a sophisticated, yet still low-cost, training solution for fundamental medical skills training.  <br/><br/>The specific objectives are a) to refine and implement a memory- and time-efficient hybrid offline-online optimal path planner for computer-guided training of basic laparoscopic skills. In this task, collision-free trajectory planning methods (such as those used in robotics) will be generated by incorporating offline-online hybrid techniques with memory and computational time efficient path repository. Thus, basic laparoscopic tasks can be planned and guided automatically, using haptic force and augmented reality visualization; b) to design and implement an intelligent, adaptive guidance controller for surgical space navigation, where a fuzzy logic and machine learning-based methods will be developed that will take into account trainees' skill levels so that optimal amount of training assistance can be provided in mastering surgical tasks; c) to design and implement visual guidance techniques through augmented reality overlays that provide 'navigational' cues, supplementing force-based control of surgical instruments; and d) to validate guided training through a pilot study. In this task, trainees' performance using computer guidance methods will be compared, using statistical analysis, to that of unguided trainees. The principal investigators will aim to increase the participation of undergraduate students, and in particular of underrepresented groups, through collaboration with the well-established programs at both PIs'  institutions and through sponsorship of senior projects and independent study courses."
"1657976","Workshop on  Architecture and Software for Emerging Applications (WASEA)","CNS","CSR-Computer Systems Research","11/01/2016","05/16/2019","Lawrence Rauchwerger","TX","Texas A&M Engineering Experiment Station","Standard Grant","Matt Mutka","10/31/2019","$49,900.00","Nancy Amato","rwerger@illinois.edu","400 Harvey Mitchell Pkwy S","College Station","TX","778454645","9798626777","CSE","7354","7556","$0.00","High-valued domain applications in areas such as medicine,<br/>biology, physics, engineering, and social phenomena demand<br/>both fast innovation and high execution speed and require<br/>productive development environments for domain experts who<br/>may not be computer science experts. This workshop brings together leading researchers in architecture,<br/>compilers and programming languages, and domain experts to discuss<br/>and debate potential approaches to accelerating progress in such<br/>high-valued domains with an emphasis on developing strategies for<br/>exploiting machine learning, including strategies for accelerating<br/>learning algorithms through parallelism.  The goal is to stimulate<br/>an in-depth discussion of the potential benefits of joint architecture<br/>and compiler approaches.  The workshop will promote broadening<br/>participation by including speakers from groups underrepresented<br/>in computing and early career researchers.<br/><br/>The workshop will produce a report providing recommendations on:<br/>joint compiler/language and architecture approaches, compiler/language<br/>support enabling more aggressive hardware capabilities, architecture<br/>support enabling more effective compilers, and applications whose<br/>development process could benefit by these advances, The report<br/>will identify research opportunities in the interaction between<br/>developers, and architecture and language/compiler researchers to<br/>enable productive domain application development and highly efficient<br/>and scalable implementation on heterogeneous computing systems.  The<br/>report will outline promising approaches and the research required for<br/>these approaches to become usable by the domain application developers."
"1614576","III: Small: Discovering and Characterizing Implicit Links in Graph Data","IIS","Info Integration & Informatics","10/01/2016","04/24/2017","Huan Liu","AZ","Arizona State University","Standard Grant","Wei Ding","09/30/2020","$503,123.00","","hliu@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","7364","7364, 7923, 9251","$0.00","Graph data represent a variety of phenomena, ranging from traffic data, biological networks to social networks. Social networks enable people to participate in a variety of online activities. A typical social networking site allows users to explicitly specify ""positive"" links to other users with ""labels"" such as family, friendships, Twitter follower, etc. Little attention is paid to ""implicit  links"" which are unspecified links among social users that may indicate competition, distrust, dislike, or antagonism. This project studies fundamental data analytics issues of understanding and identifying implicit links in graph data. The project explores new computational techniques to discover actionable and insightful patterns in large-scale graph data (e.g., social networks) and enables a large-scale study of social media user behaviors in computational social science. The research insights gained through this project are expected to lead to better design of supervised and unsupervised learning algorithms on networks with both positive and implicit links. The study of implicit social network links will be applicable to the design of new recommender systems, leading to improved services and user experience. The proposed research will involve graduate and undergraduate students in pursuing their theses or projects. Research topics and findings will be integrated in undergraduate and graduate education. <br/><br/>The proposed research addresses issues of link analysis in large-scale, incomplete, and noisy networks, such as underlying social media. As implicit links between users are typically invisible on social networking sites, discovering them entails novel challenges. The research team proposes to evaluate the value of implicit links for relationship discovery and better social network understanding. The project includes development of algorithms for graph data analytics, machine learning for positive and unlabeled link discovery in heterogeneous cross-media data, and computationally efficient implicit link predictions. The team proposes to apply the research insights to improving recommendation systems design, classification of implicit user relationships, and social user clustering. The research team plans to share results of this project, including benchmark data with the research community to promote the research on implicit link discovery in social networks via the project site (http://www.public.asu.edu/~huanliu/projects/ImplicitLinks/)."
"1637359","La Serena School for Data Science:  Applied Tools for Astroinformatics, Biomedical Informatics, and Other Data-driven Sciences","AST","OFFICE OF MULTIDISCIPLINARY AC, , International Research Collab","10/01/2016","09/06/2019","Charles Mattias Mountain","DC","Association of Universities for Research in Astronomy, Inc.","Continuing Grant","Edward Ajhar","09/30/2021","$456,642.00","R. Chris Smith, David R. Silva, Adam Bolton","Matt.Mountain.NSF@aura-astronomy.org","1331 Pennsylvania Ave. NW","Washington","DC","200040000","2024832101","MPS","1253, 1798, 7298","5974, 7556, 9178","$0.00","The volume and complexity of scientific data continue to grow rapidly.  New astronomical surveys will result in datasets much larger than those previously available.  This poses new challenges for data-driven discovery.  It also enables new opportunities for interdisciplinary research.  Astronomy is at the forefront in creating ""pi-shaped"" data scientists.  These people have both science expertise and cross-disciplinary knowledge.  Many other fields also see the need for such pi-shaped scientists.  The La Serena School for Data Science 2017-2021 will introduce undergraduates and graduates to the tools and techniques they need.  They will be ready just in time for the large scientific projects of 2020 and beyond.<br/><br/>The existence of enormous (peta-scale) datasets coming from wide-area and time-domain astronomical surveys poses significant concerns for data-driven discovery, access, and analysis, even while enabling new opportunities for interdisciplinary research in applied mathematics, statistics, machine learning, and related topics.  Astronomy provides a sandbox where scientists can come together from diverse fields to address common challenges within the ""Big Data"" paradigm.  The one-week La Serena School, each year from 2017 to 2021, will train the next generation of scientists in astronomy, biomedicine, computer science, statistics, and mathematics, from the US, Chile and other countries, in the tools and techniques of massive data.  With the experience gained from this school, participants will be prepared to lead the exploration of the extremely large datasets of tomorrow.  The pilot schools, held from 2013 to 2015, were very successful, both demonstrating the demand, and building a strong foundation for collaboration between the NSF and Chilean institutions.  This award will bring 15-18 U.S. students, roughly half of the class each year, to Chile for a combination of learning about massive data and working side-by-side with their Latin-American and other international colleagues.  Support for the schools is shared with a diverse set of Chilean institutions.  NSF funding is provided by the Office of International Science and Engineering and the Directorate for Mathematical and Physical Sciences."
"1637436","Workshop on the Algorithmic, Mathematical, and Statistical Foundations of Data Science","DMS","STATISTICS, Special Projects - CCF","04/01/2016","03/23/2016","Xiaoming Huo","GA","Georgia Tech Research Corporation","Standard Grant","Nandini Kannan","03/31/2017","$99,998.00","Petros Drineas","xiaoming@isye.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","MPS","1269, 2878","7556, 7796, 8084","$0.00","A workshop on the Algorithmic, Mathematical, and Statistical Foundations of Data Science will be held April 28-30, 2016 in Arlington, VA.  The event will bring together leading researchers in computer science, mathematics, and statistics to address foundational issues related to data science.  The objectives of the workshop are three-fold: (i) identify fundamental areas in the emerging discipline of Data Science where collaboration between computer scientists, mathematicians, and statisticians is necessary to achieve significant progress; (ii) Assess how collaboration between computer scientists, mathematicians, and statisticians could potentially contribute to workforce development by advancing and transforming the Data Science research training of Ph.D. students and post-docs; and (iii) Suggest different infrastructure modalities that could significantly promote and advance such collaborations.  The main deliverable of the workshop is a white paper that will serve as a guideline for professional societies and funding agencies.  <br/><br/>The rapid emergence of the Big Data phenomenon presents both opportunities and challenges.   While massive data may allow the generation of models and the design of algorithms that have improved inferential power, such models and algorithms may be less successful on modest-sized data sets. The challenge for researchers is to develop theoretical principles that will allow the scaling of inference and learning to massive-scale datasets, and algorithms that control errors even in the presence of heterogeneity in the data generation and data sampling processes.  These challenges will require collaborations between researchers representing theoretical computer science, mathematics, statistics, machine learning and data mining, and high performance computing. This award supports a workshop that brings together leaders from these communities of researchers to discuss the challenges and opportunities for collaborative work in this developing field."
"1551688","Advancing a Situated Neuroscience of Emotion","BCS","Social Psychology","05/01/2016","04/14/2016","Kristen Lindquist","NC","University of North Carolina at Chapel Hill","Standard Grant","Steven J. Breckler","04/30/2020","$418,808.00","Chang Nam","kristen.lindquist@unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","SBE","1332","1332","$0.00","How can we know how other people are really feeling? If emotions are intensely experienced it may be easy to tell, but some people are good at hiding their emotions, or they may be unaware of or unable to report what they are feeling. Fortunately, technological advances in measuring brain activity and emotion theory can help us to have a more objective understanding of people's emotional states. Some theories of emotion suggest that different emotional states, such as fear, anger, or happiness, arise because of specific, dedicated neural circuitry that responds in the same way any time that emotion is experienced. For example, the circuitry for happiness is the same whether it arises because you get a compliment or you learned of a promotion. In contrast, the proposed research is built on an innovative 'situated neuroscience' model of emotions. This model hypothesizes that emotions correspond to variable neural patterns. These variable patterns are hypothesized to be determined by the emotion category experienced, such as fear, and partly by the context in which the emotion is experienced. The context includes components from culture and the situation itself. Dr. Kristen Lindquist, at the University of North Carolina at Chapel Hill, and colleagues proposed three experiments to examine this model. Her research will use measures of the location of brain events (functional magnetic resonance imaging; fMRI) and the timing of brain events (electroencephalography; EEG). The integration of emotion theory and technology in this way could help to improve communication about and understanding of emotions, help us to predict when emotions influence our decisions and performance, and may have benefits for the way we interact with others.<br/> <br/>The proposed research integrates emotion theory with technological advances in measuring brain activity to ""read"" emotional feelings from brain states. The aim is to develop a situated neuroscience of emotion that will inform our understanding of the basis of emotions and their conscious experience. Three experiments are proposed that examine the role of situational and cultural contexts in emotion and brain activity. The first experiment examines fear and anger and their interaction with situational and cultural contexts on brain activity, using functional magnetic resonance (fMRI). The situation is manipulated to be either social or non-social, and culture stems from enrolling natives of the US and China who know the emotion-based norms and values of their respective culture.  The second experiment extends this investigation to the temporal dynamics of emotional brain activity using electroencephalography (EEG). The third study uses machine learning on data obtained from the prior experiments to determine which variables best predict brain activity during emotion, in the different contexts. Understanding the situated nature of emotion is crucial to an understanding of how neural circuits map on to subjective mental states. These findings may improve our understanding of emotional experience in ourselves and others, and enhance our communication, well-being, and diplomatic relations. It may also inform applied advancements in the areas of mood-related illnesses and brain-computer interface."
"1646654","EAGER: Causal Bayesian Network-Based Discrimination Discovery and Prevention","IIS","Info Integration & Informatics","09/01/2016","08/24/2016","Xintao Wu","AR","University of Arkansas","Standard Grant","Wei Ding","08/31/2019","$200,000.00","Lu Zhang","xintaowu@uark.edu","1125 W. Maple Street","Fayetteville","AR","727013124","4795753845","CSE","7364","7364, 7916, 9150","$0.00","Various business models have been built around the collection and use of customer data to make important decisions like employment, credit, and insurance. There are increasing worries of discrimination as data analytics technologies could be used to unfairly treat individuals based on their demographic information such as gender, age, marital status, race, religion or belief, membership in a national minority, disability, or illness. It is imperative to develop predictive decision models, such that the data that goes into them and the decisions made with their assistance are not subject to discrimination. This EAGER research designs practical techniques to accurately detect and remove discrimination from the datasets used to build decision models. A primary outcome of this research is a unifying framework and a prototype system for discrimination discovery and removal. This system can help individuals from disadvantaged groups determine whether they are fairly treated and help decision makers from organizations ensure their predictive decision models are discrimination free. <br/><br/>Existing discrimination discovery approaches are mainly based on correlation or association and cannot accurately discover the true discrimination. In addition, each of them targets on one or two types of discrimination only. This research categorizes discrimination based on whether discrimination is across the whole system, occurs in one subsystem, or happens to one individual, and whether discrimination is a direct effect or an indirect effect on the decision. This research then develops a unifying causal Bayesian network based framework that takes into consideration the distinctions between discrimination and general causalities and models both direct discrimination and indirect discrimination as causal effects via different paths between protected attributes and the decision. It can accurately capture and measure various types of discrimination at system, group, and individual levels. The research then develops novel discrimination discovery and prevention models and algorithms. The research also builds a testing framework for simulating different types of discrimination and evaluating the approaches based on various metrics, and integrates the discrimination discovery and prevention algorithms into an open source data mining and machine learning software system."
"1607189","US-Israel Collaboration: Collaborative Research: New Tools for Extracting Neuronal Phenotypes from a Volumetric Set of Cerebral Cortex Images","IIS","CRCNS-Computation Neuroscience, Robust Intelligence, IntgStrat Undst Neurl&Cogn Sys","09/01/2016","08/25/2016","Nir Shavit","MA","Massachusetts Institute of Technology","Standard Grant","Sylvia Spengler","08/31/2021","$332,588.00","","shanir@csail.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7327, 7495, 8624","5905, 7327, 7495, 8089, 8091","$0.00","A major limitation in connectomics is that there are few tools to transform connectomic images into a minable database. The research aim of this project is to develop a suite of tools that extract essential structural parameters from the brain's physical structure that was imaged at very high (nanometer scale) resolution. The PIs will determine, by using automated methods, the sizes and shapes of neurons, synapses and their connectivity patterns. Using their tools, the PIs will analyze this detailed and varied dataset to find the key patterns within it. It is their belief that such automated methods are a requirement to comprehend the regularities and rules that govern the formation of neural circuits in the cerebral cortex, which to date have only been studied on very small sample spaces. The cerebral cortex remains perhaps the least understood aspect of mammalian biology. No studyof this magnitude of the neuronal phenotype space has ever been conducted: the dataset will contain hundreds of thousands of somata and a billion synapses, allowing the PIs to search  for patterns that could only be guessed at with the tools used in prior research. Knowing what overarching organizational principles exist in a cerebral cortical network is crucial for understanding how brains work normally and how they may go awry in disease. Moreover, connectomic studies are beginning in a large number of different laboratories throughout the world focused on a wide range of species and parts of the brain. These tools should have direct applicability to many of these endeavors.<br/><br/>The PIs are a consortium of four laboratories with complementary areas of expertise in computer science (Shavit), systems biology (Alon), image processing (Pfister) and neurobiology (Lichtman). Together they are building a stacked set of methods that extract important parameters from connectomic images. These methods include neuron geometry extraction, network structure, motif detection, and archetypical pattern analysis. These approaches are based on two software platforms:the MapRecurse platform for generating connectome graphs and the Pareto Inference Engine for mining patterns within such graphs. The PIs will test these techniques on an a volume of mammalian cerebral cortex containing tens of thousands of cells and a billion synapses, with the aim of extracting the properties of neural circuits that would be difficult or impossible to obtain any other way. The work in this proposal will have significant impact on neuroscience. It speaks directly to the central goals of the White House BRAIN Initiative. It will provide neuroscientists with anumber of powerful and novel tools to understand the cells and circuits that underlie brain function. It should also be influential in developing approaches in machine learning and neuromorphic computing.  <br/><br/>A companion project is being funded by the US-Israel Binational Science Foundation (BSF)."
"1551731","CAREER: Understanding the Fundamental Principles Driving Household Energy and Resource Consumption for Smart, Sustainable, and Resilient Communities","CMMI","CAREER: FACULTY EARLY CAR DEV, GOALI-Grnt Opp Acad Lia wIndus, CIS-Civil Infrastructure Syst","08/15/2016","07/07/2020","Sybil Derrible","IL","University of Illinois at Chicago","Standard Grant","Yueyue Fan","07/31/2021","$548,445.00","","derrible@uic.edu","809 S. Marshfield Avenue","Chicago","IL","606124305","3129962862","ENG","1045, 1504, 1631","019Z, 029E, 036E, 039E, 1045, 116E, 1631, 9102, 9178, 9231, 9251","$0.00","Ultimately, the amount of energy and resources that Americans consume depends heavily on where as well as how they live. Thus residents of northern states tend to consume more energy at home than residents of southern states, because heating requires more energy than cooling. Similarly, people living in suburbs tend to consume more energy for transportation than people living in city centers because longer commutes consume more energy. Similar observations can be made for all types of energy and resources, from electricity and transportation to natural gas, water, and even food. This Faculty Early Career Development (CAREER) grant will help to discover the fundamental principles that govern how much ""where"" and ""how"" people live matters for energy and resource consumption. The contributions from this research will directly assist in the development of effective policies for more sustainable communities that consume less energy and resources. Moreover, a better understanding of energy flows in cities will provide planners and engineers with information that will enable them to design smarter and more resilient infrastructure systems that are decentralized and distributed. In addition, this grant will have a significant impact for the broader public as it includes the development of a smartphone application to enable anyone to calculate their daily carbon footprint and track their performance over time.<br/><br/>This research places itself at the nexus of urban metabolism and complexity theory. Urban metabolism is the study of flows of material and energy in cities. The main hypothesis of this research is that urban metabolism follows distinct mathematical laws at the community scale that can be captured using elements of complexity theory. Various mathematical laws (e.g., power law, lognormal distribution, uniform distribution) will be applied using agent-based modeling techniques to generate a theoretical space that will include every possible community profile in terms of energy and resource consumption. These laws will then be tested for individual communities using freely available data from municipal open data portals. Additionally, this research will utilize elements of machine learning to classify communities based on their energy and resource consumption patterns, and network science to better understand the inter- and co-dependence between the usage of electricity, water, natural gas, and transportation infrastructure. The research will therefore offer a significant contribution to help design a society that is smarter and more resilient while being more sustainable by requiring less energy and fewer resources to thrive."
"1618689","CIF: Small: Index Coding and Matrix Factorizations","CCF","Comm & Information Foundations","07/01/2016","06/27/2016","Georgios-Alex Dimakis","TX","University of Texas at Austin","Standard Grant","Phillip Regalia","06/30/2019","$449,094.00","","dimakis@austin.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7797","7923, 7935","$0.00","Many communication systems, content delivery systems, and machine learning algorithms rely on algorithms that use matrix factorizations with some structure constraints. A specific type of matrix factorization of significant theoretical and practical interest is called ""Index Coding"". The research in this project is centered around studying the foundations of index coding and the development of algorithms for structured matrix factorizations. Beyond the development of theoretical foundations, this research is expected to lead to better algorithms for caching in wireless networks and for designing distributed storage codes. The project also includes plans for curriculum development, student training, and tutorial presentations.<br/><br/>The specific focus of this research program consists of three thrusts: 1) Developing novel algorithms and bounds for index coding, 2) Understanding the connections of index coding with distributed storage codes other factorization problems and 3) Using index coding to obtain bounds and achievable schemes for information flow problems. Preliminary work indicates that this is possible for multiple unicasts and more possibly more generally. Connections with semidefinite programming relaxations and graph theory are also considered in this project."
"1546206","BIGDATA: Collaborative Research: F: From Data Geometries to Information Networks","IIS","Big Data Science &Engineering","01/01/2016","09/15/2015","Leonidas Guibas","CA","Stanford University","Standard Grant","Sylvia Spengler","12/31/2019","$500,000.00","Emmanuel Candes","guibas@cs.stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","8083","7433, 8083","$0.00","Big Data often results from multiple sources, giving collections that contain multiple, often partial, ""views"" of the same object, space, or phenomenon from various observers.  Extracting information robustly from such data sets calls for a joint analysis of a large collection of data sets.  The project is developing a novel geometric framework for modeling, structure detection, and information extraction from a collection of large related data sets, with an emphasis on the relationships between data.  While this approach clearly applies to data with a clear geometric character (e.g., objects in images), the work is also applied to datasets as diverse as computer networks (identifying common structure in subnets) and Massive Open Online Course homework data (automatically carrying grader annotations to similar problems in other students' homeworks).<br/><br/>The novel framework is based on the construction of maps between the objects under considerations (point clouds, graphs, images, etc...), and on the analysis of the networks of maps that result as a way of extracting information, generating latent models for the data, and transporting or inferring functional / semantic information. These tasks define a new field of map processing between data sets and require tool sets with new ideas from functional analysis, non-convex optimization, and homological algebra in mathematics, and geometric algorithms, machine learning, optimization, and approximation algorithms in computer science.  Sophisticated algorithmic techniques for attacking the large-scale non-linear optimization problems that emerge within the framework will also be investigated."
"1614661","Collaborative Research: Efficient mathematical and computational framework for biological 3D image data retrieval","DMS","ADVANCES IN BIO INFORMATICS, MATHEMATICAL BIOLOGY, MSPA-INTERDISCIPLINARY","08/15/2016","08/19/2016","Atilla Sit","KY","Eastern Kentucky University","Standard Grant","Junping Wang","07/31/2021","$143,420.00","","Atilla.Sit@eku.edu","Division of Sponsored Programs","Richmond","KY","404753100","8596223636","MPS","1165, 7334, 7454","8007, 9150","$0.00","Advances in imaging technology have led to a proliferation of three dimensional biological and medical image data from many imaging modalities, which include magnetic resonance imaging and computed tomography scans in medical imaging, neuroimaging using light-field microscopy in neuroscience, tomography for imaging cells and tissues, and cryo-electron microscopy for biomolecular structures. Images of three dimensional, volumetric, structures provide indispensable spatial information about organs, tissues, and molecules that cannot be captured using two dimensions. The development of tools for efficient and effective analysis of such volumetric data sets is, therefore, urgently required. This project will develop generally applicable mathematical and computational frameworks to effectively and accurately represent, compare, and retrieve biological and medical data in three dimensions. The methods to be developed will provide a general foundation for the analysis of volumetric images obtained using multiple imaging modalities and for multiple data types, not only from the biological domain. For example, the techniques have broader impact in areas such as human face recognition, analysis of geographical and climate data, and computer-aided design. This project, therefore, contributes to general promotion of the progress of science and technology in many domains in which imaging analysis is crucial and is of significant societal impact. <br/><br/>In this project, two complementary and synergistic methods will be developed and integrated. The first method to be developed is a mathematical moment-based approach that provides a compact representation of volumetric data and is very suitable for localized three dimensional image data comparison. A two dimensional image comparison method that is based on a moment-based invariant will be expanded to handle volumetric data. The second method is a machine learning approach that will be powerful in classifying volumetric data. These two approaches will be integrated to take advantage of both methods and validated using three dimensional protein structural data. Analyzing global and local similarities between protein shapes is critical for understanding protein function but challenging because proteins with substantially different shapes may perform the same function. Further, proteins are appropriate for this validation step not only because many structures are available in well-established public databases but also because they lack intrinsic orientation, unlike previously studied datasets of man-made objects such as cars, cups, and tables. As the proposed methods are defined for a general voxel representation of a given volume, they will be generally applicable for any data set yielding a voxel representation, including biomedical data collected using electron microscopy, magnetic resonance imaging and computed tomography. Along side the scientific impact of the project, it also leverages efforts in the interdisciplinary computational life sciences and engineering departments at Purdue University and Eastern Kentucky University by recruiting and training students through interdisciplinary coursework and direct involvement with the project."
"1618364","SHF: Small: Scalable Spectral Sparsification of Graph Laplacians and Integrated Circuits","CCF","Software & Hardware Foundation","06/15/2016","06/10/2016","Zhuo Feng","MI","Michigan Technological University","Standard Grant","Sankar Basu","02/29/2020","$450,000.00","","zfeng12@stevens.edu","1400 Townsend Drive","Houghton","MI","499311295","9064871885","CSE","7798","7923, 7945","$0.00","This research is motivated by investigations on scalable methods for design simplifications of nanoscale integrated circuits (ICs). This is to be achieved by extending the associated spectral graph sparsification framework to handle Laplacian-like matrices derived from general nonlinear IC modeling and simulation problems. The results from this research may prove to be key to the development of highly scalable computer-aided design algorithms for modeling, simulation, design, optimization, as well as verification of future nanoscale ICs that can easily involve multi-billions of circuit components. The algorithms and methodologies developed will be disseminated to leading technology companies that may include semiconductor and Electronic Design Automation companies as well as social and network companies, for potential industrial deployments. <br/><br/><br/>Spectral graph sparsification aims to find an ultra-sparse subgraph (a.k.a. sparsifier) such that its Laplacian can well approximate the original one in terms of its eigenvalues and eigenvectors. Since spectrally similar subgraphs can approximately preserve the distances, much faster numerical and graph-based algorithms can be developed based on these ""spectrally"" sparsified networks. A nearly-linear complexity spectral graph sparsification algorithm is to be developed based on a spectral perturbation approach. The proposed method is highly scalable and thus can be immediately leveraged for the development of nearly-linear time sparse matrix solvers and spectral graph (data) partitioning (clustering) algorithms for large real-world graph problems in general. The results of the research may also influence a broad range of computer science and engineering problems related to complex system/network modeling, numerical linear algebra, optimization, machine learning, computational fluid dynamics, transportation and social networks, etc."
"1639150","INSPIRE:  A Data-Driven Approach toward Exploring Natural and Anthropogenic Methane Emissions in Regions of Shale Gas Development","IIS","Information Technology Researc, AISL, Geobiology & Low-Temp Geochem, Info Integration & Informatics, SURFACE EARTH PROCESS SECTION, EnvS-Environmtl Sustainability, INSPIRE","08/01/2016","08/04/2016","Susan Brantley","PA","Pennsylvania State Univ University Park","Standard Grant","Maria Zemankova","07/31/2021","$1,000,000.00","Zhenhui Li","brantley@essc.psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","1640, 7259, 7295, 7364, 7570, 7643, 8078","043Z, 7364, 8653","$0.00","This INSPIRE project addresses the issue of high volume hydraulic fracturing, also called fracking, and its effects on ground water resources. Fracking allows drillers to extract natural gas from shale deep within the earth. Methane gas sometimes escapes from shale gas wells and can contaminate water resources or leak into the atmosphere where it contributes to greenhouse gas emissions. Monitoring for these potential leaks is difficult because methane is also released into aquifers naturally, and because monitoring is time- and resource-intensive. Such subsurface leakage may also be relatively rare. This project seeks to improve overall understanding of the impacts of natural gas drilling using both advances in computer science and geoscience, and to teach the public about such impacts. The project will elucidate both the effects of human activities such as shale gas development as well as natural processes which release methane into natural waters. Results of the proposed research will lead to a better understanding of water quality in areas of shale-gas development and will highlight problems and potentially problematic management practices. The research will advance both the fields of geoscience and computer science, will train interdisciplinary graduate students, and involve citizen scientists in collecting data and understanding environmental data analysis. <br/><br/>The project combines new hydro-geochemical strategies and data mining approaches to study the release of methane into streams and ground waters. For example, researchers will explore how to analyze the heterogeneous spatial data that describe distributions of methane concentrations in natural waters. The objectives of this project are to i) transform the ability to measure methane in streams; ii) train citizen scientists to work with project scientists to sample streams in an area of shale-gas development and publish large-volume datasets of methane in natural waters and aquifers; iii) innovate data mining and machine learning methods for environmental data to identify anomalous spots with potential leakage; iv) run field campaigns to measure methane concentrations and isotopic signatures of water samples in these spots; v) foster dialogue among nonscientists, consultants, university scientists, members of the gas industry, government agencies, and nonprofit organizations in and beyond the target region. Toward this end, the team will host workshops aimed to build dialogue among stakeholders and will release data analytic software for environmental measurements to benefit a broader research community."
"1646612","CPS: Synergy: Collaborative Research: The Sharing Economy for Electricity Services in Connected Communities","ECCS","Special Projects - CNS, CPS-Cyber-Physical Systems","10/01/2016","09/23/2019","Kameshwar Poolla","CA","University of California-Berkeley","Standard Grant","Radhakisan Baheti","09/30/2021","$1,200,000.00","Duncan Callaway, Michael Jordan, Pravin Varaiya","poolla@me.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","ENG","1714, 7918","155E, 7918","$0.00","Pressing environmental problems, energy supply security issues, and nuclear power safety concerns drive the worldwide interest in renewable energy. The US Clean Energy Challenge calls for a partnership of states and communities to expand solar to 140GW by 2020. Investment in renewables today is in utility-scale solar plants and wind farms, as well as small-scale distributed rooftop photovoltaics (PV).  Large solar plants are cheaper than rooftop PV, but this advantage is diminished when considering transmission infrastructure costs. Generous tax credits and net metering subsidies are responsible for much of the dramatic growth of distributed PV. Under net metering, utilities are mandated to buy back excess generation at retail prices. But tax credits are being phased out, and utilities strenuously oppose net metering policies as they allow PV owners to avoid paying for infrastructure costs and pose an existential threat to utility business models. The growth of distributed PV generation may decelerate.  This project aims to sustain and accelerate future growth in distributed PV investments by enabling connected communities to share electricity services. The central thesis is that shared PV ownership and operations can spur greater investment in distributed PV with minimal subsidy, without net metering, and with participants fairly paying for infrastructure, reserves and reliability costs.  <br/><br/>Our research will enable connected communities to efficiently use resources, reduce emissions, and support our collective sustainability goals. It will spur deeper penetration of distributed PV without subsidy, while defining new entrepreneurial opportunities in the sharing economy for electricity services. The project will integrate education and research through new interdisciplinary courses that combine technology, economics, policy, and power systems. This research is broadly applicable to other shared services including electricity storage, building energy management, and transportation networks. Specifically, we will (a) develop the infrastructure necessary for sharing electricity services, (b) analyze investment decisions of households under various tariff and subsidy designs, (c) construct behavioral model that predict consumer response to incentives, and (d) conduct an empirical assessment of sharing grounded in data.  In our architectural vision for sharing, agents interact with each other through a cloud based supervisory system. This system manages constraints, accepts supply and demand bids for shared resources, clears the market, and publishes prices. A key element of our architecture is software-define-power-flow to scale sharing to millions of clients under a peer-to-peer matching platform. We will make the business case for sharing in the energy sector using game-theoretic methods and micro-economic tools to analyze investment decisions in a sharing economy for electricity services.  Recruiting clients to share their resources is a key research challenge. Here, we will apply modern machine learning methods to identify, model, and target suitable clients.  Finally, we will use data analytics methods to make a compelling case for sharing based on city-scale data."
"1619452","CIF:Small:Collaborative Research:Statistics of slow mixing Markov processes: theory and applications to community detection","CCF","Comm & Information Foundations","06/15/2016","06/10/2016","Narayana Santhanam","HI","University of Hawaii","Standard Grant","Phillip Regalia","05/31/2020","$499,468.00","Aleksandar Kavcic","nsanthan@hawaii.edu","2440 Campus Road, Box 368","HONOLULU","HI","968222234","8089567800","CSE","7797","7923, 7935, 9150","$0.00","Historically, the field of statistics developed around multiple independent observations of phenomena. Yet progress in modern applications from biology to social networks increasingly requires deeper understanding of observations that are dependent. Dependent observations introduce biases unseen in independent sampling. If not interpreted properly, they often lead to misleading conclusions. This research develops a statistical framework to interpret samples generated by dependent observations. The problem is then turned around to develop algorithms that detect meaningful dependencies in data. Such algorithms are used, for example, to identify genes working together, or find the extent to which social networks are polarized on certain issues.  Not only is this research translated to classes, demonstrations and outreach programs, but also the location in Hawaii is leveraged to reach out to underrepresented communities in science and engineering, in particular, women and the Pacific Islander community.<br/><br/>Stationary properties of Markov processes may not be very well reflected by finite samples, no matter how large the sample size. This bias is often formalized as mixing, and is unseen in independent sampling.  This research analyzes Markov samples even before the samples reflect the asymptotic stationary properties, and develops a framework for inference, analysis and simulation of slow mixing Markov processes. Then, algorithmic primitives based on coupling from the past are developed for the widely studied task of community detection in a graph, where vertices are partitioned into clusters so that intra-cluster vertices are connected tighter than those in disparate clusters. As several resampling procedures in machine learning such as cross validation and bootstrap are premised on independent sampling and have no good analogs in the Markov setting, this research develops new resampling approaches for potentially slow mixing Markov processes."
"1618795","AF: Small: Algorithms and Information Theory for Causal Inference","CCF","Algorithmic Foundations","08/01/2016","06/03/2016","Leonard Schulman","CA","California Institute of Technology","Standard Grant","A. Funda Ergun","07/31/2020","$450,000.00","","schulman@caltech.edu","1200 E California Blvd","PASADENA","CA","911250600","6263956219","CSE","7796","7923, 7926","$0.00","This project is concerned, firstly, with algorithmic and information-theoretic aspects of Causal Inference. With the exception of some scientific data that is gathered purely for knowledge, most data is gathered for the purpose of potential intervention: this holds for medicine, public health, environmental regulations, market research, legal remedies for discrimination, and in many other domains. A decision-maker cannot take advantage of correlations and other structural characterizations that are discovered in data without knowing about causal relationships between variables. Historically, causality has been teased apart from correlation through controlled experiments. However there are several good reasons that one must often make do with passive observation: ethical reasons; governance constraints; and uniqueness of the system and the inability to re-run history. Absent experiments, we are without the principal arsenal of the scientific method.<br/><br/>Yet there is a special class of systems in which it is possible to perform causality inference purely from passive observation of the statistics. For a system to fall in this class one must be able to establish on physical grounds that certain observable variables are statistically independent of certain others, conditional on a third set being held fixed; the formalism for this is ``semi-Markovian graphical models"". It is known which semi-Markovian models fall in this class, subject to the assumption of perfect statistics. From this starting point there remain significant theoretical challenges before these ideas can have the greatest possible impact on practice. Some of the challenges to be addressed include:<br/><br/>(1) The PI will aim to quantify how the stability (condition number) of causal identification depends on the various sources of uncertainty (statistical error; numerical error; model error) and as a function of the structure of the graphical model. The purpose is both to understand what inference is justifiable from existing data, and to impact study design so that data with the greatest leverage is collected. For the former objective, in particular, the PI seeks an efficient algorithm to compute the condition number of a given semi-Markovian model at the specific observed statistics. For the last objective the PI seeks an efficient algorithm to compute the worst-case condition number of a given semi-Markovian model.<br/><br/>(2) Existing causal identification algorithms, applied to data inconsistent with the model (which is unavoidable due to statistical error, and normally also due to model error), will yield an inference inconsistent with the model. The project will help to understand if projection onto the model may improve stability.<br/><br/>(3) One of the obstacles to use of existing methods is that they require sample size exponential in the size of the graphical model. The project aims to determine when it is possible to infer causality using only the marginal distributions over small subsets of the observable variables; this will reduce sample size and likely improve condition number.<br/><br/>(4) In the majority of semi-Markovian models, causality is not identifiable. This leaves open however the possibility of determining (or giving a nontrivial outer bound for) the feasible interval of causal effects. No effective algorithm is currently known for this problem, and we wish to provide one. Such an algorithm could be used to show that an intervention is favorable despite the effect not being fully identifiable.<br/><br/>(5) The project aims to lift the causal-inference algorithm to time series, as well as study the connections with the distinct techniques (Granger causality and Massey's directed information) normally used in this setting.<br/><br/>Secondary emphases of the project include broader research in theoretical computer science. In particular, studying connections between ``boosting"" or ``multiplicative weights"" methods used in algorithms and machine learning, and their variants which arise out of selection or self-interest in the system dynamics of ecosystems (``weak selection"") and economic marketplaces (``tatonnement"").<br/><br/>Inseparably from the research effort, the PI will train students and postdocs in these and related areas of the theory of computation."
"1614853","The First Billion Years: a Petascale Universe of Galaxies and Quasars","OAC","Leadership-Class Computing","05/01/2016","04/29/2016","Tiziana Di Matteo","PA","Carnegie-Mellon University","Standard Grant","Edward Walker","08/31/2019","$40,000.00","Rupert Croft","tiziana@phys.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7781","","$0.00","The Hubble Deep/Ultra Deep fields are iconic, and almost certainly the<br/>most studied space telescope observations. Upcoming observations from an enormous range of<br/>current and planned instruments, from the successor to Hubble, The James Webb Telescope, to<br/>the Large Synoptic Survey Telescope (LSST), to the WFIRST Satellite will also have the<br/>opportunity to reach galaxies and black holes at the same extreme magnitudes or deeper, but<br/>over unprecedentedly wide fields. This will open up the study of the highest redshift galaxies<br/>and quasars to the type of statistical studies that have made modern cosmology a precision<br/>science. This project aims to compute theoretical predictions to make contact with current<br/>and upcoming observations of the high-redshift universe using the Blue Waters supercomputer. <br/><br/>The project will extend the BlueTides simulation, with an unprecedented<br/>volume and resolution, to cover the evolution of the first billion years of cosmic history.<br/>The goal is to significantly increase the scientific impact of this calculation to the community.<br/>Importantly, the project will attempt to make contact<br/>with observations of quasars, which have not been discovered at redshift greater than 7 (while<br/>the simulation now has run to redshift equal 8). In addition the project will be using BlueTides as<br/>a path-finder for developing methods/calculations for future cosmological hydrodynamical simulations<br/>of galaxy formation with volumes and resolutions suitable for creating mocks for next generation<br/>surveys. The impact of the proposed work will extend way beyond BlueTides. Large hydrodynamical simulations<br/>will be more and more useful in all stages of major observational projects in astrophysics<br/>and cosmology. For example, a simulation that covers a significant fraction of the entire<br/>observable universe with BlueTides resolution and runs to the present day (an epoch which<br/>is fully dominated by the hydrodynamic computations) will be needed for LSST.<br/><br/>The project will establish a theoretical framework for understanding the role of galaxies<br/>in the evolution of the universe at high redshifts. Different communities<br/>of scientists are interested in the behavior history of quasars and galaxy assembly, including<br/>cosmologists, the galaxy evolution community and high energy astrophysicists, so the results<br/>would have a wide impact across many different scientific communities. <br/><br/>Additionally, the image and catalog generation, and<br/>database techniques developed by the project will<br/>strengthen the project already on-going synergistic activities with computer science, machine learning<br/>and statistics.  Furthermore, the project will have a strong education component by involving undergraduate<br/>and graduate students in this research.  Finally, the project propose to perform outreach using<br/>the visualization and interactive Gigapan software."
"1552131","CAREER:Information-Theoretic Foundations of Community Detection and Graphical Channels","CCF","Comm & Information Foundations","02/15/2016","02/07/2020","Emmanuel Abbe","NJ","Princeton University","Continuing Grant","Phillip Regalia","01/31/2021","$500,000.00","","emmanuelabbe@gmail.com","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","7797","1045, 7935","$0.00","The main goal of this project is to establish the fundamental limits of community detection. In virtually all applications dealing with networks and large data sets, one wishes to extract sub-groups of data points that are similar, i.e., communities. While community detection techniques are expanding daily with practical successes, relatively less attention has been paid to the fundamental limits, and consequently to where current algorithms stand. By establishing the fundamental limits of community detection, this project offers a novel take on community detection algorithms, and expands information theory in a prominent area where it can naturally flourish. The project will work with real data sets from social and biological networks. In particular, it develops a new initiative to extract communities in Hi-C genomic data, contributing to unveil the 3D folding structure of DNA.<br/><br/>The project will focus in particular on the stochastic block model, a canonical model for community detection. The investigator's recent work leverages information theory to provide the first necessary and sufficient conditions for exact recovery in the stochastic block model, and an efficient algorithm achieving the limit. This opens the door to a new perspective on community detection, which is developed in this project by casting community detection as unorthodox error-control coding problems. In this context, new types of f-divergences are expected to play a key role, analogous to the Kullback-Leibler divergence in Shannon's channel coding theorem, while other weaker recovery requirements may rely on unorthodox broadcasting problems, graph entropic inequalities, and information-estimation problems. This makes the study of community detection a rich area connecting information theory, machine learning and networks; less focused on ergodic results; and more interlaced with graph theory and spectral analysis. In particular, this project will show how these problems, as well as more general low-rank approximation problems, can be studied under the novel and unifying theme of graphical channels."
"1613060","Prediction Models Based on Large Scale Image Data","DMS","STATISTICS","08/01/2016","07/25/2016","Xiao Wang","IN","Purdue University","Standard Grant","Gabor Szekely","07/31/2019","$100,000.00","","wangxiao@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","MPS","1269","8091","$0.00","Research in statistics involves the development and understanding of models based on data. Generally, these data are in the form of numbers, but more recently, statisticians have begun to develop models for data in the form of images. These functional image models have broad applications in neuroscience, engineering, and biomedical practice. This research will further the development of these image models. This project will also include the development of new courses at the undergraduate and graduate levels to train students in the use and understanding of these models. <br/><br/>This project is to develop an integrated research program that studies a broad class of large scale functional image models. The PI aims to develop the adaptive and/or local region regression, the finite mixture regression, and the transformation survival regression with ultra-high dimensional image data. The key advantages of these models are to preserve sharp edges for better interpretation, to incorporate the heterogeneity in the population for better representation, and to handle sophisticated censored data. The theoretical contributions of the proposed research are made towards addressing fundamental issues across several disciplines, including nonparametric statistics and machine learning. These functional image models have broad applications in neuroscience, engineering, and biomedical practice. Courses will be developed to train students in the use and understanding of these models."
"1633791","Collaborative Research: Academic hiring networks and scientific productivity across disciplines","SMA","APPLIED MATHEMATICS, SciSIP-Sci of Sci Innov Policy","09/01/2016","08/25/2016","Aaron Clauset","CO","University of Colorado at Boulder","Standard Grant","Cassidy Sugimoto","08/31/2020","$392,503.00","","aaron.clauset@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","SBE","1266, 7626","4444, 7626, 8050, 8251","$0.00","Advances in science come from the collective and linked efforts of thousands of researchers working within and across disciplines. This project creates rigorous models of the composition, dynamics, and network structure of the United States' scientific workforce across heterogeneous independent institutions with different strengths and emphases.  The systematic influence of these institutional and individual characteristics on scientific advances across disciplines is investigated. The results of this project will generate new insights into the composition of the scientific workforce and scientific productivity across fields. In addition, this project trains new graduate and undergraduate students in cutting-edge computational and statistical research techniques, and will develop and disseminate new large-scale open data sets on the composition of the United States' scientific workforce and provide new software for collecting structured data automatically from open unstructured sources.<br/><br/>This project uses state-of-the-art computational and statistical techniques from network science, machine learning, and social modeling to create a new technology platform for automatically and systematically collecting high-quality structured data on the composition, dynamics, and output of the scientific workforce.  These data will be combined with social survey results of individual researchers and with rigorous network methods to model the relationship between workforce composition, productivity, and observable differences at the individual and institutional levels within and between scientific fields. Mathematical models of the short- and long-term evolution of workforce in order to evaluate the likely outcomes of certain types of interventions and policies are developed."
"1619339","TWC SBE: Small: From Threat to Boon: Understanding and Controlling Strategic Information Transmission in Cyber-Socio-Physical Systems","CNS","Secure &Trustworthy Cyberspace","10/01/2016","08/24/2016","Cedric Langbort","IL","University of Illinois at Urbana-Champaign","Standard Grant","Nina Amla","09/30/2020","$499,800.00","","langbort@uiuc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","8060","7434, 7923","$0.00","As cyber-socio-physical and infrastructure systems are increasingly relying on data and integrating an ever-growing range of disparate, sometimes unconventional, and possibly untrusted data sources, there is a growing need to consider the problem of estimation in the presence of strategic and/or self-interested sensors. This class of problems, called ""strategic information transmission"" (SIT), differs from classical fault-tolerant estimation since the sensors are not merely failing or malfunctioning, but are actively trying to mislead the estimator for their own benefit. Such strategic behavior could happen through at least two mechanisms, sensor hijacking and willful misreporting, both of which have been observed in practice. One particularly stark and fundamental difference between strategic and faulty sensors is that a population of strategic sensors does not necessarily result in an erroneous estimate. In fact, depending on how each sensor expects others to act, it is even possible for the resulting estimator to converge to a correct value of the estimated quantity of interest faster than in the absence of any strategic intent! In other words, strategic information transmission appears to have the potential for being both a boon and a curse for the systems it targets. The central question for the engineer and system designer, then, is: ""where is the demarcation line?""<br/><br/>This project is concerned with a comprehensive study of SIT in the specific contexts of false data attacks in controlled systems, willful misreporting in participatory sensing applications, and adversarial machine learning. It builds on and extends existing models from the so-called ""cheap talk"" literature in Economics, where strategic information transmission has been considered before, although under widely different assumptions than in the context of security of cyber-physical systems. The novelty, pertinence, and intellectual merit of this project, lie in (1) its formulation of new models that more closely account for the specificity of strategic information transmission in the three applications of interest than existing frameworks, (2) its combined use of information theoretic and game theoretic tools to analyze these models and, (3) the use of behavioral economics experiments to help characterize, and straddle, the boundary between detrimental and beneficial strategic information transmission in practice."
"1622542","SCH: INT: Collaborative Research: Assistive Integrative Support Tool for Retinopathy of Prematurity","IIS","Smart and Connected Health","10/01/2016","07/27/2016","Jayashree Kalpathy-Cramer","MA","Massachusetts General Hospital","Standard Grant","Sylvia Spengler","09/30/2020","$688,207.00","","kalpathy@nmr.mgh.harvard.edu","Research Management","Somerville","MA","021451446","8572821670","CSE","8018","8018, 8062","$0.00","Retinopathy of prematurity (ROP) is a leading cause of childhood visual loss worldwide, and the social burdens of infancy-acquired blindness are enormous. Early diagnosis is critically important for successful treatment, and can prevent most cases of blindness. However, lack of access to expert medical diagnosis and care, especially in rural areas, remains a growing healthcare challenge. In addition, clinical expertise in ROP is lacking, and medical professionals are struggling to meet the increasing need for ROP care. As point-of-care technologies for diagnosis and intervention are rapidly expanding, the potential ability to assess ROP severity from any location with an internet connection and a camera, even without immediate ophthalmologic consultation available, could significantly improve delivery of ROP care by identifying infants who are in most urgent need for referral and treatment. This would dramatically reduce the incidence of blindness without a proportionate increase in the need for human resources, which take many years to develop.<br/><br/>This project develops a prototype assistive integrative support tool for ROP, featuring a modular design comprising: (a) image analysis, (b) information fusion of clinical, imaging, and diagnostic data, and (c) generative probabilistic and regression models with associated computationally efficient machine learning algorithms. The outcomes of the project include disease severity metrics and diagnostic estimates obtained through clinical evidence classifiers trained jointly over expert-generated labels. These labels consist of discrete diagnostic labels, as well as comparison outcomes of relative severity between pairs of images. Random process models for vessel tortuosity and diameter distributions over the retina, as well as patch-based vessel-free image analysis through the use of convolutional neural networks on the entire image, enhance and augment feature extraction. Moreover, incorporating severity comparison outcomes through novel hard and soft constraint methods force inferred severity to agree with ordinal information provided by experts and address inherent uncertainty in expert ground-truth labels. The above severity inference methods are evaluated and fine-tuned over a broad array of generative models, both through retrospective analysis, including cross-validation, longitudinal tests, and tests across multiple sites, as well as through prospective analysis, evaluating its real-world clinical impact."
"1637474","Gradient Sliding Schemes for Large-scale Optimization and Data Analysis","CMMI","OE Operations Engineering","02/15/2016","11/27/2018","Guanghui Lan","GA","Georgia Tech Research Corporation","Standard Grant","Georgia-Ann Klutke","10/31/2019","$266,731.00","","george.lan@isye.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","ENG","006Y","072E, 073E, 077E","$0.00","The rapid advances in technology for digital data collection have led to significant increases in the size and complexity of data sets, sometimes known as big data. Optimization models, when combined with novel statistical analysis, have been proven fruitful in analyzing these complex datasets. However, optimization problems arising from these applications often involve nonsmooth components that can significantly slow down the convergence of existing optimization algorithms. Moreover, the complex datasets are so big and often distributed over different storage locations that the usual assumption that an entire dataset can be completely traversed in each iteration of the algorithm is unrealistic. Gradient sliding schemes do not require this assumption and hence are ideally suited for optimization with big data.  The research aims at tackling these computational challenges through the design, analysis, and implementation of a novel class of optimization algorithms using gradient sliding schemes.  The effectiveness of these new optimization algorithms will be demonstrated by solving problems in image processing and machine learning.<br/><br/>The gradient sliding algorithms are first-order methods that use first-order information (gradients and function values) exclusively in addition to some auxiliary operations, such as projection over the feasible set. As opposed to existing first-order methods, gradient sliding methods can skip the computation of gradients from time to time, while still preserving the optimal convergence properties for solving different types of large-scale optimization problems. This research will also study a new class of conditional gradient sliding methods that require a linear optimization rather than a more involved projection over the feasible set in each iteration. These algorithms are expected to exhibit optimal rate of convergence in terms of both the number of gradient computations and the number of times for solving the linear optimization subproblem. Moreover, randomized variants of these gradient sliding algorithms which are amenable to parallel/distributed computing will also be studied.  When applied to data analysis, these algorithms can reduce, by orders of magnitude, the number of traverses through the datasets, the computational cost associated with the involved matrix-vector multiplications, as well as the communication costs for the distributed datasets."
"1607800","US-Israel Collaboration: Collaborative Research: New Tools for Extracting Neuronal Phenotypes from a Volumetric Set of Cerebral Cortex Images","IIS","CRCNS-Computation Neuroscience, IntgStrat Undst Neurl&Cogn Sys","09/01/2016","08/25/2016","Hanspeter Pfister","MA","Harvard University","Standard Grant","Sylvia Spengler","08/31/2019","$392,411.00","Jeff Lichtman","pfister@seas.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","7327, 8624","5905, 7327, 8089, 8091","$0.00","A major limitation in connectomics is that there are few tools to transform connectomic images into a minable database. The research aim of this project is to develop a suite of tools that extract essential structural parameters from the brain's physical structure that was imaged at very high (nanometer scale) resolution. The PIs will determine, by using automated methods, the sizes and shapes of neurons, synapses and their connectivity patterns. Using their tools, the PIs will analyze this detailed and varied dataset to find the key patterns within it. It is their belief that such automated methods are a requirement to comprehend the regularities and rules that govern the formation of neural circuits in the cerebral cortex, which to date have only been studied on very small sample spaces. The cerebral cortex remains perhaps the least understood aspect of mammalian biology. No studyof this magnitude of the neuronal phenotype space has ever been conducted: the dataset will contain hundreds of thousands of somata and a billion synapses, allowing the PIs to search  for patterns that could only be guessed at with the tools used in prior research. Knowing what overarching organizational principles exist in a cerebral cortical network is crucial for understanding how brains work normally and how they may go awry in disease. Moreover, connectomic studies are beginning in a large number of different laboratories throughout the world focused on a wide range of species and parts of the brain. These tools should have direct applicability to many of these endeavors.<br/><br/>The PIs are a consortium of four laboratories with complementary areas of expertise in computer science (Shavit), systems biology (Alon), image processing (Pfister) and neurobiology (Lichtman). Together they are building a stacked set of methods that extract important parameters from connectomic images. These methods include neuron geometry extraction, network structure, motif detection, and archetypical pattern analysis. These approaches are based on two software platforms:the MapRecurse platform for generating connectome graphs and the Pareto Inference Engine for mining patterns within such graphs. The PIs will test these techniques on an a volume of mammalian cerebral cortex containing tens of thousands of cells and a billion synapses, with the aim of extracting the properties of neural circuits that would be difficult or impossible to obtain any other way. The work in this proposal will have significant impact on neuroscience. It speaks directly to the central goals of the White House BRAIN Initiative. It will provide neuroscientists with anumber of powerful and novel tools to understand the cells and circuits that underlie brain function. It should also be influential in developing approaches in machine learning and neuromorphic computing.  <br/><br/>A companion project is being funded by the US-Israel Binational Science Foundation (BSF)."
"1632445","The Relationship between Natural Environments, Fatigue, and Attention","BCS","Perception, Action & Cognition, IntgStrat Undst Neurl&Cogn Sys","08/01/2016","08/09/2016","Marc Berman","IL","University of Chicago","Standard Grant","Lawrence Gottlob","07/31/2019","$402,182.00","Greg Norman","bermanm@uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","SBE","7252, 8624","7252, 8089, 8091","$0.00","The United Nations estimates that by the year 2050 roughly 70% of the world's population will be living in highly urbanized areas. This increase in urban living comes with many benefits: sustainable spaces for living, increased flow of ideas and innovation, better proximity to services, and diverse communities. Unfortunately, urban life as it stands is also associated with increased stress, worse physical and mental health, and shorter life expectancy. All of these negative effects alter our physiology, brain, and behavior. By understanding the psychological, neural, and physiological mechanisms underlying the relationship between exposure to nature and cognitive performance, future studies will be able to more easily implement interventions aimed at improving built spaces in order to enhance the potential for human productivity and vitality. This research is guided by Attention Restoration Theory, which posits that incorporating natural elements into the built environment can counteract some of the negatives of urban life. Results from this proposal could be used to help design schools, offices, homes, cities and towns to optimize human functioning.<br/><br/>Philosophers, writers, and lay people have had the intuition for centuries that interacting with nature has myriad benefits to human functioning. While this intuition is appealing, it needs rigorous testing. This project aims to identify the mechanisms through which interactions with natural environments might enhance both behavior and brain function, using cutting edge neuroscience, psychophysiological and machine-learning tools. The primary objectives of this research project are the following. First the research will examine the psychological, physiological, and neural processes that are altered during interactions with natural environments. It will address questions such as how hard must the brain work when processing more natural vs. more urban stimuli, and how does this processing affect people's ability to focus. Second, the research will determine some of the characteristics (e.g., low-level visual features such as color, edge content, fractalness, etc.) of natural and urban environments that lead to psychological improvements or decrements. This information will aid understanding of some of the visual features that may be producing these benefits, which could then be used to help design new built spaces or retrofit existing built spaces. Third, the research will test whether individuals who are in more mentally fatigued states will benefit more from natural vs. urban environment exposure. This aspect of the work will help to define the neural and psychophysiological basis of mental fatigue. In summary, this research project aims to understand how interactions with different environments affect human functioning, and what interventions can be made to improve human functioning at large scales."
"1636915","Crowdsourcing Urban Bicycle Level of Service Measures","CNS","S&CC: Smart & Connected Commun","07/15/2016","07/18/2016","Vanessa Frias-Martinez","MD","University of Maryland College Park","Standard Grant","David Corman","09/30/2019","$200,000.00","","vfrias@umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","033Y","7916, 7918","$0.00","Over the past two decades, cities across the country have experienced a tremendous growth in cycling. As cities expand and improve their bicycle networks, local governments and bicycle associations are looking into ways of making cycling in urban areas safer. However, one of the main obstacles in decreasing the number of bicycle crashes is the lack of information regarding cycling safety at the street level. Historically, Bicycle Level of Service (BLOS) models have been used to measure street safety. Unfortunately, these models require extensive information about each particular roadway section, which often times is not available. This EArly-concept Grant for Exploratory Research (EAGER) project will provide innovative tools to automatically estimate street safety levels from crowd-sourced citizens' complaints as well as to shed some light into the traffic-related reasons behind such safety values. Ultimately, the outcomes of this project will contribute to the overall vision for Smart and Connected Communities (S&CC) by helping to reduce the number of crashes and human fatalities in the city using large streams of data collected from connected citizens.  The project has strong support from multiple local institutions including Bike Share and local transportation departments.<br/><br/><br/>From a technical perspective, the main innovation will be the ability to automatically compute cycling safety measures using information extracted from citizen-generated complaints at very fine-grained spatio-temporal scales. For that purpose, the project will use data mining and machine-learning techniques to extract relevant quantitative and textual features from the crowd-sourced data. The expected outcomes of this project will be: (a) accurate and interpretable models to estimate street safety levels from user-generated data; (b) a set of easy-to-interpret, actionable items for local Departments of Transportation to improve cycling experiences and general safety; and (c) a dataset with user-generated complaints, cycling videos and safety levels per road segments to share with other researchers so as to advance the state of the art in data-driven cycling safety."
"1622049","SBIR Phase I:  An Open Source Platform for Intelligent Virtual Assistants","IIP","SBIR Phase I","07/01/2016","06/21/2016","Michael Laurenzano","MI","Clinc, Inc","Standard Grant","Peter Atherton","06/30/2017","$225,000.00","","michael.laurenzano@gmail.com","1940 Hedgenettle Ct.","Ann Arbor","MI","481039689","8582053027","ENG","5371","5371, 8032","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project will arise from the development and release of the world's first open source Intelligent Virtual Assistant (IVA) platform. The platform will be made available to the world as open source software free of charge, and will be surrounded by a set of commercial offerings that include the deployment and customization of the software as well as software-as-a-service (SaaS) offerings that will allow users of the platform to painlessly create and deploy IVAs. Beyond the commercial applications of IVA technology, the open source release of the platform will help foster a healthy, active open source community around the IVA technology, allowing it to be leveraged as a machine learning research platform and accelerating IVA adoption in important non-commercial settings such as non-profit education and improving technological access for the disabled.<br/><br/>This Small Business Innovation Research (SBIR) Phase I project will address the significant computer systems challenges involved in seamlessly orchestrating the massive amounts of computation among complex pipelines of algorithmic components required to power intelligent virtual assistant (IVA) technologies. The computation required to power IVAs is large enough that such software systems will likely run in large datacenter infrastructures comprised of complex ecosystems of different server types and accelerator platforms. This project will address these challenges by building a centralized software mechanism within the IVA platform for monitoring job execution to facilitate efficiently mapping sub-tasks within the IVA computation to the available hardware resources in the datacenter."
"1620472","Randomized Algorithms for Matrix Computations","DMS","COMPUTATIONAL MATHEMATICS","09/01/2016","08/17/2016","Per-Gunnar Martinsson","CO","University of Colorado at Boulder","Standard Grant","Leland Jameson","04/30/2019","$249,979.00","","pgm@ices.utexas.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","MPS","1271","9263","$0.00","This project will develop mathematical techniques for accelerating computational tasks such as simulating electromagnetic scattering,  medical imaging, extracting useful information from large datasets, machine learning, and many others. In all these computations, the step that tends to be the most time-consuming, and which therefore limits how large problems can be solved, concerns the manipulation of large square or rectangular arrays of numbers, called ""matrices"". Many of the matrices that arise in practical applications have redundancies, and can be compressed to enable them to be stored using less space. Using the compressed format, computations involving the matrix can also be greatly accelerated. The problems that will be addressed are deterministic in nature, but the algorithms that will be developed are probabilistic. It turns out that by exploiting certain mathematical properties of large ensembles of independent random numbers, one can build algorithms for compressing matrices that are much faster than traditional deterministic techniques. The new randomized algorithms can in theory fail, but the likelihood of failure can be shown to  be lower than 1 time out of 10,000,000,000 runs in typical applications. Randomized algorithms of this type have recently attracted much interest due to the fact that they perform  particularly well on emerging computing platforms such as mobile computing (where conserving energy is the key priority), computing using graphical processor units (where the vast numbers of computational cores create challenges), and distributed memory parallel computers. The methods also perform very well when applied  to massively large datasets that must be stored on hard drives, or on large server farms. The project will train one doctoral student, and will lead to the release of a publicly available software package that implements the methods that will be developed. <br/><br/>From a technical point of view, the objective of the project is to develop efficient algorithms for factorizing matrices and for solving large linear systems of algebraic equations. The algorithms will be based on randomized sampling, and will exploit remarkable mathematical properties of random matrices and random orthogonal projections. Such randomized algorithms require less communication  than traditional methods, which makes them particularly attractive for modern applications involving multicore processors, distributed computing, out-of-core computing, etc. Specifically, the project will address the following problems: (1) Computing full matrix factorizations (e.g. the so called ""column pivoted QR factorization"") which are core building blocks in scientific computing. Preliminary numerical experiments demonstrate speed-ups of close to an order of magnitude compared to state-of-the-art software packages. (2) Solving linear systems involving many unknowns and many equations. We expect to achieve substantial practical acceleration, and are cautiously optimistic about the possibility to develop solvers with substantially better asymptotic complexity than the cubic complexity achieved by standard techniques. (3) Developing randomized methods for accelerating computational simulations of phenomena such as electro-statics, composite materials, biochemical processes, slow fluid flows, Gaussian processes in 2 and 3 dimensions, etc. Technically, this will be achieved by developing randomized methods for compressing so called ""data-sparse"" or ""rank-structured"" matrices."
"1619630","Feature-Based Data Assimilation and Uncertainty Quantification for Complex Systems in Science and Engineering","DMS","COMPUTATIONAL MATHEMATICS","08/01/2016","06/19/2018","Matthias Morzfeld","AZ","University of Arizona","Continuing Grant","Leland Jameson","07/31/2019","$249,999.00","","mmo@math.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","MPS","1271","1303, 8396, 8399, 9263","$0.00","The basic idea of data assimilation is to update a computational model with information from sparse and noisy data so that the updated model can be used for predictions. Data assimilation is at the core of computational geophysics, most notably in numerical weather prediction, oceanography, and geomagnetism, and is used widely in engineering applications, ranging from robotics to reservoir modeling. In the usual approach one attempts to refine a computational model such that its outputs match data. However, matching model outputs directly to data is often unnecessary or even undesirable. In this project, data assimilation is extended so that computational models can be updated based on features in the data, rather than the raw data themselves. The feature approach reduces an intrinsic dimension and is applicable to large scale problems in geosciences and engineering, with specific applications in geomagnetic dipole reversals, cloud modeling, and uncertainty quantification for solar cells.<br/><br/>The primary technical aim of this project is to extend data assimilation such that computational models can be calibrated against features observed in the data, rather than the raw data. This can be achieved within a Bayesian framework by replacing the data with a suitable low-dimensional feature, computed from the data. The resulting feature-based likelihood can be used to assimilate selected aspects of fine-scale data into coarse, low-dimensional models. More generally, the use of features reduces the dimension of the likelihood, which in turn reduces the computational requirements of feature-based data assimilation by Monte Carlo methods. The mathematical foundations of the feature-based approach will be explored by rigorous analysis. New computational methods for feature-based data assimilation will be created, which combine machine learning techniques with Monte Carlo sampling. The efficiency of these methods will be assessed by interdisciplinary collaboration with scientists in geosciences and engineering in three specific applications. Specifically, feature-based data assimilation algorithms will be developed for the study of superchrons of Earth's magnetic dipole field, to determine the geophysical relevance of low-dimensional cloud models, and for uncertainty quantification of thin-film polymeric reflectors for solar power generation. These applications will collaboratively connect scientists (faculty, postdocs and students) across several disciplines (geosciences, engineering, mathematics). Undergraduate and graduate students at the University of Arizona will be trained as part of the project and will aid in producing and disseminating key results. The research activities will be accompanied by an outreach plan, implemented as part of the G-Teams program within the Department of Mathematics at the University of Arizona. A central outreach theme is to demonstrate, for K-12 teachers and their students, mathematics ""in action"" by applying mathematical concepts to problems relevant to our society."
"1647432","EAGER: Dryads - Next Generation Tree Algorithms","CCF","Software & Hardware Foundation","08/01/2016","07/28/2016","Robert Brunner","IL","University of Illinois at Urbana-Champaign","Standard Grant","Almadena Chtchelkanova","07/31/2019","$299,815.00","Vincent Reverdy","bigdog@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7798","7916, 7942","$0.00","Many data sets can be represented via a natural hierarchical ordering, which can be easily represented programmatically by using tree data structures. For example, two-dimensional spatial data can be organized by using quad-trees, while three-dimensional data can be organized by using oct-trees. As data volumes continue to increase, compact representations of the extremely large data become increasingly important since the representations can enable much more efficient data selection, transportation, and processing. Yet the development of standardized, generic and efficient tree data structures that both scale to massive data and leverage the capabilities of modern computer architectures remains an unmet need. This research effort addresses this need by designing and implementing a library of generic implicit tree abstractions that will provide the foundation for next generation analysis codes in data driven sciences. By working with the C++ standardization committee, this research will potentially impact millions of software developers, worldwide, since this low level language is implicitly used by many high-level language analysis tools and libraries.<br/><br/>This research will investigate generic and high performance tree building blocks by exploring two key elements. First, low-level bit manipulation techniques will be created that can be optimized for specific computer architectures (such as the Intel Haswell). These techniques will be developed in conjunction with the international C++ standardization committee as an open source library and will impact a wide range of applications areas including arbitrary precision arithmetic, cryptography, and tree indexing strategies. Second, a generic library of implicit tree structures will be developed, by using the previously developed bit manipulation techniques, and submitted as a new, open-source library to the Boost community for broader dissemination. Finally, to demonstrate the efficacy of these new software libraries, two example tree applications will be developed and published: an oct-tree used for numerical simulations and a decision trees used for machine learning."
"1613746","EAPSI: Locating New Therapeutic Targets to Combat Drug Resistance","OISE","EAPSI","06/15/2016","07/20/2016","Keesha Erickson","CO","Erickson                Keesha         E","Fellowship Award","Anne Emig","05/31/2017","$5,400.00","","","","Boulder","CO","80303","","O/D","7316","5924, 5978, 7316","$0.00","Drug-resistant microorganisms and cancers result in more than 8 million deaths per year globally. While there has been extensive research into the mechanisms for bacterial or cancer drug resistance, there is no comprehensive method that allows the scientific community to easily search and draw comparisons across existing datasets. This project aims to build such a tool, deemed the Resistome. This project is conducted in collaboration with Prof. Juan Hsueh-Fen at the National Taiwan University, an expert in systems biology with invaluable experience in elucidating complex mechanisms underlying human disease. The Resistome will enable large-scale computational studies that were previously impossible, which will lead to new realizations about the nature of drug-resistance and will promote the eventual development of novel therapeutics that hinder resistance.<br/><br/>The initial vision for the database was to enable tracking of resistance-conferring mutations across diverse cell types, which will allow information to be analyzed using automated machine learning techniques. However, it is increasingly evident that transient responses at the gene expression level also promote resistance, without necessarily being reflected at the genome level. Thus, this major aim of this project is to expand the current Resistome database to allow for the incorporation of transcriptome-level data. Continuous collaboration with the Juan lab will have maximum benefit during this time, such that value fields in the database can be properly selected to broadly apply to bacterial and cancer drug resistances. A central database that includes both resistance-conferring mutations and resistance-conferring gene expression changes will support novel and complex computational efforts to identify the fundamental interactions underlying drug resistance. <br/><br/>This award under the East Asia and Pacific Summer Institutes program supports summer research by a U.S. graduate student and is jointly funded by NSF and the Ministry of Science and Technology of Taiwan."
"1641030","Support for U.S.-Based Students to Attend the 2016 IEEE International Conference on Data Mining (ICDM 2016)","IIS","Info Integration & Informatics","06/15/2016","05/31/2016","Jingrui He","AZ","Arizona State University","Standard Grant","Maria Zemankova","05/31/2017","$24,000.00","","jingrui@illinois.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","7364","7364, 7556","$0.00","This grant provides travel support for about 16 U.S. based graduate student participants to participate in the 2016 International Conference on Data Mining (ICDM 2016) that will be held in Barcelona, Spain, December 12 to 15, 2016 (http://icdm2016.eurecat.org). ICDM is a premier research conference in data mining. It provides an international forum for presentation of original research results, as well as exchange and dissemination of innovative, practical development experiences. The conference covers all aspects of data mining, including algorithms, software and systems, and applications. In addition, ICDM draws researchers and application developers from a wide range of data mining related areas such as statistics, machine learning, pattern recognition, databases and data warehousing, data visualization, knowledge-based systems, and high performance computing. Besides the technical program, the conference will feature workshops, tutorials, panels, the ICDM data mining contest, demos, and the PhD Forum. The ICDM organizers recognize the importance of recruiting and engaging students in data mining research in early stage of their graduate study. The PhD Forum is designed to provide an interactive environment in which PhD students can meet, exchange their ideas and experiences both with peers and with senior researchers from the data mining community in an international scope. Students from underrepresented minority groups are particularly encouraged to apply for support and participate in ICDM.<br/><br/>The conference seeks to continuously advance the state-of-the-art in data mining. With the growth of the Web, the Internet, and data intensive technologies such as Sensor Networks, and Bioinformatics, Data Mining is an extremely important area in Information Technology. The conference proceedings are published by the IEEE Computer Society Press."
"1638757","Student Travel Support: ACM International Workshop on Big Data in Life Sciences, Seattle, WA, October 2, 2016","CCF","Info Integration & Informatics, Algorithmic Foundations","07/01/2016","04/20/2016","Jaroslaw Zola","NY","SUNY at Buffalo","Standard Grant","Mitra Basu","06/30/2017","$10,000.00","","jzola@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","7364, 7796","7364, 7556, 7931","$0.00","The ACM International Workshop on Big Data in Life Sciences (BigLS) is a workshop series focusing on computational and data challenges in broadly defined life sciences. The workshop was initiated in 2013, and it is hosted together with the ACM Conference on Bioinformatics, Computational Biology and Health Informatics - the flagship conference of the Association for Computing Machinery Special Interest Group on Bioinformatics, Computational Biology, and Biomedical Informatics (SIGBio). The workshop brings together leading researchers and practitioners working on a diverse range of big data problems relating to biology and medicine, and engages them in a discussion about current big data questions, the state of computational tools and analytics, the challenges and the future trends within life sciences.<br/><br/>This project will support up to ten students and postdoctoral researchers from US academic institutions to attend BigLS 2016 to be held on October 2, 2016 in Seattle, WA. Awardees will be selected via a widely advertised competitive process involving the submission of a travel grant application, and review by the program committee. Preference will be given to women and underrepresented minority groups, first-generation college students, and undergraduate researchers.<br/><br/>Big data problems and challenges have become a reality in modern day computational biology, bioinformatics and biomedical informatics. The workshop will feature peer-reviewed papers and invited talks on five key research themes that underline big data research in life sciences: 1) scalable algorithms and techniques for big data analytics in molecular biology; 2) statistical and integrative approaches to big data biology; 3) emerging machine learning and AI techniques for big data biology; 4) high performance computing methods and software for big data biology; 5) software and hardware foundations for managing big data in biomedical informatics."
"1621722","Collaborative Research: CDS&E-MSS: Local Approximation for Large Scale Spatial Modeling","DMS","CDS&E-MSS","09/01/2016","08/24/2016","Benjamin Haaland","GA","Georgia Tech Research Corporation","Standard Grant","Yong Zeng","04/30/2017","$75,000.00","","benhaaland@hotmail.com","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","MPS","8069","8083, 9263","$0.00","Computer simulation is growing as a means of studying complex dynamics in applied science.  Once a tool exclusive to industrial engineering and computational physics, it is increasingly common in biology, chemistry, and economics.  Gone are the days when equilibrium dynamics are appropriate and cute systems of equations can be solved by hand.  Computer experiments are becoming more diverse, they are becoming more complex and they are growing in size thanks to modern supercomputing.  We need a new vanguard of modeling tools that can cope with the needs of modern computer experiments, particularly their increasing size (big data) and rapidly evolving and refining nature as models become more sophisticated, and supercomputing environments approach the exa-scale.  This funded research targets extensions and applications of a new breed of flexible and fast response surface methods, the so-called local approximate Gaussian process (laGP).  Our motivating applications come primarily from problems in computer experiments and uncertainty quantification, and ideas are borrowed from -- and will represent an important extension to -- the related literatures of geo-statistics and machine learning.  The over-arching goal is a modernization of the response surface and surrogate modeling toolkit to better serve future applications across applied science.<br/><br/>Gaussian process (GP) models are popular in spatial modeling contexts, like geostatistics or computer experiments, where response surfaces are reasonably smooth but little else can be assumed. GP models provide accurate predictors, but increasingly impose computational bottlenecks: large dense matrix decompositions impede efforts to keep pace with modern trends in data acquisition. A scramble is on for fast approximations. Two common themes are sparsity, allowing fast matrix decompositions, and supercomputing, allowing distributed calculation. But these inroads are at capacity. Rapidly expanding mobile device networks, high-resolution satellite imagery (and GPS), and supercomputer simulation generate data of ever-increasing size. This funded research centers on local approximate GP (laGP) models as a means of enabling the powerful GP spatial modeling framework to address modern big data problems. Initial implementations show promise, expanding data size capabilities by several orders of magnitude. However much work remains to ensure that laGP methods can supplant conventional GPs in diverse spatial modeling contexts. Here we propose several methodological enhancements, many involving shortcuts that have provably minimal impact on laGP performance. We are motivated by two big data computer model emulation applications: one involving satellite positioning and another on solar power generation. Yet we are mindful that for our efforts to have impact, the wider spatial modeling context must always be kept in view."
"1546079","BIGDATA: IA: Query-by-example for Big astronomical Data","IIS","Big Data Science &Engineering","01/01/2016","09/01/2015","Lior Shamir","MI","Lawrence Technological University","Standard Grant","Nigel Sharp","12/31/2018","$115,435.00","","lshamir@ksu.edu","21000 Ten Mile Road","Southfield","MI","480751051","2482042103","CSE","1798, 8083","7433, 8083","$0.00","While the most significant projects in modern observational astrophysics generate very large data sets, the computational methodology lags behind, and has trouble effectively analyzing these data.  Although current and future astronomical surveys will produce the world's largest public databases, the methods to turn these data into scientific discoveries do not yet exist.  This project will develop an automatic query-by-example system for classifying galaxies by their similarities to other galaxies, using unsupervised machine learning techniques.  This capability does not currently exist and it can substantially enhance the experience and discovery power of digital sky surveys.  The project has an educational component, focusing in particular on undergraduate research for under-represented minority students.<br/><br/>Existing, and especially planned, surveys can image billions of galaxies, making the ability to study rare galaxies through computer analysis absolutely essential.  These uncommon objects are critical for understanding the most fundamental questions about the early, present, and future universe, as they carry crucial information on the history of the interactions of objects, their formation, and their evolution.  The system to be developed will take an image of a certain (normally peculiar) galaxy, identified by the researcher as being of interest, and will search through millions of galaxies to find the visually most similar galaxies to the query galaxy.  Because studying unusual galaxies and making scientific conclusions about their nature requires a certain population from which to derive properties that can be compared to other systems, this capability and the resulting listings will greatly increase the ability to make discoveries from sky surveys, optimizing the scientific return of these important and expensive research instruments.<br/><br/>This project connects with existing efforts to attract under-represented minorities, adding more advanced research training in the later years of their undergraduate degree.  Studies like this always include opportunities for public outreach, and the team expects to contribute to the forming big data hub in their area."
"1621996","SBIR Phase I:  User-Centered System for Improved Coordination across the Continuum of Care","IIP","SMALL BUSINESS PHASE I","07/01/2016","06/24/2016","Thaddeus Fulford-Jones","MA","Radial Analytics, Inc.","Standard Grant","Jesus Soriano Molla","06/30/2017","$225,000.00","","thaddeus@radialanalytics.com","50 Beharrell Street Suite A","Concord","MA","017420000","6178558214","ENG","5371","5371, 8042","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project focuses on using analytics and technology to benefit patients who require recovery care to get fully well after being discharged from the hospital. Transitions of care from acute (hospital) to post-acute (short-term rehabilitation and skilled nursing) settings impact millions of Americans every year. Seniors overindex on utilization of post-acute care, as a consequence of natural age-related degeneration and the need for longer recovery periods. By improving post-hospital coordination of care across the continuum, clinical outcomes and patient satisfaction stand to improve for America?s aging population. If successful, this project will help reduce costs of care for healthcare providers, payers, and government/society.<br/><br/>The proposed project aims to incorporate and improve upon methods for user-centered data capture in healthcare. Our proposed platform will combine advanced machine learning techniques with a patient/family-centered business model. The innovation will harness multiple streams of healthcare data, such as electronic health records and claims data from both acute and post-acute care settings. If successful, this research will impact the state-of-the-art in healthcare analytics and outcomes measurement."
"1631146","PFI:BIC - Smart Laser-Based Imaging and Optical Spectroscopy System: optical quantification of bacterial load, oral health surveillance, and caries prediction.","IIP","GOALI-Grnt Opp Acad Lia wIndus, PFI-Partnrships for Innovation, BioP-Biophotonics","09/01/2016","04/14/2020","Eric Seibel","WA","University of Washington","Standard Grant","Jesus Soriano Molla","08/31/2021","$1,094,253.00","Sean Munson","eseibel@u.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","ENG","1504, 1662, 7236","019Z, 116E, 1504, 1662, 8042, 9102, 9231, 9251","$0.00","Caries and gum disease are especially prevalent and severe in low-income and rural communities, which often lack access to convenient and affordable dental care. Members of these communities are more likely to visit hospital emergency departments with advanced stages of oral disease that need surgical treatment; this increases the cost of dental care for families and burdens hospital resources. Tooth decay is the most common chronic disease in American children and adolescents age 6-19 despite being a preventable disease. In 2010, untreated tooth decay, or caries, affected 2.4 billion people worldwide. Early childhood caries leads to pain, infection, and discomfort. And in adults, chronic oral infections may increase the risk of preterm birth and diseases like diabetes and atherosclerosis (hardening of the arteries). This project will develop a hand-held electric-toothbrush-like device with a user-friendly interface that can be used in the home or by community health workers in schools or rural clinics. The wand, which uses a safe optical scanning method, will send data and images to a dentist, who will be able to monitor how well dental treatments are working. The data and images collected by the device can be analyzed to discover where plaque and bacteria are present, providing a way to predict, and then prevent, disease. The wand will be able to prompt the user to address problem areas. This system, which will connect remote users to a dental provider, has the potential to improve the prevention and treatment of oral disease and the quality of life for people who do not have convenient access to regular, in-person dental care. <br/><br/>To enable this vision, this project will develop a smart system that offers screening, surveillance, and prediction for people to improve their oral health and prevent disease. A mature technology of laser-based imaging that is spatially registered with fluorescence spectral analysis will be used to study the complexities of the oral biofilm and dental demineralization. The bacterial measurement is performed with a hand-held single wavelength optical scanning probe that forms images from both reflectance and fluorescence contrast, with the option of taking laser-induced fluorescence spectra for diagnosis of enamel demineralization. To provide a smart interface, the optical information will be analyzed to identify trends, a new modality for the dental field. With analyses of the fluorescence signal from the plaque deposits displayed as a trend, the user, in collaboration with clinician, can monitor variations in oral health and the effectiveness of treatments. The research team will use an iterative process to develop, design, evaluate, and refine the tool. The tool will be tested for three scenarios: (1) a plaque- and caries-screening program for a trained lay user on an untrained ""patient"" (e.g., parent for a child, school nurse for pupils, pediatrician for young children, or staff at a remote rural clinic for patients); (2) a caries surveillance program for trained lay users electronically connected to a dentist who can guide the use of the tool; and (3) a caries prediction program, initially for clinical users. <br/><br/>This project brings together a multidisciplinary team from the University of Washington (UW), in Seattle, Washington, with expertise in human centered design, engineering, sensing and machine learning, oral biology, and dentistry: Eric J. Seibel (Principal Investigator, PI), Mechanical Engineering; Sean Munson (co-PI), Human Centered Design & Engineering; Shwetak Patel, Computer Science & Engineering; Zheng Xu, DDS, School of Dentistry, Pediatric Dentistry; Jeff McLean, School of Dentistry, Periodontics. Lead industry partner Water Pik, Inc. (Fort Collins, CO), brings experience and knowledge about building, marketing, and distributing advanced dental devices to consumers: Deborah Lyle, Director of Professional & Clinical Affairs, and Jay McCulloch, Vice President, Global Marketing, Oral Care. Broader context partners are UW CoMotion (Seattle, WA), a technology transfer partner; QualComm Tech, Inc. (San Diego, CA), a wireless technology leader; Open Photonics, Inc. (Winter Park, FL), a business partner; and the Alaska Native Tribal Health Consortium (Anchorage, AK), a non-profit community partner interested in the development of an optical diagnosis device for use with its rural beneficiaries.<br/><br/>This award is partially supported by funds from the Directorate for Computer and Information Science and Engineering (CISE), Division of Computer and Network Systems (CNS)."
"1619072","The community ecology of viromes:  Virome assembly and pathogen transmission in a changing landscape","DEB","Ecology of Infectious Diseases","08/15/2016","08/19/2016","Kurt Vandegrift","PA","Pennsylvania State Univ University Park","Standard Grant","Samuel Scheiner","07/31/2021","$2,350,000.00","Richard Ostfeld, Peter Hudson, Barbara Han, Amit Kapoor","kurtvandegrift@gmail.com","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","BIO","7242","7242, 9179","$0.00","Emerging infectious diseases (EIDs) threaten global health and security, but when and where they will occur remains unfortunately hard to predict. In part, this may be because pathogens are usually studied in isolation. An alternative approach is to consider emerging pathogens as members of an ecological community of interacting microbes. The community ecology of viruses co-occurring in hosts -- the virome -- has yet to be explored as an ecological community even though viruses are known to be important emerging disease agents. The research supported by this award will address this deficiency by focusing on the viromes living inside two widespread species that live in close association with humans: the white footed mouse (Peromyscus leucopus) and the blacklegged tick (Ixodes scapularis).  Blacklegged ticks feed abundantly on white-footed mice, and the two species freely exchange microbes during the lengthy blood meals.  By exploring how the viromes of these two species are assembled and how they interact and flow between mice and ticks, the proposed research will reveal how viromes shape transmission of existing and potential pathogens. The research will be carried out in the northeast corridor of the United States, an area known to be an excellent location for studying virus emergence and the role played by white-footed mice and blacklegged ticks in the transmission of zoonotic pathogens (those transmitted from vertebrate animals to humans).  White-footed mice and blacklegged ticks serve as reservoir host and vector, respectively, of many emerging diseases of humans, including Lyme disease. Pilot studies have revealed that these same two species also harbor a community of previously undescribed viruses that may emerge as zoonotic diseases in the future. <br/><br/>The researchers take an explicitly ecological approach to understanding viromes of reservoir and vector hosts. The researchers will: (1) characterize the viromes of both mouse and tick; (2) determine patterns of virome assembly throughout the lifetimes of both mouse and tick; (3) determine how virus communities affect the transmission of several important vector borne zoonotic pathogens; (4) identify whether viromes affect differences between individual mice and ticks in their abilities to transmit known and new pathogens; and (5) determine whether viromes and transmission probabilities change along with changing abundance of mice and between suburban/urban and more rural habitats. Data generated by each of these specific aims will be modeled using statistical (machine learning) algorithms, which accommodate diverse data types and apply a model-free analytical approach. By using complex and high-dimensional empirical data describing hosts, vectors, viromes, and their shared natural environments, these algorithms can achieve superior pattern detection  (such as virome composition) and improved ability to make useful predictions (such as what combinations of traits of vector viromes and mammal hosts best predict virus transmission)."
"1707355","BIGDATA: Collaborative Research: IA: Large-Scale Multi-Parameter Analysis of Honeybee Behavior in their Natural Habitat","IIS","ADVANCES IN BIO INFORMATICS, Big Data Science &Engineering, EPSCoR Co-Funding","09/07/2016","02/16/2017","Remi Megret","PR","University of Puerto Rico-Rio Piedras","Standard Grant","Reed Beaman","08/31/2021","$446,628.00","","remi.megret@upr.edu","18 Ave. Universidad, Ste.1801","San Juan","PR","009252512","7877634949","CSE","1165, 8083, 9150","7433, 8083, 9150","$0.00","Honey bees exhibit highly complex behavior and are vital for our agriculture. Due to the rich social organization of bees, the overall performance and health of a bee colony depends both on a successful division of labor among the bees and on adequate reaction to the environment, which involves complex behavioral patterns and biological mechanisms. Much remains to be discovered on these matters as research is currently limited by our ability to effectively collect and analyze individual's behavior at large scale, out of the laboratory. The technology developed in this project will enable biologists to study the individual behavior of thousands of bees over extended periods of time. It builds on innovative algorithms and software to analyze big data collected from colonies in the field. Study of behavioral patterns at such scale will provide unique information to advance knowledge on biological processes such as circadian rhythms that influence bee behavior in addition to playing an important role in animals and humans. The models developed will help better understand factors involved in colony collapse disorder, thus guiding future research on threats to such an important pollinator. This work will be performed through the tight collaboration of a multi-disciplinary team of researchers to combine the latest advances in computer science and data science with expertise in biology.  It will provide the opportunity to train students from underrepresented minority on research at the intersection of these fields and to reach more than 600 undergraduate students, high school students, and the general public about how the Big Data approach can contribute to current scientific and ecological challenges.<br/><br/>The project will develop a platform for the high-throughput analysis of individual insect behaviors and gain new insights into the role of individual variations of behavior on bee colony performance. Joint video and sensor data acquisition will monitor marked individuals at multiple colonies over large continuous periods, generating the first datasets of bee activities of this kind on such a scale. Algorithms and software will be developed to take advantage of a High Performance Computing facility to perform the analysis of these massive datasets. Semi-supervised machine learning will leverage the large amount of data available to facilitate the creation of new detectors for parameters such as pollen carrying bees or fanning behavior, currently annotated manually. Predictive models and functional data analysis methods will be developed to find patterns in individual behavior based on multiple parameters and over large temporal scales. These advances are expected to help uncover mechanisms of individual variations previously unobservable. They will enable the first large scale biological study on the circadian rhythms of the bee based on the variations in behavior of individuals in multiple activities instead of reasoning on single activities or averages. Progress, datasets and software will be shared with the community on the project website (sites.google.com/a/upr.edu/bigdbee)."
"1633184","BIGDATA: Collaborative Research: IA: Large-Scale Multi-Parameter Analysis of Honeybee Behavior in their Natural Habitat","IIS","Hurricane Maria 2017, ADVANCES IN BIO INFORMATICS, Big Data Science &Engineering, EPSCoR Co-Funding","09/01/2016","03/14/2018","Jose Agosto-Rivera","PR","University of Puerto Rico-Rio Piedras","Standard Grant","Reed Beaman","08/31/2021","$409,744.00","Tugrul Giray","jose.agosto1@upr.edu","18 Ave. Universidad, Ste.1801","San Juan","PR","009252512","7877634949","CSE","075Y, 1165, 8083, 9150","1165, 7433, 8083, 9150","$0.00","Honey bees exhibit highly complex behavior and are vital for our agriculture. Due to the rich social organization of bees, the overall performance and health of a bee colony depends both on a successful division of labor among the bees and on adequate reaction to the environment, which involves complex behavioral patterns and biological mechanisms. Much remains to be discovered on these matters as research is currently limited by our ability to effectively collect and analyze individual's behavior at large scale, out of the laboratory. The technology developed in this project will enable biologists to study the individual behavior of thousands of bees over extended periods of time. It builds on innovative algorithms and software to analyze big data collected from colonies in the field. Study of behavioral patterns at such scale will provide unique information to advance knowledge on biological processes such as circadian rhythms that influence bee behavior in addition to playing an important role in animals and humans. The models developed will help better understand factors involved in colony collapse disorder, thus guiding future research on threats to such an important pollinator. This work will be performed through the tight collaboration of a multi-disciplinary team of researchers to combine the latest advances in computer science and data science with expertise in biology.  It will provide the opportunity to train students from underrepresented minority on research at the intersection of these fields and to reach more than 600 undergraduate students, high school students, and the general public about how the Big Data approach can contribute to current scientific and ecological challenges.<br/><br/>The project will develop a platform for the high-throughput analysis of individual insect behaviors and gain new insights into the role of individual variations of behavior on bee colony performance. Joint video and sensor data acquisition will monitor marked individuals at multiple colonies over large continuous periods, generating the first datasets of bee activities of this kind on such a scale. Algorithms and software will be developed to take advantage of a High Performance Computing facility to perform the analysis of these massive datasets. Semi-supervised machine learning will leverage the large amount of data available to facilitate the creation of new detectors for parameters such as pollen carrying bees or fanning behavior, currently annotated manually. Predictive models and functional data analysis methods will be developed to find patterns in individual behavior based on multiple parameters and over large temporal scales. These advances are expected to help uncover mechanisms of individual variations previously unobservable. They will enable the first large scale biological study on the circadian rhythms of the bee based on the variations in behavior of individuals in multiple activities instead of reasoning on single activities or averages. Progress, datasets and software will be shared with the community on the project website (sites.google.com/a/upr.edu/bigdbee)."
"1553610","CAREER: Scaling Forensic Algorithms for Big Data and Adversarial Environments","CNS","Secure &Trustworthy Cyberspace","05/01/2016","07/24/2020","Matthew Stamm","PA","Drexel University","Continuing Grant","Balakrishnan Prabhakaran","04/30/2021","$583,578.00","","mstamm@drexel.edu","1505 Race St, 10th Floor","Philadelphia","PA","191021119","2158955849","CSE","8060","1045, 7434","$0.00","Forged digital images or video can threaten reputations or impede criminal justice, due to falsified evidence.  Over the past decade, researchers have developed a new class of security techniques known as 'multimedia forensics' to determine the origin and authenticity of multimedia information, such as potentially falsified images or videos.  However, the proliferation of smartphones and the rise of social media have led to an overwhelming increase in the volume of multimedia information that must be forensically authenticated.  Forger's capabilities have also grown dramatically, as sophisticated editing software allows forgers to perform complex manipulations of digital images and videos. Researchers have recently demonstrated that an adversarial forger can design anti-forensic attacks capable of fooling forensic algorithms. By contrast, little multimedia forensics research has focused on improving the speed at which multimedia forensics techniques operate, particularly on large data sets. This research project is focused on scaling multimedia forensic algorithms to address these new challenges that have arisen due to the evolving technical and social landscape.  <br/><br/>The research project is focusing on three main aims: (1) Scaling forensic algorithms to meet big data challenges, (2) Scaling forensic algorithms to handle complex forgeries, and (3) Scaling forensics to meet increased adversarial capabilities.  To accomplish these aims, the research is drawing from a wide variety of fields such as signal processing, estimation theory, statistical hypothesis testing, machine learning, optimization theory, and game theory."
"1708553","BIGDATA: Collaborative Research: F: From Data Geometries to Information Networks","IIS","Big Data Science &Engineering","07/01/2016","04/28/2017","Mauro Maggioni","MD","Johns Hopkins University","Standard Grant","Sylvia Spengler","12/31/2019","$499,937.00","","mauro.maggioni@jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","CSE","8083","7433, 8083","$0.00","Big Data often results from multiple sources, giving collections that contain multiple, often partial, ""views"" of the same object, space, or phenomenon from various observers.  Extracting information robustly from such data sets calls for a joint analysis of a large collection of data sets.  The project is developing a novel geometric framework for modeling, structure detection, and information extraction from a collection of large related data sets, with an emphasis on the relationships between data.  While this approach clearly applies to data with a clear geometric character (e.g., objects in images), the work is also applied to datasets as diverse as computer networks (identifying common structure in subnets) and Massive Open Online Course homework data (automatically carrying grader annotations to similar problems in other students' homeworks).<br/><br/>The novel framework is based on the construction of maps between the objects under considerations (point clouds, graphs, images, etc...), and on the analysis of the networks of maps that result as a way of extracting information, generating latent models for the data, and transporting or inferring functional / semantic information. These tasks define a new field of map processing between data sets and require tool sets with new ideas from functional analysis, non-convex optimization, and homological algebra in mathematics, and geometric algorithms, machine learning, optimization, and approximation algorithms in computer science.  Sophisticated algorithmic techniques for attacking the large-scale non-linear optimization problems that emerge within the framework will also be investigated."
"1645237","EAGER: Using the ORCID ID and Emergence Scoring to Study Frontier Researchers","SMA","SciSIP-Sci of Sci Innov Policy, SCIENCE RESOURCES STATISTICS","10/01/2016","08/22/2016","Alan Porter","GA","Search Technology Inc","Standard Grant","Cassidy Sugimoto","03/31/2019","$149,920.00","Jan Youtie","aporter@searchtech.com","6025 The Corners Parkway","Peachtree Corners","GA","300923328","7704411457","SBE","7626, 8800","7626, 7916","$0.00","This project examines the usefulness of the Open Researcher & Contributor ID (ORCID) as a method for identifying journal article authors, and as a basis for detecting emerging scientific topics studied by these authors.  Authors of scientific works may use non-standard ways of reporting their names and affiliations in journal articles and other publications, change affiliations over their careers, or have names that are similar to those of other authors.  Such variations can make it difficult to correctly match authors with their publications and institutional affiliations.  Against an assortment of government or publisher registration methods and machine learning approaches to deal with these problems, ORCID stands out for its potential to extend across methods and nations with its open source approach.  Improved capabilities for identifying authors are vital in studying scientific mobility, networks, the contribution of authors to the emergence of new scientific topics, and other subjects in the Science of Science and Innovation Policy domain.  More needs to be understood about the strengths and weaknesses in ORCID coverage by country, field, and other areas for it to be useful in Science of Science and Innovation Policy studies.<br/><br/>The project uses two bibliometric methods to examine ORCID usage over time and by author characteristics.  First, the project uses comparative sampling by disciplinary area, and considers ORCID coverage within country, organization, and citation distribution categories in these disciplinary areas. Second, the project investigates the potential of ORCID in identifying emerging science-driven technologies by employing it in the development of an emergence indicator based on a semi-automated numerical scoring system.  The emergence indicator is designed to take topical terms with recent, sharp increases in publication and/or patent activity and examine them in the context of identified scholars to potentially highlight concentrations of research and development activity on emergent fields of science and technology.  The resulting intelligence is intended to be useful in informing government, commercial, and academic analyses of leading-edge players and other contributors to rising science and technology domains."
"1632051","PFI:BIC: iSee - Intelligent Mobile Behavior Monitoring and Depression Analytics Service for College Counseling Decision Support","IIP","PFI-Partnrships for Innovation, IIS Special Projects","09/01/2016","06/11/2020","Mi Zhang","MI","Michigan State University","Standard Grant","Jesus Soriano Molla","08/31/2021","$994,999.00","Anil Jain, Alex Liu, David Mohr, Jingbo Meng","mizhang@egr.msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","ENG","1662, 7484","1662","$0.00","Depression is the leading health issue on college campuses in the U.S. Today, college students are dealing with depression at some of the highest rates in decades. Unfortunately, university counseling centers (UCCs), which are the primary access points for students to receive mental health services, are facing significant challenges in meeting the increasing demands. Specifically, clinicians at UCCs still rely on patients' inaccurate and biased self-reported symptoms for depression assessment. In addition, UCCs provide mental health services only during working hours in clinical settings. The lack of service access when needed could leave patients floundering helplessly and lead to lifelong consequences. Furthermore, with tight budgets, clinicians at UCCs have not grown and some UCCs even downsized. As a consequence, more students did not receive timely treatment. This project focuses on designing and developing iSee, a smart device based behavior monitoring and analytics platform. iSee harnesses smartphones/wristbands to extend the reach of mental health care far beyond clinical settings and to deliver timely therapies when needed. Furthermore, the continuously tracked depression symptoms allow UCCs to be more accurately informed with the severity of each patient and thus reduces unnecessary visits so that clinician time can be better utilized.  If successful, iSee has the potential to enhance mental health services in thousands of colleges and universities, benefiting millions of college students. Although focusing on depression of college students, the technology can be extended to other mental health conditions such as anxiety, bipolar disorder, dementia, and schizophrenia; adapted to patients beyond college students; and deployed at other settings such as public hospitals and private clinics.<br/><br/>iSee consists of a smartphone/wristband sensing system running on the patient side to continuously and passively track patient's daily behaviors using onboard sensors; a behavior analytics engine using machine learning and causality analysis algorithms running on the cloud side to translate behavior sensor data into meaningful analysis results for identifying the patient's depression severity and revealing behavioral causes that lead to the mitigation or the deterioration of the patient's status; and a dashboard running on the clinician side to visualize behavior information as well as analysis results to help clinicians make clinical decisions and conduct treatment. The system would allow clinicians to access an objective, quantitative, and longitudinal record of patients' daily behavior to support evidence-based clinical assessment. This project involves a multi-disciplinary and cross-organizational team of researchers from Michigan State University (lead institution) and Northwestern University (Chicago, IL). The primary industry partner is Microsoft Research (Redmond, WA), which is a large business company in U.S. Michigan State University Counseling Center (East Lansing, MI), which will be the test bed for the integration and evaluation of the iSee smart service system. Finally, the broader context partners include the MSU Office of the Vice President for Student Affairs and Services and MSU Technologies (East Lansing, MI).<br/><br/>This award is partially supported by funds from the Directorate for Computer and Information Science and Engineering (CISE), Division of Information and Intelligent Systems (IIS)."
"1618117","TWC: Small: Collaborative: Reputation-Escalation-as-a-Service: Analyses and Defenses","CNS","Special Projects - CNS, Secure &Trustworthy Cyberspace","07/01/2016","04/17/2018","Haining Wang","DE","University of Delaware","Standard Grant","Sol Greenspan","06/30/2020","$232,000.00","","hnw@vt.edu","210 Hullihen Hall","Newark","DE","197160099","3028312136","CSE","1714, 8060","025Z, 7434, 7923, 9150, 9178, 9251","$0.00","Living in an age when services are often rated, people are increasingly depending on reputation of sellers or products/apps when making purchases online. This puts pressure on people to gain and maintain a high reputation by offering reliable and high-quality services and/or products, which benefits the society at large. Unfortunately, due to extremely high competition in e-commerce or app stores, recently reputation manipulation related services have quickly developed into a sizable business, which is termed Reputation-Escalation-as-a-Service (REaaS). As REaaS attacks grow in scale, effective countermeasures must be designed to detect and defend against them.<br/> <br/>This research addresses REaaS from two aspects. First, it aims to understand the economics of  REaaS by conducting empirical studies of  e-markets. Second, it aims to develop defensive measures, which involve both technical approaches and market intervention.  The technical approaches focus on detection of REaaS from e-markets, and novel detection techniques will be developed using content analysis, machine learning, social ties, and graph theory. For market invention, after a holistic analysis of REaaS, this research aims to identify its bottleneck (the weakest link) and also measure the efficacy of intervention. The outcome of this data-driven security research will enhance security education with labs based on social-economic data analysis. The success of this research will attract more attention of industry practitioners, government sectors, and academia to jointly tackle the REaaS problem."
"1637360","AitF: Spectral Methods in the Field: New Tools for Discovering Latent Structure in Societal-Scale Data","CCF","Algorithms in the Field","09/01/2016","01/18/2017","Sham Kakade","WA","University of Washington","Standard Grant","Tracy Kimbrel","08/31/2020","$600,000.00","Joshua Blumenstock","sham@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7239","","$0.00","The rapid rise in the use of mobile phones, social media, and digital sensors has created opportunities to observe and understand the rapidly changing structure of populations around the world. In particular, the data captured on these population-scale digital networks can inform policy-relevant questions about the evolving nature of societies around the world. For example, policymakers would like to know how idiosyncratic violence impacts the resilience of local communities, how the presence of foreign troops changes local patterns of interaction, and how draughts and natural disasters affect feelings of national solidarity. Unfortunately, appropriate models and algorithms do not exist to make sense of evolving, societal-scale data. This project will develop scalable algorithms to help make sense of real-world networked sensor data, with the potential for significant impact in the increasingly connected global society.<br/><br/> The technical focus of this project is on adapting recent algorithmic advances in the theoretical computer science and machine learning literatures to real-world, societal-scale network data. This approach will leverage recent advances in spectral methods, which provide provably efficient algorithms for estimating hidden structure in data and improve upon the state of the art in three important ways. The first objective is to adapt and scale current spectral models to real-world datasets with millions of interconnected actors, which have weighted and directed edges with heavy-tailed degree distributions. The second goal is to translate existing methods to dynamic regime, to address the non-stationary nature of real-world data. The third goal is to characterize the computational and statistical limits of what can be achieved with these models. The algorithms and tools developed through this research will be made available to the broader academic community via open source code repositories such as GitHub and BitBucket."
"1631776","NRT-DESE: NRT in Integrated Computational Entomology (NICE)","DGE","NSF Research Traineeship (NRT), Project & Program Evaluation","09/15/2016","09/08/2016","Eamonn Keogh","CA","University of California-Riverside","Standard Grant","John Weishampel","08/31/2021","$2,721,142.00","Daniel Jeske, Christian Shelton, Erin Wilson Rankin, Anupama Dahanukar","eamonn@cs.ucr.edu","Research & Economic Development","RIVERSIDE","CA","925210217","9518275535","EHR","1997, 7261","9179, SMET","$0.00","This National Science Foundation Research Traineeship (NRT) award to the University of California, Riverside (UCR) will enable a team of investigators from Computer Science/Engineering and Entomology/Life Sciences to prepare the next generation of scientists and engineers to exploit the unreasonable effectiveness of data to understand insects by integrating the disciplines of computer science and entomological biology. The NRT in Integrated Computational Entomology (NICE) will train students to be at the forefront of science in computing for biological domains, providing biological scientists a foundation in computing techniques and engineers an understanding of critical entomological and ecological issues.  The project anticipates training at least forty (40) MS and PhD students, including twenty (20) funded PhD trainees from the life sciences, computer science and engineering. <br/><br/>The project will be the first program of its kind, anywhere in the world, and will meet high standards for innovation while offering a structure for demanding training in entomology/life sciences integrated with computational techniques in machine learning, data mining, and statistics. The NICE program recognizes and advances Computational Entomology as an emerging interdisciplinary field. Computational Entomology as a discipline recognizes that entomological and ecological problems generate enormous amounts of data, and that fully exploiting this data will require individuals whose knowledge spans two otherwise disparate fields. The training and research structure of the proposed project seeks to bridge large gaps in training, language, approach, perspective and knowledge that continue to divide the engineering/informatics and life sciences disciplines. Through coursework and joint projects with government agencies and companies, trainees will experience the translation of research outcomes into implemented public policy or agricultural/medical products and services. This project will scale to include graduate student trainees at UCR receiving NRT support and those not receiving funding, and will be sustainable at UCR as the new curriculum will become incorporated across the participating departments and degree programs. This project will also serve as a replicable Computational Entomology education and training model for other institutions. <br/><br/>The NSF Research Traineeship (NRT) Program is designed to encourage the development and implementation of bold, new potentially transformative scalable models for STEM graduate education training. The Traineeship Track is dedicated to effective training of STEM graduate students in high priority interdisciplinary research areas, through the comprehensive traineeship model that is innovative, evidence-based, and aligned with changing workforce and research needs."
"1563372","A Novel Dynamically Coupled Storm Surge Hazard-Infrastructure Model for Effective Real-Time Risk-Informed Decision Making","CMMI","HDBE-Humans, Disasters, and th","08/15/2016","05/24/2019","Abdollah Shafieezadeh","OH","Ohio State University","Standard Grant","Walter Peacock","07/31/2021","$502,532.00","Ethan Kubatko, Noah Dormady","shafieezadeh.1@osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","ENG","1638","036E, 041E, 042E, 1057, 116E, 9178, 9231, 9251, CVIS","$0.00","Coastal areas in the US face substantial risk from storm surge. Data from modeling efforts can provide crucial information to decision makers to act against these risks, but available models are limited in scope. Current fragility-based models for flood defense systems are primarily focused on a single mode of failure without consideration of causal relationships and temporal correlations among various failure modes. Failure assessment is treated as a snapshot in time neglecting the time evolution of failure processes. The adoption of surge hydrographs in current methods to independently determine failure probabilities from reliability models neglects the impact of the performance of geo-structures on spatio-temporal surge response. Moreover, we lack an accurate conceptualization of how this informational shortfall impairs decision makers in making critical judgments about storm surge risk and infrastructure investment. This work will provide for the development of the next generation in storm surge and fragility models  and will provide for an experimental validation and assessment of the models' effects on decision making, as follows: 1) Development of an adaptive-resolution storm surge model that responds to the changing state of flood protection systems; 2) Derivation of novel time-dependent, multi-dimensional fragility models of geo-structures, fully integrated with the storm surge model; 3) Development and utilization of human-in-the-loop experimentation to validate and test the effects of these models on real-time decision making, and 4) Creation of enhanced educational and research opportunities for students and teachers, along with dissemination of research knowledge to critical stakeholders. <br/><br/>The research performed under this project will have a significant impact on the development of the next generation of storm surge models that are fully integrated with time-dependent fragility models to improve forecasting capabilities of flooding scenarios. The research will also improve understanding of how decision makers utilize storm risk assessment information to make critical decisions. Ultimately, this research will lead to more informed decisions about catastrophic risk and infrastructure failure (e.g., evacuation decisions, search and rescue operations, infrastructure investment, and pre-, during, and post-event planning). The educational plan will provide for integrated new curriculum in infrastructure modeling, resilience and risk analysis. Moreover, the educational plan will enhance the self-efficacy of K8 teachers to teach engineering in classrooms and help engineering students to develop pedagogical skills. Results will be disseminated to (and validated with) key federal and regional stakeholders (e.g., Dept. of Homeland Security, FEMA, US Coast Guard) as well as industry partners.  The results from this project will provide a significant improvement in storm surge modeling through the development of a novel, dynamically coupled modeling system consisting of hydrodynamic and fragility model components. The stochastic finite difference models combined with machine learning techniques will enable generation of a novel class of multi-dimensional fragility surfaces that will enhance our understanding of various failure processes and characterize time evolution of failure probabilities. The coupled surge/fragility model will adapt the mesh resolution in response to changing conditions of the flood protection systems, resulting in improved forecasting capabilities. The experimental analysis will provide for an assessment of how these modeling capabilities improve real-time decision making."
"1616575","TTP: Small: Network-Level Security Posture Assessment and Predictive Analytics: From Theory to Practice","CNS","Secure &Trustworthy Cyberspace","08/15/2016","05/29/2020","Mingyan Liu","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Robert Beverly","07/31/2021","$515,982.00","Manish Karir","mingyan@eecs.umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","8060","025Z, 7434, 7923, 9102, 9178, 9251","$0.00","This project addresses the following two key questions in cyber security: (1) how is the security condition of a network assessed, and (2) to what extent can we predict data breaches or other cyber security incidents for an organization. The ability to answer both questions has far-reaching social and economic impact. Recent data breaches such as those at Target, JP Morgan, Home Depot, Office of Personnel Management (OPM), and Anthem Healthcare, to name just a few, highlight the increasing social and economic impact of such cyber security incidents. Often, by the time a breach is detected, it is too late and damage has already occurred. Consequently, being able to predict such incidents accurately can greatly enhance an organization's ability to put preventative and proactive measures in place.  The answers to these questions also have implications on public policy design - not only for the security policies themselves, but also for related incentive mechanisms.   Such mechanisms might be aimed at encouraging adoption of better security policies and cybersecurity frameworks, including cyber insurance, liability limitation, and rate recovery among others. Presidential Policy Directive (PPD) 21, on Critical Infrastructure Security and Resilience, encourages efforts to strengthen and maintain secure, functioning, and resilient critical infrastructure. Understanding the potential attack vector presented by an enterprise or organization is a crucial part of achieving this goal.<br/><br/>This project follows a comprehensive agenda aimed at transitioning to practice technologies developed by the research team in the domain of quantitative assessment of the security posture at both a network and an organizational level. The use of such assessments enables  more accurate forecasting of cyber security incidents. The technological innovation is a sound quantitative framework that combines a large collection of cybersecurity data, novel data processing methods, advanced machine learning techniques, and extensive cybersecurity domain expertise.   The resulting framework produces accurate predictions of security incidents for a given organization, thereby providing tangible information and crucial input for decision makers such as an insurance underwriter, or an enterprise customer seeking to validate vendor specifications."
"1552364","RAPID: Social Media during Rapid Transition","IIS","HCC-Human-Centered Computing","01/01/2016","08/13/2015","Amy Bruckman","GA","Georgia Tech Research Corporation","Standard Grant","William Bainbridge","12/31/2017","$180,780.00","Eric Gilbert","asb@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7367","7367, 7914","$0.00","This research will examine the impact of online social media, when a nation first becomes open to this new technology-based form of communication.  Relatively few countries in the world lack social media penetration, but from time to time additional nations offer global connectivity to their citizens.  Cuban citizens will soon get dramatically more access to the Internet. How will their increased access to the global network change their view of the world? What can this teach us about the impact of social media on the developing world? What can it teach us about the impact of social media on society more generally?  This kind of change forms a unique natural experiment that can reveal fundamental characteristics of social media in society.<br/><br/>In this work, researchers will perform in-depth, semi-structured qualitative interviews with citizens of a newly connected society about their use of social media.  Interviews will be conducted in their language via text-based chat and audio (Skype and phone) wherever possible. With permission, subjects' online postings will also be analyzed. Data will be analyzed via a grounded theory approach. The research will focus on a nation where changes have not yet begun to affect ordinary citizens. As a result, the research team will be able to gain insights into pre-existing attitudes and hopes for the future. Over this critical coming year, they will be able to track how those attitudes change.  This qualitative work is valuable in itself and will also form the basis for a machine learning study designed to automatically track changes in public opinion as expressed via social media."
"1617820","CHS: Small: Large-Scale Examination of the Impact of Shocks on Crowd Attributes and Performance in Collaborative Volunteering Systems","IIS","HCC-Human-Centered Computing","09/01/2016","05/08/2017","Daniel Romero","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","William Bainbridge","08/31/2020","$515,463.00","Lionel Robert, Ceren Budak","drom@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7367","7367, 7923, 9251","$0.00","This project will advance understanding of how groups of volunteer contributors to online resources perform in the face of sudden, unexpected events related to their work.  These volunteer contributors (collectively, ""crowds"") produce valuable resources such as Wikipedia articles, data for citizen science projects, and open source software.  However, though there has been prior research on what aspects of crowds and situations lead to high quality resources, such research normally assumes that the people, groups, and especially situations are relatively stable.  In practice, situations often encounter sudden changes, or ""shocks"": the death of a celebrity or a world event can affect Wikipedia articles related to it, while a software project might release a new version or discover a critical bug.  In this project, the investigators will use the public history of Wikipedia articles and open source projects stored on the GitHub website to analyze how crowds react to shocks, and how that affects the resources they create.  To do this they will use theories of individual and group behavior to measure meaningful attributes of both the crowds and the resources they work on, then use data analysis techniques to understand (1) how crowds change during shocks; (2) what attributes of crowds predict high resilience and resource quality in the face of shocks; and (3) how these effects change depending on the type of shock that is experienced.  The insights gained from the work will also lead to design recommendations for people who manage the software and communities that enable crowds to create these socially valuable resources.<br/><br/>The work will start by constructing features of crowds and the resources they produce.  The focus will be on features that prior empirical work in Wikipedia and work from organization theory suggest will be relevant to performance in collaborative crowdsourcing systems.  For crowds, these include elements about team composition and participation behavior including experience, diversity, and work balance; about the amount and tone of team coordination around creating the resources; and about the inferred network structure of the collaborators based on individuals' communication patterns with each other.  In the case of resources, it includes attributes including their internal and external popularity, and rated or estimated current quality.  The investigators will use these features to analyze how the crowds perform when facing a variety of kinds of shocks, including changes in resource quality, worker status, and relevant external events.  To do this, the investigators will develop algorithms to detect times when a crowd has experienced a shock, then use propensity score matching on attributes such as resource quality and crowd size to find comparison sets of similar crowds that have not experienced a shock.  They will then use machine learning classifiers and segmented regression analysis techniques to analyze changes in the composition and behavior of the crowd around resources after shocks occur, relative to how the crowd behaves around the comparison resources.  Once these models have been developed, the team will apply them to questions of predicting both the anticipated resilience of a crowd to a shock and the potential occurrence of shocks internal to the collaboration system."
"1625671","MRI: Acquisition of a Scanning Electron Microscope for Real-time Studies of Novel Materials Processes and Functionality","DMR","Major Research Instrumentation","09/01/2016","08/24/2016","Emmanuelle Marquis","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Leonard Spinu","08/31/2019","$724,730.00","John Mansfield, Bart Bartlett, Rebecca Peterson, Henry Sodano","emarq@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","MPS","1189","1189","$0.00","Researchers at the University of Michigan, Wayne State University, Eastern Michigan University, and Michigan State University want to watch what happens to materials when they are poked in real time. Their common goal is to understand how structure and chemistry of materials affect their properties, discover new phenomena, and design new materials. In doing so they want to share their excitement about materials science through inclusive classroom and outreach activities. The cornerstone instrument is a variable pressure field emission gun scanning electron microscope equipped with a unique set of complementary imaging and analysis modalities that will transform the breadth and depth of materials research in southeastern Michigan. The microscope will be housed in the Michigan Center for Materials Characterization, a University of Michigan shared user facility serving academic and industrial users from the greater Detroit area and State of Michigan. The faculty members, through their multidisciplinary expertise, will create a collaborative environment where experiments merge with modeling and computation, where an entire undergraduate classroom addresses the challenge of how to mine complex information from a microscopy dataset, and where elementary-school-age children operate a scanning electron microscope themselves and get to experience the exhilaration of scientific discovery.<br/><br/>Researchers in the Southeastern Michigan will take advantage of the unique combination of complementary signals and time-resolved probing and testing offered by the instrument, to perform novel experiments integrating data analysis and modeling approaches. The microscope will enable investigation of a wide range of materials (metals, semiconductors, polymers, biomaterials, oxides, bulk, thin films, nanostructures) and therefore a wide range of materials applications. The novelty is in the combination of complementary detectors allowing simultaneous imaging and analysis in order to, for instance, quantify kinetics of phase transformation under applied thermal load or deformation, quantify defects in III-N devices or impurities in geological materials using RS and mono-CL, or image oxidation or corrosion of surfaces via thermal and environment control. The experimental work enabled by the tool will be integrated with signal processing efforts to develop new data analytic tools, statistical algorithms, and advances in predictive modeling by leveraging machine learning and data mining. The microscope will be critical to the continued education of our undergraduate and graduate students via its use for in-depth classroom teaching and research training in state-of-the-art characterization techniques. It will also be used to develop more versatile and scalable teaching opportunities for a large body of students, post-docs, and for external users. The PIs and senior personnel will continue to strengthen and expand activities involving underrepresented minorities and stimulate STEM excitement among young students."
"1627993","NSF/SBE-BSF: Neural patterns underlying the development of planning in action production and anticipation in action perception","BCS","DS -Developmental Sciences","09/15/2016","07/27/2016","Karen Adolph","NY","New York University","Standard Grant","Peter Vishton","08/31/2019","$293,783.00","","kea1@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","SBE","1698","014Z, 1698, 9251","$0.00","A remarkable aspect of motor skill is the ability to plan actions flexibly and purposefully using a variety of objects. We achieve this by planning our initial contact with the object with the end goal in mind, even when the end goal requires multiple steps to be achieved. Thus, the process of action planning involves integration of perception, cognition, and motor behavior. Presumably, a similar integrative process occurs when we anticipate other people's goals while observing them perform actions. Previous work shows that action planning begins in infancy and improves with age, but little is known about the accompanying brain activity that underlies these age-related improvements. This interdisciplinary study combines concepts, methods, and analytic techniques from developmental psychology, neuroscience, and computer science to understand age-related changes in action planning while children 1) perform an action with multiple steps to the goal and 2) while they observe someone else perform the action. The research uses a novel combination of recording methods for children: Video, eye tracking, motion tracking, and electroencephalography (EEG) will be recorded simultaneously. The methods and data will advance the field through open sharing of the research videos and physiological data in the Databrary repository. Algorithms and analysis techniques will be shared in the Open Science Framework. <br/><br/>The investigators combine behavioral measures, neural activity recordings, and machine-learning techniques in order to understand how children and adults 1) perform complex motor tasks that involve anticipation of the end-goal and 2) passively observe others performing tasks that involve multi-step action planning in anticipation of the end-goal. Analyses of action performance will focus on the neural correlates of behavior at different stages of planning and will assess whether neural activity prior to beginning a movement can predict trial-to-trial behavioral variability in young children's ability to plan. Analyses of action observation will investigate age differences in neural activity while observing others performing actions that do, or do not, show evidence of long-range planning. The investigators will also compare the neurophysiological signatures of anticipation during passive action observation and during action performance. These findings will inform our understanding of the development of action planning across childhood. The multi-modal recording methods and the advanced multivariate analytic techniques will pave the way for research in STEM and other disciplines to explore developmental changes in children's brain and behavior.<br/><br/>This award is made as part of the NSF/BSF Opportunity for Collaborations in Economics and Psychology."
"1619346","NeTS: Small: Designing Agile and Scalable Self-Healing Functionalities for Ultra Dense Future Cellular Networks","CNS","Networking Technology and Syst, EPSCoR Co-Funding","10/01/2016","07/15/2016","Ali Imran","OK","University of Oklahoma Norman Campus","Standard Grant","Alexander Sprintson","09/30/2020","$500,012.00","","ali.imran@ou.edu","201 Stephenson Parkway","NORMAN","OK","730199705","4053254757","CSE","7363, 9150","7923, 9150, 9178, 9251","$0.00","Cellular networks are subject to cell outages of several types.  Complete outages are often caused by equipment malfunctions; partial outages or degraded performances are often caused by parameter mis-configurations. Outage rates are proportional to cell density and base station complexity. Both of these factors have been consistently on rise from 1G to 4G. Current semi-automated approaches to cell outage management have proven inadequate and highly inefficient even for today's network, and are surely unfeasible for future cellular networks marked by ultra-dense cell deployment and mounting complexity. If no intervening measures are taken, cell outage management may become a primary challenge for future cellular networks, such as 5G. To remedy this, this project will develop an Advance Cell Outage Management (ACOM) framework for fully automating cell outage detection and compensation in future ultra-dense, heterogeneous cellular networks. ACOM will be built by developing and integrating three novel solutions: 1) Autonomous Macro Cell Outage Detection and root cause analysis (MOD); 2) Autonomous Small Cell Outage Detection and root cause analysis (SOD); and 3) Autonomous Heterogeneous Cell Outage Compensation (HOC). In ACOM, the outage detected and diagnosed by MOD and SOD will be exploited by the optimization process in HOC to transform future ultra-dense, heterogeneous cellular deployments into fully self-healing systems. <br/><br/>Key distinct features of ACOM will include agility, stability and flexibility to accommodate varying user densities, cell sizes, and radio channel conditions, and ultra-dense deployments of small cells. The proposed plan lies at the nexus of profiling, anomaly detection, prediction, sparse matrix completions, multidimensional scaling, and dynamics handling. It employs machine learning, optimization, and game theory. Project outcomes will be validated using data from real network data while leveraging a full scale outdoor 5G testbed. If successful, this project is certain to make strong impact on all aspects of evolving digital society that count on reliability of cellular networks. Another key impact of this project is that it offers strong workforce training in a highly sought-after multi-disciplinary skill set needed to conduct proposed research, while ensuring participation of women and other underrepresented groups, and K-12 outreach. The project will also leverage collaboration with national and international stake holders in the cellular ecosystem to maximize its impact on standardization."
"1551994","CAREER: CDS&E: Predictive Discovery of Complex Reaction Mechanisms","CHE","Chem Thry, Mdls & Cmptnl Mthds","01/15/2016","01/11/2017","Paul Zimmerman","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Evelyn Goldfield","12/31/2020","$625,000.00","","paulzim@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","MPS","6881","1045, 7433, 8037, 8084, 8396, 8399, 9216, 9263","$0.00","Paul Zimmerman at the University of Michigan is supported by the Chemical Theory, Models and Computational Methods Program and the Computational and Data-Enabled Science and Engineering (CDS&E) Program to develop new tools to predict the outcome of chemical reactions. Computational chemistry has a long history of using the principles of quantum mechanics to create tools which provide detailed, accurate explanations for a wide variety of chemical processes. Many of these tools have reached sufficient accuracy that they can be applied to discover chemical reactions without requiring prior insight from experiment. Unfortunately, the high computational cost of these methods has prevented their broad use in predicting chemical reactivity, especially in cases where the chemistry is highly complex or poorly understood. This grant supports the development of fully computational, low-cost and highly accurate methods that are able to predict the outcome of chemical reactions starting only from the feedstock molecules. The application focus of these methods is catalytic reactions, providing a highly useful tool for research into chemistries that can be applied at an industrial level to create high-value chemical products. These tools are available to the wider computational chemistry community, enabling maximized impact of the developments to a great number of problems in chemical reactivity. Due to the diversity of potential applications for this research, the students involved in this project gain not only algorithm development abilities, but also fundamental insights into processes governing molecular behavior, cutting-edge computational research experience, and problem-solving abilities that are needed to address the challenges of the 21st century. The proposed methods are transformed into educational strategies that merge introductory laboratory exercises with real-world research, starting with a pilot study in honors organic chemistry. <br/><br/>The research program builds upon work by the Zimmerman group that shows chemical reactions can be described in terms of a small number of localized, anharmonic reaction coordinates. These reaction coordinates consist of interatomic distances, angles, and torsions, and are a reliable, transferable basis for describing atomic motion. By employing advanced single-ended chain-of-states optimization algorithms which search along these coordinates for plausible chemical reactions, this research methodology can efficiently and reliably predict reactive events without guidance from chemical intuition. The main objective of ongoing work is to expand this method to cover transition metal elements, enable efficient conformation searches in systems with floppy degrees of freedom, and develop machine learning algorithms to automatically process chemical data and provide great enhancements in computational efficiency. In sum, these new reaction search techniques enable predictive reaction discovery in a wide variety of large, highly complicated chemical systems where chemical knowledge alone is not yet sufficient to make accurate predictions. These methods are being applied to challenging cases in catalysis where uncharacterized side reactions are severe impediments to efficient product formation. Ongoing discovery of these undesired reaction pathways lead to the chemical insight required to improve reaction selectivity and catalyst stability.These tools allow beginning chemistry students to hypothesize and evaluate reactions in silico, resulting in a means for students to perform research at an early stage of their studies. Such pedagogical tools are distributed to educators outside of the University of Michigan to maximize their impact."
"1619261","STARSS: Small: GC@Scale: Synthesis, optimization, and implementation of Garbled Circuits for Scalable Privacy-Preserving Computing","CNS","Secure &Trustworthy Cyberspace","10/01/2016","08/18/2016","Farinaz Koushanfar","CA","University of California-San Diego","Standard Grant","Sandip Kundu","09/30/2020","$298,231.00","","farinaz@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","8060","021Z, 7434, 7923, 8225, 9102","$0.00","Computing on sensitive data is a standing challenge central to several modern-world applications. Secure Function Evaluation (SFE) allows mistrusting parties to jointly compute an arbitrary function on their private inputs without revealing anything but the result. The GC@Scale project focuses on novel scalable methods for addressing SFE, which directly translate to stronger cryptography and security for myriads of tasks with sensitive data. The applications are wide reaching and include privacy-preserving processing of medical, genome, and biometric data, as well as personal, government, and industrial cloud computing. The project includes an ambitious educational program that targets both undergraduate/ graduate students, and also addresses issues related to outreach.<br/><br/>The concept of SFE using Garbled Circuits (GC) was introduced by Yao. Despite a decade of research in GC implementation and several key progresses, scalability of the available methods has been hampered by the circuit representation as a directed acyclic graph, and software-level local logic optimizations. GC@Scale leverages PI's recent work, which has changed the SFE landscape by viewing GC generation as an atypical sequential logic synthesis. The project plans to advance the understanding and enable expanded exploration of SFE methodologies, while simultaneously enriching the theory, practice, and tools for logic design, synthesis, mapping and optimization. The proposed plan includes: (i) design and FPGA implementation of an efficient general purpose Garbled Processor for secure computation; (ii) Creating the challenging application-specific GC matching and search engines with a higher than linear complexity. (iii) Devising new custom SFE engines for Machine Learning tasks."
"1651276","EAGER:  Identifying Security Critical Properties of a Processor","CNS","Secure &Trustworthy Cyberspace","09/15/2016","09/13/2016","Cynthia Sturton","NC","University of North Carolina at Chapel Hill","Standard Grant","Sandip Kundu","08/31/2018","$150,000.00","","csturton@cs.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","8060","7434, 7916, 9102","$0.00","This project focuses on shoring up the security vulnerabilities that exist in computer processors. Just like in software, bugs in hardware present vulnerabilities that can be exploited by determined attackers. Prior work has developed a method whereby the processor monitors itself and sends an alert to software whenever dangerous, anomalous behavior is observed. The question of what constitutes dangerous behavior is an open one, and tackling it is the goal of this research. In doing so the project has the potential to make self-monitoring processors practical and efficacious, significantly advancing the state of the art in protecting hardware from malicious attack.<br/><br/>The self-monitoring processor works by encoding in hardware the properties that a processor should always maintain and then throwing an exception to software if one of the properties is ever violated. In this project, the researchers develop a semi-automated methodology and tool-chain to identify and build the security-critical properties encoded in a hardware design language. The project uses a set of already-patched bugs of a processor design to automatically create an initial set of security-critical properties, and machine learning techniques to infer an additional set of properties that are not tied to any particular known vulnerability, yet are critical to security."
"1618837","TWC: Small: MIST: Systematic Analysis of Microarchitectural Information Leakage on Mobile Platforms","CNS","Secure &Trustworthy Cyberspace","08/01/2016","07/26/2016","Thomas Eisenbarth","MA","Worcester Polytechnic Institute","Standard Grant","Sandip Kundu","07/31/2019","$499,999.00","Berk Sunar","teisenbarth@wpi.edu","100 INSTITUTE RD","WORCESTER","MA","016092247","5088315000","CSE","8060","7434, 7923","$0.00","Smart phones have permeated all facets of our lives facilitating daily activities from shopping to social interactions. Mobile devices collect sensitive information about our behavior via various sensors. Operating systems (OS)enforce strict isolation between apps to protect data and complex permission management. Yet, apps get free access to hardware including CPU and caches. Access to shared hardware resources result in information leakage across apps. Microarchitectural attacks have already proven to succeed in stealing information on PC and even on virtualized cloud servers. This project (MIST) quantifies the vulnerability of mobile platforms to microarchitectural attacks and develops countermeasures. <br/><br/>MIST systematically explores which resources enable these attacks on the mobile platform, identifies mechanisms that are essential to an attack, and quantifies the amount of information obtainable from given resources. By making use of machine learning techniques, methods for detecting malicious code exploiting microarchitectural leakage are developed. To remedy microarchitectural attacks, MIST explores countermeasures that can manage resources in a way that they are no longer exploitable by side channels. In addition, tools that help app developers prevent leakage when writing code processing sensitive information are provided. The studied detection and prevention techniques apply to a wide range of interactions as experienced in mobile computing today. Thereby, MIST helps to secure personal information stored on mobile platforms with immediate benefits to virtually all mobile platform users. Many of the envisioned Internet of Things hubs build on the same platforms, further increasing the impact of this research."
"1608040","SHF: Small: Bridging the Gap Between Global and Detailed Routing of Integrated Circuits","CCF","Software & Hardware Foundation","06/15/2016","04/28/2017","Azadeh Davoodi","WI","University of Wisconsin-Madison","Standard Grant","Sankar Basu","05/31/2020","$416,006.00","","adavoodi@wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","7798","7923, 7945, 9251","$0.00","As semiconductor technology enters the sub-10nm era, it has significantly complicated the computer-aided design (CAD) of integrated circuits (ICs). Today's designer must deal with increasingly complex rules, imposed by the manufacturing facility so as to ensure that the chip can be successfully fabricated. This project aims to develop CAD tools that incorporate major design rules within a key, higher stage of the design flow, namely the global routing stage. By integrating these rules into global routing, the project aims to accelerate the progress of IC design within the sub-10nm regime. Other broader goals of the project include: public release of a version of the tools developed to facilitate academic research, technology transfer to major semiconductor companies, integration with two existing courses at UW-Madison, and training graduate and undergraduate students to gain the necessary skills for today's IC-CAD job market.<br/><br/>The project tasks are centered around investigating design-rule-aware models at the global routing level of abstraction, including machine-learning techniques for better pin-access planning. In addition, the project will identify new routing challenges caused by modern design rules and will investigate appropriate optimization strategies, including graph-based and mathematical programming techniques, to address them."
"1546428","BIGDATA: IA: Exploring Analysis of Environment and Health Through Multiple Alternative Clustering","IIS","Big Data Science &Engineering","01/01/2016","06/12/2018","Jennifer Dy","MA","Northeastern University","Standard Grant","Wendy Nilsen","12/31/2020","$876,648.00","David Kaeli, April Gu","jdy@ece.neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","8083","7433, 8083, 9251","$0.00","While many disciplines have become increasingly exploratory given that large-scale and multi-source data collection has become prevalent, we find that volumes of data that were carefully collected and studied to answer project-specific questions, are neglected and unstudied, even if they hold key answers to tomorrow's questions. This project addresses this challenge by developing novel data analysis, alternative clustering, data visualization and acceleration solutions to enable exploration and identification of connections hidden in diverse data sets, leading to new discoveries and knowledge. In particular, the data analysis algorithms will be applied to a large dataset taken from an ongoing National Institute of Environmental Health Sciences (NIEHS) project that is assessing the impact of water-borne pollutants on premature birth rates in Puerto Rico.  The exploratory analysis will focus on discovering the unknown underlying environmental factors and processes that may more broadly impact health and the environment.  As such, this study promotes progress in data science, environmental science and health.  Note that this project will address women?s health in an under-served population.  In addition, this project supports education through graduate research support, development of inter-disciplinary tutorials, and creation of a new undergraduate class that addresses the intersection between machine learning approaches and parallel computing.<br/> <br/>The environmental health data comprises of multiple heterogeneous sources with varying temporal and spatial resolutions: mass spectrometer readings to identify targeted and non-targeted compounds in well and tap water, participant surveys detailing personal care and household products use in the home, and analyzed placental, blood and urine samples. Such complex data challenges traditional clustering algorithms in the following ways. The first challenge is in defining the appropriate similarity measure for each type of data source. The second step involves how to integrate information from these multiple sources for clustering. The third challenge is that in exploratory analysis, the solution found may not be what the analyst is looking for. How can one discover alternative solutions given this knowledge? In real world applications, data can often be interpreted in many different ways. However, existing multi-source fusion methods can only find a single solution. This study will develop new alternative clustering approaches, exploring multiple heterogeneous information sources. The project will deliver both visual and scalable solutions to enable sifting through the mountains of data efficiently. Moreover, the project will produce parallel libraries within Spark, and demonstrate the power of these new methods to this environmental health application."
"1549461","SBIR Phase I:  Scaling Operational Intelligence Graphs with GPU Clouds","IIP","SBIR Phase I","01/01/2016","06/29/2016","Leo Meyerovich","CA","Graphistry, Inc.","Standard Grant","Peter Atherton","12/31/2016","$179,168.00","","leo@graphistry.com","610 Shotwell St, Apt 3","San Francisco","CA","941102640","4155335329","ENG","5371","163E, 5371, 8032","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project will, most directly, be to provide institutions such as banks and national defense agencies with visibility into their graph-centric security and operations data, and thereby improve their robustness and resilience. Farther out, enabling visualization of larger graphs will also aid non-IT uses, such as to help financial analysts understand markets, marketing and sales teams understand customers, and precision medicine researchers understand gene interactions. Furthermore, generalizing the underlying GPU cloud infrastructure to scale interactive visualizations beyond graphs will help even more types of analysts comprehend data from an even wider variety of data sources. Likewise, generalizing the proposed GPU cloud infrastructure will also aid non-visual analytic tasks, such as machine learning over big data.<br/><br/>This Small Business Innovation Research Phase I project's goal is to scale visual graph analysis to enterprise level security analytics. Modern graph visualization tools handle at most 50,000 nodes, but many enterprises manage over a million devices and services. The proposed innovation scales graph visualizations to 1-2 magnitudes more data. The visualizations interact with a GPU cluster for analysis (e.g., cross-filtering), visual layout (e.g., ForceAtlas2 and edge bundling), and rendering. Key to Phase I is establishing the technical feasibility of building and deploying the distributed GPU cloud architecture. The architecture utilizes novel GPU components and optimizations for: (i) GPU analytics (ii) GPU-accelerated visual layouts; (iii) cloud GPU resource management; and (iv) a streaming renderer to draw more while maintaining perceived quality and responsiveness. In collaboration with pilot customers, the views and workflows necessary for minimal viable security analytics usage will be identified and prototyped."
"1642410","SI2-SSE: Collaborative Research: High Performance Low Rank Approximation for Scalable Data Analytics","OAC","CYBERINFRASTRUCTURE, Software & Hardware Foundation, Software Institutes","11/01/2016","09/08/2019","Haesun Park","GA","Georgia Tech Research Corporation","Standard Grant","Amy Walton","10/31/2020","$387,281.00","Barry Drake","hpark@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7231, 7798, 8004","026Z, 7433, 7942, 8004, 8005","$0.00","Big Data analytics is at the core of discovery covering vast areas such as medical informatics, business analytics, national security, and materials sciences. This project aims to model some of the key data analytics problems and design, verify, and deploy scalable methods for knowledge extraction.  The algorithms developed will be able to handle data sets of extreme sizes and will be deployable on advanced computer hardware. The goal is to realize orders-of-magnitude improvements over existing data analytics technologies, developing algorithms that are robust to incompleteness, noise, ambiguity, and high dimension in the data.  Particular focus will be parallel and distributed algorithms that can efficiently solve large problems and produce accurate solutions.  The proposed research and software development will allow domain experts to tackle Big Data sets requiring large parallel systems.  The improved performance will enable fast and scalable data analysis across applications, from social network analysis to study citizens' attitudes toward sustainability-related issues to computational marketing techniques that refine customers' shopping experiences.  The proposed work will help bridge the gap between computational science and data analytics ecosystems, two fields that stand to make great advancements from cross-fertilization.  The education and outreach plan includes graduate course creation, engagement of under-represented groups via both undergraduate and graduate research experiences, and community-building efforts by workshop and mini-symposium organization.<br/><br/>With the advent of internet-scale data, the data mining and machine learning community has adopted Nonnegative Matrix Factorization (NMF) for performing numerous tasks such as topic modeling, background separation from video data, hyper-spectral imaging, web-scale clustering, and community detection.  The goals of this proposal are to develop efficient parallel algorithms for computing nonnegative matrix and tensor factorizations (NMF and NTF) and their variants using a unified framework, and to produce a software package called Parallel Low-rank Approximation with Nonnegative Constraints (PLANCK) that delivers the high performance, flexibility, and scalability necessary to tackle the ever-growing size of today's data sets. The algorithms will be generalized to NTF problems and extend the class of algorithms we can efficiently parallelize; our software framework will allow end-users to use and extend our techniques.  Rather than developing separate software for each problem domain and mathematical technique, flexibility will be achieved by characterizing nearly all of the current NMF and NTF algorithms in the context of a block coordinate descent framework. Using this framework the shared computational kernels can be separated, which usually extend run times, from the algorithm-specific computations. Finally, the usability and practicality of the proposed software will be maintained by being application driven, establishing collaborations with early end-users, and by incrementally generalizing the framework in terms of both algorithms and problems."
"1622536","SCH: INT: Collaborative Research: Assistive Integrative Support Tool for Retinopathy of Prematurity","IIS","Smart and Connected Health","10/01/2016","07/27/2016","Stratis Ioannidis","MA","Northeastern University","Standard Grant","Sylvia Spengler","09/30/2021","$799,999.00","Deniz Erdogmus, Jennifer Dy","IOANNIDIS@ECE.NEU.EDU","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","8018","8018, 8062","$0.00","Retinopathy of prematurity (ROP) is a leading cause of childhood visual loss worldwide, and the social burdens of infancy-acquired blindness are enormous. Early diagnosis is critically important for successful treatment, and can prevent most cases of blindness. However, lack of access to expert medical diagnosis and care, especially in rural areas, remains a growing healthcare challenge. In addition, clinical expertise in ROP is lacking, and medical professionals are struggling to meet the increasing need for ROP care. As point-of-care technologies for diagnosis and intervention are rapidly expanding, the potential ability to assess ROP severity from any location with an internet connection and a camera, even without immediate ophthalmologic consultation available, could significantly improve delivery of ROP care by identifying infants who are in most urgent need for referral and treatment. This would dramatically reduce the incidence of blindness without a proportionate increase in the need for human resources, which take many years to develop. <br/><br/>This project develops a prototype assistive integrative support tool for ROP, featuring a modular design comprising: (a) image analysis, (b) information fusion of clinical, imaging, and diagnostic data, and (c) generative probabilistic and regression models with associated computationally efficient machine learning algorithms. The outcomes of the project include disease severity metrics and diagnostic estimates obtained through clinical evidence classifiers trained jointly over expert-generated labels. These labels consist of discrete diagnostic labels, as well as comparison outcomes of relative severity between pairs of images. Random process models for vessel tortuosity and diameter distributions over the retina, as well as patch-based vessel-free image analysis through the use of convolutional neural networks on the entire image, enhance and augment feature extraction. Moreover, incorporating severity comparison outcomes through novel hard and soft constraint methods force inferred severity to agree with ordinal information provided by experts and address inherent uncertainty in expert ground-truth labels. The above severity inference methods are evaluated and fine-tuned over a broad array of generative models, both through retrospective analysis, including cross-validation, longitudinal tests, and tests across multiple sites, as well as through prospective analysis, evaluating its real-world clinical impact."
"1618303","SHF: Small: Collaborative Research: Coupling Computation and Communication in FPGA-Enhanced Clouds and Clusters","CCF","Software & Hardware Foundation","06/15/2016","04/16/2018","Martin Herbordt","MA","Trustees of Boston University","Standard Grant","Almadena Chtchelkanova","05/31/2020","$240,992.00","","herbordt@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","7798","7798, 7923, 7942, 9251","$0.00","The introduction of Field Programmable Gate Arrays (FPGAs) to accelerate clusters of servers in datacenters and clouds provides a great, immediate opportunity to leverage a new technology in high-end computing. With their flexible logic and native massive communication capability, FPGAs are ideal for high-performance computing in the post-Moore?s Law world. Since the hardware adapts to the application higher efficiency can be achieved, and since FPGAs are hybrid communication/computation processors, they can be interconnected directly chip-to-chip. Large-scale communication can consequently proceed with both higher bandwidth, lower latency, and less processor impact. These features are crucial to enhancing performance beyond current levels. The proposed design allows for useful processing while data is in flight in the network resulting in reduced software overhead in parallel middleware and reduced network congestion. The key tenets of the research are to achieve programmable, intelligent acceleration of applications while emphasizing overlap of communication and computation at low latency, while also cutting substantially software overhead. <br/>The research project, FC5 (an FPGA framework for coupling communication and computation in clouds and clusters) has several thrusts. First, hardware support for FC5 and investigation of methods of configurability in FC5 to reduce communication latency and support computing in the network are studied. A second outcome is a prototype version of the Open MPI open source version of MPI-3.1 parallel middleware that utilizes FC5 to deliver the features and performance enhancements involving data movement between and within servers, mathematical data reductions, and bulk data reorganizations. Third, proof-of-concept versions of multiple FC5 software models, including direct hardware access, a transparent MPI-in-OpenCL, and an API-based mechanism that exposes essential functionality. Finally, because FC5 is evolving rapidly with major new announcements expected imminently, continued refinement is essential. At least two model applications, Molecular Dynamics and Map-Reduce, will be used as test cases. <br/>With the continued consolidation of computing services into the cloud, the potential broader impact is to increase both the scale and availability of parallel applications. The broad range of uses of cloud and cluster computing for commercial, government, and academic applications means that acceleration offered will have a widespread impact applicable across many sectors. The growing acceptance of high performance computing in industry (e.g., fast machine learning) is one particular potential commercial sector that will be enhanced by this project."
"1565431","SHF: Large: Collaborative Research: Next Generation Communication Mechanisms exploiting Heterogeneity, Hierarchy and Concurrency for Emerging HPC Systems","CCF","Software & Hardware Foundation","08/15/2016","08/22/2018","William Barth","TX","University of Texas at Austin","Standard Grant","Almadena Chtchelkanova","07/31/2020","$422,457.00","Carlos Rosales-Fernandez, Jerome Vienne","bbarth@tacc.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7798","7925, 7942","$0.00","This award was partially supported by the CIF21 Software Reuse Venture whose goals are to support pathways towards sustainable software elements through their reuse, and to emphasize the critical role of reusable software elements in a sustainable software cyberinfrastructure to support computational and data-enabled science and engineering.<br/><br/>Parallel programming based on MPI (Message Passing Interface) is being used with increased frequency in academia, government (defense and non-defense uses), as well as emerging uses in scalable machine learning and big data analytics. The emergence of Dense Many-Core (DMC) architectures like Intel's Knights Landing (KNL) and accelerator/co-processor architectures like NVIDIA GPGPUs are enabling the design of systems with high compute density. This, coupled with the availability of Remote Direct Memory Access (RDMA)-enabled commodity networking technologies like InfiniBand, RoCE, and 10/40GigE with iWARP, is fueling the growth of multi-petaflop and ExaFlop systems. These DMC architectures have the following unique characteristics: deeper levels of hierarchical memory; revolutionary network interconnects; and heterogeneous compute power and data movement costs (with heterogeneity at chip-level and node-level). <br/>For these emerging systems, a combination of MPI  and other programming models, known as MPI+X (where X can be PGAS, Tasks, OpenMP, OpenACC, or CUDA), are being targeted.  The current generation communication protocols and mechanisms for MPI+X programming models cannot efficiently support the emerging DMC architectures.  This leads to the following broad challenges: 1) How can high-performance and scalable communication mechanisms for next generation DMC architectures be designed to support MPI+X (including Task-based) programming models? and 2) How can the current and next generation applications be designed/co-designed with the proposed communication mechanisms?<br/><br/>A synergistic and comprehensive research plan, involving computer scientists from The Ohio State University (OSU) and Ohio Supercomputer Center (OSC) and computational scientists from the Texas Advanced Computing Center (TACC), San Diego Supercomputer Center (SDSC) and University of California San Diego (UCSD), is proposed to address the above broad challenges with innovative solutions.  The research will be driven by a set of applications from established NSF computational science researchers running large scale simulations on Stampede and Comet and other systems at OSC and OSU.  The proposed designs will be integrated into the widely-used MVAPICH2 library and made available for public use.  Multiple graduate and undergraduate students will be trained under this project as future scientists and engineers in HPC. The established national-scale training and outreach programs at TACC, SDSC and OSC will be used to disseminate the results of this research to XSEDE users. Tutorials will be organized at XSEDE, SC and other conferences to share the research results and experience with the community."
"1565414","SHF: Large: Collaborative Research: Next Generation Communication Mechanisms exploiting Heterogeneity, Hierarchy and Concurrency for Emerging HPC Systems","CCF","CI REUSE, CSR-Computer Systems Research, Software & Hardware Foundation","08/15/2016","08/25/2017","Dhabaleswar Panda","OH","Ohio State University","Standard Grant","Almadena Chtchelkanova","07/31/2020","$1,171,893.00","Karen Tomko, Hari Subramoni, Khaled Hamidouche","panda@cse.ohio-state.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","6892, 7354, 7798","7354, 7433, 7925, 7942","$0.00","This award was partially supported by the CIF21 Software Reuse Venture whose goals are to support pathways towards sustainable software elements through their reuse, and to emphasize the critical role of reusable software elements in a sustainable software cyberinfrastructure to support computational and data-enabled science and engineering.<br/><br/>Parallel programming based on MPI (Message Passing Interface) is being used with increased frequency in academia, government (defense and non-defense uses), as well as emerging uses in scalable machine learning and big data analytics. The emergence of Dense Many-Core (DMC) architectures like Intel's Knights Landing (KNL) and accelerator/co-processor architectures like NVIDIA GPGPUs are enabling the design of systems with high compute density. This, coupled with the availability of Remote Direct Memory Access (RDMA)-enabled commodity networking technologies like InfiniBand, RoCE, and 10/40GigE with iWARP, is fueling the growth of multi-petaflop and ExaFlop systems. These DMC architectures have the following unique characteristics: deeper levels of hierarchical memory; revolutionary network interconnects; and heterogeneous compute power and data movement costs (with heterogeneity at chip-level and node-level). <br/>For these emerging systems, a combination of MPI  and other programming models, known as MPI+X (where X can be PGAS, Tasks, OpenMP, OpenACC, or CUDA), are being targeted.  The current generation communication protocols and mechanisms for MPI+X programming models cannot efficiently support the emerging DMC architectures.  This leads to the following broad challenges: 1) How can high-performance and scalable communication mechanisms for next generation DMC architectures be designed to support MPI+X (including Task-based) programming models? and 2) How can the current and next generation applications be designed/co-designed with the proposed communication mechanisms?<br/><br/>A synergistic and comprehensive research plan, involving computer scientists from The Ohio State University (OSU) and Ohio Supercomputer Center (OSC) and computational scientists from the Texas Advanced Computing Center (TACC), San Diego Supercomputer Center (SDSC) and University of California San Diego (UCSD), is proposed to address the above broad challenges with innovative solutions.  The research will be driven by a set of applications from established NSF computational science researchers running large scale simulations on Stampede and Comet and other systems at OSC and OSU.  The proposed designs will be integrated into the widely-used MVAPICH2 library and made available for public use.  Multiple graduate and undergraduate students will be trained under this project as future scientists and engineers in HPC. The established national-scale training and outreach programs at TACC, SDSC and OSC will be used to disseminate the results of this research to XSEDE users. Tutorials will be organized at XSEDE, SC and other conferences to share the research results and experience with the community."
"1565336","SHF: Large: Collaborative Research: Next Generation Communication Mechanisms exploiting Heterogeneity, Hierarchy and Concurrency for Emerging HPC Systems","CCF","Software & Hardware Foundation","08/15/2016","08/04/2016","Amitava Majumdar","CA","University of California-San Diego","Standard Grant","Almadena Chtchelkanova","07/31/2020","$405,651.00","Mahidhar Tatineni","majumdar@sdsc.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7798","7925, 7942","$0.00","This award was partially supported by the CIF21 Software Reuse Venture whose goals are to support pathways towards sustainable software elements through their reuse, and to emphasize the critical role of reusable software elements in a sustainable software cyberinfrastructure to support computational and data-enabled science and engineering.<br/><br/>Parallel programming based on MPI (Message Passing Interface) is being used with increased frequency in academia, government (defense and non-defense uses), as well as emerging uses in scalable machine learning and big data analytics. The emergence of Dense Many-Core (DMC) architectures like Intel's Knights Landing (KNL) and accelerator/co-processor architectures like NVIDIA GPGPUs are enabling the design of systems with high compute density. This, coupled with the availability of Remote Direct Memory Access (RDMA)-enabled commodity networking technologies like InfiniBand, RoCE, and 10/40GigE with iWARP, is fueling the growth of multi-petaflop and ExaFlop systems. These DMC architectures have the following unique characteristics: deeper levels of hierarchical memory; revolutionary network interconnects; and heterogeneous compute power and data movement costs (with heterogeneity at chip-level and node-level). <br/>For these emerging systems, a combination of MPI  and other programming models, known as MPI+X (where X can be PGAS, Tasks, OpenMP, OpenACC, or CUDA), are being targeted.  The current generation communication protocols and mechanisms for MPI+X programming models cannot efficiently support the emerging DMC architectures.  This leads to the following broad challenges: 1) How can high-performance and scalable communication mechanisms for next generation DMC architectures be designed to support MPI+X (including Task-based) programming models? and 2) How can the current and next generation applications be designed/co-designed with the proposed communication mechanisms?<br/><br/>A synergistic and comprehensive research plan, involving computer scientists from The Ohio State University (OSU) and Ohio Supercomputer Center (OSC) and computational scientists from the Texas Advanced Computing Center (TACC), San Diego Supercomputer Center (SDSC) and University of California San Diego (UCSD), is proposed to address the above broad challenges with innovative solutions.  The research will be driven by a set of applications from established NSF computational science researchers running large scale simulations on Stampede and Comet and other systems at OSC and OSU.  The proposed designs will be integrated into the widely-used MVAPICH2 library and made available for public use.  Multiple graduate and undergraduate students will be trained under this project as future scientists and engineers in HPC. The established national-scale training and outreach programs at TACC, SDSC and OSC will be used to disseminate the results of this research to XSEDE users. Tutorials will be organized at XSEDE, SC and other conferences to share the research results and experience with the community."
"1563991","SHF: Medium: Spectral Profiling: Understanding Software Performance without Code Instrumentation","CCF","Software & Hardware Foundation","07/15/2016","07/07/2016","Alessandro Orso","GA","Georgia Tech Research Corporation","Standard Grant","Sol Greenspan","06/30/2021","$850,000.00","Alenka Zajic, Milos Prvulovic","orso@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7798","7924, 7944","$0.00","Dynamic analyses such as profiling program execution are widely used because they can measure various aspects of the runtime behavior of a software system and have a wide range of applications in software engineering. These analyses are typically carried out by adding probes to the software, which imposes space/time overhead, is intrusive, and can negatively affect software behavior. To address these issues, we propose a novel approach that allows for analyzing software behavior accurately and non-intrusively by leveraging the electromagnetic emissions produced by a computer as it executes code. Our approach can collect runtime information about a software system by simply placing a device next to the system. It can thus not only enable profiling for a variety of software systems for which this was previously impossible (e.g., embedded systems), but also benefit dynamic analyses in more traditional contexts.<br/><br/>This project will combine various machine learning and static analysis techniques to build likely electromagnetic signatures for different code patterns, investigate which code granularity provides the most accurate matching of electromagnetic emissions to code, and explore adaptive and hierarchical techniques for performing this matching at runtime. This research is inherently interdisciplinary and promises to break new ground and have broader impact in several combined areas, including software engineering, programming languages, computer architecture, and electromagnetics. Unlike previous work on electromagnetic emissions analysis, our approach will collect runtime information that is fine-grained enough to measure the execution of short sequences of statements, if not individual instructions. This will let us apply our approach to several software engineering tasks. In fact, if successful, this research will both provide a solid conceptual foundation, which other researchers will be able to leverage, and investigate a set of specific techniques and tools that build on this foundation to support tasks such as zero-overhead performance measurement, debugging, and anomaly detection."
"1563343","A Data-Driven and Real-time Approach to Personalized Bundle Recommendation and Pricing; from Theory to Practice","CMMI","OE Operations Engineering","07/01/2016","03/21/2016","Georgia Perakis","MA","Massachusetts Institute of Technology","Standard Grant","Georgia-Ann Klutke","06/30/2020","$395,000.00","","georgiap@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","ENG","006Y","071E, 072E, 073E, 077E, 078E, 9102","$0.00","A good pricing strategy plays a crucial role in any retailer's business process. This is largely due to the fact that pricing approaches are highly visible to both customers and competitors, and thus have an immediate and dramatic impact on the bottom line. The online market for retail goods has grown enormously over the past decade. The development of a sophisticated personalized product and price recommendation system can provide the necessary competitive edge for any Internet retailer, making the difference on the order of billions in profits. The challenge of the new retailing paradigm is to design models for bundling and personalized pricing that are able to extract value from the new streams of customer data that an e-tailer has access to. The goal of this research will be the development of a personalized bundling and pricing model that recommends a bundle of related products to a consumer, but at an incentivized price (lower than the sum of the prices of the individual items) during their online session.  <br/><br/>The innovation in this model will consist of balancing several key factors in terms of bundle and pricing offerings simultaneously, while developing a new approach to personalized demand estimation. The focal points of the work consist of four considerations: (i) developing meaningful personalization models (in terms of bundling of products but also in terms of pricing these products), (ii) developing inventory balancing methods that will balance the tradeoff between losing money due to future markdowns and at the same time facing potentially future inventory stock-outs, (iii) developing interesting structural results and insights on how personalization can tradeoff with bundling in terms of the pricing strategies. Finally, and very importantly, (iv) developing efficient solution approaches (theoretically but also computationally, the latter using actual industry data) that could impact practice. The goal will be to combine these distinct goals in the modeling: personalization, inventory balancing, pricing insights and efficiency/practicality. This research will design innovative models, analyzing them first from a theoretical as well as an applied standpoint but also in exploring the relationships between them and testing them first with real data. Furthermore, this research will build an integrated framework, models and solution techniques from machine learning, integer optimization, stochastic and robust optimization to key personalized bundle pricing problems that are accessible to all, in order to help academics and practitioners in the area of pricing."
"1636933","BD Spokes: SPOKE: SOUTH: Large-Scale Medical Informatics for Patient Care Coordination and Engagement","OAC","BD Spokes -Big Data Regional I","09/01/2016","08/31/2016","Gari Clifford","GA","Emory University","Standard Grant","Beth Plale","08/31/2020","$1,000,000.00","Taylor Herman","gari.clifford@emory.edu","1599 Clifton Rd NE, 4th Floor","Atlanta","GA","303224250","4047272503","CSE","024Y","028Z, 7433, 8083","$0.00","This project brings together six universities to design and construct a patient-focused and personalized health system that addresses the fractured nature of healthcare information, and the lack of engagement of individuals in their own healthcare. By taking advantage of the enormous amount of information being created about our environment, through the confluence of real-time, mobile and wearable devices and the availability of rich social media data on patient behavior, the team will create a detailed and comprehensive picture of a patient's health, and a tool to help manage patients' engagement with their health care providers. The system has four key aims to: (1) provide a human-centered approach for integrating electronic health record data generated by traditional methods with data collected ""in the wild"" (such as personal fitness devices, mobile phone usage, local weather, pollution or even fast food restaurant maps, etc.); (2) develop a framework for deciding which data sources are trustworthy; (3) create a cloud-based system to allow users to view and track their own data over time and improve healthcare outcomes; and (4) provide educational outreach and community participation, particularly in minority populations, to design a system which benefits users in both the short term (through employment and education) and the long term (through increased engagement and trust).<br/><br/>This project will leverage modern distributed cloud-based computing infrastructure (including mobile phones and Amazon Web Services), and the unique capacities of the South BD Hub to house and analyze the enormous volumes of health-related data that are generated every day by people, and their environment. By linking electronic medical records, external databases and data 'in the wild' harvested from patient's Internet-enabled devices, the project will address several issues related to the integration of high-resolution data for longitudinal tracking of patients. These include acceptability of the technology, particularly by vulnerable groups, usability, veracity of data collected, and scalability/integration across a large heterogeneous landscape.  By employing patient-centric agile development, the team will work with communities to implement a cloud-based architecture to improve tracking of study participants, increase the ease with which data can be captured, improve patient engagement, and facilitate care coordination. The resultant platform will integrate big data analytics, real time scalable data collection, and social media analytics on patient behavior to analyze cardiovascular disease outcomes among disadvantaged African American and Hispanic patient populations.  Additionally, the team will implement data fusion techniques to ensure the veracity of the varying qualities of data collected, and develop machine learning models to identify at-risk patient populations in order to reduce health disparities. Finally, patient engagement and health outcomes will be measured to assess the validity and success of the system."
"1642391","SI2-SSE: Hearing the Signal through the Static: Realtime Noise Reduction in the Hunt for Binary Black Holes and other Gravitational Wave Transients","OAC","LIGO RESEARCH SUPPORT, COMPUTATIONAL PHYSICS, Software Institutes","11/01/2016","09/04/2018","Chad Hanna","PA","Pennsylvania State Univ University Park","Continuing Grant","Bogdan Mihaila","10/31/2020","$400,000.00","","crh184@psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","1252, 7244, 8004","7433, 7483, 7569, 8004, 8005, 8084","$0.00","Gravitational waves - tiny ripples in the fabric of space - were detected for the first time in history on September 14, 2015 by the US-led gravitational wave observatory, LIGO. This watershed event has ushered in a new era of gravitational wave astronomy, which will transform our understanding of the Universe by providing information through a previously inaccessible channel. LIGO operates at the very edge of the sensitivity required to detect gravitational waves, and therefore, non-stationary noise caused by the environment, e.g., weather and man-made noise, limits LIGO's sensitivity to short-duration gravitational wave transient signals such as the one detected in September. In order to ensure the most opportunity for the advancement of science, this project aims to mine the extensive auxiliary information available in the LIGO observatories in realtime in order to mitigate the impact of non-stationary noise and increase the rate of transient gravitational wave detections. Doing so will afford increased opportunities for joint gravitational wave and electromagnetic observations, which are thought to be rare.<br/><br/>This project aims to address a key piece of missing software infrastructure to use machine learning and inference techniques to utilize auxiliary information such as seismometers, microphones and various control loop signals, to identify non-stationary noise that couples to the gravitational-wave channel in near realtime. The software will be broken into three components: a signal decomposer, a signal classifier and a signal trainer. The signal classifier will determine, given the decomposed, instantaneous output of auxiliary channels and the training data, the probability that non-stationary noise is present. These software components will be built from reusable resources that can be applied across the time-domain gravitational wave community in data calibration, data transfer, and analysis.<br/><br/>This project is supported by the Division of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Physics Division in the Directorate of Mathematical and Physical Sciences."
"1626338","MRI: Development of iSNARLD (Instrument for Situational Network Awareness for Real-time and Long-term Data)","OAC","Major Research Instrumentation, Information Technology Researc","09/01/2016","08/03/2016","Gregory Peterson","TN","University of Tennessee Knoxville","Standard Grant","Stefan Robila","08/31/2020","$822,120.00","Hairong Qi, Garrett Rose, Victor Hazlewood","gdp@utk.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","CSE","1189, 1640","1189, 9150","$0.00","In partnership with Alliance Technology Group (Alliance), The University of Tennessee (UT) proposes to develop an instrument for Situational Network Awareness for Real-time and Long-term Data (iSNARLD). Developing this unique instrument enables network packet capture coupled with a powerful analytics engine capable of providing insights into the characteristics of network traffic, breadth of traffic flows, detection of potential attacks or unauthorized data exfiltration, sources of inefficiencies or incorrect behaviors, and the behaviors of user or automated traffic. This instrument will enable, for the first time, analysis that can focus on a specific point in time (such as for real-time analysis), across a range of time spanning months to years, or both. Leveraging the respective strengths in the current the partnership will greatly enhance the prospects for developing an effective instrument with unprecedented capabilities and versatility. Furthermore, the collected network traffic data and associated analytics capabilities will provide information enabling revolutionary new insight into a spectrum of exciting new research thrusts, such as cyber security in forensics and intrusion detection, real-time network situational awareness, operations research into network performance and bottleneck identification, as well as a limitless panoply of other possibilities that heretofore have been impossible. Ergo, the project has extraordinary potential impact on the broad research community as well as to society as a whole. Moreover, the system will impact national cyberinfrastructure and local research as well as the educational mission at UT. Finally, the proposed system has excellent prospects for wide commercial adoption through the partnership with Alliance Technology.<br/><br/>The proposed instrument for Situational Network Awareness for Real-time and Long-term Data (iSNARLD) will fundamentally transform networking and security research through its packet capture and impressive analytics capability. The instrument is the next major step in the evolution of networking infrastructure that provides high bandwidth coupled with security capabilities to detect intrusion and protect computational and data assets. The instrument will build upon a 20Gb/sec SentryWire Sentry 250 system from Alliance with an effective capacity of up to 4 PB (sufficient for months of network data, depending on specific traffic patterns). The proposed system will provide: (1) high-bandwidth, lossless packet capture; (2) metadata extraction to summarize and aggregate packet data to enable efficient analytics; (3) (near) real-time data analytics to support situational awareness of flows and performance along with security applications such as intrusion detection and prevention of data exfiltration; (4) batch or off-line analytics for machine-learning-based discovery; (5) visualization support for enhanced situational awareness and network usage insight; and (6) unprecedented scale of network data (months to years) for analysis. The proposed system presents dramatically improved capabilities for providing situational awareness and understanding network usage in real-time or across periods spanning months to years, providing revolutionary opportunities across a broad spectrum of networking research, operations, and even social science."
"1611333","Data-Driven Dynamic Reliability Assessment of Lithium-Ion Battery Considering Degradation Mechanisms","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","08/15/2016","08/02/2016","Chao Hu","IA","Iowa State University","Standard Grant","Radhakisan Baheti","07/31/2020","$330,000.00","Shan Hu","chaohu@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","ENG","7607","155E","$0.00","The objective of this project is to create a dynamic reliability assessment platform for lithium-ion (Li-ion) battery. Real-time diagnostics/prognostics and predictive maintenance/control of Li-ion battery are essential for reliable and safe battery operation in a wide-range of battery-powered applications, from hybrid and electric vehicles (HEVs/EVs) and medical devices, to the emerging smart grid and all electric airplanes. The proposed platform enables a battery management system (BMS) to develop predictive maintenance/control of a Li-ion battery through concurrently analyzing degradation mechanisms and anticipating failure modes. Successful execution of this research will advance our understanding of how to extend life and prevent catastrophic failure of Li-ion batteries, and will potentially lead to development of battery-powered devices that are more durable and safer than current devices. This project will disseminate research findings to battery industry by demonstrating the platform with a Li-ion battery in an implantable application, through collaboration with a leading industry partner. The project will offer a wide range of education and outreach programs, including 1) incorporating research findings into the Reliability Engineering curriculum, 2) leveraging university research programs to attract undergraduate and K-12 students to engineering career, and 3) organizing paper sessions and panels on Design for Failure Prevention of Li-Ion Battery in major conferences. <br/><br/>To date, real-time diagnostics/prognostics of Li-ion battery has been exploited only empirically and largely in isolation with the underlying degradation mechanisms. This may be attributed to the lack of cognizance of the causal relationship between degradation mechanisms and failure modes. This project will bridge the gap between physical mechanisms and functional failures by creating a dynamic reliability assessment platform that facilitates a synergistic integration of physics-based modeling and sensor-based prognostics. The platform will allow for: 1) identification and quantitative analysis of multiple degradation mechanisms through online estimation of the degradation parameters; and 2) anticipation of the failure modes through online prediction of their remaining useful lives (RULs). The creation of the platform involves three research thrusts: 1) validation of multiphysics models, which updates multiphysics battery models using high precision charge-discharge cycling data; 2) training of health estimators, which adopts machine learning to quantitatively analyze multiple degradation mechanisms from a single measurement of charge curve; and 3) prognostics of failure modes, which leverages the quantitative degradation analysis for prediction of failure mode RULs. The platform provides the methods and tools needed to leverage prognostics and prognostics-informed predictive maintenance/control for achieving the failure prevention capability of BMS. Although this project focuses on the specific case of dynamically updating battery reliability with measured electrical data, the methodology will be applicable to other engineering cases in which measured data are used to dynamically update reliability estimates that support maintenance/control decision making."
"1618014","CCF:  Small:  Accelerating Irregular Algorithms using Cache-Coherent FPGA Accelerators","CCF","Software & Hardware Foundation","08/01/2016","07/27/2016","James Hoe","PA","Carnegie-Mellon University","Standard Grant","Yuanyuan Yang","07/31/2020","$330,000.00","","jhoe@cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7798","7923, 7941","$0.00","Until recently, FPGA acceleration of computations has largely focused on algorithms that exhibit a high degree of regularity and predictability in their parallelism and memory access. The advent of high-capacity FPGA accelerators connected to the processor and main memory through a high-performance cache-coherent interconnect enables algorithms with irregular parallelism to be considered. These irregular algorithms, including many data analytic and machine learning kernels, operate on very large, memory-resident, pointer-based data structures. This project will study the opportunity to accelerate irregular algorithms for performance and energy efficiency on emerging cache-coherent FPGA accelerators. The outcome of this investigation has potential for practical commercial impact by helping to establish cache-coherent FPGA acceleration as a viable new platform option for accelerating irregular algorithms that are fundamental to datacenter workloads. This project will also provide valuable training to both graduate and undergraduate students, and improve graduate-level coursework.<br/><br/>Instead of the traditional ""off-load"" model of FPGA acceleration, this project seek to develop a new tightly-coupled FPGA-processor collaboration model that takes advantage of the low-latency, fine-grain shared-memory interactions between the processor and FPGA that are now possible. The project studies fine-grain concurrent mappings of irregular algorithms where the processor and FPGA work together---each leveraging its own characteristic advantages, e.g., large cache, high frequency ALUs for the processor and energy-efficient spatial hardware concurrency for the FPGA---to outperform what either can achieve alone. An integral part of the investigation is also to develop new insights toward what should cache-coherent FPGA accelerators ultimately look like, especially with the support for irregular algorithms in mind."
"1632211","EarthCube RCN IS-GEO: Intelligent Systems Research to Support Geosciences","ICER","Robust Intelligence, EarthCube","08/15/2016","08/23/2016","Suzanne Pierce","TX","University of Texas at Austin","Standard Grant","Eva Zanzerkia","07/31/2020","$299,998.00","","spierce@tacc.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","GEO","7495, 8074","7433","$0.00","This project will foster collaborations between computer scientists and geoscientists that will advance research in both areas.  Geoscience problems are complex and often involve data that changes across space and time. Frequently geoscience knowledge and understanding provides valuable information and insight for problems related to energy, water, climate, agriculture, mineral resources, and our understanding of how the Earth evolves through time. Simultaneously, many grand challenges in the geosciences cannot be addressed without the aid of computational support and innovations. Intelligent and Information Systems (IS) research in computer science includes a broad range of topics and computational methods such as knowledge representation, information integration, machine learning, robotics, adaptive sensors, and intelligent interfaces. IS research has an important role to play in accelerating the speed of scientific discovery in geosciences and thus in solving challenges that cannot be addressed by other means. Similarly, many aspects of Geosciences (GEO) research pose novel large-scale problems for IS researchers to improve and validate their methods. Intelligent Systems for Geosciences (IS-GEO) represent an emerging community of interdisciplinary researchers producing fundamental new capabilities for understanding Earth systems and how the application of IS technologies can cultivate key new developments in both fields.<br/><br/>The EarthCube Research Coordination Network for Intelligent Systems for Geosciences (IS-GEO RCN) will catalyze collaborations to enable advances in our understanding of Earth systems through innovative applications of intelligent and information systems to fundamental geosciences problems. The goal of the IS-GEO RCN is to leverage expertise and generate interactions between both the geosciences and computing sciences communities to provide advanced scientific capabilities. To enable the network, the IS-GEO RCN will host meetings and other activities that: (1) foster an active and broad-based community across GEO and IIS areas; (2) identify barriers to research, such as terminology differences among the disciplines involved and highlight knowledge gaps that hinder collaboration across the disciplines; (3) establish and enhance communication channels between GEO and IS researchers; (4) defining grand challenges in geosciences that are well suited to IS techniques; and (5) encourage robust, long-term collaborations.  Furthermore, the educational component aims to identify new approaches to teaching students in this new interdisciplinary area, seeking to raise a new generation of scientists that are better able to apply IS methods and tools to geoscience challenges of the future. By providing avenues for IS and GEO researchers to work together the IS-GEO RCN will serve as both a point of contact, as well as an avenue for educational outreach across the disciplines for the nascent community of research and practice.<br/> <br/>The initial efforts are focused on connecting the communities in ways that help researchers understand opportunities and challenges that can benefit from IS-GEO collaborations. The uncertain, heterogeneous and disparate nature of geoscience data paired with recent IS advances and increases in observational data offer unique opportunities for new approaches and discoveries through joint efforts. The IS-GEO RCN will jumpstart interdisciplinary research collaborations in this emerging new area so that progress across both disciplines can be accelerated."
"1617691","CIF: Small: Efficient Discriminant Analysis Through Parsimonious Probabilistic Models","CCF","Comm & Information Foundations","07/01/2016","06/28/2016","Xin Zhang","FL","Florida State University","Standard Grant","Phillip Regalia","06/30/2020","$414,215.00","Qing Mai","henry@stat.fsu.edu","874 Traditions Way, 3rd Floor","TALLAHASSEE","FL","323064166","8506445260","CSE","7797","7923, 7936","$0.00","In the Big-Data era, there is a pressing need to develop more efficient and easy-to-interpret data analysis methods to face the new challenges rising from real life applications, such as cancer diagnosis, brain connectivity network and neuroimaging studies. Besides their intimidating sizes, data sets in such applications demand researchers to understand the intrinsic and complex structure behind them. This research involves new directions in discriminant analysis and classification by exploring state-of-art statistical methods and contemporary computation techniques. The investigators will develop new statistical methods and computational tools to utilize the intrinsic structure of the data, such as high correlation, network structure and tensor/array data structure. The research results will advance the research in statistics, machine learning, text mining, bio-medical research, finance, neuroimaging analysis, among other fields. The research will also be integrated with substantial educational and outreach activities.<br/><br/>In this research, the investigators aim to develop parsimonious probabilistic discriminant analysis models for efficiently analyzing data with intrinsically complicated structures, and for improved estimation of parameters and accuracy in predictions. Three sets of problems will be investigated: (1) parsimonious linear discriminant analysis with envelope, aiming to integrate a nascent technique of envelope modeling with the classical linear discriminant analysis model; (2) simultaneous discriminant analysis and differential network estimation, aiming to develop a novel and unified framework for simultaneously studying differential networks, estimating multiple covariance matrices, and training quadratic discriminant analysis classifier; and (3) sparse tensor discriminant analysis with feature selection, aiming to develop a sparse discriminant analysis method for tensor-valued data that directly uses the tensor-valued features for discriminant analysis, while simultaneously achieving feature selection and preserving interpretable tensor structure."
"1650851","EAGER: Toward Predictive Models for Diagnosis & Severity Assessment of Heart Disease from Diverse Temporal Data","IIS","Info Integration & Informatics","09/01/2016","09/01/2016","Hagit Shatkay","DE","University of Delaware","Standard Grant","Sylvia Spengler","08/31/2019","$100,000.00","","shatkay@udel.edu","210 Hullihen Hall","Newark","DE","197160099","3028312136","CSE","7364","7364, 7916, 9150","$0.00","The project aims to lay the foundation for developing computational methods for improved diagnosis and prediction of heart disease from noisy data gathered from patients. The data comprises both time-series signals and images gathered from patients suffering from Hypertrophic Cardiomyopathy (HCM), the most common cardiac genetic disease. Current diagnosis limitations lead to undetected cases that may result in sudden cardiac death, often in active young individuals. The project focuses on advancing the computational means for utilizing data that is relatively fast and low-cost to obtain but is hard to interpret, namely sonogram images (Echo) and electrocardiogram time-series (ECG). Developing tools that improve diagnosis and prediction while making the basis for such tools simpler and less-expensive stands to benefit large populations, especially in under-privileged areas where state-of-the-art imaging technology is scarce.<br/><br/>This project develops probabilistic, supervised and unsupervised machine learning methods and tools. Specifically, it extends the framework of hidden Markov model to support effective use of 12-lead ECG as well as integration of Echo image data. These information sources are relatively inexpensive and nonintrusive but have hitherto been under-utilized in computer-assisted HCM-detection. Developing the proposed models and methods provides a unique training opportunity for young scientists, by conducting inter-disciplinary research while utilizing diverse types of data. The research also paves the road toward introducing probabilistic integrative methods in other disease-domains, as well as in other data-intensive applications outside medicine."
"1642553","Workshop: Theory and Science of Obfuscation as a Methodology for Privacy and Security","SES","Secure &Trustworthy Cyberspace","09/01/2016","08/09/2016","Finn Brunton","NY","New York University","Standard Grant","Sara Kiesler","08/31/2017","$50,000.00","Helen Nissenbaum","fb42@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","SBE","8060","7434, 7556","$0.00","Obfuscation studies how to insert noise into an existing signal to make data or information-stream more ambiguous, confusing, harder to exploit, and more difficult to act on. As an important topic in both privacy/security and information processing, it has been studied from many different perspectives in many related areas, e.g., data privacy, databases, machine learning, ethics, etc. This workshop represents one of the first attempts to consolidate and shape the area as a whole, and help develop a new research community that carries out obfuscation research in a holistic fashion.<br/><br/>Specifically, this workshop aims to foster and develop new interdisciplinary research partnerships around obfuscation, by involving researchers, designers, and system producers, including but not limited to academic researchers, industry researchers and practitioners, and independent software producers and privacy activists.  The workshop seeks to create benchmarks, assessment tools, and evaluation frameworks for obfuscation techniques, formalizing the notion of obfuscation, and identifying its key challenges and use cases. Through open, online publications, this workshop contributes to the broader conversation about putting obfuscation in practice, and therefore has broader impacts on industry and government, as well as user and developer communities who are interested in producing the next-generation privacy technologies."
"1617369","III: Small: High-Throughput Annotation of Cellular Functions of Intrinsic Disorder in Proteins","IIS","Info Integration & Informatics","10/01/2016","08/01/2016","Lukasz Kurgan","VA","Virginia Commonwealth University","Standard Grant","Sylvia Spengler","09/30/2020","$500,000.00","","lkurgan@vcu.edu","P.O. Box 980568","RICHMOND","VA","232980568","8048286772","CSE","7364","7364, 7923","$0.00","One of fundamental problems in molecular biology is to decipher functions of millions of uncharacterized protein sequences that are rapidly generated by high-throughput genome sequencing. The sequence-to-structure-to-function paradigm was used for decades to determine functions of proteins. However, recent research has broadened this paradigm by adding new players, proteins with intrinsic disorder (ID). They are highly abundant and cannot be solved with the structure-driven approach. While there are many widely used computational methods that accurately predict ID in protein sequences, methods for the prediction of the many functions of ID are lacking. This project will develop a family of novel, accurate, and high-throughput computational methods that predict all major functions of ID in protein sequences. It will produce putative functional annotations on an unprecedented scale of thousands of species, addressing the problem of high rate acquisition of raw sequence data and contributing to the increase of the rate of scientific discovery. These results will advance our understanding of fundamental biological processes and human health given the high prevalence of ID in human diseases and attractiveness of proteins with ID as drug targets. This project will also contribute to training of STEM students and researchers via short workshops and undergraduate and graduate level lectures and mentoring, focusing on demonstrating relevance and value of education and research in the emerging areas of protein bioinformatics.<br/><br/>This project will conceptualize, design, rigorously test and deploy predictors of all major functions of intrinsic disorder including protein-RNA, protein-DNA, protein-protein, protein-ligand and protein-lipid interaction, flexible linkers and spacers, regions that host post-translational modifications, and moonlighting regions. These methods will provide fast and accurate predictions in the absence of sequence similarity when the commonly used sequence alignment-based approaches fail. The design will include a novel, hybrid function-specific feature extraction, empirical feature selection, and optimization of predictive models generated with modern machine learning algorithms. The inputs to these predictive models will be quantified and aggregated from a comprehensive set of empirically selected and sequence-derived structural and biophysical characteristics of proteins. The resulting methods will be benchmarked and made freely available to the broad research community via a web-based, server-side portal. The results will be also deposited into relevant public databases."
"1658306","I-Corps: Smart Energy Analytic Disaggregation System","IIP","I-Corps","12/01/2016","11/18/2016","Ronnie Lipschutz","CA","University of California-Santa Cruz","Standard Grant","Pamela McCauley","05/31/2018","$50,000.00","Sue Carter","rlipsch@ucsc.edu","1156 High Street","Santa Cruz","CA","950641077","8314595278","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project is to provide home owners with reliable, real-time appliance energy information and guidance to manage their electricity consumption more efficiently and to facilitate the transition to distributed renewable energy generation and local microgrids through a low cost electricity data acquisition and analytic device. A smart energy system relies on balancing various internal power demands in order to maximize efficiency and minimize cost to users. This technology offers a low-cost, easily-installed device that can be used in residential settings, allowing consumers to identify which appliances are operating at a particular time and the real-time cost of electricity consumption. It can provide real-time data to the residential customers in order to facilitate broader energy management of distributed installations.<br/><br/>This I-Corps project is based on a smart energy analytic disaggregation system is built with open-source, embedded system composed of low-cost, off-the-shelf components, combining hardware and software. The system uses mathematical transformations to compress the data and machine-learning classification algorithms on the embedded processor to decompose building electric circuit load signals. Data are linked to appliance specific components by capturing and analyzing current and voltage data at a high sampling rate, differentiating among individual appliance signals, their harmonics, and unique signatures. The device has capacity for multiple sensors and can be updated continuously or daily via the existing internet infrastructure. The more devices and the longer it measures and analyzes, the better the algorithm becomes at appliance energy use identification."
"1648807","INSPIRE: Architectural Principles of Coherent Quantum Networks and Circuits","PHY","AMO Experiment/Atomic, Molecul, OFFICE OF MULTIDISCIPLINARY AC, EPMD-ElectrnPhoton&MagnDevices, Information Technology Researc, Special Projects - CCF, QIS - Quantum Information Scie, INSPIRE","09/15/2016","09/09/2016","Hideo Mabuchi","CA","Stanford University","Standard Grant","Julio Gea-Banacloche","08/31/2019","$1,000,000.00","","hmabuchi@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","MPS","1241, 1253, 1517, 1640, 2878, 7281, 8078","026Z, 7203, 7928, 8653, 8990","$0.00","This INSPIRE project is jointly funded by the Quantum Information Science (QIS) Program in the Physics Division in the Mathematical and Physical Sciences Directorate, the Atomic Molecular and Optical Physics Experiment (AMO-E) Program in the Physics Division in the Mathematical and Physical Sciences Directorate, the Algorithmic Foundations (AF) Program in the Computing and Communications Foundations Division in the Computer and Information Science and Engineering Directorate, the Electronics, Photonics and Magnetic Devices (EPMD) Program in the  Electrical, Communications and Cyber Systems (ECCS) Division in the Engineering Directorate,  and the NSF Office of Integrative Activities (OIA).   A revolutionary approach to computing is under development using quantum information processing.   However, there is still a gap between the testbeds for quantum information processing that exist today, and the more perfect hardware needed for an ideal quantum computer.  This project seeks to fill this gap by finding ways to use variable amounts of quantum behavior to incrementally improve computing systems.  The project will examine how to use quantum effects in order to provide benefits in terms of speed, energy, and hardware efficiency for applications in signal processing and machine learning.  The project will produce open source software for modeling quantum networks and will train graduate students to develop quantum information processing systems.<br/><br/>This effort builds on work designing circuits for autonomous continuous-time quantum error correction and ultra-low power information processing systems in photonic architectures.  The theoretical component of the project will provide tools to study quantum feedback in various computer architectures operating with open quantum systems.   This will be done using quantum stochastic differential equations (a form of quantum field theory that is adapted to resemble ordinary stochastic differential equations that are widely used in modern engineering) in order to improve the publicly-available open source software package (Mabuchilab/QNET on GitHub)  and explore ways to extend these methods to nano-optomechanics and superconducting circuit QED.  The experimental component of this project will complement the high-level study by testing lower-level architectural principles based on coherent-feedback quantum control, going beyond previous work by incorporating nonlinear controller dynamics and pulsed signal fields. Together, the theoretical and experimental parts of this project will advance the quantum information community's ability to rapidly construct rigorous quantum-optical models for complex coherent networks/circuits, facilitating the exploration of new high-level architectural principles."
"1632268","SBIR Phase II:  Monolithic CMOS-Integration of Electroplated Copper MEMS Inertial Sensors","IIP","SBIR Phase II","08/15/2016","08/20/2018","Noureddine Tayebi","CA","InSense Inc","Standard Grant","Muralidharan Nair","01/31/2020","$1,417,930.00","","noureddine.tayebi@insenseinc.com","2627 Hanover St","Palo Alto","CA","943041118","6502132012","ENG","5373","116E, 1185, 165E, 169E, 5373, 8035, 8240, 9139, 9231, 9251, HPCC","$0.00","The broader impact/commercial potential of this project can lead to a revolution in the consumer electronics market<br/>(mobile handsets, tablets, game consoles and wearables), wherein high performance, low power, small footprint<br/>multisensing (not limited to inertial sensing) platforms with timing devices, are all directly microfabricated on a common<br/>ASIC substrate. Sensor fusion can produce unprecedented user experiences by using data collected from all sensors and<br/>processed using machine learning algorithms. This can further boost the sensor and timing markets that are expected to<br/>exceed $6 billion dollars by 2017. Moreover, the emergent Internet of Things (IoTs) and wearable markets are expected to<br/>reach $20 billion dollars by 2025, which can induce a rapid growth of such intelligent sensor fusion market. This can have<br/>a tremendous societal impact as wearable devices and IoT systems, interfaced with mobile platforms, can be used to<br/>monitor people?s health, safety and energy consumption. Making these solutions affordable will make it amenable to low<br/>income households not only in the US but also around the world. It will also enable researchers to attain new frontiers of<br/>knowledge such as in digital sensory systems. The long-term goals are to provide such intelligent sensor fusion solutions.<br/><br/>This Small Business Innovative Research (SBIR) Phase 2 project seeks to demonstrate wafer-scale microfabrication of<br/>Micro-Electro-Mechanical Systems (MEMS) inertial sensors directly on the application specific integrated circuit (ASIC)<br/>substrates, by using electroplated copper (e-Cu) as a structural material. MEMS inertial sensors, such as gyroscopes and<br/>accelerometers, are pervasively used in consumer electronics and automotive industries. Current trends are, however,<br/>requiring higher device performance with smaller footprints, wherein multi-degree-of-freedom sensors are integrated on<br/>the same package, to enable new capabilities and user experiences. These requirements can be met by monolithically<br/>fabricating inertial sensors on ASIC substrates, which is complex to achieve with silicon as a structural material. Using e-<br/>Cu, which is currently used for ASIC metal interconnects, as the structural material, can enable easier routing to<br/>implement optimized mechanical structures, smaller dimensions given the high density of copper, extremely low cost as<br/>no wafer bonding is required, smaller form factors, multiple sensors on a single die, and much smaller parasitics providing<br/>low noise and higher performance. Phase II tasks will be to wafer-scale fabricate an inertial measurement unit that is<br/>monolithically integrated with its ASIC with optimal performance parameters."
"1634683","I-Corps: System for Automatic Analysis of Surgical Skills","IIP","I-Corps","05/15/2016","05/11/2016","Irfan Essa","GA","Georgia Tech Research Corporation","Standard Grant","Steven Konsek","01/31/2017","$50,000.00","","irfan@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","ENG","8023","","$0.00","This project, based on research in the areas of robotics and intelligent systems, involves automated techniques for evaluating surgical skills.<br/><br/>Routine evaluation of basic surgical skills in medical schools require considerable time and effort from the supervising faculty. For each surgical trainee, a supervisor has to observe the trainees in person or look at recorded videos. This manual assessment of surgical skills poses a significant resource problem to medical schools and teaching hospitals and results in complications in scheduling/executing their day-to-day activities. In addition to the extensive time requirements, manual assessments are often subjective and domain experts do not always agree on the assessment scores. Every surgical resident has to master basic suturing and knot tying tasks before they can move on to more complex surgeries. Considering the burden on supervising surgery faculty from performing actual surgeries and the amount of trainees that need to go through basic surgical skills training, a system for automated assessment of basic surgical tasks can benefit medical schools and teaching hospitals.<br/><br/>The project addresses the potential commercialization of a system for automated assessment of basic surgical skills. The system consists of a central station with user interface and specialized instruments and gloves for the user. Data about movement of the gloves and instruments are captured using accelerometers and transmitted wirelessly to the central station. Using machine learning, the data will be processed and compared to the performance of a skilled surgeon on the same task. Based on that analysis, a score on the surgical skill will be displayed for the user. The users will be able to keep track of their previously obtained scores, which will allow them to see their progress over time. This will also be beneficial for the supervising surgeon who can monitor trainees' skill improvement before allowing them to move on to more complex procedures. Having such a system available in teaching hospitals and medical schools could help teaching surgeons save valuable time that they could spend doing more surgeries and also give students more opportunities for valuable feedback. This could result in a higher patient throughput for hospitals and improve the overall healthcare system. Through NSF I-Corps, the team will conduct interviews with expert surgeons and residents from different teaching hospitals to validate the need for an automated system for grading. Moreover, the team will also conduct interviews with people in dentistry to see if the proposed technology can be used in their domain as well. The team also plans to interview administrators of medical schools and teaching hospitals to understand their process for evaluating and purchasing training aids."
"1546392","BIGDATA: Collaborative Research: F: From Data Geometries to Information Networks","IIS","Big Data Science &Engineering","01/01/2016","09/15/2015","Mauro Maggioni","NC","Duke University","Standard Grant","Sylvia Spengler","05/31/2017","$500,000.00","Pankaj Agarwal","mauro.maggioni@jhu.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","8083","7433, 8083","$0.00","Big Data often results from multiple sources, giving collections that contain multiple, often partial, ""views"" of the same object, space, or phenomenon from various observers.  Extracting information robustly from such data sets calls for a joint analysis of a large collection of data sets.  The project is developing a novel geometric framework for modeling, structure detection, and information extraction from a collection of large related data sets, with an emphasis on the relationships between data.  While this approach clearly applies to data with a clear geometric character (e.g., objects in images), the work is also applied to datasets as diverse as computer networks (identifying common structure in subnets) and Massive Open Online Course homework data (automatically carrying grader annotations to similar problems in other students' homeworks).<br/><br/>The novel framework is based on the construction of maps between the objects under considerations (point clouds, graphs, images, etc...), and on the analysis of the networks of maps that result as a way of extracting information, generating latent models for the data, and transporting or inferring functional / semantic information. These tasks define a new field of map processing between data sets and require tool sets with new ideas from functional analysis, non-convex optimization, and homological algebra in mathematics, and geometric algorithms, machine learning, optimization, and approximation algorithms in computer science.  Sophisticated algorithmic techniques for attacking the large-scale non-linear optimization problems that emerge within the framework will also be investigated."
"1553490","CAREER: A Scalable Multiplane Data Center Network","CNS","Networking Technology and Syst","05/15/2016","04/20/2020","George Porter","CA","University of California-San Diego","Continuing Grant","Darleen Fisher","04/30/2021","$697,261.00","","gmporter@cs.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7363","1045","$0.00","Large Internet data center providers, both public and private, must support ever-increasing data rates between literally hundreds of thousands of servers to meet processing and storage demand.  Operators have relied on similar scale-out network fabrics (typically folded-Clos topologies) to construct their networks.  Since their deployment in the mid-2000s, these scale-out designs have leveraged the steadily increasing performance and decreasing cost of complementary metal-oxide semiconductor (CMOS)-based switching silicon to keep pace with demand.  Unfortunately, these trends cannot continue: network switches face the same CMOS process-scaling limitations that currently hamper central processing unit (CPU) manufacturers.  Just as CPUs have moved to multi-core designs to side-step their scaling limitations, so too will data center operators need to adopt alternative architectures to scale to next-generation link rates.<br/><br/>This project will demonstrate a hybrid electrical/optical nework topology, called SelectorNet, which scales to hundreds of thousands of servers at link rates reaching 1.6 terabits per second.  Unlike recent proposals which utilize two dimensional- or three-dimensional microelectromechanical systems (2D or 3D-MEMS) optical crossbar switches, SelectorNet relies on a novel optical device that abandons the crossbar abstraction. Instead, it relies on indirection to deliver packets between hosts that are not directly connected by our novel ""selector"" switches.  The result is a network fabric that is not only cost-competitive with state-of-the-art Clos-based designs in 2020, but continues to scale in terms of cost, energy, performance, and reliability as link rates surpass 400 gigabits per second.<br/><br/>Broader Impact: Ensuring that the benefits of this work have impact beyond the traditional metrics of research is integral to its design.  The results of this research will make it easier to design and build scalable, efficient, and highly-available cloud and data center services.  By reducing the cost to deploy cloud infrastructure, the researchers hope to lower costs for the largest operators, while reducing the barrier to entry of the cloud for smaller organizations.  They will further expand the research skills of graduate and undergraduate students to address necessary datacenter efficiency and cloud computing research challenges in a hands-on manner.  Exposing undergraduate students to cloud computing technologies in their courses and through mentored research will enhance their marketability at graduation and has the potential to inspire their curiosity and encourage the pursuit of graduate studies.  Teaching students how to build state-of-the-art networked systems that are grounded in rigorous analysis and practical constraints is essential in our increasingly networked world.  An additional component of this research will be the creation and dissemination of videos that will broaden public awareness and appreciation of the science and engineering challenges facing large-scale computing, machine learning, and Internet systems."
"1617046","CSR: Small: Scalable, heterogeneity-aware load balancing","CNS","CSR-Computer Systems Research","10/01/2016","09/07/2016","Anshul Gandhi","NY","SUNY at Stony Brook","Standard Grant","Marilyn McClure","09/30/2020","$394,981.00","","anshul@cs.stonybrook.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","7354","7923","$0.00","Several extremely large online services are provided by distributed systems. Load balancers play a vital role in such systems by distributing incoming requests among the back-end nodes. However, given the rise in cloud computing and the increasing popularity of online services, load balancers today face significant challenges that impact performance. Key among them is the need to quickly distribute millions of requests among heterogeneous nodes while adapting to changing system conditions. Failure to serve requests in a timely manner can lead to loss of revenue due to customer abandonment. This research proposes novel, scalable algorithms that enable high-throughput load balancers for cloud deployments. By leveraging concepts from queueing theory, this research will investigate dynamic, near-optimal load distribution policies with provable performance guarantees. The proposed algorithms will be designed for easy adoption in existing open-source load balancers, including Apache, HAProxy, and nginx. <br/><br/>Recent network function virtualization trends have put the spotlight back on software network functions. This project will explore novel software load balancers for modern computing environments, including dedicated clusters and shared cloud environments. Given the need for high-throughput load balancing decisions, this research focuses on simple yet powerful randomized load balancer designs. The key idea of the proposed research is to dynamically adapt the routing probability based on inferred changes in the workload and infrastructure. Queueing theoretic models will be leveraged to understand the impact of routing on performance, and machine learning techniques will be employed for detecting system changes. The proposed load balancers will be evaluated in Web and data-dependent environments, including MapReduce implementations. The integrated theory-systems research approach will provide unique interdisciplinary educational and collaborative opportunities, including (cross-listed) course development, student training, and technology transfer with industrial partners."
"1614777","Collaborative Research: Efficient mathematical and computational framework for biological 3D image data retrieval","DMS","ADVANCES IN BIO INFORMATICS, CI REUSE, Cross-BIO Activities, MATHEMATICAL BIOLOGY, MSPA-INTERDISCIPLINARY","08/15/2016","08/19/2016","Daisuke Kihara","IN","Purdue University","Standard Grant","Junping Wang","07/31/2020","$542,383.00","","dkihara@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","MPS","1165, 6892, 7275, 7334, 7454","7433, 8007","$0.00","Advances in imaging technology have led to a proliferation of three dimensional biological and medical image data from many imaging modalities, which include magnetic resonance imaging and computed tomography scans in medical imaging, neuroimaging using light-field microscopy in neuroscience, tomography for imaging cells and tissues, and cryo-electron microscopy for biomolecular structures. Images of three dimensional, volumetric, structures provide indispensable spatial information about organs, tissues, and molecules that cannot be captured using two dimensions. The development of tools for efficient and effective analysis of such volumetric data sets is, therefore, urgently required. This project will develop generally applicable mathematical and computational frameworks to effectively and accurately represent, compare, and retrieve biological and medical data in three dimensions. The methods to be developed will provide a general foundation for the analysis of volumetric images obtained using multiple imaging modalities and for multiple data types, not only from the biological domain. For example, the techniques have broader impact in areas such as human face recognition, analysis of geographical and climate data, and computer-aided design. This project, therefore, contributes to general promotion of the progress of science and technology in many domains in which imaging analysis is crucial and is of significant societal impact. <br/><br/>In this project, two complementary and synergistic methods will be developed and integrated. The first method to be developed is a mathematical moment-based approach that provides a compact representation of volumetric data and is very suitable for localized three dimensional image data comparison. A two dimensional image comparison method that is based on a moment-based invariant will be expanded to handle volumetric data. The second method is a machine learning approach that will be powerful in classifying volumetric data. These two approaches will be integrated to take advantage of both methods and validated using three dimensional protein structural data. Analyzing global and local similarities between protein shapes is critical for understanding protein function but challenging because proteins with substantially different shapes may perform the same function. Further, proteins are appropriate for this validation step not only because many structures are available in well-established public databases but also because they lack intrinsic orientation, unlike previously studied datasets of man-made objects such as cars, cups, and tables. As the proposed methods are defined for a general voxel representation of a given volume, they will be generally applicable for any data set yielding a voxel representation, including biomedical data collected using electron microscopy, magnetic resonance imaging and computed tomography. Along side the scientific impact of the project, it also leverages efforts in the interdisciplinary computational life sciences and engineering departments at Purdue University and Eastern Kentucky University by recruiting and training students through interdisciplinary coursework and direct involvement with the project."
"1617640","CSR: Small: Energy-efficient Embedded Signal-processing Inference Systems","CNS","Special Projects - CNS, CSR-Computer Systems Research","10/01/2016","03/26/2019","Niraj Jha","NJ","Princeton University","Standard Grant","Matt Mutka","09/30/2020","$488,000.00","","jha@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","1714, 7354","7923, 9251","$0.00","Machine-learning algorithms enable pattern recognition from data that are too complex to model analytically. This pattern recognition is of fundamental importance in diverse domains. These algorithms are becoming an essential part of embedded systems that find use in infrastructure, environmental monitoring, personal health monitoring, energy management, food supply chain, assembly lines, etc. This research has the potential to enable significant advances in such systems by enabling highly energy-efficient on-sensor inference to be performed. With its plans for involving students from underrepresented groups, industrial engagement, outreach to the broader public, and online distribution of tools, it is expected to have a broad impact.<br/> <br/>The aim of the proposed work is to explore the energy savings achievable by embedded signal-processing inference systems through random projections. Random projections have previously been employed in the context of compressive sensing to reduce system energy. We have found that when random projections are used to compress Nyquist signals, the compression mechanism is far more robust, while offering the possibility of two orders of magnitude system energy savings.  We term this mechanism compressed signal processing.  We propose work on bringing this concept to fruition through new methodologies and signal-processing architectures. In addition, we propose the use of genetic programming and error-aware inference to tackle the nonlinear signal-processing problem.  We plan extensive evaluations of the system-level energy-accuracy tradeoffs the proposed mechanisms offer."
"1546113","BIGDATA: F: DeepWalking Graphs for Feature Extraction","IIS","Big Data Science &Engineering","01/01/2016","09/14/2015","Steven Skiena","NY","SUNY at Stony Brook","Standard Grant","Sylvia Spengler","12/31/2020","$731,260.00","","skiena@cs.sunysb.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","8083","7433, 8083","$0.00","The sparsity of large networks makes it difficult to efficiently extract features for machine learning algorithms. Recent work on network embeddings (DeepWalk) has revealed how neural language modeling can be applied to a very general class of graph analysis problems in data mining and information retrieval.  This project will improve training algorithms and data representation for large-scale networks, creating better, more powerful graph embeddings for weighted and attributed networks.  It will also enable meaningful comparison of the relative performance of network connectivity features vs. more text-oriented features. It is possible that there might be more usable information in links than in the readable content itself.<br/><br/>This project will develop these methods in several new directions, including extensions to new graph classes and speed/scale enhancements.  The original DeepWalk induced latent representations only from unweighted, undirected, and connected graphs. But there is considerable interest in applying it to more general graphs arising in data analysis. Doing the right thing on such natural networks as bipartite and disconnected graphs presents surprisingly subtle issues of theoretical and practical significance.  This project will also explore several ideas to increase training performance of network embeddings, including more efficient gradient updates and improved graph sampling methods and particularly the power of self-avoiding random walks to oversample otherwise rare nodes.  This project seeks to extend the effective range of DeepWalk by several orders of magnitude, from the 10 million vertex graphs we routinely handle today to web-scale networks on billions of nodes.  The broader impacts of this work are far reaching across data mining and information retrieval, including user profiling/demographic inference, online advertising, and fraud detection.  The software and data resources developed under this research project will be released as open source. They will be directly applicable to the biomedical and social sciences, and serve as both an educational and scholarly resource.  For further information, see the project website at http://www.cs.stonybrook.edu/~skiena/deepwalking."
"1547396","RTG: Computational and Applied Mathematics in Statistical Science","DMS","APPLIED MATHEMATICS, STATISTICS, WORKFORCE IN THE MATHEMAT SCI","07/01/2016","08/05/2019","Lek-Heng Lim","IL","University of Chicago","Continuing Grant","Gabor Szekely","06/30/2021","$1,423,312.00","Chao Gao, Jonathan Weare, Daniel Sanz-Alonso, Rina Barber, Mihai Anitescu, Michael Stein","lekheng@galton.uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","MPS","1266, 1269, 7335","7301","$0.00","This Research Training Group (RTG) project supports creation of a dynamic, interactive, and vertically integrated community of students and researchers working together in computational and applied mathematics and statistics. The activity recognizes the ways in which applied mathematics and statistics are becoming increasingly integrated. For example, mechanistic models for physical problems that reflect underlying physical laws are being combined with data-driven approaches in which statistical inference and optimization play key roles. These developments are transforming research agendas throughout statistics and applied mathematics, with fundamental problems in analyzing data leading to new areas of mathematical and statistical research. A result is a growing need to train the next generation of statisticians and computational and applied mathematicians in new ways, to confront data-centric problems in the natural and social sciences. <br/><br/>The research and educational activities of the project lie at the interface of statistics, computation, and applied mathematics. The research includes investigations in chemistry and molecular dynamics, climate science, computational neuroscience, convex and nonlinear optimization, machine learning, and statistical genetics. The research team is made up of a diverse group of twelve faculty, including researchers at Toyota Technological Institute at Chicago and Argonne National Laboratory. The RTG is centered on vertically integrated research experiences for students, and includes innovations in both undergraduate and graduate education. These include the formation of working groups of students and postdocs to provide an interactive environment where students can actively explore innovations in computation, mathematics, and statistics in a broad range of disciplines. Post-docs will assume leadership roles in mentoring graduate students and advanced undergraduates. Participants in the RTG will receive an educational experience that provides them with strong preparation for positions in industry, government, and academics, with an ability to adopt approaches to problem solving that are drawn from across the computational, mathematical, and statistical sciences."
"1632678","STTR Phase II:  Microdevice for Rapid Blood Typing without Reagents and Hematocrit Determination","IIP","STTR Phase II, SBIR Phase II","09/15/2016","10/06/2016","Robert Minerick","MI","Microdevice Engineering Inc.","Standard Grant","Ruth Shuman","02/28/2019","$750,000.00","Laura Brown","rob@vortimac.com","1401 Sugar Maple Lane","Houghton","MI","499312709","9062312011","ENG","1591, 5373","1591, 7236, 7909, 8038","$0.00","The broader impact/commercial potential of this Small Business Technology Transfer (STTR) Phase II project is the development of a portable, low cost blood typing and anemia screening device for use in blood donation centers, hospitals, humanitarian efforts and the military.   This helps society because every two seconds in United States someone needs blood, yet less than 4% of Americans donate blood.  Unfortunately, 15-20% of blood collected is wasted due to over collection of unneeded blood types and related blood type logistics.  12 million units of blood are collected annually in US with 108 million units worldwide.   U.S. blood centers are under significant economic pressures to reduce per unit blood costs and thus waste reduction tools and strategies are in demand.  Blood unit costs approach $200 per pint of blood, so this device provides the ability to pre-screen donors by blood type and selectively direct the donation process (i.e. plasma, red cells) to reduce blood product waste and better match supply with hospital demand.   This portable technology could also be translated to remote geographical locations for disaster relief applications. The potential economic savings has the potential to be $400M and will contribute to reducing the overall cost of U.S. health care.<br/><br/>The proposed project will advance knowledge across multiple fields.  It adapts knowledge in microfluidics and the use of electric fields to characterize cells to identify the molecular expression on blood cells responsible for ABO-Rh blood type.  This project advances the use of electric fields to rapidly measure cell concentration.  This project develops software for real time tracking of cell population motion, which is highly valuable in many cell microscopy applications.  This project also adapts advanced pattern recognition tools like machine learning to extract even more information from the cell behaviors. This work also extends statistical analysis from static population means to analysis of functional data - a field in its infancy - via a critical application.   Finally, the device and electronics engineering will advance under the principle that ""simple is best"", leading to fewer potential failure points and less costly manufacture. This work advances scientific knowledge and will be published and widely disseminated after securing additional IP. It is also a powerful alternative to expensive antigen/antibody molecular recognition reactions (i.e. traditional blood typing) for medical screening and diagnosis for future point of care diagnostic applications."
"1633286","A Visual Analytic Observatory of Scientific Knowledge","SMA","SciSIP-Sci of Sci Innov Policy, STAR Metrics","09/01/2016","08/26/2016","Chaomei Chen","PA","Drexel University","Standard Grant","Cassidy Sugimoto","08/31/2020","$381,842.00","","chaomei.chen@drexel.edu","1505 Race St, 10th Floor","Philadelphia","PA","191021119","2158955849","SBE","7626, 8022","7626","$0.00","This project increase the accessibility of scientific knowledge by creating  a cross-referenced repository of scientific assertions, metrics and indicators of research impact and trends, and tools for accessing and exploring scientific knowledge. The project expands the scope of the study of scientific knowledge by directly addressing the integral role of uncertainties and discrepancies in understanding and communicating scientific knowledge. The observatory tracks how a scientific assertion evolves over time in the scientific literature. The project advances the capabilities of the study of scientific knowledge by providing references so that researchers can routinely and systematically compare and evaluate metrics and algorithms designed to capture the impact of scientific contributions. The increased accessibility of scientific knowledge improves research productivity as well as the understanding of science and relevant policies.<br/><br/>The project represents scientific knowledge as a semantically organized and adaptive network of scientific assertions, structures of reasoning and argumentation, and relevant evidence extracted from the growing body of scientific literature. Extracted concepts, semantic relations, and argumentative structures are synthesized across scientific publications using techniques from machine learning, computational linguistics, quantitative studies of science, and visual analytics. In addition, uncertainties and discrepancies associated with scientific assertions and their impacts are identified and cross-referenced in the repository so as to keep track of their evolution over time. Existing and newly developed metrics and indicators of research impact are linked directly to assertion-level evidence and patterns.  The observatory serves as a sustainable platform for researchers and tool developers to access scientific knowledge and relevant resources for conducting their own studies and evaluating the quality of new metrics and algorithms in the broad context of the science of science and innovation policy research."
"1564466","ABI INNOVATION: Characterizing protein-DNA interactions from high-resolution assays","DBI","ADVANCES IN BIO INFORMATICS","06/01/2016","03/25/2016","Shaun Mahony","PA","Pennsylvania State Univ University Park","Standard Grant","Peter McCartney","05/31/2020","$657,054.00","","sam77@psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","BIO","1165","","$0.00","The identity and health of a particular type of cell depends on the proteins carrying out its functions, and how the functions change depends on the cell's environment. Many of the responses in a cell are due to changes in what proteins are produced; cells turn on protein production, or turn it off, when a particular group of proteins, regulatory proteins, interact with the DNA. Regulatory proteins may act alone or in groups to turn genes on and off, and the timing and number of combinations in these binding events can be very complicated.  There are experiments that show where a specific protein attaches to the DNA, giving us clues that regulation might be occurring; with improving laboratory methods we are able to find the DNA binding locations (profiles) of more and more proteins very specifically. This research will develop special computational and statistical methods for analyzing all of this data together, so that we can see what combinations of proteins act together to regulate genes. From these results we will understand much more about how gene regulation is working in functioning cells. All products produced by this research will be made freely available and accessible to other researchers and the public. Undergraduate and graduate students will be trained to use these bioinformatics research techniques, and strong efforts will be made to recruit students from under-represented groups. Interesting exercises will be created, based on the research methods and results, for undergraduate bioinformatics students and for students in workshops, as well as lessons suitable for high school students studying genetic regulation in their biology courses. <br/><br/>Current bioinformatics analysis techniques do not fully capture the structural information provided by the shape of read distributions produced in high-resolution genomic assays. For example, careful analysis of cross-linking patterns in collections of ChIP-exo datasets can potentially inform which proteins are interacting with one another in higher-order protein-DNA complexes. This project aims to develop a suite of shape-aware machine-learning tools for the analysis of high-resolution protein-DNA binding data that will: 1) deconvolve distinct genomic interaction modes from a single dataset; 2) detect and correct experimental artifacts and biases that arise in the new assays; 3) characterize the organization of higher-order protein-DNA complexes across multiple data types; and 4) detect changes in genomic event locations and interaction modes across multiple experimental conditions. This project will therefore enable integrative models of diverse protein-DNA complexes, directly impacting our understanding of gene regulation in a wide variety of organisms."
"1638702","MSB-FRA Modeling Invasion Dynamics Across Scales (MIDAS)","DEB","MacroSysBIO & NEON-Enabled Sci","09/15/2016","03/20/2020","Songlin Fei","IN","Purdue University","Standard Grant","Elizabeth Blood","08/31/2021","$1,020,171.00","Qinfeng Guo, Hao Zhang, Andrew Liebhold, Kevin Potter","sfei@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","BIO","7959","7350, 7959, 9178, 9251","$0.00","Invasions by exotic species pose a major threat to nearly all ecosystems.  In fact, about 40 percent of U.S. forests have been invaded by exotic plants, resulting in significant economic loss (over $100 billion annually) and ecological damages.  Unfortunately, current understanding of large scale invasion patterns and distribution processes is still limited.  This project, Modeling Invasion Dynamics Across Scales (MIDAS), seeks to understand how the underlying biological, geophysical and socioeconomic factors lead to the emergence of large scale invasion patterns.  Findings of the project will help to inform forest managers, policy makers, and other stakeholders on how to better prevent and mitigate the economic and ecological damages of invasive species through multiple venues such as workshops and webinars, and online planning tools.  The project will have high impact on the education of a diverse group of students and researchers through research opportunities, mentoring, and workshops.  It will facilitate broad, long-lasting interdisciplinary and cross-institutional collaborations.  In addition, the project will contribute to greater understanding of the general phenomenon of how ""things"" (e.g., disease, technology) spread, thus helping to sustain the spread of benefits while limiting unintended harms to ecosystems and society that emerge from social connectivity.<br/><br/>The goal of this project is to determine the underlying mechanisms contributing to macroscale invasion patterns for two important taxa of invaders (plants and insect pests) in forests across the continental United States.  The project will utilize a novel cross-modeling approach by combining: (1) machine learning-based statistical models aimed at capturing key within- and cross-scale interactions and tipping points among data challenged by different spatial and temporal resolutions, and (2) Bayesian models designed to capture model and parameter uncertainties.  This project will result in improved mechanistic understanding and more robust predictions of macroscale invasion.  The proposed conceptual framework can advance the field of invasion ecology by formulating new invasion theories and consolidating various existing ones.  The project will also help advance the field of MacroSystems Biology by synthesizing and developing multidisciplinary datasets and by developing and refining a highly useful analytical tool for investigations involving many variables in which manipulative experiments would be challenging."
"1622433","Statistical Modeling and Computation of Extreme Values in Large Datasets","DMS","CDS&E-MSS","09/01/2016","08/30/2018","Huiyan Sang","TX","Texas A&M University","Continuing Grant","Christopher Stark","08/31/2019","$150,000.00","Shuguang Cui","huiyan@stat.tamu.edu","400 Harvey Mitchell Pkwy South","College Station","TX","778454375","9798626777","MPS","8069","7433, 8083, 9263","$0.00","Numerous problems in environmental, earth, and biological sciences nowadays involve large amounts of spatial data, obtained from remote ground sensors, satellite images, geographic information systems, and public health sources, etc. Analysis of extreme values is of particular interests in many such applications. For instance, natural hazardous events such as severe tides, heat waves, heavy rainfalls, and extreme air pollution events can cause substantial damages in our society. The goal of this project is to better understand spatially dependent extreme events for efficient quantitative risk management. The project has a broad impact on multiple interdisciplinary fields including statistics, geoscience, environmental science, operations research, machine learning, and risk management. The modeling and computational approaches for extreme value analysis and prediction in big data can be applied to a wide range of practical and important problems including extreme climate change studies, environmental hazardous event analysis, insurance risk assessments, and agriculture planning.<br/><br/>Extreme events are rare events by definition. Until recently, analysis of spatial extreme values starts to become feasible, thanks to the availability of big spatial data, which provides great opportunities to accurately quantify the risk of extreme events, better understand the links among extreme events, promptly monitor changes in the frequency and intensity of extreme events, and reliably predict extreme values at unobserved locations. However, such big data sizes also impose challenges for statistical modeling and computation. The objective of this project is to combine theoretical methods and computational approaches to develop novel models, along with inference and prediction algorithms, to meet the increasing demand of efficient analytical tools for extreme values in big data. In particular, the project will focus on the following research thrusts. First, a new class of nonstationary max-stable process models will be developed with flexible and desirable dependence structures for high-dimensional spatial extreme values. Then new scalable and parallelizable inference tools will be proposed for the estimation of the proposed nonstationary max-stable process models. Afterwards, divide-and-conquer conditional sampling algorithms will be studied for the prediction of extremes over large spatial data, which provides both point estimations and uncertainty measures for the predicted values at unobserved locations. Finally, the developed method will be applied to solve real problems."
"1561348","Cyberinfrastructure-Enabled Collaboration Networks","SMA","SciSIP-Sci of Sci Innov Policy","09/01/2016","07/19/2016","Jian Qin","NY","Syracuse University","Standard Grant","Cassidy Sugimoto","08/31/2019","$381,481.00","Jeffery Hemsley","jqin@syr.edu","OFFICE OF SPONSORED PROGRAMS","SYRACUSE","NY","132441200","3154432807","SBE","7626","7626","$0.00","Cyberinfrastructure enables collaborative research and significantly impacts scientific capacity and knowledge diffusion. In response to the growing need for quantitatively evaluating outcomes and impact of federal investment on research, this project deploys new data, tools, metrics, and methods for assessing the impact of cyberinfrastructures and the data services built on them. This research helps researchers and policy makers understand how cyberinfrastructure affects collaboration dynamics and network structures of researchers.  Datasets organized by longitudinal, thematic, topical, geographical, institutional, and author dimensions provided, which researchers, policy makers, and students can access and use to explore data-intensive science of science and innovation policy related research.<br/><br/>Metadata from GenBank, patent data from U.S. Patent and Trademark Office and funding data from NIH ExPORT are analyzed with descriptive statistics and models from Complex Network Analysis. The project not only examines the topological properties of the data submission and publication networks, but also the temporal ordering of collaborative relationships and the overlap of the sequence submission and publication networks. Through slicing, plotting, and visualizing data, appropriate sampling strategies and algorithms are developed to more deeply explore collaboration networks, both structurally and temporally. Algorithms used in community detection, machine learning, and visualization serve as primary computational methods in this research. Data products to be shared with research communities include 1) discovery lifecycle datasets containing sequence submissions, publications, and patents as well as the links between them and 2) funding factor datasets containing links between U.S. federal funding data and the discovery lifecycle datasets."
"1637312","S&CC:  Promoting a Healthier Urban Community: Prioritization of Risk Factors for the Prevention and Treatment of Pediatric Obesity","CNS","","09/01/2016","08/05/2016","Ming Dong","MI","Wayne State University","Standard Grant","David Corman","08/31/2019","$199,996.00","Dongxiao Zhu, Elizabeth Kuhl","mdong@cs.wayne.edu","5057 Woodward","Detroit","MI","482023622","3135772424","CSE","033y","042Z, 7916","$0.00","Urban communities are facing many challenges due to the increasing complexity of urban life, declining urban services and growing health and economic disparities. While diverse stakeholders are engaged in understanding and solving these issues, progress has not been commensurate with the effort, attributed partially to the limited collaboration and data sharing. The persistence of obesity disparities in early childhood is one example of the negative consequences of such isolated efforts. Obesity is a multi-faced health outcome. While some risk-factors for obesity are universal, others are highly specific to the community in which a particular child lives. As such, successful efforts to prevent and treat pediatric obesity depend upon integration of data from multiple community sources and systems. The overall objective of the proposed research is to develop an innovative data-driven health informatics system (Preschool Risk for Obesity Portal; PROP) that aims to promote comprehensive, efficient, and personalized obesity-related care for preschoolers living in urban communities. Through the data sharing and integration within the community and the development, along with the beta-test of PROP, the project has the potential to promote a healthier urban community. Through the data sharing and integration within the community and the development, along with the beta-test of PROP, the project has the potential to promote a healthier urban community. The approach taken could be adapted for older pediatric age-groups, adults, and to address other health disparity issues in urban communities. <br/><br/>From a technical perspective, the PIs will: 1) design innovative multi-level mixed effects machine learning methods and scalable algorithms that can precisely identify and prioritize a preschooler's personalized risk factors for obesity and 2) develop a data- and tool-rich online system dedicated to pediatric obesity. Specifically, design (Phase one) and proof-of-concept testing (Phase two) for the PROP algorithms will be completed in this exploratory work. After the successful completion, the second component of PROP (an eHealth intervention) will be developed in a separate, bigger project for efficacy trial (Phase three) and effectiveness research (Phase four). The significant intellectual merit of this project lies in the novel algorithms for information extraction and understanding from multi-scale, correlated, and heterogeneous datasets. The online system dedicated to pediatric obesity will be built for the rapid dissemination of core computational techniques to researchers."
"1618606","CSR: Small: Reconfigurable In-Sensor Architectures for High Speed and Low Power In-situ Image Analysis","CNS","Special Projects - CNS, CSR-Computer Systems Research","10/01/2016","04/11/2019","Christophe Bobda","AR","University of Arkansas","Continuing Grant","Marilyn McClure","09/30/2019","$493,870.00","Xiaoqing Liu","cbobda@ufl.edu","1125 W. Maple Street","Fayetteville","AR","727013124","4795753845","CSE","1714, 7354","7923, 9150, 9251","$0.00","Cameras are pervasively used for surveillance and monitoring applications and can capture a substantial amount of image data. The processing of this data, however, is either performed a posteriori or at powerful backend servers. While a posteriori and non-real-time video analysis may be sufficient for certain groups of applications, it does not suffice for applications such as autonomous navigation in complex environments, or hyper spectral image analysis using cameras on drones, that require near real-time video and image analysis, sometimes under SWAP (Size Weight and Power) constraints. <br/><br/>This work hypothesizes that future data challenges in real-time imaging can be overcome by pushing computation into the image sensor. Such systems will exploit the massive parallel nature of sensor arrays to reduce the amount of data analyzed at the processing unit. To this end, vertically integrated technology, such as focal plane sensor processors (FPSP), have been developed to overcome the limitations of conventional image processing systems. While some of these devices are programmable and offer the benefits of close-to-sensor processing such as performance and bandwidth reduction, they exhibit many drawbacks. For instance, each column of pixels is handled by a single processor, which reduces the parallelism and all pixels are treated equally and processed at the same rate, despite differences in input relevance for the application at hand. Consequently, systems spend more time spinning on non-relevant data, which increases sensing and computation time and power consumption. Research on FPSPs has mostly focused on technology aspects with some proof of concepts. Architectural design approaches, that involve high-level synthesis with the goal of mapping applications to low-level architectures, have not gained a lot of attention.<br/><br/>To overcome the limitations of existing architectures, the goal of this research is the design of a highly parallel, hierarchical, reconfigurable and vertically-integrated 3D sensing-computing architecture (XPU), along with high-level synthesis methods for real-time, low-power video analysis. The architecture is composed of hierarchical intertwined planes, each of which consists of computational units called XPUs. The lowest-level plane processes pixels in parallel to determine low level shapes in an image while higher-level planes use outputs from low-level planes to infer global features in the image. The proposed architecture presents three novel contributions: a hierarchical, configurable architecture for parallel feature extraction in video streams, a machine learning based relevance-feedback method that adapts computational performance and resource usage to input data relevance, and a framework for converting sequential image processing algorithms to multiple layers of parallel computational processing units in the sensor. <br/><br/>The results of this projects can be used in other fields, where large amounts of processing need to be performed on data collected by generic sensors deployed in the field. Furthermore, mechanisms for translating sequential constructs into functionally equivalent accelerators using hardware constructs will lead to highly parallel and efficient sensing units that can perform domain specific tasks more efficiently."
"1622390","Network Comparison, a Cornerstone of the Foundations of Network Science","DMS","CDS&E-MSS","09/01/2016","09/28/2017","Laurent Hebert-Dufresne","NM","Santa Fe Institute","Standard Grant","Yong Zeng","06/30/2018","$125,000.00","Joshua Grochow, Laurent Hebert-Dufresne","lhebertd@uvm.edu","1399 HYDE PARK ROAD","SANTA FE","NM","875018943","5059462727","MPS","8069","8083, 9150, 9263","$0.00","""Big data"" increasingly means big networks, because such data either directly concerns relational structures as in human and animal mobility patterns, gene interaction networks, or is influenced by an underlying relational structure as in epidemiological studies, urban studies, and cultural diffusion. Most applications of networks rely crucially on comparing networks, for example to detect changes in one network across time, to categorize or classify multiple networks of similar types, or to build analogies across fields by comparing networks of different origins. The question of how to compare two networks in a principled way, without relying on the ad hoc choice of statistics used by many current comparison methods, is key to the foundations of network science. This project will bring to bear new ideas from mathematics, computer science, and statistical physics on the problem of principled, structural comparison of networks. Through pre-existing collaborations, the PIs will leverage these new comparison methods to address questions in several different areas, for example, about: how food webs change across gradients like latitude, altitude, and temperature,morphological growth patterns of bacterial colonies, the evolution of human culture and communities, and links between socio-economic indicators and epidemiology.<br/><br/><br/>Our project will develop new rigorous and principled methods of comparing the structure of complex networks. The methods to be pursued aim to get away from single-scale summary statistics; to break new ground, we must think in terms of structural distance rather than statistical inference. In combination with tools from machine learning, such structural comparison methods are an important step towards defining the ""space of real-world networks"", which could serve as a more rigorous basis for a theory of complex networks. These methods have four advantageous features: (1) they systematically consider multiple scales of network organization, (2) they do not depend on an identification of the nodes of the two networks beforehand, (3) they can compare networks of different sizes, and (4) they are not dependent on any particular generative model of network growth. Very few, if any, of the existing network comparison methods have all of these features, and those that do exist have not been extensively developed. These features enable many new applications in a range of areas, including ecology, microbiology, cultural evolution, and epidemiology."
"1612978","From Approximate to Exact Designs with Applications to Big Data","DMS","STATISTICS","06/01/2016","05/20/2016","Wei Zheng","IN","Indiana University","Standard Grant","Gabor Szekely","04/30/2018","$149,983.00","","wzheng9@utk.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","MPS","1269","7433, 8083","$0.00","Design of experiments is an integral part of the scientific process in many areas of research with a direct impact on society, such as the biological sciences, the health sciences, the social sciences, engineering, marketing, and education.  A well-chosen design facilitates the collection of data that, at a minimum cost, maximizes the information for the scientific questions of interest.  Many scientific studies allow for repeated use of conceptual units, so that developing tools for optimal design for these problems has great potential impact.  Particularly in the realm of big data, there is much room for improvement of existing methods for design of experiments, and the tools and concepts under development in this research project have potential to lead to significant gain of information without increasing computational cost.  Results from the project will be made available to researchers in other areas through easy-to-use software that implements the algorithms to be developed. Graduate students will be trained to become researchers in design of experiments. <br/><br/>This project aims to result in a major leap forward in understanding and knowledge of optimal design of experiments. Recent work in the field has had a significant impact on the advancement of optimal crossover designs and designs for interference models for arbitrarily given covariance structures and design size configurations. However, these results have for the most part been limited to approximate designs for relatively simple models. While these results are arguably important in their own right, this project will extend methods and tools to achieve the ultimate goal of deriving exact designs for a wider spectrum of practical models. The results will be a much needed addition to our collective design toolbox. Most importantly, this project will go beyond the territory of design and apply the tools and ideas from design of experiments to subsampling problems emerging in big data with both statistical and machine learning methods under consideration. Preliminary results indicate that this is an opportune time to make these challenging but critical steps."
"1617690","SHF: Small: Collaborative Research: Coupling Computation and Communication in FPGA-Enhanced Clouds and Clusters","CCF","Software & Hardware Foundation","06/15/2016","01/23/2017","Anthony Skjellum","AL","Auburn University","Standard Grant","Almadena Chtchelkanova","01/31/2018","$232,945.00","","tony-skjellum@utc.edu","310 Samford Hall","Auburn University","AL","368490001","3348444438","CSE","7798","7923, 7942, 9150, 9251","$0.00","The introduction of Field Programmable Gate Arrays (FPGAs) to accelerate clusters of servers in datacenters and clouds provides a great, immediate opportunity to leverage a new technology in high-end computing. With their flexible logic and native massive communication capability, FPGAs are ideal for high-performance computing in the post-Moore?s Law world. Since the hardware adapts to the application higher efficiency can be achieved, and since FPGAs are hybrid communication/computation processors, they can be interconnected directly chip-to-chip. Large-scale communication can consequently proceed with both higher bandwidth, lower latency, and less processor impact. These features are crucial to enhancing performance beyond current levels. The proposed design allows for useful processing while data is in flight in the network resulting in reduced software overhead in parallel middleware and reduced network congestion. The key tenets of the research are to achieve programmable, intelligent acceleration of applications while emphasizing overlap of communication and computation at low latency, while also cutting substantially software overhead. <br/>The research project, FC5 (an FPGA framework for coupling communication and computation in clouds and clusters) has several thrusts. First, hardware support for FC5 and investigation of methods of configurability in FC5 to reduce communication latency and support computing in the network are studied. A second outcome is a prototype version of the Open MPI open source version of MPI-3.1 parallel middleware that utilizes FC5 to deliver the features and performance enhancements involving data movement between and within servers, mathematical data reductions, and bulk data reorganizations. Third, proof-of-concept versions of multiple FC5 software models, including direct hardware access, a transparent MPI-in-OpenCL, and an API-based mechanism that exposes essential functionality. Finally, because FC5 is evolving rapidly with major new announcements expected imminently, continued refinement is essential. At least two model applications, Molecular Dynamics and Map-Reduce, will be used as test cases. <br/>With the continued consolidation of computing services into the cloud, the potential broader impact is to increase both the scale and availability of parallel applications. The broad range of uses of cloud and cluster computing for commercial, government, and academic applications means that acceleration offered will have a widespread impact applicable across many sectors. The growing acceptance of high performance computing in industry (e.g., fast machine learning) is one particular potential commercial sector that will be enhanced by this project."
"1562659","SHF: Medium: Collaborative Research: Next-Generation Message Passing for Parallel Programming: Resiliency, Time-to-Solution, Performance-Portability, Scalability, and QoS","CCF","Software & Hardware Foundation","06/01/2016","06/07/2017","Anthony Skjellum","AL","Auburn University","Continuing Grant","Almadena Chtchelkanova","02/28/2018","$283,173.00","","tony-skjellum@utc.edu","310 Samford Hall","Auburn University","AL","368490001","3348444438","CSE","7798","7924, 7942, 9150, 9251","$0.00","Parallel programming based on MPI is being used with increased frequency in academia, government (defense and non-defense uses), as well as emerging uses in scalable machine learning and big data analytics.  Emerging supercomputer systems will have more faults and MPI needs to be able to workaround such faults to be appropriate to these emerging situations, rather than causing an entire application to fail.  Collaborative, transformative message passing research for High Performance Computing (HPC) critical to performance-portable parallel programming in new and forthcoming scalable systems (with a strategy of ""best practice-first, standardization-later"") is being reduced to practice. A substantial subset of the Message Passing Interface (MPI-3/4) application programmer interface is being made fault tolerant through extensions with weak collective transactions that synchronize between parallel tasks. <br/><br/>This research studies  the novel model that localizes faults, provides tunable fault-free overhead, allows for multiple kinds of faults, enables hierarchical recovery, and is data-parallel relevant.  Fault modeling of underlying networks is being studied. Application developers control the granularity and fault-free overhead in this effort. Performance and scalability results of the middleware prototype are being demonstrated principally through compact applications that relate to real use cases of practical and academic interest. The impact of this work ranges from users of the largest supercomputers in government labs to practical clusters that have long-running, time-critical applications, and to space-based and other parallel processing in ""hostile"" environments where faults occur more frequently than in past years.  The project is producing usable free software that will be widely shared in the community as well as guidance on how better parallel programs can be written in academia, industry, and government.  The project also provides guidelines for how to update existing or legacy programs to use the new capabilities that are being reduced to practice."
"1616774","NeTS: Small: Software-Defined Data Plane for Datacenters","CNS","Networking Technology and Syst","09/01/2016","08/29/2016","Arvind Krishnamurthy","WA","University of Washington","Standard Grant","Darleen Fisher","08/31/2019","$400,000.00","Xi Wang","arvind@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7363","7923","$0.00","Many popular cloud applications that power our daily lives, such as Facebook, Google, Amazon, and Twitter, spend a large fraction of their time performing network operations.  Even supposedly computationally intensive applications such as parallel machine learning are often limited by communication performance.  Datacenter network designers have been racing to keep up with this increased reliance on network usage, but they have had only limited success in achieving desirable system properties as they are often constrained by the lack of switch support for deploying new protocols.<br/><br/>Fortunately, innovation in switch and network interface card (NIC) design is now focused on building not just faster but also more flexible packet processing devices. Whereas traditional switches and NICs typically provide functionality to route and forward packets, many upcoming switches have support for transforming the packet as well as performing computations on the packet before routing it towards its destination.  These devices have the potential to revolutionize the use of networking devices within datacenters as they provide the ability to reconfigure packet processing and deploy new protocols.<br/><br/>This project will investigate flexible packet processing functionality on NICs and switches and their potential for optimizing high performance networked systems inside the datacenter. This work provides new abstractions and building blocks for the use of flexible packet processing pipelines, while respecting the hardware constraints that will be associated with these technologies.  The researchers will examine how the resulting data plane functionality can be used to implement resource allocation mechanisms inside the datacenter so as to enable congestion control, performance isolation, adaptive routing, and efficient load balancing.  They will also examine implementing on the NICs and switches some of the packet processing traditionally done in end-host software. The goal is to show how and by how much flexible packet processing can benefit widely used datacenter applications and also provide guidance on what features that future iterations of the hardware should provide in order to significantly improve application performance.<br/><br/>Broader Impact: Network-intensive datacenter applications are used by literally billions of people around the globe on a daily basis. By improving the efficiency of network operations, results from this project can dramatically reduce the cost of provisioning existing public services, like Wikipedia, as well as make it much cheaper for new public services to be developed. By enabling the deployment of more effective resource allocation protocols on flexible switches, this work also has the potential to provide substantial improvements to the networking performance within datacenters.  The researchers will publicly release the developed software and enable a rich set of network protocols and high-performance datacenter applications."
"1566292","CRII: SCH: Accelerating Human Microbiome Analysis using Lightning-Fast Cloud Computing","IIS","CRII CISE Research Initiation","04/01/2016","03/21/2016","Tae Hyuk Ahn","MO","Saint Louis University","Standard Grant","Wendy Nilsen","03/31/2019","$174,174.00","","ahnt@slu.edu","221 N. Grand Blvd.","St Louis","MO","631032006","3149773925","CSE","026Y","8018, 8228, 9150","$0.00","Nontechnical description: Humans carry ten times more bacterial cells than human cells, and a hundred times more bacterial genes than the inherited human genome. Human microbes also hold secrets for maintaining health and preventing disease. For the last decade, a cultivation-independent metagenomics approach, in which all microorganisms in a sample are directly sequenced together, has been intensely applied to understand microbes' impact on human health. A new generation of sequencing technologies accelerated research, but left a vast amount of metagenomic sequencing data to be analyzed. Software and high-performance computing systems that could speed analysis are still lacking. The PI proposes to develop novel computational algorithms and cloud computing software to decipher terabytes of metagenomic sequencing data for studying the human microbiome. Experience from these pursuits will accelerate development of the proposed tools for better understanding the ecosystem in our bodies. Ultimately, this may contribute to better diagnosis, prevention, and treatment of disease. Furthermore, the proposed cloud computing algorithms and techniques could be adapted to many other applications demanding high computation complexity. A key proposal ingredient is offering graduate and undergraduate computer science students a unique opportunity for interdisciplinary research designing algorithms and software to solve biological problems. <br/><br/>Novel computational algorithms and a cloud computing software tool are proposed, to analyze large-scale metagenomic sequencing data to study the human microbiome. The project would feature Apache Spark, a cutting-edge, open-source cluster computing framework for large-scale data processing. It supports a rich set of high-level tools including scalable machine learning and graph processing libraries. The primary novelty is a cloud scalable de novo assembler, and the ability to compare assembled sequences to existing reference genomes using Spark libraries. This new approach will speed identification of novel genomes and composition of microbes from large metagenomic data. Most existing metagenomic analysis methods separately execute de novo sequence assembly and taxonomy classification with many existing reference genomes. Key technical innovations of the proposed work are (i) cloud computing algorithms enabling a fast and scalable metagenome assembler, (ii) taking assembled sequences directly for taxonomy to dramatically reduce computation time, and (iii) a cloud container package allowing researchers to analyze metagenomic data easily and cheaply. Providing a cloud container package with a simple Web interface will enable researchers to analyze their large-scale metagenomic sequence data readily and quickly for human health, biosurveillance, and pan-genomic analysis of microbiota."
"1632460","SBIR Phase II:  Versatile Robot Hands for Warehouse Automation","IIP","STTR Phase II","09/01/2016","12/10/2018","Lael Odhner","MA","RightHand Robotics, LLC","Standard Grant","Muralidharan Nair","03/31/2019","$750,000.00","","lael@righthandrobotics.com","21 Wendell St Apt 20","Cambridge","MA","021381850","6175010085","ENG","1591","1591, 6840, 8035, 9139, HPCC","$0.00","The broader impact/commercial potential of this project affects one of the fastest-growing sectors of the US economy. E-commerce sales in 2015 accounted for 7.4% of total U.S. retail and are expected to rapidly rise. The potential for the commercial impact of general each-picking systems is high, as current manual labor methods are pain points for distribution centers; human picking is unpleasant, expensive and inefficient due to high absenteeism, high turnover and human error. The success of the proposed technology will also contribute to American competitiveness in the robotics industry. Of the top 20 distribution system integrators, only three are currently based in the U.S. Robotics is going to be the key driver of progress in this area, where each-picking, our core product capability, is a key component of future automated distribution systems. Beyond warehousing logistics, applications that our technology can benefit include: broad applications of industrial automation and manufacturing; military applications (e.g., IED disposal, where robots can perform tasks that are dangerous for humans to perform); and assistive healthcare (e.g., where robots must be compliant enough to be safe around humans while interacting successfully with unknown environments).<br/><br/>This Small Business Innovation Research Phase II project will focus on the development of a state-of-the-art each-picking robotic system and its deployment, initially targeted at the order fulfillment industry. To date, robotic systems have enabled significant progress on transporting inventory on shelves or in totes. However, there has not yet been a deployed system that can perform the task of picking individual items from inventory bins and placing them in boxes for shipment. During Phase I of this project, RightHand Robotics developed a picking system far in advance of the research literature on robotic grasping, picking tens of thousands of items previously unseen objects, with error rates of less than 0.1%. During Phase II, the project will focus on advancing the state of the art in data-driven refinement of grasp planning using machine learning techniques, and will develop methods for box-packing that exploit the company?s advanced compliant grippers. These improvements will result in an average pick-and-place time of 6 seconds or less and an undetected placement failure rate of fewer one in ten thousand."
"1659139","III: Small: Collaborative Research: Reducing Classifier Bias in Social Media Studies of Public Health","IIS","Info Integration & Informatics","08/01/2016","08/24/2016","Sherry Emery","IL","National Opinion Research Center","Standard Grant","Maria Zemankova","07/31/2018","$139,992.00","","emery-sherry@norc.org","1155 E. 60th Street","Chicago","IL","606372745","7732566000","CSE","7364","7364, 7923, 9102","$0.00","Social media creates a new opportunity for public health research, giving greater reach at lower cost than traditional survey methods.  Online content offers several potential advantages over traditional survey data; one can in real-time measure how behaviors and attitudes change in response to rare events such as legal changes, new products, and marketing campaigns.  Machine learning techniques for classification can be used to tailor interventions that improve health outcomes while minimizing costs.  However, online content is not a random sample, potentially biasing the outcomes.  This proposal develops techniques to overcome this problem, enabling effective use of publicly available social media data for public health research.  The approaches are evaluated against a traditional survey-based approach to evaluate end-to-end effectiveness in a real-world public health scenario, determining effectiveness of smoking cessation campaigns.<br/><br/>The project builds on well-grounded statistical approaches to eliminate classifier bias.  Key innovations are extending this to the high-dimensional, noisy domain of textual social media data (specifically Twitter), robustness to confounding variables, and scalable methods to identify comparison groups.  Noisy data will be addressed through advancing multiple imputation techniques.  The project will develop a model-based approach to identifying comparison groups that addresses confounding variable issues.  The methods will be evaluated in the context of an actual public health study of smoking cessation, based on historical Twitter data and traditional surveys conducted before and after a CDC campaign as well as a survey of smokers on perceived risk factors of e-cigarettes."
"1566175","CRII: CSR: Large-scale Systems Software Atop Scale-out In-memory Storage","CNS","CSR-Computer Systems Research","05/15/2016","01/13/2016","Ryan Stutsman","UT","University of Utah","Standard Grant","Marilyn McClure","04/30/2019","$174,949.00","","stutsman@cs.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","7354","8228, 9150","$0.00","Research over the past 5 years on in-memory data center storage has resulted in systems that can provide on-demand access to billions of pieces of information per second. However, the benefits of these fast systems has not resulted in more powerful and interesting applications for users. The key problem is that these new storage systems are stripped down to provide speed and can only fulfill very basic requests for information. To compensate, applications must make many requests to these storage systems to perform the same operations they were able to perform with a single request to older database systems.  The result is that applications spend most of the performance benefits they gain from these systems overcoming their limited programming model.<br/><br/>This work seeks to understand the hidden costs of the limited interfaces of these large-scale in-memory key-value stores and to alleviate those costs with a new stored procedure model uniquely suited to these systems. To explore these costs, data-intensive applications from three different domains will be prototyped on today?s low-latency key-value stores, first with their conventional interfaces and then again with custom application logic built into the storage system. The results of this exploration will motivate and aid in the design of the main contribution of this work: a generalized model for safely collocating application logic inside of key-value stores. Application prototypes will include a scale-out database kernel, a simple scale-out graph database, and large-scale machine learning algorithms.<br/><br/>A rich programming model for developing large-scale software systems will lower the barrier for building information-intensive applications: an art that requires specialized expertise. Low-latency systems equipped with fast stored procedures will be a powerful tool for building new, data-intensive systems in industry, science, and the military. They will enable deep real-time analysis of social and natural graphs; rich, interactive worlds where millions of users manipulate and modify a shared environment; the fine-grained coordination and routing of millions of autonomous vehicles on the highways in a metropolitan area; and real-time decision support for military intelligence. All components will be built as a practical and usable software system with all development done publicly and available as open-source."
"1612914","Conference on Modeling Neural Activity: Statistics, Dynamical Systems, and Networks","DMS","STATISTICS","05/15/2016","05/10/2016","Robert Kass","PA","Carnegie-Mellon University","Standard Grant","Gabor Szekely","04/30/2017","$20,000.00","","kass@stat.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","MPS","1269","7556","$0.00","This award supports participation in the conference on Modeling Neural Activity: Statistics, Dynamical Systems, and Networks (MONA2) held June 22-24, 2016 in Lihue, Hawaii. Many disorders, such as ADHD, autism, and schizophrenia, as well as stroke and various neurodegenerative diseases, are thought to involve dysfunction of neural networks. Because computational neuroscience aims to supply principles for understanding the activity of individual and collective neural firing patterns, its successes can help in formulating mechanistic descriptions of pathophysiology. This conference will bring together statisticians and computational neuroscientists from the US and Japan in order to enhance collaborations between scientists in the two countries. NSF funding will help support the involvement of the US researchers. <br/><br/>Computational neuroscience has grown, in distinct directions, from the success of biophysical models of neural activity, the attractiveness of the brain-as-computer metaphor, and the increasing prominence of statistical and machine learning methods throughout science. This has helped create a rich set of ideas and tools associated with ""computation"" to study the nervous system, but it has also led to a kind of balkanization of expertise. There is, especially, very little overlap between mathematical and statistical research in this area. Important breakthroughs in computational neuroscience could come from research strategies that are able to combine what are currently largely distinct approaches. This award seeks to encourage the creation and enhancement of collaborations between scientists from the US and Japan.  More information can be found on the conference web site http://www.stat.cmu.edu/mona2."
"1602592","Statistical Challenges in Modern Astronomy VI","DMS","SPECIAL PROGRAMS IN ASTRONOMY, STATISTICS","05/01/2016","04/21/2016","Chad Schafer","PA","Carnegie-Mellon University","Standard Grant","Gabor Szekely","04/30/2017","$20,000.00","Shirley Ho","cschafer@stat.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","MPS","1219, 1269","1206, 7556","$0.00","A conference on Statistical Challenges in Modern Astronomy VI will be held at Carnegie-Mellon University, June 6-10, 2016. (See http://www.scma6.org/.) Astronomical data are continuously being gathered from surveys that are increasingly ambitious in scope, and astronomers face a wide range of challenges when transforming this information into scientific conclusions. This conference brings together statisticians and astronomers to work on problems in astronomy involving large amounts of data. It has been held every five years since 1991. <br/><br/>The conference Statistical Challenges in Modern Astronomy VI will be held at Carnegie-Mellon University, June 6-10, 2016. Over 15 years the Sloan Digital Sky Survey (SDSS) has collected 100 terabytes data from hundreds of millions of stars and galaxies, as well as a million supermassive black holes; by contrast, the Large Synoptic Survey Telescope (LSST), planned for completion by 2022, will gather 15 terabytes of data per night over its ten-year life. The full scientific potential of these surveys will be realized not only with ingenuity, but also with novel methods of data analysis. Experts in statistics and machine learning can make crucial contributions both to these specific domains, and to general methodological development for Big Data applications. This conference is a major effort to increase the interaction of astronomers and data scientists and to build the next generation of scientists to bridge the gap between these disciplines."
"1560332","REU Site: Diverse Undergraduate Research Experiences in Statistics","DMS","WORKFORCE IN THE MATHEMAT SCI","02/01/2016","04/22/2018","Mark Ward","VA","American Statistical Association","Continuing grant","Nandini Kannan","01/31/2019","$380,340.00","Jessica Utts","mdw@purdue.edu","732 North Washington Street","Alexandria","VA","223141943","7036841221","MPS","7335","9250","$0.00","The American Statistical Association (ASA) will invigorate the undergraduate research community in statistics by implementing a distributed network of research experiences for undergraduates sites.  At each site, student teams will work with complex data sets from varied scientific and engineering disciplines in which statistics plays a key role for data analysis.  Over a three year period, the project will have nine sites with four students per site, allowing 36 students to have a research experience in statistics during the lifetime of the grant.  Recruitment will target women, minorities, and persons with disabilities.  Supported students must be U.S. citizens, nationals, or permanent residents.  Key metrics for success will be whether the students subsequently pursue graduate studies, what kind of career choices they make, and how their statistics training impacts their choice of careers and their success in such careers.  This initiative should spark and sustain a new excitement about undergraduate research throughout the statistics discipline and has the potential to impact fields that rely on statistics such as engineering, atmospheric science, healthcare, and all kinds of public policy.  <br/><br/>A unifying theme of the funded proposals is that students will engage in all stages of the data analysis cycle, including: data verification, data cleaning, and data visualization, all the way to statistical modeling, prediction, and data mining/machine learning/computational statistics, as an integral part of their statistics research.  One result of these REUs will be a larger number of students who are immersed in the kind of data science analysis that is emerging in every branch of science and engineering.  At least 2 faculty mentors are necessary for each project.  Priority will be given to faculty who either (a) have a strong track record as a mentor with undergraduate student researchers in the past, and/or (b) have a strong track record of independent research and who are eager to begin working with undergraduate students.  The ASA views this new initiative as an opportunity to teach the teachers, too, by coordinating faculty training sessions."
"1555816","SBIR Phase II:  Anomaly and malware detection using AC power analysis","IIP","SMALL BUSINESS PHASE II","04/01/2016","09/21/2017","Benjamin Ransford","MI","Virta Laboratories Inc.","Standard Grant","Peter Atherton","09/30/2018","$897,869.00","","ben@virtalabs.com","1395 Folkstone Court","Ann Arbor","MI","481052888","9176215524","ENG","5373","169E, 5373, 8032, 8033","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project will be an improved cybersecurity posture among industries that rely on mission-critical computing systems that are difficult to patch and/or monitor. Many technical failures on these systems, including information breaches, equipment malfunctions, and malware infestations, stem from longstanding problems that go undetected.  This project develops nonintrusive measurement and analytics tools that give IT operators operational visibility into high-assurance assets, increasing confidence in their correct operation.  This project will reduce losses due to unscheduled maintenance across industries, improve the trustworthiness of embedded and semi-embedded systems such as software-based medical devices, and reduce the business risk of breaches and malware damage at organizations that rely on hard-to-manage devices. This project also advances the state of the art in detecting rogue software execution and other anomalies through nonintrusive side channels, such as observing power signals collected on a wall outlet. <br/><br/>This Small Business Innovation Research (SBIR) Phase II project aims to extend operational visibility of IT departments into mission-critical computing equipment using a novel combination of nonintrusive signal collection and machine learning. From infusion pumps to Internet routers to retail point-of-sale terminals, organizations rely on fixed-purpose computing systems. With this reliance comes three key risks. First, the effects of unscheduled interruptions to critical systems ripple outward to other business areas. Second, critical systems are often incompatible with constantly changing mainstream tools such as host-based antivirus and intrusion-detection systems. Third, critical systems often lag behind other systems patch levels because they are rarely taken out of service for patching. This project addresses these challenges by providing nonintrusive monitoring for critical systems in situ, reducing the risk of unscheduled downtime due to abnormal behavior. The company's monitoring hardware and software observe software execution from the vantage point of the power line, requiring no modifications to monitored systems and extending the ability of operators to understand what critical systems are doing."
"1648337","SBIR Phase I:  Virtual Fences for Sustainable Protection","IIP","SMALL BUSINESS PHASE I","12/15/2016","12/04/2016","Kenneth Parker","OH","The Samraksh Company","Standard Grant","Rick Schwerdtfeger","02/28/2018","$224,963.00","","kenneth.parker@samraksh.com","5980 Venture Dr, Suite 1B","Dublin","OH","430172267","6142101145","ENG","5371","5371, 8028, 8033","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project comes from addressing a demand in many parts of the world for a scalable technological solution for managers of nature resources to deal with human-animal conflicts and rapid deforestation. Natural resources have large and sometimes surprising ecosystem impact. Even the direct economic value of individual wildlife and forest reserves is often in the hundreds of millions of dollars. Operationally relevant protection technologies that are sustainable can serve as an import tool in effective management. Having a low total cost of ownership and effectively co-existing with conventional human surveillance methods is key to sustainability. We seek to commercialize a protection system that is sustainable as it offers low cost, long lived devices, ease of use and maintenance, robust performance across diverse natural environments, and stealthy operation. It also augments accountability of the guards, and automates tasks such as notifying management and sharing forensic evidence of incidents in settings where communication infrastructure is not readily available.<br/><br/>The proposed project addresses the challenge with reducing energy consumption for our battery powered, radar device based, mesh network solution for persistent, time-sensitive protection of natural resources. Commercial impact for the proposed system can only be realized if the sensor is low-power, the data driven machine learning based signal processing is low-power, and the wide area networking is low-power. Addressing all these will unlock commercial value, but the impact is limited by the least efficient part of the system. Today the radar's efficiency is the essential bottleneck in system performance; it consumes about 40 milliWatts (mW). Reducing the radar's consumption to 5mW would double, and reducing it to 1mW would at least triple, the time between battery changes. The proposed effort would design, implement, and validate power reduction in the radar subsystem and integrate the result into an overall system. The largest technical risk in the plan is the Radio Frequency subsystem, which will therefore be addressed in Phase I. Phase II will implement power reductions in all the other radar subsystems, integrate the results into a coherent sensor, and integrate that sensor into the overall system. This integration into the larger system will also deal with whole system optimizations."
"1648466","SBIR Phase I: Design, deployment, and algorithmic optimization of zoomorphic, interactive robot companions","IIP","SMALL BUSINESS PHASE I","12/15/2016","09/27/2017","William Knox","TX","Emoters, Inc.","Standard Grant","Muralidharan S. Nair","05/31/2018","$225,000.00","","brad@emotersrobots.com","1701 Maple Ave","Austin","TX","787021433","5125423333","ENG","5371","5371, 6840, 8034, 8035, HPCC","$0.00","The broader impact/commercial potential of this project spans the near-term and many years of future development. Over both phases, this proposal covers research and development (R&D) to create a robot pet companions with the potential to sell millions of units in the U.S. toy industry. The proposal also supports the development of R&D infrastructure that will be a critical component of the expansion in subsequent years to the U.S. pet industry (as a robot companion), which is larger and has less direct competition for robotic entrants. The first-generation robot product, a result of the Phase I project, will support science, technology, engineering, and math (STEM) education and robot hobbyists of all ages by facilitating user-friendly modification of its hardware and software as well as the creation of users? own robots and behavioral programs. Given the interactive nature of these modifiable robots, they are likely to have a strong appeal to females, who are underrepresented in STEM fields. The project?s ultimate goal is to develop and market interactive robots that can improve the quality of life for anyone through companionship.<br/><br/>This Small Business Innovation Research (SBIR) Phase I project proposes to develop first-generation robot pets that will be ready to place in thousands of customers? hands and to situate the awarded company to grow to deliver millions of sophisticated robot pets across the world, including to the many people who cannot have pets. Towards these outcomes, the following innovations will be pursued in Phase I: a puppeteer platform that wirelessly controls robot characters; specification of a mobile, social robot character through machine learning; perception of robots and their environment; reliable autonomous recharging; and a simple cloud-based infrastructure for gathering usage data and conducting field experiments on versions of robot characters."
"1647377","SBIR Phase I:  Delivering Medical translations with a Health Belief Model Recommender Engine","IIP","SMALL BUSINESS PHASE I, SBIR Outreach & Tech. Assist","12/01/2016","02/01/2017","Michelle Archuleta","CO","AIpiphany, Inc.","Standard Grant","Jesus Soriano Molla","09/30/2017","$269,906.00","","marchu0399@gmail.com","1415 Park Ave W","Denver","CO","802052103","3032188507","ENG","5371, 8091","5371, 8018, 8023, 8032, 8042","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is the development of a first-in-class patient education tool. The project aims to develop a method to customize and personalize the delivery of medical content with the goal of influencing medication compliance and health outcomes. Imagine patients are discharged from the hospital and understand their discharge instructions. Imagine patients fill a list of ten medications but understand what they are for and why they need to take them. Imagine patients recently diagnosed with cancer that do not feel embarrassed to ask their doctor questions. Imagine parents caring for sick children or older children caring for elderly parents that feel assured of their ability to get the best care for loved ones. Imagine a reduction in sick days, hospital readmission rates, and morbidity and mortality rates. Imagine an overall improvement in the quality of care and health of the nation all due to the simple ingredients of patient understanding and patient empowerment. <br/><br/>The proposed project will produce a novel tool to deliver medical content to patients based of an assessment of their limiting beliefs and perceptions. It has long been established that the effectiveness of medical interventions aimed at changing behavior are limited by patient's perceptions and beliefs. The proposed project will result in the development of novel algorithms that use machine-learning (ML) based techniques to determine limiting beliefs. The goal is to customize patient education and medical content to the beliefs, perception, and language of the patients."
"1621899","SBIR Phase I:  Adaptive E-Triage in Emergency Medicine","IIP","SMALL BUSINESS PHASE I","07/01/2016","06/24/2016","Eric Hamrock","MD","Stocastic, LLC","Standard Grant","Jesus Soriano Molla","08/31/2017","$224,777.00","","info@stocastic.com","629 S. Belnord Ave.","Baltimore","MD","212243804","410848750","ENG","5371","5371, 8018, 8023, 8032, 8042","$0.00","The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase 1 project is to drive safer and more cost-effective emergency department care pathways by improved risk stratification at patient presentation (triage) compared to the current standards of care. E-triage addresses the ED crowding crisis (136 million visits in US annually) that adversely affects patients' health outcomes and has led to a state of financial unsustainability in America's safety net.  E-triage's approach supports new ED operational models to separate service streams for acutely ill and non-urgent patients.  New streaming models are needed to mitigate ED crowding by: (1) conserving scarce ED resources for patients truly in need of emergency care, and (2) preventing unnecessary waiting and costly resource over-utilization for non-urgent patients.  It does this by using local ED electronic health record (EHR) data to scientifically risk-stratify patients based on risk of critical events and severity of illness.  E-triage meets a commercial opportunity to mitigate crowding, enhance ED operational performance, and improve the value of healthcare delivered to ED patients.  The proposed project will transition E-triage to a scalable and commercially available platform under a business model that supports growth.  <br/><br/>The proposed project will yield a scaled and commercially available e-triage decision support platform that is currently being piloted in multiple emergency departments (EDs).  E-triage deploys a novel combination of data-science methods and flexible information technology architecture that supports usability by diverse ED customers. The tool relies on advancements in machine learning methods, mechanisms to harness user feedback, and software technology that is flexible and interoperable with EHR systems.  It must also securely transmit and store patient data and be computationally efficient to accommodate fast-paced ED environments.  E-triage enables rapid data-driven prognostication of ED patients at presentation based on risk of critical events and severity of illness using common locally collected ED data.  Compared to US triage practice standards, which relies heavily on provider subjective judgment, e-triage demonstrates improved identification of high- and low-risk patients based on evidence from retrospective and prospective evaluation.  E-triage is disruptive in its design to support new ED operational models that separate service streams for acutely ill and non-urgent patients toward reducing the burden of ED crowding."
"1566388","CRII: SaTC: Towards Non-Intrusive Detection of Resilient Mobile Malware and Botnet using Application Traffic Measurement","CNS","CRII CISE Research Initiation, Special Projects - CNS","08/01/2016","04/10/2017","Qiben Yan","NE","University of Nebraska-Lincoln","Standard Grant","Nina Amla","07/31/2019","$182,739.00","","qyan@msu.edu","151 Prem S. Paul Research Center","Lincoln","NE","685031435","4024723171","CSE","026Y, 1714","025Z, 8228, 9150, 9178, 9251","$0.00","The development of the mobile Internet economy has brought numerous benefits to people and society, with the promise of providing ubiquitous computing and communications. Mobile devices have penetrated almost every aspect of our lives and, as a result, are storing a large amount of personal data. Unfortunately, the promise of the mobile Internet is easily undermined by ""smart"" malware and botnets, creating a precarious situation in which sensitive data stored on mobile devices could be leaked to adversaries through the mobile Internet or a wealth of compromised mobile devices could launch a denial of service attack to destruct the mobile infrastructure. This project develops non-intrusive, network-based solutions to detect mobile malware and botnets and mitigate their impact to ensure that mobile communications are carried out in a trustworthy manner despite the potential security threats. The research offers valuable insights into mobile malware's spreading mechanisms and malicious intents and will inspire studies in network behavior analysis of mobile applications. The project also has an important educational impact via the creation of new mobile security course projects and modules, widening students' views of mobile system security, and guiding next-generation mobile developers to include security and privacy considerations in designing mobile protocols and apps.  <br/><br/>This project addresses three closely intertwined research issues in developing a network-based mobile malware detection system. The first part focuses on investigating malware traffic collection by identifying malware's network-related application program interfaces (APIs) and designing novel inputs to activate the malware's covert network behaviors. The second part focuses on designing a network-based malware detection system that identifies potential malware features based on their malicious network behaviors, which in turn will provide precise and unique identification of mobile malware. The third part focuses on the development of group behavior based detection mechanisms to identify organized network activities from malicious botnets that are built on the cooperation of malware. A local testbed will be developed to evaluate the performance of the proposed techniques and system designs, which aims to guarantee that the technologies developed are suitable for deployment in real mobile systems. The project uses machine learning techniques, statistical tools, and network traffic analysis to support secure communications in mobile networks."
"1629201","XPS: EXPL: Exploring the Design Space of Augmented Memory Controllers with Native Support for In-Memory Data Storage","CCF","Exploiting Parallel&Scalabilty","07/01/2016","06/28/2016","Tong Zhang","NY","Rensselaer Polytechnic Institute","Standard Grant","Yuanyuan Yang","06/30/2020","$299,621.00","","tzhang@ecse.rpi.edu","110 8TH ST","Troy","NY","121803522","5182766000","CSE","8283","","$0.00","As the cornerstone of the global information technology infrastructure, large-scale parallel processing platforms host an ever-increasing amount of real-time in-memory computing applications (e.g., data/graph analytics, transaction processing, machine learning, and business intelligence). As a result, large-scale distributed in-memory data storage has become a very important component in large-scale parallel processing platforms. Nevertheless, conventional realization of in-memory data storage tends to occupy a large amount of memory capacity and consume substantial CPU cycles. This makes in-memory data storage subject to a significant cost overhead in terms of both memory and CPU resources. How well this cost challenge can be addressed largely determines the overall system performance and efficiency of future large-scale parallel processing platforms. This project will have significant impact on the research community and the industry, while providing interdisciplinary training of graduate and undergraduate students, and draw broad participation of students of different levels and backgrounds in collaborative research and education.<br/><br/>This project proposes to improve the cost effectiveness of in-memory data storage by enhancing the function and data processing capability of the hardware memory controller. In particular, the in-memory filesystem and memory controller will explicitly cooperate together across the software/hardware layers and share the responsibility for optimizing the implementation of in-memory data storage. Such a cross-layer design framework enables the use of memory footprint reduction techniques to reduce memory resource cost without incurring CPU overhead. Moreover, the memory controller will integrate customized hardware engines that can carry out certain storage-oriented data processing tasks. Those customized storage data processing engines in the memory controller can be leveraged to directly reduce the memory and CPU resources overhead in the realization of in-memory data storage. An FPGA-based platform will be implemented to carry out experiments to further empirically validate the feasibility and potential effectiveness of the developed design solutions."
"1617735","AF: Small: Algorithms in Computational Geometry and Medical Applications","CCF","Algorithmic Foundations","09/01/2016","06/03/2016","Danny Chen","IN","University of Notre Dame","Standard Grant","Tracy Kimbrel","08/31/2021","$450,000.00","","dchen@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","CSE","7796","7923, 7929","$0.00","Modern medical research and imaging technologies together present physicians and clinicians with mind-boggling amounts of detailed molecular or cellular structures. For example, one 2-dimensional image slide can contain millions of cells. In fact, pathologists use microscopes to examine only tens or hundreds of cells at a time, missing most cells, and also missing possible patterns in the intermediate range. This project addresses the need to study patterns in cell images through powerful computational and analytic techniques in computational geometry, and to validate these algorithmic techniques, and software built from them, in medical research and applications.  <br/><br/>The project formulates challenging geometric and algorithmic problems that emerge in the diagnosis, prognosis, and analysis of diseases (e.g., breast cancer, inflammatory bowel disease, and rheumatoid arthritis), identification and classification of different types of immune cells (e.g., neutrophils, eosinophils, plasma cells, and lymphocytes), and tracking and analysis of collectively moving and swarming bacteria (e.g., Myxococcus xanthus and Pseudomonas aeruginosa) in terms of a class of Voronoi diagrams (developed by the lead investigator) in which collections of cells influence nearby neighbors. Specific geometric techniques include clustering-induced Voronoi diagrams, coupled optimal shape approximation, and matching based on earth mover's distance.<br/><br/>Exploring these geometric structures in important medical problems will not only lead to new methods in combinatorial optimization and machine learning, and new publically-available software for medical applications, but also train students in the broad range of interdisciplinary research and education from geometric algorithms, software development, to medical applications. By providing appropriate analysis tools for the physicians, this project and its students will help save lives."
"1646470","CPS:  Breakthrough: Wearables With Feedback Control","CNS","CPS-Cyber-Physical Systems","09/01/2016","09/02/2016","John Stankovic","VA","University of Virginia Main Campus","Standard Grant","Sylvia Spengler","08/31/2020","$425,000.00","","jas9f@virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","7918","7918, 8234","$0.00","Recently there is an increasing availability of smart wearables including smart watches, bands, buttons and pendants. Many of these devices are part of human-in-the-loop Cyber Physical Systems (CPS). With future fundamental advances in the intersection of communications, control, and computation for these energy and resource limited devices, there is a great potential to revolutionize many CPS applications. Examples of possible applications include detecting and controlling hand washing to prevent transmission of infections or bacteria, monitoring and using interventions to keep factory workers safe, detecting activities in the home for monitoring the elderly, and improving rehabilitation of stroke victims via controlled exercises. However, to date, much of the work on wearables concentrates on only sensing, collecting and presenting data. For use in CPS it is necessary to consider the increased use of new sensing modalities, to apply feedback to close the control loop, and to focus on the fundamental issues of how both the environment and human behavior affect the cyber. In particular, since humans are intimately involved with wearables it is necessary to increase understanding of how human behaviors affect and can be affected by the control loops and how the systems can maintain safety.<br/><br/>This work develops generic underlying algorithms for processing smart wearable data rather than one-off solutions, it extends the understanding and control of wearable systems by addressing humans-in-the-loop behaviors, and it explicitly focuses on the impact of the environment and human behavior on the cyber. Novel ideas are proposed for each of these areas along with a structure for their integration. For example, the algorithmic approach to support more robust, accurate and efficient activity recognition using wearable devices is based on five fundamental concepts: (i) Direction Agnostic Modeling, (ii) Direction Aware Modeling, (iii) Spatial Reachability, (iv) Spatiotemporal Segmentation, and (v) Dynamic Space Time Warping. For dealing with humans-in the-loop behaviors, Model Predictive Control (MPC) is extended to semantic based MPC. This solves control problems that are not amenable to electromechanical laws and employs machine learning. Many CPS projects do not explicitly address how the uncertain world affects how the cyber must be developed in order to perform robustly and safely. A new *-aware software development paradigm focuses on physical-cyber CPS issues as central tenets and that serves as an integrating platform for all the proposed work. The *-aware paradigm focuses on how software must be made robust to handle the physical world, while meeting safety and adaptability requirements"
"1566137","CRII: AF: Breaking Barriers for Geometric Data","CCF","","05/01/2016","04/12/2016","Benjamin Raichel","TX","University of Texas at Dallas","Standard Grant","Joseph Maurice Rojas","10/31/2019","$161,277.00","","Benjamin.Raichel@utdallas.edu","800 W. Campbell Rd., AD15","Richardson","TX","750803021","9728832313","CSE","026y","7796, 7929, 8228","$0.00","It is surprising how often geometric abstractions help us deal with understanding large systems:  molecules become balls and sticks, complex fluid or combustion simulations are shown as contours or isosurfaces, and movies become points in a high dimensional space to allow recommendations based on which other points are near one's favorite movies.  Computational Geometry, which develops efficient computer algorithms for problems stated in geometric terms, can thus play a central role in data analytics.  Traditionally, the focus in Computational Geometry was on exact algorithms with guaranteed performance on all possible inputs, including worst-case inputs. <br/><br/>This project recognizes that many practical data analysis tasks do not generate worst-case instances, and seeks to identify structural aspects of given problems that allow existing or new algorithms with better guarantees than the worst-case bounds for realistic cases, often using approximation, probabilistic analysis, parameterized complexity, or output sensitivity.<br/><br/>Understanding the huge volume of data from a combustion simulation run on a super computer gives a 3d example: Contour trees, a data structure used to summarize interactions between density or temperature isosurfaces in a simulation, take more than linear time to compute in the worst case, but by parameterizing on tree shape one can show that trees that are balanced can be computed in linear time.  Machine learning and clustering problems, like recommendation systems, give higher-dimensional examples in which one desires to extract a smaller and lower dimensional representation of the input, while preserving some feature of interest.  A geometric form of this problem is known as extracting a coreset; in the worst case the coreset size can be exponential in the dimension.  On real inputs however, there is often hidden low dimensional structure; rather than designing an algorithm whose running time depends on the worst case coreset size, the running time should adapt to the size required by the given instance.<br/><br/>Advancing non-worst-case analysis techniques helps bridge the gap between theory and practice, as there is often a disconnect between running times predicted by worst-case analysis and those seen on real data sets. The investigator will incorporate non-worst-case analysis techniques into his course curricula, as such techniques are essential yet severely lacking in standard algorithms courses. This project will also be used to support student research at the graduate as well as undergraduate levels on this topic."
"1631325","NCS-FO: Integrative Knowledge Modeling in Cognitive Neuroimaging","IIS","ECR-EHR Core Research, IntgStrat Undst Neurl&Cogn Sys","08/01/2016","08/17/2016","Angela Laird","FL","Florida International University","Standard Grant","James Donlon","07/31/2020","$727,731.00","Jessica Turner","alaird@fiu.edu","11200 SW 8TH ST","Miami","FL","331990001","3053482494","CSE","7980, 8624","8089, 8091, 8551","$0.00","Neuroimaging research is increasing in volume and scope, needing ""big data"" methods for discovery. A number of important resources already exist for neuroimaging, including data repositories, crowd-sourcing knowledge bases, standardized ontologies and terminologies, and meta-analytic repositories. However, while data and code-sharing efforts are growing, there is little interaction and limited sharing of knowledge across platforms. Machine learning methods are being applied in other fields to extract knowledge locked in text (e.g., journal articles, patient records, social media posts), with the goal of recognizing relations among entities (e.g., ""drug X causes adverse event Y""). Cognitive neuroscientists also determine relations, specifically between brain regions and cognitive, perceptual, and motor processes (e.g., ""mental function X activates brain network Y""), but are hampered in using high-throughput automated methods on the ever-growing published text. It is not a simple process to identify which cognitive processes were studied in a given project, or what brain networks were identified as related to which mental process. The investigators propose an integrative metadata framework that describes the experimental design characteristics and results, as well as the knowledge that the research provides.  Efficient knowledge sharing may best be achieved via an interactive data ecosystem that uses standards for transparency and openness when describing knowledge derived from cognitive neuroimaging experiments. Developing this integrative metadata framework for neuroimaging will increase the community's ability to share data and evaluate reliability in the resulting relationships between mind and brain. This project aims to provide improvements in large-scale integration of the scientific literature, with more rapid understanding of the complexity of brain research and neurocognitive models, within an educational setting for training STEM students and accelerated research productivity.<br/><br/>Neuroscientific research frequently requires efficiency, transdisciplinary collaborations, and cross-domain flexibility. Efficient knowledge sharing may best be achieved via an interactive data ecosystem that relies on an integrative metadata framework. Such a framework would address scientific reproducibility by providing standards for transparency and openness when describing knowledge derived from cognitive neuroimaging experiments. Moreover, development of an integrative metadata framework for cognitive neuroimaging will enhance interaction between existing neuroinformatics resources, increasing the community's ability to share data and evaluate reliability in experimental findings. This project will develop knowledge modeling tools for cognitive neuroimaging studies, as well as large-scale meta-analytic evaluations of cognitive models. The investigators will build on previous work extracting experimental design features from the text to create an ensemble of classifiers for full text papers. The investigators will work with an External Advisory Board for evaluation and feedback, and will use the framework to automatically extract knowledge regarding mind/brain models within the exemplar domains of executive function, affective processing, and reward feedback. The investigators will integrate classifiers and methods with other international standards for data and results sharing (e.g., NI-DM, CEDAR and ISA-TAB, BioCaddie) and other repositories (e.g. Neurosynth, BrainSpell) for broader use in the community. The intellectual merit of this project is the enhanced access to cognitive neuroscience knowledge that is currently locked in text. This project's success will allow the research community to collectively address hurdles such as annotating their own data and sharing their data/results via integrated annotations in a public repository, journal, or knowledge discovery platforms, and ultimately lead to long-term strategies for cross-domain neurocognitive model development. This project has been designed to have high integrative value and will interact, harmonize, and share data and algorithms with existing neuroinformatics resources that will benefit from enhanced knowledge modeling techniques. Moreover, in an effort to promote transparency, reliability, and reproducibility, this project will be publicly available on the Open Science Framework and Github (e.g., Labels, Classifiers, Code). Such an integrative metadata framework may be viewed as the connective tissue that will facilitate a new generation of cognitive model development, providing a potentially transformative strategy for modeling the literature, and ultimately leading to more informed, evidence-based, and reproducible neurocognitive models of brain function."
"1562306","SHF: Medium: Collaborative Research: Next-Generation Message Passing for Parallel Programming: Resiliency, Time-to-Solution, Performance-Portability, Scalability, and QoS","CCF","Software & Hardware Foundation","06/01/2016","06/07/2019","Purushotham Bangalore","AL","University of Alabama at Birmingham","Continuing Grant","Almadena Chtchelkanova","05/31/2021","$413,920.00","","puri@uab.edu","AB 1170","Birmingham","AL","352940001","2059345266","CSE","7798","7924, 7942, 9150, 9251","$0.00","Parallel programming based on MPI is being used with increased frequency in academia, government (defense and non-defense uses), as well as emerging uses in scalable machine learning and big data analytics.  Emerging supercomputer systems will have more faults and MPI needs to be able to workaround such faults to be appropriate to these emerging situations, rather than causing an entire application to fail.  Collaborative, transformative message passing research for High Performance Computing (HPC) critical to performance-portable parallel programming in new and forthcoming scalable systems (with a strategy of ""best practice-first, standardization-later"") is being reduced to practice. A substantial subset of the Message Passing Interface (MPI-3/4) application programmer interface is being made fault tolerant through extensions with weak collective transactions that synchronize between parallel tasks. <br/><br/>This research studies  the novel model that localizes faults, provides tunable fault-free overhead, allows for multiple kinds of faults, enables hierarchical recovery, and is data-parallel relevant.  Fault modeling of underlying networks is being studied. Application developers control the granularity and fault-free overhead in this effort. Performance and scalability results of the middleware prototype are being demonstrated principally through compact applications that relate to real use cases of practical and academic interest. The impact of this work ranges from users of the largest supercomputers in government labs to practical clusters that have long-running, time-critical applications, and to space-based and other parallel processing in ""hostile"" environments where faults occur more frequently than in past years.  The project is producing usable free software that will be widely shared in the community as well as guidance on how better parallel programs can be written in academia, industry, and government.  The project also provides guidelines for how to update existing or legacy programs to use the new capabilities that are being reduced to practice."
"1565387","TWC: Large: Collaborative: Computing Over Distributed Sensitive Data","CNS","Algorithmic Foundations, Secure &Trustworthy Cyberspace","05/01/2016","05/11/2020","Yaacov Nissim Kobliner","MA","Harvard University","Continuing Grant","Nina Amla","04/30/2021","$1,750,061.00","Salil Vadhan, Stephen Chong, James Honaker","kobbi.nissim@georgetown.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","7796, 8060","025Z, 7434, 7925, 7926, 7927, 9178, 9251","$0.00","Information about individuals is collected by a variety of organizations including government agencies, banks, hospitals, research institutions, and private companies. In many cases, sharing this data among organizations can bring benefits in social, scientific, business, and security domains, as the collected information is of similar nature, of about similar populations. However, much of this collected data is sensitive as it contains personal information, or information that could damage an organization's reputation or competitiveness. Sharing of data is hence often curbed for ethical, legal, or business reasons. <br/><br/>This project develops a collection of tools that will enable the benefits of data sharing without having the data owners share the data. The techniques developed respect principles of data ownership and privacy requirement, and draw on recent scientific developments in privacy, cryptography, machine learning, computational statistics, program verification, and system security. The tools developed in this project will contribute to the existing research and business infrastructure, and hence enable new ways to create value in information whose use would have been otherwise restricted. The project supports the development of new curricula material and train a new generation of researchers and citizens with the multidisciplinary perspectives required to address the complex issues surrounding data privacy."
"1565609","CRII: CSR: Pervasive Gesture Recognition Using Ambient Light","CNS","CSR-Computer Systems Research","05/01/2016","03/18/2016","Muhammad Shahzad","NC","North Carolina State University","Standard Grant","Marilyn McClure","04/30/2020","$174,878.00","","mshahza@ncsu.edu","2701 Sullivan DR STE 240","Raleigh","NC","276950001","9195152444","CSE","7354","8228","$0.00","As computing devices are becoming smaller, smarter, and more ubiquitous, computing has started to embed into our environment in various forms such as intelligent thermostats, smart appliances, remotely controllable household equipment, and weather based automated lawn irrigation systems. Consequently, we need new ways to seamlessly and effectively communicate and interact with such ubiquitous and always-available computing devices. A natural choice for such communication and interaction is human gestures because gestures are an integral part of the way humans communicate and interact with each other in their daily lives. This project aims at using ambient light and cheap commercial off-the-shelf light sensors to develop a gesture recognition system. The intuition behind this approach is that as a user performs a gesture in a room that is lit with light, the amount of light that he/she reflects and blocks changes, resulting in changes in the intensity of light in all parts of the room. The patterns of change in the intensity of light are different for different gestures, which can be learnt and used to recognize the gestures.<br/><br/>In developing the ambient light based gesture recognition system, this project has two primary objectives: (1) environment independence, i.e., making the system agnostic to the characteristics of the environment, such as different lighting conditions, and (2) user independence, i.e., making the system agnostic to the number of users in a room and their routine activities. Several challenges arise in developing the ambient light based gesture recognition system, such as automatically detecting the start and end of a gesture, removing noise from the time-series of sensor values, handling the varying time durations of the different occurrences of the same gesture, simultaneously recognizing the gestures of multiple people, and recognizing the gestures of non-stationary users. This project will not only address these and other similar challenges, but will also advance the knowledge and understanding of the use of ambient light for novel systems by yielding a theoretical foundation for modeling human gestures and routine activities using changes in the intensity of light.<br/><br/>The successful completion of this project will greatly benefit our society. First, this project will make the data set collected during this project publicly available for research. Second, the proposed ambient light based gesture recognition system will introduce a new and convenient way for users to interact with the computing embedded in their environments. Third, the proposed project will bridge several different communities such as systems, signal processing, machine learning, mobile computing, and human computer interaction; and foster interaction and communication among them. Fourth, the educational side of the project will integrate the research findings into the undergraduate and graduate curricula at North Carolina State University."
"1564274","CSR: Medium: Augmenting Logs with Static Analysis and Symbolic Execution","CNS","Special Projects - CNS, CSR-Computer Systems Research","08/01/2016","06/14/2019","Vikram Adve","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Marilyn McClure","07/31/2021","$932,000.00","Tao Xie, Swarup Sahoo","vadve@cs.uiuc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","1714, 7354","7924, 9251","$0.00","Internet services such as Facebook, Amazon, Gmail, iTunes, or Office 360 are a backbone of the modern economy, and include millions of consumer applications and business services provided by large, medium-sized and small businesses. All these services are essentially organized as complex distributed software systems.  When such a system encounters a failure (a service outage or an unexpected performance problem), the potential costs of the failure can quickly reach millions of dollars in lost revenue or lost productivity.  Diagnosing the root causes of the problem quickly and restoring the service as soon as possible are both critical.<br/><br/>The basic approach to this problem is to record (or ""log"") information frequently as system components execute and interact, and then to analyze these logs after a failure in attempting to diagnose the root causes.  While these analyses use increasingly sophisticated statistical and machine-learning based techniques, they are limited by the information gathered in the logs: the state-of-the-art approaches today record limited information pertaining to major events so that they do not slow down the services significantly during normal operation.  In particular, these systems do not record detailed internal information within components or detailed network traffic between components that can be enormously useful in diagnosing failures more quickly.<br/><br/>This project develops new techniques that make it possible to reconstruct this missing information by leveraging recent advances in program analysis and automated testing.  Moreover, the project develops a new strategy that breaks down the reconstruction task into small steps that will allow the analysis to scale to very large distributed services.  The project also develops new, more sophisticated analysis techniques for individual components that go beyond the current state-of-the-art, by tailoring them to the specific problems faced in this work.  The project engages industrial partners to evaluate the techniques on their systems, and makes the resulting software solutions available in open source form to the research community as well as the industrial partners. Together, these techniques have the potential to result in large cost-savings for a wide range of Internet services."
"1613857","EAPSI: Determining Disk Galaxy Evolutionary State through Spatially-Resolved Gas Mass Fractions","OISE","EAPSI","06/15/2016","06/21/2016","Zachary Pace","WI","Pace                    Zachary        J","Fellowship Award","Anne Emig","05/31/2017","$5,400.00","","","","Madison","WI","537031556","","O/D","7316","5978, 7316, 9200","$0.00","Current research in galaxy formation and evolution seeks to answer one main question: how did the heterogeneous universe we observe--with its vast tracts of virtually-empty space between immensely compact islands of stars and hot gas--form from an overwhelmingly homogeneous beginning, the Big Bang? This is indicated robustly in individual galaxies by the gas mass fraction, the percentage of mass contained in both hot plasma and cool atomic & molecular hydrogen--as opposed to mass contained in stars. A small gas fraction indicates--in general--an evolved system with reduced capacity for forming new stars, while a large gas fraction suggests the potential for large star formation and metal-enrichment of the interstellar medium. In collaboration with Prof. Yanmei Chen (Nanjing University: Nanjing, Jiansu, China), the PI will construct statistically-robust estimates of stellar mass, and compare to his existing measurements of spatially-resolved gas mass. The radial variation of gas mass fraction will be compared with estimates of total galaxy mass, star formation rate, and galaxy environment density.<br/><br/>In the new era of integral-field spectroscopic surveys (such as MaNGA, SAMI, and CALIFA), responsive, robust, and cheap methods of stellar-continuum fitting are necessary counterparts to large, nightly datastreams. Most methods of stellar-mass estimation are computationally costly, and do not take advantage of modern advances in statistics and machine learning. Prof. Yanmei Chen has developed a method of fitting the stellar continuum, which relies on the one-off principal-component analysis (PCA) of a library of single stellar population (SSP) models, which yields order-of-magnitude improvements in compute time and will enable the use of larger, more flexible SSP libraries. The PI has obtained a set of fully-theoretical SSP models, with a spectral resolution exceeding that of MaNGA. Such SSPs include near-IR features like the CaII triplet (necessary for breaking the age-metallicity degeneracy), and improve on existing models of the thermally-pulsating asymptotic giant branch (TP-AGB). Combining the high-resolution stellar library with Chen's PCA code (translated by the PI from IDL to Python) will yield measurable improvements in resolved stellar-mass estimates. When paired with resolved total gas mass (HI + H2), estimated using new optical-spectroscopic methods (rather than from expensive, radio-band follow-up), a census of stars and gas in nearby galaxies will result. These single-source estimates of galaxy evolutionary state will improve current knowledge of galactic chemical evolution and mass assembly, and place the Milky Way in a cosmic context.<br/><br/>This award under the East Asia and Pacific Summer Institutes program supports summer research by a U.S. graduate student and is jointly funded by NSF and the Ministry of Science and Technology of China."
"1562041","AF: Medium: Generalized Algebraic Graph Theory: Algorithms and Analysis","CCF","Algorithmic Foundations","09/01/2016","08/06/2019","Daniel Spielman","CT","Yale University","Continuing Grant","A. Funda Ergun","08/31/2021","$774,121.00","","spielman@cs.yale.edu","Office of Sponsored Projects","New Haven","CT","065208327","2037854689","CSE","7796","7924, 7926","$0.00","The PI will develop new methods for analyzing, reasoning about, and making predictions about graphs and networks.  Graphs and networks appear throughout society and science.  They include road and transportation networks, communication networks, power networks and social networks.  They are one of the dominant abstractions of data in Computer Science, and are used to model abstract interactions in fields ranging from Genomics to Image Processing and Machine Learning.<br/><br/>The project has two major research thrusts.  The first is the development of faster algorithms for performing existing analyses.  The second is the development of new approaches to understanding the structure of graphs and networks.  During the project, the PI will also develop and distribute course materials to teach recent developments in the field, will give public lectures on related material, will train graduate and undergraduate students in research, and will develop software that others can use to perform these analyses.<br/><br/>The fundamental object to be studied in this project are graph structured block matrices (GSBMs)---block matrices whose nonzero structure corresponds to the edges of a graph.  The first part of the project will involve the development of fast algorithms for the solution of systems of linear equations in GSBMs that can be written as a sum of positive semindefinte matrices with each matrix corresponding to one edge of the graph.  These GSBMs are generalizations of Laplacian matrices and arise in many application areas, including Optimization, Computational Science, and Image Processing.  The second part of the project will involve the generalization of spectral graph theory to the study of the expected characteristic polynomials of GSBMs with randomly chosen block matrices.  Spectral graph theory has been one of the most useful tools for analyzing graphs and networks.  The extension of the theory to random GSBMs should enable analyses that are not possible with the standard approach."
"1565365","TWC: Large: Collaborative: Computing Over Distributed Sensitive Data","CNS","Secure &Trustworthy Cyberspace","05/01/2016","05/02/2019","Marco Gaboardi","NY","SUNY at Buffalo","Continuing Grant","Nina Amla","04/30/2021","$527,867.00","","gaboardi@bu.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","8060","7434, 7925","$0.00","Information about individuals is collected by a variety of organizations including government agencies, banks, hospitals, research institutions, and private companies. In many cases, sharing this data among organizations can bring benefits in social, scientific, business, and security domains, as the collected information is of similar nature, of about similar populations. However, much of this collected data is sensitive as it contains personal information, or information that could damage an organization's reputation or competitiveness. Sharing of data is hence often curbed for ethical, legal, or business reasons. <br/><br/>This project develops a collection of tools that will enable the benefits of data sharing without having the data owners share the data. The techniques developed respect principles of data ownership and privacy requirement, and draw on recent scientific developments in privacy, cryptography, machine learning, computational statistics, program verification, and system security. The tools developed in this project will contribute to the existing research and business infrastructure, and hence enable new ways to create value in information whose use would have been otherwise restricted. The project supports the development of new curricula material and train a new generation of researchers and citizens with the multidisciplinary perspectives required to address the complex issues surrounding data privacy."
"1553528","CAREER: Fast and Scalable Combinatorial Algorithms for Data Analytics","IIS","Info Integration & Informatics","04/01/2016","02/18/2019","Assefaw Gebremedhin","WA","Washington State University","Continuing Grant","Amarda Shehu","03/31/2021","$517,268.00","","assefaw.gebremedhin@wsu.edu","280 Lighty","PULLMAN","WA","991641060","5093359661","CSE","7364","1045, 7364","$0.00","We are in an age when massive digital data continues to be collected at an extraordinarily rapid rate and with high and growing complexity (and concomitant uncertainty), when the data and the actors behind it are increasingly interconnected, and when architectures of computing platforms continue to rapidly change. Fast, robust and scalable algorithms that are simultaneously cognizant of all three dimensions (data, interconnection, and computing platform) are acutely needed for the purpose of analyzing massive datasets and extracting knowledge and insight from the data. This project (named FASCADA, Fast and Scalable Combinatorial Algorithms for Data Analysis) will explore the interplay between graph and matrix algorithms in order to develop methods for data analytics that perform at scale on contemporary platforms, with a primary focus on data that are expressed in terms of networks. Algorithmic research progress to be made in the project will be realized in software implementations, will be integrated with existing software tools when applicable, and will be made available to the wider community as open-source software. As part of the project's integrated education and outreach component, two new innovative courses, an undergraduate course on Data Science and a graduate course on Network Science, will be developed and taught. The educational effort will contribute to meeting the rapidly expanding need for a trained workforce in data science in the US economy and will contribute to US competitiveness in the global market. Underrepresented minority groups in computing sciences and engineering will be recruited and mentored through an existing, effective program in the Pacific Northwest (LSAMP), and undergraduate students from Heritage University will be mentored through summer internships at Washington State University.<br/><br/>The specific research aims of FASCADA are organized under four intertwined areas. (1) Enabling Scalable Data Analytics: devise novel ""problem-partitioning"" methods that are useful for solving, at scale, optimization problems underlying a large class of machine learning algorithms. (2) Network Analysis: develop fast algorithms for discovering and analyzing dense subgraphs in real-world networks arising from diverse domains. (3) High Performance Computing: develop effective paradigms for the parallelization of inherently sequential graph algorithms targeting many-core architectures.  (4) Algorithmic Differentiation (AD): advance AD as a technology by designing better graph-based algorithms for Hessian computation, and use AD in emerging applications, including quantifying uncertainty. A common thread that runs through all four of the areas is a focus on graph problems and their solution. The novelty of the proposed approach lies in the exploration of the bidirectional interaction between graph and matrix algorithms. Results from this effort will advance fundamental knowledge at the intersection of a range of areas, including data science, computational science and engineering, computational mathematics, and high performance computing. For further information, visit the project webpage http://www.eecs.wsu.edu/~assefaw/fascada."
"1642385","SI2-SSE: Collaborative Research: High Performance Low Rank Approximation for Scalable Data Analytics","OAC","Software & Hardware Foundation, Software Institutes","11/01/2016","09/08/2016","Grey Ballard","NC","Wake Forest University","Standard Grant","Amy Walton","10/31/2020","$167,713.00","","ballard@wfu.edu","1834 Wake Forest Road","Winston Salem","NC","271098758","3367585888","CSE","7798, 8004","7433, 7942, 8004, 8005","$0.00","Big Data analytics is at the core of discovery covering vast areas such as medical informatics, business analytics, national security, and materials sciences. This project aims to model some of the key data analytics problems and design, verify, and deploy scalable methods for knowledge extraction.  The algorithms developed will be able to handle data sets of extreme sizes and will be deployable on advanced computer hardware. The goal is to realize orders-of-magnitude improvements over existing data analytics technologies, developing algorithms that are robust to incompleteness, noise, ambiguity, and high dimension in the data.  Particular focus will be parallel and distributed algorithms that can efficiently solve large problems and produce accurate solutions.  The proposed research and software development will allow domain experts to tackle Big Data sets requiring large parallel systems.  The improved performance will enable fast and scalable data analysis across applications, from social network analysis to study citizens' attitudes toward sustainability-related issues to computational marketing techniques that refine customers' shopping experiences.  The proposed work will help bridge the gap between computational science and data analytics ecosystems, two fields that stand to make great advancements from cross-fertilization.  The education and outreach plan includes graduate course creation, engagement of under-represented groups via both undergraduate and graduate research experiences, and community-building efforts by workshop and mini-symposium organization.<br/><br/>With the advent of internet-scale data, the data mining and machine learning community has adopted Nonnegative Matrix Factorization (NMF) for performing numerous tasks such as topic modeling, background separation from video data, hyper-spectral imaging, web-scale clustering, and community detection.  The goals of this proposal are to develop efficient parallel algorithms for computing nonnegative matrix and tensor factorizations (NMF and NTF) and their variants using a unified framework, and to produce a software package called Parallel Low-rank Approximation with Nonnegative Constraints (PLANCK) that delivers the high performance, flexibility, and scalability necessary to tackle the ever-growing size of today's data sets. The algorithms will be generalized to NTF problems and extend the class of algorithms we can efficiently parallelize; our software framework will allow end-users to use and extend our techniques.  Rather than developing separate software for each problem domain and mathematical technique, flexibility will be achieved by characterizing nearly all of the current NMF and NTF algorithms in the context of a block coordinate descent framework. Using this framework the shared computational kernels can be separated, which usually extend run times, from the algorithm-specific computations. Finally, the usability and practicality of the proposed software will be maintained by being application driven, establishing collaborations with early end-users, and by incrementally generalizing the framework in terms of both algorithms and problems."
"1609484","Locality in Network Optimization","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","09/01/2016","07/29/2016","Sekhar Tatikonda","CT","Yale University","Standard Grant","Radhakisan Baheti","08/31/2020","$450,000.00","","sekhar.tatikonda@yale.edu","Office of Sponsored Projects","New Haven","CT","065208327","2037854689","ENG","7607","092E","$0.00","Many recent problems in control, communication, computation, and machine learning can be posed in the framework of network optimization. Despite the great number of algorithms that have been proposed to address modern large-scale problems there remains a crucial challenge of building computationally efficient, accurate, scalable, distributed procedures. A key issue is that of understanding locality. Locality is a structural property of optimization instances that captures the property that information need not propagate across the entire network but only across local portions of it. As a consequence decisions made at a node can be made based only on local information thus reducing both communication requirements and computational complexity.  This research involves identifying when locality occurs, designing algorithms that take advantage of locality, and applying these algorithms to variety of applications. The project also provides an opportunity for training graduate students and postdoctoral researchers in the disciplines of optimization, networking, and control.<br/><br/>Typically sensitivity results concern the objective function evaluated at the optimal solution not the optimal solution itself.  Herein sensitivity is characterized by the change in the optimal solution at a given node given a change in a network parameter at another node.   Locality holds when the sensitivity decays with the distance between the two nodes.  The main objectives of this research project are: (1) Theoretical Analysis: developing a general theory to characterize the local sensitivity of optimal solutions in a variety of network optimization problems and developing analytic tools to quantify the rate at which sensitivity decays with distance; (2) Algorithm Development: developing computationally efficient local message-passing algorithms to solve a variety of network optimization problems; and (3) Applications: applying these algorithms to problems in network optimization, distributed computation, and cooperative control."
"1553471","CAREER: Generative Programming and DSLs for Safe Performance Critical Systems","CCF","Software & Hardware Foundation","04/15/2016","06/26/2019","Tiark Rompf","IN","Purdue University","Continuing Grant","Anindya Banerjee","03/31/2021","$409,141.00","","tiark@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7798","1045, 7943","$0.00","Most performance critical software is developed using very low-level techniques, close to the underlying hardware. But low-level code in unsafe languages attracts security vulnerabilities, developer productivity suffers without the software engineering benefits of higher-level languages, and in the age of heterogeneous hardware and big data workloads, a single hand-optimized codebase may no longer provide good performance across different target platforms. Generative programming is a radical rethinking of the role of high-level languages and low-level languages. Instead of running whole systems in a high-level managed language runtime, the idea is to focus the abstraction power of high-level languages on composing pieces of low-level code, making runtime code generation and domain-specific optimization a fundamental part of the program logic.  This project will conduct a fundamental study of generative design patterns, which will be extracted from existing and emerging program generators and domain-specific languages. The intellectual merits are a deeper understanding of how to develop software in a generative style. The project's broader significance and importance are to establish generative programming as a part of every performance-minded programmer?s toolbox, enabling the use of high-level programming in more situations than currently possible.<br/><br/>Generative programming, and the shift in perspective that goes along with it, has been shown to be extremely effective in areas like databases (query compilation), protocol and data format parsers, hardware circuit generation, signal processing kernels, machine learning, and big data processing on heterogeneous computing devices?traditional strongholds of low-level languages. But while the general idea of program generation is well understood, the technique has remained esoteric?a black art, accessible only to the most skilled and daring of programmers. What is missing is a discipline of practical generative programming, including design patterns, best practices and so on. To achieve these broader goals, the project includes an education program, which, driven by the project?s research, will teach generative programming to a wide audience of students and developers in industry. This education effort will also serve as a large-scale usability study, closing the feedback loop into the research on generative programming techniques."
"1700775","I-Corps: Software/Hardware Controller for Real Time Control of Battery Energy Storage System in a Grid","IIP","I-Corps","12/01/2016","08/10/2018","Rajit Gadh","CA","University of California-Los Angeles","Standard Grant","Anita La Salle","01/31/2019","$50,000.00","","rgadh@seas.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","ENG","8023","","$0.00","The broader impact/commercial potential of this I-Corps project will be in the modernization of the electrical power grid and solving the challenges of integrating large scale renewable sources to the grid while improving the reliability, resilience, affordability, and flexibility of the electric power grid. This project's hardware/software controller turns the battery energy storage system from a passive component in the grid to an active and smart component which is able to benefit both grid operators and customers automatically. This technology will benefit utilities and independent system operators through providing enhanced grid services and reducing investment cost of system expansion. The commercial potential of this technology is due to its capabilities for improving power quality, increasing power system reliability, decreasing green gas emission, reducing peak demand, and maximizing renewable energy utilization. This technology will also benefit commercial, residential, and industrial customers by potentially reducing their electricity bill, increasing power reliability, and providing them with revenue in the energy market. The end product is a software/hardware controller panel which will be integrated with a battery energy storage system consisting of battery modules, inverter and measurement devices.<br/><br/>This I-Corps project involves a real time hardware/software controller for smart operation of Battery Energy Storage System (BESS) in the grid. The developed technology incorporates a novel control algorithm/structure in the form of an integrated hardware/software device, to control the power flow of BESS. The control hardware is configured for effective interaction of the controller with inverter, battery management system, and measurement units. The control software is structured to manage the communication, safe operation and monitoring of the system. This product, by employing control, machine learning, discrete signal processing, and optimization methods, enables BESS as a buffer to dynamically compensate for the renewable intermittency and load uncertainty in the power grid. The main advantage of the controller is to minimize the size of allocated battery for applications such as voltage regulation, renewable compensation, and load compensation."
"1612843","Natural Hazards Engineering Research Infrastructure: Computational Modeling and Simulation Center","CMMI","Natural Hazards Engineering Re","10/01/2016","09/18/2019","Sanjay Govindjee","CA","University of California-Berkeley","Cooperative Agreement","Joy Pauschke","09/30/2021","$10,949,973.00","Ahsan Kareem, Gregory Deierlein, Satish Rao, Laura Lowes, Camille Crittenden","s_g@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","ENG","013Y","036E, 037E, 038E, 039E, 040E, 041E, 043E, 1057, 151E, 1576, 7231, 7556, CVIS","$0.00","The Natural Hazards Engineering Research Infrastructure (NHERI) will be supported by the National Science Foundation (NSF) as a distributed, multi-user national facility that will provide the natural hazards engineering community with access to research infrastructure. NHERI will be comprised of separate research infrastructure awards for a Network Coordination Office (NCO), Cyberinfrastructure (CI), Computational Modeling and Simulation Center (SimCenter)-this award, and Experimental Facilities for earthquake and wind hazards engineering research, including a post-disaster, rapid response research (RAPID) facility. NHERI awards will contribute to NSF's role in the National Earthquake Hazards Reduction Program (NEHRP) and the National Windstorm Impact Reduction Program. NHERI continues NSF's emphasis on earthquake engineering research infrastructure previously supported under the George E. Brown, Jr. Network for Earthquake Engineering Simulation as part of NEHRP, but now broadens that support to include wind and coastal hazards engineering research infrastructure. NHERI has the broad goal of supporting research that will improve the resilience and sustainability of civil infrastructure, such as buildings and other structures, underground structures, levees, and critical lifelines, against the natural hazards of earthquakes and windstorms, in order to minimize loss of life, damage, and economic loss. Information about NHERI resources is available on the NHERI CI web portal (http://www.DesignSafe-ci.org). <br/><br/>This award will provide the SimCenter component for NHERI. The goal of the SimCenter is to provide the natural hazards engineering research and education community with access to next generation computational modeling and simulation software tools, user support, and educational materials needed to advance the nation's capability to simulate the impact of natural hazards on structures, lifelines, and communities, and to make informed decisions about the need for and effectiveness of potential mitigation strategies. These tools will be available on the NHERI CI's DesignSafe-ci.org Discovery Workspace. The SimCenter will provide computational modeling and simulation tools using a new open source simulation framework that: (1) addresses various natural hazards, such as windstorms, storm surge, tsunamis, and earthquakes; (2) tackles complex, scientific questions of concern to disciplines involved in natural hazards research, including earth sciences, geotechnical and structural engineering, architecture, urban planning, risk management, social sciences, public policy, and finance; (3) utilizes machine learning to facilitate and improve modeling and simulation using data obtained from experimental tests, field investigations, and previous simulations; (4) quantifies uncertainties associated with the simulation results obtained; (5) utilizes the high-performance parallel computing, data assimilation, and related capabilities provided by the DesignSafe-ci.org web portal to easily combine software applications into workflows of unprecedented sophistication and complexity; (6) extends and refines software tools for carrying out performance-based engineering evaluations and supporting decisions that enhance the resilience of communities susceptible to multiple natural hazards; and (7) utilizes existing applications that already provide many of the pieces of desired computational workflows. The resulting cloud-based ecosystem will allow multidisciplinary specialists and various stakeholders to collaborate on solutions to complex natural hazard engineering problems regardless of their local resources and geographic proximity. In tandem with the framework and applications being developed, in-person and online educational programs, including massive open online courses, will be implemented within a new Virtual Community of Practice that will provide an online meeting place to obtain and exchange community-generated ideas, feedback, best practices for modeling and simulation, insights, and innovations for natural hazards engineering research. The SimCenter will also provide user workshops and a graduate student research traineeship program and will host two students as part of the NCO's Research Experiences for Undergraduates program."
"1622490","Theory and practice for exploiting the underlying structure of probability models in big data analysis","DMS","CDS&E-MSS","08/01/2016","07/17/2018","Babak Shahbaba","CA","University of California-Irvine","Continuing Grant","Christopher Stark","07/31/2019","$249,964.00","Hong-Kai Zhao, Jeffrey Streets","babaks@uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","MPS","8069","8083, 9263","$0.00","Ever-increasing use of data-intensive methods in scientific discoveries has led to a paradigm shift in science in recent years. High throughput scientific experiments, routine use of digital sensors, and intensive computer simulations have created a data deluge imposing new challenges on scientific communities to find effective and computationally feasible methods for processing and analyzing very large datasets. Despite many attempts, however, the necessary development of theoretical and computational foundations for big data analysis is lagging far behind. Many existing statistical methods are not capable of handling such data-intensive problems in terms of theoretical foundation as well as computational complexity and scalability. For analyzing high dimensional data with possibly complex structures, this research will offer a set of fundamental solutions using principled statistical methods. The resulting methods will provide a robust framework for big data analysis and allow scientists to use statistical models beyond their current limited applicability. The techniques developed in this project are likely to gain widespread acceptance across a broad spectrum of scientific disciplines, as well as in industry.<br/><br/>The focus of this research is mainly on Bayesian statistics. Many recent methods aim to improve computational efficiency of Bayesian models by approximating the likelihood function using a small subset of data. In contrast, the objective of this research is to explore underlying structures of probability models and exploit these features to design efficient and scalable computational methods and algorithms for Bayesian inference in big data analysis. To this end, (1) the PIs will define and study the structure of probability distributions in order to develop novel geometrically motivated methods for statistical inference; (2) the PIs will develop efficient and scalable computational methods that accurately approximate probability distributions by exploiting their geometric properties; (3) finally, the PIs will apply these methods to real computationally-intensive problems from biological sciences. Due to its interdisciplinary nature, this research is expected to contribute to several fields, including statistics, machine learning, applied mathematics, and data-intensive computing."
"1622679","SCH: INT: Collaborative Research: Assistive Integrative Support Tool for Retinopathy of Prematurity","IIS","Smart and Connected Health","10/01/2016","07/27/2016","Michael Chiang","OR","Oregon Health & Science University","Standard Grant","Sylvia Spengler","09/30/2020","$407,404.00","Robison Chan, John Campbell, Kemal Sonmez","chiangm@ohsu.edu","3181 S W Sam Jackson Park Rd","Portland","OR","972393098","5034947784","CSE","8018","8018, 8062","$0.00","Retinopathy of prematurity (ROP) is a leading cause of childhood visual loss worldwide, and the social burdens of infancy-acquired blindness are enormous. Early diagnosis is critically important for successful treatment, and can prevent most cases of blindness. However, lack of access to expert medical diagnosis and care, especially in rural areas, remains a growing healthcare challenge. In addition, clinical expertise in ROP is lacking, and medical professionals are struggling to meet the increasing need for ROP care. As point-of-care technologies for diagnosis and intervention are rapidly expanding, the potential ability to assess ROP severity from any location with an internet connection and a camera, even without immediate ophthalmologic consultation available, could significantly improve delivery of ROP care by identifying infants who are in most urgent need for referral and treatment. This would dramatically reduce the incidence of blindness without a proportionate increase in the need for human resources, which take many years to develop. <br/><br/>This project develops a prototype assistive integrative support tool for ROP, featuring a modular design comprising: (a) image analysis, (b) information fusion of clinical, imaging, and diagnostic data, and (c) generative probabilistic and regression models with associated computationally efficient machine learning algorithms. The outcomes of the project include disease severity metrics and diagnostic estimates obtained through clinical evidence classifiers trained jointly over expert-generated labels. These labels consist of discrete diagnostic labels, as well as comparison outcomes of relative severity between pairs of images. Random process models for vessel tortuosity and diameter distributions over the retina, as well as patch-based vessel-free image analysis through the use of convolutional neural networks on the entire image, enhance and augment feature extraction. Moreover, incorporating severity comparison outcomes through novel hard and soft constraint methods force inferred severity to agree with ordinal information provided by experts and address inherent uncertainty in expert ground-truth labels. The above severity inference methods are evaluated and fine-tuned over a broad array of generative models, both through retrospective analysis, including cross-validation, longitudinal tests, and tests across multiple sites, as well as through prospective analysis, evaluating its real-world clinical impact."
"1621880","SBIR Phase I:  Software with Breakthrough Composite Distance Method for Zero Defects in Advanced Manufacturing","IIP","SBIR Phase I","07/01/2016","06/22/2016","Anil Gandhi","CA","Qualicent Analytics, Inc.","Standard Grant","Rajesh Mehta","03/31/2017","$224,988.00","","anil.gandhi@qualicent.net","4219 Verdigris Circle","san jose","CA","951341546","4088844033","ENG","5371","5371, 8029","$0.00","This SBIR Phase I project aims to develop novel machine learning-based software for early warning and elimination of potential field failures in automotive and medical device industries. The Composite Distance technique at the heart of the software is crucial for early identification of failing units for these industries where field failures carry the risk of injury or death. In the automotive industry, there has been a surge in the number of field incidents involving injury or death. In the medical device industry, in the last four years, four out of five class I recalls -i.e. those leading to severe injury or death, are due to the failures from complex electronics. As the electronics going into cars and medical devices take up an increasing share of the product while simultaneously manufacturing processes become more complex, this is making it difficult to detect defects before units/devices are shipped. The software being developed in this project will detect defects resulting from the combined effects of many subtle flaws in the device. These defects are not detected using standard testing currently done in manufacturing. This project is in line with the National Science Foundation?s direction to support innovative and transformational technology for advanced manufacturing that has substantial benefits to society. On completion of this project, the technology will enable manufacturers to detect and eliminate devices that have a high probability of failing in the field, thereby protecting the lives of drivers and patients. Aside from these benefits to the society, commercialization of this technology will contribute to tax revenue and create jobs for dozens of engineers and managers.<br/><br/> <br/><br/>Field failures resulting from combined effects of many influences / variables on the unit are extremely difficult to detect - these units pass all specifications during manufacturing (or else they would not have been shipped). In the automotive and medical device markets field failure can be catastrophic and can result in loss of life and limb. In this project we develop breakthrough technology to detect and flag units that are predicted to fail downstream while passing all current specifications and control limits. The effectiveness of this unique algorithm stood out in a competition involving major analytics players in an onsite client evaluation. In this evaluation the Composite Distance produced the highest predictive accuracy and lowest cost due to yield loss. The method has two major steps ? variable reduction and Composite Distance computation, while we use proprietary methods to iterate between these two steps to arrive at the key variables of importance that are used to calculate the Composite Distance (CD). This parameter CD, computed for each unit during manufacturing reflects the interaction of all variables of importance and provides a measure of anomalous behavior and allows to identify maverick out-of-pattern parts with high likelihood of field failure.  The intellectual property, and therefore the novelty, lies in the way important variables are identified and unimportant variables, that only serve to add noise, are removed iteratively. The goal of this project is to produce a demonstration software for real-time anomaly detection, with low latency big data capability."
"1618684","TWC: Small: Collaborative: Reputation-Escalation-as-a-Service: Analyses and Defenses","CNS","Secure &Trustworthy Cyberspace","07/01/2016","06/10/2016","Sencun Zhu","PA","Pennsylvania State Univ University Park","Standard Grant","Sol Greenspan","06/30/2020","$300,000.00","","szhu@cse.psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","8060","7434, 7923","$0.00","Living in an age when services are often rated, people are increasingly depending on reputation of sellers or products/apps when making purchases online. This puts pressure on people to gain and maintain a high reputation by offering reliable and high-quality services and/or products, which benefits the society at large. Unfortunately, due to extremely high competition in e-commerce or app stores, recently reputation manipulation related services have quickly developed into a sizable business, which is termed Reputation-Escalation-as-a-Service (REaaS). As REaaS attacks grow in scale, effective countermeasures must be designed to detect and defend against them.<br/> <br/>This research addresses REaaS from two aspects. First, it aims to understand the economics of  REaaS by conducting empirical studies of  e-markets. Second, it aims to develop defensive measures, which involve both technical approaches and market intervention.  The technical approaches focus on detection of REaaS from e-markets, and novel detection techniques will be developed using content analysis, machine learning, social ties, and graph theory. For market invention, after a holistic analysis of REaaS, this research aims to identify its bottleneck (the weakest link) and also measure the efficacy of intervention. The outcome of this data-driven security research will enhance security education with labs based on social-economic data analysis. The success of this research will attract more attention of industry practitioners, government sectors, and academia to jointly tackle the REaaS problem."
"1553437","CAREER: Trustworthy Social Systems Using Network Science","CNS","Secure &Trustworthy Cyberspace","02/01/2016","04/02/2020","Prateek Mittal","NJ","Princeton University","Continuing Grant","Sara Kiesler","01/31/2021","$520,957.00","","pmittal@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","8060","1045, 7434","$0.00","Social media systems have transformed our societal communications, including news discovery, recommendations, societal interactions, E-commerce, as well as political and governance activities. However, the rising popularity of social media systems has brought concerns about security and privacy to the forefront. This project aims to design trustworthy social systems by building on the discipline of network science. First, the project is developing techniques for analysis of social media data that protect against risks to individual privacy; new research is needed since existing approaches are unable to provide rigorous privacy guarantees.  Second, the project is developing new approaches to mitigate the threat of ""fake accounts"" in social systems, in spite of attempts by the creators of those accounts to elude detection. Both deployed and academic approaches remain vulnerable to strategic adversaries, motivating the development of novel defense mechanisms based on network science. The findings and new designs from this research will directly impact the security and privacy of a broad class of social network users.<br/><br/>The private network analytics thrust builds on the ideas of differential privacy, ensuring sufficient uncertainty in results to hide individual relationships. The project introduces dependent differential privacy, which protects against disclosure of information associated with an individual, as well as mutual information privacy, an entropy-based measure. The Sybil mitigation thrust is based on the idea of adversarial machine learning:  the creators of fake accounts are presumed to adapt their mechanisms to changing detection approaches.  This work exploits new features, such as temporal dynamics of the network, to address this problem. Finally, the project aims to integrate the research with an educational initiative for developing pedagogical approaches and content for trustworthy social systems."
"1609525","Energy Landscape Approaches to Understanding Soft Glassy Materials","DMR","CONDENSED MATTER & MAT THEORY","12/15/2016","07/18/2019","John Crocker","PA","University of Pennsylvania","Continuing Grant","Daryl Hess","11/30/2020","$360,000.00","Robert Riggleman","jcrocker@seas.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","MPS","1765","7433, 8084","$0.00","NONTECHNICAL ABSTRACT<br/>This award supports computer simulation and associated education aimed to help understand the mechanical properties of a class of materials that are viscoelastic, being neither entirely solid nor liquid, and which include soap foams, toothpaste, mayonnaise and living cells.  Remarkably, these seemingly very different materials respond very similarly to being pulled or squeezed, and have largely defied deep understanding. This project builds upon a recent breakthrough that stems from thinking of the system as analogous to a particle rolling downhill on an abstract energy landscape.  That is, the seemingly complicated motion of the individual bubbles in a foam, when projected into this abstract space, resembles the twists and turns of a river canyon.  The PIs will seek to better understand the shape of the landscape that causes the 'river' to twist and turn as it does, to make new versions of the model that give rise to 'rivers' having different shapes and to add real-world detail to the model.  Similar 'landscape' approaches have also been found to be useful in machine learning algorithms with many applications, suggesting that the algorithms we develop and our findings may find useful application outside the field of Materials Research.  This project will train both graduate and undergraduate students in this important state of the art research area.<br/><br/><br/>TECHNICAL ABSTRACT <br/>This award supports theoretical and simulation research and education to understand soft glassy materials, a broad class of materials including foams, emulsions, pastes, slurries and even living cells, that share a common set of unusual viscoelastic properties whose physical origins remain deeply mysterious.  It will build upon recent studies using a foam model that show that the unusual mechanics of soft glassy materials - power-law rheology, super-diffusive particle motion and avalanches - are due to fractal properties of the system?s path in configuration space.  Specifically, it will extend the model to capture the effects of viscous damping and applied shear, produce models having configuration paths with different fractal dimensions as well as probe the fractal geometry of the energy landscape itself.  The net result is the development of a suite of tools for the actuation of non-equilibrium systems to probe and understand the geometry of energy landscapes and its relation to material properties.  More broadly, it appears that the behavior of many complex adaptive systems, including the cells in our bodies, may be determined by the minimization of some very complex energy or fitness landscape embedded in a high-dimensional space, whose in-depth study is only now becoming practical due to advances in computation. This interdisciplinary project will provide ample opportunity for graduate and undergraduate student training in the development of state of the art tools for studying such high-dimensional energy and fitness landscapes.  The tools developed in this proposal will be disseminated openly, and we anticipate them finding utility outside of material science."
"1640386","Exploring the Transition of Research-Derived Cyber-Threat Data","OAC","","10/01/2016","09/12/2017","Phillip Porras","CA","SRI International","Standard Grant","Kevin Thompson","09/30/2019","$829,655.00","","porras@csl.sri.com","333 RAVENSWOOD AVE","Menlo Park","CA","940253493","7032478529","CSE","P419, P420, Q293","7434, 8237","$0.00","Transitioning later stage cybersecurity research into operational capabilities remains a national priority. Both the 2011 and 2016 Federal Cybersecurity Research and Development Strategic Plans, collaboratively authored by agencies which participate in the NITRD (Networking and Information Technology R&D) program, explicitly identify Transition to Practice (TTP) as a goal set by the Federal agencies that fund Cybersecurity research, including NSF. The 2016 Plan states that agencies should: ""Assess barriers and identify incentives that could accelerate the transition of evidence-validated effective and efficient cybersecurity research results into adopted technologies, especially for emerging technologies and threats."" This project considers the unique challenges in the transition of large-scale data or analytic results that are embodied as data feeds, distilled threat intelligence, or online portal services. With the growing attention by U.S. agencies in sponsoring cross-disciplinary research from the Data Science and cybersecurity research communities, the study is both timely and important.<br/><br/>Novel security applications can be derived through the use of many newly emerging massively scalable machine learning technologies. Organized cyber-adversaries regularly mount sophisticated large-scale attacks and malware-driven campaigns that target large Universities, research organizations, industries, nations, and even the Internet itself. The dire need for deep reasoning cyber-threat-relevant applications now coincides with the growing availability of massive Internet datasets and the data mining solutions to capitalize on this data. This proposal explores the issues involved in transitioning the results of such large-scale data analytics into practical use."
"1620455","Harmonic analysis, non-convex optimization, and large data sets","DMS","COMPUTATIONAL MATHEMATICS","10/01/2016","09/13/2016","Thomas Strohmer","CA","University of California-Davis","Standard Grant","Leland Jameson","09/30/2019","$179,979.00","","strohmer@math.ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","MPS","1271","9263","$0.00","Future scientific and technological progress will depend heavily on the generation of new information technology capabilities and novel methods from signal and image processing to deal with today's massive volumes of data. A research effort is proposed to create mathematical concepts and computational methods to address some of the key challenges in this important area. In particular, the PI will focus on the areas of imaging, high-dimensional data analysis, machine learning, and information theory. The project uses tools from computational harmonic analysis, operator theory, random matrix theory, and optimization yielding efficient numerical algorithms with rigorously-established properties under carefully stated conditions. The payoffs for society at large are many, including new information technology capabilities, improved methods for signal- and image processing, as well as better understanding of data mining tools for Big Data.<br/><br/>Two concrete topics of this research effort are:(i) Fast and reliable algorithms of non-convex problems: When dealing with massive data sets, many tasks involve the use of a heuristic algorithm to solve a non-convex optimization problem. Often these heuristic algorithms get stuck in local minima, that are far away from the global minimum. We will develop fast numerical algorithms that come with theoretical performance guarantees for a range of important data analysis tasks; (ii) Efficient algorithms for heterogenous and high-dimensional data: Existing methods for high-dimensional data are often computationally rather expensive and rely on stationarity and homogeneity of the data, thus limiting their use for massive, heterogenous data sets. The PI will derive a framework of computationally efficient methods for properly fusing and efficiently processing heterogeneous, high-dimensional data."
"1621746","Collaborative Research: CDS&E-MSS: Local Approximation for Large Scale Spatial Modeling","DMS","CDS&E-MSS","09/01/2016","08/16/2018","Robert Gramacy","VA","Virginia Polytechnic Institute and State University","Continuing Grant","Christopher Stark","08/31/2019","$150,000.00","","rbg@vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","MPS","8069","8083, 9263","$0.00","Computer simulation is growing as a means of studying complex dynamics in applied science.  Once a tool exclusive to industrial engineering and computational physics, it is increasingly common in biology, chemistry, and economics.  Gone are the days when equilibrium dynamics are appropriate and cute systems of equations can be solved by hand.  Computer experiments are becoming more diverse, they are becoming more complex and they are growing in size thanks to modern supercomputing.  We need a new vanguard of modeling tools that can cope with the needs of modern computer experiments, particularly their increasing size (big data) and rapidly evolving and refining nature as models become more sophisticated, and supercomputing environments approach the exa-scale.  This funded research targets extensions and applications of a new breed of flexible and fast response surface methods, the so-called local approximate Gaussian process (laGP).  Our motivating applications come primarily from problems in computer experiments and uncertainty quantification, and ideas are borrowed from -- and will represent an important extension to -- the related literatures of geo-statistics and machine learning.  The over-arching goal is a  modernization of the response surface and surrogate modeling toolkit to better serve future applications across applied science.<br/><br/>Gaussian process (GP) models are popular in spatial modeling contexts, like geostatistics or computer experiments, where response surfaces are reasonably smooth but little else can be assumed. GP models provide accurate predictors, but increasingly impose computational bottlenecks: large dense matrix decompositions impede efforts to keep pace with modern trends in data acquisition. A scramble is on for fast approximations. Two common themes are sparsity, allowing fast matrix decompositions, and supercomputing, allowing distributed calculation. But these inroads are at capacity. Rapidly expanding mobile device networks, high-resolution satellite imagery (and GPS), and supercomputer simulation generate data of ever-increasing size. This funded research centers on local approximate GP (laGP) models as a means of enabling the powerful GP spatial modeling framework to address modern big data problems. Initial implementations show promise, expanding data size capabilities by several orders of magnitude. However much work remains to ensure that laGP methods can supplant conventional GPs in diverse spatial modeling contexts. Here we propose several methodological enhancements, many involving shortcuts that have provably minimal impact on laGP performance. We are motivated by two big data computer model emulation applications: one involving satellite positioning and another on solar power generation. Yet we are mindful that for our efforts to have impact, the wider spatial modeling context must always be kept in view."
"1564892","CRII: SCH: Characterizing, Modeling and Evaluating Brain Dynamics","IIS","CRII CISE Research Initiation, Smart and Connected Health, IntgStrat Undst Neurl&Cogn Sys","05/01/2016","05/05/2017","Ruogu Fang","FL","Florida International University","Standard Grant","Wendy Nilsen","10/31/2017","$190,991.00","","ruogu.fang@bme.ufl.edu","11200 SW 8TH ST","Miami","FL","331990001","3053482494","CSE","026Y, 8018, 8624","8018, 8089, 8091, 8228, 9251","$0.00","Brain dynamics, which reflects the healthy or pathological states of the brain with quantifiable, reproducible, and indicative dynamics values, remains the least understood and studied area of brain science despite its intrinsic and critical importance to the brain. Unlike other brain information such as the structural and sequential dimensions that have all been extensively studied with models and methods successfully developed, the 5th dimension, dynamics, has only very recently started receiving systematic analysis from the research community. The state-of-the-art models suffer from several fundamental limitations that have critically inhibited the accuracy and reliability of the dynamic parameters' computation. First, dynamic parameters are derived from each voxel of the brain spatially independently, and thus miss the fundamental spatial information since the brain is ?connected?. Second, current models rely solely on single-patient data to estimate the dynamic parameters without exploiting the big medical data consisting of billions of patients with similar diseases. <br/><br/>This project aims to develop a framework for data-driven brain dynamics characterization, modeling and evaluation that includes the new concept of a 5th dimension - brain dynamics - to complement the structural 4-D brain for a complete picture. The project studies how dynamic computing of the brain as a distinct problem from the image reconstruction and de-noising of convention models, and analyzes the impact of different models for the dynamics analysis. A data-driven, scalable framework will be developed to depict the functionality and dynamics of the brain. This framework enables full utilization of 4-D brain spatio-temporal data and big medical data, resulting in accurate estimations of the dynamics of the brain that are not reflected in the voxel-independent models and the single patient models. The model and framework will be evaluated on both simulated and real dual-dose computed tomography perfusion image data and then compared with the state-of-the-art methods for brain dynamics computation by leveraging collaborations with Florida International University Herbert Wertheim College of Medicine, NewYork-Presbyterian Hospital / Weill Cornell Medical College (WCMC) and Northwell School of Medicine at Hofstra University. The proposed research will significantly advance the state-of-the-art in quantifying and analyzing brain structure and dynamics, and the interplay between the two for brain disease diagnosis, including both the acute and chronic diseases. This unified approach brings together fields of Computer Science, Bioengineering, Cognitive Neuroscience and Neuroradiology to create a framework for precisely measuring and analyzing the 5th dimension - brain dynamics - integrated with the 4-D brain with three dimensions from spatial data and one dimension from temporal data. Results from the project will be incorporated into graduate-level multi-disciplinary courses in machine learning, computational neuroscience and medical image analysis. This project will open up several new research directions in the domain of brain analysis, and will educate and nurture young researchers, advance the involvement of underrepresented minorities in computer science research, and equip them with new insights, models and tools for developing future research in brain dynamics in a minority serving university."
"1613892","EAPSI:  Creating Fast and Accurate Data Mining Algorithms that Preserve Privacy","OISE","EAPSI","06/15/2016","07/19/2016","Benjamin Fish","IL","Fish                    Benjamin       S","Fellowship Award","Anne Emig","05/31/2017","$5,400.00","","","","Chicago","IL","606471728","","O/D","7316","5912, 5978, 7316","$0.00","This project will use new techniques to create fast and accurate algorithms that preserves the privacy of people's data in big data mining, i.e. tasks that require large amounts of data.  While big data mining has extraordinarily broad and important uses, releasing the output of data mining algorithms to data analysts and the public can result in leaking the private data of individuals whose data is being used. Even when data mining techniques are able to keep data private, they may be too slow or inaccurate to be used widely in practice. Previous approaches to big data mining tasks have typically either focused on a) keeping private data private or b) achieving fast and accurate algorithms, but not both. This research aims to find algorithms for big data mining tasks that are simultaneously privacy-preserving, fast, and accurate, to the degree that this is possible. This project will be conducted at The University of Melbourne with Benjamin Rubinstein, a leading expert in fast techniques for achieving privacy-preserving algorithms.<br/><br/>Focus will be on finding such fast differentially-private techniques for the Frequent Itemset mining problem (FI), a fundamental question in knowledge discovery about extracting frequently occurring sets of items in a database, and related problems. Recent approaches have demonstrated fast and accurate approximation algorithms for these types of problems. Previous methods for solving this problem were not necessarily amenable for achieving privacy-preservation quickly and accurately. These new approaches, however, which involve sampling techniques and using under-utilized tools from the theory of machine learning, are especially helpful for creating privacy-preserving algorithms. This research will leverage this approach to create algorithms that are not only fast and accurate, but also keep individuals' data private. This research will also analyze the tradeoffs between speed, accuracy, and privacy in such solutions.<br/><br/>This award under the East Asia and Pacific Summer Institutes program supports summer research by a U.S. graduate student and is jointly funded by NSF and the Australian Academy of Science."
"1645131","Student Support for Participation in the 2016 IEEE International Conference on Bioinformatics and Biomedicine (IEEE BIBM 2016)","IIS","Info Integration & Informatics","09/01/2016","08/08/2016","Xiaohua (Tony) Hu","PA","Drexel University","Standard Grant","Maria Zemankova","08/31/2017","$20,000.00","","xh29@drexel.edu","1505 Race St, 10th Floor","Philadelphia","PA","191021119","2158955849","CSE","7364","7364, 7556","$0.00","Bioinformatics and biomedicine research are fundamental to our understanding of complex biological systems, impacting the science and technology of fields ranging from agricultural and environmental sciences to pharmaceutical and medical sciences. The research requires close collaboration among multi-disciplinary teams of researchers in quantitative sciences, life sciences, and their interfaces. The 10th Annual IEEE International Conference on Bioinformatics and Biomedicine (BIBM-2016) is held in Shenzhen, China from December 15-18, 2016 (https://cci.drexel.edu/ieeebibm/bibm2016/index.html). It will provide an open and interactive forum to promote multi- and interdisciplinary research and education in Bioinformatics and Biomedicine, facilitating the cross fertilization of ideas and bridging knowledge gaps. The scientific program will cover synergistic themes on genomics, systems biology, translational bioinformatics, and cross-cutting bioinformatics infrastructure to promote new research collaboration. Two special panels, Big Data and Bioinformatics, and the Bioinformatics Educational Workshop will foster discussion on research and education opportunities and barriers. As an effort to engage students, BIBM-2016 has involved them in the meeting organization and has included mentoring activities in the conference program. This support will provide the crucial funding needed to support the participation of undergraduates and graduate students in U.S.-based institutions, especially those from underrepresented groups, as a training opportunity for the next generation of scientists and engineers, thereby, broadening the scientific impact of this international conference.<br/><br/>The BIBM conference solicits high-quality original research papers in new computational techniques and methods in machine learning; data mining; text analysis; pattern recognition; knowledge representation; databases; data modeling; combinatorics; stochastic modeling; string and graph algorithms; linguistic methods; robotics; constraint satisfaction; data visualization; parallel computation; data integration; modeling and simulation and their application in life science domain. The conference proceedings will be published and indexed in IEEE Digital Library."
"1738285","CAREER: Structured Nonlinear Estimation via Message Passing: Theory and Applications","CCF","Comm & Information Foundations, SIGNAL PROCESSING","07/05/2016","09/19/2017","Alyson Fletcher","CA","University of California-Los Angeles","Continuing grant","Phillip Regalia","02/28/2019","$364,646.00","","akfletcher@ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","7797, 7936","1045, 7936","$0.00","A fundamental challenge in engineering and science today is that systems contain tremendous numbers of interconnected components with complex interactions.  Examples include communication and sensor networks, high-dimensional medical images, or biological systems such as vast sets of interconnected spiking neurons responding to a large array of stimuli.  Graphical models provide a probabilistic framework for modeling such systems, and contemporary message-passing algorithms lead to computationally feasible operations by decomposing problems on larger systems into smaller ones.  This research develops a broader methodology and new algorithms to address larger classes of more complex nonlinear interconnected systems with potential for great technological impact.  For wider dissemination, this is coupled with educational initiatives including developing courses combining perspectives in signal processing, machine learning, and statistics in the context of modern applications.  An open-source code base will foster cross-disciplinary research in students, educators, and industry.<br/><br/>This research combines the power of high-dimensional graphical models with recent advances in random systems theory to tackle a much wider scope of problems than traditional message-passing or linear methods allow.  The investigator addresses the key gaps in scalable estimation and model inference for structured nonlinear systems and develops powerful general algorithms for solving core problems.  Four main objectives address  aspects of this broader goal: (i) systematic general methods for representing systems characterized by arbitrary interconnections of linear and nonlinear components; (ii) computationally scalable message-passing algorithms for estimation; (iii) rigorous quantification of high-dimensional performance; and (iv) validation of the methods on real data, including neurological system identification.  These research thrusts greatly expand the scope of statistical estimation techniques and provide a rigorous approach to large-scale signal processing problems underlying the big data technology of today."
"1633747","Collaborative Research: Academic hiring networks and scientific productivity across disciplines","SMA","APPLIED MATHEMATICS, SciSIP-Sci of Sci Innov Policy, EPSCoR Co-Funding","09/01/2016","08/25/2016","Daniel Larremore","NM","Santa Fe Institute","Standard Grant","Cassidy Sugimoto","12/31/2017","$157,497.00","Mirta Galesic","daniel.larremore@colorado.edu","1399 HYDE PARK ROAD","SANTA FE","NM","875018943","5059462727","SBE","1266, 7626, 9150","4444, 7626, 8050, 8251","$0.00","Advances in science come from the collective and linked efforts of thousands of researchers working within and across disciplines. This project creates rigorous models of the composition, dynamics, and network structure of the United States? scientific workforce across heterogeneous independent institutions with different strengths and emphases.  The systematic influence on these institutional and individual characteristics on scientific advances across disciplines is investigated. The results of this project will generate new insights into the composition of the scientific workforce and scientific productivity across fields. In addition, this project trains new graduate and undergraduate students in cutting-edge computational and statistical research techniques, and will develop and disseminate new large-scale open data sets on the composition of the United States? scientific workforce and provide new software for collecting structured data automatically from open unstructured sources.<br/><br/>This project uses state-of-the-art computational and statistical techniques from network science, machine learning, and social modeling to create a new technology platform for automatically and systematically collecting high-quality structured data on the composition, dynamics, and output of the scientific workforce.  These data will be combined with social survey results of individual researchers and with rigorous network methods to model the relationship between workforce composition, productivity, and observable differences at the individual and institutional levels within and between scientific fields. Mathematical models of the short- and long-term evolution of workforce in order to evaluate the likely outcomes of certain types of interventions and policies are developed."
"1628889","Cross-Validation for High-Dimensional and Nonparametric Models in Econometrics","SES","Economics","09/15/2016","08/31/2016","Zhipeng Liao","CA","University of California-Los Angeles","Standard Grant","Nancy Lutz","12/31/2018","$238,700.00","Denis Chetverikov","zhipeng.liao@econ.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","SBE","1320","1320","$0.00","It is now well-known in empirical economics that nonparametric/high-dimensional methods are important because these methods substantially reduce misspecification of the economic models, thereby minimizing the possibility of inconsistent estimation of model parameters. The implementation of nonparametric/high-dimensional methods, however, typically requires the practitioners to select a smoothing parameter. Nevertheless, adequate data-driven procedures for selecting smoothing parameters are not readily available, which might explain the slow adoption of nonparametric methods in applied work. In addition, estimators based on an inappropriate choice of the smoothing parameter may have large estimation or approximation error, and hence lead to detrimental mistakes in policy recommendations. This project therefore focuses on how to select a smoothing parameter for Lasso estimators and sieve estimators, because Lasso estimators are commonly used to estimate high-dimensional models using ""big data"" and machine learning techniques, and sieve estimators are one of important approaches to estimate non-parametric models. The investigators will derive theoretical results for these estimators and provide theoretically justified procedures to select the smoothing parameter. <br/><br/>This project investigates two lines of research: first, convergence rates of cross-validated Lasso estimator, and second, convergence rates of cross-validated sieve estimator. In the first line of research, the investigators will provide novel and practical cross-validation-based procedure to choose the regularization parameter for the Lasso estimator and show that the cross-validated Lasso estimator achieves the fastest possible rate of convergence up-to the logarithmic factor under certain conditions. In the second line of research, the investigators demonstrate that cross-validation produces sieve estimators with optimal rates of convergence for a large class of sieve estimators. This project will thus derive theoretical results for a large class of non-linear nonparametric/high-dimensional estimators when the smoothing/regularization parameter for the estimators is selected using cross-validation. Therefore, this project will provide a data-driven procedure for selecting the smoothing parameter, which should be of interest to researchers in diverse academic fields as well as in industry."
"1633164","BIGDATA: Collaborative Research: IA: Large-Scale Multi-Parameter Analysis of Honeybee Behavior in their Natural Habitat","IIS","ADVANCES IN BIO INFORMATICS, Big Data Science &Engineering, EPSCoR Co-Funding","09/01/2016","08/25/2016","Remi Megret","PR","University of Puerto Rico Mayaguez","Standard Grant","Reed Beaman","03/31/2017","$446,628.00","Edgar Acuna","remi.megret@upr.edu","Call Box 9000","Mayaguez","PR","006809000","7878312065","CSE","1165, 8083, 9150","7433, 8083, 9150","$0.00","Honey bees exhibit highly complex behavior and are vital for our agriculture. Due to the rich social organization of bees, the overall performance and health of a bee colony depends both on a successful division of labor among the bees and on adequate reaction to the environment, which involves complex behavioral patterns and biological mechanisms. Much remains to be discovered on these matters as research is currently limited by our ability to effectively collect and analyze individual?s behavior at large scale, out of the laboratory. The technology developed in this project will enable biologists to study the individual behavior of thousands of bees over extended periods of time. It builds on innovative algorithms and software to analyze big data collected from colonies in the field. Study of behavioral patterns at such scale will provide unique information to advance knowledge on biological processes such as circadian rhythms that influence bee behavior in addition to playing an important role in animals and humans. The models developed will help better understand factors involved in colony collapse disorder, thus guiding future research on threats to such an important pollinator. This work will be performed through the tight collaboration of a multi-disciplinary team of researchers to combine the latest advances in computer science and data science with expertise in biology.  It will provide the opportunity to train students from underrepresented minority on research at the intersection of these fields and to reach more than 600 undergraduate students, high school students, and the general public about how the Big Data approach can contribute to current scientific and ecological challenges.<br/><br/>The project will develop a platform for the high-throughput analysis of individual insect behaviors and gain new insights into the role of individual variations of behavior on bee colony performance. Joint video and sensor data acquisition will monitor marked individuals at multiple colonies over large continuous periods, generating the first datasets of bee activities of this kind on such a scale. Algorithms and software will be developed to take advantage of a High Performance Computing facility to perform the analysis of these massive datasets. Semi-supervised machine learning will leverage the large amount of data available to facilitate the creation of new detectors for parameters such as pollen carrying bees or fanning behavior, currently annotated manually. Predictive models and functional data analysis methods will be developed to find patterns in individual behavior based on multiple parameters and over large temporal scales. These advances are expected to help uncover mechanisms of individual variations previously unobservable. They will enable the first large scale biological study on the circadian rhythms of the bee based on the variations in behavior of individuals in multiple activities instead of reasoning on single activities or averages. Progress, datasets and software will be shared with the community on the project website (sites.google.com/a/upr.edu/bigdbee)."
"1648033","SBIR Phase I:  Development of an innovative software architecture for co-robots and smart devices to augment human capacity with regard to mundane tasking in the service sector","IIP","SMALL BUSINESS PHASE I","12/15/2016","12/05/2016","Gregory Scott","VA","Service Robotics & Technologies","Standard Grant","Muralidharan S. Nair","11/30/2017","$225,000.00","","greg@srtlabs.com","2231 Crystal Dr #1000","Arlington","VA","222023584","5713278763","ENG","5371","5371, 6840, 8034, 8035, HPCC","$0.00","The broader impact/commercial potential of this project will be to create a user-friendly, universal control<br/>system for robots and sensors that for the first time would make it possible for nontechnical service<br/>industry professionals to task and control co-robots deployed in their facility. This simplification of<br/>software for an end-user will make systems of robots as easy to control as a single robot. Data collected<br/>over the lifecycle of the robots and sensors (which is currently not done) may aid in identification of new<br/>technologies to revolutionize system capabilities in the field of robotics. Although this concept could be<br/>applied to a wide array of service sector industries, SRT proposes to efficiently integrate and<br/>cooperatively control a fleet of robots and sensors to support custodial service industry tasks, because?<br/>though essential?custodial services add no commercial value to an organization. Thus the sector is<br/>clamoring for ways to reduce personnel turnover and other costs. In the U.S., custodial services are a $40<br/>billion industry; a mere 5% increase in efficiency would result in $2 billion in savings annually. This<br/>sector is rife with robots from disparate manufacturers, yet facility managers need an easy-to-use method<br/>to ?employ? multiple, specialized robots.<br/><br/>This Small Business Innovation Research (SBIR) Phase I project will develop a common software<br/>architecture that will serve as a universal receptor for service robotics systems. The predominant technical<br/>hurdle is effectively combining the software protocols and coordinating control of the robots. There<br/>currently is no common standard for robotics software systems. The actual robot (the hardware) is<br/>typically the primary focus for product developers, while the software is built from a variety of<br/>proprietary, heritage and open sources, and is as unique as the company developing it. Little or no<br/>consideration is given to how robots from different manufacturers could interact in the real world. By<br/>standardizing the data protocols for robots and smart devices, and making that openly available, SRT<br/>intends to drive future open API development for hardware. Standardized APIs will still protect the<br/>proprietary components, but will quickly allow for data sharing across multiple devices?facilitating<br/>research in robotics, machine learning, and big data?by allowing end-users to quickly integrate multiple<br/>components into the same system, and develop their own algorithms to utilize the shared data streams."
