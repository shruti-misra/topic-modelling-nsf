DocID,AwardNumber,Title,NSFOrganization,Program(s),StartDate,LastAmendmentDate,PrincipalInvestigator,State,Organization,AwardInstrument,ProgramManager,EndDate,AwardedAmountToDate,Co-PIName(s),PIEmailAddress,OrganizationStreet,OrganizationCity,OrganizationState,OrganizationZip,OrganizationPhone,NSFDirectorate,ProgramElementCode(s),ProgramReferenceCode(s),ARRAAmount,Abstract
0,1550705,EAGER: A Theory of Local Learning,IIS,Robust Intelligence,9/1/15,7/23/15,Pierre Baldi,CA,University of California-Irvine,Standard Grant,Weng-keen Wong,2/28/17,"$150,000.00 ",,pfbaldi@ics.uci.edu,"141 Innovation Drive, Ste 250",Irvine,CA,926173213,9498247295,CSE,7495,"7495, 7916",$0.00 ,"Artificial intelligence draws its inspiration from biological intelligence, and both rely on learning to achieve intelligent behavior. Thus, within AI, the field of machine learning plays a central role having indeed achieved impressive successes in dealing with many complex tasks, ranging from computer vision to language understanding, and thereby benefiting billions of humans. Machine learning uses large networks of artificial neurons, which are simplified versions of biological neurons, where learning is implemented by progressively adjusting the weights of the connections between these neurons. The deep learning problem faced by both biological and artificial neural networks is precisely the problem of how deep neurons, located far away from the network inputs or outputs, can adjust their connection weights to ensure that the networks behave intelligently. This fundamental problem and the space of its possible solutions are not well understood. Because deep learning has wide range of applications in technology and science, from computer vision to protein structure prediction, progress in our fundamental understanding of deep learning is likely to have a broad impact across multiple areas.  Furthermore, the theory of local learning is inspired by biological considerations and it has the potential for strengthening the bridge between AI/machine learning and neuroscience. The resulting theory, algorithms, data, software, and results will be broadly disseminated through multiple channels and integrated into educational and outreach efforts.  The project PI will continue his broad activities bringing research into undergraduate and graduate courses, outreach to local high school students through hosting at a summer program and lectures to high school students.<br/><br/>To try to address the deep learning problem, over half a century ago D. Hebb proposed a vague strategy often summarized by the expression ""neurons that fire together, wire together"". The essence of this effort is to improve our fundamental understanding of deep learning by bringing clarity to Hebb's proposal and providing a novel rigorous framework for studying learning rules. The framework requires first introducing the notion of local learning: in a physical neural system, learning rules for adjusting connection weights must be local, i.e. functions of only variables that are available locally. Thus one must separate the definition of local variables from the functional form that ties them together into a learning rule. This separation enables the creation of a systematic program for studying local learning rules, by first stratifying learning rules according to their functional complexity, and then studying their behaviors in networks of increasing complexity, from shallow and linear to deep and non-linear. The proposed program of study is likely to lead to the discovery of new learning rules and a better understanding of the capacity and limitations of local learning, ultimately advancing our theoretical and practical understanding of the deep learning problems and its solutions."
1,1453651,CAREER: New Directions in Deep Representation Learning from Complex Multimodal Data,IIS,"Info Integration & Informatics, Robust Intelligence",9/1/15,8/8/19,Honglak Lee,MI,Regents of the University of Michigan - Ann Arbor,Continuing Grant,Sylvia Spengler,8/31/21,"$488,617.00 ",,honglak@eecs.umich.edu,3003 South State St. Room 1062,Ann Arbor,MI,481091274,7347636438,CSE,"7364, 7495","1045, 7364, 7495",$0.00 ,"The goal of deep learning is to learn an abstract representation of data with a hierarchical and compositional structure. Deep learning methods can effectively learn discriminative features from high-dimensional input data (e.g., for classification), and have been successfully applied to many real-world problems, such as image classification, speech recognition, and text modeling. Despite these successes, there still remains a challenging open question: how can we learn a robust deep representation that allows for holistic understanding and high-level reasoning from complex data?  This CAREER project aims to address this question and is expected to result in novel deep architectures, graphical models, and algorithmic advances for inference, learning, and optimization in deep representation learning.  The research outcomes will be disseminated through publications, talks, and tutorials. In addition to advancing the state of the art in deep learning and the many applications it entails, the project will integrate research and education through 1) developing courses in machine learning that include deep learning as a key topic; 2) mentoring significant graduate and undergraduate research activities; and 3) reaching out to K-12 students via hosting demo sessions and mentoring for science fair/research projects. <br/> <br/>This project investigates the following closely interrelated and complementary thrusts:  First, it develops deep learning algorithms to disentangle factors of variation from complex data. This is done by modeling higher-order interactions between multiple groups of latent variables with a deep generative model (e.g., modeling face images via interaction of latent factors that correspond to identity, viewpoint, and emotion). In addition to better generalization, this approach is amenable to high-level reasoning, such as making analogies. Modeling higher-order interaction will be approached by learning a sub-manifold for each factor of variation, where correspondence information is used for regularizing the latent representation. The project will also develop weakly-supervised and semi-supervised disentangling algorithms that automatically establish correspondences without manual supervision. Second, the project develops deep representation learning methods for structured prediction problems. Specifically, it will develop a graphical model with deep representations that can model complex dependencies between output variables. This framework can be also viewed as data-driven modeling of higher-order prior on structured data, and can be used for modeling higher-order conditional random fields that permit efficient inference and learning. In addition, the project develops stochastic conditional generative models for structured prediction problems that involve uncertainty (i.e., one-to-many mappings).  Third, the project develops novel deep learning algorithms for constructing shared representations from multiple heterogeneous input modalities, such as image and text, audio and video, and multiple sensor streams. The main idea is to separately model conditional distribution of each input modality given other modalities. This approach addresses the well-known difficulty of modeling a joint distribution across heterogeneous multimodal input, and provides a theoretical analysis on conditions under which the approach can recover a consistent generative model. This formulation allows for robust recognition and high-level reasoning from heterogeneous multimodal data. Overall, these three thrusts are complementary and are expected to play synergistic roles in tackling a broader range of AI problems and moving beyond the current state-of-the-art in deep learning."
2,1520228,SBIR Phase I:  Automated Public Speaking Assessment,IIP,SBIR Phase I,7/1/15,12/14/15,Debra Cancro,MD,Autonomy Engine LLC,Standard Grant,Glenn H. Larsen,6/30/16,"$179,995.00 ",,debra@myvoicevibes.com,7224 Shub Farm Rd,Marriottsville,MD,211041171,4107461696,ENG,5371,"118E, 163E, 5371, 8031, 8032, 8039",$0.00 ,"This SBIR Phase I project will study the feasibility of automated speaking assessment to help students improve their oral communication skills.  According to a survey of human resource officials, only 25 percent of today's college graduates enter the workforce with well-developed speaking skills. This means many people are unable to effectively persuade an audience of their position, thus limiting their ability to sell new ideas and be successful in their jobs.  The project will investigate novel research in speech technology that enables students to receive objective, personalized feedback at any time.  By reinforcing communication skills through self-paced practice and feedback, users will be better prepared with the communications skills necessary to perform job tasks and move into leadership roles.  The completed system will initially be offered to over two million people in the United States who participate in public speaking training annually.  Potential future applications for the technology include teacher assessments, call center monitoring, interview training, role playing, human resources assessments, patient care, services for the deaf, language learning and student assessments.<br/><br/>This project will develop key concepts for automated public speaking assessment such that a student's vocal delivery can be objectively measured and presented in a manner that creates an independent, personalized learning experience.  Linking listener perceptions to speech behaviors is a novel direction in automated assessment for speech.  Automated assessment for speech has already been demonstrated in the area of spoken language proficiency, which leverages automated speech recognition and semantic analysis.  Automated voice assessment has also been utilized in lie detection and emotion detection, which focus on autonomic responses in the user's voice, such as when stress affects the vocal cords.  The hypothesis behind this SBIR project is that software can help speakers to consciously use and modify non-semantic speech behaviors to produce more desirable listener perceptions.  The Phase 1 objectives are to identify key features of voice that can be used to predict audience perception and develop initial software models to estimate aspects of audience perception.  To achieve these objectives, a combination of expert feature enumeration, deep learning feature identification and machine learning will be applied and iteratively tested against a large corpus of actor voices and human perception ratings."
3,1551489,EAGER:Topological Machine Learning,CCF,Comm & Information Foundations,9/1/15,8/25/15,Abbas Ourmazd,WI,University of Wisconsin-Milwaukee,Standard Grant,Phillip Regalia,8/31/18,"$199,891.00 ",Dimitrios Giannakis,ourmazd@uwm.edu,P O BOX 340,Milwaukee,WI,532010340,4142294853,CSE,7797,"7916, 7936",$0.00 ,"Deep learning, first proposed in 1989, still represents the most effective means for extracting specific information from large datasets. This approach exploits many nonlinear processing layers to develop representations of data at increasing levels of abstraction. Deep learning has demonstrated best-in-class performance in a range of applications, including image and speech recognition, and demonstrated promising results for tasks based on natural language understanding and translation. Crudely speaking, deep learning acquires ¡®knowledge¡¯ by tuning large numbers (¡Ý200) of fitting parameters during a supervised learning phase. These parameters are then used to extract information from previously ¡®unseen¡¯ data. Ultimately, deep learning is premised on using a large number of tuning parameters to develop nonlinear feature detectors capable of efficiently representing the intrinsic structure of the data at an abstract level.¡±<br/><br/>This project will examine two potentially powerful, but highly speculative alternative approaches to extract information from data. These approaches exploit the intrinsic properties of the data rather than an extensive set of tuning parameters. Both approaches are based on conjectures made by the PIs regarding possible extensions of techniques successfully applied in very different branches of mathematics. The first concerns the extraction of specific structural information from random sightings of objects; the second, forecasting the behavior of dynamical systems [6]. The ultimate goal of this project is to determine whether the aforementioned algebraic topological approaches or techniques developed for the forecasting of high-dimensional time-series or some variations thereof, can be exploited to create a new class of non-iterative unsupervised learning algorithms. The broader impact of the project is the training of young scientists at the hitherto unexplored intersection of abstract mathematics and machine learning, with possible applications in science,<br/>technology, and commerce."
4,1545471,2015 Association for Computational Linquistics (ACL) Student Research Workshop,IIS,ROBUST INTELLIGENCE,7/1/15,5/28/15,Emily Bender,WA,University of Washington,Standard Grant,Tatiana D. Korelsky,6/30/17,"$10,500.00 ",,ebender@u.washington.edu,4333 Brooklyn Ave NE,Seattle,WA,981950001,2065434043,CSE,7495,"7495, 7556",$0.00 ,"The Association for Computational Linguistics (ACL) is the primary international organization for computational linguistics and natural language processing.  It also is one of the primary application areas for researchers in machine learning and artificial intelligence.  The proceedings of its annual meeting provide the foundation of the field; it is the most cited and most respected publication in computational linguistics.  Thus, it is also the most important gathering of researchers in computational linguistics and natural language processing.  ACL has a twenty-year history of including a Student Research Workshop, which helps create a new generation of researchers with a more thorough understanding of their field, with connections and collaborations across institutions, and with innovative and exciting research programs.  This contributes to America's pool of researchers with the needed scientific and engineering knowledge and skills. The workshop encourages a spirit of collaborative research and builds a supportive environment for a new generation of computational linguists. To this end, the grant will subsidize travel, registration and accommodation expenses for student participants and organizers traveling from the US to the ACL in Beijing.<br/><br/>The student research workshop will be a part of the 2015 meeting of the ACL held in Beijing, China from July 27 to July 29.  The workshop will solicit submissions in two categories: (1) thesis proposals for advanced students who have decided on a thesis topic and wish to receive feedback and (2) research papers describing completed work or work in progress with significant preliminary results.  Each accepted paper will be assigned an established research mentor who will meet with the student during the conference to provide individual feedback.  Both paper types will be presented in the poster session of the main conference.  The Student Research Workshop enriches the intellectual life of the participating students.  Each student is mentored by an experienced researcher; this not only shapes the student's specific project but helps build the student's awareness of the field.  The students gain exposure by presenting their work earlier than they would otherwise (i.e., in a form not yet ready for the main conference).  This is particularly valuable for researchers from smaller institutions and undergraduate students. The student organizers also benefit from the organization of the workshop, gaining experience with the full process from soliciting reviewers, to managing the review process, producing the proceedings, and running the event.  They also build connections with the senior researchers who participate as mentors in the event.  This grant will subsidize the travel, conference registration and lodging for both student participants and student organizers."
5,1524565,Comp Cog:  Collaborative Research on the Development of Visual Object Recognition,BCS,"DS -Developmental Sciences, Perception, Action & Cognition, Robust Intelligence",8/1/15,9/2/16,James Rehg,GA,Georgia Tech Research Corporation,Continuing Grant,Peter Vishton,7/31/19,"$313,582.00 ","Fuxin Li, Maithilee Kunda",rehg@cc.gatech.edu,Office of Sponsored Programs,Atlanta,GA,303320420,4048944819,SBE,"1698, 7252, 7495","1698, 7252, 7298, 7495, 9179",$0.00 ,"Human visual object recognition is fast and robust.  People can recognize a large number of visual objects in complex scenes, from varied views, and in less than optimal circumstances.  This ability underlies many advanced human skills, including tool use, reading, and navigation.  Artificial intelligence devices do not yet approach the level of skill of everyday human object recognition. This project will address one gap in current knowledge, an understanding of the visual experiences that allow skilled object recognition to develop, by capturing and analyzing the visual experiences of 1- to 2-year-old toddlers.  This is a key period for understanding human visual object recognition because it is the time when toddlers learn a large number of object categories, when they learn the names for those objects, and when they instrumentally act on and use objects as tools.  Two-year-old children, unlike computer vision systems, rapidly learn to recognize many visual objects.  This project seeks to understand how the training experiences (everyday object viewing) of toddlers may be optimal for building robust visual object recognition. The project aims to (1) understand the visual and statistical regularities in 1- to 2-year-old children's experiences of common objects (e.g., cups, chairs, trucks, dogs) and (2) determine whether a training regimen like that experienced by human toddlers supports visual object recognition by state-of-the art machine vision. <br/><br/>Considerable progress in understanding adult vision has been made by studying the visual statistics of ""natural scenes."" However, there is concern about possible artifacts in these scenes because they typically photographs taken by adults and thus potentially biased by the already developed mature visual system that holds the camera and frames the pictures. Also, photographed scenes differ systematically from the scenes sampled by people as they move about and act in the world.  Accordingly, there is increased interest in egocentric views collected from body-worn cameras, the method used in the present work.  Toddlers will wear lightweight head cameras as they go about their daily activities, allowing the investigators to capture the objects the toddlers see and the perspectives and contexts in which they see them.  The research will analyze the frequency, views, visual properties, and range of seen objects for the first 100 object names normatively learned by young children, providing a first description of the early learning environment for human visual object recognition.  These toddler-perspective scenes  will be used as inputs to machine learning models to better understand how the visual information in the scenes supports and constrains the development of visual object recognition. Machine-learning experiments will determine which properties and statistical regularities are most critical for learning to recognize common object categories in multiple scene contexts.  Data collected will be shared through Databrary, an open data library for developmental science."
6,1523982,Comp Cog:  Collaborative Research on the Development of Visual Object Recognition,BCS,"DS -Developmental Sciences, Perception, Action & Cognition",8/1/15,9/2/16,Linda Smith,IN,Indiana University,Continuing Grant,Chalandra Bryant,7/31/18,"$405,158.00 ",Chen Yu,smith4@indiana.edu,509 E 3RD ST,Bloomington,IN,474013654,3172783473,SBE,"1698, 7252","1698, 7252, 7298, 9178",$0.00 ,"Human visual object recognition is fast and robust.  People can recognize a large number of visual objects in complex scenes, from varied views, and in less than optimal circumstances.  This ability underlies many advanced human skills, including tool use, reading, and navigation.  Artificial intelligence devices do not yet approach the level of skill of everyday human object recognition. This project will address one gap in current knowledge, an understanding of the visual experiences that allow skilled object recognition to develop, by capturing and analyzing the visual experiences of 1- to 2-year-old toddlers.  This is a key period for understanding human visual object recognition because it is the time when toddlers learn a large number of object categories, when they learn the names for those objects, and when they instrumentally act on and use objects as tools.  Two-year-old children, unlike computer vision systems, rapidly learn to recognize many visual objects.  This project seeks to understand how the training experiences (everyday object viewing) of toddlers may be optimal for building robust visual object recognition. The project aims to (1) understand the visual and statistical regularities in 1- to 2-year-old children's experiences of common objects (e.g., cups, chairs, trucks, dogs) and (2) determine whether a training regimen like that experienced by human toddlers supports visual object recognition by state-of-the art machine vision. <br/><br/>Considerable progress in understanding adult vision has been made by studying the visual statistics of ""natural scenes."" However, there is concern about possible artifacts in these scenes because they typically photographs taken by adults and thus potentially biased by the already developed mature visual system that holds the camera and frames the pictures. Also, photographed scenes differ systematically from the scenes sampled by people as they move about and act in the world.  Accordingly, there is increased interest in egocentric views collected from body-worn cameras, the method used in the present work.  Toddlers will wear lightweight head cameras as they go about their daily activities, allowing the investigators to capture the objects the toddlers see and the perspectives and contexts in which they see them.  The research will analyze the frequency, views, visual properties, and range of seen objects for the first 100 object names normatively learned by young children, providing a first description of the early learning environment for human visual object recognition.  These toddler-perspective scenes  will be used as inputs to machine learning models to better understand how the visual information in the scenes supports and constrains the development of visual object recognition. Machine-learning experiments will determine which properties and statistical regularities are most critical for learning to recognize common object categories in multiple scene contexts.  Data collected will be shared through Databrary, an open data library for developmental science."
7,1513128,CI-EN: Enhancements for the Kaldi Speech Recognition Toolkit,CNS,"CCRI-CISE Cmnty Rsrch Infrstrc, Robust Intelligence",7/1/15,2/20/18,Jan Trmal,MD,Johns Hopkins University,Standard Grant,Tatiana Korelsky,6/30/19,"$839,999.00 ",Daniel Povey,jtrmal@gmail.com,1101 E 33rd St,Baltimore,MD,212182686,4439971898,CSE,"7359, 7495","7359, 7495",$0.00 ,"Societal benefits of automatic speech recognition (ASR) technology are beginning to accrue, thanks to creative new devices and applications, such as voice-enabled search on mobile devices, in-car functions via voice commands, and educational software for language learning.  Application developers, however, have a difficult time mastering the complexities of ASR technology.  Academic research who use ASR, e.g. for studying human-computer dialog or language acquisition by children, also have a similarly significant barrier to entry.  Finally, ASR researchers themselves need state-of-the-art baseline systems on which to make further improvements, which is a daunting task for most academic and even some industry groups.  There is thus a strong demand for an ASR toolkit that is freely available, easy to use, state-of-the-art, and kept up to date with new advances in ASR technology.  The Kaldi speech recognition toolkit, whose development was partially supported by past NSF awards is (i) freely available via http://kaldi.sourceforge.net, (ii) provides easy-to-use recipes to develop high-performing ASR systems for a number of widely used datasets, and (iii) is being adopted by hundreds of researchers to fulfill the aforementioned needs.  Keeping Kaldi up-to-date and providing advice and technical support to Kaldi users is therefore becoming a crucial enabler of the research of faculty, students and developers in a variety of academic disciplines and industrial sectors.<br/><br/>This CISE research infrastructure project seeks to enhance and maintain the Kaldi speech recognition toolkit.  Johns Hopkins University researchers, who first created Kaldi, will further develop its capabilities to better enable future research, both in core ASR and ASR applications.  They have recently added support for deep neural network training and online (real-time) decoding.  Specific new features that will be developed and added to Kaldi include improved voice activity detection, a faster decoder, support for recurrent neural net language models, a more flexible framework for experimenting with deep neural networks, and related capabilities such as recurrent neural net acoustic models.  The investigators will publish their know-how and disseminate these enhancements through scientific conferences and workshops frequently attended by Kaldi users, and will continually solicit their feedback about future enhancements through discussion forums and conference participation.  They have also already set up a mailing list and a discussion forum for Kaldi users to post technical questions and exchange solutions to commonly encountered problems.  User support will also be provided by this project via these on-line discussion lists and developers forums."
8,1524427,RI: Small: Theory and Algorithms for Learning Perturbation Models,IIS,Robust Intelligence,9/1/15,8/5/15,Tommi Jaakkola,MA,Massachusetts Institute of Technology,Standard Grant,Weng-keen Wong,8/31/18,"$407,091.00 ",,tommi@csail.mit.edu,77 MASSACHUSETTS AVE,Cambridge,MA,21394301,6172531000,CSE,7495,"7495, 7923",$0.00 ,"Machine learning concerns designing and understanding computer programs that learn from experience. Modern complex settings (for example natural language) require the use of flexible probability models that permit one to entertain large numbers of possible hypotheses (semantics) underlying the observations (sentences). In such models likely structures (parse trees) are guided by functions that assess the suitability of structures by breaking them into smaller pieces. Richer models require larger subsets making it challenging to efficiently explore large sets of possible hypotheses. <br/><br/>This project takes a fresh look at structured modeling by developing a new paradigm for modeling by combining randomization of parameters and combinatorial optimization. The combination provides a mechanism for inducing complex distributions over structures yet explicitly maintaining easy generation of likely structures. We pursue a comprehensive plan to understand, extend, and design these perturbation models towards the end goal of solving significant cross-cutting applied problems in natural language processing such as parsing or structured recommender tasks such as paraphrasing. Beyond modeling, the proposed work has the potential to merge tools and techniques across areas from theoretical computer science (stability, tractability), combinatorial optimization (relaxations, certificates), to probability (sampling from convex bodies). The tools developed will be broadly useful across prominent areas, from computer vision, natural language processing, to medical informatics and computational biology. The proposed work by its very nature compels strong collaborative relationships across disciplinary boundaries, from theory to applications. The PI will actively pursue these opportunities. All the software produced in this project will be open-sourced, and made available for download. The PI will also engage in outreach activities that enable high school students to participate."
9,1463988,CRII: III: Scaling up Distance Metric Learning for Large-scale Ultrahigh-dimensional Data,IIS,Info Integration & Informatics,3/1/15,3/3/15,Tianbao Yang,IA,University of Iowa,Standard Grant,Maria Zemankova,2/28/18,"$174,576.00 ",,tianbao-yang@uiowa.edu,2 GILMORE HALL,IOWA CITY,IA,522421320,3193352123,CSE,7364,"7364, 8228, 9150",$0.00 ,"This project is to research and develop highly scalable stochastic optimization algorithms for distance metric learning (DML) for large-scale ultrahigh-dimensional (LSUD) data. DML is a fundamental problem in machine learning aiming to learn a distance metric such that intra-class variation is small and inter-class variation is large.  When the scale and dimensionality of data is very large, the computational cost of DML is prohibitive. Domains utilizing machine learning techniques such as computer vision, natural language processing and bioinformatics will be directly impacted by this research. For example, one application is fine-grained image classification, e.g., categorizing different types of flowers or models of vehicles from pictures (this application will be used as one criteria to evaluate success of the research.) The research will enable data scientists to extract more knowledge from massive high-dimensional data complementing the White House BIG DATA Initiative to analyze large and complex data sets. Beyond its research impact, this project will facilitate the development of a new machine learning course at the University of Iowa (UI), and contribute to training future professionals  in big data analytics. Broader impact will be further affected by dissemination of results through publications, open-sourced software, etc.<br/><br/>This project addresses the computational challenges of LSUD-DML by scaling up the state of the art stochastic gradient descent (SGD) methods. A key computational bottleneck in applying SGD to DML is to project the updated solution into a complicated feasible domain at each iteration. The innovative proposed ideas lie at reducing the total cost of projections by (i) constructing and exploring a low-rank structured stochastic gradient to reduce the cost of projection, and (ii) dividing iterations into epochs and performing a projection-efficient SGD at each epoch to reduce the number of projections. Investigating data-dependent sampling strategies (i.e., selective sampling, importance sampling, and a combination of both) for LSUD-DML will further scale up the proposed methods. This research will provide experimental evidence regarding the scalability of the proposed algorithms while revealing insights into the proposed techniques and various analytical tradeoffs.<br/><br/>For further information see the project web site at: <br/>http://homepage.cs.uiowa.edu/~tyng/dml.html."
10,1539012,Affect-Based Video Retrieval,IIS,Robust Intelligence,9/1/15,4/27/20,Qiang Ji,NY,Rensselaer Polytechnic Institute,Standard Grant,Jie Yang,12/31/20,"$482,000.00 ",,qji@ecse.rpi.edu,110 8TH ST,Troy,NY,121803522,5182766000,CSE,7495,"002Z, 7495, 9251",$0.00 ,"This project develops advanced machine learning and computer vision technologies for affect-based video retrieval to retrieve videos according to their emotional content.  Introducing such a personal touch into video retrieval can improve user's interaction experiences with videos by allowing user to retrieve and organize videos based on their specific emotional needs.  In addition, the project also has impacts on a wide range of fields including advertisement and education, allowing the video creators such as advertisers, and educators to effectively customize the videos to best serve the users' emotional needs. The project also contributes to education and student training.  The project is integrated with education by (a) introducing a course on computer vision for affective computing; (b) involving undergraduate and graduate students in this project, especially those from under-represented groups; and (c) organizing workshops and tutorials in major computer vision and affective computing conferences on topics related to this research for further dissemination of the research ideas and results.<br/><br/>This research addresses problems in video affective content analysis. Affect-based video retrieval faces two major challenges. First, there exists a significant semantic gap between the low level video features and the high level affective content of the video. Second, due to the subjective nature of user's perception of emotion, user's emotional responses to videos vary significantly with people. For the first challenge, the PI develops a novel generative deep model to automatically learn an affect-sensitive multi-modal middle level video representation from the raw video data.  To further improve the characterization of the video's affective content, the PI augments it with semantic video attributes derived from well-established video production knowledge to produce a hybrid multi-modal middle level video representation. The hybrid multi-modal middle level representation can effectively bridge the gap between the raw video and its affective content.  For the second challenge, the PI employs a multi-task deep learning method to tailor the middle level representation to each user's specific affective preferences in order to maximize their experience with videos."
11,1506882,Nonlinear and Nonstationary Time Series,DMS,STATISTICS,8/1/15,6/8/18,Zhao Ren,PA,University of Pittsburgh,Continuing Grant,Gabor Szekely,7/31/19,"$337,449.00 ",,zren@pitt.edu,300 Murdoch Building,Pittsburgh,PA,152603203,4126247400,MPS,1269,,$0.00 ,"This project focuses on two problems in analyzing complex data collected over time such as daily stock market returns.  Such irregular time series data occur in many diverse fields such as biology and medicine, ecology, genetics, geoscience, speech recognition, econometrics and finance, and computer vision to mention a few.  Because the data are irregular, problems such as predicting highly volatile periods are difficult.  In general, rather than using explicit mathematical formulas, one must rely on numerical or computer-based optimization.  However, for such data, existing methods have poor properties. The goal of this project is to vastly improve on the existing computational methods. The second project focuses on the fast detection of genes in long DNA sequences.  While many methods have been developed for a thorough micro-analysis of short sequences, there is a shortage of powerful procedures for the macro-analysis of long DNA sequences.<br/><br/>The project focuses on two problems in nonlinear and nonstationary time series analysis. First, there has been an intense focus on the analysis of nonlinear and non-Gaussian time series models via numerical methods.  Particle samplers are a promising approach for classical and Bayesian estimation, but they are plagued by particle degeneration and by poor mixing.  However, there is no need to abandon particle methods; they can be improved, and this is the goal of this project.  For example, particle Gibbs methods can be fashioned to be fast and efficient while improving the mixing property of the sampler.  The basic idea is to build a particle-filter-like procedure that avoids path degeneracy by conditioning on particles. This conditioning implies an invariance property, which is key to its applicability as a particle sampler.  The invariance property is also key to providing the asymptotic accuracy of the sampler.  It is not enough to be asymptotically accurate because of the curse of dimensionality, which we try to avoid.  Moreover, while the technique is not perfect, the methodology can be used as a basis from which to explore faster methods while avoiding poor mixing. The method can also be used in classical inference to perform derivative free maximum likelihood estimation (e.g., EM algorithm) when the likelihood can only be evaluated numerically. The main interest of the second project is on the detection of coding (genes) and other interesting features in very long DNA sequences.  In particular, the focus is on fast detection of change points in long DNA sequences based on the concept of spectral envelope using a wavelet basis.  Rapid accumulation of genomic sequences has increased demand for methods to decipher the genetic information gathered in data banks. Combining statistical analysis with modern computer power makes it feasible to search, at high speeds, for diagnostic patterns within long sequences.  This combination provides an automated approach to evaluating similarities and differences among patterns in very long sequences and aids in the discovery of the biochemical information hidden in these organic molecules."
12,1539739,Support for Doctoral Students to Attend International Conferences: Artificial Intelligence in Education (AIED 2015) and Educational Data Mining Society (EDM 2015),IIS,Cyberlearn & Future Learn Tech,5/1/15,4/10/15,Beverly Woolf,MA,University of Massachusetts Amherst,Standard Grant,christopher hoadley,4/30/16,"$18,500.00 ",,bev@cs.umass.edu,Research Administration Building,Hadley,MA,10359450,4135450698,CSE,8020,"7556, 8045",$0.00 ,"The Cyberlearning and Future Learning Technologies Program funds efforts that support envisioning the future of learning technologies and advancing what we know about how people learn in technology-rich environments.  Capacity-building (CAP) projects help build the capacity of the field to do high-quality, high-impact research on learning.  This project supports the mission of NSF to train more advanced professionals in Science, Technology, Engineering, and Mathematics (STEM) by supporting doctoral students attendance at the International Conference on Artificial Intelligence in Education (AIED) and the 6th International Conference on Educational Data Mining (EDM) to be held in Madrid, Spain in June of 2015. <br/><br/>The intellectual merit of the grant rests on the quality of the research being presented at the conferences. Together, the conferences provide cross-fertilization of information and ideas from artificial intelligence, cognitive science, machine learning, education, learning sciences, educational technology, psychology, philosophy, sociology, anthropology, linguistics, and the many domain-specific areas for which cyberlearning systems are designed and built. The broader impact of the work rests in the inculcation of students into this field. Doctoral students will be selected from U.S. institutions. The criteria for selection include either having a paper accepted for one of the conferences and/or or submitting an strong rationale for what the student would learn at the conference.  All students receiving support will be assigned a mentor.  The selecting committee will strive to ensure that there is diversity of institutions, topics, disciplines, ethnicities, and gender in the cohort of awardees.  Selected students will receive up to $1,000 to partially cover expenses."
13,1552238,Computation of crowded geodesics on the universal Teichmueller space for planar shape matching in computer vision,DMS,"APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS",6/30/15,9/15/15,Akil Narayan,UT,University of Utah,Continuing Grant,Leland Jameson,7/31/17,"$195,562.00 ",,akil@sci.utah.edu,75 S 2000 E,SALT LAKE CITY,UT,841128930,8015816903,MPS,"1266, 1271","9150, 9263",$0.00 ,"Quantifying the (dis)similarity between two shapes is a central problem in computer vision. One distance metric on the space of planar shapes is realized by identifying this space as a subset of the Universal Teichmueller Space, and equipping it with the Weil-Petersson metric. This results in a metric that is scale- and translation-invariant on shapes, and has unique geodesic flow between two shape endpoints. The work of this proposal develops robust computational methods for the computation of metric distances and geodesics between shapes on this space. The major difficulty lies in computations involving ""crowded"" shapes, i.e., those with elongated, winding, or extended protrusions. Such shapes stymie finite-precision computations because direct algorithms suffer from severe roundoff error. The major thrusts of this proposal develop algorithmic methodologies to address roundoff error and related issues: The Zipper conformal mapping algorithm will be augmented to produce accurate conformal maps for crowded shapes. The velocity field representation on a geodesic will be rewritten into a form that is resistant to roundoff error. The geodesic equation will be transformed into a expression that takes advantage of the aforementioned velocity field transformation, and can effectively flow between crowded shapes. The final phase of this project will demonstrate accurate geodesic flow and distance computations between crowded shapes. The methods developed under this project can be applied to several related problems in scientific computing: solutions to differential equations on irregular geometries through conformal mapping, conservative integration methods with ill-conditioned particle systems, and moving-mesh kernel approximations.<br/><br/>The work of this project can contribute to far-reaching applications in scientific and computer vision problems: automated object recognition (e.g. projectile identification), outline classification (determination of an animal's species), medical imaging (usage of MRI to diagnose dementia and related diseases), and artificial intelligence (visual recognition and interpretation) to name a few. All computational deliverables (computer code, example simulations, documentation) will be made publicly available. Through the engagement of students in related research tasks, this project will contribute to the educational development of future engineers, mathematicians, and computer scientists."
14,1539011,VEC: Medium: Large-Scale Visual Recognition: From Cloud Data Centers to Wearable Devices,IIS,"Information Technology Researc, Special Projects - CNS",10/1/15,2/26/19,Jia Deng,MI,Regents of the University of Michigan - Ann Arbor,Continuing Grant,Maria Zemankova,9/30/20,"$960,000.00 ","Thomas Wenisch, Kevin Pipe, Jason Mars, Lingjia Tang",jiadeng@princeton.edu,3003 South State St. Room 1062,Ann Arbor,MI,481091274,7347636438,CSE,"1640, 1714","002Z, 1640, 7924",$0.00 ,"Advances in computer hardware and software promise to revolutionize the ways in which society interacts with visual information. However, visual recognition systems are limited by the lack of a practical means to classify the millions of concepts that arise in visual scenes and thus efficiently recognize when a small number of these concepts appear in a given scene. Furthermore, while real-time processing of visual data could significantly expand our perception of our surroundings, state-of-the-art vision systems cannot currently be implemented on wearable devices such as smartphones due to the limited heat dissipation (e.g., no fans or liquid cooling) and power such devices can provide. This research will overcome these challenges by developing artificial intelligence (AI) systems that efficiently manage the resources most crucial for high-performance wearable-based visual recognition, including the wearable device's real-time power consumption and computation. These systems will be empowered to initiate bursts of intense computation that are thermally managed by materials within the wearable device which are engineered to melt during heavy heating and solidify between bursts. Moreover, the AI systems will govern the communication between the device and external (cloud-based) computation resources as well as large-scale visual concept databases housed in data centers, thus providing extreme performance in a wearable form factor. Central concepts of this work will be integrated in undergraduate and graduate coursework, and a demonstration system will be made available to the research community and used in educational modules for high school students.<br/><br/>This effort seeks to advance the core capabilities of large-scale visual recognition by co-designing visual models and computing infrastructure. The goal is to enable encyclopedic, real-time visual recognition through seamless integration of visual computing on wearable devices and in the cloud. The PIs envision a wearable visual recognition system that continuously captures live video input while providing intelligent, real-time assistance through automatic or on-demand visual recognition by means of a combination of computation at the device and offloading to the cloud. Such a system is not currently feasible due to a number of fundamental challenges. First, the severe energy and thermal constraints of wearable devices render them incapable of performing the intensive computation necessary for visual recognition. Second,  it remains an open question how to support encyclopedic recognition in terms of both visual models and data center infrastructure. In particular, it remains unclear how current visual models, although highly successful at recognizing 1,000 object categories, can scale to millions or more distinct visual concepts. Moreover, such an encyclopedic visual model must be supported through data center infrastructure, but little progress has been made on how to build such infrastructure. This project addresses these fundamental challenges through an interdisciplinary approach integrating computer vision, hardware architecture, VLSI design, and heat transfer. The PIs will investigate three research thrusts. In Thrust 1, the PIs will develop a new type of deep neural networks that allow resource-efficient execution of modules. This new framework provide a unified way to design, learn, and run scalable visual models that can maximize the utility of recognition subject to resource constraints, such as latency, energy, or thermal dissipation of a wearable device. In Thrust 2, the PIs will design and fabricate a visual processing chip capable of computational sprinting (bursts of extreme computation well above steady-state thermal dissipation capabilities), leveraging the new framework developed in Thrust 1. In Thrust 3, the PIs will design datacenter infrastructure that supports large-scale hierarchical indexing of visual concepts for encyclopedic recognition, with a focus on latency, throughput, and energy efficiency. Finally, the PIs will build a demonstration system to evaluate the proposed algorithms, software, and hardware components and to assess the overall performance of an end-to-end system. The project web site (http://mivec.eecs.umich.edu/) will provide access to the results of this research including technical reports, datasets, and source code."
15,1453721,CAREER: Statistical Information Retrieval Modeling for Complex Search,IIS,Info Integration & Informatics,2/1/15,2/15/18,Grace Hui Yang,DC,Georgetown University,Continuing Grant,Wei-Shinn Ku,1/31/21,"$552,012.00 ",,huiyang@cs.georgetown.edu,37th & O St N W,Washington,DC,200571789,2026250100,CSE,7364,"1045, 7364",$0.00 ,"With the increasing popularity of Web applications and users' deep involvement in the Web, search engines face great challenges with a new degree of complexity. For instance, location-based services collect more complex contextual information such as geo-locations, season, time and temperature. Users' search activities have become more complex and usually task-based generating a variety of feedback and engagement signals such as clicks, mouse movements, eye tracking results, and query reformulations.  Moreover, search is not only an individual user's personalized activity, but also activities shared by many users with similar information needs. Search engines are presented with the richest types of information and the largest amount of data ever and the complexity of the available information is tremendous. This demands that search engines be upgraded from retrieval systems that basically look for documents for single queries to decision engines that can pick the best choices for information seeking tasks. Through disseminating research results in papers and tools, the project will make three types of broad impact. First, the techniques developed in this project will benefit a broad population of everyday users and empower them to deal with complex, task-oriented web search. Second, the algorithms and software developed will provide fellow researchers and practitioners a handful of useful tools for solving IR problems incorporating dynamics. Third, the project will reach out to middle school girls and elementary school students. It will be easy for any search engine user to start using the proposed new search engine. However, to be an expert on IR, students need to be good at mathematics, natural language processing, user interface, artificial intelligence, and programming. This will be an excellent project to attract young people and minorities to these STEM disciplines.<br/><br/>This project aims to create the next generation search engines, to be more specific, decision engines. The focus will be on designing, experimenting, and deploying statistical models for modeling the dynamics presented in the search process. The technical challenges are: (1) given the complexity of the available data, integrating a search engine appropriately into the right places in the larger context for the ultimate information seeking tasks; (2) providing theoretical and practical support to formal modeling of user engagement and other dynamics in retrieval models for better retrieval effectiveness; (3) modeling a user's exploration in the information space and optimizing a search engine's actions and algorithms; and (4) modeling interactions between a user and a search engine as well as interactions among multiple users, creating the dynamic environment for them all to interact and to game with each other and achieve a win-win optimization. The success of this project will start a new research field in IR: dynamic IR modeling. The results of this research will be highly influential with great impact on the next generation search engines. The work will build a foundation for future advances in the fields of reinforcement learning in IR and game theory in IR."
16,1526301,RI: Small: Knowledge Representation and Reasoning under Uncertainty with Probabilistic Answer Set Programming,IIS,Robust Intelligence,8/1/15,8/4/15,Joohyung Lee,AZ,Arizona State University,Standard Grant,James Donlon,7/31/19,"$342,795.00 ",,joolee@asu.edu,ORSPA,TEMPE,AZ,852816011,4809655479,CSE,7495,"7495, 7923",$0.00 ,"Combining logic and probability is an important subject in Artificial Intelligence, and is recently being extensively studied in the area of statistical relational learning, where the main goal of representation is to express probabilistic models in a compact way that reflects the relational structure of the domain and ideally supports efficient learning and inference. However, in comparison with main knowledge representation languages, such languages do not allow natural, elaboration tolerant representation of commonsense knowledge. Currently, there is a big gap between the state of the art languages that are used in knowledge representation and the state of the art languages in which machine learning is done.  The success of this project will identify fundamental issues in bridging the gap between the two areas, will produce a uniform framework for both expressive representation and learning, and will contribute to the integration of knowledge representation and machine learning. The outcome of the research will be useful for many applications that require integration of knowledge representation and other areas, such as vision, robotics, and event recognition, where commonsense reasoning has to be applied on uncertain knowledge and data. The software systems developed under this project will be freely available as open source software. The research will involve both graduate and undergraduate students, contributing to a strengthened relationship between education and research. <br/><br/>The goal of the project is to design and implement a knowledge representation language that allows elaboration tolerant representation of expressive commonsense knowledge involving logic and probability, which can be efficiently computed by the techniques developed in related areas.  The proposed research aims at shifting the current logic-based foundation of answer set programming to a novel foundation that combines logic and probability, and achieving its computation by intelligently adapting and combining the methods from probabilistic reasoning and machine learning. It will build upon the existing works on answer set programming, statistical relational learning, and probabilisitic logic programming.  The project will (i) enhance the mathematical foundation of answer set programming to the novel foundation that combines logic and probability. (ii) relate it to other existing approaches in statistical relational learning, Pearl's causal models, and P-Log; (iii) design inference and learning algorithms; (iv) design a high level action language that allows elaboration tolerant representation of probabilistic transition systems; (v) apply probabilistic answer set programming to event recognition; (vi) implement and evaluate involved software systems."
17,1550179,CAREER: New Directions for Metric Learning,IIS,Robust Intelligence,1/7/15,9/22/15,Kilian Weinberger,NY,Cornell University,Continuing grant,Weng-keen Wong,2/28/18,"$211,313.00 ",,kilianweinberger@cornell.edu,373 Pine Tree Road,Ithaca,NY,148502820,6072555014,CSE,7495,"1045, 7495",$0.00 ,"Quantifying similarity is a fundamental challenge in artificial intelligence and machine learning which - if performed perfectly - would reduce many tasks to a trivial nearest neighbor search. For example, determining whether an email were spam would be as simple as searching a labeled database of emails and assigning it the same label (spam or not) as the email considered most similar to it. But how can one measure the similarity of two email messages? Does the same measurement still apply when comparing medical images? How does our understanding of similarity depend on the problem specification? Metric learning optimizes distance functions specifically for a given task, taking into account both the learning problem and the data. Initial successes with linear metrics show great improvements on many ""k-nearest neighbors""-based learning tasks. <br/><br/>This project pursues four research directions that strengthen the theoretical understanding of metric learning within the research community, broaden its impact and significantly improve the current state-of-the-art: <br/><br/>1. Are there non-linear transformations that lead to equally elegant and efficient optimization problems as existing linear metrics? As data sets grow and become increasingly complex, linear metrics are no longer sufficient to capture similarity relations. By exploring the use of non-linear metrics, this research can substantially improve the impact of metric learning and the accuracy of similarity relations. <br/><br/>2. Can the impact of metric learning be extended to machine learning frameworks beyond nearest neighbors? Designing new metric learning algorithms that explicitly optimize distances for a broad variety of machine learning algorithms will significantly increase the number of applications and learning methods that can directly benefit from metric learning. <br/><br/>3. Can metrics be learned from weak supervision? Removing the dependency on labeled data will reduce the cost of metric learning and increase its applicability. <br/><br/>4. Can one develop a solid theoretical framework to explain preliminary empirical successes and to direct future research? This will strengthen the theoretical understanding of metric learning within the research community. <br/><br/>Successful resolution of the proposed problems will lead to novel learning methods which will be immediately applicable to ongoing high-impact medical research collaborations of the principal investigator. In conjunction with these research directions, the principal investigator will also pursue educational goals, including the co-development of a K-12 curriculum module estimated to impact 2,500 high-school students. Many topics in the proposed research plan have components ideal for introducing the research process to undergraduate and graduate students, and the principal investigator plans to use his research as a vehicle to instruct and inspire future computer scientists and next-generation researchers."
18,1526431,III: Small: Collaborative Research: Adaptive Integration of Textual and Geospatial Information for Mining Massive Map Collections,IIS,Info Integration & Informatics,9/1/15,8/18/15,Erik Learned-Miller,MA,University of Massachusetts Amherst,Standard Grant,Maria Zemankova,8/31/19,"$297,859.00 ",,elm@cs.umass.edu,Research Administration Building,Hadley,MA,10359450,4135450698,CSE,7364,"7364, 7923",$0.00 ,"Libraries and archives are digitizing historical maps for widespread online access. Without technology for searching them, large map collections relevant to a given problem or question may remain obscure even in online archives. If all of the text in a map can be read automatically by computer, a wealth of information becomes quickly available -- location names, geographic features, and often statistics. This project will increase capacity for search and analysis of historical maps by automatically recognizing place names and other text in these digitized artifacts while simultaneously aligning them with modern geography. The improvements this project will make to current text recognition methods will facilitate more powerful uses of humanity's trove of old maps -- for example, by allowing scientists and policymakers to establish changes in land usage, waterways, or borders over time. By creating free, open-source tools for studying historical maps, this project will increase public engagement with science and technology and empower any Internet user to explore the intersection of technology and history. This research will train a diverse group of graduate and undergraduate students in constructing, learning, and making predictions with adaptive models featuring heterogeneous yet highly interdependent entities.<br/><br/>Although many institutions are digitizing hundreds of thousands of historical maps, most digitized map images are poorly annotated, limiting their usefulness. Manual annotation and metadata association is highly laborious. This project's primary objectives are (1) to fully automate text and shape-based georeferencing (aligning map images to the known global geography) while (2) indexing words and place names (for search) by enhancing text detection and recognition methods in these complex artifacts. These innovations will address the shortcomings of manual georeferencing and current automated text recognition algorithms. The researchers will employ an iterative interpretation process for solving problems including text/graphics separation, text recognition, and georeferencing. For example, the fact that all members of a given class of text entities on a map (e.g., county names) are typically rendered in the same text style can be used to inform predictions about difficult members of the category with information derived from more easily-recognized members. The researchers will use a dataset of annotated maps containing over 12,000 words in 9,000 place names as benchmark data for testing the algorithms developed in the project. Software, data, and benchmarks will be broadly distributed on the project website (http://www.cs.grinnell.edu/~weinman/research/maps.shtml). Findings will be shared with the research community through journals and conferences in the computer vision, artificial intelligence, and GIS communities."
19,1526350,III: Small: Collaborative Research: RUI: Adaptive Integration of Textual and Geospatial Information for Mining Massive Map Collections,IIS,"Info Integration & Informatics, EPSCoR Co-Funding",9/1/15,8/18/15,Jerod Weinman,IA,Grinnell College,Standard Grant,Maria Zemankova,12/31/19,"$202,140.00 ",,jerod@acm.org,1121 Park Street,Grinnell,IA,501121690,6412694983,CSE,"7364, 9150","7364, 7923, 9150, 9229",$0.00 ,"Libraries and archives are digitizing historical maps for widespread online access. Without technology for searching them, large map collections relevant to a given problem or question may remain obscure even in online archives. If all of the text in a map can be read automatically by computer, a wealth of information becomes quickly available -- location names, geographic features, and often statistics. This project will increase capacity for search and analysis of historical maps by automatically recognizing place names and other text in these digitized artifacts while simultaneously aligning them with modern geography. The improvements this project will make to current text recognition methods will facilitate more powerful uses of humanity's trove of old maps -- for example, by allowing scientists and policymakers to establish changes in land usage, waterways, or borders over time. By creating free, open-source tools for studying historical maps, this project will increase public engagement with science and technology and empower any Internet user to explore the intersection of technology and history. This research will train a diverse group of graduate and undergraduate students in constructing, learning, and making predictions with adaptive models featuring heterogeneous yet highly interdependent entities.<br/><br/>Although many institutions are digitizing hundreds of thousands of historical maps, most digitized map images are poorly annotated, limiting their usefulness. Manual annotation and metadata association is highly laborious. This project's primary objectives are (1) to fully automate text and shape-based georeferencing (aligning map images to the known global geography) while (2) indexing words and place names (for search) by enhancing text detection and recognition methods in these complex artifacts. These innovations will address the shortcomings of manual georeferencing and current automated text recognition algorithms. The researchers will employ an iterative interpretation process for solving problems including text/graphics separation, text recognition, and georeferencing. For example, the fact that all members of a given class of text entities on a map (e.g., county names) are typically rendered in the same text style can be used to inform predictions about difficult members of the category with information derived from more easily-recognized members. The researchers will use a dataset of annotated maps containing over 12,000 words in 9,000 place names as benchmark data for testing the algorithms developed in the project. Software, data, and benchmarks will be broadly distributed on the project website (http://www.cs.grinnell.edu/~weinman/research/maps.shtml). Findings will be shared with the research community through journals and conferences in the computer vision, artificial intelligence, and GIS communities."
20,1520031,"IBSS: The Spread and Impact of Moral Messages: Machine Learning, Network Evolution, and Behavioral Prediction",SMA,Interdiscp Behav&SocSci IBSS,8/15/15,8/15/17,Morteza Dehghani,CA,University of Southern California,Standard Grant,Antoinette WinklerPrins,1/31/19,"$640,267.00 ","Jesse Graham, Morteza Dehghani, Stephen Vaisey",mdehghan@usc.edu,University Park,Los Angeles,CA,900890001,2137407762,SBE,8213,"8213, 8605",$0.00 ,"In the immediate aftermath of the 2013 Boston Marathon tragedy, hundreds of thousands of prosocial acts were evident on social media, such as reposted links for blood donation sites, information regarding how to get in touch with loved ones, and even offers to provide food and shelter for those in need. Far from isolated acts, these behaviors occurred within social networks, amid shared moral messages of empathy and solidarity.  This interdisciplinary research project will examine how people respond to public crises and how moral reactions shape these responses in social networks.  The project will contribute new theoretical insights and methodological advances in moral psychology, network sociology, computer science, and other fields.  It will enhance understanding of how moral concerns spread through social networks and explore new theoretical frameworks dealing with human moral decision making and group dynamics.  These theoretical frameworks will guide the development of artificial intelligence techniques for building descriptive models of morality, and the new methods of sentiment analysis and machine learning will be used to assess theoretical models of moral concerns and social influence in networks.  By examining factors influencing the spread of moral messages and participation in prosocial activities, such as charitable giving, the project may help increase the well-being of individuals in emergency situations.  The project also will facilitate future inquiry into how the public and persistent nature of social media may provide new ways to understand and forecast social change.<br/><br/>The interdisciplinary science of morality has developed well-validated measures of moral concerns using a number of different approaches, such as Moral Foundations Theory and Schwartz's Values Circumplex.  Empirical research in this field usually has assessed moral judgments via questionnaires gathering information well after actions have occurred, however.  Sociology has done more to assess behavior as it occurs but has used even more limited measures.  Recent innovations in computer science offer new ways to gather information about the structure of moral judgments and large-scale behavior in natural settings as well as the relationships between the two.  The investigators will employ these new computer-based methods to examine texts from social media in order to examine the structure of moral concerns and values without relying on preset questionnaires.  They will investigate the network dynamics of the spread of moral messages and behaviors, and they will determine how moral content in social media can predict real-world behavior at both individual and societal scales. The investigators will couple machine learning and sentiment analysis techniques with theories about moral cognition and social dynamics.  Among questions they will pursue are how well everyday moral judgments (made without researcher prompting) correspond with dominant psychological theories of morality and whether it is possible to model and predict how moral influence can lead to subsequent prosocial or antisocial behavior.  This project is supported through the NSF Interdisciplinary Behavioral and Social Sciences Research (IBSS) competition."
21,1526723,RI: Small: Bayesian Modeling of Situated Communicative Goals,IIS,"Perception, Action & Cognition, Robust Intelligence",9/1/15,6/10/16,Matthew Stone,NJ,Rutgers University New Brunswick,Continuing Grant,Tatiana Korelsky,8/31/19,"$499,945.00 ",Pernille Hemmer,matthew.stone@rutgers.edu,33 Knightsbridge Road,Piscataway,NJ,88543925,8489320150,CSE,"7252, 7495","7252, 7495, 7923",$0.00 ,"This multidisciplinary project undertakes a program of research in natural language generation (NLG), the subfield of artificial intelligence that aims to construct intuitive, accessible utterances to communicate the data, knowledge and reasoning of computational systems.  NLG capabilities have an important role in facilitating new, more natural interaction with computers, both in current applications such as mobile information access and in emerging ones such as personal assistants and human-robot interaction.  NLG systems remain inflexible and difficult to build, however.  This research aims to addresses this problem by developing techniques to train NLG systems to match human language use.  The project is a close collaboration that links psychological experiments, designed to uncover the strategies human speakers use, to computational experiments, which apply these strategies in NLG systems using machine learning.<br/><br/>The theoretical framework at the center of this project is Bayesian cognitive modeling, a probabilistic approach that explains human information processing in terms of decision making under uncertainty.  Applied to language use, Bayesian cognitive modeling involves estimating the communicative goals speakers adopt, the knowledge and meanings available to speakers, and the choices speakers make to express needed information in suitable linguistic terms.  Such knowledge and strategies can then be used to drive NLG systems.  The specific research of the project investigates three key domains for applying NLG to construct messages to describe real-world situations: making lexical choices, constructing complex linguistic structures compositionally, and fulfilling multiple overlapping communicative goals.  The project explores each domain through interrelated activities carried out by an interdisciplinary team of computer scientists and psychologists: to formalize speaker choices using a range of Bayesian cognitive models; to fit the models to visually-grounded language corpora using machine learning; to evaluate the empirical scope of goal-directed reasoning by comparing the learned models both to attested human choices and to baseline learned models; and to assess how well the models match human comprehension of linguistic meaning.  The intellectual merits of the project lie in bridging the gap between traditional goal-directed rational models of human behavior and state-of-the-art computational methods that instantiate templates or reproduce likely patterns.  In addition to the societal impacts of the technology, the broader impacts of the project include the construction of data resources, models and modeling tools that will be distributed to facilitate further research, and contributions to ongoing initiatives for education in cognitive science at Rutgers."
22,1545721,"RI: Student Travel Support for the 2015 International Conference on Case-Based Reasoning; September 28-30, 2015; Frankfurt, Germany",IIS,"INFORMATION TECHNOLOGY RESEARC, ROBUST INTELLIGENCE",8/1/15,7/20/15,David Wilson,NC,University of North Carolina at Charlotte,Standard Grant,James Donlon,7/31/17,"$10,000.00 ",,davils@uncc.edu,9201 University City Boulevard,CHARLOTTE,NC,282230001,7046871888,CSE,"1640, 7495","7495, 7556",$0.00 ,"This proposal provides international and domestic travel support for U.S. based student participants to attend the 23rd International Conference on Case-Based Reasoning (ICCBR 2015), which will take place from the 28th to 30th of September 2015 in Frankfurt am Main, Germany. ICCBR is the premier, annual meeting of the Case-Based Reasoning (CBR) community and the primary international conference on this topic, bringing together researchers across artificial intelligence, machine learning, cognitive science, and domain application areas.  The CBR community welcomes interdisciplinary participation and experts from industry and other related academic areas.  ICCBR has a rich history of encouraging strong student participation in the conference program.  Today's students are tomorrow's international leaders in the research community.  As part of this, the ICCBR organizers aim to facilitate and encourage student participation by providing support for students to attend the conference.  In addition to the main technical program, the conference features opportunities for student participation and mentoring, including themed workshops, an application domain challenge competition, and the 7th in a successful doctoral consortium series.  A strong representation of U.S. based researchers, and students in particular, is useful in maintaining U.S. scientific and technical competitiveness in the field and also contributes to the career development of the students.<br/> <br/><br/>ICCBR is the premier conference in the field of Case-Based Reasoning and has contributed significantly to scientific and industry advances across the broader areas of artificial intelligence, cognitive science, and cognate disciplines since 1993. ICCBR provides a dynamic and comprehensive program for research, education, publication, and interaction; it is a leading international forum for CBR researchers, students, and practitioners to exchange and discover leading-edge ideas, results, tools, techniques, and experiences.  The intellectual merit of the proposal is to (1) expand the technical content, scope and depth of this forum by supporting and encouraging greater student attendance and participation; (2) foster technical advances in the field through the greater training and collaboration opportunities afforded by direct student participation in the conference. This project will provide partial support for students from U.S. institutions to attend and present research work at ICCBR 2015. ICCBR student travel support will encourage the research interests and involvement of students in the field, particularly those who are at early stages of their research or without current funding sources that might otherwise limit their participation. ICCBR 2015 students will have the opportunity to participate in the following interactive activities as partially supported by this project: main technical program, focus-area workshops, the computer cooking contest, face-to-face meetings with both peers and leading researchers, as well as a doctoral consortium. These opportunities will have a long lasting impact on the future career of the participants. The primary broader impact is to help train the future generation of leaders and workforce in the field; in turn, this will plant the seeds for future innovation and help to shape the field itself.<br/> <br/>"
23,1520594,SBIR Phase I: An augmented learning platform for mobile devices,IIP,SMALL BUSINESS PHASE I,7/1/15,12/10/15,Tarun Pondicherry,CA,"LightUp, Inc.",Standard Grant,Glenn H. Larsen,6/30/16,"$179,999.00 ",,founders@lightup.io,5208 Lodestar Way,Elk Grove,CA,957586724,7327051198,ENG,5371,"163E, 5371, 8031, 8032, 8039, 9180",$0.00 ,"This SBIR Phase I project will develop and test the software foundation for a novel augmented learning platform enabled by today's mobile devices. While research has shown that project-based learning is crucial for children to understand science, technology, engineering and math (STEM) topics, it currently requires knowledgeable facilitators providing guidance to be effective. This greatly limits the feasibility of project-based learning in schools, homes and informal learning environments. Augmented learning platforms hold the potential to change this, but are not widely deployed because they typically require expensive and unwieldy table top displays. The technology being developed in this project turns any mobile device into an augmented learning platform, transforming ordinary hands-on learning environments into interactive experiences by enhancing them with digital information. This project aligns with the National Science Foundation's mission to support science and engineering education at all levels by developing a novel educational platform that opens up STEM topics to all children, regardless of their prior experience or learning environment. The smart education and learning market is valued at $122 billion (2014), meaning that the commercial success of this U.S.-based company selling through U.S. schools and retailers will positively impact both domestic tax revenue and jobs.<br/><br/>The augmented learning platform that will be developed in this project will enable project-based learning at an unprecedented scale. The platform will perform three tasks. It will employ computer vision to detect electronic circuits and convert them into a graph data structure. Then, it will use artificial intelligence and simulation to analyze the resulting graph data structure. Finally, it will provide feedback to users with an augmented reality overlay within a few milliseconds. The goal of the research is to develop and test this platform with the company's current users by integrating it into an electrical engineering tutor app and evaluating their engagement and learning outcome compared to pure hardware or software systems that teach engineering concepts. The system will initially be tested with data from automated mobile analytics tools from a wide-scale deployment to current users of the electrical engineering tutor app. In the future, the underlying technology can work with any subject that produces a graph data structure for analysis. The novel platform in this project enables every mobile device to be a learning platform, giving millions access to project-based learning."
24,1456382,"SBIR Phase II:  A Question of Numbers: Numeracy, Learning, and Learning about Learning",IIP,SBIR Phase II,3/1/15,6/28/17,Brent Milne,CO,Simbulus Inc,Standard Grant,Rajesh Mehta,8/31/19,"$1,275,000.00 ",,brent.milne@simbulus.com,2017 10TH ST STE B,Boulder,CO,803025186,3034496284,ENG,5373,"115E, 165E, 5373, 7218, 8031, 8032, 9102, 9177",$0.00 ,"This Small Business Innovation Research (SBIR) Phase II project aims to answer the presidential call to ""create digital tutors that are as effective as personal tutors."" More than any other subject, mathematical learning is cumulative, and as students fall behind their classmates, new material becomes less comprehensible and they can face an ever-widening gap to their peers. Formative assessment (FA) practices have been well established as effective in closing these gaps and informing teacher decision-making. While the influx of mobile computing devices has enormous potential to help facilitate change in education, the potential is heavily dependent on the availability of proven, research-backed software and services. This project will help close achievement gaps by providing students with adaptive, personalized instruction and also providing teachers with valuable FA techniques, data, and suggestions. More broadly, the data will yield opportunities to research and model student understanding and to analyze the learning process, enabling additional research into effective practices for the teaching and learning of mathematics. With its strong customer value propositions and innovations that will enable new forms of software-enhanced teaching and learning, this project will also create significant commercial value within the educational market. <br/><br/>This project will create digital learning environments that go beyond current state-of-the-art systems to deliver adaptively selected instructional video segments and highly interactive problems, and do so while maintaining a flow of content that feels natural - as if the learning was occurring in the presence of an actual tutor. The ability of the system to adapt to the needs of an individual student is based upon a real-time assessment of student understanding and leverages cutting edge research from the fields of formative assessment, machine learning, artificial intelligence and big data. The ability to model student understanding and analyze the learning process will lead to the creation of new learning analytics tools and enable additional research into effective practices for the teaching and learning of mathematics. The proposed research will seek to demonstrate, in a randomized crossover trial, the effectiveness of the adaptive, online system over a control treatment. The researched solutions will also employ novel FA implementations such as collaborative review and white-boarding (via wireless communication), record and playback of teacher work, use of student sentiment, groupings of peers for collaborative work, and models of student understanding that incorporate teacher input (teacher plus software in the FA loop)."
25,1508880,"Recurrent Deep Learning Machines for Robust, Adaptive, or Accommodative Filtering",ECCS,EPCN-Energy-Power-Ctrl-Netwrks,7/15/15,6/29/15,James Lo,MD,University of Maryland Baltimore County,Standard Grant,Anil Pahwa,6/30/20,"$340,736.00 ",,jameslo@umbc.edu,1000 Hilltop Circle,Baltimore,MD,212500002,4104553140,ENG,7607,030E,$0.00 ,"Prediction and estimation of a process, called a signal process, given a relevant process, called a measurement process, both of which usually involve randomness, is a fundamental problem in a broad range of fields. As the signal evolves and measurements keep coming in, an algorithm is needed to predict or estimate the signal at each time instant using the measurement at the same instant to update the prediction or estimate without requiring the use of the preceding  measurements. Such an algorithm is called a filter. When the signal or measurement process is affected by an uncertain or changing environment, a filter that adapts to the environment is called an adaptive filter. In many applications, whether an uncertain or changing environment is involved, large individual errors in estimation or prediction may cause undesirable or even disastrous consequences and are to be avoided. A filter that can reduce large errors is called a robust filter. A robust filter must balance filtering accuracy and robustness. Optimal Filtering for nonlinear signal or measurement processes was a long-standing notorious problem until neural filters were proposed in 1992. Although neural filtering has many advantages over its main competitor, the particle filter, the local-minimum problem in training neural filters plagued the approach until now. The local-minimum problem has finally been overcome by a technique called the gradual deconvexification method developed under a recent NSF grant. Neural filters are now ready for application. The purpose of this project is to develop adaptive and robust neural filters.<br/><br/>In particular, the following filters will be developed:<br/><br/>(1) Accommodative neural filters. Properly trained RNNs (recurrent neural networks) with fixed weights are proven to have adaptive ability and are called accommodative neural networks. They are not adjusted online. This is an important advantage because the signal process is usually unavailable online for weight adjustment. An adaptive filter that is an accommodative neural network is called an accommodative neural filter.<br/><br/>(2) Adaptive neural filters with long- and short-term memories. If the nonlinear and linear weights of an RNN, which affect the RNN's outputs in a nonlinear and linear manner respectively, are used as long- and short-term memories (LASTMs) respectively, it has been proven that the long-term memory can be trained offline for different environments and only the short-term memory needs to be adjusted online to adapt to the environment. An adaptive neural filter that has LASTMs is called an adaptive neural filter (with LASTMs). Such filters are expected to have better generalization capability than accommodative neural filters.<br/><br/>(3) Robust neural filters. The risk-sensitivity index in the normalized risk-sensitive error (NRSE) criterion for training a neural network determines its degree of robustness. Depending on whether; being positive or negative, the NRSE averts larger ""risks"" or ignores ""outliers"" to induce robust engineering or robust statistical performance respectively. Existence of robust neural filters has been proven. It is also proven that as the risk-sensitivity index grows without bound, the NRSE approaches the minimax criterion.<br/><br/>(4) Robust accommodative neural filters. If both adaptive and robust performances are required of a filter and online adjustment of the filter is undesirable, then a robust accommodative filter can be used.<br/><br/>(5) Robust adaptive neural filters with long- and short-term memories. If both adaptive and robust performances are required of a filter and better generalization ability of the filter is desirable, then a robust adaptive filter with LASTMs can be used."
26,1515639,EAPSI:Evaluating Anticipatory Communication Strategies for Human-Robot Teaming,OISE,EAPSI,6/1/15,6/18/15,Abhizna Butchibabu,NJ,Butchibabu              Abhizna,Fellowship Award,Anne Emig,5/31/16,"$5,070.00 ",,,,Edison,NJ,88174229,,O/D,7316,"5912, 5978, 7316",$0.00 ,"Robotic systems are being integrated into complex and safety critical domains, where these systems work with humans as teammates. For example, in domains such as emergency response systems, robotic systems are deployed to conduct tasks that may not be feasible by humans. One of the challenges for fluent human-robot teaming is effective communication, where the human and the robot exchange of the right information at the right time is critical. Prior work shows that high-performing human teams tend to share information by anticipating the needs of their teammates (referred as implicit coordination) and that to work effectively with a human teammate, the robot must be interpredictable and communicate in a way expected by the human teammate. The objective of the project is to develop a computational model for the robot where the robot will proactively communicate with human teammates using implicit coordination strategies.  This is one of the first studies to address how anticipated communication strategies could be used by robots to improve team performance when working with humans and how these strategies could be learned by the robots using machine learning artificial intelligence (AI) techniques. This award will provide a U.S. graduate student with the opportunity to work in collaboration with Professor Liz Sonenberg, an expert in human-robot teaming, in the Department of Computing and Information Systems at the University of Melbourne, Australia.<br/><br/>To computationally model the robot?s AI, a Mixed Observability Markov Decision Process (MOMDP) framework will be used. This framework is commonly used as part of machine learning to learn from prior data and optimize on a reward function. The MOMDP framework for this study will use previously collected human-human team communication data. In this framework, a reward function will be implemented where using implicit coordination will provide higher reward for the robot. This will enable the robot to compute the optimal policy and solution space for communicating with the human teammate. This model will be then evaluated through human-subject experiments at MIT starting in September after the EAPSI program. <br/><br/>This NSF EAPSI award is funded in collaboration with the Australian Academy of Science."
27,1551338,EAGER: Similarity Measures Based on Refinement Operators and Metric Embedding Applied to the Analysis of Immune Repertoires,IIS,Robust Intelligence,9/1/15,8/19/15,Santiago Ontanon,PA,Drexel University,Standard Grant,Jie Yang,8/31/17,"$139,849.00 ","Ali Shokoufandeh, Uri Hershberg",santi@cs.drexel.edu,"1505 Race St, 10th Floor",Philadelphia,PA,191021119,2158955849,CSE,7495,"7495, 7916",$0.00 ,"The notion of similarity plays a key role in modern machine learning and artificial intelligence (AI) in general, since it serves as an organizing principle by which algorithms classify objects, form concepts, and make generalizations. While similarity assessment has been widely studied, the important special case of assessing similarity in domains where the data of interest is structured has not received sufficient attention. These structured representations, however, play a key role in many domains, such as biomedicine, where data of interest naturally lends itself to structured representations. The research performed in this project aims at filling the gap in structural similarity knowledge by creating a novel generalized framework for similarity assessment. To achieve the creation of this framework the PIs will focus on the specific biomedical application of immune cell populations and their dynamics during development and in response to disease. By focusing on this specific domain, the performed research will evaluate the new approach in a real-world setting, while leading to significant contributions to the understanding of immune dynamics.<br/><br/>The key concepts that will be developed in this research project are refinement operators and metric embedding. The key insight of the proposed work is that refinement operators can be used to define similarity measures, and to abstract away from the underlying representation formalism. This will lead to a new framework for similarity assessment that is applicable to a broad range of representation formalisms. Moreover, we propose to use metric embedding techniques to provide computationally efficient numerical approximations to the resulting similarity measures. The definition of general and tractable similarity measures, applicable to a range of structured representations, will be a significant contribution to structured machine learning and AI. The research team will use data collected from high throughput sequencing experiments, and evaluate the generality and performance of the proposed similarity measures by using them to analyze how repertoires of immune cell populations can be described and compared by their clonotypes (sets of cells with the same progenitor cell). The results from applying similarity measures to this problem will help us start to construct a comprehensive view of the impact of clonotype and whole repertoire information on our understanding of the dynamics of immune responses in general."
28,1528205,SHF: Small: Development of Integrated Memristive Crossbar Circuits for Pattern Classification Applications,CCF,"SOFTWARE & HARDWARE FOUNDATION, IntgStrat Undst Neurl&Cogn Sys",7/15/15,8/7/17,Dmitri Strukov,CA,University of California-Santa Barbara,Standard Grant,Sankar Basu,6/30/18,"$524,950.00 ",,strukov@ece.ucsb.edu,Office of Research,Santa Barbara,CA,931062050,8058934188,CSE,"7798, 8624","7923, 7945, 8089, 8091",$0.00 ,"Building artificial neural networks capable of matching the performance and functionality of their biological counterparts is one of the grand challenges in computing. The broad goal of this research project is to address one important aspect of this grand challenge, e.g., creating efficient hardware for implementing artificial neural networks. Artificial neural network based information processing, suitable for low precision applications, may indeed be particularly important in the present day context of energy efficient computing. If successful, this research has the potential to have broad and long lasting societal impact by improving energy efficiency and enriching functionality of existing electronics, and creating a large number of novel applications. The project will involve graduate and undergraduate students, include members of underrepresented groups and will thus help enlarge the workforce in information and communication technologies.<br/><br/>The high complexity, connectivity and parallelism of neural networks make conventional technology hardware implementations rather inefficient.  The core idea of this project is to utilize emerging memory devices, specifically memristors, which are essentially super-dense analog nonvolatile memory devices, to implement compact and energy efficient artificial neural networks. A particular experimental focus of the project is on demonstration of a hybrid memristive crossbar circuit implementation of small-scale (hundreds of neurons, thousands of synapses) multilayer perceptron performing pattern classification task. Although such a demonstration may only have a rather simple functionality, the resulting classifier would have all the key features of state-of-the-art deep learning convolutional neural network classifiers. The major focus is on the development of training algorithms compatible with memristor switching kinetics and investigation of the tradeoffs between complexity of hybrid circuits and classification performance. Theoretical modeling will guide experimental work towards most efficient implementations as well as ensure scaling of the approach to perform practical applications. Resolving hardware challenges for relatively simple neural networks would be essential for the development of more advanced neural networks capable of performing complex cognitive tasks."
29,1527668,III: Small: Sampling Techniques in Computational Logic,IIS,"Info Integration & Informatics, ",9/1/15,6/28/18,Moshe Vardi,TX,William Marsh Rice University,Standard Grant,Wei Ding,8/31/20,"$488,286.00 ",,vardi@rice.edu,6100 MAIN ST,Houston,TX,770051827,7133484820,CSE,"7364, R217","7364, 7923, 8237",$0.00 ,"Constrained sampling and counting are two fundamental problems in data analysis. In constrained sampling the task is to sample randomly from among possible solutions to a Boolean formula. A related problem is that of constrained counting, determining the number of possible solutions to a Boolean formula.  These problems have applications in machine learning, probabilistic reasoning, and planning, among other areas  In particular, te project looks at the electronic-design-automation industry to determine what practical solutions to the problems require. Both problems can be viewed as aspects of one of the most fundamental problems in artificial intelligence, which is to understand the structure of the solution space of a given set of constraints.<br/><br/>This project focuses on the development of new algorithmic techniques for constrained sampling and counting, based on a universal hashing - a classical algorithmic technique in theoretical computer science. Many of the ideas underlying the proposed approach go back to the 1980s, but they have never been reduced to practice. This project builds on recent progress in Boolean reasoning to develop methods to reduce these algorithmic ideas to practice. Methods for approximations with formal guarantees provide opportunities to scale what is fundamentally a computationally intractable problem. Pruning techniques can also reduce ""waste"" in hashed solutions, but introduce challenges in ensuring samples are independently distributed. This work has potential for breakthrough results in constrained sampling and counting, providing a new algorithmic toolbox in machine learning, probabilistic reasoning, and the like."
30,1528179,CSR: Small: Automatic Storage and Network Contention Management for Large-scale High-performance Computing Systems,CNS,CSR-Computer Systems Research,9/1/15,8/12/15,Darrell Long,CA,University of California-Santa Cruz,Standard Grant,Marilyn McClure,8/31/19,"$450,000.00 ",,darrell@cs.ucsc.edu,1156 High Street,Santa Cruz,CA,950641077,8314595278,CSE,7354,"7354, 7923",$0.00 ,"High performance computing is essential to science, industry, and the environment, from resource exploration to the design of the next generation of consumer electronics. These high performance computer systems are among the most complex and expensive computer systems and require that their resources be used in the most efficient manner. Many of the applications that utilize high performance computing are data-intensive, and storage system performance is a crucial aspect of system performance. However, storage systems are notoriously sensitive to contention caused by competition among storage clients for limited bandwidth and disk access. This is a significant problem for shared storage systems. <br/><br/>This project provides an automatic storage contention alleviation and reduction system (ASCAR) for large-scale high-performance storage to increase bandwidth utilization and fairness of resource allocation. ASCAR uses machine learning methods combined with several heuristics to discover the fittest control strategy. It is a highly scalable and fully automatic storage contention and congestion management system, which can improve the efficiency of both legacy and new systems, with no need to change either server hardware/software or existing applications. ASCAR regulates I/O traffic from the client side using a rule based algorithm. It employs a shared-nothing design and requires no runtime coordination between clients or with a central coordinator whatsoever, because runtime coordination is slow and unscalable. The effectiveness of ASCAR relies on the quality of traffic control. The research team has designed a prototype algorithm, the SHAred-nothing Rule Producer (SHARP), which produces rules in an unsupervised manner by systematically exploring the solution space of possible designs. Starting from one initial rule, SHARP uses heuristics similar to random-restart hill climbing to find the optimal parameters without the need for an exhaustive search. ASCAR monitors the workloads running on the system and uses several heuristics to pick up the fittest rules. <br/><br/>It is clear that computer systems are getting ever more sophisticated, and human-lead empirical-based approach towards system optimization is not the most efficient way to realize the full potential of these modern, complex, high performance computing systems. This research brings machine learning, artificial intelligence, and big data methods to systems research and could lead to a very low cost I/O performance increase for a wide range of systems."
31,1526059,"RI: Small: Combining Reinforcement Learning and Deep Learning Methods to Address High-Dimensional Perception, Partial Observability and Delayed Reward",IIS,Robust Intelligence,9/1/15,8/6/15,Satinder Baveja,MI,Regents of the University of Michigan - Ann Arbor,Standard Grant,Rebecca Hwa,8/31/20,"$499,886.00 ","Richard Lewis, Honglak Lee",baveja@umich.edu,3003 South State St. Room 1062,Ann Arbor,MI,481091274,7347636438,CSE,7495,"7495, 7923",$0.00 ,"Consider the problem faced by a machine agent that has to interact with some dynamical environment to achieve some goals. Concretely, imagine an agent engaged in a virtual competition as a human would. It can see the screen composed of many moving objects. At any time, it can choose one of a dozen or so actions. Its action controls one of the objects on the screen, but it often is not clear which one. Every so often the an evaluation is given of the competition. At some point the competition ends. How should such an agent choose actions, or more importantly how can we build agents that can learn to compete, i.e., achieve high scores, through trial and error. In this project methods will be developed and evaluated to build such agents.   <br/><br/>The above problem is an instance of what is called a reinforcement learning (RL) problem. Such problems abound in sequential decision-making settings. Applications in industry include factory optimization, robotics, and chronic disease management (to list but three diverse domains of interest). Like many of these RL problems, Atari games (used as a testbed here to evaluate learning strategies) have three characteristics of interest to this project. First, they generate high-dimensional images and so the agent faces a difficult perception problem. Second, they often have deeply-delayed rewards; i.e., actions have long-term consequences. For example, losing a resource may not cost at the moment of loss, but could lead to very high losses much later when that resource is critically necessary. Third, they have deep partial observability, i.e., to compete effectively one has to often remember the deep past. For example, a location encountered far back in the past may become valuable much later because a critical resource becomes available at that time and the agent would have to find its way back to that location to use the resource. It is proposed to address these three challenges respectively with new neural network architectures for predicting the consequences of actions, new methods for intrinsically motivating agents even when reward is delayed, and new recurrent neural network architectures to remember the past effectively. Success of the proposed work is expected to significantly expand the scope of application of reinforcement learning. Finally, Atari games will be used instead of, say, factory optimization as an evaluation domain because they are readily available.  They will be used to draw high-school and under-represented undergraduate students interest into complex ideas underlying the proposed work; their fun visualizations will allow them to be integrated into teaching in the PIs' classes, and there are a variety of games that vary in the degree of difficulty of the three challenge dimensions allowing more effective control of the evaluations more effectively."
32,1533771,"XPS: FULL: DSD: Collaborative Research: FPGA Cloud Platform for Deep Learning, Applications in Computer Vision",CCF,Exploiting Parallel&Scalabilty,9/1/15,8/13/15,Alexander Berg,NC,University of North Carolina at Chapel Hill,Standard Grant,Yuanyuan Yang,8/31/20,"$300,955.00 ",,aberg@cs.unc.edu,104 AIRPORT DR STE 2200,CHAPEL HILL,NC,275991350,9199663411,CSE,8283,,$0.00 ,"We stand on the verge of dramatic advances in deep learning applications, which will soon enable practicality and widespread adoption of computer vision based recognition in scientific inquiry, commercial applications, and everyday life.  Grand challenge problems are within our reach; we will soon be able to build automated systems that recognize nearly everything we see, systems that can recognize the tens of thousands of basic-level categories that psychologists posit humans can recognize, systems that continuously learn from photos, video, and web content in order to create more complete and accurate visual models of the world.  However, while it is clear that the computational capabilities for deep learning are within reach, it is equally clear that the required computational power cannot come from general-purpose processors.  To succeed, we will need to build specialized domain-specific computing systems based on hardware accelerators that are capable of exploiting the extreme fine-grained parallelism inherent in deep-learning workloads.  This project leverages parallelization and reconfigurable hardware to create an automated system that distributes computer vision algorithms onto a large number of field-programmable gate arrays (FPGA Cloud). <br/><br/>This project builds on recent advances in domain-specific hardware generation tools in order to bring the potential parallelism and performance per watt advantages of FPGAs to large-scale computer vision problems.   By developing a platform to run deep learning algorithms on large clouds of FPGAs, this proposal explicitly addresses scaling algorithms beyond what a single chip can process.  This involves addressing a wide range of challenging problems in algorithm analysis, building domain-specific hardware generators, communication for scaling algorithms across multiple FPGAs, and extensive validation of generating hardware for state-of-the-art deep learning approaches applied to computer vision problems.  This project advances tools for designing domain-specific FPGA implementations of algorithms, taking a step toward making more efficient computing with greater parallelism more widely available.  In particular, for computer vision, there will be significant benefits from a product of multiple improvements: higher parallelism, lower gate requirement by moving to fixed point when possible, and better performance per watt leading to higher computation density in servers.  Together, these have the potential to significantly increase the extent to which computer vision can be a part of our daily lives, making computers better able to understand the context of our world."
33,1533739,"XPS:FULL:DSD: Collaborative Research: FPGA Cloud Platform for Deep Learning, Applications in Computer Vision",CCF,Exploiting Parallel&Scalabilty,9/1/15,8/13/15,Michael Ferdman,NY,SUNY at Stony Brook,Standard Grant,Anindya Banerjee,8/31/20,"$574,044.00 ",Peter Milder,mferdman@cs.stonybrook.edu,WEST 5510 FRK MEL LIB,Stony Brook,NY,117940001,6316329949,CSE,8283,,$0.00 ,"We stand on the verge of dramatic advances in deep learning applications, which will soon enable practicality and widespread adoption of computer vision based recognition in scientific inquiry, commercial applications, and everyday life.  Grand challenge problems are within our reach; we will soon be able to build automated systems that recognize nearly everything we see, systems that can recognize the tens of thousands of basic-level categories that psychologists posit humans can recognize, systems that continuously learn from photos, video, and web content in order to create more complete and accurate visual models of the world.  However, while it is clear that the computational capabilities for deep learning are within reach, it is equally clear that the required computational power cannot come from general-purpose processors.  To succeed, we will need to build specialized domain-specific computing systems based on hardware accelerators that are capable of exploiting the extreme fine-grained parallelism inherent in deep-learning workloads.  This project leverages parallelization and reconfigurable hardware to create an automated system that distributes computer vision algorithms onto a large number of field-programmable gate arrays (FPGA Cloud). <br/><br/>This project builds on recent advances in domain-specific hardware generation tools in order to bring the potential parallelism and performance per watt advantages of FPGAs to large-scale computer vision problems.   By developing a platform to run deep learning algorithms on large clouds of FPGAs, this proposal explicitly addresses scaling algorithms beyond what a single chip can process.  This involves addressing a wide range of challenging problems in algorithm analysis, building domain-specific hardware generators, communication for scaling algorithms across multiple FPGAs, and extensive validation of generating hardware for state-of-the-art deep learning approaches applied to computer vision problems.  This project advances tools for designing domain-specific FPGA implementations of algorithms, taking a step toward making more efficient computing with greater parallelism more widely available.  In particular, for computer vision, there will be significant benefits from a product of multiple improvements: higher parallelism, lower gate requirement by moving to fixed point when possible, and better performance per watt leading to higher computation density in servers.  Together, these have the potential to significantly increase the extent to which computer vision can be a part of our daily lives, making computers better able to understand the context of our world."
34,1546829,EAGER: Automated Content-Based Detection of Public Online Harrassment,CNS,Secure &Trustworthy Cyberspace,7/1/15,7/9/15,Jennifer Golbeck,MD,University of Maryland College Park,Standard Grant,Ralph Wachter,12/31/16,"$150,000.00 ",,jgolbeck@umd.edu,3112 LEE BLDG 7809 Regents Drive,COLLEGE PARK,MD,207425141,3014056269,CSE,8060,"7434, 7916, 9102",$0.00 ,"Public, online harassment takes many forms, but at its core are posts that are  offensive, threatening, and intimidating.  It is not an isolated problem. The Pew Research Center found 73% of people had witnessed harassment online, and a full 40% of people had experienced harassment directly. This research develops a method for analyzing the things people post online, and automatically detecting which posts fall into the category of severe public online harassment -- messages posted simply to disrupt, offend, or threaten others. This helps websites better limit what messages are posted and reduce the amount of harassment people experience online.<br/><br/>The researchers develop a corpus of online comments from a number of media outlets and social media platforms where each post is labeled as harassing or non-harassing. Then, they apply a set of computational linguistic techniques that describe features of the message, including types of words and language structure, which is passed to rule-based and machine learning artificial intelligence systems for classification. The goal is to develop models that can automatically detect the public online harassment messages with high accuracy."
35,1525943,AF: Small: Is the Simulation of Quantum Many-Body Systems Feasible on the Cloud?,CCF,Algorithmic Foundations,8/1/15,8/21/17,Pawel Wocjan,FL,The University of Central Florida Board of Trustees,Standard Grant,Dmitri Maslov,7/31/19,"$385,434.00 ","Dan Marinescu, Eduardo Mucciolo",wocjan@cs.ucf.edu,4000 CNTRL FLORIDA BLVD,Orlando,FL,328168005,4078230387,CSE,7796,"7923, 7928, 9251",$0.00 ,"Simulating quantum mechanics with its unique effects such as superposition, interference, and entanglement is a hard problem for classical computing systems including supercomputers and computer clouds with a very large number of servers. To efficiently simulate large quantum-mechanical systems using a computer cloud one has to overcome major obstacles. This research investigates optimal algorithms for contracting tensor networks which arising in the study of condensed matter physics. These algorithms minimize the required communication between the nodes of the computer cloud and exploit its hierarchical organization. The broader research objective in this effort is to optimally exploit the architecture of hierarchically organized systems for big data applications that exhibit fine-grained parallelism.<br/>This research project aims to find new efficient methods for simulating large quantum systems that are important for quantum information processing, condensed matter physics, materials science, and chemistry. Its goals are to design and implement novel parallel and distributed simulation algorithms optimized for cloud computing environments such as Amazon Web Services and the National Science Foundation?s future cloud for scientific computing. The ultimate motivation of this project is to enable researchers world-wide to significantly push the boundary in terms of the size of quantum systems that they can simulate reliably and within a reasonable time and with a reasonable budget. While the research mainly concentrates on efficient algorithms and their implementation for the study of properties of condensed matter systems, it also attempts to derive generic strategies to other classes of applications, for instance, in artificial intelligence and in machine learning."
36,1656051,EAGER: Collaborative Research: Scaling Up Discriminative Learning for Natural Language Understanding and Translation,IIS,Robust Intelligence,8/27/15,4/30/17,Liang Huang,OR,Oregon State University,Standard Grant,Tatiana Korelsky,12/31/18,"$98,394.00 ",,huanlian@oregonstate.edu,OREGON STATE UNIVERSITY,Corvallis,OR,973318507,5417374933,CSE,7495,"7495, 7916, 9251",$0.00 ,"This EArly Grant for Exploratory Research aims to improve automatic understanding of natural language by machines, and automatic translation between languages such as Chinese and English. In the realm of understanding, the project develops methods for syntactically and semantically analyzing, or parsing, sentences. Improved parsing can help in accessing the enormous amount of information available in unstructured text on the web and in databases of newspapers and scanned books. Improved translation between languages increases opportunities for trade as well as for dissemination of information generally between nations and cultures. Machine translation is widely used today despite its generally poor quality, and any improvement in quality will improve access to information for millions of people.  This project aims to exploit the power of machine learning algorithms that are designed to discriminate between correct and incorrect outputs by numerically optimizing mathematical functions that are defined in terms of the data available for training.  Discriminative structured prediction algorithms have witnessed great success in the field of natural language processing (NLP) over the past decade, generally surpassing their generative counterparts. However, there remain two major problems which prevent discriminative methods from scaling to very large datasets: first, they typically assume exact search (over a prohibitively large search space), which is rarely possible in practice for problems such as parsing and translation. Secondly, they normally assume the data is completely annotated, whereas many naturally occurring datasets are only partially annotated: for example a parallel text in machine translation includes the source and target sentence pairs but not the derivation between them. As a result of these two problems, the current methods are not taking full advantage of the enormous and ever increasing amount of text data available to us.<br/><br/>This EArly Grant ofr Exploratory Research (EAGER) aims to: <br/>- Develop a linear-time structured learning framework specifically tailored for inexact search, which hopefully retains theoretical properties of structured learning (e.g. convergence) under exact search.  <br/>- Extend this framework to handle latent variables, such as derivations in machine translation, syntactic structures in semantic parsing, and semantic representations in question answering.  <br/>If the exploratory extension to latent variable frameworks is sucessful, it will enable longer-term research to: <br/>- Apply these efficient learning algorithms to discriminative training of machine translation systems over the entire training dataset rather than only on a small development set.  <br/>- Apply these efficient learning algorithms to discriminative training for syntactic and semantic parsing, with the goal of scaling up semantic parsing to enable web-scale knowledge extraction."
37,1558232,I-Corps: Cloud pathology platform for computer aided digitized histopathology image processing and analysis system,IIP,I-Corps,9/1/15,8/25/15,Sos Agaian,TX,University of Texas at San Antonio,Standard Grant,lydia mcclure,2/29/16,"$50,000.00 ",,sos.agaian@csi.cuny.edu,One UTSA Circle,San Antonio,TX,782491644,2104584340,ENG,8023,,$0.00 ,"Owing to the potential negative side effects of prostate cancer treatment, there is a real need to accurately diagnose and not over treat prostate cancer. The development of a system capable of quickly and accurately processing digitized prostate biopsy slides to detect, classify and score prostate cancer will result in fewer unnecessary surgeries with potentially harmful side effects and more efficient use of healthcare dollars. This I-Corps team believes that pathologists using such a system will be more productive and better able to handle the increasing workload that an aging population will produce. The system will also provide critical access to a ""digital"" pathologist for developing countries where access to pathologists is severely limited.<br/> <br/>This I-Corps team will conduct customer discovery and more fully analyze the commercial potential for a cloud pathology platform for computer aided diagnostic and decision support system for prostate cancer. The team will demonstrate a system where a user will input a digitized prostate biopsy slide and the system will analyze the slide, identify cancerous tissue regions, classify and grade the prostate tumors and output a Gleason score. The proposed innovation leverages advances in computer vision, image processing and neural networks. The system utilizes a fuzzy color standardization method as a preprocessing step that allows the system to analyze images from different sources. A novel algorithm automatically segments prostate tissue structures based on color decomposition and extracts morphological and architectural features from prostate needle biopsy images and automatically classifies the prostate cancer biopsy images based on the Gleason grading system. Finally, a new high-speed learning algorithm for the quaternion neural network, substantially reducing network training time, while providing equivalent performance to quaternion backpropagation."
38,1548567,EAGER: Robots that Learn to Communicate with Humans Tthrough Natural Dialog,IIS,Robust Intelligence,9/1/15,7/13/15,Raymond Mooney,TX,University of Texas at Austin,Standard Grant,Tatiana Korelsky,8/31/17,"$150,000.00 ",Peter Stone,mooney@cs.utexas.edu,"3925 W Braker Lane, Ste 3.340",Austin,TX,787595316,5124716424,CSE,7495,"7495, 7916",$0.00 ,"This EArly Grant for Exploratory Research explores the possibility of developing more user-friendly and capable robots that learn to understand commands in natural human language. The experimental system developed aims to engage users in natural conversation, clarifying linguistic instructions that cannot be understood, and learning from this interaction to more robustly interpret future commands. This fundamentally new approach is hypothesized to overcome limitations of more-costly previous approaches that require either direct programming or detailed annotation of per-assembled linguistic data, and still frequently fail to cover issues that arise in real user interactions. The resulting exploratory prototype is evaluated on real interactions with human users, experimentally testing its ability to improve its accuracy and flexibility at interpreting human instructions over time, through normal everyday use. This novel approach aims to improve human interaction with intelligent multi-robot systems that aid the residents and visitors of a large, multi-use building. This fundamental research also supports computer-science education in the growing areas of natural-language processing, human-robot interaction, and machine learning, where there is significant national demand for knowledgeable personnel.<br/><br/>The technical approach explored is a novel integration of learning techniques from three currently disparate areas: semantic parsing, spoken dialog management, and perceptual language grounding. Semantic parsing is the task of mapping natural language to a formal computer-interpretable language using compositional semantics based on syntactic linguistic structure. Dialog management concerns controlling multi-turn natural language interaction to aid comprehension and task completion. Perceptual grounding concerns associating words and phrases in language to objects, properties and relations in the world as perceived by the robot's sensors. Although there has been recent significant progress in each of these individual areas, no one has previously explored integrating them to support learning for human-robot communication through natural dialog. This exploratory research adapts and integrates techniques for semantic-parser learning using combinatory categorial grammar, dialog management using Partially Observable Markov Decision Processes, and multi-modal language grounding using both visual and haptic sensors, in order to develop a novel dialog system for communicating with robots that comprise the innovative Building Wide Intelligence system being developed at the University of Texas at Austin. The exploratory methods are evaluated using controlled experiments on a range of tasks using both on-line simulations and crowdsourced users, and natural user interaction with a mobile robot platform consisting of a wheeled Segway base and a Kinova robot arm."
39,1525902,SHF: Small: Deep Learning Software Repositories,CCF,Software & Hardware Foundation,9/1/15,7/20/15,Denys Poshyvanyk,VA,College of William and Mary,Standard Grant,Sol Greenspan,8/31/19,"$400,000.00 ",,dposhyvanyk@wm.edu,Office of Sponsored Programs,Williamsburg,VA,231878795,7572213966,CSE,7798,"7923, 7944",$0.00 ,"Improvements in both computational power and the amount of memory in modern computer architectures, have enabled new approaches to canonical machine learning tasks. Specifically, these architectural advances have enabled machines, which are capable of learning deep compositional representations of massive data repositories. The rise of deep learning has ushered tremendous advances in several fields, and, given the complexity of software repositories, our hypothesis is that deep learning has the potential to usher new analytical frameworks and methodologies for Software Engineering research as well practice.<br/><br/>The research program addresses three main goals by applying deep learning where conventional machine learning has been used before. First is the design of new models based on deep architectures for Software Engineering tasks. The project will develop deep software language models for sequence analysis tasks and deep information retrieval models for document analysis tasks. Second, the project will apply the internal representations to practical problems in Software Engineering by instantiating deep learning to support tasks such as code suggestion, improving software lexicons, model-based testing, code search and clone detection. Third, the project will conduct empirical evaluations designed to demonstrate ways of modeling software artifacts that will inform entirely novel suites of learned features that can be used from task to task. The move from traditional machine learning to deep learning will improve results in many software analysis tasks and in empirical Software Engineering research."
40,1527371,AF:  Small:  Linear Algebra++ and applications to machine learning,CCF,Algorithmic Foundations,6/15/15,5/25/16,Sanjeev Arora,NJ,Princeton University,Standard Grant,Tracy Kimbrel,5/31/19,"$466,000.00 ",,arora@cs.princeton.edu,Off. of Research & Proj. Admin.,Princeton,NJ,85442020,6092583090,CSE,7796,"7923, 7926, 9251",$0.00 ,"Many areas of unsupervised learning (i.e, learning with data that has not been labeled by humans) currently rely on heuristic algorithms that lack provable guarantees on solution quality or running time. In fact the underlying problems - as currently formulated - are often known to be computationally intractable (NP-hard, to use a technical term). This proposal identifies a big set of these problems that can be seen as twists - involving constraints such as nonnegativity and sparsity - on classical linear algebra problems like solving linear systems, rank computation, and eigenvalues/eigenvectors. The PI has proposed calling this set of problems collectively as Linear Algebra++. The project will seek to develop  algorithms with provable guarantees for these problems. The methodology will be to make suitable assumptions about the structure of realistic inputs.  The algorithms will be applied to problems in unsupervised learning, in areas such as topic modeling, natural language processing, semantic embeddings, sparse principle components analysis (PCA), deep nets, etc.<br/><br/>The work will lead to new and more efficient algorithms for big data processing that will come with provable guarantees of quality. It will bring new rigorous approaches to machine learning. (Some recent work of the PI shows that the new rigorous approaches can be quite practical.) It will advance the state of art in theoretical computer science by expanding its range and its standard toolkit of algorithms. It will contribute fundamentally new primitives to classical linear algebra and applied mathematics.<br/><br/>The project will train a new breed of graduate students who will be fluent both in theoretical algorithms and machine learning. The PI has a track record in this kind of work and training during the past few years and will continue this including working with undergrads and Research Experiences for Undergraduates (REU) students.  Any new algorithms discovered as part of this project will be released as open source code. The PI also plans a series of other outreach activities in the next few years including (a) A workshop. (b) A special semester or year at the Simons Institute in 2016-17 on provable bounds in machine learning which he will coorganize. (c) A new book on graduate algorithms based upon his new grad course, which tries to re-orient algorithms training for today's computer science problems. (d) A series of talks aimed at broad audiences, of which he gives several each year.<br/><br/>The techniques will build upon recent progress by the PI and others on problems such as nonnegative matrix factorization, sparse coding, alternating minimization etc. They involve average case analysis, classical linear algebra, convex optimization, numerical analysis, etc., as well involve completely new ideas. They could have a transformative effect on machine learning and algorithms."
41,1526473,CHS: Small: Human-Directed Optical Music Recognition,IIS,HCC-Human-Centered Computing,9/1/15,5/11/17,Christopher Raphael,IN,Indiana University,Standard Grant,Ephraim Glinert,8/31/19,"$504,197.00 ",Erik Stolterman,craphael@indiana.edu,509 E 3RD ST,Bloomington,IN,474013654,3172783473,CSE,7367,"7367, 7923, 9251",$0.00 ,"Vast quantities of character-encoded text form the foundation for the information retrieval revolution of recent decades.  In contrast, very little symbolically-represented music exists, preventing music from fully participating in the 21st century.  The International Music Score Library Project (IMSLP) is a large and rapidly growing open library of public domain machine-printed classical music scores, actively used by many musicians, scholars, and researchers around the world.  Optical music recognition (OMR) forms the natural bridge between the IMSLP and the missing symbolic music data.  While there has been active OMR research since the 1960s, the state of the art still is not sufficiently well-developed to create symbolic data from realistic documents, as represented on the IMSLP.  This is because music notation contains a thicket of special cases, exceptions to general rules, image pathologies, and interpretation challenges, whose recognition requires a deep level of content understanding.  With this in mind, the PI has developed prototype software named Ceres for supporting a hybrid human-computer team, in which both machine and person partner in a collaborative recognition effort.  The human guides the computer through the recognition task, identifying and providing crucial missing pieces of information, while allowing the computer to fill in the details, consistent with the human guidance.  The ultimate goal is to build a Wikipedia-like community centered around Ceres and the IMSLP with the mission of creating a definitive, open access, symbolic music library that distributes music scores electronically and globally, allowing for adaptive display and automatic transformation and registration of scores with audio and video.  The prevalence of symbolic music data would open up a world of possibilities to music-science researchers, including systems for music information retrieval, expressive performance, musical accompaniment, transcription and arranging, performance assistance, and many others.  Last but not least, the symbolic music library would enable innovative commercial applications; tablet computers will likely be the sheet music ""delivery system"" of the future, allowing automatic page turning, performance feedback, and various kinds of content-based annotation. <br/><br/>The challenge of integrating both human and algorithmic intelligence to create a tractable and efficient OMR solution constitutes the heart of this project.  The PI's approach is to adopt the interface paradigm of constrained optimization; the human uses domain understanding to supply crucial missing pieces of information when needed, and the computer uses this guidance to re-recognize and reinterpret subject to these user-supplied constraints.  A preliminary experiment conducted by the PI using a medium-difficulty test set showed a 17% error rate on the part of his prototype system, accounting for both false positives and false negatives at the primitive level.  The human-computer interface is where the recognition results become tangible and subject to manipulation, so its design is critical; this is an area where the PI expects to make contributions to HCI in general.  The PI argues that to be useful for OMR the interface should be almost completely open, providing a set of tools and options, and imposing only the minimal required structure (e.g., staff recognition must be verified before we identify page structure, while the latter must be verified before it is worth continuing to the symbol recognition phase).  The interface development strategy will be one of iterative refinement, the evaluation of each version to involve time- and effort-oriented metrics as well as open-ended user comments.  For example, the Ceres user interface (the null hypothesis for the current project) superimposes the recognized results on the original image, making discrepancies readily apparent (in contrast to other systems that present side-by-side original and recognized notation which is cognitively more difficult to compare), but maybe even better solutions are possible?  Other aspects of the work will include exploration of the roles of visualization (including directing the user's attention) and music playback (hearing the score).  OMR is just one of many computer vision problems that fall into the constrained optimization category, and the approach also applies to natural language processing, machine listening, and others; the essential process to be explored here would extend to these domains as well, providing a general template with far-reaching significance.  For OMR the constraints are individual pixel labels, but for other problem domains they could equally well refer to labelling of individual samples, words, or whatever fundamental units compose the data.  In this way, the approach uses a generic and flexible view for human input that doesn't require the human to understand the inner workings of the recognition processes."
42,1535086,SI2-SSE: Human- and Machine-Intelligent Software Elements for Cost-Effective Scientific Data Digitization,OAC,"ADVANCES IN BIO INFORMATICS, Software Institutes",8/1/15,4/22/16,Andrea Matsunaga,FL,University of Florida,Standard Grant,Bogdan Mihaila,7/31/20,"$488,048.00 ",Mauricio Tsugawa,ammatsun@ufl.edu,1 UNIVERSITY OF FLORIDA,GAINESVILLE,FL,326112002,3523923516,CSE,"1165, 8004","7433, 8005",$0.00 ,"In the era of data-intensive scientific discovery, Big Data scientists in all communities spend the majority of their time and effort collecting, integrating, curating, transforming, and assessing quality before actually performing discovery analysis. Some endeavors may even start from information not being available and accessible in digital form, and when it is available, it is often in non-structured form, not compatible with analytics tools that require structured and uniformly-formatted data. Two main methods to deal with the volume and variety of data as well as to accelerate the rate of digitization have been to apply crowdsourcing or machine-learning solutions. However, very little has been done to simultaneously take advantage of both types of solutions, and to make it easier for different efforts to share and reuse developed software elements. The vision of the Human- and Machine-Intelligent Network (HuMaIN) project is to accelerate scientific data digitization through fundamental advances in the integration and mutual cooperation between human and machine processing in order to handle practical hurdles and bottlenecks present in scientific data digitization. Even though HuMaIN concentrates on digitization tasks faced by the biodiversity community, the software elements being developed are generic in nature, and expected to be applicable to other scientific domains (e.g., exploring the surface of the moon for craters require the same type of crowdsourcing tool as finding words in text, and the same questions of whether machine-learning tools could provide similar results can be tested).<br/><br/>The HuMaIN project proposes to conduct research and develop the following software elements: (a) configurable Machine-Learning  applications for scientific data digitization (e.g., Optical Character Recognition and Natural Language Processing), which will be made automatically available as RESTful services for increasing the ability of HuMaIN software elements to interoperate with other elements while decreasing the software development time via a new application specification language; (b) workflows leading to a cyber-human coordination system that will take advantage of feedback loops (e.g., based on consensus of crowdsourced data and its quality) for self-adaptation to changes  and increased sustainability of the overall system, (c) new crowdsourcing micro-tasks with ability of being reusable for a variety of scenarios and containing user activity sensors for studying time-effective user interfaces, and (d) services to support automated creation and configuration of crowdsourcing workflows on demand to fit the needs of individual groups. A cloud-based system will be deployed to provide the necessary execution environment with traceability of service executions involved in cyber-human workflows, and cost-effectiveness analysis of all the software elements developed in this project will provide assessment and evaluation of long standing what-if scenarios pertaining human- and machine-intelligent tasks. Crowdsourcing activities will attract a wide range of users with tasks that require low expertise, and at the same time it will expose volunteers to applied science and engineering, potentially attracting interest of K-12 teachers and students."
43,1547055,BIGDATA: EAGER: Deep Learning in Higher Education Big Data to Explore Latent Student Archetypes and Knowledge Profiles,IIS,ECR-EHR Core Research,9/1/15,9/1/15,Zachary Pardos,CA,University of California-Berkeley,Standard Grant,John Cherniavsky,8/31/19,"$289,888.00 ",,pardos@berkeley.edu,Sponsored Projects Office,BERKELEY,CA,947101749,5106433891,CSE,7980,"7433, 7916, 8083",$0.00 ,"BIGDATA: Deep Learning in Higher Education Big Data to Explore Latent Student Archetypes and Knowledge Profiles<br/><br/>Data science techniques have revolutionized many academic fields and led to terrific gains in the commercial sector. They have to date been underutilized in solving critical problems in the US educational system, particularly in understanding Science, Technology, Engineering and Mathematics (STEM) learning and learning environments, broadening participation in STEM, and increasing retention for students traditionally underserved in STEM. The goals of the Directorate for Education and Human Resources (EHR), through the EHR Core Research program, for the Critical Techniques and Technologies for Advancing Foundations and Applications of Big Data Science & Engineering (BIGDATA) program are to advance fundamental research aimed at understanding and solving these critical problems, and to catalyze the use of data science in Education Research. Deep learning is a relatively novel method in machine learning (ML) that has shown great improvements over prior ML approaches in successful classification with very little a priori definitions applied to the data. For example, in image classification, these techniques have a high rate of success at distinguishing species and particular animals. This exploratory proposal will investigate the possibilities of using this approach with educational data from Massive Open Online Courses (MOOCs) and Learning Management Systems (LMSs). Research has shown that it is often difficult to use traditional study designs with MOOC data as enrollment is open and many different types of people enroll in MOOCs. This factor has made it difficult to make these MOOCs adaptive to individual learners, an optimal approach to improving learning. The Principal Investigators will group people in MOOCs who are similar to each other to understand the different types of people taking the MOOCs so that learning activities can be tailored to them.<br/><br/>The Principal Investigators will examine a variety of data, from the micro-level of backend data on timing to complete an exercise, or pauses between activities, to macro-level data such as course history and grades. They will use deep learning techniques to identify groups of learners with similar characteristics, the first step in making a learning environment more adaptive. The proposed research is ambitious and risky. Deep learning techniques show great promise and the Principal Investigators demonstrate that they could be extremely helpful in solving key educational challenges. The Principal Investigators have the technical and learning science expertise to carry out this ambitious endeavor. As more and larger educational datasets are developed, the field needs to expand its methodologies to learn from them. This proposal is unique in its potential for catalyzing this effort.<br/><br/>This award is supported by the EHR Core Research (ECR) program.  The ECR program emphasizes fundamental STEM education research that generates foundational knowledge in the field.  Investments are made in critical areas that are essential, broad and enduring:  STEM learning and STEM learning environments, broadening participation in STEM, and STEM workforce development."
44,1549932,EAGER: Cloud-based analysis of mass spectrometry proteomics data,DBI,Cross-BIO Activities,9/1/15,8/21/15,William Noble,WA,University of Washington,Standard Grant,Peter McCartney,8/31/17,"$300,000.00 ","Jeffrey Bilmes, Michael MacCoss",noble@gs.washington.edu,4333 Brooklyn Ave NE,Seattle,WA,981950001,2065434043,BIO,7275,7916,$0.00 ,"Proteins carry out a vast range of critical activities including signaling, DNA replication, gene regulation, immune response, etc. Tandem mass spectrometry is the only high-throughput way to characterize these proteins in complex biological mixtures. This project will produce algorithms with the potential to dramatically improve the ability of proteomics scientists around the world to interpret their tandem mass spectrometry data, thereby providing critical insights into these fundamental biological processes. Because proteins are the primary functional molecules in the cell, such insights are foundational to our scientific understanding of life.<br/><br/>The key intellectual contributions in this proposal lie in the development of novel machine learning methods for proteomics. Specifically, the project will develop novel methods for spectrum feature extraction, including deep neural network autoencoding and low-dimensional representation learning, learning deep similarity metrics that allow efficient spatial data access methods, applications of autoencoding for spectrum and experiment compression, submodular summarization methods on proteomic data, and classification and ranking methodologies that take advantage of network effects via a data embedding. These tools will then be used for proteomic metadata inference, cloud-based peptide and protein identification, and retrieval and ranking of similar proteomic experiments. The research will hence yield a profoundly more powerful, data-driven approach to jointly interpreting massive mass spectrometry data sets, thereby giving scientists valuable new tools to glean insights into protein function."
45,1456186,SBIR Phase II:  Linguistic Analysis of Web Content for 21st Century Inquiry Learning,IIP,SBIR Phase II,3/1/15,8/26/17,Eleni Miltsakaki,PA,Choosito!,Standard Grant,Rajesh Mehta,2/28/19,"$742,971.00 ",,eleni@choosito.com,462 Ballytore Rd,Wynnewood,PA,190962309,6106421816,ENG,5373,"165E, 5373, 8031, 8039, 8240, 9177",$0.00 ,"This Small Business Innovation Research Phase II project engages K-12 students in inquiry-based learning not only in STEM education but across the curriculum.  The coming of the information age has mandated radical changes in the skills set required for 21st century. To teach these skills, as articulated in the Common Core Standards, teachers are expected to target students? ability to conduct short as well as more sustained research projects. The Internet is not only the obvious go-to place for teaching these skills but one of the reasons for mandating them. It is the easy access to vast amounts of information that has made it possible for everyone to conduct research, analyze data and build new knowledge. The problem, however, is that the Internet is not a library with reliable and catalogued information that can be searched by topic and, in school libraries, by reading level. The development of the proposed technology, a personalized librarian on-demand, solves this problem. It offers unprecedented opportunity for learners of all ages to engage in inquiry, it enables differentiated learning and gives learners a life-long tool to help them understand and critically engage with digital information. The target market is not limited to the U.S. The developed linguistic algorithms apply to non-English, allowing early targeting of the international market by building web curators for other languages. <br/><br/><br/>This project blends research on linguistics, psychology of reading, natural language processing (NLP)  and machine learning. Innovative NLP technology is used to detect age-appropriate sites that are relevant to the curriculum. The developed technology analyzes the reading difficulty and thematic content of websites automatically in real time search. The purpose of the system is to make adaptive individualized recommendations that match the student's reading level and are appropriate for her current familiarity with the topic. The patent pending adaptive component is built using machine learning and collaborative filtering methods. The resulting research tool, a personalized web curator, is based on novel methods for modeling the student's familiarity with the topic of web resources. These methods are extended to include novel analysis of video content. Unlike other adaptive educational products, the system's adaptive recommendations do not rely on obtaining test scores from the user and corresponding them to  pre-labeled, leveled data. The complete solution includes an integrated gamified experience that helps the students acquire basic and advanced research skills and develop the habit of applying them consistently. The project team includes educational technology experts from academia and schools and conducts extensive studies evaluating learning benefits by the afforded differentiation in instruction, including special education programs and English Language Learners."
46,1462502,EAGER-DynamicData: Principled and Scalable Probabilistic Frameworks for Dynamic Multi-modal Data,ECCS,"Big Data Science &Engineering, ",9/1/15,9/4/15,Lawrence Carin,NC,Duke University,Standard Grant,akbar sayeed,8/31/17,"$100,000.00 ",Piyush Rai,lcarin@ee.duke.edu,"2200 W. Main St, Suite 710",Durham,NC,277054010,9196843030,ENG,"8083, O395","153E, 5384, 7916",$0.00 ,"Emergence of the Big Data phenomenon has given rise to data collections that are massive, highly heterogeneous and multi-modal, dynamically evolving, as well as incomplete, noisy and imprecise. These characteristics are becoming increasingly prevalent in data from a diverse range of domains, such as robotics, cognitive neuroscience, sensor generated data (e.g., in geoscience and remote sensing), and the dynamically evolving data on the web. The heterogeneity, complexity, dynamic evolution, and the often real-time processing requirements, call for methods that are both statistically rigorous as well as computationally scalable. Moreover, performing fast feature-extraction and/or predictions at *test time* is another key requirement, especially in problems involving dynamic data arriving at high speeds. This project will innovate on scalable statistical methods for learning from such massive dynamic multi-modal data, with a focus on designing novel probabilistic models for multi-layer latent feature extraction for such data. These multi-layer latent feature representations  of the data will help capture the underlying dynamics and allow reconciling the data heterogeneity arising due to diverse data types and widely differing spatial and temporal resolutions across the different modalities, while also being useful for a wide range of fundamental data analysis tasks, such as classification, clustering, and predicting missing data. At the same time, the focus will also be on developing methods that are efficient at test time, so that fast feature extraction and predictions can be made in real time, to make these methods readily applicable to dynamic streaming data.<br/><br/>This EArly Grant for Exploratory Research (EAGER) project endeavors to move beyond existing ad hoc approaches currently used for these problems, and develop a probabilistically grounded, statistically rigorous, and computationally scalable framework, based on Bayesian and nonparametric Bayesian modeling. Taking a Bayesian generative modeling approach will naturally enable modeling the dynamic behavior of the data and seamlessly integrate diverse types of data, while handling issues such as missingness, noise and the imprecise nature of the data. In addition, the nonparametric Bayesian treatment will provide the much-needed modeling flexibility and address many of the limitations of the existing Deep Learning models, e.g., by doing away with the need of extensive hand-tuning, incorporating rich prior knowledge about the model parameters, and allowing a natural sharing of statistical strength across the multiple data modalities. To handle the associated computational challenges, the framework will provide novel inference machinery in form of online Bayesian inference methods that will naturally handle dynamic, real-time data, and parallel and distributed Bayesian inference methods to handle massive multi-modal data that are too large for the capacity (storage and/or computational) of a single computing node. Furthermore, due to its inherent ability of quantifying model uncertainty, the proposed Bayesian framework will naturally facilitate a dynamic integration between model computation (inference) and data acquisition, and help design informed data acquisition (i.e., ""active"" sensing) methods in the context of dynamic multi-modal data. An overarching goal of this project is to also help synergize two important research directions in machine learning - nonparametric Bayesian methods and Deep Learning methods. By designing scalable nonparametric Bayesian solutions to the type of problems Deep Learning methods have been applied for, the project will convince the skeptics of Deep Learning methods to adopt these methods more openly. At the same time, the compelling range of problems and applications Deep Learning are being used for, will broaden the appeal of nonparametric Bayesian methods from a practical sense. We expect this synergy between these two areas will significantly advance the state-of-the-art in both areas."
47,1528045,SHF: Small: Collaborative Research: Resilient Computing Systems Using Deep Learning Techniques,CCF,Software & Hardware Foundation,8/1/15,8/4/15,Vijay Janapa Reddi,TX,University of Texas at Austin,Standard Grant,Yuanyuan Yang,7/31/18,"$265,000.00 ",,vj@eecs.harvard.edu,"3925 W Braker Lane, Ste 3.340",Austin,TX,787595316,5124716424,CSE,7798,"7923, 7941",$0.00 ,"Over the past decade, computer systems have become prone to a variety of hardware failures. Traditionally, hardware failures were circumvented by operating the system at less than peak computing efficiency, effectively compromising efficiency to achieve reliability. Such a conservative approach is no longer a viable option because it leads to significant energy inefficiency. Since datacenters containing thousands of computers are one of the largest and fastest growing consumers of electricity, it is important to decouple the relationship between hardware failures and energy efficiency. <br/><br/>The PIs' research will lay the groundwork for an intelligent computing system that operates at peak efficiency, but manages its fault resiliency and reliability using machine-learning based deep learning techniques. In effect, the system learns to steer itself clear of danger whenever its deep neural nets anticipate a failure. The research will address several important issues involving the scalability, flexibility and efficiency of deep learning techniques for various types of hardware failures. If successful, the research product will minimize, if not eliminate, penalties to the system that stem from the various circuit and micro-architectural techniques that are commonly used to mitigate and overcome hardware failures."
48,1518865,CSR: CHS: Large: Wearable Cognitive Assistance,CNS,"Special Projects - CNS, CSR-Computer Systems Research",8/1/15,4/22/19,Mahadev Satyanarayanan,PA,Carnegie-Mellon University,Continuing Grant,Marilyn McClure,7/31/21,"$2,847,787.00 ","Roberta Klatzky, Martial Hebert, Daniel Siewiorek",satya@cs.cmu.edu,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,CSE,"1714, 7354","1714, 7354, 7925, 9102, 9251",$0.00 ,"This research explores the deep technical challenges of a new class of computing systems that integrate a wearable device (such as Google Glass) with cloud-based processing to guide a user step by step through a complex task. Although easy to describe, many challenges in computer systems, computer vision and human-computer interaction must be overcome for this concept to become reality. Human cognition is a remarkable feat of real-time processing. It involves the synthesis of outputs from real-time analytics on multiple sensor stream inputs. An assistive system amplifies human cognition with compute-intensive processing that is so responsive that it fits into the inner loop of the human cognitive workflow. In its most general form, cognitive assistance is a very broad and ambitious concept that could be applied to virtually all facets of everyday life.   As a pioneering effort, this research is more narrowly focused on user assistance for well-defined tasks that require specialized knowledge and/or skills, and for which task state and task-relevant actions are fully accessible to computer vision algorithms.   <br/><br/>The research is organized into four broad thrusts. The first thrust decouples and cleanly separates low-level mobile computing and cloud computing issues such as resource management, network latency, placement, provisioning, scalability, and load balancing from the task-centric foci of the other tasks. The second thrust focuses on the computer vision research necessary to address the challenges of wearable cognitive assistance. Vision is the dominant sensing modality for the kinds of tasks addressed in this research, but the validation experiments will include proof-of-concept use of other sensing modalities such as audio and location. The third thrust focuses on task description, tracking, sequencing and user guidance. Its goal is to create a set of generalizable principles and tools that can be applied to a wide range of tasks. Matching task assistance to task demands and user capabilities will be integral to this thrust. The fourth thrust involves continuous integration of research from the first three thrusts and applies it towards end-to-end validation on a series of tasks of increasing sophistication and difficulty. This thrust involves close collaboration with industry partners. <br/><br/>This research will advance computer science by producing scientific insights, algorithms, system designs, implementation techniques, and experimental validations at the intersection of computer systems (including mobile computing, cloud computing, virtual machines, operating systems, wireless networking, and sensor networks), vision technologies (including computer vision and machine learning), and human-computer interaction.  More broadly, society will benefit from wearable cognitive application in areas such as health care training, industrial troubleshooting and consumer product assembly. From an educational viewpoint, this research offers many unique opportunities to train graduate and undergraduate students on how to approach problems from a broad cross-disciplinary viewpoint."
49,1526399,SHF: Small: Collaborative Research: Resilient Computing Systems Using Deep Learning Techniques,CCF,Software & Hardware Foundation,8/1/15,8/4/15,Sek Chai,CA,SRI International,Standard Grant,Yuanyuan Yang,7/31/18,"$234,959.00 ",,sek.chai@sri.com,333 RAVENSWOOD AVE,Menlo Park,CA,940253493,7032478529,CSE,7798,"7923, 7941",$0.00 ,"Over the past decade, computer systems have become prone to a variety of hardware failures. Traditionally, hardware failures were circumvented by operating the system at less than peak computing efficiency, effectively compromising efficiency to achieve reliability. Such a conservative approach is no longer a viable option because it leads to significant energy inefficiency. Since datacenters containing thousands of computers are one of the largest and fastest growing consumers of electricity, it is important to decouple the relationship between hardware failures and energy efficiency. <br/><br/>The PIs' research will lay the groundwork for an intelligent computing system that operates at peak efficiency, but manages its fault resiliency and reliability using machine-learning based deep learning techniques. In effect, the system learns to steer itself clear of danger whenever its deep neural nets anticipate a failure. The research will address several important issues involving the scalability, flexibility and efficiency of deep learning techniques for various types of hardware failures. If successful, the research product will minimize, if not eliminate, penalties to the system that stem from the various circuit and micro-architectural techniques that are commonly used to mitigate and overcome hardware failures."
50,1524817,RI: Small: Advancing Visual Recognition with Feature Visualizations,IIS,Robust Intelligence,9/1/15,7/6/16,Antonio Torralba,MA,Massachusetts Institute of Technology,Continuing Grant,Kenneth Whang,8/31/19,"$460,000.00 ",,torralba@csail.mit.edu,77 MASSACHUSETTS AVE,Cambridge,MA,21394301,6172531000,CSE,7495,"7495, 7923",$0.00 ,"The goal of this work is to develop a set of tools to visualize the information extracted by computer vision systems so that it is easier for researchers, and users, to understand their behavior. With the success of new computational architectures for visual processing, such as deep neural networks with many processing layers (e.g., convolutional neural networks) and access to large databases with millions of annotated images (e.g., ImageNet, Places), the state of the art in computer vision is advancing rapidly, and becoming integrated into many commercial products. But these advances come with the price that systems are becoming more complex, and it becomes harder for researchers and users to diagnose and understand the representations built by these systems. The goal of this work is to develop new techniques for visualizing what the algorithms are doing in order to elucidate their behavior. <br/><br/>The work will focus on developing algorithms for generic feature inversion. Most features perform complex non-linear operations over the image and it is not always possible to obtain analytic expressions to invert those computations. The goal of the proposal is to introduce new techniques that will allow inverting descriptors without constraining the descriptors. The second challenge will consists in understanding the inversion properties in order to allow comparison among different descriptions. If the inversion contains approximations, comparisons among descriptors might not be possible. Therefore it will be important to understand the convergence properties of the inversion algorithms. Another issue arises from the compressive nature of most descriptors. In general, some part of the input image information will be lost when encoded by an image descriptor. Therefore, the inversion will have to be a one-to-many function. Understanding the space of equivalent images under a particular descriptor will provide insights about what will be the likely errors made by a recognition system using them. This proposal will perform a variety of experiments with the feature visualizations, such as examining invariances in both engineered features and learned features from deep learning, visualizing learned models and decision boundaries, and diagnosing false alarms and missed detections."
51,1527294,RI: Small: Learning to Eliminate Heuristics in Stereo Vision,IIS,Robust Intelligence,9/1/15,5/16/16,Philippos Mordohai,NJ,Stevens Institute of Technology,Continuing Grant,Jie Yang,8/31/20,"$432,031.00 ",,Philippos.Mordohai@stevens.edu,CASTLE POINT ON HUDSON,HOBOKEN,NJ,70305991,2012168762,CSE,7495,"7495, 7923",$0.00 ,"This project develops technologies to improve stereo and multi-view stereo algorithms by removing heuristics and hand-tuning using machine learning techniques. Stereo matching is the process of estimating depth of points, or 3D coordinates in a scene, and is enabled by the estimation of correspondences between pixels or other primitives in two or more images. Even the most successful current stereo matching algorithms, however, use a large number of heuristics. The developed methods from this project eliminate the heuristics from binocular and multi-view stereo matching and deliver algorithms with higher accuracy, interpretability of the results and higher portability to different settings. Stereo vision plays an important role in many applications, such as 3D modeling, augmented reality, driver assistance, autonomous navigation and human computer interaction. The educational and outreach aspects of the project focus on involving K-12 and undergraduate students in STEM education and research. <br/><br/>This research addresses stereo vision by training classifiers that learn from pairs, or larger sets of images, with ground truth depth to make more accurate predictions about unobserved data than those obtained by hand-crafted rules. The approach is comprehensive and tackles all stages of the binocular stereo matching process, including the matching cost function, cost aggregation, optimization and refinement. Representations for multi-view stereo based on surface patches, depth maps or occupancy grids and the corresponding algorithms are also supported by the same framework. Random forest classifiers are well suited for use in inhomogeneous feature spaces and classifier calibration can ensure that their outputs are close to the true posterior probabilities of the classes under consideration. The resulting algorithms and findings can be transferred to other computer vision problems that require pixel correspondences, such as optical flow estimation, image stitching and template matching."
52,1554264,EAGER: Large-Scale Distributed Learning of Noisy Labels for Images and Video,IIS,Robust Intelligence,9/1/15,3/28/17,Zichun Zhong,MI,Wayne State University,Standard Grant,Jie Yang,8/31/18,"$233,995.00 ",,zichunzhong@wayne.edu,5057 Woodward,Detroit,MI,482023622,3135772424,CSE,7495,"7495, 7916",$0.00 ,"This project develops algorithms for learning from images and video with noisy labels. The overwhelming amounts of images and video freely available online present unprecedented challenges for machine learning and computer vision research communities. They also bring tremendous opportunities and great potentials for addressing human-machine semantic gaps in image understanding and for revolutionizing our ways to index, retrieve, and interact with images and video. Inaccurate labels and mislabeled data are common problems for image and video datasets. Noisy labels would cause problems with the existing learning algorithms. This project can have broad impacts on other big data problem. The project is integrated with education by training students, ensuring broad participation of underrepresented groups, and outreaching general public.<br/><br/>This research exploring distributed learning methods for large-scale images and video with noisy labels. The PI investigates the learning problem of loss functions with both smooth and non-smooth regularization terms, and accordingly develops new distributed learning algorithms that are capable of leveraging the abundance of images that are too large to fit into a single machine. The research has an immense potential in image and video analysis, and computer vision applications. Specifically, this research emphasizes both algorithmic and theoretic aspects by (1) developing distributed learning based approaches for optimization and learning of noisy labels; and (2) investigating issues such as guaranteed convergence, convergence rate, and scalability. This work provides new methods that are widely applicable to many economically, medically and scientifically important large-scale datasets for novel discoveries across many domains."
53,1559558,CAREER: Rich and Scalable Optimization for Modern Bayesian Nonparametric Learning,IIS,Robust Intelligence,7/1/15,2/11/19,Brian Kulis,MA,Trustees of Boston University,Continuing Grant,Rebecca Hwa,6/30/20,"$486,255.00 ",,bkulis@bu.edu,881 COMMONWEALTH AVE,BOSTON,MA,22151300,6173534365,CSE,7495,"1045, 7495",$0.00 ,"Large-scale data analysis has become an indispensable tool throughout academia and industry.  When the amount of data is very large, one often faces a tradeoff between the richness, flexibility, and potential predictive power of the models, and the computational requirements.  While recent advances in statistics and machine learning provide us with a rich set of models and tools, many of these cannot be applied in contemporary applications due to the sheer volume of data available for analysis.  In particular, Bayesian nonparametric models are a rich class of models which have largely been restricted to small-scale setting.  This class of methods, in contrast to standard parametric statistical models, does not fix the complexity of the model, instead allowing the data to determine how complex the resulting models are.  While these models appear well-suited for large-scale data analysis, current methods for inference in Bayesian nonparametric models have not been shown to work at scale. In terms of broader impacts, applications of machine learning continue to emerge in fields such as medicine, engineering, and the humanities; this research has the potential to impact a number of problems in these fields.  Further, the PI's research has applications in the field of computer vision, which itself has emerging impacts in autonomous driving and eldercare.  The project also includes an effort to further integrate coursework in computer science with coursework in statistics, aiming to continue to bridge the gap between the fields.  Finally, the research will introduce undergraduate and high school students to research, and will also yield new software for nonparametric problems that can be applied by practitioners outside the machine learning field.<br/><br/>This CAREER project explores scalable optimization methods aimed at making Bayesian nonparametric models applicable in large-scale settings.  The research in this project is focused on three general themes:<br/><br/>1) Small-variance asymptotics for scalable nonparametric modeling.  This part of the project aims to develop new and scalable algorithms for nonparametric problems that can be applied to problems such as topic modeling, image segmentation, and image feature learning.  The goals include extending the technique of small-variance asymptotics to new Bayesian nonparametric models, improving the theoretical underpinnings of the asymptotic techniques, and developing large-scale software for several problems.<br/><br/>2) New variational inference techniques for nonparametric problems.  This part of the project focuses on extending variational inference methods to new settings.  In particular, the PI focuses on applying variational inference to gamma process models, and will develop variational inference methods for the emerging class of exponential-family completely random measures.<br/><br/>3) New applications for large-scale Bayesian nonparametrics.  This part of the project uses the results in the previous two parts to explore applications of Bayesian nonparametric models that were previously unattainable.  Applications include large-scale image modeling, social network analysis, and large-scale document analysis."
54,1513721,TWC SBE: Medium: Context-Aware Harassment Detection on Social Media,CNS,"Special Projects - CNS, Secure &Trustworthy Cyberspace",9/1/15,2/23/17,Amit Sheth,OH,Wright State University,Standard Grant,Sara Kiesler,6/30/20,"$957,104.00 ","Krishnaprasad Thirunarayan, Valerie Shalin",amit@sc.edu,3640 Colonel Glenn Highway,Dayton,OH,454350001,9377752425,CSE,"1714, 8060","025Z, 7434, 7924, 9178, 9251",$0.00 ,"As social media permeates our daily life, there has been a sharp rise in the use of social media to humiliate, bully, and threaten others, which has come with harmful consequences such as emotional distress, depression, and suicide. The October 2014 Pew Research survey shows that 73% of adult Internet users have observed online harassment and 40% have experienced it. The prevalence and serious consequences of online harassment present both social and technological challenges. This project identifies harassing messages in social media, through a combination of text analysis and the use of other clues in the social media (e.g., indications of power relationships between sender and receiver of a potentially harassing message.) The project will develop prototypes to detect harassing messages in Twitter; the proposed techniques can be adapted to other platforms, such as Facebook, online forums, and blogs. An interdisciplinary team of computer scientists, social scientists, urban and public affairs professionals, educators, and the participation of college and high schools students in the research will ensure wide impact of scientific research on the support for safe social interactions.<br/><br/>This project combines social science theory and human judgment of potential harassment examples from social media, in both school and workplace contexts, to operationalize the detection of harassing messages and offenders. It develops comprehensive and reliable context-aware techniques (using machine learning, text mining, natural language processing, and social network analysis) to glean information about the people involved and their interconnected network of relationships, and to determine and evaluate potential harassment and harassers. The key innovations of this work include: (1) identification of the generic language of insult, characterized by profanities and other general patterns of verbal abuse, and recognition of target-dependent offensive language involving sensitive topics that are personal to a specific individual or social circle; (2) prediction of harassment-specific emotion evoked in a recipient after reading messages by leveraging conversation history as well as sender's emotions; (3) recognition of a sender's malicious intent behind messages based on the aspects of power, truth (approximated by trust), and familiarity; (4) a harmfulness assessment of harassing messages by fusing aforementioned language, emotion, and intent factors; and (5) detection of harassers from their aggregated behaviors, such as harassment frequency, duration, and coverage measures, for effective prevention and intervention."
55,1452903,CAREER: Rich and Scalable Optimization for Modern Bayesian Nonparametric Learning,IIS,ROBUST INTELLIGENCE,2/1/15,2/2/15,Brian Kulis,OH,Ohio State University,Continuing grant,Todd Leen,10/31/15,"$94,320.00 ",,bkulis@bu.edu,Office of Sponsored Programs,Columbus,OH,432101016,6146888735,CSE,7495,"1045, 7495",$0.00 ,"Large-scale data analysis has become an indispensable tool throughout academia and industry.  When the amount of data is very large, one often faces a tradeoff between the richness, flexibility, and potential predictive power of the models, and the computational requirements.  While recent advances in statistics and machine learning provide us with a rich set of models and tools, many of these cannot be applied in contemporary applications due to the sheer volume of data available for analysis.  In particular, Bayesian nonparametric models are a rich class of models which have largely been restricted to small-scale setting.  This class of methods, in contrast to standard parametric statistical models, does not fix the complexity of the model, instead allowing the data to determine how complex the resulting models are.  While these models appear well-suited for large-scale data analysis, current methods for inference in Bayesian nonparametric models have not been shown to work at scale. In terms of broader impacts, applications of machine learning continue to emerge in fields such as medicine, engineering, and the humanities; this research has the potential to impact a number of problems in these fields.  Further, the PI's research has applications in the field of computer vision, which itself has emerging impacts in autonomous driving and eldercare.  The project also includes an effort to further integrate coursework in computer science with coursework in statistics, aiming to continue to bridge the gap between the fields.  Finally, the research will introduce undergraduate and high school students to research, and will also yield new software for nonparametric problems that can be applied by practitioners outside the machine learning field.<br/><br/>This CAREER project explores scalable optimization methods aimed at making Bayesian nonparametric models applicable in large-scale settings.  The research in this project is focused on three general themes:<br/><br/>1) Small-variance asymptotics for scalable nonparametric modeling.  This part of the project aims to develop new and scalable algorithms for nonparametric problems that can be applied to problems such as topic modeling, image segmentation, and image feature learning.  The goals include extending the technique of small-variance asymptotics to new Bayesian nonparametric models, improving the theoretical underpinnings of the asymptotic techniques, and developing large-scale software for several problems.<br/><br/>2) New variational inference techniques for nonparametric problems.  This part of the project focuses on extending variational inference methods to new settings.  In particular, the PI focuses on applying variational inference to gamma process models, and will develop variational inference methods for the emerging class of exponential-family completely random measures.<br/><br/>3) New applications for large-scale Bayesian nonparametrics.  This part of the project uses the results in the previous two parts to explore applications of Bayesian nonparametric models that were previously unattainable.  Applications include large-scale image modeling, social network analysis, and large-scale document analysis."
56,1446601,CPS: Synergy: TTP Option: Anytime Visual Scene Understanding for Heterogeneous and Distributed Cyber-Physical Systems,CNS,"Information Technology Researc, Special Projects - CNS",1/1/15,10/23/18,Srinivasa Narasimhan,PA,Carnegie-Mellon University,Standard Grant,David Corman,12/31/19,"$1,413,793.00 ","Martial Hebert, James Hoe, Christoph Mertz, James Bagnell",srinivas@cs.cmu.edu,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,CSE,"1640, 1714","8235, 9251",$0.00 ,"Despite many advances in vehicle automation, much remains to be done: the best autonomous vehicle today still lags behind human drivers, and connected vehicle (V2V) and infrastructure (V2I) standards are only just emerging. In order for such cyber-physical systems to fully realize their potential, they must be capable of exploiting one of the richest and most complex abilities of humans, which we take for granted: seeing and understanding the visual world.  If automated vehicles had this ability, they could drive more intelligently, and share information about road and environment conditions, events, and anomalies to improve situational awareness and safety for other automated vehicles as well as human drivers.  That is the goal of this project, to achieve a synergy between computer vision, machine learning and cyber-physical systems that leads to a safer, cheaper and smarter transportation sector, and which has potential applications to other sectors including agriculture, food quality control and environment monitoring.<br/><br/>To achieve this goal, this project brings together expertise in  computer vision, sensing, embedded computing, machine learning, big data analytics and sensor networks to develop an  integrated edge-cloud architecture for (1) ""anytime scene understanding""  to unify diverse scene understanding methods in computer vision, and (2)  ""cooperative scene understanding"" that leverages vehicle-to-vehicle and vehicle-to-infrastructure protocols to  coordinate with multiple systems, while (3) emphasizing how security and  privacy should be managed at scale without impacting overall  quality-of-service. This architecture can be used for autonomous driving and driver-assist systems, and can be embedded within infrastructure (digital signs, traffic lights) to avoid traffic congestion, reduce risk of pile-ups and improve situational awareness. Validation and transition of the research to practice are through integration within City of Pittsburgh public works department vehicles, Carnegie Mellon University NAVLAB autonomous vehicles, and across the smart road infrastructure corridor under development in Pittsburgh.  The project also includes activities to foster development of a new cyber-physical systems workforce, though involvement of students in the research, co-taught multi-disciplinary courses, and co-organized workshops."
57,1456154,SBIR Phase II:  Representation and Deep Learning for Free Text Applications,IIP,SBIR Phase II,3/1/15,3/4/19,Stephen Gallant,MA,"Textician, LLC",Standard Grant,Peter Atherton,5/31/19,"$943,539.00 ",,sgallant@mmres.com,51 Fenno St,Cambridge,MA,21386737,6176424959,ENG,5373,"165E, 169E, 5373, 8032, 8039",$0.00 ,"The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project derives from an enhanced capability for automated processing of free text and other structured data.  Motivation for this approach comes from neural networks and, in turn, it has applications to neural modeling and our understanding of how the brain processes information.  In the software industry, commercial innovation continues to revolve around automated processing of web pages, which plays a key role in creating many new companies.  Therefore, the ability to automate decision-making from free text is increasing in importance. A better way to represent text for use with machine learning will open new capabilities wherever the structure of sentences must be taken into account.  This has the potential to lead to new startup ventures, thereby resulting in new products and services. A successful project will result in platform technology that can provide a substantial competitive edge to companies that take advantage of it, provide new and better capabilities for consumers, and help advance the nation's lead in technological innovation.<br/><br/>This Small Business Innovation Research (SBIR) Phase II project seeks to further develop new ways to process textual material so that computers can better learn applications related to natural language.  Applications include sentiment analysis (assigning either positive or negative views to a body of text), summarization of documents, and classification of documents using multiple labels from a fixed set of many classes.  The project will further develop new techniques to improve performance, and prototype components for transforming text and applying computerized learning methods. The new techniques represent words simultaneously with document structure using a single high-dimensional vector (for example, a list of 1,000 numbers).  The project is aimed at improving computational capabilities involving documents, web pages, and other text as well as providing new techniques that can be applied to automated translation, better computer understanding of images, and genomic information."
58,1536022,"The reach of the visible hand: government acknowledgments in US patents and technological change, 1975-2015",SMA,SciSIP-Sci of Sci Innov Policy,9/1/15,9/11/15,Lee Fleming,CA,University of California-Berkeley,Standard Grant,Mark Fiegener,8/31/17,"$493,628.00 ",Hillary Greene,lfleming@ieor.berkeley.edu,Sponsored Projects Office,BERKELEY,CA,947101749,5106433891,SBE,7626,7626,$0.00 ,"It has long been received wisdom that investment in research greatly facilitates the technological progress that ultimately improves economic productivity and living standards.  Unfortunately, the systematic and quantitative evidence to support these arguments remains thin and expensive to produce.  This project generates and makes public detailed observations of all patents that acknowledge government support.  More specifically, it calculates the proportion of patents issued through 2015 that have relied, directly or indirectly, upon government investment.  This research provides a sharper picture of how government sponsored research impacts technological change.  <br/><br/>Using advanced data techniques, mainly natural language processing and machine learning, this project identifies every US patent since 1976 that contains an acknowledgment of government support.  A real-time visualization tool will enable the past and future citation networks of any U.S. patent since 1976.  Taken together, this work measures and illustrates the extent to which today?s patented technology, both domestic and foreign, can be traced (whether directly or indirectly) back to US taxpayer investment. This project investigates the total number of government supported patents since 1976 and their characteristics to discern the impact of government support on subsequent invention, relative to similar private sector technology.  Augmented with characteristics of inventors the project will enable examination of the career paths of government supported inventors and estimate US government support going to foreign inventors.  The project measures the impact of that support on technological trajectories and the knowledge diffusion of ? and the foreign brain drain from ? US government investment.  It illustrates these results in a dynamic rendering of the backward and forward citation network for any user-specified patent.  These easily communicated and illustrated findings facilitate more informed discussion and debate regarding science and technology research funding.<br/>"
59,1452898,"CAREER: Creation, Visualization, and Mining of Domain Textual Graphs: Integrating Domain Knowledge and Human Intelligence",IIS,EPSCoR Co-Funding,2/1/15,2/3/15,Wei Jin,ND,North Dakota State University Fargo,Continuing grant,James French,4/30/17,"$249,216.00 ",,wei.jin@unt.edu,Dept 4000 - PO Box 6050,FARGO,ND,581086050,7012318045,CSE,9150,"1045, 7364, 9150",$0.00 ,"It is understood that textual information is growing at an astounding pace, creating an enormous challenge for analysts trying to discover valuable information that is buried within. For example, new non-trivial trends, patterns, and associations among entities of interest, such as associations between genes, proteins and diseases, and the connections between different places or the commonalities of people, are such forms of underlying knowledge. The goal of this research is to explore automated solutions for sifting through these extensive document collections to detect interesting links and hidden information that connect facts, propositions or hypotheses. In addition, a more comprehensive view of discovered knowledge will be provided by generating an in-depth and concise cross-document summary explaining the underlying meaning of each connection, along with relevant links and explanations acquired from the Wikipedia knowledge base, which serves as the primary means of complementing or enhancing existing information in text collections. The project will impact many areas, such as homeland security, aviation safety, biomedical and healthcare applications. The techniques will have the potential to expose new information available in large document collections and to provide a multi-view perspective of discovered hypotheses by integrating domain knowledge and relevant information acquired from Wikipedia. Research-based education and training opportunities will be offered by this project to prepare students at all levels in information analysis and discovery. Specific attention will also be paid to promoting the participation of underrepresented groups in the research efforts.<br/><br/>This project focuses on the exploration of a novel textual knowledge representation, integration, and mining framework that will cover the following areas: (i) automatic construction of graphical frameworks for entity relationship discovery, a new representation conducive to fine-grained information search and discovery; (ii) effective integration of information from multiple sources, including knowledge contained in representative data collections, domain-specific knowledge (e.g., domain ontologies), and world knowledge (e.g., lexical resources such as WordNet and large-scale knowledge repositories such as Wikipedia); (iii) new discovery algorithms and tools that identify hidden connections among entities; (iv) enhancement of domain modeling through enabling automatic ontology-driven scenario detection and topic-level modeling; and (v) interactive visualization tools for the graphical framework and discovered hypotheses. This research proposes that next-generation search tools require the capability of integrating information from multiple interrelated units and combining various evidence sources, which will make fundamental advances in the current state of the art for information search and discovery. A combination of techniques in Natural Language Processing (NLP), Information Extraction (IE), Information Retrieval (IR), Data Mining, Machine Learning, and Semantic Web will be explored to attack critical information discovery problems. For further information see the project web site: http://www.cs.ndsu.nodak.edu/~wjin/WSD-RelMiner."
60,1523162,EXP: Partners in Learning: Building Rapport with a Virtual Peer Tutor,IIS,Cyberlearn & Future Learn Tech,9/1/15,4/13/18,Justine Cassell,PA,Carnegie-Mellon University,Standard Grant,Amy Baylor,8/31/19,"$588,617.00 ","Louis-Philippe Morency, Amy Ogan",justine@cs.cmu.edu,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,CSE,8020,"8045, 8841, 9251",$0.00 ,"The Cyberlearning and Future Learning Technologies Program funds efforts that will help envision the next generation of learning technologies and advance what we know about how people learn in technology-rich environments. Cyberlearning Exploration (EXP) Projects explore the viability of new kinds of learning technologies by designing and building new kinds of learning technologies and studying their possibilities for fostering learning and challenges to using them effectively. This project seeks to understand, and capitalize on, how teachers or tutors build rapport with learners by building technologies that support rapport. Research will study what rapport with learners looks like, when students deploy rapport techniques, when and how deploying rapport techniques (whether by people or automated agents) increases learning, and how rapport evolves over time. The project will build software that can help measure rapport between learners and with computers. <br/><br/>The project will begin by building a multimodal sensing rapport-detection system , based on recent advances in computer vision, signal processing, and machine learning which will automatically recognize audio and visual behaviors during learner interaction with an intelligent tutoring system. Human-human tutoring interactions will be used to guide development of the rapport detection system. Both short term and longitudinal analyses will be conducted using students working with an AI-based math tutor, focused on their visual behaviors (head gaze estimation will be used to measure facial action units and gestures like head nods or shakes, and mutual gaze between humans), verbal behaviors (using CoreNLP and other software to detect verbal utterances that represent rapport-related social constructs such as politeness, friendship, etc.), and entrainment behaviors (synchrony or asynchrony, divergence and convergence). The project will then design RAPT, the Rapport-Aligned Peer Tutor, which encompasses both the rapport detection system and an intelligent pedagogical agent that accounts for the persistent social states of rapport and non-rapport. Mockups/simulations of the interface will be used to test the designs before the full pedagogical agent is built. Trials will be conducted in 9-11 grade classrooms working with an intelligent geometry tutor using a two-iteration design-based research study."
61,1514118,RI: Medium: Collaborative Research: Learning to Summarize User-Generated Video,IIS,Robust Intelligence,9/1/15,9/1/17,Kristen Grauman,TX,University of Texas at Austin,Continuing Grant,Jie Yang,8/31/21,"$547,005.00 ",,grauman@cs.utexas.edu,"3925 W Braker Lane, Ste 3.340",Austin,TX,787595316,5124716424,CSE,7495,"7495, 7924",$0.00 ,"Today there is far more video being captured - by consumers, scientists, defense analysts, and others - than can ever be watched.  With this explosion of video data comes a pressing need to develop automatic video summarization algorithms.  Video summarization takes a long video as input and produces a short video as output, while preserving its information content as much as possible.   As such, summarization techniques have great potential to make large video collections substantially more efficient to browse, search, disseminate, and facilitate communication.  Such increased efficiency will play a vital role in many important application areas.  For example, with reliable summarization systems, a primatologist gathering long videos of her animal subjects could quickly browse a week's worth of their activity before deciding where to inspect the data most closely.  A young student searching YouTube to learn about Yellowstone National Park could see at a glance what content exists, much better than today's simple thumbnail images can depict.  An intelligence agent could rapidly sift through reams of aerial video, reducing the resources required to analyze surveillance data to identify suspicious activities.<br/><br/>This project develops new machine learning and computer vision algorithms for video summarization.  Unsupervised methods, which are the cornerstone of nearly all existing approaches, have become increasingly limiting due to their reliance on hand-crafted heuristics.  By instead posing video summarization as a supervised learning problem, this project investigates a markedly different formulation of the task.  The research team is investigating four key new ideas: powerful probabilistic models for learning to select the optimal subset of video frames for summarization, semi-supervised learning models and co-summarization algorithms for leveraging the abundance of multiple related videos, algorithms for exploiting photos on the Web to improve summarization, and evaluation protocols that assess summaries in a way that aligns well with human comprehension.  The broader impact of the proposed research includes practical tools for video summarization, scientific advances that appeal broadly to several communities, publicly disseminated research results, inter-disciplinarily trained graduate students, and outreach activities to engage young students in STEM education and career paths."
62,1513966,RI: Medium: Collaborative Research: Learning to Summarize User-Generated Video,IIS,ROBUST INTELLIGENCE,9/1/15,6/9/15,Fei Sha,CA,University of Southern California,Continuing grant,Jie Yang,3/31/16,"$112,248.00 ",,feisha@usc.edu,University Park,Los Angeles,CA,900890001,2137407762,CSE,7495,"7495, 7924",$0.00 ,"Today there is far more video being captured - by consumers, scientists, defense analysts, and others - than can ever be watched.  With this explosion of video data comes a pressing need to develop automatic video summarization algorithms.  Video summarization takes a long video as input and produces a short video as output, while preserving its information content as much as possible.   As such, summarization techniques have great potential to make large video collections substantially more efficient to browse, search, disseminate, and facilitate communication.  Such increased efficiency will play a vital role in many important application areas.  For example, with reliable summarization systems, a primatologist gathering long videos of her animal subjects could quickly browse a week's worth of their activity before deciding where to inspect the data most closely.  A young student searching YouTube to learn about Yellowstone National Park could see at a glance what content exists, much better than today's simple thumbnail images can depict.  An intelligence agent could rapidly sift through reams of aerial video, reducing the resources required to analyze surveillance data to identify suspicious activities.<br/><br/>This project develops new machine learning and computer vision algorithms for video summarization.  Unsupervised methods, which are the cornerstone of nearly all existing approaches, have become increasingly limiting due to their reliance on hand-crafted heuristics.  By instead posing video summarization as a supervised learning problem, this project investigates a markedly different formulation of the task.  The research team is investigating four key new ideas: powerful probabilistic models for learning to select the optimal subset of video frames for summarization, semi-supervised learning models and co-summarization algorithms for leveraging the abundance of multiple related videos, algorithms for exploiting photos on the Web to improve summarization, and evaluation protocols that assess summaries in a way that aligns well with human comprehension.  The broader impact of the proposed research includes practical tools for video summarization, scientific advances that appeal broadly to several communities, publicly disseminated research results, inter-disciplinarily trained graduate students, and outreach activities to engage young students in STEM education and career paths."
63,1552651,CAREER: Common Links in Algorithms and Complexity,CCF,Algorithmic Foundations,12/15/15,12/17/15,Ryan Williams,CA,Stanford University,Continuing grant,Tracy Kimbrel,5/31/17,"$209,701.00 ",,rrw@mit.edu,450 Jane Stanford Way,Stanford,CA,943052004,6507232300,CSE,7796,"1045, 7926, 7927",$0.00 ,"The field of algorithm design builds clever programs that can quickly solve computational problems of interest. The field of complexity theory mathematically proves ""lower bounds,"" showing that no such clever program exists for (other) core problems. Intuitively, it appears that these two fields work on polar-opposite tasks. The major goal of this project is to discover counter-intuitive new connections between algorithm design and complexity theory, and to study the scientific consequences of the bridges built by these connections. It is hard to overestimate the potential impact---societal, scientific, and otherwise---of a theoretical framework which would lead to a fine-grained understanding of what computers can and cannot do. This project is focused on exploring concrete steps towards a better understanding, via studying links between the seemingly opposite tasks of algorithms and lower bounds. Another goal of the project is to bring complexity research closer to real-world computing, and to introduce practitioners to aspects of complexity that will impact their work. A final goal is educational outreach, through online forums dedicated to learning computer science, teaching summer school courses, and collaboration with the media on communicating theoretical computer science (including links between algorithms and lower bounds) to the public.<br/><br/>The PI seeks common links between algorithms and complexity: counter-intuitive similarities and bridges which will lead to greater insight into both areas. A central question in computer science is the famous P versus NP open problem, which is about the difficulty of combinatorial problems which admit short solutions. Such problems can always be solved via ?brute force?, trying all possible solutions. Can brute force always be replaced with a cleverer search method? This question is a major one; no satisfactory answers are known, and concrete answers seem far away. The conventional wisdom is that in general, brute force cannot be entirely avoided, but it is still mathematically possible that most natural search problems can be solved extremely rapidly, without any brute force. <br/><br/>Computational lower bounds are among the great scientific mysteries of our time: there are many conjectures and beliefs about them, but concrete results are few. Moreover, the theory is hampered by ?complexity barriers? which show that most known proof methods are incapable of proving strong lower bounds. The PI's long-term objective is to help discover and develop new ways of thinking that will demystify lower bounds, and elucidate the limits of possibilities of computing. The PI hypothesizes that an algorithmic perspective on lower bounds is the key: for example, earlier work of the PI shows that algorithms for the circuit satisfiability problem (which slightly beat brute force search) imply circuit complexity lower bounds. The PI has developed several new links within the past few years, and has proposed many more to be investigated. Among the various angles explored in this project, the potential scientific applications are vast, ranging from logical circuit design, to network algorithms, to improved hardware and software testing, to better nearest-neighbor search (with its own applications in computer vision, DNA sequencing, and machine learning), and to cryptography and security."
64,1551113,EAGER: Collaborative Research: Models of Child Speech,IIS,ROBUST INTELLIGENCE,9/1/15,3/24/16,Abeer Alwan,CA,University of California-Los Angeles,Standard Grant,Tatiana D. Korelsky,2/28/18,"$148,000.00 ",,alwan@ee.ucla.edu,10889 Wilshire Boulevard,LOS ANGELES,CA,900951406,3107940102,CSE,7495,"7495, 7916, 9251",$0.00 ,"In contrast to the production, modeling, and machine recognition of adult speech, which have been studied for decades, the production, acoustic modeling and recognition of child speech have not received the same level of attention. The lack of scholarly resources for dealing with children's speech is problematic as new applications for child speech and language development become increasingly important and commonplace. This is especially true for elementary school children. As children grow, their articulators grow as well, resulting in variations in their speech sounds. For example, the waveform of the word 'sunny' spoken by a 6-year old can be quite different than that of the same child when she is 9 years old. This is why current machine recognition of children's speech does not perform well for young children and does not scale up as the child grows. That is, these systems tend to be age dependent. Understanding and modeling child speech as children grow is important not only to developing better recognition systems but also for better understanding and diagnosis of speech-language pathology (SLP). As the negative long-term ramifications of deficits in early childhood development gain increasingly broad recognition, the opportunities and the need for social and health services and technological applications targeted toward young children are growing. In particular, it is now understood that early deficits in language development and literacy persist into adulthood, and the demand for SLP services in public schools is significantly outpacing supply. As a result, it is no longer feasible for clinicians and teachers to provide the most effective treatments or the necessary attention to every child. Better speech recognition systems would provide an opportunity for improved diagnosis and more intense computer-based therapy.  This Early Grant for Exploratory Research project aims to model how speech and language develop during elementary school and how children with speech disorders differ in their articulation of speech sounds.  Models of child speech will lead to the development of computer programs which can be used for educational as well as therapeutic purposes.<br/><br/>Scientifically, the exploratory project will 1) reveal processes of speech production development in 20-26 elementary school-aged children through a unique combination of articulatory and acoustic analyses, and 2) develop acoustic models and eventually automatic speech recognition systems for children's speech which can be scalable with age (as opposed to being age-dependent systems). This can only be achieved by understanding how the articulation and corresponding acoustics develop with age. The different aspects of the project are therefore synergistic: findings from articulation and acoustic experiments will inform the development of algorithms essential to automatic speech recognition.  Production data will include real-time 3D ultrasound recordings of the tongue, video recordings of the lips, palate impressions, microphone recordings, and accelerometer recordings of neck skin vibrations which have been shown to be beneficial in automatic speech and speaker recognition applications. The causal relationship between articulatory and acoustic variability will be explored, as will their relationship to misrecognition of child speech. Articulatory features will be incorporated into new automatic speech recognition systems along with acoustic features. The exploratory project will contribute to knowledge of variability between children, as well as variability over time as children grow and will provide, for the first time, normative data and scientific models. These can lead to robust child speech recognition systems as well as tools that will be useful for a variety of applications such as educational games, training of speech-language pathologists, automatic or semi-automatic transcription systems, and speech articulation visualization systems. It will train undergraduate and graduate students in important cross-disciplinary activities of technological and scientific significance. We believe that the proposed project is transformative in its advancement of the scientific and technological state of the art related to child speech."
65,1551131,EAGER: Collaborative Research: Models of Child Speech,IIS,Robust Intelligence,9/1/15,7/23/15,Steven Lulich,IN,Indiana University,Standard Grant,Tatiana Korelsky,2/28/18,"$160,000.00 ",,slulich@indiana.edu,509 E 3RD ST,Bloomington,IN,474013654,3172783473,CSE,7495,"7495, 7916",$0.00 ,"In contrast to the production, modeling, and machine recognition of adult speech, which have been studied for decades, the production, acoustic modeling and recognition of child speech have not received the same level of attention. The lack of scholarly resources for dealing with children's speech is problematic as new applications for child speech and language development become increasingly important and commonplace. This is especially true for elementary school children. As children grow, their articulators grow as well, resulting in variations in their speech sounds. For example, the waveform of the word 'sunny' spoken by a 6-year old can be quite different than that of the same child when she is 9 years old. This is why current machine recognition of children's speech does not perform well for young children and does not scale up as the child grows. That is, these systems tend to be age dependent. Understanding and modeling child speech as children grow is important not only to developing better recognition systems but also for better understanding and diagnosis of speech-language pathology (SLP). As the negative long-term ramifications of deficits in early childhood development gain increasingly broad recognition, the opportunities and the need for social and health services and technological applications targeted toward young children are growing. In particular, it is now understood that early deficits in language development and literacy persist into adulthood, and the demand for SLP services in public schools is significantly outpacing supply. As a result, it is no longer feasible for clinicians and teachers to provide the most effective treatments or the necessary attention to every child. Better speech recognition systems would provide an opportunity for improved diagnosis and more intense computer-based therapy.  This Early Grant for Exploratory Research project aims to model how speech and language develop during elementary school and how children with speech disorders differ in their articulation of speech sounds.  Models of child speech will lead to the development of computer programs which can be used for educational as well as therapeutic purposes.<br/><br/>Scientifically, the exploratory project will 1) reveal processes of speech production development in 20-26 elementary school-aged children through a unique combination of articulatory and acoustic analyses, and 2) develop acoustic models and eventually automatic speech recognition systems for children's speech which can be scalable with age (as opposed to being age-dependent systems). This can only be achieved by understanding how the articulation and corresponding acoustics develop with age. The different aspects of the project are therefore synergistic: findings from articulation and acoustic experiments will inform the development of algorithms essential to automatic speech recognition.  Production data will include real-time 3D ultrasound recordings of the tongue, video recordings of the lips, palate impressions, microphone recordings, and accelerometer recordings of neck skin vibrations which have been shown to be beneficial in automatic speech and speaker recognition applications. The causal relationship between articulatory and acoustic variability will be explored, as will their relationship to misrecognition of child speech. Articulatory features will be incorporated into new automatic speech recognition systems along with acoustic features. The exploratory project will contribute to knowledge of variability between children, as well as variability over time as children grow and will provide, for the first time, normative data and scientific models. These can lead to robust child speech recognition systems as well as tools that will be useful for a variety of applications such as educational games, training of speech-language pathologists, automatic or semi-automatic transcription systems, and speech articulation visualization systems. It will train undergraduate and graduate students in important cross-disciplinary activities of technological and scientific significance. We believe that the proposed project is transformative in its advancement of the scientific and technological state of the art related to child speech."
66,1448616,SBIR Phase I:  Exploring the Feasibility of Deployable Crowd-Powered Real-Time Captioning Supplemented with Automatic Speech Recognition,IIP,SMALL BUSINESS PHASE I,1/1/15,12/11/14,Walter Lasecki,PA,Legion Labs LLC,Standard Grant,Glenn H. Larsen,12/31/15,"$150,000.00 ",,wlasecki@umich.edu,1401 Beechwood Blvd,Pittsburgh,PA,152171326,4129450708,ENG,5371,"5371, 8031, 8032, 9177",$0.00 ,"This SBIR Phase I project will investigate the feasibility of a high-quality speech-to-text service that combines the input of multiple non-expert human workers with the input of automatic recognition. Real-time captioning converts speech to text quickly (in less than five seconds), and is a vital accommodation that allows deaf and hard of hearing students to participate in mainstream classrooms and other educational activities. The current accepted approach for real-time captioning is to use expert human captionists (stenographers) that are very expensive ($150-300 per hour) and difficult to schedule. Computers can also convert speech to text via automatic speech recognition, but this technology is still unreliable in realistic settings and is likely to remain unreliable in the near- and medium-term future. This award will advance a higher-quality and more affordable alternative systems for real-time captioning that uses computation to coordinate multiple workers who can be more readily drawn from the existing labor force than highly specialized typing experts. This project will allow for increased access for deaf and hard of hearing people, resulting greater opportunities to participate in science and engineering. This in turn may afford deaf and hard of hearing people greater employment opportunities. <br/><br/>The approach advanced by this project combines the partial captions provided on-demand by human workers using computation to convert speech to text with very low latencies (less than five seconds). Advances in human-computer interaction will allow each constituent worker to be directed to type only part of what he or she hears via both aural and visual cues, and will optimally adjust the playback rate of the audio to each worker's current typing speed. Novel algorithms based on multiple sequence alignment (often used in gene sequencing) will merge the resulting partial captions into a final output stream that can be forwarded back to the user. The incorporation of automatic speech recognition will further reduce costs and increase the scalability of the approach. As the service is developed and automatic speech recognition improves, the service will rely less on humans and more on computation, providing a path toward full automation in the future. This award will investigate the appropriateness and feasibility of captioning systems based on this approach by deploying it in the field, measuring the quality of the captions generated, and collecting qualitative feedback from deaf and hard of hearing students in science and engineering fields."
67,1530989,"I-Corps:  SDNatics: Big Data Analytics of Software Defined Networks to Understand, Predict and Protect Critical Computer Networks",IIP,I-Corps,4/1/15,3/25/15,Yan Luo,MA,University of Massachusetts Lowell,Standard Grant,Rathindra DasGupta,9/30/15,"$50,000.00 ",,Yan_Luo@uml.edu,Office of Research Admin.,Lowell,MA,18543692,9789344170,ENG,8023,,$0.00 ,"Conventional computer network management has serious limitations on granularity, responsiveness and predictability, which can lead to network failures costing millions of dollars in critical domains, e.g., public safety and health care. Today's networks are currently managed under various enterprise policies and with state-of-the-art tools such as software defined networking (SDN) controllers. However, the faults and problems in the networks still arise because there are always cases to which predefined rules and algorithms do not respond well, causing traffic flooding, congestions, or vulnerability to attacks. Understanding in-depth of the network behavior patterns and applying the knowledge in network management and planning are fundamental to providing new solutions and best practices.<br/><br/>This team has developed a suite of protocols and analytics methods to understand network dynamics by using software-defined networking (SDN) and machine learning technologies.  With this information the team is able to report abnormal network behavior and alert potential faults.  This allows for much more of the network to be managed algorithmically, automatically, and intelligently. Specifically, this team's technology is able to track network flows to a much higher granularity (i.e. resolution) using SDN technology.  However, while current SDN approaches are better than traditional networks in defining and enforcing rules around network flows, they do not have a capability to analyze in-depth usage behavior and dynamically apply such information to improve network management and planning.  By using machine learning techniques (i.e. Deep Learning), the team will develop the SDNatics software platform that will allow for more advance uses of SDN.  Such improvements should improve security, predictability, speed, and data volume in network systems."
68,1522125,"SCH: INT: Collaborative Research: Replicating Clinic Physical Therapy at Home: Touch, Depth, and Epidermal Electronics in an Interactive Avatar System",IIS,Smart and Connected Health,9/1/15,8/19/15,Pamela Cosman,CA,University of California-San Diego,Standard Grant,Wendy Nilsen,8/31/20,"$1,775,020.00 ","Todd Coleman, Sujit Dey, Truong Nguyen",pcosman@ucsd.edu,Office of Contract & Grant Admin,La Jolla,CA,920930621,8585344896,CSE,8018,"8018, 8062",$0.00 ,"Physical therapy is often hampered by lack of access to therapists, and lack of adherence to home therapy regimens.  This research develops a physical therapy assistance system for home use, with emphasis on stroke rehabilitation.  As a person exercises, inexpensive cameras observe color and depth, and unobtrusive tattoo sensors monitor detailed muscle activity.  The 3D movement trajectory is derived and compared against the exercise done with an expert therapist.  The patient watches a screen avatar where arrows and color coding guide the patient to move correctly.  In addition to advancing fields such as movement tracking, skin sensors, and assistive systems, the project has the potential for broad impact by attracting women and under-represented minorities to engineering through health-related engineering coursework and projects, and because home physical therapy assistance can especially help rural and under-served populations. <br/><br/>This project uses bio-electronics, computer vision, computer gaming, high-dimensional machine learning, and human factors to develop a home physical therapy assistance system.  During home exercises, patient kinematics and physiology are monitored with a Kinect color/depth camera and wireless epidermal electronics transferable to the skin with a temporary tattoo. The project involves optimization of electrode design and wireless signaling for epidermal electronics to monitor spatiotemporal aspects of muscle recruitment, hand and body pose estimation and tracking algorithms that are robust to rapid motion and occlusions, and development of machine learning and avatar rendering algorithms for multi-modal sensor fusion and expert-trained optimal control guidance logic, for both cloud and local usage.  The system aims to provide real-time feedback to make home sessions as effective as office visits with an expert therapist, reducing the time and money required for full recovery."
69,1453378,CAREER: Synthesizing Highly Efficient Hardware Accelerators for Irregular Programs: A Synergistic Approach,CCF,Software & Hardware Foundation,3/1/15,5/1/19,Zhiru Zhang,NY,Cornell University,Continuing Grant,Sankar Basu,2/28/21,"$453,036.00 ",,zhiruz@cornell.edu,373 Pine Tree Road,Ithaca,NY,148502820,6072555014,CSE,7798,"1045, 7945",$0.00 ,"This CAREER research project aims to significantly improve the design productivity and quality of heterogeneous computer architectures, which extensively integrate specialized hardware accelerators to continue to provide the computing improvements essential to all aspects of our society. Achieving this goal requires the development of a new class of truly integrated design automation methodologies and tools to enable productive modeling, exploration, and generation of hardware accelerators from high-level programs, especially for the irregular programs that are commonplace in emerging application domains such as computer vision, machine learning, physical simulation, and social network analytics. The project also has a broad yet thematically focused plan for educational outreach, which aims to cultivate the next generation of engineers and scientists who can bridge the chasm between the software and hardware design paradigms. The PI will lead hands-on design sessions for underrepresented minority high school students and organize engineering seminars with engaging demonstrations for first-year undergraduates to increase their interest and participation in computer engineering. In addition, the PI will actively integrate the research outcomes into undergraduate and graduate curriculum development, and leverage industrial collaborations to effectively disseminate the research results on heterogeneous computing to a broader audience.<br/><br/><br/>Diminished benefits of technology scaling have led to a growing interest in heterogeneous accelerator-rich system architectures to improve performance under tight power and energy efficiency constraints. Irregular programs are gaining prominence in many important application domains; but these programs are much more difficult to parallelize on conventional data-parallel accelerators such as GPUs, as they typically exhibit less-structured data access patterns and difficult-to-predict dynamic parallelism. This project aims to develop a synergistic design automation framework where a set of novel programming abstractions, architectural templates, synthesis optimization algorithms, and hardware prototypes all play concerted roles to overcome the many challenges raised by the irregular programs. Specifically, the key idea is to automatically generate softly synthesized accelerators that are capable of decoupling data access from computation for tolerating memory latency and performing run-time optimizations for exploiting the irregular parallelism."
70,1521532,"SCH: INT: Collaborative Research: Replicating Clinic Physical Therapy at Home: Touch, Depth, and Epidermal Electronics in an Interactive Avatar System",IIS,Smart and Connected Health,9/1/15,5/15/18,Sri Kurniawan,CA,University of California-Santa Cruz,Standard Grant,Wendy Nilsen,8/31/20,"$133,679.00 ",,srikur@soe.ucsc.edu,1156 High Street,Santa Cruz,CA,950641077,8314595278,CSE,8018,"8018, 8062, 9251",$0.00 ,"Physical therapy is often hampered by lack of access to therapists, and lack of adherence to home therapy regimens.  This research develops a physical therapy assistance system for home use, with emphasis on stroke rehabilitation.  As a person exercises, inexpensive cameras observe color and depth, and unobtrusive tattoo sensors monitor detailed muscle activity.  The 3D movement trajectory is derived and compared against the exercise done with an expert therapist.  The patient watches a screen avatar where arrows and color coding guide the patient to move correctly.  In addition to advancing fields such as movement tracking, skin sensors, and assistive systems, the project has the potential for broad impact by attracting women and under-represented minorities to engineering through health-related engineering coursework and projects, and because home physical therapy assistance can especially help rural and under-served populations. <br/><br/>This project uses bio-electronics, computer vision, computer gaming, high-dimensional machine learning, and human factors to develop a home physical therapy assistance system.  During home exercises, patient kinematics and physiology are monitored with a Kinect color/depth camera and wireless epidermal electronics transferable to the skin with a temporary tattoo. The project involves optimization of electrode design and wireless signaling for epidermal electronics to monitor spatiotemporal aspects of muscle recruitment, hand and body pose estimation and tracking algorithms that are robust to rapid motion and occlusions, and development of machine learning and avatar rendering algorithms for multi-modal sensor fusion and expert-trained optimal control guidance logic, for both cloud and local usage.  The system aims to provide real-time feedback to make home sessions as effective as office visits with an expert therapist, reducing the time and money required for full recovery."
71,1519164,EAGER: Automatic Speech Recognition for Uyghur,IIS,"Linguistics, Robust Intelligence, DEL",2/15/15,2/18/15,Arienne Dwyer,KS,University of Kansas Center for Research Inc,Standard Grant,D.  Langendoen,8/31/16,"$27,338.00 ",Tanja Schultz,anthlinguist@ku.edu,2385 IRVING HILL RD,Lawrence,KS,660457568,7858643441,CSE,"1311, 7495, 7719","7495, 7916, 9150",$0.00 ,"Advances in speech engineering now allow audio to be transcribed as text, even for languages for which there are few computational resources. Automating text transcription for more languages allows public, community, and researcher access to previously inaccessible materials. This project uses several thousand hours of radio broadcasts in an under-resourced language as a test case to improve rapid audio-to-text development techniques, which are applicable to any language. The project allows speech engineers to apply technology to new languages, to learn about the characteristics of new languages and their impact on speech recognition performance, and how to overcome them with the goal of building better speech recognition systems. It also enables communities to preserve their language, distribute tools and data, and overall, improve the current extreme resource limitations of their language. The project encourages students to work and think across the fields of  speech engineering, linguistics and journalism.<br/><br/>In this EAGER project, the Uyghur language (ISO 639-3: uig), a severely under-resourced Turkic language of Xinjiang in Central Asia with about 11 million speakers, is used to test the rapid development of an Automatic Speech Recognition (ASR) system with the long-term vision of creating web-based speech and language services including pronouncing dictionary generation, audio and text data archiving, and part-of-speech tagging. The project is exploratory because the language is devoid of computationally tractable resources, yet bootstrapping through a related language (Turkish) promises rapid ASR development. The project can serve as a model for such development for any language, large or small, and is potentially transformative -- first because so many of the world's languages are like Uyghur in having few available computational resources., and second because so many documentary linguists still rely entirely on non-automated methods."
72,1544267,EAGER: Computational Teichmuller Theory,CCF,Algorithmic Foundations,6/15/15,6/18/19,Wei Zeng,FL,Florida International University,Standard Grant,Joseph Maurice Rojas,5/31/20,"$265,989.00 ",,wzeng@cs.fiu.edu,11200 SW 8TH ST,Miami,FL,331990001,3053482494,CSE,7796,"7916, 7929, 9251",$0.00 ,"Shape transformation and matching plays a fundamental role in engineering and biomedicine. Shape metric with powerfully discriminative ability is critical and highly desired in practice for machine learning in big geometric data. This project aims to develop the rigorous computational framework for finding the intrinsic mapping between shapes and the intrinsic shape metric among general surfaces based on Teichmüller theory. This proposed research will open a new paradigm for geometric analysis, lay down the theoretical foundation and develop a new class of algorithms and software tools for shape transformation and matching. It is expected to greatly increase the applicability of mining and learning technologies for the emerging ubiquitous large-scale geometric data. Its success will significantly advance computational/digital geometry and will enhance the abilities of geometry and topology theories to solve real-world shape analysis problems. The resulting algorithms will have a broad range of applications in the fields of science, engineering and biomedicine. Potential applications include morphometry analysis, cancer detection and abnormality prediction in medical imaging, motion/deformation tracking and dynamics analysis in graphics and vision, and facial recognition, expression analysis and information. <br/><br/>This research aims to develop the rigorous computational framework of Teichmüller theory for the Teichmüller Map, which is unique and has the minimal angle distortion in its homotopy class, and the derived Teichmüller Distance among general surfaces. The computation approach is based on the insight from the quasiconformal Teichmüller theory and the variational principle. It first solves the computation of the holomorphic quadratic differentials and then computes the Teichmüller maps. The discrete Teichmüller theory will be established, and the existence and uniqueness of the solution is guaranteed. The results are applied for shape analysis and deep learning in big data, including facial recognition, expression analysis, brain study, etc. The new paradigm of discrete Teichmüller theory will make major impacts on computer science, including geometric modeling, computer graphics, visualization, vision, networking, and medical imaging. The methodology will also have impacts on pure sciences, engineering and biomedicine, and potential benefits for homeland security and national defense. The algorithms developed during the research will be made freely available on the world wide web. The PI will encourage minority groups at FIU to pursue research in geometry, and make the research accessible to more audience through the seminars, course development, workshops and conferences."
73,1513853,"RI: Medium: Assessing Speaker and Teacher Effectiveness through Gestural Analysis, EEG Recordings, and Eye Tracking",IIS,"Robust Intelligence, IntgStrat Undst Neurl&Cogn Sys",9/1/15,8/28/15,John Kender,NY,Columbia University,Standard Grant,James Donlon,8/31/19,"$899,796.00 ",Paul Sajda,kender@cs.columbia.edu,2960 Broadway,NEW YORK,NY,100276902,2128546851,CSE,"7495, 8624","7495, 7924, 8089, 8091",$0.00 ,"This project helps speakers and teachers to measure and improve their impact on their audiences.  It uses visual observations of body, head, and hand gestures of the communicator, plus recordings of brain activity and eye movements of the audience.  Together, these determine which sections of a presentation elicit the most audience engagement. The project is developing new methods to capture and calibrate electroencephalogram and eye-tracking data from listeners and from students. It is determining new ways to relate this subject information to what a speaker or teacher can be seen to be doing while developing an argument or reviewing a concept.  The project produces analyses of when and how the communicator is most effective. This system is being ported to the Columbia Video Network distance education facility, for their use in improving the online delivery of Columbia University Master's level technical courses. This project continues a research effort that has involved women, minorities, disabled students, and undergrads.<br/><br/>This research investigates the degree to which certain speaker gestures can convey significant information that are correlated to audience engagement, in speeches and in classroom lectures.  The project develops and validates a catalog of gestural attributes derived from pose and movements of body, head, and hand, and automatically extracts these attributes from videos.  It demonstrates correlations between gesture attributes and an objective method of measuring audience engagement: electroencephalography (EEG).  The project leverages a multi-disciplinary approach, with neural engineers and computer/media scientists collaborating to build a system that identifies and tracks physiological measures of engagement, and relates these to features in the video as well as information content.  It records subjects' high-density EEG, and tracks their eyes and pupillary responses while they are watching video lectures.  It uses machine learning, specifically novel methods which expand upon canonical correlation analysis, to relate inter- and intra-subject correlations, between the physiological changes and the gestural features derived from the video by using enhanced computer vision techniques.  These measures are further integrated with pupillary measures, which have been shown to correlate with arousal, as well as with gaze measures, which are indicative of attention.  The project is producing an analysis of body, head, and hand gestures useful in persuasion and in education, and a catalog of their influence on engagement and speaker effectiveness."
74,1448221,SBIR Phase I:  Robotic System for the Sorting of Construction and Demolition Recyclables,IIP,SBIR Phase I,1/1/15,6/29/15,Matanya Horowitz,CO,Cognitive Robotics,Standard Grant,Muralidharan Nair,12/31/15,"$175,000.00 ",,mhorowit@caltech.edu,17795 W 59th Dr,Golden,CO,804031103,7204700812,ENG,5371,"163E, 5371, 6840, 8035, 9139, HPCC",$0.00 ,"The broader impact/commercial potential of this project will be to change the fundamental economics of the recycling process. Analysis suggests that the system can drive the cost of recycling&#8232; to levels competitive with, or below those of, landfilling, drastically changing the waste landscape. No longer will recycling be driven by government mandate, and grow slowly only because of public education and pressure. Instead, market forces will be aligned with waste diversion. The roadmap begins with attacking Construction & Demolition waste, followed by the entire solid waste stream. Although it is estimated that up to 95% of the waste stream could be recycled, only a third of the 250 million tons that are generated each year in the United States is currently diverted. Greater diversion would provide immense savings in landfill and processing costs, and benefit the environment as well. Tens of millions of pounds of greenhouse gas emissions from virgin material mining may be eliminated, and pollution from landfill waste reduced. The existing sorting process is expensive and unprofitable, requiring human workers to manually sort debris, an extremely dull, dirty, and dangerous profession. This innovation has the potential to eliminate these trade offs between cost and environment. <br/><br/>This Small Business Innovation Research Phase I project will create a scalable, integrated robotic system that autonomously sorts Construction & Demolition waste for recycling. This advance in autonomous systems is made possible by a series of innovations in robotics: (1) new safety features in robotic hardware that ensure efficient and &#8232;safe collaboration with human workers, allowing for the system to be deployed with virtually zero retrofitting in existing facilities; (2) new motion planning techniques that allow for trajectories to be generated in real time, customized for the characteristics of the waste, safety, and any uncertainty in individual objects? position;&#8232; (3) modern machine learning techniques that allow the system to classify waste at levels approaching human performance, with a continual training signal obtained via human supervision; and (4) tremendous improvements in both computer vision and robotic manipulation, allowing for previously unseen objects to be modeled and manipulated reliably. These innovations pave the way for a new era in recycling, where waste is sorted cheaply, safely, and reliably on a universal scale."
75,1516695,"Big Data on Small Organisms: Petascale Simulations of Data-Driven, Whole-Cell Microbial Models",OAC,Leadership-Class Computing,8/1/15,6/23/15,Ilias Tagkopoulos,CA,University of California-Davis,Standard Grant,Edward Walker,7/31/20,"$40,000.00 ",,iliast@cs.ucdavis.edu,OR/Sponsored Programs,Davis,CA,956186134,5307547700,CSE,7781,,$0.00 ,"This project aims to develop the next-generation of genome-scale, data-driven models for microbial organisms. The project will first focus on the most-studied microbe, the gram-negative bacterium Escherichia coli, due to the availability of high-throughput data, cellular organization, its significance to industry and human health. The project will take advantage of the multi-omics datasets that resulted from advances in parallel high-throughput molecular profiling over the past fifteen years, the emergence of data-driven, integrative, multi-scale models with substantial improvement of their predictive power and new techniques in machine learning, especially those related to deep learning. Accurate prediction of microbial fitness and cellular state can have profound implications to the way we test hypotheses that are directly related to health, social or economic benefits. This award will support the training of multiple undergraduate and graduate students in computational modeling and high-performance simulations of biological systems through undergraduate courses, IGEM teams and other initiatives.<br/><br/>This project will support the generation of knowledge from the largest normalized omics compendia for the most widely used microbe that will be a boon for the development and training of the next generation of data-driven predictive methods in molecular and cellular biology. It will provide the computational resources to evaluate a state-of-the-art multi-scale model with the capacity to predict phenotypic characteristics and environmental conditions from collective omics data. This will be the first systems-level simulator that targets a specific microbe (E. coli) and will be able to simulate populations of cells with a resolution ranging from individual gene concentrations to population dynamics. To achieve that, process migration, load-balancing and strong scaling techniques have to be adopted and applied in this context, which are all novel features for the area of whole-cell modeling. The proposed HPC simulations will be intimately related to hypothesis generation and testing. The simulations will address questions related to what their phenotype and expression profiles of microbial cultures are in complex environments. In the context of systems biology, integration of these techniques has the potential of being transformative, but only if the necessary computational infrastructure able to handle these tasks is available. The Blue Waters Supercomputer with its unique architecture, large-scale simulation capabilities and professional support staff provides the ideal platform to achieve this ambitious goal."
76,1559719,Making words disappear or appear: A neurocognitive and behavioral investigation of effects of speech rate on spoken word recognition,BCS,"Perception, Action & Cognition",9/1/15,8/1/16,Navin Viswanathan,KS,University of Kansas Center for Research Inc,Standard Grant,Betty Tuller,7/31/18,"$58,000.00 ",,mailnavin@gmail.com,2385 IRVING HILL RD,Lawrence,KS,660457568,7858643441,SBE,7252,"7252, 7298, 9179, 9251",$0.00 ,"Understanding how humans comprehend speech is an unsolved and challenging problem, in part because factors such as different speakers, dialects, and speaking rates introduce a great deal of temporal and spectral variability into the speech signal. The focus of this research is on the influence of temporal context on perception of segments, syllables, and words. Results of the research may offer insights into treatment of disorders that involve disruption of speech rate (e.g., dysarthria, stuttering, Parkinson's disease, and aphasia), inform approaches to improve speech technology applications (e.g., enhanced automatic speech recognition, more natural sounding computer-generated speech), and lead to new discoveries related to brain mechanisms involved in understanding spoken language. The investigators will also involve students in the research, including those from a primarily undergraduate institution collaborating on the project.<br/><br/>The investigators will test different accounts of temporal phenomena in the perception of speech. They propose two interacting cognitive mechanisms controlling phenomena at lexical and phonetic levels, each driven by a different neural timing mechanism. The hypothesis is that effects of lexical rate primarily stem from top-down, speech-specific temporal expectancies, while phonetic rate effects originate in bottom-up, transient rhythmic expectations that are not specific to speech. This hypothesis will be assessed using psychoacoustic studies, non-invasive measures of brain activity, and theoretical modeling in order to identify the processing characteristics revealed by neural representations of temporal properties of speech."
77,1500738,Collaborative Research: Contributions of Endangered Language Data for Advances in Technology-enhanced Speech Annotation,IIS,Robust Intelligence,7/1/15,6/1/20,Andreas Kathol,CA,SRI International,Standard Grant,D.  Langendoen,10/31/20,"$286,358.00 ",Vikramjit Mitra,kathol@speech.sri.com,333 RAVENSWOOD AVE,Menlo Park,CA,940253493,7032478529,CSE,7495,"1311, 7298, 7495, 7719, 7791, 9179, 9251",$0.00 ,"Linguists have increased efforts to collect authentic speech materials from endangered and little-studied languages to discover linguistic diversity. However, the challenge of transcribing these speech into written form to facilitate analysis is daunting. This is because of both the sheer quantity of digitally collected speech that needs to be transcribed and the difficulty of unpacking the sounds of spoken speech.  <br/><br/>Linguist Andreas Kathol and computer scientist Vikramjit Mitra of SRI international and linguist Jonathan D. Amith of Gettysburg College will team up to create software that can substantially reduce the language transcription bottleneck. Using as a test case Yoloxochitl  Mixtec, an endangered language from the state of Guerrero, Mexico, the team will develop a software tool that will use previously transcribed Yoloxochitl Mixtec speech data to both train a new generation of native speakers in practical orthography and to develop automatic speech recognition software. The output of the recognition software will be used as preliminary transcription that native speakers will correct, as necessary, to create additional high-quality training data. This recursive method will create corpus of transcribed speech large enough so that software will be able to complete automatic transcription of newly collected speech materials.<br/><br/>The project will include the training of undergraduate and graduate students in software development and the analysis of the Yoloxochitl Mixtec sound system. The project will also train native speakers as documenters in an interactive fashion that systematically introduces them to the transcription conventions of their language. This software tool will help in establishing literacy in Yoloxochitl Mixtec among a broader base of speakers.<br/><br/>The results of this project will be available at the Archive of Indigenous Languages of Latin America (University of Texas, Austin), Kaipuleohone (University of Hawai'i Digital Language Archive),  and at the Linguistic Data Consortium (University of Pennsylvania)."
78,1500595,Collaborative Research: Contributions of Endangered Language Data for Advances in Technology-enhanced Speech Annotation,IIS,Robust Intelligence,7/1/15,8/15/19,Jonathan Amith,PA,Gettysburg College,Standard Grant,D.  Langendoen,6/30/20,"$252,837.00 ",,jamith@gettysburg.edu,North Washington Street,Gettysburg,PA,173251483,7173376505,CSE,7495,"1311, 7298, 7495, 7719, 9179, 9251",$0.00 ,"Linguists have increased efforts to collect authentic speech materials from endangered and little-studied languages to discover linguistic diversity. However, the challenge of transcribing these speech into written form to facilitate analysis is daunting. This is because of both the sheer quantity of digitally collected speech that needs to be transcribed and the difficulty of unpacking the sounds of spoken speech. <br/><br/>Linguist Andreas Kathol and computer scientist Vikramjit Mitra of SRI international and linguist Jonathan D. Amith of Gettysburg College will team up to create software that can substantially reduce the language transcription bottleneck. Using as a test case Yoloxochitl Mixtec, an endangered language from the state of Guerrero, Mexico, the team will develop a software tool that will use previously transcribed Yoloxochitl Mixtec speech data to both train a new generation of native speakers in practical orthography and to develop automatic speech recognition software. The output of the recognition software will be used as preliminary transcription that native speakers will correct, as necessary, to create additional high-quality training data. This recursive method will create corpus of transcribed speech large enough so that software will be able to complete automatic transcription of newly collected speech materials. <br/><br/>The project will include the training of undergraduate and graduate students in software development and the analysis of the Yoloxochitl Mixtec sound system. The project will also train native speakers as documenters in an interactive fashion that systematically introduces them to the transcription conventions of their language. This software tool will help in establishing literacy in Yoloxochitl Mixtec among a broader base of speakers. <br/><br/>The results of this project will be available at the Archive of Indigenous Languages of Latin America (University of Texas, Austin), Kaipuleohone (University of Hawai'i Digital Language Archive), and at the Linguistic Data Consortium (University of Pennsylvania)."
79,1550800,CAP:  Building Partnerships for Education and Speech Research,IIS,"DISCOVERY RESEARCH K-12, Cyberlearn & Future Learn Tech",9/1/15,7/28/15,Chad Dorsey,MA,Concord Consortium,Standard Grant,John Cherniavsky,8/31/17,"$49,996.00 ",Cynthia D'Angelo,cdorsey@concord.org,25 Love Lane,Concord,MA,17422345,9784053205,CSE,"7645, 8020","7556, 8045, 8055, 8244",$0.00 ,"A new age of technology is dawning on the field of speech recognition and analysis. This has begun to become publicly visible through the increasing availability of impressive tools such as Siri and Google Translate, but these consumer-ready tools only scratch the surface of the potential that research-quality speech engineering applications unlock. However, these capabilities remain practically unrecognized by the education research community. This one-year Capacity-Building Project (CAP) will build partnerships among top researchers and develop expertise from both education and speech engineering backgrounds through a series of workshops to bring together four leading spoken language technology and education research groups. <br/><br/>This project will unite the extensive education research and educational technology backgrounds at the Concord Consortium and SRI International's Center for Technology in Learning and bring them together with two of the strongest groups in spoken language technology research, the Speech Technology and Research Laboratory at SRI International and the Center for Robust Speech Systems at the University of Texas at Dallas.  This will provide the foundations for educational research areas as diverse as collaboration, argumentation, discourse analysis, teacher questioning, emotion, and engagement. Spoken language technologies will provide efficiencies, insight, and entire new methodologies for approaching these research areas, and they will do so while students and teachers assume more natural modes - those of the naturalistic language-based interactions that have formed the basis of educational interchange for millennia. This project will gather and summarize applicable knowledge about the current state of these fields and generate central papers and momentum suitable for bringing together and helping launch a new interdisciplinary field of study, spoken language technology for education. The work of this CAP proposal will generate the initial necessary connections and create the first definitions required to move toward proposals, actions, and research."
80,1550145,EAGER: Matching Non-Native Transcribers to the Distinctive Features of the Language Transcribed,IIS,ROBUST INTELLIGENCE,8/1/15,8/4/15,Mark Hasegawa-Johnson,IL,University of Illinois at Urbana-Champaign,Standard Grant,Tatiana Korelsky,7/31/18,"$150,000.00 ","Preethi Jyothi, Lav Varshney",jhasegaw@illinois.edu,1901 South First Street,Champaign,IL,618207406,2173332187,CSE,7495,"7495, 7916",$0.00 ,"Automatic speech recognition (ASR) systems must be trained using hundreds of hours of speech, with synchronized text transcriptions.  Transcribing that much speech is beyond the means of most language communities; therefore ASR systems do not exist for most languages.   To overcome this bottleneck, this exploratory EAGER project asks people who don't understand a particular language to transcribe it as if they were listening to nonsense syllables.  Of course, when people try to transcribe speech in a language they don't understand, they make mistakes.  However there are patterns to those mistakes which can be modeled using decoding strategies developed for telephone and wireless communication, and used to route each transcription task to people whose native language helps them to perform it. The resulting transcriptions are then fused in order to recover correct transcriptions.  Five different languages are to be tested, including languages with lexical tone, and languages with a variety of consonant contrasts very different from English.  The resulting transcriptions can then train ASR systems in all five languages, and the quality of the research evaluated based on its ability to train those systems without using transcriptions produced by native speakers. <br/> <br/>Mismatched crowdsourcing is formalized as a noisy channel; the talker encodes meaning in a string of symbols (phonemes) not all of which are reliably distinguishable by the perceiver.  Models of second-language speech perception for each transcriber can be initialized using a perceptual assimilation model, then specialized.  In particular, this proposal seeks increases in the scale and robustness of mismatched crowdsourcing by using error-correcting codes to divide the transcription task, and by then distributing each sub-task to transcribers whose native language contains the distinctive feature requested.  It also seeks to develop new theory at the intersection of the current fields of crowdsourcing (the learnability of a function under conditions of label noise) and grammar induction (the learnability of a function from one language to another), and to perform grammar induction under conditions of label noise.  Preliminary bounds exist for some aspects of this problem; the proposed research is designed to develop more detailed theoretic results, and test and apply them to determine the feasibility of creating serviceable ASR systems for under-resourced languages without having to use fluent speakers of those languages to transcribe speech in those languages."
81,1454094,CAREER: Understanding Real-World Auditory Scene Analysis,BCS,"PERCEPTION, ACTION & COGNITION",4/1/15,4/26/17,Joshua McDermott,MA,Massachusetts Institute of Technology,Continuing grant,Betty H. Tuller,10/31/16,"$181,477.00 ",,jhm@MIT.EDU,77 MASSACHUSETTS AVE,Cambridge,MA,21394301,6172531000,SBE,7252,"1045, 7252",$0.00 ,"A fundamental question in auditory science concerns how people can recognize speech and other sounds in the presence of competing sound sources, as when conversing with a dinner partner at a crowded restaurant. The process of hearing a sound of interest when it is embedded in a mixture of other sounds is known as ""sound segregation"" and human listeners vastly outperform machine systems for segregating sounds. However, the process is frequently effortful,  is highly vulnerable to hearing impairment, including hearing impairment that typically accompanies  normal aging. Understanding the basis of sound segregation in human listeners, and the factors that limit human segregation abilities, would enhance efforts to develop assistive listening devices and machine systems for robust speech recognition and sound recognition. The project will be complemented by an educational effort to stimulate interest in audition in the general public and in middle- and high-school students through a series of publicly available online video presentations describing auditory research with associated sound demonstrations.<br/><br/>This CAREER award is aimed at enriching the understanding of human auditory perception by exploring the basis of sound segregation with natural sounds. The experiments will leverage recent advances in speech analysis and synthesis methods to 1) manipulate grouping cues in natural speech and test their effect on sound segregation in human listeners; 2) manipulate voice and speech structure to probe their role in segregation; and 3) test the ability of human listeners to attend to and track target sound sources. The long-term goals are to inspire signal-processing algorithms that facilitate segregation by human listeners and replicate their competence in machine systems."
82,1539133,RIDIR:  Collaborative Research: Enabling Access to and Analysis of Shared Daylong Child and Family Audio Data,SMA,"CYBERINFRASTRUCTURE, Data Infrastructure",9/1/15,8/24/15,Mark VanDam,WA,Washington State University,Standard Grant,John Yellen,2/29/20,"$281,958.00 ",,mark.vandam@wsu.edu,280 Lighty,PULLMAN,WA,991641060,5093359661,SBE,"7231, 8294","7433, 8004, 9178, 9179",$0.00 ,"A child's language development in the first few years of life predicts long-term cognitive development, academic achievement, and expected income as an adult. Early language development in turn depends on linguistic interactions with adults. Increasingly, researchers are using daylong audio recordings to study child language development and child-caregiver interactions. Compared to short language samples, daylong recordings capture the full range of experiences a child has over the course of a day. Daylong audio recordings are also being used in applied settings. For example, studies show that by the time they enter First Grade, children from higher socioeconomic backgrounds hear tens of millions more words than children from lower socioeconomic backgrounds, perpetuating social inequalities. Multiple large-scale intervention projects targeting low socioeconomic households, including the Thirty Million Words Initiative in Chicago and the Providence Talks program, are using daylong audio recordings to provide automated, personalized feedback to parents on when and how often their child hears adult words and experiences conversational turns. The features of daylong recordings that are advantageous for researchers and practitioners also pose unique challenges. For one, their long durations are ideal for studying the temporal dynamics of child-adult interaction, but taking advantage of the long durations requires the enlistment of automated speech recognition technology. Current automatic speech recognition systems have difficulties with child speech and are challenged by the noisy and varied acoustic environments represented in the recordings. Another challenge is that the recordings capture private moments that require long hours of human listening to remove. This makes it difficult for researchers to share the recordings publicly, so that the potential value of the recordings collected by individual research labs is not fully realized.<br/><br/>This project will create a new resource, called HomeBank, that will have three key components: (1) a public dataset containing daylong audio recordings that have had private information removed by human listeners, (2) a larger dataset containing about ten to one hundred times as many hours of recording that have not had private information removed and will be free but restricted to those who have demonstrated training in human research ethics, and (3) an open-source repository of computer programs to automatically analyze the daylong audio recordings. HomeBank will take advantage of an existing cyberinfrastructure for sharing linguistic data called TalkBank. The daylong audio recordings included in the datasets will represent both typically developing and clinical groups, a range of ages from newborn infants to school age children, and a range of language and socioeconomic backgrounds. We expect the primary users to be basic and applied child development researchers as well as engineers developing automatic speech recognition technologies. The free-to-access database and the open source computer programs will ultimately improve both the data on which early interventions are based and the tools available for providing parents with feedback on the linguistic input they provide their children."
83,1539129,RIDIR:  Collaborative Research: Enabling Access to and Analysis of Shared Daylong Child and Family Audio Data,SMA,"CYBERINFRASTRUCTURE, Data Infrastructure",9/1/15,8/24/15,Anne Warlaumont,CA,University of California - Merced,Standard Grant,William Badecker,4/30/18,"$447,225.00 ",,warlaumont@ucla.edu,5200 North Lake Road,Merced,CA,953435001,2092012039,SBE,"7231, 8294","7433, 8004, 9179",$0.00 ,"A child's language development in the first few years of life predicts long-term cognitive development, academic achievement, and expected income as an adult. Early language development in turn depends on linguistic interactions with adults. Increasingly, researchers are using daylong audio recordings to study child language development and child-caregiver interactions. Compared to short language samples, daylong recordings capture the full range of experiences a child has over the course of a day. Daylong audio recordings are also being used in applied settings. For example, studies show that by the time they enter First Grade, children from higher socioeconomic backgrounds hear tens of millions more words than children from lower socioeconomic backgrounds, perpetuating social inequalities. Multiple large-scale intervention projects targeting low socioeconomic households, including the Thirty Million Words Initiative in Chicago and the Providence Talks program, are using daylong audio recordings to provide automated, personalized feedback to parents on when and how often their child hears adult words and experiences conversational turns. The features of daylong recordings that are advantageous for researchers and practitioners also pose unique challenges. For one, their long durations are ideal for studying the temporal dynamics of child-adult interaction, but taking advantage of the long durations requires the enlistment of automated speech recognition technology. Current automatic speech recognition systems have difficulties with child speech and are challenged by the noisy and varied acoustic environments represented in the recordings. Another challenge is that the recordings capture private moments that require long hours of human listening to remove. This makes it difficult for researchers to share the recordings publicly, so that the potential value of the recordings collected by individual research labs is not fully realized.<br/><br/>This project will create a new resource, called HomeBank, that will have three key components: (1) a public dataset containing daylong audio recordings that have had private information removed by human listeners, (2) a larger dataset containing about ten to one hundred times as many hours of recording that have not had private information removed and will be free but restricted to those who have demonstrated training in human research ethics, and (3) an open-source repository of computer programs to automatically analyze the daylong audio recordings. HomeBank will take advantage of an existing cyberinfrastructure for sharing linguistic data called TalkBank. The daylong audio recordings included in the datasets will represent both typically developing and clinical groups, a range of ages from newborn infants to school age children, and a range of language and socioeconomic backgrounds. We expect the primary users to be basic and applied child development researchers as well as engineers developing automatic speech recognition technologies. The free-to-access database and the open source computer programs will ultimately improve both the data on which early interventions are based and the tools available for providing parents with feedback on the linguistic input they provide their children."
84,1539010,RIDIR:  Collaborative Research: Enabling Access to and Analysis of Shared Daylong Child and Family Audio Data,SMA,"CYBERINFRASTRUCTURE, Data Infrastructure",9/1/15,8/24/15,Brian MacWhinney,PA,Carnegie-Mellon University,Standard Grant,William Badecker,2/28/19,"$256,256.00 ",,macw@cmu.edu,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,SBE,"7231, 8294","7433, 8004",$0.00 ,"A child's language development in the first few years of life predicts long-term cognitive development, academic achievement, and expected income as an adult. Early language development in turn depends on linguistic interactions with adults. Increasingly, researchers are using daylong audio recordings to study child language development and child-caregiver interactions. Compared to short language samples, daylong recordings capture the full range of experiences a child has over the course of a day. Daylong audio recordings are also being used in applied settings. For example, studies show that by the time they enter First Grade, children from higher socioeconomic backgrounds hear tens of millions more words than children from lower socioeconomic backgrounds, perpetuating social inequalities. Multiple large-scale intervention projects targeting low socioeconomic households, including the Thirty Million Words Initiative in Chicago and the Providence Talks program, are using daylong audio recordings to provide automated, personalized feedback to parents on when and how often their child hears adult words and experiences conversational turns. The features of daylong recordings that are advantageous for researchers and practitioners also pose unique challenges. For one, their long durations are ideal for studying the temporal dynamics of child-adult interaction, but taking advantage of the long durations requires the enlistment of automated speech recognition technology. Current automatic speech recognition systems have difficulties with child speech and are challenged by the noisy and varied acoustic environments represented in the recordings. Another challenge is that the recordings capture private moments that require long hours of human listening to remove. This makes it difficult for researchers to share the recordings publicly, so that the potential value of the recordings collected by individual research labs is not fully realized.<br/><br/>This project will create a new resource, called HomeBank, that will have three key components: (1) a public dataset containing daylong audio recordings that have had private information removed by human listeners, (2) a larger dataset containing about ten to one hundred times as many hours of recording that have not had private information removed and will be free but restricted to those who have demonstrated training in human research ethics, and (3) an open-source repository of computer programs to automatically analyze the daylong audio recordings. HomeBank will take advantage of an existing cyberinfrastructure for sharing linguistic data called TalkBank. The daylong audio recordings included in the datasets will represent both typically developing and clinical groups, a range of ages from newborn infants to school age children, and a range of language and socioeconomic backgrounds. We expect the primary users to be basic and applied child development researchers as well as engineers developing automatic speech recognition technologies. The free-to-access database and the open source computer programs will ultimately improve both the data on which early interventions are based and the tools available for providing parents with feedback on the linguistic input they provide their children."
85,1603323,Understanding Prosody and Tone Interactions through Documentation of Two Endangered Languages,BCS,"IIS Special Projects, DEL",10/26/15,7/28/17,Christian DiCanio,NY,SUNY at Buffalo,Continuing Grant,Joan Maling,10/31/19,"$287,601.00 ",,dicanio@haskins.yale.edu,520 Lee Entrance,Buffalo,NY,142282567,7166452634,SBE,"7484, 7719","1311, 7484, 7719",$0.00 ,"For accurate automated translation of text-to-spoken-word or spoken-word-to-text, it is necessary to understand the prosodic patterning of speech since prosody, i.e. stress, rhythm, and pauses between phrases, is central in conveying meaning and structuring discourse.  In addition, many languages use tone to change the meaning of words but little is known about how prosodic features interact with tone.  Since tone and prosody are conveyed by using some of the same acoustic dimensions (pitch, duration, voice quality), one must know how these two systems interact for automatic speech recognition and translation.<br/><br/>To investigate and test hypotheses about the relationship of tone and prosody, Christian Di Canio, along with an interdisciplinary team, will create a database of 30 hours of transcribed narratives from Itunyoso Trique and expand an existing similar database in Yoloxóchitl Mixtec, both both Mixtecan languages from eastern central Mexico.  These transcribed narratives will then be parsed into smaller units applying a  'forced alignment' tool used in automatic speech recognition.   Important results from the project will include the testing and improvement of the forced alignment tool; new corpora and expanded dictionaries for and Yoloxóchitl Mixtec; and an analysis of tone and prosody interactions in these two languages.  The resulting prosodically segmented and tagged corpora will be a first of its kind for an endangered language that will be of use to the speaker-community and the broader scientific community.<br/><br/>This project is partially supported by funds from the Robust Intelligence program."
86,1453104,CAREER:  Scaling Source Separation to Big Audio Data,CCF,Comm & Information Foundations,8/1/15,7/12/19,Paris Smaragdis,IL,University of Illinois at Urbana-Champaign,Continuing Grant,Phillip Regalia,7/31/20,"$549,863.00 ",,paris@illinois.edu,1901 South First Street,Champaign,IL,618207406,2173332187,CSE,7797,"1045, 7936",$0.00 ,"The world we live in is composed out of mixed signals. It is practically impossible to easily obtain a clean recording of speech, music, environmental, mechanical, underwater, or biomedical sounds. This, in turn, complicates any further processing due to the presence of unwanted elements (e.g. background noise in speech recognition). We traditionally address this issue by using source separation and denoising methods that allow us to extract only a desired signal from a mixture. Unfortunately such methods do not scale at ?big data? levels, which means that most of the audio data we gather today remains at a state unsuitable for automatic content analysis or further processing. This project addresses the use of source separation methods when confronted with very large data sets. It considers use of modern data analysis methods to efficiently process large amounts of data, and also the effects of training on large signal corpora in order to improve source separation performance. The ultimate goals are to improve source separation performance by leveraging large signal collections, and to enable large-scale signal analysis by making modern source separation algorithms more efficient.<br/><br/>In order to enable processing at such large scales, this project takes advantage of recent developments in manifold structure analysis, deflation methods for spectral decompositions, hashing strategies, and quantization. Using a quantized manifold representation, large signal data sets can be approximated using a compact and efficiently accessible structure. Such representations can then be used as priors to a source separation algorithm and help guide it to extract signals that match them. Applying such models on large data to perform source separation is further accelerated by making use of a deflation method. Instead of performing the textbook (and computationally intensive) model matching process, this project uses a greedy approach to quickly extract target components while bypassing many unnecessary calculations. Finally, given the latest research on unifying multiple models of source separation, this project considers the application of such concepts to multiple data analysis models at once (such as HMM models, continuous dynamical systems, etc.), thereby becoming relevant to a wide range of signals and mixing situations."
87,1513420,Statistical Analysis of Trajectories on Riemannian Manifolds,DMS,STATISTICS,8/1/15,7/30/15,Jingyong Su,TX,Texas Tech University,Standard Grant,Gabor Szekely,7/31/19,"$104,974.00 ",,jingyong.su@ttu.edu,349 Administration Bldg,Lubbock,TX,794091035,8067423884,MPS,1269,,$0.00 ,"This project addresses the development of statistical tools for the analysis of complex data structures that arise with the use of modern technology.  In many applications, the observations naturally lie on a nonlinear space, such as a sphere, and it is important to study the temporal and spatial behavior of these processes.  The research will impact several data-rich applications including visual speech recognition, action/emotion recognition, and the analysis of medical images.  This project will use techniques from geometry, statistics, and computer science to develop algorithms and tools that can help practitioners derive much needed insight from these data.<br/><br/>In this research, a comprehensive framework for estimation, registration and analysis of manifold-valued processes is proposed. Functional data analysis in Euclidean spaces has been explored extensively in the literature.  In this project, the functions that are studied take values on nonlinear manifolds, rather than in vector spaces.  The observed data are noisy and discrete, and observed at random/arbitrary times.  Two main problems are investigated: the first involves estimation of smoothing trajectories using time-indexed points.  The second problem focuses on the development of a comprehensive framework for joint registration and analysis of multiple manifold-valued processes.   The approach takes into account the temporal variability, derives a rate-invariant metric, and generates statistical summaries (sample mean, covariance), which can be subsequently used for registering and modeling multiple trajectories."
88,1464306,CRII: CHS: A Plug-and-Play Deformable Model Based on Extended Domain Decomposition,IIS,HCC-Human-Centered Computing,3/1/15,5/6/15,Yin Yang,NM,University of New Mexico,Standard Grant,Ephraim Glinert,2/28/18,"$190,829.00 ",,yin5@clemson.edu,"1700 Lomas Blvd. NE, Suite 2200",Albuquerque,NM,871310001,5052774186,CSE,7367,"7367, 7453, 8228, 9150, 9251",$0.00 ,"Advances in data acquisition tools have led to a dramatic increase in the geometric complexity of 3D data.  Efficiently modeling, simulating, and analyzing these scanned large-scale real-world models become a serious challenge, because the numerical integration of high dimensional partial differential equations (over millions of degrees of freedom) is prohibitive for time-critical applications such as surgical simulation, bio-medical imaging, virtual/augmented reality, and physically-based animation.  The problem becomes significantly more acute in situations where the rest-shape geometries of the 3D models are frequently altered and there is a need for collision detection/response coupled with high fidelity visualization of heterogeneous material properties and efficient transmission over the network to facilitate collaborative interaction.  In this project the PI will address this challenge by developing a research program to create a modularized computational framework for efficient deformable simulation by partitioning the deformable body into small-size domains and re-connecting them back using weakened linkages.  Domain-level computations are independent and reusable; thus, the expensive deformable simulation is reframed as a plug-and-play computational assemblage just like playing with LEGO blocks, and orders of magnitude speedup can be obtained.  The plug-and-play deformable model that will be the primary project outcome will advance state-of-the-art techniques in physical simulation, animation and visualization, and will also profoundly benefit a broad range of interdisciplinary fields that directly impact people in their daily lives, from the modeling and registration of deformable human organs for surgical simulation, to the analysis of roadway pavement stress, to silent speech recognition.<br/><br/>The PI's approach pivots on the transformative concept of divide-and-conquer deformable model.  Unlike most state-of-the-art techniques that simulate a deformable object in its entirely by means of a ""one-stop"" solver, the PI will develop innovative algorithms that break  a simulation into independent computational modules, with the final result being obtained by incrementally assembling the local computations.  The PI will seek theoretical solutions to two general questions: ""how to smartly divide"" and ""how to effectively conquer"" in the context of deformable simulation.  In particular, he will investigates a theoretically grounded domain decomposition and coupling mechanism so that domain-level computation is independent, reusable, modularized and also a good fit with existing parallel computing architectures such as multi-core CPUs or GPUs.  The PI will develop a new theory for the real-time spectral deformation processing that divides the simulation not only spatially but also spectrally, based on a power iteration and inertia analysis.  He will also explore possible solutions to the problem of optimal domain partitioning, in which the simulation is parameterized geometrically and the most effective partition is obtained by solving a geometry optimization problem similar to the Voronoi diagram.  As the test-bed for the aforementioned theoretical and algorithmic advances, the PI will develop a haptic-enabled collaborative digital fabrication system, which will ultimately allow multiple users, from distant sites to smoothly interact to design and craft physically simulated virtual objects, which can then be 3D printed if desired."
89,1462990,A telescopic algorithm for two-dimensional hidden Markov models with application to genetic studies,DMS,NIGMS,7/15/15,7/15/15,Xiang-Yang Lou,AL,University of Alabama at Birmingham,Continuing Grant,Rosemary Renaut,3/31/16,"$771,862.00 ",,zjulxy219@yahoo.com,AB 1170,Birmingham,AL,352940001,2059345266,MPS,8047,"4075, 9150",$0.00 ,"Family-based genetic studies use a number of statistical techniques to understand how genetic information flows from parent to offspring. The investigators will develop computationally efficient algorithms for identifying genetic information from family data using a technique known as a Hidden Markov model. This model has demonstrated considerable success in a broad range of scientific disciplines, including areas such as speech recognition in telephone conversations and face identification from sequences of images. When the complexity of the problem increases, computational algorithms confront challenges with finding the solution of the model in realistic and practical time frames. The investigators' research on novel computational and statistical approaches will provide efficient algorithms, and software tools, of broad scientific relevance. In their work the tools developed will yield a more powerful approach for finding genes underlying complex diseases. The collaborative team will also directly apply their techniques to forestry genetic data describing a multi-year plant-breeding program.  Educationally, the trainees involved will be integrated into a vibrant interdisciplinary research team, gaining exposure to techniques for the solution of real world problems. <br/><br/>Multi-dimensional Markov processes are ubiquitous in the real world. Dependencies in interacting particle systems, images, videos, digitized documents, and gene transmission are all examples of multi-dimensional Markov processes. A Markov process is a random process (i.e., a statistical phenomenon in which the possible outcomes of a sequence of events or variables vary) in which the prediction of the future state is made just using the information of the current state, independent of both past history and unknown future states. This idea generalizes for higher dimensions.  For example, gene transmission from parent to child is a two-dimensional Markov process, the determination of the future state, the child, depends on the neighbors in two dimensions, the parents.  Most cases of interest, however, are hidden Markov processes, in which the current state of a variable cannot be observed, but must be inferred by consideration of all possible future states given all possible current states.  As the number of quantities to be determined increases, the computational demand in both computer memory and computing time increases dramatically. To apply the concept, therefore, to many important real applications requires the development of novel and powerful computational algorithms.  This project is focused on the design of a novel software package, specifically designed for the solution of hidden Markov models. The techniques developed will be relevant for the solution of problems from a broad array of disciplines, not only for wider family based genetic studies but also areas such as text analysis from images. Thus, this project will increase the computational ability to solve real-world problems across many engineering, geosciences and biological disciplines, with commensurate potential positive societal impact. In this case an interesting application arises in plant breeding through the collaboration with experts in forestry science."
90,1632985,A telescopic algorithm for two-dimensional hidden Markov models with application to genetic studies,DMS,NIGMS,10/1/15,2/23/16,Xiang-Yang Lou,LA,Tulane University,Continuing Grant,Junping Wang,4/30/17,"$771,862.00 ",,zjulxy219@yahoo.com,6823 ST CHARLES AVENUE,NEW ORLEANS,LA,701185698,5048654000,MPS,8047,"4075, 9150",$0.00 ,"Family-based genetic studies use a number of statistical techniques to understand how genetic information flows from parent to offspring. The investigators will develop computationally efficient algorithms for identifying genetic information from family data using a technique known as a Hidden Markov model. This model has demonstrated considerable success in a broad range of scientific disciplines, including areas such as speech recognition in telephone conversations and face identification from sequences of images. When the complexity of the problem increases, computational algorithms confront challenges with finding the solution of the model in realistic and practical time frames. The investigators' research on novel computational and statistical approaches will provide efficient algorithms, and software tools, of broad scientific relevance. In their work the tools developed will yield a more powerful approach for finding genes underlying complex diseases. The collaborative team will also directly apply their techniques to forestry genetic data describing a multi-year plant-breeding program.  Educationally, the trainees involved will be integrated into a vibrant interdisciplinary research team, gaining exposure to techniques for the solution of real world problems. <br/><br/>Multi-dimensional Markov processes are ubiquitous in the real world. Dependencies in interacting particle systems, images, videos, digitized documents, and gene transmission are all examples of multi-dimensional Markov processes. A Markov process is a random process (i.e., a statistical phenomenon in which the possible outcomes of a sequence of events or variables vary) in which the prediction of the future state is made just using the information of the current state, independent of both past history and unknown future states. This idea generalizes for higher dimensions.  For example, gene transmission from parent to child is a two-dimensional Markov process, the determination of the future state, the child, depends on the neighbors in two dimensions, the parents.  Most cases of interest, however, are hidden Markov processes, in which the current state of a variable cannot be observed, but must be inferred by consideration of all possible future states given all possible current states.  As the number of quantities to be determined increases, the computational demand in both computer memory and computing time increases dramatically. To apply the concept, therefore, to many important real applications requires the development of novel and powerful computational algorithms.  This project is focused on the design of a novel software package, specifically designed for the solution of hidden Markov models. The techniques developed will be relevant for the solution of problems from a broad array of disciplines, not only for wider family based genetic studies but also areas such as text analysis from images. Thus, this project will increase the computational ability to solve real-world problems across many engineering, geosciences and biological disciplines, with commensurate potential positive societal impact. In this case an interesting application arises in plant breeding through the collaboration with experts in forestry science."
91,1452831,Identifying Neurosensory Solutions to the Binding Problem in Animal Behavior,IOS,Activation,5/15/15,5/7/18,Mark Bee,MN,University of Minnesota-Twin Cities,Continuing Grant,Sridhar Raghavachari,4/30/21,"$680,000.00 ",,mbee@umn.edu,200 OAK ST SE,Minneapolis,MN,554552070,6126245599,BIO,7713,"9178, 9179",$0.00 ,"Human and animal behavior is guided by continuous and often complex sensory input. Nervous systems must parse this input stream and bind together those pieces corresponding to actual objects in the environment. As an illustration, consider the command to STOP. The human visual system effortlessly binds an octagonal shape with red coloration into a unified visual percept that elicits stopping behavior. Likewise, different sounds in the spoken word ""stop"" become bound into an auditory percept that also elicits stopping behavior. Efforts to understand how nervous systems solve these so-called ""binding problems"" have advanced the fields of cognitive and computational neuroscience. By comparison, much less is known about how nonhuman animals create bound percepts that correspond to the variety of things of interest to animals (e.g. prey, predators, mates, communication signals). Hence, important knowledge gaps remain concerning the brain mechanisms that allow animals to solve binding problems. This project integrates behavioral and electrophysiological experiments to uncover general mechanisms of auditory perceptual binding in an animal model for which vocal communication in noisy social environments is key to successful reproduction. This research is important because basic knowledge of neurosensory mechanisms that enable animals to solve auditory binding problems could benefit society by helping to improve hearing prosthetics and speech recognition systems, which perform poorly in noisy acoustic scenes. This research will also lead to answering new questions about how neural systems shape the evolution of communication behaviors. In addition, the project will create research experiences for a minimum of 15 undergraduates, advance the training of a postdoctoral scholar, and integrate research and teaching with public outreach aimed at elementary school kids in a large metropolitan area. <br/><br/>The project investigates auditory binding in green treefrogs (Hyla cinerea), a well-known animal model in studies of hearing and sound communication. Aim 1 uses behavioral experiments to identify cues that promote auditory binding. Two experiments will test the hypothesis that synchronous onsets/offsets, common spatial origin, and harmonic relatedness function to bind together separate parts of the frequency spectrum of vocalizations analogous to formants in human vowel sounds. Aim 2 involves electrophysiological recordings from single neurons in the auditory midbrain to identify neural correlates of auditory binding. Three experiments will test the hypothesis that changes in the responses of neurons sensitive to spectral combinations correlate with changes in the behavioral decisions made in response to manipulations of the auditory binding cues from Aim 1."
92,1623834,SHF: Medium: Collaborative Research: Ultra-Responsive Architectures for Mobile Platforms,CCF,Software & Hardware Foundation,7/1/15,1/27/16,Thomas Wenisch,MI,Regents of the University of Michigan - Ann Arbor,Continuing Grant,tao li,4/30/17,"$115,228.00 ",,twenisch@umich.edu,3003 South State St. Room 1062,Ann Arbor,MI,481091274,7347636438,CSE,7798,"7924, 7941",$0.00 ,"Conventional microprocessors are designed primarily for sustained<br/>performance; they can operate at near-peak performance essentially<br/>indefinitely until their energy source is exhausted. In battery and<br/>cooling constrained environments such as mobile devices, sustained power<br/>(and, consequently, peak performance) must be limited to at most a few<br/>watts so that the device can dissipate heat using only passive<br/>convection. However, many interactive mobile applications (such as<br/>handwriting/speech recognition or image-based search) instead call for<br/>bursts of intense computation in response to user input, creating the<br/>need for a new ultra-responsive operating regime: rather than limit peak<br/>power assuming sustained operation, systems should instead exploit heat<br/>storage to enable brief computation bursts that greatly exceed<br/>sustainable thermal limits without overheating.  This project<br/>investigates an approach called ""computational sprinting"", the central<br/>essence of which is to compute at unsustainable rates but only briefly<br/>so that temperatures do not reach unsafe levels.<br/><br/>The goal of this project is to address the architectural, thermal,<br/>electrical, and software barriers to ultra-responsiveness via<br/>computational sprinting. In particular, the project explores: (1)<br/>architectures and memory systems that sprint via parallelism (activating<br/>tens of reserve functional units/cores) and voltage boosting<br/>(overdriving cores for single-thread performance) while facilitating<br/>fast burst onsets; (2) thermal designs that improve thermal response<br/>behavior to enable longer and more intense sprints; (3) mobile-optimized <br/>electrical designs that provide stable supply voltages<br/>despite current surges of an order-of-magnitude or more; and <br/>(4) software mechanisms that explicitly manage limited thermal budgets <br/>and anticipate and stage data needed during the sprint.  The project<br/>includes the fabrication of an experimental testbed to approximate the<br/>computational, thermal and electrical capabilities of a future many-core<br/>mobile device and to provide practical experience with sprinting.<br/>Project impacts include (1) developing and advancing techniques for<br/>improving the responsiveness of mobile devices and (2) integrating the<br/>discoveries into a new cross-departmental course on computer system<br/>design."
93,1525162,SHF: Small: Bio-inspired ultra-broadband RF scene analysis,CCF,"Software & Hardware Foundation, IntgStrat Undst Neurl&Cogn Sys",7/15/15,7/8/15,Soumyajit Mandal,OH,Case Western Reserve University,Standard Grant,Sankar Basu,6/30/19,"$439,733.00 ",Michael Lewicki,soumyajit@ece.ufl.edu,"Nord Hall, Suite 615",CLEVELAND,OH,441064901,2163684510,CSE,"7798, 8624","7923, 7945, 8089, 8091",$0.00 ,"The detection and analysis of structured signals in noisy and cluttered environments is a fundamental problem in areas ranging from radio communications to image processing and speech recognition. Biological sensory systems have been optimized by millions of years of evolution to solve this problem with exquisite precision and efficiency; man-made communication and signal processing systems do not achieve anywhere near the same level of performance or even share similar fundamental design principles. This project will try to bridge this gap by understanding the universal information processing principles used by the auditory system to analyze natural sounds, and then adapting them to analyze man-made radio frequency (RF) signals. In particular, it will focus on developing electronics and algorithms that emulate some of the amazing capabilities of the biological cochlea (inner ear) and auditory pathway. Graduate and undergraduate students including members of underrepresented groups will be trained as part of this research, thus enlarging the technologically trained workforce of the future.<br/><br/>The bio-inspired approach of this project was motivated by two observations. Firstly, the process by which the auditory system, beginning with the cochlea, analyzes the fine time-frequency content of sounds is both extremely precise and also highly efficient from an algorithmic viewpoint. Secondly, audio and RF scenes are generated by similar physics (wave propagation, absorption, scattering, diffraction, and interference), even though the relevant velocities and time delays differ by a factor of about a million. Thus audio and RF scenes share many of the same characteristics, which makes it interesting to consider models of cochlear mechanics, signal transduction, and auditory coding that are scaled to operate at much higher frequencies.  The first research goal is to build a single-chip cochlear model that analyzes RF signals in the GHz range and encodes frequency, amplitude, and phase information into parallel event-driven outputs that are analogous to auditory nerve fibers. The second goal is to allow higher-level properties, such as source locations and categories, to be efficiently extracted from input signals by developing a robust coding framework to create compressed representations of the cochlear outputs."
94,1533569,Targeted Infusion Project: Development of a Knowledge-Based System for Integrating Artificial Intelligence into the Undergraduate Engineering Curriculum,HRD,Hist Black Colleges and Univ,8/1/15,7/24/15,Yachi Wanyan,TX,Texas Southern University,Standard Grant,Claudia Rankins,12/31/19,"$400,000.00 ",David Olowokere,wanyany@tsu.edu,3100 Cleburne Street,Houston,TX,770044501,7133137457,EHR,1594,9178,$0.00 ,"The Historically Black Colleges and Universities Undergraduate Program (HBCU-UP) through Targeted Infusion Projects supports the development, implementation, and study of evidence-based innovative models and approaches for improving the preparation and success of HBCU undergraduate students so that they may pursue STEM graduate programs and/or careers. The project at Texas Southern University seeks to infuse innovative electrical and computer engineering specialized artificial intelligence (AI) tools into traditional engineering problem-solving routines with a problem-based learning approach to bridge current curricula gaps, enhance engineering students' problem-solving and critical thinking skills, expose them to new technology, prepare them for diverse and multidisciplinary workforce requirements, and attract and encourage students to pursue professional engineering licensure or post-graduate studies in engineering field. The activities and strategies are evidence-based and a strong plan for formative and summative evaluation is part of the project.<br/><br/>There are five key objectives: 1) to develop an interactive and comprehensive intelligent database to document, compare, and analyze cutting-edge AI applications in the civil engineering field and use it as the platform and educational media for curricula development and implementation; 2) to develop one new interdisciplinary course  ""AI Tools for Engineering Problem Solving"" for senior engineering students; 3) to enrich current curricula by integrating innovative AI application case studies into twelve existing civil engineering junior and senior level courses; 4) to foster an interdisciplinary academic setting by hosting a server-based intelligent database; and 5) to support undergraduate students' early involvement in research. The project activities can serve as a model for other institutions that desire to strengthen undergraduate education in their engineering and technology programs."
95,1521921,Doctoral Mentoring Consortium at the 24th International Joint Conference on Artificial Intelligence (IJCAI 2015),IIS,ROBUST INTELLIGENCE,3/1/15,3/12/15,Maria Gini,MN,University of Minnesota-Twin Cities,Standard Grant,Hector Munoz-Avila,2/29/16,"$25,000.00 ",,gini@cs.umn.edu,200 OAK ST SE,Minneapolis,MN,554552070,6126245599,CSE,7495,"7495, 7556",$0.00 ,"The project funds attendance of US-based graduate students to the doctoral consortium at the International Joint Conference on Artificial Intelligence (IJCAI). The goals of doctoral consortium are for students to discuss research, have an opportunity to talk about their research, build professional connections with other researchers, and receive individual mentoring from a senior member of the community.  <br/><br/>The outcomes of the project will be assessed with pre- and post-surveys administered to the students.  Special efforts will be made to select students from underrepresented groups and from smaller groups who have fewer opportunities to interact with international researchers.  Participation will help graduate students network with other researchers, and develop into global scientists.  In the longer term, this will help advance science and high-tech industry."
96,1456093,SBIR Phase II:  A virtual role-playing simulation for social emotional learning using artificially intelligent characters and crowdsourcing,IIP,SMALL BUSINESS PHASE II,3/1/15,8/26/17,Geoff Marietta,MA,"Giant Otter Technologies, Inc.",Standard Grant,Rajesh Mehta,5/31/18,"$1,165,984.00 ",,geoff@giantotter.com,212 Elm St,Somerville,MA,21442959,4102946786,ENG,5373,"110E, 116E, 165E, 169E, 5373, 8031, 8032, 8042, 9102, 9251",$0.00 ,"This SBIR Phase II project will develop novel 3D web and mobile role-playing simulations that can improve social skills in children. Virtual characters participate in nuanced conversations with students, powering a transformational experience for practicing and assessing communication skills.  Rigorous evaluations have demonstrated the feasibility and efficacy of teaching perspective-taking and social skills through simulated role-playing, beneficial to students struggling with autism spectrum disorder (ASD), attention deficit hyperactivity disorder (ADHD), or bullying.  These simulations will disrupt the multi-billion dollar market for social emotional learning (SEL) by allowing students to learn and practice social skills as they would with teachers or therapists, but at a fraction of the cost.  Current solutions for practicing and assessing social skills rely on face-to-face interaction, which is costly and difficult to scale.  Producing socially rich, open-ended simulations has previously been infeasible, due to technical challenges required to replicate the complexity of human language.  This project overcomes these barriers using an innovative technology with the potential to revolutionize how people learn and practice social skills by delivering real-time personalized feedback at scale.  Future applications of the social behavior capture (So-Cap) technology that underlies this project range from contextually aware intelligent personal assistants to socially aware robotics.<br/> <br/>This project's unprecedented ability to respond authentically to open-ended natural language input relies on an innovative crowdsourced approach to artificial intelligence (AI), which imbues machines with the ability to understand dialogue in context and engage in extended conversations covering multiple topics.  Individualized responses are tailored to user input by drawing from a massive database of recorded human dialogue, captured from online role-playing.  The process for mining meaningful patterns from this data also employs crowdsourcing, relying on non-experts hired online to cluster and annotate words, utterances, and events.  A computational model formed by these patterns powers a real-time conversational engine, which selects responses by combining AI techniques including plan recognition and case-based planning.  The goal of this research is to pioneer a practical, repeatable process for democratizing the production of social simulations, minimizing cost while maximizing societal impact by providing social skills practice at scale."
97,1458272,IRES: Avatar-based Adaptive Context System,OISE,IRES Track I: IRES Sites (IS),6/15/15,10/16/18,Avelino Gonzalez,FL,The University of Central Florida Board of Trustees,Standard Grant,Anne Emig,5/31/20,"$231,974.00 ",Annie Wu,gonzalez@ucf.edu,4000 CNTRL FLORIDA BLVD,Orlando,FL,328168005,4078230387,O/D,7727,"5936, 5980, 7727",$0.00 ,"Part 1<br/>This interdisciplinary partnership between the University of Central Florida and the Fraunhofer Institute for Digital Media Technology (FIDMT) in Ilmenau, Germany is focused on adaptive conversational avatars, the rapidly emerging field crossing computer engineering, computer science, education, communications, and social science.  Immediate applications of this research field include artificial intellegence and national security (including cyber-security), interactive robotics, improvement of quality of life for disbaled, and health and caretaking for children and elderly.<br/>This project will place students from the University of Central Florida under the mentorship of the PI (Dr. A. J. Gonzalez) and of Dr. Klaus Jantke, the counterpart at FIDMT in Ilmenau, Germany. Dr. Jantke is the director of the Children's Media Department of FIDMT located in Erfurt, Germany and has a long and illustrious history in research in computing media. The international aspect of innovative and advanced research is essential in modern research hence the PIs will work with three cohorts of students, one during each year of the project's existence. Each cohort will include one graduate student and either two undergraduates (the first year) or four (in each of the subsequent years).  The research period for each cohort will be 16 weeks - eight weeks in the US and eight weeks in Germany for each year of the grant period.<br/>This research project is motivated by an ancient art of storytelling. In our pursuit of an artificially intelligent computer agent, the IRES project seeks to build a capability to autonomously synthesize possible scenarios for the system development and to modify them dynamically upon listener request. More specifically, the topic of the research in this project is the creation of an avatar-based system that can synthesize and adapt a scenario according to the user's request in real time and without any pre-scripted pathways. Good storytellers were treasured in medieval times, given the lack of other media through which to relate a story to a mostly illiterate population. Therefore, the project seeks to embody the storyteller in a lifelike avatar that resembles an actual person. This avatar will tell the story to the listener in spoken natural language, and interact with her/him when the latter requests changes to the story. <br/><br/>Part 2<br/>Storytelling media have evolved over time, from oral stories to modern E-books. Since the development of the computer, storytelling systems have become a science of their own, and have evolved from simple systems that can only generate a single short story to systems that respond to the listener's actions by modifying the story dynamically in real time. Digital storytelling has therefore become a growing field within artificial intelligence. The project seeks to take this evolution of storytelling media one step further by doing research to create a virtual storyteller who tells a dynamic story. The story is modifiable through a request by the listener (typically a child, a student, or an elderly person), yet will seek to remain realistic as well as interesting. Every story has a story space. That is, only so many things can happen in a story. We use contextual reasoning to represent the story space. In the real world, courses of action are influenced by the current context, making some conversational avatars very attractive while others unattractive when addressing the current situation within the story space. In a similar manner, the situation faced by the protagonist in the dynamic scenario will limit the choices of actions that he/she would otherwise have, thereby taking the story in various directions, none of which need be specifically pre-scripted.  The PIs base the proposed research on the use of formal methods to manipulate the story space within the main theme of the story. By formal methods the PIs mean that one represents the story knowledge formally in terms of strings, interaction sequences such as storyboards and graphs, formulas (for conditions), and the like. Formal methods, therefore, will give the ability to reason with formal methods (string comparison, unification, anti-unification and the like) in the story space. Formal methods have been used in the literature to manipulate contextual information."
98,1534190,SBIR Phase II:  Assistive Digital Vision for the Blind,IIP,SBIR Phase II,9/15/15,9/14/15,Arman Ghodousi,VA,Ghodousi LLC,Standard Grant,Muralidharan Nair,8/31/17,"$730,331.00 ",,arman@g-technologygroup.com,5702 General Washington DR.,Alexandria,VA,223120000,4805443192,ENG,5373,"5373, 6840, 8035, 9139, HPCC",$0.00 ,"The broader impact/commercial potential of this project is to address the unmet needs of blind and visually impaired population in order to increased their mobility independence with the aid of an intelligent wearable electronic device, compatible with the white cane, in a simple and cost-effective manner. The outcome of this project will contribute to scientific and technological understanding of environmental sensing and process automation by developing an integrated system comprised of sensors and underlying algorithms capable of gathering, processing, and interpreting environmental data and communicating relevant information to the user. The envisioned product can potentially meet the needs of 2.1% of the U.S. population who are considered legally blind, as well as make further contribution to the field of robotics, artificial intelligence, and other assistive technologies wherein a better understanding of the environment is desired.          <br/><br/>This Small Business Innovation Research (SBIR) Phase 2 project seeks to create a state of the art wearable device for the visually impaired that will enhance their situational awareness through the use of multiple sensors and intelligent algorithms. The main research objective is to develop an intelligent  wearable device that will provide ability to detect above ground obstacles and recognize various categories of objects. In spite of significant technological breakthroughs, leading to many product innovations, there is very little technological progress for the visually impaired despite increased population longevity and significant demand for products that promote independent living. The proposed project will address the needs of the visually impaired in an intuitive and automated manner."
99,1444182,CNIC: U.S.-Netherlands Planning Visit for Cooperative Research on Intelligent Methods Under Uncertainty for Renewable Energy Driven Smart Grids,OISE,Catalyzing New Intl Collab,5/1/15,4/17/15,Prashant Doshi,GA,University of Georgia Research Foundation Inc,Standard Grant,Anne Emig,4/30/17,"$33,608.00 ",,pdoshi@cs.uga.edu,310 East Campus Rd,ATHENS,GA,306021589,7065425939,O/D,7299,"5948, 5979, 8231, 8399",$0.00 ,"This new, catalytic U.S.-Netherlands research collaboration addresses renewable energy-driven smart grids. Renewable energy sources include resources that are regularly replenished, such as sunlight, wind, rain, tidal waves, and geothermal heat.  To pursue innovative approaches for managing the uncertainty of renewable energy sources, the U.S. principal investigator (PI) and a graduate student will visit the Netherlands to begin a collaboration with counterparts at the Delft University of Technology, a leader in European smart energy research.  There they intend to work together to improve current smart grid technology for better prediction of consumer demand in the face of uncertain power generation, as is often the case in renewable energy systems.  If successful, their preliminary results should contribute to improving bidirectional communication between grid operators and consumers.  Early results and follow-on research may have broader impact by shaping management strategies through new approaches to modeling consumer energy usage.  Success could mean better long-term prediction by employing new artificial intelligence approaches (AI), i.e., smart controls for power grids.<br/><br/>The team expects to identify the challenges posed by the uncertainty of renewable energy generation and begin investigating intelligent methods for meeting these challenges in two priority areas: (a) planning for decentralized power generation and storage, and (b) managing congestion in grids due to asynchrony between renewable energy supply and consumer demand.  The PI will work with an experienced team of eminent Dutch researchers in AI, power systems, and technology policy.  They will have real operating and energy-use data from a medium voltage grid in Netherlands and intend to start developing scalable algorithms for individual decision making in multi-agent settings.  Further, broader impacts are anticipated from this collaboration with an introduction of smart energy systems into research and teaching at the University of Georgia, thereby contributing to training U.S. undergraduate and graduate students in an innovative and rapidly growing energy sector with industrial relevance."
100,1514490,CHS: Medium: Transforming Scientific Presentations with Co-Presenter Agents,IIS,HCC-Human-Centered Computing,7/1/15,4/17/20,Timothy Bickmore,MA,Northeastern University,Continuing Grant,Ephraim Glinert,6/30/21,"$1,164,306.00 ",Harriet Fell,bickmore@ccs.neu.edu,360 HUNTINGTON AVE,BOSTON,MA,21155005,6173733004,CSE,7367,"7367, 7924",$0.00 ,"Although journal and conference articles are recognized as the most formal and enduring forms of scientific communication, oral presentations are central to science because they are the means by which researchers, practitioners, the media, and the public hear about the latest findings thereby becoming engaged and inspired, and where scientific reputations are made.  Yet despite decades of technological advances in computing and communication media, the fundamentals of oral scientific presentations have not advanced since software such as Microsoft's PowerPoint was introduced in the 1980's.  The PI's goal in this project is to revolutionize media-assisted oral presentations in general, and STEM presentations in particular, through the use of an intelligent, autonomous, life-sized, animated co-presenter agent that collaborates with a human presenter in preparing and delivering his or her talk in front of a live audience.   The PI's pilot studies have demonstrated that audiences are receptive to this concept, and that the technology is especially effective for individuals who are non-native speakers of English (which may be up to 21% of the population of the United States).  Project outcomes will be initially deployed and evaluated in higher education, both as a teaching tool for delivering STEM lectures and as a training tool for students in the sciences to learn how to give more effective oral presentations (which may inspire future generations to engage in careers in the sciences).<br/><br/>This research will be based on a theory of human-agent collaboration, in which the human presenter is monitored using real-time speech and gesture recognition, audience feedback is also monitored, and the agent, presentation media, and human presenter (cued via an intelligent wearable teleprompter) are all dynamically choreographed to maximize audience engagement, communication, and persuasion.  The project will make fundamental, theoretical contributions to models of real-time human-agent collaboration and communication.  It will explore how humans and agents can work together to communicate effectively with a heterogeneous audience using speech, gesture, and a variety of presentation media, amplifying the abilities of scientist-orators who would otherwise be ""flying solo.""  The work will advance both artificial intelligence and computational linguistics, by extending dialogue systems to encompass mixed-initiative, multi-party conversations among co-presenters and their audience.  It will impact the state of the art in virtual agents, by advancing the dynamic generation of hand gestures, prosody, and proxemics for effective public speaking and turn-taking.  And it will also contribute to the field of human-computer interaction, by developing new methods for human presenters to interact with autonomous co-presenter agents and their presentation media, including approaches to cueing human presenters effectively using wearable user interfaces."
101,1515258,EAPSI: Identifying Relations between Computer-Generated and Manually Annotated Interpretations of Activities for Planning and Plan Recognition Tasks,OISE,EAPSI,6/1/15,6/3/15,Richard Freedman,MA,Freedman                Richard        G,Fellowship,Anne Emig,5/31/16,"$5,070.00 ",,,,Amherst,MA,10021372,,O/D,7316,"5921, 5978, 7316",$0.00 ,"While humans perceive activities using words and cluster similar activities by some properties, computers using unsupervised learning algorithms do not necessarily identify them the same way and thus generate different interpretations. The goal of this research is to develop an analogy between human and machine definitions of activities so that artificial intelligence planning and plan recognition methods do not need to be adjusted for each set of definitions. Usually, either humans define the activities in a way which is too vague for machines to make accurate computations or computers define activities such that people cannot understand the underlying reasoning. Developing an interpreter for each entity will not only smooth the interaction between users and devices when solving problems together, but also contribute to bridging the human-computer gap. This project will use techniques in the research areas of interest to Dr. Alex Fukunaga at the University of Tokyo who studies many facets of artificial intelligence, especially those regarding autonomous planning, search, and optimization.<br/><br/>Extending prior research on unsupervised activity recognition using topic models, this work proposes a two-step approach consisting of constraint optimization and heuristic search. The first phase uses constraint optimization to align a computer's recognized activity sequence with a given human's annotation of the same sequence. Then the second phase similarly aligns a computer's recognized activity sequence with an annotation of the sequence derived by heuristic search over a human-defined hierarchical task network. To develop these methods, the project will include formalizing the constraint and search problems, coding their formulations, and testing results of the identified mappings between the two definition sets. This test will be performed by combining an unsupervised activity recognition method previously developed by the PI for machine-interpreted actions with a commonly used plan recognition method that uses human-perceived action representations. This NSF EAPSI award is funded in collaboration with the Japan Society for the Promotion of Science."
102,1527434,RI: Small: New Directions in Computational Social Choice and Mechanism Design,IIS,Robust Intelligence,9/1/15,8/3/15,Vincent Conitzer,NC,Duke University,Standard Grant,James Donlon,8/31/19,"$499,972.00 ",,conitzer@cs.duke.edu,"2200 W. Main St, Suite 710",Durham,NC,277054010,9196843030,CSE,7495,"7495, 7923",$0.00 ,"We often need to make collective decisions: who will represent us as president, where will we all go out for dinner tonight, who will receive the award, and so on. Similar problems are faced in multiagent systems in artificial intelligence. What are the best procedures for reaching such decisions? The agents could vote over the outcome, but what should the exact procedure be? This is studied in the theories of social choice and mechanism design, with the latter focusing particularly on agents that act strategically in their own self-interest. However, an ever-increasing amount of activity is moving online, and collective decision making is no exception. For example, we rate or vote on content, products, and people online. Key aspects of these novel applications are not present in the traditional models of social choice and mechanism design. For example, in an Internet-based mechanism, who gets to and who will participate? How can we know that a single agent is not participating multiple times? How can we allow agents to meaningfully participate when the number of voting events is potentially overwhelming? The proposed research aims to extend the traditional models to incorporate these aspects and to develop new algorithmic and other techniques to ensure outcomes are meaningful and increase economic efficiency and human welfare.<br/><br/>In many domains, multiple self-interested agents need to make a collective decision. The theory of social choice concerns how such collective decisions should be made. Closely related, the theory of mechanism design concerns how to design mechanisms for such problems that result in good outcomes even when agents behave strategically. In recent years, major progress has been made on understanding the computational aspects of both social choice and mechanism design. In the proposed research, the PI and his team set out to adapt these techniques to novel domains such as those enabled by the Internet or the availability of new data. For example, one key issue is the identity of the participating agents. In an Internet-based mechanism, who gets to and who will participate? How can we know that a single agent is not participating multiple times? The PI and his team aim to address such issues with techniques based on social network structure, as well as on the effort that agents expend participating. Another key issue is the possibility of agents strategically providing inaccurate data. Again, explicit modeling of the agents' effort costs in doing so will play a key role."
103,1543845,Symposium on Combinatorial Search - 2015,IIS,ROBUST INTELLIGENCE,6/15/15,6/23/15,Richard Korf,CA,University of California-Los Angeles,Standard Grant,James Donlon,11/30/16,"$5,000.00 ",,korf@cs.ucla.edu,10889 Wilshire Boulevard,LOS ANGELES,CA,900951406,3107940102,CSE,7495,"7495, 7556",$0.00 ,"This grant is to provide partial support for U.S.-based graduate and undergraduate students to attend the Eighth Symposium on Combinatorial Search (SoCS), a scientific conference to be held at the Dead Sea in Israel from June 11-15, 2015.  The general field of the meeting is computer science.  Combinatorial search is an area of computer science that deals with systematic trial-and-error exploration of a very large number of alternative solutions to a problem.  For example, a package delivery truck needs to visit all the addresses to which packages must be delivered, and return to its home base, ideally via the shortest possible route.  NSF funding is crucial to support students who would otherwise not be able to attend the symposium.  Attending such meetings and presenting their research is an important part of the professional development of students, addressing a critical shortage of highly-skilled computer scientists in the U.S.<br/><br/>SoCS brings together researchers in heuristic search and combinatorial optimization from all areas of artificial intelligence, planning, robotics, constraint programming, operations research, bioinformatics, and computer games.  The intellectual merit of this activity stems from bringing together in one place at one time researchers from otherwise diverse areas of computer science that both advance the state of the art in heuristic search and/or combinatorial optimization, and also use these tools in their research.  The broader impacts come from cross-fertilization of different fields that advance and/or use these tools, by promoting research in this area by providing a small intimate meeting on these topics, a forum for presenting such work, and archival proceedings for publishing work in this area.  These latter goals are instrumental to training new researchers in this area."
104,1514253,RI: Medium: Sentential Decision Diagrams,IIS,"Robust Intelligence, Unallocated Program Costs",7/1/15,5/15/18,Adnan Darwiche,CA,University of California-Los Angeles,Continuing Grant,James Donlon,6/30/19,"$705,865.00 ",,darwiche@cs.ucla.edu,10889 Wilshire Boulevard,LOS ANGELES,CA,900951406,3107940102,CSE,"7495, 9199","7495, 7924, 9251",$0.00 ,"Logical and probabilistic reasoning are now routinely used in various fields of computer science and engineering, including artificial intelligence in particular. These modes of reasoning currently underlie systems that perform automated diagnosis, planning, software and hardware verification, web information extraction, bioinformatics, vision and robotics. This project aims at advancing the state of the art in logical and probabilistic reasoning, to allow scientists and engineers to learn and reason with much larger models than is currently possible. The project is based on a particular computational paradigm, known as knowledge compilation, which transforms knowledge into forms that facilitate their efficient processing by reasoning and learning algorithms. The results expected from this project will provide domain-independent, highly scalable, tools and techniques for addressing computational problems that arise in healthcare, industrial automation, and information management. The project will also provide a context for training graduate students in the computational paradigm of knowledge compilation, and will target the integration of this paradigm into computer science curricula.<br/><br/>More specifically, the project aims to develop a new framework for knowledge compilation based on the recently discovered Sentential Decision Diagram (SDD). The SDD is a target compilation language, which generalizes the Ordered Binary Decision Diagram (OBDD) that has been quite influential in many areas of computer science and engineering. This project has two parts. The first part is concerned with developing the SDD compilation language further, both theoretically and practically. On the theoretical side, there is a number of pending of questions relating to lower and upper bounds on SDDs, in addition to questions that must be answered to fully understand their relation to OBDDs. On the practical side, the SDD package needs to be extended to enhance its scalability and to provide new functionality that is needed for fully exploiting SDDs in a wider spectrum of applications. The second part of the project is concerned with a more recent discovery: The probabilistic SDD (PSDD). This compilation language aims at inducing probability distributions over propositional theories, in a very principled and efficient manner. Our objective here is to develop PSDDs into a mature tool, with a corresponding public package, for learning tractable probabilistic models under massive logical constraints, and for compiling probabilistic graphical models into PSDDs for the purpose of more scalable probabilistic reasoning.<br/>"
105,1456817,Collaborative Research: Navigation and the Neural Integration of Multimodal Sensory Information in the Brain of an Arthropod,IOS,"AISL, Animal Behavior",8/1/15,4/1/16,Eileen Hebets,NE,University of Nebraska-Lincoln,Standard Grant,Patrick Abbot,7/31/21,"$285,215.00 ",,ehebets2@unl.edu,151 Prem S. Paul Research Center,Lincoln,NE,685031435,4024723171,BIO,"7259, 7659","7259, 7659, 9178, 9179, 9251, SMET",$0.00 ,"The ability of animals to navigate through their environment often far exceeds human capabilities (without the help of technology). Exceptional navigation is not limited to animals with large brains, like birds and mammals. It can also be found in animals with simpler nervous systems. The tropical amblypygid, a scorpion-like animal, is able to find its way home at night over distances exceeding 10 meters through dense, tropical forest understory.  The study of how different types of sensory information (visual, chemical, tactile) are processed by amblypygids as they solve navigation problems can reveal fundamental design properties of simple nervous systems that are somehow capable of controlling complex, learned behavior. These design properties can inspire engineering solutions applicable to robotic and artificial intelligence systems. The study of charismatic tropical amblypygids also serves as an alluring gateway for teachers to introduce K-12 students to the importance of neuroscience for understanding how organisms acquire and process information from their environment and how this information influences learning, memory and associated behavior. To support engagement with K-12 students, their teachers and the general public, researchers will, among other activities, develop internet-based educational materials in both English and Spanish and develop various scientific inquiry activities for science events. <br/><br/>By conducting behavioral experiments that assess amblypygid (Phrynus pseudoparvulus) movements after they are displaced from a home refuge, researchers will assess the relative importance of visual, chemical and mechanical information in supporting navigation. These experiments will either involve manipulation of animal sense organs or the sensory cues in their environment. The neurobiological work will focus on a brain area known as the ""mushroom bodies"", which are thought to support spatial memory. In parallel with the behavioral work, researchers will explore the nervous system routes by which information from different sensory stimuli is sent to the mushroom bodies. Particular attention will be given to how the mushroom bodies ""engineer"" or ""integrate"" the different sensory inputs.  The integration of sensory inputs is hypothesized to be necessary to support complex navigation and will likely be crucial for the design of any sophisticated artificial system. Finally, the importance of the mushroom bodies in navigation, and their capacity to combine different sources of sensory information, will be tested under the same conditions of the behavioral experiments noted above, except using animals whose mushroom bodies are impaired."
106,1456221,Collaborative Research: Navigation and the Neural Integration of Multimodal Sensory Information in the Brain of an Arthropod,IOS,Animal Behavior,8/1/15,7/24/15,Wulfila Gronenberg,AZ,University of Arizona,Standard Grant,Jodie Jawor,7/31/20,"$365,000.00 ",,wulfi@neurobio.arizona.edu,888 N Euclid Ave,Tucson,AZ,857194824,5206266000,BIO,7659,"7659, 9179",$0.00 ,"The ability of animals to navigate through their environment often far exceeds human capabilities (without the help of technology). Exceptional navigation is not limited to animals with large brains, like birds and mammals. It can also be found in animals with simpler nervous systems. The tropical amblypygid, a scorpion-like animal, is able to find its way home at night over distances exceeding 10 meters through dense, tropical forest understory.  The study of how different types of sensory information (visual, chemical, tactile) are processed by amblypygids as they solve navigation problems can reveal fundamental design properties of simple nervous systems that are somehow capable of controlling complex, learned behavior. These design properties can inspire engineering solutions applicable to robotic and artificial intelligence systems. The study of charismatic tropical amblypygids also serves as an alluring gateway for teachers to introduce K-12 students to the importance of neuroscience for understanding how organisms acquire and process information from their environment and how this information influences learning, memory and associated behavior. To support engagement with K-12 students, their teachers and the general public, researchers will, among other activities, develop internet-based educational materials in both English and Spanish and develop various scientific inquiry activities for science events. <br/><br/>By conducting behavioral experiments that assess amblypygid (Phrynus pseudoparvulus) movements after they are displaced from a home refuge, researchers will assess the relative importance of visual, chemical and mechanical information in supporting navigation. These experiments will either involve manipulation of animal sense organs or the sensory cues in their environment. The neurobiological work will focus on a brain area known as the ""mushroom bodies"", which are thought to support spatial memory. In parallel with the behavioral work, researchers will explore the nervous system routes by which information from different sensory stimuli is sent to the mushroom bodies. Particular attention will be given to how the mushroom bodies ""engineer"" or ""integrate"" the different sensory inputs.  The integration of sensory inputs is hypothesized to be necessary to support complex navigation and will likely be crucial for the design of any sophisticated artificial system. Finally, the importance of the mushroom bodies in navigation, and their capacity to combine different sources of sensory information, will be tested under the same conditions of the behavioral experiments noted above, except using animals whose mushroom bodies are impaired."
107,1457304,Collaborative Research: Navigation and the Neural Integration of Multimodal Sensory Information in the Brain of an Arthropod,IOS,Animal Behavior,8/1/15,7/24/15,Verner Bingman,OH,Bowling Green State University,Standard Grant,Patrick Abbot,7/31/21,"$270,000.00 ",Daniel Wiegmann,vbingma@bgsu.edu,302 Hayes Hall,Bowling Green,OH,434030230,4193722481,BIO,7659,"7659, 9178, 9179, 9251, SMET",$0.00 ,"The ability of many animals to navigate through their environment far exceeds what humans are able to do without the help of technology. Exceptional navigation is not limited to animals with large brains, like birds and mammals. It can also be found in animals with simpler nervous systems. The tropical amblypygid, a scorpion-like animal, is able to find its way home at night through dense, tropical forest understory.  The study of how different types of sensory information (visual, chemical, tactile) are processed by amblypygids as they solve navigation problems can reveal fundamental design properties of simple nervous systems that are somehow capable of controlling complex, learned behavior. These design properties can inspire engineering solutions applicable to robotic and artificial intelligence systems. The study of charismatic tropical amblypygids also serves as an alluring gateway for teachers to introduce K-12 students to the importance of neuroscience for understanding how organisms acquire and process information from their environment and how this information influences learning and memory. To support engagement with K-12 students, their teachers and the general public, researchers supported by this grant will develop internet-based educational materials in both English and Spanish. <br/><br/>By conducting behavioral experiments that assess amblypygid (Phrynus pseudoparvulus) movements after displacement from a home refuge, researchers will assess the relative importance of visual, chemical and mechanical information in supporting navigation. These experiments will either involve manipulation of animal sense organs or the sensory cues in their environment. The neurobiological work will focus on a brain area known as ""mushroom bodies"", which are thought to support spatial memory. In parallel with the behavioral work, researchers will explore the nervous system routes by which information from different sensory stimuli is sent to the mushroom bodies. Particular attention will be given to how the mushroom bodies ""engineer"" or ""integrate"" the different sensory inputs.  The integration of sensory inputs is hypothesized to be necessary to support complex navigation and will likely have applied potential for design of sophisticated artificial systems. Finally, the importance of the mushroom bodies in navigation, and their capacity to combine different sources of sensory information, will be tested under the same conditions of the behavioral experiments noted above, except using animals whose mushroom bodies are impaired."
108,1523316,CAP: Doctoral Consortium for the 2015 Learning Analytics and Knowledge Conference,IIS,Cyberlearn & Future Learn Tech,4/1/15,3/30/15,Stephanie Teasley,MI,Regents of the University of Michigan - Ann Arbor,Standard Grant,Tatiana Korelsky,3/31/17,"$18,280.00 ",,steasley@umich.edu,3003 South State St. Room 1062,Ann Arbor,MI,481091274,7347636438,CSE,8020,"7556, 8045, 8055",$0.00 ,"Using data analytics has revolutionized many academic disciplines, such as Astrophysics and Biology. In addition, it has changed the commercial world and critical services like healthcare. The field of education research is also beginning this change. The Society for Learning Analytics Research (SoLAR) is a key player in effecting this change. This project will support a Doctoral Consortium at the SoLAR 2015 conference - the 5th Annual International Learning Analytics and Knowledge (LAK) Conference, to be held in Poughkeepsie, NY in March 2015. Participating in the consortium will prepare current doctoral students from the diverse research backgrounds that make up the interdisciplinary field of Learning Analytics (LA), including computer science, information science, psychology, communication, education, artificial intelligence, and cognitive science. It will also allow them to network with each other and with professors and practitioners who are currently engaged in learning analytics research and related work to ensure the continued development of this community. These activities are critical and timely. The upcoming generation of LA researchers will play a vital role in realizing the potential of using data to improve outcomes for elementary, secondary, and post-secondary students in learning, motivation, and perseverance.<br/><br/>Learning Analytics is an interdisciplinary field whose goal is to advance and apply knowledge about learning sciences and education to improve all aspects of learning. LA methods include data visualization, data mining, data science, and mixed methods approaches combining qualitative and quantitative methods (e.g., interviews and back-end, clickstream data analysis). Capacity building is a central concern within the LA community. SoLAR has historically addressed these needs, in part, through specialized workshops held in conjunction with the Society's major conference. The Doctoral Consortium workshops host PhD students who are grappling with their dissertation research. This grant provides travel support to US scholars selected through a competitive application process to participate in this event. They present their work for feedback both in the context of workshop, where they receive advice from a panel of expert mentors, and in a poster session in which they interact with the full conference audience. In addition, their work is published in the proceedings of the conference."
109,1519252,Support for the ICAPS-15 Doctoral Consortium,IIS,Robust Intelligence,2/1/15,1/28/15,Sven Koenig,CA,Association for the Advancement of Artificial Intelligence,Standard Grant,James Donlon,1/31/17,"$16,000.00 ",,skoenig@usc.edu,2275 E BAYSHORE RD STE 160,East Palo Alto,CA,943033224,6503283123,CSE,7495,"7495, 7556",$0.00 ,"The proposal requests travel, housing funding and conference registration for US-based doctoral students selected to participate in the Doctoral Consortium (DC). The DC is co-located with The 25th International Conference on Automated Planning and Scheduling (ICAPS), which will be held in Jerusalem in May 4-8, 2015. ICAPS is the main international conference on automated planning. The ICAPS DC aims at broadening the participating of US-based PhD students into the planning area as well as improving the retention of these students. <br/>"
110,1522485,WORKSHOP: The 2015 HRI Pioneers Workshop at the 2015 ACM/IEEE International Conference on Human-Robot Interaction,IIS,HCC-Human-Centered Computing,3/1/15,2/2/15,Matthias Scheutz,MA,Tufts University,Standard Grant,Ephraim Glinert,8/31/15,"$31,600.00 ",,matthias.scheutz@tufts.edu,136 Harrison Ave,Boston,MA,21111817,6176273696,CSE,7367,"7367, 7556",$0.00 ,"This is funding to support a Pioneers Workshop (doctoral consortium) of approximately 18 graduate students and post-docs from diverse research communities (e.g., computer science and engineering, psychology, cognitive science, robotics, human factors, human-computer interaction design, and communications), along with distinguished research faculty.  The event will take place on Monday, March 2, 2015, immediately preceding the Tenth International Conference on Human Robot Interaction (HRI 2015), to be held March 2-5 in Portland, Oregon, and which is jointly sponsored by ACM and IEEE.  HRI is a single-track, highly selective annual international conference that seeks to showcase the very best inter- and multi-disciplinary research in human-robot interaction with roots in social psychology, cognitive science, HCI, human factors, artificial intelligence, robotics, organizational behavior, anthropology and many more, and invites broad participation.  The theme of HRI 2015 is ""Broadening HRI: Enabling Technologies, Designs, Methods, and Knowledge"" which seeks contributions from a broad set of perspectives, including technical, design, methodological, behavioral, and theoretical, that advance fundamental and applied knowledge and methods in human-robot interaction, with the goal of enabling human-robot interaction through new technical advances, novel robot designs, new guidelines for design, and advanced methods for understanding and evaluating interaction.  More information about the conference is available online at http://humanrobotinteraction.org/2015. <br/><br/>The Pioneers Workshop is designed to complement the conference, by providing a forum for students and recent graduates in the field of HRI to share their current research with their peers and a panel of senior researchers in a setting that is less formal and more interactive than the main conference.  During the workshop, participants will talk about the important upcoming research themes in the field, encouraging the formation of collaborative relationships across disciplines and geographic boundaries.  To these ends, the workshop format will encompass a variety of activities including two keynotes, a distinguished panel session, and breakout sessions.  To start the day, all workshop attendees will briefly introduce themselves and their interests.  Following the opening keynote, approximately half of the participants will present 3-minute overviews of their work, leading into an interactive poster session.  This will enable all participants to share their research and receive feedback from students and senior researchers in an informal setting.  The workshop organizers will facilitate the post-presentation discussion and will encourage participants to ask questions of their peers during the interactive break and poster session.  After lunch, the remaining workshop participants will give their 3-minute overviews, followed by presentation of their posters during a second interactive poster session.  Senior researchers (in addition to those on the panel) will be invited to attend the student presentations and poster sessions in order to provide feedback to participants, and workshop participants will be invited to present their posters during the main poster session of the HRI conference as well.  The conversations between the panel and participants will continue over lunch and during dinner.<br/><br/>This workshop will afford a unique opportunity for the best of the next generation of researchers in human-robot interaction to be exposed to and discuss current and relevant topics as they are being studied in several different research communities (including but not limited to computer science and engineering, psychology, robotics, human factors and ergonomics, and HCI).  This is important for the field, because it has been recognized that transformative advances in research in this fledgling area can only come through the melding of cross-disciplinary knowledge and multinational perspectives.  Participants will be encouraged to create a social network both among themselves and with senior researchers at a critical stage in their professional development, to form collaborative relationships, and to generate new research questions to be addressed during the coming years.  Participants will also gain leadership and service experience, as the workshop is largely student organized and student led.  The PI has expressed his strong commitment to recruiting women and members from under-represented groups.  To further ensure diversity the event organizers will consider an applicant's potential to offer a fresh perspective and point of view with respect to HRI, will recruit students who are just beginning their graduate degree programs in addition to students who are further along in their degrees, and will limit the number of participants accepted from a particular institution to at most two (in which case, one of the participants must be female)."
111,1545599,EAGER: Mobile Solutions for Multifold Increase of Survival Rates Through High Quality Chest Compressions,IIS,Smart and Connected Health,8/1/15,7/23/15,Ram Dantu,TX,University of North Texas,Standard Grant,Wendy Nilsen,7/31/18,"$140,845.00 ",,rdantu@unt.edu,1155 Union Circle #305250,Denton,TX,762035017,9405653940,CSE,8018,"7916, 8018",$0.00 ,"It is estimated that approximately 600,000 people each year in the United States experience a cardiac arrest, and survival rates for arrests that occur in community settings are less than 6%. On June 30, 2015, the Institute of Medicine released a report on strategies for increasing survival rates after cardiac arrest.  The report calls for effective treatment, demanding an immediate response from bystanders to recognize cardiac arrest, call 911, and initiate cardiopulmonary resuscitation (CPR). Given that the time interval for Emergency Medical Service (EMS) arrival is often 7 to 8 minutes or longer, based on American Heart Association (AHA) research, survival falls 7% to 10% for each minute without CPR.  Chest compressions during CPR can generate a small but critical amount of blood flow to vital organs such as the brain and heart until circulation is restored by EMS personnel. While defining high-quality CPR, the AHA puts priority on specific characteristics including the rate of compressions, depth, and full release after each compression (recoil).  To follow AHA's guidelines and improve CPR technique, researchers at the University of North Texas propose an effective smartphone application that can be used to provide real-time evaluation and feedback during CPR. The developed science and technology from this EAGER project can be used to evaluate and certify community workers, provide resuscitation quality improvement (RQI), and offer assistance to bystanders during a cardiac arrest. <br/><br/>Typical CPR training for health care workers entails watching videos and listening to lectures in a classroom setting every two years. Looking to the future, the EAGER campaign has the potential to transform the way CPR training and administration is handled. Based on the data yielded from the EAGER proposal, healthcare workers would only need to spend 5-10 minutes each month to effectively improve CPR administration and substantially increase the survival rate of patients. In order to accomplish this, we propose to: i) change the CPR training period to 5-10 minutes each month instead of one day every two years, ii) effectively assist bystanders who have no prior training in CPR administration, and iii) create a special glove that will house the mobile phone, start the CPR application automatically, and communicate with 9-1-1 operators and physicians.  Hence, the success of EAGER can transform the way we administer and offer CPR training. With favorable outcomes of the proposed research, variability of effective-CPR would dramatically be reduced, leading to higher survival rates. This EAGER project advances interdisciplinary knowledge (mechanics, fluid control, signal processing, and artificial intelligence) in the hopes that cardiac arrest survival no longer depends on environmental factors (e.g., inside or outside of hospital)."
112,1454190,CAREER: Automated scientific discovery and the philosophical problem of natural kinds,SES,"Cross-Directorate  Activities, STS-Sci, Tech & Society",4/1/15,4/16/19,Benjamin Jantzen,VA,Virginia Polytechnic Institute and State University,Continuing Grant,Frederick Kronz,3/31/21,"$443,427.00 ",,bjantzen@vt.edu,Sponsored Programs 0170,BLACKSBURG,VA,240610001,5402315281,SBE,"1397, 7603","1045, 1353, 9179",$0.00 ,"General Audience Summary <br/><br/>This is a Faculty Early Career Development (CAREER) award, the NSF's most prestigious awards in support of junior faculty who exemplify the role of teacher-scholars through outstanding research, excellent education and the integration of education and research within the context of the mission of their organizations. This award supports an integrated research and education project that addresses a fundamental scientific question:  Out of countless number of empirical quantities related to some phenomenon of interest, to which quantities should attention be directed in order to successfully discover the regularities or laws behind the phenomenon? Only a special few facilitate accurate generalization from a few particular facts to a great many that are not in evidence, and yet in the course of their work scientists efficiently choose variables that support generalization. That scientists are able to do this is both fascinating and perplexing. This project will clarify and test a new approach to solving this puzzle by constructing a series of computer algorithms that automatically carry out a process of variable choice in the service of autonomous scientific discovery. The inductive success of these algorithms when applied to genuine problems in current scientific settings will serve as tangible validation of the theory underlying these algorithms. The automated discovery algorithms produced will be leveraged to introduce a generation of graduate students in philosophy and science to the deep connections between physical computing and formal epistemology. A recurring summer school will train graduate students in basic programming and formal methods, with hands on development of automated discovery systems.<br/><br/>Technical Summary <br/><br/>This project connects the philosophical problem of natural kinds with computational problems of automated discovery in artificial intelligence. It tests a new approach, a dynamical natural kinds theory, denoted the Dynamical Kinds Theory, by deriving discovery algorithms from that theory's normative content and then applying these algorithms to real-world phenomena. The inductive success of these algorithms when applied to genuine problems in current scientific settings will serve as tangible validation of the philosophical theory. More dramatically, these discovery algorithms have the potential to produce more than one equally effective but inconsistent classification of phenomena into kinds. The existence of such alternatives plays a central role in debates over scientific realism. Outside of philosophy, the application of the discovery algorithms to open problems in areas of ecology, evolution, metagenomics, metabolomics, and systems biology has the potential to suggest previously unconceived theories of the fundamental ontology in these fields. In particular, the algorithms will be applied to agent-based models of evolutionary dynamics to search for population-level laws, and to publicly available long-term ecological data to search for stable dynamical kinds outside the standard set of ecological categories."
113,1516684,Increasing Learning and Efficacy about Emerging Technologies through Transmedia Engagement by the Public in Science-in-Society Activities,DRL,AISL,8/1/15,7/19/18,Edward Finn,AZ,Arizona State University,Continuing Grant,Julie Johnson,7/31/21,"$2,999,999.00 ","David Guston, Steve Gano, Ruth Wylie, Rae Ostman",edfinn@asu.edu,ORSPA,TEMPE,AZ,852816011,4809655479,EHR,7259,8244,$0.00 ,"The range of contemporary ""emerging"" technologies with far-reaching implications for society (economic, social, ethical, etc.) is vast, encompassing such areas as bioengineering, robotics and artificial intelligence, genetics, neuro and cognitive sciences, and synthetic biology. The pace of development of these technologies is in full gear, where the need for public understanding, engagement and active participation in decision-making is great. The primary goal of this four-year project is to create, distribute and study a set of three integrated activities that involve current and enduring science-in-society themes, building on these themes as first presented in Mary Shelley's novel, Frankenstein, which will be celebrating in 2018 the 200th anniversary of its publication in 1818. The three public deliverables are: 1) an online digital museum with active co-creation and curation of its content by the public; 2) activities kits for table-top programming; and 3) a set of Making activities.  The project will also produce professional development deliverables: workshops and associated materials to increase practitioners' capacity to engage multiple and diverse publics in science-in-society issues.  The initiative is funded by the Advancing Informal STEM Learning (AISL) program, which seeks to advance new approaches to, and evidence-based understanding of, the design and development of STEM learning in informal environments. This includes providing multiple pathways for broadening access to and engagement in STEM learning experiences, advancing innovative research on and assessment of STEM learning in informal environments, and developing understandings of deeper learning by participants.<br/><br/>This project by Arizona State University and their museum and library collaborators around the country will examine the hypothesis that exposing publics to opportunities for interactive, creative, and extensive engagement within an integrated transmedia environment will foster their interest in science, technology, engineering and mathematics (STEM), develop their 21st century skills with digital tools, and increase their understanding, ability, and feelings of efficacy around issues in science-in-society.  These three distinct yet interlocking modes of interaction provide opportunities for qualitative and quantitative, mixed-methods research on the potential of transmedia environments to increase the ability of publics to work individually and collectively to become interested in and involved with science-in-society issues."
114,1523788,NSF Postdoctoral Fellowship in Biology FY 2015,DBI,Broadening Participation of Gr,9/1/15,7/29/15,Phillip Barden,NY,Barden                  Phillip        M,Fellowship Award,Amanda Simcox,8/31/17,"$138,000.00 ",,,,New York,NY,100245192,,BIO,1157,7137,$0.00 ,"This action funds an NSF Postdoctoral Research Fellowship in Biology for FY 2015, Broadening Participation. The fellowship supports a research and training plan in a host laboratory for the Fellow and a plan to broaden participation of groups under-represented in science.  The title of the research plan for this fellowship to Phillip Barden is ""Utilizing fossils in the age of genomes: a case study of ants and amber."" The host institution for this fellowship is Rutgers University-Newark, and the sponsoring scientist is Dr. Jessica Ware. <br/><br/>As the cost and difficulty of generating molecular (DNA) data decreases, new questions are emerging regarding the role of paleontological information in the ""age of genomes."" While massive DNA-based datasets offer unprecedented insight into the history of life on Earth, fossils can provide otherwise unknowable details related to evolutionary timing, ancient morphology, and biogeography. The fellowship research seeks to evaluate the utility of fossils in large-scale molecular datasets through the lens of one of nature's greatest success stories. Today, ants comprise over 13,000 highly social, diverse, and ecologically impactful species found across terrestrial vegetated landscapes worldwide - but this was not always the case. The fossil record suggests that ants were a relatively minor component of arthropod fauna until approximately 50 million years ago. In addition, paleontological evidence, particularly amber fossil deposits, suggests that many of the earliest ants were distinct from their modern relatives and ultimately doomed to extinction. What drove some lineages to extinction while others led to modern levels of diversity and prevalence? Why did prevalence remain low for so long? Paleontological data are being derived from newly discovered fossils from approximately 100 to 50 million years ago, as well as numerous species known from other key moments in ant diversification and extinction. High throughput DNA sequencing is being utilized as a cost-effective method for obtaining large amounts of molecular data for living species. Combined analyses of molecular and morphological data provide a foundation for testing hypotheses relating to the history of ants, as well as a case study for incorporating fossil and large-scale molecular datasets. Ants are an emerging model system for research ranging from artificial intelligence to the study of aging and gene networks, and genome-scale molecular datasets are generated for numerous other organismal groups with rich fossil histories.<br/> <br/>Training goals include gaining expertise in molecular sequencing and bioinformatics to compliment previous training in paleontology and systematics.  Educational outreach at Rutgers University-Newark, recognized as one of the most diverse national universities in the United States, includes serving as a role model and mentoring high school and undergraduate students in fossil description, molecular sequencing, and analysis to generate excitement and encouragement for scientific research among underrepresented groups."
115,1532846,NCS-FO: A circuit theory of cortical function,ECCS,"Engineering of Biomed Systems, IntgStrat Undst Neurl&Cogn Sys",8/1/15,8/10/15,Charles Gilbert,NY,Rockefeller University,Standard Grant,Kenneth Whang,7/31/19,"$970,091.00 ","George Reeke, Winrich Freiwald",gilbert@rockefeller.edu,1230 YORK AVENUE,New York,NY,100656307,2123278309,ENG,"5345, 8624","004E, 8089, 8091, 8551",$0.00 ,"This project aims to develop and test a new conceptual framework for understanding brain function, and informing biologically based artificial intelligence systems.  The underlying theory holds that the properties of any neuron and any cortical area are not fixed but undergo state changes with changing perceptual task, expectation and attention.   Because of the multiple routes by which this top-down information can be conveyed, each neuron is essentially a microcosm of the brain as a whole.   <br/><br/>In this framework, a neuron is viewed as an adaptive processor rather than merely a link in a labeled line, taking on functions that are required for performing the current task.  The theory accounts for cortical function at the circuit level.  Through an interaction between feedback and intrinsic connections neurons select inputs that are relevant to a task and suppress inputs that are irrelevant.  The experiments will combine visual psychophysics, fMRI, large scale high density electrode array recordings and optogenetic manipulation.  These techniques will be used to measure changes in effective connectivity between cortical areas and the relationship between effective connectivity and the information represented by neurons at different recording sites as animals perform different visual recognition tasks.  Computational models will be developed to account for how task-dependent gating of connections can be achieved and will reproduce the functional dynamics observed experimentally.  Though the experiments will focus on the visual modality, the findings from the work will formulate a general theory of brain function that is broadly applicable to the brain as a whole."
116,1448848,STTR Phase I:  A Social and Data-Driven Platform for Searching Healthcare Providers,IIP,STTR Phase I,1/1/15,1/11/16,Matthew Wiley,CA,SmartBot360,Standard Grant,Peter Atherton,7/31/16,"$225,000.00 ",Evangelos Christidis,mwiley63@gmail.com,3499 10th St,Riverside,CA,925013617,3057818044,ENG,1505,"1505, 8032",$0.00 ,"The broader impact/commercial potential of this Small Business Technology Transfer (STTR) Phase I project results from it providing a novel way to more effectively match patients to healthcare providers. Patients will be able to use the developed system to discover and compare the most suitable and experienced healthcare providers for their unique health needs, instead of just viewing a directory of providers in their area. For example, the proposed system will recommend to a patient searching for ""knee replacement surgery"" orthopedic surgeons with experience in this or similar procedures, positive health outcomes and high past patients satisfaction levels. The patient may then compare these surgeons by expertise, relative cost, location and several other attributes. By helping patients select the best healthcare providers, the proposed system not only has the potential to improve health outcomes, because providers will see the patients more relevant to their expertise, but may also reduce the number of ""shopping-around"" visits which may lead to a reduction in overall healthcare costs.<br/><br/>This Small Business Technology Transfer (STTR) Phase I project will study methods to exploit Big Data, ranging from government-published health metrics and surveys to public social media data, to better match patients to healthcare providers. A key challenge is that many of these sources include free text, which must be analyzed to extract medically significant information. Further, different sources refer to the same medical terms in different ways; for example ""myocardial infarction"" vs. ""heart attack"", or ""doctor knows her stuff"" vs. ""doctor is knowledgeable"". Data mining and artificial intelligence techniques will be leveraged to identify which provider properties - medical school, years of experience, affiliated hospitals, and so on - should be used when searching for providers and how these properties should be best weighted and combined, given the healthcare needs of a patient."
117,1549515,Enhancing Education and Awareness of Shannon Theory,CCF,"Special Projects - CNS, Special Projects - CCF, IIS Special Projects",8/15/15,8/8/16,Christina Fragouli,NJ,"Institute of Electrical & Electronics Engineers, Inc.",Standard Grant,Phillip Regalia,7/31/18,"$299,935.00 ",Sergio Verdu,christina.fragouli@ucla.edu,445 HOES LANE,Piscataway,NJ,88544141,7325626520,CSE,"1714, 2878, 7484",7916,$0.00 ,"Consistent with the National Science Foundation's goal ""to initiate and support ... programs to strengthen scientific and engineering research potential [and] science and engineering education programs at all levels"", this project develops materials that will support education and broad awareness of the importance of key engineering advances . In particular, it will support creation of educational films and corresponding lesson materials for K-12 mathematics and science teachers that will allow students to be exposed not just to the advances in information theory, but also to how an ordinary person played a pivotal role in fostering them. By adapting material previously restricted to graduate-level courses and technical conferences to a larger audience, a broader dissemination of information theory should profitably inform teachers and researchers in other fields, thereby fueling the nation's STEM workforce and improving commercial technology. <br/><br/>This project focuses on the efforts and advances of Claude Shannon, who established the field of information theory by stating some of its most fundamental problems and solving them. The technologies that his work made possible form a major driving force of our economy. His information theory concepts provide the foundation for nearly every aspect of modern information technology and have been applied to many fields, including communication, language, genetics, computing, cryptography, psychology, perception, memory, artificial intelligence, quantum physics, and others."
118,1543986,MATH: EAGER: Online Collaborative Problem Solving in Remedial College Mathematics,DUE,IUSE,9/15/15,9/11/15,Mark Warschauer,CA,University of California-Irvine,Standard Grant,Myles Boylan,8/31/17,"$300,000.00 ","Sarah Eichhorn, Di Xu",markw@uci.edu,"141 Innovation Drive, Ste 250",Irvine,CA,926173213,9498247295,EHR,1998,"005Z, 7916, 8209, 8244, 9178",$0.00 ,"This is a study of the impact of the learning environment on undergraduate students learning problem-solving in remedial pre-calculus mathematics. The study will examine the comparative effectiveness of face-to-face discussion groups and 2 different types of online designs, compared to individual practice. Students will be randomly assigned to one of four groups: three treatment groups that carry out collaborative problem solving using (1) face-to-face discussion, (2) an audio capability with a whiteboard online tool, or (3) an online virtual environment combining audio plus whiteboard plus avatar-based interaction; or a control group of individual problem solving. Currently, pre-calculus instruction is been carried out online in this university using the web-based ALEKS tutorial software, an online interactive learning system that uses an artificial intelligence algorithm to assess and report student mastery of content material. In search of improved student performance, this project is conducting an early in-depth study of the effectiveness of a richer online design for learning problem solving that was recently constructed for management courses, called VirBELA (Virtual Business Education Leadership Assessment). It is a learning platform that offers students the opportunity to work in teams. In mathematics, VirBELA has good potential for facilitating group problem-solving. It provides a whiteboard that allows students to draw diagrams and functions as needed and also provides an audio capability for discussion. It has the added feature of giving students a virtual online embodiment by creating an avatar for each student. There is evidence from other sources that avatars create a richer, more interactive and engaging online collaborative environment that more closely replicates the advantages of in-person communication. <br/><br/>A wide range of data will be collected and assessed, including usage logs of the tutorial learning software, common final exam scores in both pre-calculus and calculus, and individual institutional data. Other outcome variables will include how students perform on the problem solving, how they use the tutorial-based mathematics software before and after problem solving sessions, and whether they persist to the next calculus course, what grades they earn in the following calculus course, and how their attitudes toward STEM study change based on pre- and post-surveys. The study will be repeated in three different quarters."
119,1545858,PIRE: International Program for the Advancement of Neurotechnology (IPAN),OISE,"Cross-BIO Activities, PIRE- Prtnrshps Inter Res & Ed",11/1/15,8/10/18,Euisik Yoon,MI,Regents of the University of Michigan - Ann Arbor,Continuing Grant,Cassandra Dudka,10/31/21,"$5,000,000.00 ","Kensall Wise, Edward Stuenkel, Gyorgy Buzsaki, Gregory Quirk",esyoon@umich.edu,3003 South State St. Room 1062,Ann Arbor,MI,481091274,7347636438,O/D,"7275, 7742","5921, 5927, 5936, 5942, 5946, 7298, 7742",$0.00 ,"This project entitled ""International Program for the Advancement of Neurotechnology (IPAN)"" is about understanding the complexity and mysteries of the brain. It is cited by many as the biggest scientific challenge of this century. In this International Program for the Advancement of Neurotechnology (IPAN), the researchers are creating a holistic system for studying brain activity by closely integrating hardware from leading neurotechnologists with novel software from leading neuroscientists.  Enabling this large-scale collaboration should accelerate the rate of discovery in neuroscience. This in turn will pave the way to improved treatments for neurological disorders and to breakthroughs in artificial intelligence in the next decade. The PIRE team will also provide advanced educational opportunities for undergraduates with the express purpose of recruiting future U.S. STEM (science, technology, engineering and mathematics) researchers. Graduate students and postdocs will also be enrolled in a unique cross-training program between neuroscience and neurotechnology laboratories. The resulting experience will prepare a new generation of globally-connected multi-disciplinary engineers and scientists while driving critical advances in neurotechnology.<br/><br/>IPAN is an explicit partnership of leading neuroscientists and technologists to develop and deliver a hardware and software system that fundamentally simplifies the ability of a neuroscientist to (i) identify recorded neuron types, (ii) reconstruct local neural circuits, and (iii) deliver biomimetic or synthetic inputs in a cell-specific targeted manner. This project teams the University of Michigan, New York University, Howard Hughes Medical Institute, and the University of Puerto Rico with the University of Freiburg,  the  University  of  Hamburg-Eppendorf,  the  Korea  Institute  of  Science  and  Technology, Singapore?s Institute for Microelectronics, and University College London.   Complementary strengths, world-class infrastructures, and strong student exchange programs are an important part of this IPAN team, with major thrusts in Technology, Neuroscience, and Education. The enabling technology to meet these three system goals (i-iii) will be next-generation neural probes equipped with novel optoelectronics, high-density recording interfaces, and low-noise multiplexed digital outputs. The neuroscience thrust will help define the technology from the onset and are developing novel software tools to accelerate the analysis of large neurophysiological data sets. The team includes leading system neuroscientists with unique  capabilities  specializing  in  memory,  sensory,  fear,  and  development, and  will  work  with technologists to validate both the technology and the software tools in distinctive neuroscience applications."
120,1534770,SBIR Phase II:  A Game-Based Leadership Program,IIP,SMALL BUSINESS PHASE II,9/1/15,1/26/17,Robert Brown,NC,Triad Interactive Media,Standard Grant,Ben Schrag,8/31/17,"$659,682.00 ",,robert@triadinteractivemedia.com,1601 Guilford College Road,Jamestown,NC,272829383,3369085884,ENG,5373,"5373, 8031, 8032, 8043, 8240, 9117",$0.00 ,"This Small Business Innovation Research Phase II project is an online, game-based program to improve the leadership skills of individuals in science, education, the military, government, and industry. Leadership skills are key to innovation, efficiency, and effectiveness in all organizations. The program being developed is built on a proven leadership model and converts that traditional model into a role-playing game. The empirically tested model consists of three basic strategies and six practices that expand leaders' perspectives and enhance their problem-solving skills. The program uses a futuristic narrative, 3D-animated multimedia, and challenging leadership dilemmas to engage learners, who play the role of a novice world leader. The program provides instruction in leadership, and then learners apply their new leadership skills to build an alliance among warring factions and collectively solve a global problem. A series of game quests test learners' understanding, and they receive immediate feedback on all decisions and actions. Learners communicate through social media tools with other players and, at the end, participate in a virtual synchronous debriefing exercise with peers and trained facilitators to ensure full comprehension. An administrative dashboard provides real-time performance data. The game is scalable, accessible, and designed for repeat playability.<br/><br/>The most profound innovation of this project is that it conceptualizes, designs, and develops a system of game mechanics and algorithms that mimic the facilitator-led human aspect of a research-based, face-to-face model of leadership training. This design entails building out a backend system as an artificial intelligence tool designed to guide users engaging with instructional materials, anticipate player actions, make recommendations on actions and anticipated actions, and provide feedback similar to guidance an on-site facilitator would provide. Furthermore, the game logic supports multiple paths to success, with some paths being more efficient, optimal, elegant, or otherwise more appropriate choices. The game logic and technical design are built to reflect problem solving and leadership in the real world, which is largely based on open-ended decision making. The game mechanics and related backend technology are designed to support open-ended decision-making, thus making game play both more engaging and authentic for players. A final innovation is the linking of an asynchronous online video game learning experience with virtual facilitator-led synchronous debriefing, which can be done on a worldwide scale.  The program will be tested as a global collaboration exercise with both high school students and corporate leaders."
121,1461192,REU Site: BME Community of Undergraduate Research Scholars for Cancer (BME CUReS Cancer),EEC,EWFD-Eng Workforce Development,6/1/15,1/23/15,Mia Markey,TX,University of Texas at Austin,Standard Grant,Mary Poats,6/30/18,"$287,684.00 ",Laura Suggs,mia.markey@utexas.edu,"3925 W Braker Lane, Ste 3.340",Austin,TX,787595316,5124716424,ENG,1360,"116E, 9178, 9250",$0.00 ,"BROADER SIGNIFICANCE OF THE PROJECT:<br/><br/>An underlying theme of the current project is that CUReS Cancer Scholars will make contributions towards fundamental understanding of the physical principles of cancer development, prevention, and treatment during their summer experience, but more importantly, will go on in their careers to function in multi-disciplinary teams of researchers from disparate fields in medicine, biological science, physical science, engineering, and healthcare fields to advance human health. This ability is a hallmark of Biomedical Engineering and will positively impact Scholars' respective scientific fields as well as the health of our society. Increasing the number of PhDs from underrepresented groups is also a critical step toward increasing minority role models, in turn making the academic engineering culture more appealing to underrepresented minorities and women. A key feature of this REU site is the development of a partnership with Texas 4000, a non-profit organization that cultivates student leaders and engages communities in the fight against cancer. The partnership with the Texas 4000 will increase public awareness of cancer research, in particular how physical science and engineering positively impact healthcare.<br/><br/>PROJECT DESCRIPTION:<br/><br/>The scientific theme of the ""BME Community of Undergraduate Research Scholars for Cancer (BME CUReS Cancer),"" REU Site is leveraging Biomedical Engineering to Open a New Frontier in Oncology. This project is supported by the Division of Engineering Education and Centers for three summers, for 10 weeks each summer, from late May to early August. The focus of this experience is rising juniors, particularly students from groups traditionally underrepresented in engineering. BME CUReS Cancer Scholars are able to choose a project of appropriate scope from 36 different labs representing a wide range of research topics pertinent to cancer research, such as biomaterials, drug delivery, optical imaging, ultrasound, and artificial intelligence in medicine. This topics were selected based on a 2008 National Cancer Institute meeting where a number of barriers to achieving progress in cancer research were identified. The result of the meeting was a series of ""strategic actions"" that represent scientific challenges that need to be addressed. Following the NCI Meeting Report of 2008 it is becoming more broadly recognized that understanding how the range of physical laws and principles governing the behavior of all matter are operative in cancer at every scale will be critical to understanding and controlling cancer. Cancer is a disease of complexity and engineering principles in particular are developed to study, model, and solve complex problems. Hence, Biomedical Engineering is uniquely poised to make a significant intellectual contribution. These critical lines of investigation are used to organize our community of cancer Scholars towards shared goals and synergistic activities. The intellectual merit of this project is in addressing these key challenges in cancer research using an engineering approach."
122,1527497,"AF: Small: Using Ordinal Information to Approximate Cardinal Objectives in Social Choice, Matching, Group Formation, and Assignment Problems",CCF,Algorithmic Foundations,6/15/15,4/27/20,Elliot Anshelevich,NY,Rensselaer Polytechnic Institute,Standard Grant,Tracy Kimbrel,5/31/21,"$350,470.00 ",,eanshel@cs.rpi.edu,110 8TH ST,Troy,NY,121803522,5182766000,CSE,7796,"7923, 7926, 7932, 9251",$0.00 ,"Many modern algorithms must make decisions using only limited information: they not only need to make the best choices given the input, but also don't know what the ""true"" input actually is; and yet they are required to make good choices anyway. This problem arises often in settings where the goal is to maximize the total happiness (a.k.a. social welfare or total utility) of the system, such as social choice settings in which voters submit their preferences for different alternatives, matching settings (e.g., matching people with job openings or organ donors with patients), assigning people to groups or projects, economic market settings, and many others. In all of these settings, the people or agents involved may care deeply about which outcome is selected (e.g., which alternative is selected by the voting mechanism, or which patients are assigned a donated kidney), with the mechanism designer's goal being to maximize the overall welfare and satisfaction. Unfortunately, in all these applications, only limited information is usually available: it is relatively easy to obtain *ordinal* information (which choice is preferred to which other choice by each participant), but almost impossible to obtain the underlying numerical information (how *much* each choice is preferred by each participant). This project will use a novel notion of approximation to give new insight into the design and evaluation of many mechanisms for the settings mentioned above. The approximation algorithms resulting from this project will be used to suggest new protocols, which would not only optimize some notion of fairness (as is common in social choice), or maximize the size of a matching (as is common in kidney exchange), but would have provable guarantees on the quality of the outcomes. One reason why such guarantees have not been considered in the past is that without the knowledge of exact numerical utilities or exact compatibilities between matches, protocols can only rely on ordinal, or otherwise limited, information. However, as preliminary work shows, one can often design algorithms which behave well no matter what the *true* information is, as long as the underlying (unknown) numerical values have some reasonable structure, or are at least correlated in some way, which is certainly the case for most applications. Because of this, this project will provide a different perspective, and will result in algorithms which produce provably good outcomes while using only limited ordinal information.  Due to the applications touched by this project, the work done should be of interest to researchers in many fields, including Social Choice, Artificial Intelligence, Game Theory, Social Networks, and Economics. This research will be strongly complemented by the PI's education plan, which includes teaching several courses with research components, presenting this work at numerous scientific seminars, and recruiting several graduate and undergraduate students to work on this project.<br/><br/>The primary goal of this project is to design and analyze algorithms which only know ordinal information, and yet create solutions which are provably close to the ""true"" optimal solution: the one which would be chosen if the full numerical information were known. The project will specifically focus on the settings of social choice, matching, group formation, and economic markets. Very little is known about approximation algorithms in the presence of ordinal information, and designing such algorithms for the settings above will likely require new and interesting techniques. When the numerical values are completely uncorrelated, it is of course impossible to form good approximations from only ordinal information, so this work will involve looking at different kinds of correlations (e.g., lying in a metric space, symmetric values, values from a common distribution, etc), and determining how much power this structure gives to the ordinal information, as compared to the true numerical information. The PI will also consider optimization problems with other interesting constraints which deserve further study, focusing especially on computing good solutions in the presence of self-interested agents, in the contexts of social choice, matching, and envy-free pricing. This work should lead to basic understanding of the fundamental power of ordinal information, by determining under which settings and conditions ordinal information is enough to approximate the numerical truth, and when such an approximation is impossible."
123,1622402,CAREER: Reasoning under Uncertainty in Cybersecurity,CNS,Secure &Trustworthy Cyberspace,8/7/15,2/11/16,Xinming Ou,FL,University of South Florida,Continuing Grant,Sylvia Spengler,2/28/17,"$23,249.00 ",,xou@usf.edu,4019 E. Fowler Avenue,Tampa,FL,336172008,8139742897,CSE,8060,1045,$0.00 ,"Cyber security, like security in the physical world, relies upon investigation methodologies that piece together dispersed evidence spread across multiple places, and come to a conclusion on what security breaches have happened and how they happened. While effective evidential reasoning based on manual analysis are used in the physical world by law-enforcement agencies, in the cyber world we need automated reasoning methodologies to handle the automated cyber attacks against our nation's information infrastructures every day. This research aims at discovering and developing such automated reasoning methodologies. The problem is  difficult due to the uncertain nature of such reasoning, which is compounded by the characteristics of cyber attacks.<br/><br/>The uncertainty in cyber security comes from two sources. The first is the uncertainty from not knowing the attacker's actions and choices. Since hackers are essentially invisible in the cyberworld, we have to rely upon various types of sensors that report symptoms of potential attacks. The second source of uncertainty comes from these sensors. Since in most cases the symptoms of cyber<br/>attacks significantly overlap with symptoms from benign network activities, it is not possible to rely on a single sensor to give an absolutely correct judgment on whether an attack has happened and succeeded. A key question is how to use these imperfect sensors to conduct reasoning so that one can come up with almost certain conclusions regarding a system's security status. <br/><br/>This challenge of reasoning under uncertainty is not new. In the past four decades computer science researchers have developed an array of reasoning models and methods for uncertainty, especially in the area of artificial intelligence. However, the emergence of cyber threats poses a new<br/>challenge to this problem. The existing methodologies typically require a knowledge-engineering process to build a knowledge model for the problem domain. This has worked reasonably well with the more static and well-behaved problem domains such as disease diagnosis. A key difference between these problem domains and cyber security is that the latter has to deal with an active<br/>malicious attacker who will try to break whatever assumptions made in the reasoning model. For this reason, the knowledge model for cyber security cannot be static because then they can be easily evaded. What will be an effective and practical knowledge engineering approach to handle the uncertainty in cyber security is the biggest open problem that needs to be answered from the<br/>research.<br/><br/>This research adopts an empirical, bottom-up approach to tackle the above challenges. Instead of starting from the existing theories, the PI will start from empirical study on how a human security analysts would reason about cyber events and try to capture the essence of the reasoning in the process. Then, the PI will carry out this empirical study by running intrusion detection sensors on production networks and work with system administrators to understand and reason about the alerts. The next step is to develop a reasoning model that simulates the human reasoning process, and apply the automated reasoning engine on fresh new data to see how it fares. In this spiral theory development process the PI can always make sure that the methodologies are applicable to real cyber-security analysis and constantly find gaps in the model that reveal what will be the most appropriate theories and how to apply them in this problem. The eventual goal is to find the right theoretical framework for reasoning under uncertainty in cyber-security, and validate such theories through repeatable experiments on data from production systems.<br/><br/>This research is tightly integrated into the PI?s education efforts both for students and targeted at the society at large. The empirical nature of the research provides a valuable venue for dialogue between security practitioners and researchers, which will result in a two-way education process: students working on the project can acquire the essential skills of applying advanced knowledge to a practical problem; and security practitioners like system administrators can learn the state-of-the art in cyber security technology through collaborative work with the research team. The empirical study carried out from the research will provide endless data and examples to refresh the materials of the cyber-security courses taught by the PI. New courses with a focus on uncertainty in cyber security defense will be developed. There will be a number of undergraduate students who take part in the research efforts, which will provide a unique education experience for them. Moreover, the test-bed infrastructure produced from the research will also be used as an education platform for the general public about cyber-security problems, with the help of the out-reach programs already established at Kansas State University."
124,1541029,EarthCube IA: Collaborative Proposal: LinkedEarth: Crowdsourcing Data Curation & Standards Development in Paleoclimatology,ICER,EarthCube,9/1/15,7/28/15,Julien Emile-Geay,CA,University of Southern California,Standard Grant,Eva Zanzerkia,8/31/19,"$684,779.00 ",Yolanda Gil,julieneg@usc.edu,University Park,Los Angeles,CA,900890001,2137407762,GEO,8074,7433,$0.00 ,"Natural climate variability signficantly modulates anthropogenic global warming, and only paleoclimate observations can adequately constrain it. Moreover, such observations are most powerful when many records are brought together to provide a spatial understanding of past variability. However, there is currently no universal way to share paleoclimate data between users or machines, hindering integration and synthesis. Large-scale, international, paleoclimate data syntheses have a long and successful history, but have been needlessly labor-intensive. Recognizing that (1) paleoclimate data curation requires expert knowledge; (2) top-down data management approaches are ineffectual; (3) existing infrastructure does not foster standardization; there emerges a critical need for a flexible platform enabling crowdsourced data curation and standards development.The platform will be combined with editorial and community-driven processes which will result in a system that has the potential to engage a broad user base in geoscientific data curation. The proposed framework will lower barriers to participation in the geosciences, enabling more ""dark data"" to join the public domain using community-sanctioned protocols. The pilot project will facilitate the work of hundreds of paleoclimate scientists, accelerating scientific discovery and the dissemination of its results to society.<br/><br/>Semantic wikis provide a simple, intuitive interface to semantic languages and infrastructure that build on open Web architecture. Like traditional wikis, they enable the collaborative authoring of content. Secure access and time-stamped content also enable the tracking of changes and the accountability of users, as well as moderation capabilities by community members of recognized expertise. In contrast to traditional wikis, semantic wikis allow contributors to assign meaning to their content, specifying relationships between the objects they describe. This enables artificial intelligence reasoners to parse, process and translate these data into more useful forms. The technology is well-proven, scalable, and completely transparent to the user, requiring no computer science knowledge or more sophisticated technology than a web browser. The LinkedEarth Wiki will automatically translate this information into Linked Open Data, a universal format to share data across the Web. To demonstrate this concept?s broad applicability across paleoclimate science, the project?s target community is the PAGES2k consortium, an international collaboration dedicated to the climate of the Common Era. Social technologies will be developed to power collective curation, standards development and quality control by the community itself. The project will demonstrate applicability to other paleogeosciences, serving as a potential template for other geoscientific disciplines."
125,1526189,AF: Small: Exact algorithms for the quantum satisfiability problem,CCF,Algorithmic Foundations,9/1/15,6/8/15,Sevag Gharibian,VA,Virginia Commonwealth University,Standard Grant,Dmitri Maslov,12/31/18,"$196,593.00 ",,sgharibian@vcu.edu,P.O. Box 980568,RICHMOND,VA,232980568,8048286772,CSE,7796,"7923, 7928",$0.00 ,"Among the most fundamental problems in theoretical computer science is the k-SATISFIABILITY problem (k-SAT), which roughly asks: Given a set of Boolean constraints of a special form, each acting on k out of n bits, does there exist an assignment to all n bits which simultaneously satisfies every constraint? This problem has far-reaching applications in areas ranging from artificial intelligence to electronic design automation to theorem proving, thus underscoring its significance. <br/><br/>More recently, a quantum generalization of k-SAT has arisen, known as k-QSAT, which finds applications in areas such as quantum error-correcting codes. Moreover, k-QSAT is physically well-motivated, as it can be thought of as modeling how quantum systems in nature are governed by local quantum constraints. Unlike k-SAT, however, much less is known about k-QSAT. The aim of this project is precisely to close this fundamental knowledge gap. In particular, this project broadly asks: In what cases can non-trivial algorithms be developed for solving k-QSAT? <br/><br/>The resolution of this question will yield deep insights into which properties of quantum systems can be computed efficiently by a classical computer. Moreover, the results obtained will be disseminated through a variety of avenues, including conferences, new course materials, and high school workshops aimed at exposing young computer scientists to the frontiers of research."
126,1540996,EarthCube IA: Collaborative Proposal: LinkedEarth: Crowdsourcing Data Curation & Standards Development in Paleoclimatology,ICER,EarthCube,9/1/15,7/28/15,Nicholas McKay,AZ,Northern Arizona University,Standard Grant,Eva Zanzerkia,8/31/18,"$113,015.00 ",,Nicholas.McKay@nau.edu,"ARD Building #56, Suite 240",Flagstaff,AZ,860110001,9285230886,GEO,8074,7433,$0.00 ,"Natural climate variability signficantly modulates anthropogenic global warming, and only paleoclimate observations can adequately constrain it. Moreover, such observations are most powerful when many records are brought together to provide a spatial understanding of past variability. However, there is currently no universal way to share paleoclimate data between users or machines, hindering integration and synthesis. Large-scale, international, paleoclimate data syntheses have a long and successful history, but have been needlessly labor-intensive. Recognizing that (1) paleoclimate data curation requires expert knowledge; (2) top-down data management approaches are ineffectual; (3) existing infrastructure does not foster standardization; there emerges a critical need for a flexible platform enabling crowdsourced data curation and standards development.The platform will be combined with editorial and community-driven processes which will result in a system that has the potential to engage a broad user base in geoscientific data curation. The proposed framework will lower barriers to participation in the geosciences, enabling more ""dark data"" to join the public domain using community-sanctioned protocols. The pilot project will facilitate the work of hundreds of paleoclimate scientists, accelerating scientific discovery and the dissemination of its results to society.<br/><br/>Semantic wikis provide a simple, intuitive interface to semantic languages and infrastructure that build on open Web architecture. Like traditional wikis, they enable the collaborative authoring of content. Secure access and time-stamped content also enable the tracking of changes and the accountability of users, as well as moderation capabilities by community members of recognized expertise. In contrast to traditional wikis, semantic wikis allow contributors to assign meaning to their content, specifying relationships between the objects they describe. This enables artificial intelligence reasoners to parse, process and translate these data into more useful forms. The technology is well-proven, scalable, and completely transparent to the user, requiring no computer science knowledge or more sophisticated technology than a web browser. The LinkedEarth Wiki will automatically translate this information into Linked Open Data, a universal format to share data across the Web. To demonstrate this concept?s broad applicability across paleoclimate science, the project?s target community is the PAGES2k consortium, an international collaboration dedicated to the climate of the Common Era. Social technologies will be developed to power collective curation, standards development and quality control by the community itself. The project will demonstrate applicability to other paleogeosciences, serving as a potential template for other geoscientific disciplines."
127,1533708,NCS-FO: Identifying Design Principles of Neural Cells,ECCS,"Engineering of Biomed Systems, IntgStrat Undst Neurl&Cogn Sys",8/1/15,8/10/15,Amina Qutub,TX,William Marsh Rice University,Standard Grant,Shubhra Gangopadhyay,7/31/18,"$920,000.00 ","Daniel Wagner, Jacob Robinson",amina.qutub@utsa.edu,6100 MAIN ST,Houston,TX,770051827,7133484820,ENG,"5345, 8624","004E, 137E, 8089, 8091, 8551",$0.00 ,"PI: Qutub, Amina    <br/>Proposal Number: 1533708<br/><br/>This proposal seeks to develop a robust theory of how single neural cells form electrically active networks. The project integrates emerging methods in computer science, systems biology, neuroengineering and developmental biology to offer insight into the brain's organization. Results of experiments performed in this project have the potential to impact the design of new computing devices, a $300B industry. Methods introduced by the investigators can be used broadly by scientists to rapidly characterize brain cells, and can aid in the discovery of new therapies for neurological diseases, which affect 1 in 6 people worldwide.<br/><br/>Neural differentiation, the process of neural progenitor cells transforming into neurons, holds the key to understanding the brain's ability to self-repair. Understanding this complex process can inform us how the structure of neural networks relates to their function, which is an important unsolved problem in neuroengineering. This project's ultimate goal is a mechanistically-detailed theory of how neural networks form as a result of decisions made by single neural progenitor cells.  Integrating methods from three disciplines (systems biology, nanotechnology and developmental biology), the investigators will identify single cell features critical to network formation, and predict how heterogeneity and noise in the cell population defines the network's function. The investigators will employ three emerging methods (proteomic barcoding, ImageOmics, and E-phFACS) to define neural cell phenotypes as a function of chemical signaling, morphology and electrical activity (Aim 1). Observed changes in these single cell phenotypes will be mapped to neural network formation by coupling a state machine model with a graph-based analysis (Aim 2). The effects of cell heterogeneity will be explored computationally using a new framework developed by the investigators and iteratively compared to in vitro live-imaging assays, and in vivo assays. The tight coupling of chemical, morphological and electrical measurements enabled by the technologies introduced here, can be broadly applicable paradigm for integrative scientific research. Understanding how cells interact to form neural networks has relevance to organism development and tissue engineering. State-machines can be adapted across tissues and species to develop theories of cell decision-making. The phenotyping tools that correlate electrophysiology and protein expression with multi-cellular network topology will provide a powerful resource for neurobiologists. Furthermore, the E-phFACS device provides a high-throughput way to record electrical activity and sort cells. To further engage the scientific community, the investigators will (1) provide an open source ImageOmics platform and (2) host an international crowd-sourced neuronal network Design Challenge."
128,1533512,NCS-FO: Collaborative Research: Sleep's role in determining the fate of individual memories,BCS,IntgStrat Undst Neurl&Cogn Sys,9/1/15,8/10/15,Ken Paller,IL,Northwestern University,Standard Grant,Betty Tuller,2/28/19,"$398,095.00 ",,kap@northwestern.edu,750 N. Lake Shore Drive,Chicago,IL,606114579,3125037955,SBE,8624,"8089, 8091, 8551",$0.00 ,"Identifying the cognitive, computational and neural mechanisms responsible for determining why some memories survive when others fade is one of the many grand challenges facing researchers of the human mind and brain. It is widely understood that sleep plays a critical role in long-term remembering, yet what exactly happens during sleep to affect the persistence of memories remains largely unknown. This project brings together a team of researchers who will integrate multiple independent lines of work in cognitive neuroscience, cognitive psychology, and computer science in order to investigate the precise mechanisms undergone by recently-formed memory representations as a person sleeps and how these mechanisms determine which memories survive and which fade. The proposed integration of cutting-edge neural data analysis methods for EEG and neuroimaging data, basic human memory theory, and neural network modeling make possible the ability to non-invasively track individual memories in the human brain as they compete with each other and are modified during sleep. The potential advances from this work could impact education, training situations, and public health by facilitating the development of new strategies for ensuring that important memories survive after initial learning.<br/><br/>Research suggests that memories compete for neural space such that reactivating one particular memory can exert ""collateral damage"" on other related memories. In other words, accessing one memory can come at the expense of later being able to access other nearby memories in the network space.  The proposed studies test the hypothesis that importance shapes neural dynamics during sleep by selectively boosting memory reactivation; this boost ensures that important memories out-compete related memories during sleep, resulting in strengthening of important memories and weakening of less-important memories. To test this hypothesis, competition between memories will be elicited during sleep by playing sound cues, each of which was linked (during wake) to two different picture-location memories. Multiple interlocking approaches will track how memory competition during sleep shapes a memory's persistence versus fading. Neural network models will be used to generate predictions about how reward responses during encoding shape competitive dynamics during sleep, and how these competitive dynamics determine the eventual fates of competing memories. Predictions will be tested by using fMRI to measure neural activity associated with reward processing during encoding, EEG to measure brain activity during sleep, and pattern classifiers to decode memory activation from the sleep EEG data. Observations of competitive dynamics during sleep will then be related to later memory performance and to multivariate fMRI measures of memory change. The project has the potential to provide, for the first time, a comprehensive look ""under the hood"" at the life of a memory as it is acquired, processed during sleep, and eventually recalled. Pivotal knowledge will be gained about how variance in reward processing at encoding influences sleep replay dynamics, and about how sleep replay dynamics affect subsequent memory performance and the structure of neural representations."
129,1533511,NCS-FO: Collaborative Research: Sleep's role in determining the fate of individual memories,BCS,IntgStrat Undst Neurl&Cogn Sys,9/1/15,8/10/15,Kenneth Norman,NJ,Princeton University,Standard Grant,Betty Tuller,2/28/19,"$594,090.00 ",,knorman@princeton.edu,Off. of Research & Proj. Admin.,Princeton,NJ,85442020,6092583090,SBE,8624,"8089, 8091, 8551",$0.00 ,"Identifying the cognitive, computational and neural mechanisms responsible for determining why some memories survive when others fade is one of the many grand challenges facing researchers of the human mind and brain. It is widely understood that sleep plays a critical role in long-term remembering, yet what exactly happens during sleep to affect the persistence of memories remains largely unknown. This project brings together a team of researchers who will integrate multiple independent lines of work in cognitive neuroscience, cognitive psychology, and computer science in order to investigate the precise mechanisms undergone by recently-formed memory representations as a person sleeps and how these mechanisms determine which memories survive and which fade. The proposed integration of cutting-edge neural data analysis methods for EEG and neuroimaging data, basic human memory theory, and neural network modeling make possible the ability to non-invasively track individual memories in the human brain as they compete with each other and are modified during sleep. The potential advances from this work could impact education, training situations, and public health by facilitating the development of new strategies for ensuring that important memories survive after initial learning.<br/><br/>Research suggests that memories compete for neural space such that reactivating one particular memory can exert ""collateral damage"" on other related memories. In other words, accessing one memory can come at the expense of later being able to access other nearby memories in the network space.  The proposed studies test the hypothesis that importance shapes neural dynamics during sleep by selectively boosting memory reactivation; this boost ensures that important memories out-compete related memories during sleep, resulting in strengthening of important memories and weakening of less-important memories. To test this hypothesis, competition between memories will be elicited during sleep by playing sound cues, each of which was linked (during wake) to two different picture-location memories. Multiple interlocking approaches will track how memory competition during sleep shapes a memory's persistence versus fading. Neural network models will be used to generate predictions about how reward responses during encoding shape competitive dynamics during sleep, and how these competitive dynamics determine the eventual fates of competing memories. Predictions will be tested by using fMRI to measure neural activity associated with reward processing during encoding, EEG to measure brain activity during sleep, and pattern classifiers to decode memory activation from the sleep EEG data. Observations of competitive dynamics during sleep will then be related to later memory performance and to multivariate fMRI measures of memory change. The project has the potential to provide, for the first time, a comprehensive look ""under the hood"" at the life of a memory as it is acquired, processed during sleep, and eventually recalled. Pivotal knowledge will be gained about how variance in reward processing at encoding influences sleep replay dynamics, and about how sleep replay dynamics affect subsequent memory performance and the structure of neural representations."
130,1551318,Interdisciplinary Workshop on Statistical Natural Language Processing Methods for Software Engineering,CCF,"IIS Special Projects, Software & Hardware Foundation",10/1/15,9/8/15,Premkumar Devanbu,CA,University of California-Davis,Standard Grant,Sol Greenspan,9/30/18,"$65,792.00 ",,devanbu@cs.ucdavis.edu,OR/Sponsored Programs,Davis,CA,956186134,5307547700,CSE,"7484, 7798","7556, 7944",$0.00 ,"This grant funds a workshop to bring together two communities:  Software Engineering and Natural Language Processing. Statistical natural language processing (NLP)  techniques and tools have become very powerful and are applicable in many domains. Software Engineering researchers have begun to to adopt NLP techniques and tools in the context of Software Engineering to analyze repositories of source code as if they were natural language artifacts. The purpose of this grant is to bring together the Software Engineering and Natural Language communities to address these problems.  Many Software Engineering processes and products involve natural language artifacts, such as requirements documents and code comments, and peer reviews, which play a role in program comprehension, code search, traceability and many other purposes in the software development process. The workshop will accelerate the pace of Software Engineering research in these areas and beyond.  Natural Language researchers will identify new research directions stimulated by challenges in the Software Engineering domain.  Natural Language can be a mode of communication to assist disabled such as people who are visually impaired and or have muscular injuries, thus one of the broader impacts is to advance research that would make programming tasks and jobs more accessible to those populations."
131,1526745,"RI: Small: Modeling Lexical Borrowing to Bridge the ""Linguistic Divide"" in Natural Language Processing",IIS,Robust Intelligence,9/1/15,8/11/16,Chris Dyer,PA,Carnegie-Mellon University,Standard Grant,D.  Langendoen,8/31/18,"$450,000.00 ",,cdyer@cs.cmu.edu,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,CSE,7495,"7495, 7923",$0.00 ,"The rich ecosystem of intelligent, language-aware technologies (e.g., personal assistants, content recommendation, spam detection, etc.) that users of English and other high-resource languages have access to depends on the existence of language-specific data resources. Developing the resources that enable these technologies has usually required a substantial investment -- both monetarily and in terms of trained native speakers -- meaning that without new strategies, most of the 7,000+ languages in the world would likely remain resource-poor and their speakers underserved.  This project addresses the problem of bootstrapping linguistic resources required for language technologies in low-resource languages more economically by identifying cross-linguistic correspondences between high- and low-resource languages and projecting resources (e.g., translations, lexical ontologies, and syntactic annotations) accordingly. To identify these correspondences, this work develops computational models of linguistic borrowing, which is the process by which words from a donor language are adapted by speakers of a recipient language as a result of language contact and bilingualism. In addition to enabling the transfer of resources from high- to low-resource languages, being able to identify borrowing enables corpus-based studies of the social factors (power differences between countries, public opinion, and personal attributes such as geographic location, gender, and race/ethnicity) that have been identified as correlates with which words are borrowed. Thus, by observing language change, this work enables changes in social relations to be quantified.<br/><br/>Words are not left unchanged by the process of borrowing, and modeling this process is the central challenge to identifying instances of borrowing. Fortunately, the adaptation processes are generally regular and amenable to computational modeling, and this work uses weighted finite-state transducers parameterized with features derived from Optimality Theory (OT). OT-derived features not only provide increased statistical efficiency relative to conventional linguistically naive statistical models but they also provide a new kind of corpus-based verification of some of the central claims of phonological theory. The borrowing model identifies lexical correspondences across dozens of typologically representative language pairs (primary text data is obtained from open resources such as Wikipedia, Twitter, blogs, and online news), enabling projection of resources and development of core natural language processing technologies. Finally, the borrowing model enables instances of borrowed words to be identified in text as it is generated over time, enabling corpus-based sociolinguistic studies."
132,1502543,Neurotechnologically inspired multilayered polymer electrolyte membranes to harness ion concentration gradient for energy restoration,DMR,"DMR SHORT TERM SUPPORT, POLYMERS",7/1/15,3/28/18,Thein Kyu,OH,University of Akron,Standard Grant,Andrew Lovinger,6/30/19,"$407,000.00 ",,TKYU@UAKRON.EDU,302 Buchtel Common,Akron,OH,443250001,3309722760,MPS,"1712, 1773","8091, 8396, 8399, 9178, 9251",$0.00 ,"NON-TECHNICAL SUMMARY:<br/><br/>The main concept of this project emerges from the neuronal circuits of the body as paradigms for novel types of solid-state batteries based on mechanisms operative in neurotransmission.   The brain controls various functions of the body through the nervous system composed of neuronal networks. Neurons are excitable, individual cells making specific contacts with other surrounding neurons. Their signal-processing is empowered by ion osmosis, driven by ion concentration gradients across the cell membrane which regulates passage of selective ions via ionic channels.  The concept of polymer-based solid lithium ion batteries to be explored in this project shares this common origin with neuronal networks, as it operates by harnessing ion concentration gradients across the proposed ""multilayered polymer electrolyte membranes"" (MLPEM) which contain different ion concentrations in each layer, thus generating an internal voltage. The proposed concentration-gradient approach to battery design is conceptually similar to the neuronal operation of an electric eel, whereby series of thousands of innervated and non-innervated cell membranes are capable of generating internal voltages of about 600 volts to fend off predators. Just as the neural network of the electric eel allows this voltage to be regenerated, the proposed MLPEM batteries could be rechargeable on their own.  The working principle of the self-rechargeable battery in this project is that the mobile lithium cation will be transported to the cathode during discharging, but it will revert back to the anode during battery resting, thereby restoring the ion concentration gradient and hence a voltage. This project will explore these aspects by synthesizing and processing multilayered polymer electrolyte membranes allowing ionic concentration gradients, evaluate and attempt to optimize the ionic conductivity, the thermal and electrochemical stability, and the mechanical properties of the battery.  If successful, this project may benefit society by leading to novel lightweight, shape-conformable, thermally and electrochemically stable, flame-retardant, self-rechargeable batteries. The project also includes integration of research and education through interdisciplinary training of students and outreach activities.<br/><br/><br/>TECHNICAL SUMMARY:<br/><br/>This project is inspired by the neuronal circuits of the body as paradigms for novel types of solid-state batteries based on mechanisms operative in neurotransmission, e.g. the generation of high voltages by electric eels followed by internal recharging. It focuses on five thrust areas: (1) Development of all-solid-state multilayered polymer electrolyte membranes (MLPEM) having specific chemical and electrochemical compatibility with electrodes for enhancing energy-storage capacity. MLPEM will be fabricated by stacking individual polymer electrolyte (PEM) layers having different ion populations by photopolymerizing network-precursor (poly(ethylene glycol) diacrylate)/solid plasticizer (succinonitrile)/ionic salt (lithium bis-trifluorosulfonylimide). The ion concentration gradient thus produced in MLPEM will create potential differences across the membrane interfaces, thereby affording self-rechargeability of the battery. (2) Fabrication of directionally aligned phase-separated domains having various concentration gradients via holographic photopolymerization-induced phase separation in multicomponent solid electrolytes containing plasticizer and modifiers as a means of creating networks of micro-electrolyte cells. (3) Synthesis of PEM additives such as amido-carbonyl carbamate and amido-carbamate to prevent uncontrolled solid electrolyte interface formation on electrodes. (4) Grafting of poly(ethylene glycol) diamine to multiwall carbon nanotube (MWCNT) followed by end-capped reaction with carbamate derivatives to improve interface compatibility of MLPEM with carbonaceous anode and concurrently increase in ionic conductivity. (5) Modification of MWCNT surface by grafting of lithiated PEG-chains and/or arborescent PEG to raise lithium ion storage capacity and provide separate pathways for electron and ion conductions. The network of lithiated arborescent hyperbranched PEG resembles a neuronal network structurally and functionally. The ion conductivity and mobility will be determined by AC impedance, solid-state NMR, and Raman spectroscopy. Electrochemical stability will be evaluated by means of cyclic voltammetry and galvanostatic charge/discharge cycling in half-cell configurations. By virtue of the self-restored potential difference between the electrodes afforded by the ion concentration gradient of MLPEM, the battery would be rechargeable in the rest state, thereby prolonging the battery life. The project includes integration of research and education through interdisciplinary training of students and outreach activities."
133,1464371,CRII: RI: Large-Scale Discovery and Organization of Subcategories and Parts from Image and Video Segments,IIS,Robust Intelligence,6/1/15,5/31/18,Fuxin Li,OR,Oregon State University,Standard Grant,Jie Yang,9/30/18,"$173,375.00 ",,lif@eecs.oregonstate.edu,OREGON STATE UNIVERSITY,Corvallis,OR,973318507,5417374933,CSE,7495,"7495, 8228, 9251",$0.00 ,"This project develops a system for understanding of visual categories given limited annotations. The system automatically detects subcategories and object parts with only category-level annotations. Such detailed understanding is important for autonomous systems to perform interactions with objects or to recognize them under occlusion. For this purpose, current deep neural networks will be extended to better support objects and parts of irregular shape. Once understandings at such level have been achieved, it helps to construct new categories from just a few exemplars, which has broad applications in autonomous systems.<br/><br/>This research generalizes previously successful approaches in semantic segmentation and unsupervised video segmentation for an efficient approach to learn subcategories and parts. The framework starts from overlapping figure-ground segment proposals, computes least squares regressors from input segments against segment overlaps, and utilizes the Sherman-Morrison-Woodbury formula and structures from the quadratic loss function for efficient optimization of thousands to hundreds of thousands of subcategories and parts simultaneously. This project then explores the training of deep convolutional networks with initializations from these subcategories and parts defined on free-form segments. This requires generalization of the neural network architecture to handle free-form segments that can deform through a video sequence. It is proposed to use the geodesic distance transform on spatial-temporal segments to define customized filters for different localities for improved performance and better interpretability."
134,1513116,CI-P: Towards the Creation of a Unified Repository for MultiLingual and CrossLingual Multiword Expressions,CNS,CCRI-CISE Cmnty Rsrch Infrstrc,6/15/15,6/2/15,Mona Diab,DC,George Washington University,Standard Grant,Tatiana Korelsky,8/31/17,"$100,000.00 ",,mtdiab@gwu.edu,1922 F Street NW,Washington,DC,200520086,2029940728,CSE,7359,7359,$0.00 ,"Single concepts that cross word boundaries, such as kick the bucket and traffic light, are typically referred to as multiword expressions (MWE). MWE usage is pervasive in natural languages. MWEs pose a significant challenge from a processing perspective. Explicitly identifying, classifying, and modeling MWEs in text is shown to improve NLP technologies.  The objective of this proposal is to conduct exploratory research and build consensus towards the creation of a Unified Repository for Multilingual and Cross-lingual MWE cutting across different languages (Arabic, Chinese, English, Spanish, Persian), genres and domains while adding contextual references and links to existing lexical resources. Understanding the space of MWE across different languages will have a significant impact on multilingual and cross-lingual NLP applications as well as studies in Linguistics in general. Such a large-scale resource will provide for a disciplined investigation into language universals and language-specific studies by adding insights into shared and varying cultural and language concepts as grounds for extensive typological and etymological studies of how concepts are formed across peoples. <br/><br/>Multiword Expressions (MWE) occupy a significant portion of the semantic space. With deeper understanding of MWE, their nature, behavior and usage, natural language processing (NLP) practitioners can build more robust systems to achieve the goal of Natural Language Understanding (NLU). Looking at a diverse set of languages simultaneously would lead to more insights into language universals and language specific phenomena having profound impact in how we design overall natural language solutions. The objective of this proposal is to conduct exploratory research and build consensus towards the creation of a Unified Repository for Multilingual and Cross-lingual MWE cutting across different genres and domains while adding contextual references and links to existing lexical resources such as WordNet. The vision is for a repository that is consistently annotated with links across 5 different languages: Arabic, Chinese, English, Spanish, and Persian.  This grant will support the following: i) investigating universal linguistic information that is common across MWEs in various languages as well as identifying differentiating features that are language-specific or found in typologically related languages; ii) conducting pilot annotation studies on essential annotation tasks; iii) building the basic infrastructure for harvesting, storing, and annotating the data; iv) developing tools to bootstrap the envisioned resource from existing resources; and, v) Organizing two 2-day workshops to gather input from leading researchers in the field and to build consensus on key issues for such a resource."
135,1451173,Doctoral Dissertation Research: Investigating cognitive and communicative pressures on natural language lexicons,BCS,DDRI Linguistics,3/15/15,3/11/15,Edward Gibson,MA,Massachusetts Institute of Technology,Standard Grant,William Badecker,8/31/16,"$11,984.00 ",Kyle Mahowald,egibson@mit.edu,77 MASSACHUSETTS AVE,Cambridge,MA,21394301,6172531000,SBE,8374,"1311, 9179",$0.00 ,"Understanding how humans produce and comprehend language is a critical step in understanding high-level human cognition and the human brain more generally. Moreover, basic research into human language has been, and will continue to be, useful for building computational natural language processing systems that enable humans to interact naturally with computers. The lexicons of the world's thousands of languages--that is, the sets of words that exist in any given language--offer a particularly rich source of insight into the language production and comprehension mechanism. The words of any given language have undergone thousands of years of evolution, sometimes changing dramatically over one or two generations as sounds change, new words are invented or borrowed from other languages, and old words die. What all languages have in common, however, is that they enable their speakers to successfully communicate with one another. Therefore, a language's lexicon is necessarily constrained by the cognitive and communicative demands of speakers. Consequently, studying the statistical properties of lexicons, the ways that lexicons evolve, and the process by which words are formed is a promising avenue for answering fundamental questions about human cognition.<br/><br/>Building on previous work by this research group showing that lexicons tend to be structured for efficient communication, this research will harness the power of large cross-linguistic data sets available through the Internet, including Wikipedia and Google Books, in order to study the lexicons of a large number of world languages (~100). Specifically, this analysis will focus on how words cluster or spread out in phonetic space, exploring competing demands for words to consist of easy-to-pronounce and easy-to-comprehend sequences but also to be phonetically distinct from one another. A second major component of this work is a series of human-participant behavioral experiments that, in a controlled laboratory setting and in a smaller number of languages, explore the mechanisms that underlie how words change over time. Finally, a computational model will be used to integrate the insights of the statistical analyses and behavioral experiments in order to explore and predict how words enter and exit the lexicon over time. This research program has implications not just for higher-level human cognition but for any engineering applications that require human-computer interaction involving natural language and also for any applications that require building a cognitively tractable communication system that allows people to communicate efficiently."
136,1523637,RI: Small: Deep Natural Language Understanding with Probabilistic Logic and Distributional Similarity,IIS,Robust Intelligence,9/1/15,11/30/15,Katrin Erk,TX,University of Texas at Austin,Standard Grant,D.  Langendoen,8/31/19,"$416,287.00 ",,katrin.erk@mail.utexas.edu,"3925 W Braker Lane, Ste 3.340",Austin,TX,787595316,5124716424,CSE,7495,"7495, 7923, 9251",$0.00 ,"The web offers huge amounts of information, but that also makes it harder to find and extract relevant information. Natural  language processing has made huge strides in developing tools that extract information and automatically answer questions, often with relatively simple methods aimed at relatively superficial analysis. This project explores methods for a deeper analysis and detailed natural language understanding. Contemporary intelligent systems have long used logic to describe precisely what a sentence means and how its pieces connect. But this precision has a downside: Logic needs the data to exactly match its expectations, or it breaks down. This is problematic for applications like question answering because language is hugely variable. There are often many different ways to say the same thing, or to say things that are not exactly the same but similar enough to be relevant. This project combines logic with a technology that identifies words and passages that are  similar but not exact matches. Also, language often only implies things rather than stating them outright. The project handles this through a mechanism that draws conclusions that are likely but not 100% certain, and that states its level of confidence in a conclusion. <br/><br/>Being highly interdisciplinary, the project gives students insights into logic and inferences, as well as methods that determine word similarity based on occurrences in large amounts of text. This project also forges new links between computational and theoretical linguistics by transferring ideas in both directions. Through its combination of precision and approximation, this project paves the way for language technology that understands language more deeply and thus will enhance societally important applications such as information extraction and and automatic question answering. Tasks in natural language semantics are requiring increasingly complex and fine-grained inferences. This project pursues the dual hypotheses that (a) logical form is best suited for supporting such inferences, and that (b) it is necessary to reason explicitly about uncertain, probabilistic information at the lexical level. This project combines logical form representations of sentence meaning with weighted inference rules derived from distributional similarity. It uses Markov Logic Networks for probabilistic inference over logical form with weighted rules, testing on the task of Recognizing Textual Entailment. It also develops new methods for describing word meaning in context distributionally in a way that is amenable to determining lexical entailment."
137,1454904,Reorganization of a Dopamine-Sensitive Locomotor Neural Network,IOS,Activation,8/1/15,6/29/20,Karen Mesce,MN,University of Minnesota-Twin Cities,Standard Grant,Sridhar Raghavachari,7/31/21,"$559,940.00 ",David Schulz,mesce001@umn.edu,200 OAK ST SE,Minneapolis,MN,554552070,6126245599,BIO,7713,"1096, 9178, 9179, 9232, 9251",$0.00 ,"Almost all living organisms need to move for their survival, and that locomotion is often rhythmic and highly coordinated across an animal's legs, wings, fins, or other body parts.  How such coordination is orchestrated is not well understood, but the consequences of losing nerve cells that are vital for locomotion, for example, during a spinal cord injury, are strikingly clear.  This project examines how the nervous system can be retuned after a significant perturbation or injury to regain its former ability to generate rhythmic patterns of locomotion.  Such reorganization addresses an emerging and highly significant problem in the neurosciences -that of understanding the cellular mechanisms of homeostatic plasticity.  Essentially, this plasticity enables a system to go back to its set point or regain its original operational status after a perturbation.  Currently, very little is known about locomotor-related homeostatic mechanisms.  To understand such events, at the level of individual neurons, this project will use a combination of cellular, molecular and behavioral methods to study how a recovering nervous system achieves its transformation.   <br/><br/>The scientific team has chosen to study the medicinal leech as a model of locomotor control and homeostatic plasticity because of its experimental accessibility and well-studied locomotor circuits. Its central pattern generators (CPGs) for crawling behavior have been shown to be regulated by dopamine, a universal modulator of motor activity in most animals; each of these CPGs is located within each and every segmental ganglion comprising the nerve cord.  The compelling aspect of this project is discovering how the crawl CPGs become retuned or reconfigured so that all timing events are fully restored without the physical reconnection of cephalic inputs.  Three hypotheses will be tested: 1) proprioceptive inputs from the body wall substitute for brain-specific timing events; 2) the CPG in the ganglion immediately below the site of nerve cord injury takes the lead in initiating and directing the metachronal crawl waves in the caudal direction; 3) changes in gene expression patterns in the lead CPG provide for a lower crawl-activation threshold to DA and/or proprioceptive inputs.  Experimental methods will include electrophysiology, immunocytochemistry, confocal imaging, behavioral video-capture, computational neuroscience, and cutting-edge single-cell quantitative PCR. This project's outcomes have potential to impact disparate fields outside of the biological sciences, including physics, math, computer science and engineering.  The planned development of a leech transcriptome will also propel the medicinal leech model into the era of genomics. The project will support the training of female and minority participants, several leech-based laboratory teaching modules, and community outreach projects involving a leech Critter-Cam for middle school students."
138,1452663,Collaborative Research: What's the question? A cross-linguistic investigation into compositional and pragmatic constraints on the question under discussion,BCS,Linguistics,6/1/15,5/20/15,David Beaver,TX,University of Texas at Austin,Standard Grant,Tyler Kendall,11/30/18,"$174,926.00 ",,dib@mail.utexas.edu,"3925 W Braker Lane, Ste 3.340",Austin,TX,787595316,5124716424,SBE,1311,"1311, 7298, 9179",$0.00 ,"The greatest value of language resides in its meaning, in the information that is exchanged in discourse. This project is part of a broad scholarly effort to find out how that meaning is determined, and to identify what inferences can be drawn from what is said. Recent research on meaning in language, including much published work by the four PIs on this project, shows that the meaning of a sentence is dramatically affected by the context in which the sentence is uttered. Specifically, the three-institution project team has shown in past work how the meaning of a sentence is greatly affected by what question the speaker seeks to address. The fact that a sentence meaning can only be fully grasped in terms of the question the uttered sentence answers creates a problem because the question addressed by a given utterance is typically implicit. So, often a listener can only fully understand the meaning of an uttered sentence by recreating the question that was answered. To do this, the listener must reason from many available clues, for example the intonation used in the utterance, and what is known of the speaker's intentions. The goal of this project is to conduct empirical and theoretical work which will identify the process by which implicit questions are revealed. <br/><br/>The work to be undertaken in this project is of intrinsic interest in understanding human culture and human communication, and also has significant practical applications, for example in the field of Natural Language Processing. To build computers that can understand and use language, the features that inform human language understanding must be identified. The project also offers advanced training opportunities for young researchers at US institutions, develops experimental and other research methodologies that can impact a broad range of fields, and analyzes a foundational problem in language which is related to strategically valuable natural language technology. <br/><br/>The project is set in the context of a Question Under Discussion model in which the context incorporates a dynamically evolving stack of questions. The problem for this model is to figure out what question is addressed by any given stretch of discourse. Three types of constraint on the question will be explored cross-linguistically: lexical constraints contributed by, for instance, focus sensitive expressions and factive predicates, information-structural constraints, including those imposed by intonation and cleft constructions, and contextual constraints, such as Gricean principles. This goal is pursued by expanding methodologies which the three-institution team has jointly pioneered in previous work: a mix of experimental research, corpus-based studies of naturally occurring utterances, and cross-linguistic fieldwork. The development effort is spearheaded with English and Paraguayan Guaraní (Tupí-Guaraní) and selective work is performed on other languages to test aspects of the work that are particularly germane, including K'iche' and Kaqchikel (both Mayan), Hungarian (Ugric) and Tagalog (Austronesian). Throughout the project, there will be an emphasis on the development of data-collection methods appropriate for use with theoretically untrained native speakers"
139,1452674,Collaborative Research: What's the question? A cross-linguistic investigation into compositional and pragmatic constraints on the question under discussion,BCS,Linguistics,6/1/15,7/23/20,Marie-Catherine de Marneffe,OH,Ohio State University,Standard Grant,Tyler Kendall,9/30/21,"$286,420.00 ",Craige Roberts,demarneffe.1@osu.edu,Office of Sponsored Programs,Columbus,OH,432101016,6146888735,SBE,1311,"1311, 7298, 9179",$0.00 ,"The greatest value of language resides in its meaning, in the information that is exchanged in discourse. This project is part of a broad scholarly effort to find out how that meaning is determined, and to identify what inferences can be drawn from what is said. Recent research on meaning in language, including much published work by the four PIs on this project, shows that the meaning of a sentence is dramatically affected by the context in which the sentence is uttered. Specifically, the three-institution project team has shown in past work how the meaning of a sentence is greatly affected by what question the speaker seeks to address. The fact that a sentence meaning can only be fully grasped in terms of the question the uttered sentence answers creates a problem because the question addressed by a given utterance is typically implicit. So, often a listener can only fully understand the meaning of an uttered sentence by recreating the question that was answered. To do this, the listener must reason from many available clues, for example the intonation used in the utterance, and what is known of the speaker's intentions. The goal of this project is to conduct empirical and theoretical work which will identify the process by which implicit questions are revealed. <br/><br/>The work to be undertaken in this project is of intrinsic interest in understanding human culture and human communication, and also has significant practical applications, for example in the field of Natural Language Processing. To build computers that can understand and use language, the features that inform human language understanding must be identified. The project also offers advanced training opportunities for young researchers at US institutions, develops experimental and other research methodologies that can impact a broad range of fields, and analyzes a foundational problem in language which is related to strategically valuable natural language technology. <br/><br/>The project is set in the context of a Question Under Discussion model in which the context incorporates a dynamically evolving stack of questions. The problem for this model is to figure out what question is addressed by any given stretch of discourse. Three types of constraint on the question will be explored cross-linguistically: lexical constraints contributed by, for instance, focus sensitive expressions and factive predicates, information-structural constraints, including those imposed by intonation and cleft constructions, and contextual constraints, such as Gricean principles. This goal is pursued by expanding methodologies which the three-institution team has jointly pioneered in previous work: a mix of experimental research, corpus-based studies of naturally occurring utterances, and cross-linguistic fieldwork. The development effort is spearheaded with English and Paraguayan Guaraní (Tupí-Guaraní) and selective work is performed on other languages to test aspects of the work that are particularly germane, including K'iche' and Kaqchikel (both Mayan), Hungarian (Ugric) and Tagalog (Austronesian). Throughout the project, there will be an emphasis on the development of data-collection methods appropriate for use with theoretically untrained native speakers"
140,1452669,Collaborative Research: What's the question? A cross-linguistic investigation into compositional and pragmatic constraints on the question under discussion,BCS,Linguistics,6/1/15,5/20/15,Mandy Simons,PA,Carnegie-Mellon University,Standard Grant,Tyler Kendall,11/30/19,"$81,998.00 ",,simons@andrew.cmu.edu,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,SBE,1311,"1311, 7298, 9178, 9179",$0.00 ,"The greatest value of language resides in its meaning, in the information that is exchanged in discourse. This project is part of a broad scholarly effort to find out how that meaning is determined, and to identify what inferences can be drawn from what is said. Recent research on meaning in language, including much published work by the four PIs on this project, shows that the meaning of a sentence is dramatically affected by the context in which the sentence is uttered. Specifically, the three-institution project team has shown in past work how the meaning of a sentence is greatly affected by what question the speaker seeks to address. The fact that a sentence meaning can only be fully grasped in terms of the question the uttered sentence answers creates a problem because the question addressed by a given utterance is typically implicit. So, often a listener can only fully understand the meaning of an uttered sentence by recreating the question that was answered. To do this, the listener must reason from many available clues, for example the intonation used in the utterance, and what is known of the speaker's intentions. The goal of this project is to conduct empirical and theoretical work which will identify the process by which implicit questions are revealed. <br/><br/>The work to be undertaken in this project is of intrinsic interest in understanding human culture and human communication, and also has significant practical applications, for example in the field of Natural Language Processing. To build computers that can understand and use language, the features that inform human language understanding must be identified. The project also offers advanced training opportunities for young researchers at US institutions, develops experimental and other research methodologies that can impact a broad range of fields, and analyzes a foundational problem in language which is related to strategically valuable natural language technology. <br/><br/>The project is set in the context of a Question Under Discussion model in which the context incorporates a dynamically evolving stack of questions. The problem for this model is to figure out what question is addressed by any given stretch of discourse. Three types of constraint on the question will be explored cross-linguistically: lexical constraints contributed by, for instance, focus sensitive expressions and factive predicates, information-structural constraints, including those imposed by intonation and cleft constructions, and contextual constraints, such as Gricean principles. This goal is pursued by expanding methodologies which the three-institution team has jointly pioneered in previous work: a mix of experimental research, corpus-based studies of naturally occurring utterances, and cross-linguistic fieldwork. The development effort is spearheaded with English and Paraguayan Guaraní (Tupí-Guaraní) and selective work is performed on other languages to test aspects of the work that are particularly germane, including K'iche' and Kaqchikel (both Mayan), Hungarian (Ugric) and Tagalog (Austronesian). Throughout the project, there will be an emphasis on the development of data-collection methods appropriate for use with theoretically untrained native speakers"
141,1545574,"Collaborative Research: A Neurodynamic Programming Approach for the Modeling, Analysis, and Control of Nanoscale Neuromorphic Systems",ECCS,EPCN-Energy-Power-Ctrl-Netwrks,4/1/15,8/16/16,Silvia Ferrari,NY,Cornell University,Continuing grant,Radhakisan Baheti,8/31/17,"$93,739.00 ",,ferrari@cornell.edu,373 Pine Tree Road,Ithaca,NY,148502820,6072555014,ENG,7607,"092E, 102E, 1653, 9102",$0.00 ,"The objective of this research is to develop new neurodynamic programming (NDP) learning algorithm for controlling neuron-level activity (spiking) and synaptic-level plasticity in CMOS/memristor devices, such that the subsequent system-level response achieves desired sensorimotor behavioral goals.  The approach is to uses a radically new training paradigm that induces functional plasticity by controlling the neural activity of selected input neurons via programming voltages, rather than by directly manipulating the synaptic weights, as do virtually all existing training algorithms.<br/><br/>Intellectual merit<br/>This research aims to develop a model of the closures required to translate synaptic-level plasticity into functional-level plasticity that results into high-level behavioral goals and problem solving abilities.  The same critical challenge has been identified in neuroscience research aimed at reverse engineering the brain, and in the regulation of deep-brain stimulation (DBS).  Due to this knowledge gap, even when a measure of adequate or desired behavior is available, it may not be easily utilized to stimulate a neural network at the cell level in order to produce the appropriate macroscopic behavior.<br/><br/>Broader impact<br/>The learning model developed in this research will be used toward the development of nanoscale neuromorphic systems that mimic neuro-biological architectures in the nervous system.  Thanks to their abilities to recreate the synaptic plasticity, device density, scalability, and fault-tolerance of biological neuronal networks, these neuromorphic systems can enable a wide range of technological advancements, such as intelligent robots with highly-sophisticated sensorimotor skills, and neuroprosthetic devices capable of adapting to changing conditions and environments."
142,1542303,North American Chapter of the Association for Computational Linguistics (NAACL-HLT) 2015 Student Research Workshop,IIS,ROBUST INTELLIGENCE,6/1/15,4/30/15,Smaranda Muresan,NY,Columbia University,Standard Grant,Tatiana D. Korelsky,5/31/16,"$15,000.00 ",,smara@columbia.edu,2960 Broadway,NEW YORK,NY,100276902,2128546851,CSE,7495,"7495, 7556",$0.00 ,"The Association for Computational Linguistics (ACL) is the primary international scientific organization in the fields of computational linguistics, natural language processing and human language technologies.  The annual meeting of its North American chapter (NAACL-HLT) is one of the most prestigious and selective international conferences in these fields.  The goal of this grant is to subsidize travel, conference and housing expenses for students selected to participate in the NAACL-HLT 2015 Student Research Workshop, which will take place during the conference May 31 - June 5, 2015 in Denver, Colorado. The workshop aims to provide valuable research opportunity for students at different stages of their academic career (undergraduate, early graduate and advanced graduate). Accepted papers are presented as posters during the main poster session of the conference. This format has the advantages of both allowing longer, more personalized interaction between the presenting student and interested researchers, and attracting a larger audience among conference attendees who are already at the general poster session.  In addition, each accepted paper is assigned a mentor. The mentor is a senior researcher from academia or industry who will prepare in-depth comments before the poster session and will provide feedback to the student author.  The workshop is organized and run by students. The NAACL-HLT Student Research Workshop contributes to the development of a skilled and diverse computational linguistics and natural language processing workforce.  This year, two of the student organizers are female graduate students, which will help promote and broaden the participation of females in natural language processing and computational linguistics fields.<br/><br/>The Student Research Workshop (SRW) is an established tradition at ACL and NAACL conferences, allowing students to present their research and receive feedback from senior researchers.  The goal is to aid students at multiple stages in their education, from senior-level undergraduates to graduate students at different stages. Therefore, the workshop allows three types of papers: 1) thesis proposals; 2) research papers by graduate students, and 3) for the first time, a special undergraduate research track. The first option is appropriate for seasoned graduate students who wish to get feedback on their proposal and strengthen their final work. The second track, research papers by graduate students, is most appropriate for graduate students new to academic conferences. The third option is specifically dedicated to undergraduate students. The SRW also provides valuable experience for the student organizers, who are directly involved in recruiting reviewers, managing the review process and running the workshop. In addition, the students who are involved in running and reviewing for the Student Research Workshop have ample opportunities to interact with the researchers on the organizing committee of the main conference. Overall, the opportunities for interaction with other students and with senior researchers will positively influence the students' experience in research and will likely inspire many to devote further effort to academic studies and careers.  To sustain the outreach tradition of the student workshop, this grant aims to subsidize travel, conference and housing expenses for both student participants and student organizers."
143,1521481,STTR Phase I:  Non-Invasive Through-The-Eyelid Tonometer for Frequent Eye Pressure Measurements,IIP,STTR Phase I,7/1/15,6/27/15,Peter Polyvas,AZ,EPVSENSORS Limited Liability Company,Standard Grant,Jesus Soriano Molla,6/30/16,"$225,000.00 ",Jonathan Sprinkle,polyvasp@epvsensors.com,4949 E. Alta Vista St,Tucson,AZ,857122011,5203196628,ENG,1505,"1505, 8018, 8023, 8032, 8042",$0.00 ,"The broader impact/commercial potential of this Small Business Technology Transfer (STTR) Phase I project will be in the ability to better screen for and manage glaucoma. Americans over the age of 40 need to be screened for elevated eye pressure which may lead to glaucoma and blindness. This STTR project aims to develop a non-invasive low-cost instrument that will allow frequent eye pressure measurements through the eye lid. Approximately 4 million Americans suffer from elevated eye pressure and require regular monitoring, the technology developed under this grant will improve public health, reduce cost of eye pressure measurements and will lead to an annual market opportunity in excess of 1B US dollars.  Given the universal need for convenient eye pressure measurement systems a similar market opportunity exists in Europe and the developed countries of Asia and Latin America.<br/>  <br/><br/>The proposed project will develop a patient-specific calibration method. Through-the-eye lid tactile tonometry is an indirect pressure measurement technique. While universal calibration may still produce relative pressure measurements, a patient specific calibration will improve the accuracy to a level comparable with existing instruments such as Goldman applanation tonometers. The proposed research will examine the ability of a single measurement to produce an accurate patient-specific calibration.  The proposed approach utilizes an artificial neural network trained a-priory by a finite element model of the human eye capturing easy-to-measure model parameters. It is anticipated that the accuracy of the technique can reach +/- 5 mmHg or better with respect to Goldman applanation tonometry."
144,1515462,EAPSI:Neutron Events Study of Kaon Decays for Matter-Antimatter Asymmetry in Our Universe,OISE,EAPSI,6/1/15,6/5/15,Stephanie Su,MI,Su                      Stephanie      Y,Fellowship,Anne Emig,5/31/16,"$5,070.00 ",,,,Ann Arbor,MI,481052473,,O/D,7316,"5921, 5978, 7316",$0.00 ,"Much physics and astronomy research have been carried out to understand the early state and the development of the universe. Our current living visible world is mainly composed of matter rather than antimatter. However, the Big Bang should have created equal amounts of matter and antimatter. In the elementary physics study, charge-parity (CP) symmetry violation can explain the matter-antimatter asymmetry phenomenon. The KOTO experiment studies a rare neutral Kaon decay, which is a direct CP violating decay. The summer research will focus on the study neutron events in the experiment, which are major events that contribute to the background to identify the rare neutral Kaon decay. This research will be conducted at Osaka University, as well as at Japan Proton Accelerator Research Complex (J-PARC), in collaboration with Professor Taku Yamanaka.<br/><br/>The Standard Model provides a clean calculation on the branching ratio of the rare neutral Kaon decay into a pion and two neutrinos. By measuring the branching ratio of this decay, we can eliminate many currently established physics models and perhaps discover new physics. Due to the missing neutrinos that cannot be detected by the detector, event signals for the rare decay will appear to have large transverse momentum. However, a neutron event generated upstream of the detector or inside the calorimeter could possibly appear to be the same. The research will use the normalization of Kaon decays with final particles to be three pions, two pions, or two photons to reconstruct the decay vertex. It will focus on the analysis on cluster information of neutron events using Monte Carlo simulation, along with known neutron events, to distinguish them from other events. Once the clustering information for the neutron events is analyzed, neural network will be trained to identify neutron events. This NSF EAPSI award is funded in collaboration with the Japan Society for the Promotion of Science."
145,1550905,ComputEL: A workshop to explore the use of computational methods in the study of endangered language,BCS,DEL,8/1/15,7/25/19,Lane Schwartz,IL,University of Illinois at Urbana-Champaign,Standard Grant,D.  Langendoen,7/31/20,"$24,587.00 ",,lanes@illinois.edu,1901 South First Street,Champaign,IL,618207406,2173332187,SBE,7719,"1311, 7495, 7556, 7719",$0.00 ,"In June 2014, with support from NSF, the first ComputEL workshop was held in conjunction with the 52nd Annual Meeting of the Association for Computational Linguistics (ACL) in Baltimore, Maryland. The goal of that workshop was to serve as an initial meeting point between computer scientists and computational linguists active in ACL and documentary linguists engaged in work on endangered languages. This proposal seeks to build on lessons learned in the first ComputEL workshop, and to encourage new collaborations between the two research communities. Contemporary efforts to document the world's endangered languages are dependent on the widespread availability of modern recording technologies, in particular digital audio and video recording devices and software to annotate the recordings that such devices produce. However, despite well over a decade of funding programs dedicated to the documentation of endangered languages, the technological landscape that supports the activities of those involved in this work remains fragmented, and the promises of new technology remain largely unfulfilled. Moreover, the efforts of computer scientists, on the whole, are mostly disconnected from the day-to-day work of documentary linguists, making it difficult for the knowledge of each group to inform the other. <br/><br/>The second ComputEL workshop will address this state of affairs by bringing together field linguists and computer scientists to explore how computational techniques and tools can be created to better aid in the analysis of endangered languages. A particular focus will be on how to facilitate the process through which research results in this domain can be developed into software that is well-supported for use by those documenting endangered languages in the field. The workshop will be co-located with the Conference on Empirical Methods in Natural Language Processing (EMNLP) in Austin, Texas, in October 2016. This will allow it to reach a wider audience than would otherwise be possible, while taking advantage of its location to involve participants from nearby areas. On the first day, participants will take part in discussions as part of a closed meeting that will be designed to lay out a near-term agenda for the improved application of computational methods to the study of endangered languages. The second day will consist of research presentations scheduled jointly with the main EMNLP conference. The papers from the workshop will be published in the Association for Computational Linguistics conference proceedings, and an additional report will be produced summarizing the conclusions of the closed meeting. Smaller speaker communities worldwide will benefit from the results which will support better documentation and preservation of endangered languages.<br/>"
146,1529929,Group Travel Grant for the Doctoral Consortium of the IEEE Conference on Computer Vision and Pattern Recognition,IIS,"Information Technology Researc, Robust Intelligence",6/1/15,4/23/15,Adriana Kovashka,PA,University of Pittsburgh,Standard Grant,Jie Yang,5/31/16,"$15,050.00 ",,kovashka@cs.pitt.edu,300 Murdoch Building,Pittsburgh,PA,152603203,4126247400,CSE,"1640, 7495","7495, 7556",$0.00 ,"This grant partially supports the participation of 20 students from US institutions in the Doctoral Consortium at the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2015. CVPR is the premier annual conference in computer vision with about 2000 senior, junior and student participants. It is held in North America and attended by members of the international research community. The goal of the Doctoral Consortium is to highlight the work of senior PhD students who are within six months of receiving their degrees (including recent graduates), and to give these students an opportunity to discuss their research and career options with faculty and researchers who have relevant expertise. NSF support covers some of the costs for selected US graduate students to attend the conference.<br/><br/>The intellectual merit of this project rests in the selection of top-quality PhD students whose research is showcased and to whom feedback is provided by senior researchers. The opportunity to receive advice on their research work and career plans from experts from different institutions, and with potentially different perspectives, is often not available internally at one's own institution. The broader impacts of this project include supporting the career development of some of the brightest junior researchers in computer vision, contributing to the research community in general by drawing attention to an important aspect of graduate student development, potentially increasing the number of active researchers and educators in STEM, and ensuring that the computer vision community, through its recent graduates, makes fast advances in solving problems that will benefit society as a whole. The Doctoral Consortium aims to have representation from a diverse group of participants in terms of gender, ethnic background, academic institution, and geographic location."
147,1461121,REU Site: NSF Research Experience for Undergraduates in Computer Vision,CNS,RSCH EXPER FOR UNDERGRAD SITES,5/1/15,8/2/16,Mubarak Shah,FL,The University of Central Florida Board of Trustees,Standard Grant,Harriet Taylor,4/30/18,"$517,241.00 ",Niels da Vitoria Lobo,shah@eecs.ucf.edu,4000 CNTRL FLORIDA BLVD,Orlando,FL,328168005,4078230387,CSE,1139,"7556, 9250",$0.00 ,"This project represents a continuation of a Research Experience for Undergraduates site in Computer Vision which has operated successfully at the University of Central Florida for the past twenty-seven years. Approximately two hundred and seventy undergraduate students from sixty five  institutions all over the country have participated in this program over the years.  Each year, students will participate in a 12-week duration full-time Summer program.  In past years, a substantial fraction of participants have been able to prepare a paper for submission to a refereed conference, have the paper accepted and then attend the conference to present the paper. The program will build these skills in participants: ability to write complex computer programs; ability to communicate orally about a research problem; ability to present one's project via technology such as PowerPoint; ability to do a poster presentation about the project, and answer questions; ability to think about computer vision issues in real life problems; ability to do initial reading of a research paper in computer vision; ability to contribute to discussion of a vision problem; and ability to embark on applying to graduate school. <br/><br/><br/>The research emphasizes development of new algorithms for solving scientific problems in visual geo-localization and segmentation, object detection, multi-target tracking, activity and event recognition.  It advances both theory and practice; in mathematical modeling and analysis of difficult vision problems and developing algorithms, while at the same time building real systems for demonstrating those solutions in real life situations.  The educational model employed by this site includes round-the-clock mentoring by a team that includes a professor and a  graduate student; a streamlined short course that lets participants start their research projects sooner; daily meetings with mentors to plan activities throughout the day; training in MatLab for quick turnaround of research ideas. Participants take the short course on fundamentals of computer vision, match themselves to a project topic that they most desire, and spend sufficient time in focused research. Some possible research projects include: Location-Aware Semantic Segmentation; Multi-target Tracking with Social Behavior, Model, Human Action Recognition etc. After summer, students then can opt for follow-through over the year by working with the professors to write a technical report on their project, to prepare for the GREs and to apply to graduate programs."
148,1542337,"Group Travel Grant for the Doctoral Consortium at the International Conference on Computer Vision (ICCV) 2015; Dec 11 - 18, 2015; Santiago, Chile",IIS,"Information Technology Researc, Robust Intelligence",8/1/15,7/23/15,Dhruv Batra,VA,Virginia Polytechnic Institute and State University,Standard Grant,Jie Yang,7/31/16,"$15,000.00 ",,dbatra@vt.edu,Sponsored Programs 0170,BLACKSBURG,VA,240610001,5402315281,CSE,"1640, 7495","7495, 7556",$0.00 ,"This grant partially supports the participation of about 20 students from US institutions in the Doctoral Consortium at the International Conference on Computer Vision (ICCV) 2015. ICCV is the premier biennial conference in computer vision with about 2000 senior, junior and student participants. It is attended by members of the international research community. The goal of the Doctoral Consortium is to highlight the work of senior PhD students who are who are close to finishing their degrees, or recent graduates, and to give these students an opportunity to discuss their research and career options with faculty and researchers who have relevant expertise. NSF support covers some of the costs for selected US graduate students to attend the conference.<br/><br/>The intellectual merit of this project rests in the selection of top-quality PhD students whose research is showcased and to whom feedback is provided by senior researchers. The opportunity to receive advice on their research work and career plans from experts from different institutions, and with potentially different perspectives, is often not available internally at one's own institution. The broader impacts of this project include supporting the career development of some of the brightest junior researchers in computer vision, contributing to the research community in general by drawing attention to an important aspect of graduate student development, potentially increasing the number of active researchers and educators in STEM, and ensuring that the computer vision community, through its recent graduates, makes fast advances in solving problems that will benefit society as a whole. The Doctoral Consortium aims to have representation from a diverse group of participants in terms of gender, ethnic background, academic institution, and geographic location."
149,1541033,CRISP Type 2: Collaborative Research: Simulation-Based Hypothesis Testing of Socio-Technical Community Resilience Using Distributed Optimization and Natural Language Processing.,SES,"Information Technology Researc, Special Projects - CNS, CYBERINFRASTRUCTURE, EFRI Research Projects",10/1/15,9/10/15,Leonardo Duenas Osorio,TX,William Marsh Rice University,Standard Grant,Robert O'Connor,9/30/20,"$517,968.00 ",,leonardo.duenas-osorio@rice.edu,6100 MAIN ST,Houston,TX,770051827,7133484820,SBE,"1640, 1714, 7231, 7633","008Z, 029E, 036E, 039E",$0.00 ,"Critical infrastructures, such as electric, manufacturing, and financial systems, are key to the functioning of society and the health of communities. The new knowledge from this project will improve the design and management of critical infrastructure to build resilience in the face of minor disruptions and large disasters. The project focuses on the social and technical links between different types of critical infrastructure. The research provides insights into the influence of social forces on critical infrastructure and the roles of critical infrastructure in promoting a community's identity and well-being. The knowledge and tools generated from this research inform strategies to improve the functioning and operation of critical infrastructure in order to achieve socially defined goals. Houston and Seattle area experts and decision makers contribute to and evaluate project outcomes to ensure that resulting tools are relevant to stakeholders concerned with increasing community resilience capacity. <br/><br/>Research methods from civil engineering, computer science, and social science combine to achieve the goals of the project. Three primary goals of the research are to 1) systematically rethink critical infrastructure as a web of social and technical systems, 2) build computer simulation models to explore critical infrastructure performance after major and minor disruptions, and 3) test hypotheses to appreciate how critical infrastructure can improve resilience and support the diverse needs of communities. The research team integrates qualitative and quantitative data to construct the project's simulation models. The scholars compile and analyze qualitative data about past critical infrastructure disruptions and disasters from many text sources, such as social media, news stories, government documents, and industry reports. They use new natural language processing (NLP) methods analyze the text data in order to identify key variables describing critical infrastructure and community resilience, as well as the relationships among them. They collect quantitative and geographic data describing related variables from existing sources, such as government and academic databases. They elicit quantitative data also from experts using customized survey techniques during facilitated workshops. They then use the results of the data analysis to specify computer models that simulate the many events, resource exchanges, and decisions that occur across multiple geographic scales after critical infrastructure disruptions and disasters. Finally, the team devises techniques to integrate and optimize the constructed computer models. This permits efficient testing of hypotheses about the relationships between critical infrastructure performance and community resilience."
150,1526033,RI: Small: Inferring Non-Rigid Geometry from Object Categories,IIS,Robust Intelligence,9/1/15,9/17/15,Simon Lucey,PA,Carnegie-Mellon University,Continuing Grant,Jie Yang,8/31/19,"$460,000.00 ",,slucey@cs.cmu.edu,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,CSE,7495,"7495, 7923",$0.00 ,"This project integrates new theoretical developments in group sparse coding and non-rigid structure from motion (NRSFM) within model-based methods for computer vision. Geometry is at the heart of visual perception. Humans invert the procedure of 3D to 2D projection effortlessly, blissfully ignorant of the mathematics required to make such inversion possible. Computer vision has been striving to unlock these mathematical secrets for the past few decades, with the view that to create any machine that truly ""sees"" it must be able to perform a similar inversion from 2D to 3D. Inferring the camera position and the 3D structure of a scene/object from an ensemble of 2D projected points is known within the field of computer vision as structure from motion (SFM).  By definition a static 3D structure is rigid, however, the set of 3D structures with the same object category label is inherently non-rigid; making large-scale NRSFM crucial for model-based category classification and detection. <br/><br/>Model-based methods for object category classification and detection attempt to understand the interplay between an object's projected photometric appearance and its underlying geometry. These methods, however, have largely been abandoned in computer vision over the last two decades in favor of methods that rely solely on appearance (i.e. view-based approaches). As the space of computer vision and robotics continues to merge it is becoming increasingly important to not only recognize an object, but also understand how to grasp or interact with it - a task much more suited to a model-based methodology. Further, as the space of augmented reality becomes more sophisticated it is clear that 3D understanding of a scene/object is crucial - something that model-based approaches to perception naturally provide. Finally, vision machines are demanding an increasingly deeper understanding of how the visual world is allowed to vary during learning. A model-based framework can naturally accommodate this type of 3D geometric variation within a learning framework."
151,1541025,CRISP Type 2: Collaborative Research: Simulation-Based Hypothesis Testing of Socio-Technical Community Resilience Using Distributed Optimization and Natural Language Processing,SES,"CIS-Civil Infrastructure Syst, EFRI Research Projects",10/1/15,6/13/18,Scott Miles,WA,University of Washington,Standard Grant,Robert O'Connor,2/28/21,"$1,224,929.00 ","Mehran Mesbahi, Noah Smith",milessb@uw.edu,4333 Brooklyn Ave NE,Seattle,WA,981950001,2065434043,SBE,"027y, 1631, 7633","008Z, 029E, 036E, 039E, 9251",$0.00 ,"Critical infrastructures, such as electric, manufacturing, and financial systems, are key to the functioning of society and the health of communities. The new knowledge from this project will improve the design and management of critical infrastructure to build resilience in the face of minor disruptions and large disasters. The project focuses on the social and technical links between different types of critical infrastructure. The research provides insights into the influence of social forces on critical infrastructure and the roles of critical infrastructure in promoting a community's identity and well-being. The knowledge and tools generated from this research inform strategies to improve the functioning and operation of critical infrastructure in order to achieve socially defined goals. Houston and Seattle area experts and decision makers contribute to and evaluate project outcomes to ensure that resulting tools are relevant to stakeholders concerned with increasing community resilience capacity. <br/><br/>Research methods from civil engineering, computer science, and social science combine to achieve the goals of the project. Three primary goals of the research are to 1) systematically rethink critical infrastructure as a web of social and technical systems, 2) build computer simulation models to explore critical infrastructure performance after major and minor disruptions, and 3) test hypotheses to appreciate how critical infrastructure can improve resilience and support the diverse needs of communities. The research team integrates qualitative and quantitative data to construct the project's simulation models. The scholars compile and analyze qualitative data about past critical infrastructure disruptions and disasters from many text sources, such as social media, news stories, government documents, and industry reports. They use new natural language processing (NLP) methods analyze the text data in order to identify key variables describing critical infrastructure and community resilience, as well as the relationships among them. They collect quantitative and geographic data describing related variables from existing sources, such as government and academic databases. They elicit quantitative data also from experts using customized survey techniques during facilitated workshops. They then use the results of the data analysis to specify computer models that simulate the many events, resource exchanges, and decisions that occur across multiple geographic scales after critical infrastructure disruptions and disasters. Finally, the team devises techniques to integrate and optimize the constructed computer models. This permits efficient testing of hypotheses about the relationships between critical infrastructure performance and community resilience."
152,1505087,BOOST:Bridge Opportunities Offered for the Sophomore Transition,DUE,"ENGINEERING RESEARCH CENTERS, IUSE",8/15/15,8/13/15,Deborah Won,CA,California State L A University Auxiliary Services Inc.,Standard Grant,Abby Ilumoka,7/31/18,"$249,917.00 ","Adel Sharif, Arturo Pacheco-Vega, Gustavo Menezes",dwon@calstatela.edu,5151 State University Drive,Los Angeles,CA,900324221,3233433648,EHR,"1480, 1998","8209, 9178",$0.00 ,"Current education research identifies a critical need for students to develop their ""engineering identity.""  The literature points to solutions that help students explore engineering and encourage peer-to-peer as well as student-to-faculty interaction to promote early assimilation into engineering colleges. The College of Engineering, Computer Science, and Technology (ECST) at California State University Los Angeles (CSULA) has recently institutionalized a high school-to-freshman Summer Transition to ECST Program (STEP) to support engineering freshmen, a majority of whom are low-income, underrepresented minorities (URMs), and first-generation college students. STEP has successfully led to significantly higher levels of math and English placement, and yet, the ECST sophomores are no exception to the pervasive ""sophomore slump"" experienced nationwide.  Unlike typical bridge programs, the Bridge Opportunities Offered for Sophomore Transition (BOOST) project will target engineering students transitioning between the freshman and sophomore years.  BOOST will fit nicely between an existing college-funded freshman-to-sophomore bridge program, a first-year experience that will be created in ECST with new funding from the Chancellor's Office to reduce the achievement gap, and a grass-roots revamp of the sophomore engineering core.  The literature identifies the large population of URM students that leave science, technology, engineering, and mathematics (STEM) majors as one of the main sources of potential STEM professionals.  BOOST has the potential to create an innovative model for ensuring the success of rising URM sophomores in engineering. In addition to giving students a foretaste of engineering, BOOST will also provide community engagement, as well as career development opportunities, which are expected to motivate ECST's largely URM, first-generation engineering student body to persist and excel in engineering while ultimately positively impacting the nation's ability to increase and diversify its STEM workforce.<br/><br/>The investigators will explore the impact of design-focused, urban-centered service learning on the development of engineering identity and ultimately on the retention of engineering students.  The project will include the development of a research tool that will help to identify at-risk students early, so as to better target students who can most benefit from the additional support from BOOST.  The investigators will address not only whether but also how BOOST will help their students to succeed in engineering as well as whether or not they are targeting the students who will benefit most from the additional support of BOOST. To do so, they will develop an adaptive artificial neural network (ANN) that will model the relationships among various student factors and the probability of student success.  The research will address the questions: what aspects of the program (service learning, urban setting, practical design experience, exposure to engineering concepts are needed in subsequent courses, what is the optimal program timing between freshman and sophomore year, what collaborations (as members of a team) are most beneficial to students' development of engineering identity and ultimately, to their success?  Assessment and evaluation will be conducted using mixed methods, including, but not limited to, cognitive assessments such as course examinations and concept inventories, and affective assessments such as questions on the College Pedagogical Practice Inventory.  The anticipated local outcome will be a 10% to 15% improvement in second year retention and six year graduation rates in ECST.  The project will potentially lead to a model for improving student success at other similar institutions."
153,1541647,Algebraic Vision Conference 2015,DMS,COMPUTATIONAL MATHEMATICS,9/1/15,9/3/15,Rekha Thomas,WA,University of Washington,Standard Grant,rosemary renaut,8/31/16,"$19,200.00 ",,thomas@math.washington.edu,4333 Brooklyn Ave NE,Seattle,WA,981950001,2065434043,MPS,1271,"7556, 9263",$0.00 ,"This travel grant will support for US-based participants at the first meeting on Algebraic Vision to be held at the Technical University in Berlin from October 8-9, 2015. Algebraic Vision is an emerging research area at the intersection of computer vision and mathematics that seeks to develop the algebro-geometric foundations of fundamental geometric problems that arise in the reconstruction of 3-dimensional scenes and cameras from noisy 2-dimensional images. This structural understanding will allow the possibility of designing specialized algorithms for the many practical problems in vision on the one hand, while also exposing pure mathematicians to rich and structured algebraic problems that arise in applications. This exchange has the potential to benefit and enrich computer vision, algebraic geometry and polynomial optimization.<br/><br/>The primary goal of the meeting is to initiate connections between researchers in computer vision, algebraic geometry and polynomial optimization so as to identify a core set of problems that can benefit from established tools in the three fields. For this purpose, the meeting agenda will revolve around short talks by experts from both sides with plenty of time in between for discussions. Young researchers will have the opportunity to be exposed to this new exchange of problems and ideas which will create the possibility of training a new generation with a common set of sophisticated and useful skills from these three closely related fields."
154,1464252,CRII: RI: What do you mean? -- Automatic identification of inferences drawn from text,IIS,Robust Intelligence,2/1/15,2/4/15,Marie-Catherine de Marneffe,OH,Ohio State University,Standard Grant,D.  Langendoen,1/31/19,"$143,374.00 ",,demarneffe.1@osu.edu,Office of Sponsored Programs,Columbus,OH,432101016,6146888735,CSE,7495,"7495, 8228",$0.00 ,"When dealing with language, readers/hearers understand far more than only the literal meaning of the words they read/hear. For instance, if your son's teacher tells you ""I doubt your son will pass his exam"", you will infer that your son will probably fail the exam. This project investigates how to automatically derive systematic inferences that people commonly draw. To approximate human understanding, it is essential for natural language processing (NLP) systems to develop broad-coverage models that capture what is conveyed in language without being explicitly said. The project focuses on how events are perceived: in the example above, do people believe that the son will pass? Accurately identifying events that are agreed upon and taken as facts has implications for any NLP task that require an accurate inference process, such as in information extraction.  The outcomes of the project consist of a better grasp of how linguistic insights can be used to automatically approximate human-level understanding, and publicly available data that will serve to develop robust, broad-coverage NLP systems as well as to evaluate and sharpen linguistic theories.<br/> <br/>The goal of automatically deriving common inferences is pursued by developing classifiers that bring in, as features, linguistic insights studied in semantics and pragmatics, and by constructing a dataset of naturally occurring examples, from different genres, annotated with humans' intuitions via crowdsourcing techniques. A large body of work in NLP is recently focusing on the power of ""surface"" features for NLP tasks. But such features are reaching a limit. This project demonstrates how specialized linguistic features go beyond what can be approximated with surface-level information given available data and leads to fundamental advances in NLP systems. Independently of performance on NLP tasks, semantic and pragmatic features are of interest from a theoretical linguistics perspective. By quantitatively studying the interactions of linguistic features on a large amount of naturally-occurring examples, this project has an impact not only for NLP but also for semantic and pragmatic theories."
155,1463493,US-Ireland Partnership Programme: Bridge Health Monitoring Using Cameras and Computer Vision Methods,CMMI,"Structural and Architectural E, International Research Collab",6/1/15,6/19/19,Fikret Catbas,FL,The University of Central Florida Board of Trustees,Standard Grant,Caglar Oskay,11/30/20,"$374,306.00 ",,catbas@ucf.edu,4000 CNTRL FLORIDA BLVD,Orlando,FL,328168005,4078230387,ENG,"1637, 7298","036E, 039E, 040E, 043E, 1057, 1637, 6869, 7298, CVIS",$0.00 ,"This project is a US-Ireland partnership program pursuing research in structural health monitoring of bridges. The goal of this collaborative research project is to use cameras and video-image processing to identify loss of load carrying capacity of bridge structures. The camera images can be captured continuously with minimum human intervention. The image processing algorithm can be automated to assess significant change in structural health and load carrying capacity of the bridge. Conversion of camera and computer images for assessment of traffic loads and evaluation of load carrying capacity of the bridge are major research contributions of this project. Early detection of change in load carrying capacity will allow timely repair and thus extend life of the bridge. Investigators from three institutions, one each from three countries of Ireland, Northern Ireland (United Kingdom) and United States will collaborate to develop recognition of traffic loads, changes in vibration characteristics of the bridge, computer vision output, and algorithms to predict load capacity and remaining life of the bridge. The developed algorithm will be validated by observations of a bridge in Northern Ireland in the field. The project is funded by two divisions in the National Science Foundation, the Civil, Mechanical, and Manufacturing Innovation and International Science and Engineering.<br/><br/>The intellectual merit of this project will be the development of algorithms that transforms the recorded video images of a bridge into damage characteristics for the bridge structure. The structural health will be assessed within the context of structural identification with input and output characterization. This development will require new adaptations and transformations that can manage computer vision that are continuous in space for structural engineering applications. The novelties in this project are: vehicle load modeling using computer vision; bridge response using image processing; image- based structural identification using input-output signals; and framework for remaining bridge life evaluation. The development will be validated with laboratory and field experiments. The international activities among the three collaborating partners in the areas of research, education, and outreach will support the intellectual contributions and broader impact of this research."
156,1616950,The 2016 NAACL Student Research Workshop,IIS,ROBUST INTELLIGENCE,12/15/15,12/1/15,Nianwen Xue,MA,Brandeis University,Standard Grant,Tatiana D. Korelsky,11/30/17,"$15,000.00 ",,xuen@cs.brandeis.edu,415 SOUTH ST MAILSTOP 116,WALTHAM,MA,24532728,7817362121,CSE,7495,"7495, 7556",$0.00 ,"This proposal requests funding to subsidize travel, conference and housing expenses of students selected to participate in the NAACL-HLT 2016 Student Research Workshop, which will take place during the main NAACL-HLT conference June 12 - June 17, 2016 in San Diego, California. The annual meeting of its North American chapter of the Association of Computational Linguistics (NAACL) is one of the most prestigious and selective international conferences in these fields. The Student Research Workshop (SRW) is an established tradition at ACL and NAACL conferences, allowing students to present their research and receive feedback from senior researchers. The Student Research Workshop provides a valuable research opportunity for students at different stages (undergraduate students, early graduate students and advanced graduate students) of their academic careers, and contributes to the development of a skilled and diverse computational linguistics and natural language processing workforce. This year, two of the student organizers are women, which will help promote and broaden the participation of women in natural language processing and computational linguistics.<br/><br/>The NAACL-HLT 2016 Student Research Workshop (SRW) encourages submissions in three categories: 1) thesis proposals, for advanced students who have decided on a thesis topic and wish to get feedback on their proposal and research direction; 2) research papers, for graduate students to present either completed work or work in progress that has already achieved preliminary results; 3) undergraduate research papers, for undergraduate students to present their work.  Each accepted paper will be assigned a mentor who will meet with the student during the NAACL conference and provide individual feedback. By including students at different stages of their academic careers (undergraduate students, early graduate students and advanced graduate students) and by encouraging a spirit of collaborative research, the Student Research Workshop aims to build a supportive environment for a new generation of computational linguists. In addition, the SRW is organized by students themselves, advised by two senior researchers in the field. The students who are involved in running and reviewing for the workshop have ample opportunities to interact with the researchers on the organizing committee of the main conference. The opportunities for interaction with other students and with senior researchers will positively influence the students? experience in research and will likely inspire many to devote further effort to academic studies and careers."
157,1526423,RI: Small: Tasking on Natural Image Statistics: 2D and 3D Object and Category Detection in the Wild,IIS,"Robust Intelligence, Comm & Information Foundations",9/1/15,6/3/16,Alan Bovik,TX,University of Texas at Austin,Continuing Grant,Jie Yang,8/31/19,"$441,933.00 ",,bovik@ece.utexas.edu,"3925 W Braker Lane, Ste 3.340",Austin,TX,787595316,5124716424,CSE,"7495, 7797","7495, 7923, 7936",$0.00 ,"This project develops ""distortion-aware"" computer vision models and algorithms suitable for today's mobile camera devices, such as are found in cell phones. Today's mobile camera devices contain remarkably powerful computing capability, sufficient, in fact to contemplate performing sophisticated computer vision problems such as three-dimensional depth estimation, object detection and object recognition. However, mobile camera capabilities are much more limited due to distortions on capture, such as low-light noise, blur, saturation, over/under exposure, and processing artifacts such as compression. These distortions cause most computer vision algorithms to ""break,"" making them unable to accurately recreate the 3D world or to find and recognize objects in it.  This project creates computer vision algorithms with similar capability, using new and emerging models of visual neuroscience (how people see) and detailed and accurate statistical models of the three dimensional visual world (called natural scene statistic models). The project can impact many other camera devices, including low-cost surveillance and security cameras, mobile medical cameras, military cameras operating under battlefield conditions, and more.<br/><br/>This research develops principled approaches to using natural scene statistics models to solve difficult single-image visual tasking problems under poor imaging conditions.  Specifically, the research team studies robust 'distortion-aware' statistical image models and algorithms for single-image 2D and 3D object and object category detection and synergistic 3D depth estimation. The research work includes (1) developing algorithms for fast, generic object detection and categorization ""in-the-wild"" that operate on single photographic images suffering authentic artifacts from digital cameras; (2) designing object and object class detection mechanisms augmented by 3D depth estimation processes, driven by powerful 2D and 3D prior natural image constraints; and  (3) constructing a new annotated Color+3D database of HD precision-calibrated RGBD data using a Reigl VZ-400 Terrestrial Lidar Scanner on object categories of interest, yielding data of higher resolution and richness than existing datasets, complete with image labels as well as hand annotations of bounding-box object locations. This database is free to the community at large once it is available."
158,1547120,NSF EAGER:  Topic Models for Population Genetics,CCF,ALGORITHMIC FOUNDATIONS,7/1/15,7/20/15,Itshack Pe'er,NY,Columbia University,Standard Grant,Mitra Basu,6/30/18,"$200,000.00 ",,itsik@cs.columbia.edu,2960 Broadway,NEW YORK,NY,100276902,2128546851,CSE,7796,"7916, 7931",$0.00 ,"The project breaks new ground by revealing the compelling analogy between analysis of natural language and genetics. In text analysis, documents are modeled as discussing different topics, each with its characteristic vocabulary. Similarly, modern day individuals can be thought of as having ancestry in multiple populations, each with its characteristic genetic patterns. Applied to state-of-the-art genomic data from contemporary individuals and archaeological remains, the unified framework proposed by this project is expected to resolve great historical mysteries, such as the decline of the Mayans, the spread of agriculture, or the evolution of the Indian caste system.<br/><br/>The project is expected to adapt Topic Modeling techniques, a framework from Natural Language Processing which employs Latent Dirichlet Allocation to population genetics. The project will pursue three goals:<br/><br/>1. Formulate existing analysis methods in population genetics as Topic Models, leveraging the existing framework in other domains to improve efficiency and accuracy of genomic analysis<br/>2. Introduce the domain-specific concepts of time and space, across which populations evolve in a theoretically understood way. <br/>3. Integrate the components of the model to create a graphical model of ancestral populations, which describes the genetic history of contemporary and historical populations whose genomes had been sequenced.<br/><br/>The project will compare accuracy and efficiency of models vs. the existing standards in the field.  All software tools that will be developed as part of the project will be made available to the research community."
159,1459932,"Language, Laws, and Labor Contracts in the 20th Century",SES,"ECONOMICS, Science of Organizations",6/15/15,6/9/15,Suresh Naidu,NY,Columbia University,Standard Grant,Seung-Hyun Hong,5/31/17,"$160,000.00 ",W. Bentley MacLeod,sn2430@columbia.edu,2960 Broadway,NEW YORK,NY,100276902,2128546851,SBE,"1320, 8031",,$0.00 ,"This resaerch will contribute to our understanding of how firms and workers allocate authority, and how this can be organized to increase productivity and reduce conflict. These questions go back to Ronald Coase and Herbert Simon, as well as the institutionalist school of labor economics. Outside of economics, our results will relate to the large literature in labor and legal history, sociology, and American political development that has concerned itself with the role of the law in the labor contract. Our new integrated data set of labor laws and labor contracts will be of value to both qualitative and quantitative researchers interested in computational linguistics, contracts, law, and the history of U.S. labor unions in the 20th century. <br/><br/>The productivity gains associated with successful firm-worker relationships rely not only on the terms of written agreements, but also on the background legal institutions that interpret, enforce, and elaborate such agreements. The purpose of this research is to explore the complementary roles of laws and contracts through the construction and analysis of a new data set on collective bargaining agreements. After merging the contract data with data on laws and firms, we use natural language processing tools to extract those textual features of statutes, court cases, and contracts that matter for economic outcomes. We then address two central questions. First, does the law affect written contracts? Second, do contracts affect economic performance?"
160,1514512,Modeling rich inter-image relationships in big visual collections,SMA,SPRF-IBSS,9/1/15,8/1/16,Alexei Efros,CA,University of California-Berkeley,Continuing Grant,Josie S. Welkom,8/31/17,"$225,906.00 ",Phillip Isola,efros@eecs.berkeley.edu,Sponsored Projects Office,BERKELEY,CA,947101749,5106433891,SBE,8209,,$0.00 ,"The Directorate of Social, Behavioral and Economic Sciences offers postdoctoral research fellowships to provide opportunities for recent doctoral graduates to obtain additional training, to gain research experience under the sponsorship of established scientists, and to broaden their scientific horizons beyond their undergraduate and graduate training. Postdoctoral fellowships are further designed to assist new scientists to direct their research efforts across traditional disciplinary lines and to avail themselves of unique research resources, sites, and facilities, including at foreign locations. This postdoctoral fellowship supports a rising scientist in the interdisciplinary area overlapping computer vision and psychology, with a research project that investigates the web of relationships within visual data in both humans and machines. To a human observer, no photograph is an island: it is connected to the rest of the visual world by a web of similarities, associations, and other relationships. For example, two photos of Paris share a certain similarity; images of boats are associated with images of water; a photo of a tadpole and a photo of a frog show the same organism at two stages of life. In each of these cases, a human can readily reason about the link between two images. Not only can people identify that a relationship exists, but can also identify the nature of this relationship. These relationships shed light on how the human brain organizes visual information, and also give insight into how to build intelligent systems that automatically make visual connections. The latter will bring the field closer to producing an intelligent visual web, able to organize visual information in the same way as the current Internet is able to organize text. <br/><br/>Computer vision scientists and psychologists have both studied relationships between visual data, but from different directions. In computer vision, the focus has been on models of natural image similarity. These models handle complex stimuli but are usually limited to one simple kind of relationship, namely similarity in appearance. Psychologists have studied a richer set of relationships -  association, causation, analogy, antonymy, transformation, etc. - but their models usually only apply to simple, artificial stimuli. This project unites the best of both fields by modeling subtle visual relationships between complex, natural images. The objective is to model both which images humans consider to be related and how are those images related. An additional objective is to study how certain relationships, such as visual associations, can arise in an unsupervised manner from natural visual experience. This will help explain how humans might learn about the relationships in the first place. Better models of inter-image relationships will have deep implications across cognitive psychology. In particular, similarity and association play fundamental roles in theories of human learning and memory. A sense of similarity underlies our ability to learn from one visual experience and then apply our knowledge in a future, similar setting. The associations made from the experience additionally impact human memory of it. The present project also has applications toward computer vision systems. Reverse image search has recently become a popular tool. However, current systems are only able to retrieve look-alike images. If the computer is instead able to retrieve images linked by more diverse kinds of relationships, many possibilities open up. For example, one could imagine a system that lets users navigate through artistic styles, or that recommends shoes that match a pair of pants. If successful, this project could pave the way toward a world-wide web of visual connections that parallels the current web of hypertext connections."
161,1456173,SBIR Phase II:  Semantically Intelligent Knowledge Hub for Course Authoring,IIP,SBIR Phase II,3/1/15,5/15/18,Ramji Raghavan,MA,Pragya Systems Corp.,Standard Grant,Rajesh Mehta,11/30/19,"$1,350,121.00 ",,ramji@pragyasystems.com,185 High Street,Winchester,MA,18903364,9783942581,ENG,5373,"165E, 169E, 5373, 8031, 8032, 8033, 8034, 8039, 8240",$0.00 ,"This Small Business Innovation Research Phase II project aims to develop a system to dramatically simplify how course content is shared, discovered, curated and managed in educational institutions. The vast majority of content in educational institutions is in multiple silo-ed repositories. Hence, compliance tracking, aligning curricula with competencies and course authoring are manual and expensive processes. This project aims to unlock these silos and make it simple to search, tag and curate courseware at topic-level across multiple platforms. The technology also extracts student feedback and assessment information and maps it against granular learning objectives to provide new insights to track and improve learning outcomes. By unlocking courseware stuck in silo-ed college systems, this project facilitates a cross-institutional marketplace that enables any institution to share and/or monetize curated high quality content, and conversely have access to curated course packs that may be taught by adjunct faculty, a $2B market opportunity. Simplifying faculty access to Open Educational Resources and curated learning content from other colleges will help drive down student course-pack costs significantly.<br/><br/>The key technical innovation in this project is seamless extraction of learning content from multiple educational repositories coupled with a syllabus-driven semantically intelligent search and recommendation engine woven into the course curation process. Developing an abstracted mechanism to interface with different types of learning management systems and other repositories, and making it portable without requiring a common content format is a significant innovation. Natural Language Processing techniques combined with curated controlled vocabularies and taxonomies are applied to simplify curation of a course, notably automatic topic identification and instructional tagging. The curation data is leveraged in the semantic search engine to improve recommendation of courseware during course authoring. The goal of the adaptive recommendation engine is to improve discovery and usage of both open educational resources and courseware developed in other colleges, without requiring expensive professional services. Another goal of the project is to extract student feedback seamlessly using IMS defined standards regardless of which learning management system is used to deliver the content. The student feedback, assessment data, grades and other information pulled from various college systems are then correlated to provide rich analytics and visibility of student outcomes and alignment of courseware with competencies."
162,1462638,Mapping the Underworld of Buried Utilities - A Hybrid Sensing Approach,CMMI,CIS-Civil Infrastructure Syst,8/15/15,7/24/15,Hubo Cai,IN,Purdue University,Standard Grant,Walter Peacock,7/31/19,"$216,521.00 ",,hubocai@ecn.purdue.edu,Young Hall,West Lafayette,IN,479072114,7654941055,ENG,1631,"029E, 036E, 039E, 1057, CVIS",$0.00 ,"This grant will pioneer a novel hybrid sensing approach to accurately map underground utilities using both sensor and non-sensor data. For the massive underground utilities, knowing their locations, dimensions, spatial configurations, and materials is critical to effectively maintaining, restoring, and upgrading them to provide essential services to the society: water, electricity, gas, and the Internet. Lack of accurate utility location information is the main reason for over six million incidents of excavation damage to utilities every year, causing injuries, fatalities, and property damage, on the order of billions of dollars. Current sensor-based utility mapping methods are less effective in the utility-congested urban environment where many utility lines occupy a tight space, causing sensor signal interferences and consequent erroneous utility information. The anticipated approach will create a method to supplement textual data from utility regulations to digital sensor data, enabling automated and more accurate mapping of underground utilities. With accurate utility information, 22% of the excavation-related incidents could be avoided. Therefore, results from this research will benefit society through cost savings, reduced hazards to citizens, consistency in services supplied by critical underground lifelines, and improved sustainability of urban communities. Research outcomes will be integrated into engineering curriculum, K-12 education, and industry workshops to promote subsurface utility engineering as a specialization area.<br/><br/>The objective of this research is to create a hybrid, multi-sensing method to automate the process of interpreting and fusing heterogeneous utility data to generate accurate utility maps. This novel approach is expected to shift the current paradigm in mapping and labeling underground utilities in utility-congested urban areas by devising methods that exploit utility specifications and plans as a ""virtual sensor."" This approach will provide contextual information for underground utilities. This information will be integrated with information from the processing of ground penetrating radar(GPR) scans. The research team will create algorithms to automatically detect multiple and transformed signatures from underground utilities created with the GPR scans, estimate utility locations, dimensions, and spatial configurations, and recognize pipe materials. Natural language processing algorithms will be devised to mine the textual data in utility specifications and plans. These algorithms will enable extraction of spatial rules constraining utilities and their surroundings, which will be fed into a spatial reasoning algorithm. This use of utility specifications and plans for this purpose is revolutionary. A novel framework for utility data/information fusing will be created to address spatial inference and data correspondence computation problems in utility mapping applications that are as of yet untapped. The credibility of the system and methods will be validated at each research phase through experiments."
163,1509372,"Distributed Recursive Robust Estimation: Theory, Algorithms and Applications in Single and Multi-Camera Computer Vision",ECCS,CCSS-Comms Circuits & Sens Sys,7/1/15,6/16/15,Namrata Vaswani,IA,Iowa State University,Standard Grant,Lawrence Goldberg,6/30/19,"$250,000.00 ",Nicola Elia,namrata@iastate.edu,1138 Pearson,AMES,IA,500112207,5152945225,ENG,7564,153E,$0.00 ,"Many computer vision problems require recursive robust estimation. Some examples include surveillance applications involving a large indoor or outdoor area monitored by a network of static cameras; natural background scenes' recovery, e.g. forest scenes with moving leaves and branches, and their subspace estimation for texture synthesis applications in animated movies; and multi-site video conferencing. The surveillance problem requires tracking moving objects; this can be an easy problem if the background is static. However consider outdoor scene monitoring on a rainy or very foggy day. As the fog moves and its density changes, it results in complex and changing backgrounds and the tracking algorithms need to be robust to this type of 'large' background noise. The texture synthesis for animation is a well-studied problem if there are no occlusions (in this case the background sequence is directly available) but becomes difficult in the presence of severe (large-sized and persistent) occlusions, e.g. moving and occasionally static animals occluding the background scenes. We show in this project that the most challenging step in all the above problems can either be posed as a distributed recursive robust principal components' analysis (PCA) problem, that is robust to outliers, or as a distributed recursive robust sparse recovery problem, that is robust to large but structured noise (noise that is non-sparse and lies in a low-dimensional subspace). The main goal of this project is to develop distributed algorithms to solve these problems for the multi-camera setting. The algorithms will be developed in the context of a multi-site video combining application (needed for multi-site video conferencing).  <br/><br/>This project is developing the first set of online distributed solutions for the decomposition of a matrix into a sum of a sparse and a low-rank matrix. Robust PCA and robust sparse recovery are special cases of this more general problem. Our online solutions will be significantly faster and memory-efficient compared to existing batch methods. Moreover, unlike most batch methods, these will provably work even when for slow moving or occasionally static foreground objects (these result in the sparse matrix also becoming rank deficient and hence batch methods do not work in this case). This advantage comes because our methods exploit accurate initial subspace knowledge and slow subspace change (both are usually practically valid assumptions in real videos). The key novelty of our work within the computer vision literature is that it is robust to slow changing backgrounds or to frequent and persistent occlusions (depending whether the foreground or the background is the layer of interest)."
164,1455533,CAP: Advancing Technology and Practice for Learning Reading and Writing Skills in Secondary Science Education,IIS,Cyberlearn & Future Learn Tech,1/15/15,12/5/14,Rebecca Passonneau,NY,Columbia University,Standard Grant,Tatiana D. Korelsky,5/31/17,"$59,973.00 ","Smaranda Muresan, Dolores Perin",rjp49@cse.psu.edu,2960 Broadway,NEW YORK,NY,100276902,2128546851,CSE,8020,"8045, 8055",$0.00 ,"The proposed workshop brings together researchers and educators to discuss how to advance technology and practice to better promote secondary school science literacy. There is a clear need for new directions in science literacy. Over the past decade, the National Center for Education Statistics has consistently identified poor reading and writing skills as a serious problem, one which undoubtedly contributes to low science achievement. In addition, written and oral communication skills have been identified as a significant dimension of science practice and education. Secondary school, the focus of this workshop, is a critical period when students first experience a separation between subject matter and literacy. The resulting disciplinary silos may create discontinuities in students' learning of both science content and literacy skills. It may be advantageous for instruction in science and the English language arts to be brought closer together at the secondary level, and to facilitate this through digital learning environments. <br/><br/>Secondary science education has benefited from a range of digital technologies, including computer supported collaborative learning systems (CSCL) and intelligent tutoring systems (ITS).  The hypothesis motivating the workshop is that results from a decade of work in computer-based learning for science and inquiry can be applied to science literacy. One precondition is appropriate application of automated techniques to analyze the texts that students read and write, so as to provide students with tailored feedback in an online setting. A second precondition is to apply insights from the psychology of education on the acquisition of general literacy skills, and argumentation skills in particular, to foster evidence-based communication. The workshop includes researchers from the three critical areas of computer-based learning environments, natural language processing, and the psychology of education. It has three main goals. The first is to share results, datasets and demonstration systems in order to identify the potential for collaborations that would both deepen the analysis of existing data, and identify criteria for the collection of novel datasets that specifically address science literacy. The second workshop goal is to foster collaborations among researchers in the three disciplines, with participation from practitioners, to study the potential impact on students' learning of science literacy skills.  The third workshop goal is to identify mechanisms to foster an interdisciplinary community that will continue to investigate science literacy."
165,1551192,"SoCS: Collaborative Research: Data-Driven, Computational Models for Discovery and Analysis of Framing",IIS,SOCIAL-COMPUTATIONAL SYSTEMS,7/1/15,7/29/15,Justin Gross,MA,University of Massachusetts Amherst,Standard Grant,William Bainbridge,9/30/16,"$38,540.00 ",,jhgross@umass.edu,Research Administration Building,Hadley,MA,10359450,4135450698,CSE,7953,7953,$0.00 ,"This project studies framing, a central concept in political communication that refers to portraying an issue from one perspective with corresponding de-emphasis of competing perspectives.  Framing is known to significantly influence public attitudes toward policy issues and policy outcomes.  As social media allow greater citizen engagement in political discourse, scientific study of the political world requires reliable analysis of how issues are framed, not only by traditional media and elites but by citizens participating in public discourse. Yet conventional content analysis for frame discovery and classification is complex and labor-intensive. Additionally, existing methods are ill-equipped to capture those many instances when one frame evolves into another frame over time. <br/><br/>This project therefore develops new computational modeling methods, grounded in data-driven computational linguistics, aimed at improving the scientific understanding of how issues are framed by political elites, the media, and the public.  This collaboration between political scientists and computer scientists has four goals: (a) developing novel methods for semi-automated frame discovery, whereby computational models guided by political scientists? expert knowledge speed up and augment their analytical process; (b) developing novel algorithms based on natural language processing for automatic frame analysis, producing measurably accurate results comparable with reliable human coders; (c) establishing the validity of these processes on well-understood cases; and (d) applying these methods to several current policy issues, using data across years and across traditional and social media streams. The resulting evolutionary framing data will help unpack the mechanisms of framing and help predict trends in public opinion and policy."
166,1539131,VEC: Small: Collaborative Research: Wide Field of View Monocentric Computational Light Field Imaging,IIS,Information Technology Researc,9/1/15,9/24/15,Gordon Wetzstein,CA,Stanford University,Continuing grant,Jie Yang,8/31/18,"$235,000.00 ",,gordon.wetzstein@stanford.edu,450 Jane Stanford Way,Stanford,CA,943052004,6507232300,CSE,1640,002Z,$0.00 ,"This project targets the development of monocentric camera systems for high-resolution, wide field-of-view (FOV) light field imaging in small device form factors. Building on the benefits of recently-developed monocentric optics - ultra-high resolution, small physical footprint, low weight, and high light collection - monocentric light field imagers provide a transformative platform for a range of future experiential imaging and computing applications. In particular, light field-enabled monocentric optics allow for spatially-varying digital focus for complex and wide FOV scenes, 3D imaging capabilities, stereo view synthesis, and imaging through partial occluders. As opposed to any existing technology, monocentric light field imagers enable immersive content for emerging head-mounted displays with support for focus cues to be captured with low-cost, mobile devices. A range of computer vision algorithms directly benefit from the targeted computational imaging platform, including 4D feature detection, localization and mapping, segmentation, recognition, tracking, depth estimation, matting, object removal, and hole filling. The developed monocentric light field imaging system provides benefits for society at large; the enabled 3D image capture and editing capabilities offered in a small device form factor could profoundly impact future means of inter-personal digital communication, remote collaboration and education as well as remote operation of vehicles. Newly-developed computer vision algorithms are beneficial for navigation of autonomous vehicles. Live content for a range of applications can be easily recorded and edited, for example for simulation, training, phobia treatment, and cultural heritage. Light field optics and algorithm design will be tightly integrated into the syllabus of multiple graduate-level courses at Stanford and UCSD and made available to industry professionals via online learning platforms.<br/><br/>This research investigates a viable solution for these challenges and provides a next-generation computational imaging platform. Leveraging the expertise of PIs from University of California San Diego and Stanford University, this project aims at (i) designing and fabricating a wide field of view light field imager via monocentric optics, conformal microlenses, and fiber coupling, (ii) developing end-to-end computational imaging pipelines, from coded capture to display on emerging head mounted displays, and (iii) evaluating computer vision and scene understanding algorithms, including feature detection, localization, mapping, segmentation, classification, tracking, matting, classification, and object removal. The research question driving this project is the quest for a small, computational imaging system that is flexible enough to unlock a range of visual and experiential computing applications that cannot be easily provided by cameras available today. Monocentric optics offer great benefits for such applications: wide field of view, high resolution, high light collection, and a small form factor. Yet, future visual computing applications require even more functionality: 3D imaging, adaptive digital focus over a large FOV, compatibility with emerging virtual and augmented reality displays, enhanced image editing modes, such as object segmentation, removal, insertion, localization, and more."
167,1527684,SBE: Small: Behavioral Control of Deceivers in Online Attacks,SES,Secure &Trustworthy Cyberspace,9/1/15,9/10/15,Lina Zhou,MD,University of Maryland Baltimore County,Standard Grant,Sara Kiesler,3/31/19,"$499,912.00 ",Dongsong Zhang,lzhou8@uncc.edu,1000 Hilltop Circle,Baltimore,MD,212500002,4104553140,SBE,8060,"7434, 7923, 9102, 9179",$0.00 ,"Online attacks can cause not only temporary asset loss, but long-term psychological or emotional harm to victims as well. The richness and large scale of online communication data open up new opportunities for detecting online attacks. However, attackers are motivated to constantly adapt their behaviors to changes in security operations to evade detection. Deception underlies most attacks in online communication, and people are poor at detecting deception. Against this backdrop, this project aims to improve the resilience of solutions to online attacks and enable predictive methods for their detection. Although a complete set of deception behaviors of online attackers is assumed to be unknown, there is a reason to expect that some behaviors are more difficult for attackers to control than others. By identifying such behaviors and their relations in online communication, the project lays the groundwork for the development of resilient and predictive approaches to the detection of online attacks, and advances the state of knowledge on online deception behavior and its identification. At the educational front, the project provides new educational material for enriching the curriculum in cyber security and related disciplines. The interdisciplinary nature of this work contributes to graduate student training toward a new generation of scientists who are capable of conducting multi-disciplinary cutting-edge research using a variety of research methods. The PIs actively engage students at both graduate and undergraduate levels in their research activities, particularly making a strong effort to engage women and underrepresented minorities.<br/><br/>Online attackers' evolving behaviors can make the existing solutions to online attacks become ineffective quickly. This project not only discovers new deception behaviors and their relations from the discourse and structure of online communication, but also determines attackers' behavioral control during online attacks by comparing different types of online deception behavior. Further, this project develops techniques for automatic extraction of deception behaviors from online communication by building upon natural language processing and network analysis techniques. Some anticipated advances include: (1) deception theory extension by investigating deception behavior in online attacks via a new lens of behavior control, (2) guidelines on how to improve the resilience of online attack detection methods by identifying deception behaviors that likely escape the attackers' control attempt, (3) a predictive approach to attack detection in online communication by exploring the temporal relationships among deception behaviors, and (4) techniques for extracting deception behaviors from online discourse and structure. This project can lead to integrative and effective methods for online attack detection."
168,1528027,"III: Small: RUI: Efficient Search, Comparison, and Annotation for Biological Sequences",IIS,Info Integration & Informatics,8/1/15,7/24/15,Abdullah Arslan,TX,Texas A&M University-Commerce,Standard Grant,Sylvia Spengler,7/31/18,"$76,756.00 ",,Abdullah.Arslan@tamuc.edu,Office of Sponsored Programs,Commerce,TX,754293011,9038865964,CSE,7364,"7364, 7923, 9229",$0.00 ,"This project aims to develop fast algorithms for searching, comparing, and annotating protein and RNA sequences. Regular expression matching is commonly used in UNIX-based systems for searching texts, and in PROSITE website for searching patterns in protein sequences. Context free grammar matching is used in searching RNA sequences. Annotating biological sequences (DNA, protein, and RNA sequences) using regular expression and context free grammar described motifs is an important application available in many public databases, websites, and software tools (e.g. PROSITE, Locomotif). The results of the proposed project will be helpful for programmers who develop pattern matching and parsing applications which would benefit from fast algorithms for searching and annotating sequences. An example of such an application outside bioinformatics is parsing with multiple context free grammars, which is used in natural language processing and program compiling. There will be strong student involvement during the entire project. As implementations become complete, students will help the PI present these implementations, and make them available for use in project web pages. This project involves fundamental computer science theory with applications in bioinformatics. It will yield new knowledge and case study results in automata and formal languages which are essential parts of the computer science curriculum. It will also help involved students master these topics.<br/><br/>This project will develop algorithms for searching, comparing and annotating protein and RNA sequences by using (1) Seed-based matching: For finding an approximate match to a given sequence, popular alignment and search tool BLAST locates first an exact match of fixed length region (seed) and extends the matching region around the seed. This project generalizes the use of seeds in novel ways to pattern matching and annotation problems for regular expression and context free grammar described patterns. Initial results indicate that the proposed seed-based approach finds matches about 2.5 times faster than UNIX GREP utility on 2MB texts; (2) Suffix tree/array-based matching: For the annotation problem with bounded-length patterns over fixed alphabets, this project proposes using a suffix tree (or a suffix array) extended with additional information in order to identify from a candidate set, a regular expression or a context free grammar that generates a given string; and (3) A new representation for RNA: This project proposes a new RNA secondary structure representation in which two-dimensional structure information is embedded in the sequence with desirable features. For RNA sequences, the proposed new algorithms will exploit the advantages of this representation for fast RNA search, annotation, and comparison (of multiple RNAs to locate common substructures). Seed-based search and RNA comparison problems (using the new representation) will be addressed in the first year, and annotation problems in the second year as sequence annotation will make use of the results from the first year. The project will create experimental databases from publicly available databases such as PROSITE, Rfam, RNA STRAND, rCAD (in particular RNA sequences in .bpseq files) for the purpose of explaining and showing the results of the developed algorithms. Every semester, during the project, students will be involved in developing, implementing, testing new algorithms, user interfaces, and relevant support tools."
169,1528037,"RI: Small: Fast, Scalable Joint Inference for NLP using Markov Logic",IIS,Robust Intelligence,9/1/15,7/29/15,Vincent Ng,TX,University of Texas at Dallas,Standard Grant,Tatiana Korelsky,8/31/20,"$360,348.00 ",Vibhav Gogate,vince@hlt.utdallas.edu,"800 W. Campbell Rd., AD15",Richardson,TX,750803021,9728832313,CSE,7495,"7495, 7923",$0.00 ,"Many fundamental tasks in natural language processing (NLP) such as coreference resolution and event extraction involve complex output constraints. Markov Logic Networks (MLNs), a joint inference framework that combines logical and probabilistic representations, enable manual specification of such constraints in a compact manner, effectively allowing easy incorporation of background knowledge into NLP systems to improve their performance. While theoretically appealing, MLNs have been relatively underused in NLP applications.  Owing to issues of scalability, researchers have largely restricted themselves to simple MLNs that either make simplifying, sometimes unreasonable assumptions or ignore complex output constraints. This project seeks to bring transformative changes to the way joint inference is applied in NLP. The idea is to develop fast, scalable learning and inference techniques for MLNs so that rich models (i.e., models with high-dimensional features and/or complex output constraints) can be efficiently trained and applied to large data sets. A key component of the project is the formulation and evaluation of rich MLN-based models for important and complex NLP tasks such as coreference resolution. These rich models, especially when trained on large data sets, can potentially yield breakthrough results in NLP, which in turn can have profound societal impact. For example, improvements in coreference technologies stand to benefit essentially all NLP applications the general public relies on every day, such as search, information extraction, and question answering.<br/><br/>Successful application of rich MLN-based models to complex NLP tasks requires the development of fast, scalable learning and inference techniques. To scale up weight learning in MLNs, this project develops approaches that leverage advanced algorithms from the constraint satisfaction literature for fast, approximate solution counting. To scale up probabilistic inference, it employs lifted inference algorithms to reduce the domain size of variables in MLNs by exploiting exact as well as approximate symmetries (e.g., paraphrases). The core NLP tasks it focuses on, such as coreference resolution and temporal relation extraction, are sufficiently complex that they provide convincing testbeds for evaluating the scalability of these learning and inference techniques. Equally importantly, as approximate language is a phenomenon that occurs across NLP tasks, these advances are likely to impact a wide swath of tasks in NLP."
170,1539983,"NSF Travel Fellowships for ISWC (October 11-15, 2015) Student Participants",IIS,Info Integration & Informatics,8/1/15,7/24/15,Mark Musen,CA,Stanford University,Standard Grant,nan zhang,7/31/16,"$20,000.00 ",,musen@stanford.edu,450 Jane Stanford Way,Stanford,CA,943052004,6507232300,CSE,7364,"7364, 7556",$0.00 ,"This project will provide Student Travel Fellowships to support U.S. based students traveling to the 14th International Semantic Web Conference (ISWC 2015). The conference, which will be held in Bethlehem, Pennsylvania from October 11-15, 2015, is the premier major international forum for state-of-the-art research on all aspects of the Semantic Web - the next generation World Wide Web. Student Fellowships help cover the travel costs for US students, making it possible for them to attend the conference. This funding will allow students to meet key members of the Semantic Web research community, it will give them the opportunity to disseminate their work, and it will provide a venue for them to interact with future national and international scientific collaborators.<br/><br/>The International Semantic Web Conference, which is now in its fourteenth year, is an interdisciplinary conference that includes work on: Data Management, Natural Language Processing, Knowledge Representation and Reasoning, Ontologies and Ontology Languages, Semantic Web Engineering, Linked Data, User Interfaces and Applications. It regularly has several hundred attendees. In addition to the main technical tracks, the conference includes a variety of events that provide opportunities for deeper interaction amongst researchers at different institutions, at different stages of their research careers, and researchers who are interested in many different aspects of Semantic Web Research. Students may benefit from the Doctoral Consortium - a full day event where students can get critical, but encouraging, feedback on their work from senior members of the community. They can also benefit from the career mentoring lunch, where experienced members of the community from both academia and industry answer questions in an informal setting.<br/><br/>For further information see the project web site at: http://iswc2015.semanticweb.org"
171,1539120,VEC: Small: Collaborative Research: Wide Field of View Monocentric Computational Light Field Imaging,IIS,Information Technology Researc,9/1/15,9/18/15,Joseph Ford,CA,University of California-San Diego,Continuing Grant,Jie Yang,8/31/18,"$150,000.00 ",,jeford@ucsd.edu,Office of Contract & Grant Admin,La Jolla,CA,920930621,8585344896,CSE,1640,002Z,$0.00 ,"This project targets the development of monocentric camera systems for high-resolution, wide field-of-view (FOV) light field imaging in small device form factors. Building on the benefits of recently-developed monocentric optics - ultra-high resolution, small physical footprint, low weight, and high light collection - monocentric light field imagers provide a transformative platform for a range of future experiential imaging and computing applications. In particular, light field-enabled monocentric optics allow for spatially-varying digital focus for complex and wide FOV scenes, 3D imaging capabilities, stereo view synthesis, and imaging through partial occluders. As opposed to any existing technology, monocentric light field imagers enable immersive content for emerging head-mounted displays with support for focus cues to be captured with low-cost, mobile devices. A range of computer vision algorithms directly benefit from the targeted computational imaging platform, including 4D feature detection, localization and mapping, segmentation, recognition, tracking, depth estimation, matting, object removal, and hole filling. The developed monocentric light field imaging system provides benefits for society at large; the enabled 3D image capture and editing capabilities offered in a small device form factor could profoundly impact future means of inter-personal digital communication, remote collaboration and education as well as remote operation of vehicles. Newly-developed computer vision algorithms are beneficial for navigation of autonomous vehicles. Live content for a range of applications can be easily recorded and edited, for example for simulation, training, phobia treatment, and cultural heritage. Light field optics and algorithm design will be tightly integrated into the syllabus of multiple graduate-level courses at Stanford and UCSD and made available to industry professionals via online learning platforms.<br/><br/>This research investigates a viable solution for these challenges and provides a next-generation computational imaging platform. Leveraging the expertise of PIs from University of California San Diego and Stanford University, this project aims at (i) designing and fabricating a wide field of view light field imager via monocentric optics, conformal microlenses, and fiber coupling, (ii) developing end-to-end computational imaging pipelines, from coded capture to display on emerging head mounted displays, and (iii) evaluating computer vision and scene understanding algorithms, including feature detection, localization, mapping, segmentation, classification, tracking, matting, classification, and object removal. The research question driving this project is the quest for a small, computational imaging system that is flexible enough to unlock a range of visual and experiential computing applications that cannot be easily provided by cameras available today. Monocentric optics offer great benefits for such applications: wide field of view, high resolution, high light collection, and a small form factor. Yet, future visual computing applications require even more functionality: 3D imaging, adaptive digital focus over a large FOV, compatibility with emerging virtual and augmented reality displays, enhanced image editing modes, such as object segmentation, removal, insertion, localization, and more."
172,1453192,CAREER: Coherent Computational Imaging: Micro Measurements in a Macro World,IIS,Robust Intelligence,2/1/15,2/2/15,Oliver Cossairt,IL,Northwestern University,Standard Grant,Jie Yang,1/31/21,"$485,935.00 ",,olivercossairt@gmail.com,750 N. Lake Shore Drive,Chicago,IL,606114579,3125037955,CSE,7495,"1045, 7453, 7495",$0.00 ,"This project is building fundamentally new types of cameras that combine novel optics and algorithm design to overcome the diffraction limit for macroscopic scenes, achieving unprecedented levels of precision in image, depth, and material acquisition. This project improves computer vision tasks performed on images of distant objects, increasing precision for applications such as surveillance, remote sensing, robot navigation, and autonomous vehicles. In addition, the work has direct applications in ongoing art conservation efforts to non-destructively acquire microscopic surface details of paintings and drawings. The research program is also tightly integrated with a comprehensive education program incorporating imaging and photography. The research team is developing curriculum for Chicago afterschool programs to introduce at-risk youth to basic concepts in optics, electronics, electrodynamics, and image processing. <br/><br/>The focus of this research is to increase the level of detail that can be recovered when imaging macroscopic objects at large distances. The research is based on a wave-model of light with computational photography. This new theory of coherent light transport incorporates even the most complex effects of visual appearance such as participating media, sub-surface-scattering, multiple-bounce inter-reflections, diffraction, and interference. The research team is developing theory, hardware, and algorithms that lead to fundamental improvements in image, shape, and material acquisition for computer vision applications. The project is developing fundamentally new types of cameras for computer vision that rely on a synergistic combination of coherent optics (both active and passive) and novel algorithm design to overcome the diffraction limit. This project is constructing computational cameras that can resolve scene details well below the diffraction limit."
173,1525431,RI: Small: Efficient Statistical Computing on Riemannian Manifolds with Applications to Medical Imaging and Computer Vision,IIS,"Robust Intelligence, IntgStrat Undst Neurl&Cogn Sys",9/1/15,9/17/15,Baba Vemuri,FL,University of Florida,Continuing Grant,Jie Yang,8/31/19,"$455,177.00 ",,vemuri@cise.ufl.edu,1 UNIVERSITY OF FLORIDA,GAINESVILLE,FL,326112002,3523923516,CSE,"7495, 8624","7495, 7923, 8089, 8091",$0.00 ,"This project develops efficient incremental algorithms are proposed for computing averages and other statistical quantities of interest from pools of data incrementally acquired. Many existing data acquisition and processing methods have reached a level of sophistication so as to be able to acquire and/or synthesize data that reside in curved spaces such as spheres, hyperboloids etc. As such data have become ubiquitous in many Science and Engineering fields, need for efficient statistical analysis of these data has emerged as an area of significant importance. Further, in this era of massive and continuous streaming data, samples of data are acquired sequentially over time. Hence, from an applications and computational efficiency perspective, the desired averaging algorithm ought to be amenable to incremental updates to accommodate the newly acquired data over time. The developed algorithms can be applied different applications, such as face recognition from videos, action recognition, trajectory averaging and clustering from videos, image and video restoration, pattern clustering and classification, etc. In the context of diagnostic medical imaging, methods developed in this project can be used to automatically discriminate between various disease classes, such as Parkinson's and Essential Tremor which are distinct types of movement disorders.<br/><br/>This research investigates a general framework for recursive computation of the intrinsic mean and the principal geodesic analysis on several commonly encountered manifolds such as the manifold of symmetric positive definite matrices, the Grassmann, the Stiefel manifolds, the hypersphere, the manifold of special orthogonal matrices, and several others. The research team applies the developed recursive framework of computing statistics from manifold-valued data to several tasks namely, atlas computation from diffusion MRI in Medical Imaging, inter-class discrimination between sub-types of a neuro-degenerative disorder using diffusion MRI, face and action recognition, image and shape retrieval in Computer Vision applications."
174,1461166,"REU Site: Socially Relevant Computing in Pervasive Computing, Computer Vision, and Human-Computer Interaction",IIS,RSCH EXPER FOR UNDERGRAD SITES,3/1/15,3/19/18,Heather Richter Lipford,NC,University of North Carolina at Charlotte,Standard Grant,Wendy Nilsen,2/28/19,"$323,815.00 ","David Wilson, Richard Souvenir",Heather.Lipford@uncc.edu,9201 University City Boulevard,CHARLOTTE,NC,282230001,7046871888,CSE,1139,"9102, 9250",$0.00 ,"The Socially Relevant Computing REU Site at UNC Charlotte engages a diverse group of undergraduate students in an intensive 9-week summer research experience. REU students conduct research as part of a collaborative research community, working with faculty, graduate students, and peers. Each project has a socially relevant theme, providing undergraduate students with the opportunity to apply and extend their computing knowledge and deepen their understanding of the societal and global impact of computing research. The goals of the program are to increase interest in attending graduate school, enhance preparation for careers in research and innovation, and broaden participation of underrepresented groups in the field of computing.<br/><br/>The REU students will be involved in research that leads to the creation of new technologies that support humans in socially relevant tasks, requiring advances in the areas of pervasive computing, computer vision, and human computer interaction. Projects include: creating robust and scalable methods to analyze large sets of medical images to support clinical decision-making and diagnosis, developing new algorithms and systems that support large scale scientific data collection in crowdsensing and citizen science projects, and creating novel interfaces and systems for analyzing user interactions while preserving privacy in intelligent spaces. More information about the Socially Relevant Computing REU Site can be found at the program website: www.reu.uncc.edu."
175,1523586,Extra-Propositional Aspects of Meaning in Computational Linguistics Workshop,IIS,Robust Intelligence,4/1/15,8/16/17,Eduardo Blanco,TX,University of North Texas,Standard Grant,Tatiana Korelsky,9/30/18,"$6,000.00 ",,eduardo.blanco@unt.edu,1155 Union Circle #305250,Denton,TX,762035017,9405653940,CSE,7495,"7495, 7556",$0.00 ,"This award provides student support for a one-day workshop entitled ""Extra-Propositional Aspects of Meaning (ExProM) in Computational Linguistics"", to be held collocated with NAACL in Denver on June 5, 2015. Computational semantics has grown significantly in the last decade, but extra-propositional aspects of meaning such as modality, negation and attribution are much less studied: annotations, models and tools are still in their early stages. This workshop brings together students and established researchers interested in defining theoretical frameworks, modelling and implementing real systems to process ExProM, as well analyzing the impact of ExProM in natural language processing applications.<br/><br/>Extra-propositional meaning is ubiquitous across domains and genres: it is found in scientific texts, news, social media posts, etc. Humans do not always (want to) say (the whole) truth, and propositional meaning alone disregards significant meaning encoded in everyday texts. The workshop encourages submissions on the following topics (the list is by no means complete): negation: verbal/non-verbal, analytic/synthetic, clausal/subclausal and ordinary/metalinguistic, scope and focus detection; modality: defining and annotating types; factuality: determining factuality changes within and across documents; veridicity and veridicality: measuring author commitment; attribution and perspectives: determining who says what and their purpose; irony and sarcasm; integrating ExProM in the NLP pipeline; and ExProM for NLP applications. The workshop covers these phenomena without targeting any specific domain, and encourages submissions of novel theoretical frameworks and tasks, as well as empirical results on known or novel corpora. Preliminary ideas and results that may lead to active discussion during the event are encouraged. The workshop's format fosters interaction between students and senior researchers, and includes an invited speaker."
176,1551064,INSPIRE: Co-Design of Adiabatic Quantum Annealers,CCF,"EPMD-ElectrnPhoton&MagnDevices, Information Technology Researc, Special Projects - CCF, INSPIRE",9/1/15,9/1/15,Federico Spedalieri,CA,University of Southern California,Standard Grant,Almadena Chtchelkanova,12/31/17,"$650,000.00 ","Robert Lucas, Kevin Knight, Daniel Lidar",spedalie@usc.edu,University Park,Los Angeles,CA,900890001,2137407762,CSE,"1517, 1640, 2878, 8078","2878, 7796, 7928, 8653",$0.00 ,"This INSPIRE award is partially funded by the Algorithmic Foundations Quantum Computing in the CCF Division in the CISE Directorate; the Electronics, Photonics and Magnetic Devices program in the ECCS Division in the ENG Directorate; and Office of Integrative Activities. <br/><br/>Quantum computing has been an object of investigation for three decades. While general-purpose quantum computers are not yet viable, open-system adiabatic quantum annealing (QA) devices are already available. In the near future, such devices could possibly address a range of nondeterministic polynomial time (NP) problems that can be very challenging for today's classical computers. Unfortunately, the designers of QA systems didn't have specific target applications in mind when designing their machines, and therefore lack clear guidelines to weigh the engineering trade-offs between such things as richer qubit connectivity versus qubit quality. The main goal of this project is to develop a set of guidelines to exploit application-specific features to improve the design of open-system, adiabatic quantum annealers, possible extendable to other quantum devices. Quantum annealers are special-purpose quantum computing devices that are now available as experimental prototypes. Building these complex and fragile systems in a scalable way requires making many engineering compromises. Currently available quantum annealers target a very broad range of problems, leading to very stringent design constraints that greatly reduce the efficient use of their resources. This project takes a specialized view, where engineering design trade-offs are guided by the features of specific applications and should lead to more efficient adiabatic quantum annealers and other quantum devices.<br/><br/>One of the most restrictive constraints in the current implementation of quantum annealers is the limited connectivity of the underlying information processing elements (known as quantum bits or qubits). This limited connectivity is due both to the physical constraints imposed by the 2-dimensional layout of the device and the requirement for a scalable design. The resulting architecture was selected and implemented in order to represent the most general type of problem which the system is designed to solve, but this can be done only by paying a significant cost in terms of the size of the problems that can be solved. A problem in variables generally requires   physical qubits to be mapped into the architecture of the quantum annealer. This project looks at two specific applications - graph partitioning and natural language processing - both of which involve solving combinatorial optimization problems. The idea is to use the specific features and structures of these problems to design quantum annealing architectures that make more efficient use of the available physical qubits. The results of this research will be widely disseminated through publications and by reaching out to other researchers actively working on building these devices. The new guidelines derived from this effort are expected to greatly improve and accelerate the development of useful quantum devices beyond quantum annealers."
177,1518878,NeTS: Large: Measuring and Modeling Internet Choke Points as Threats to Online Freedom,CNS,Networking Technology and Syst,9/1/15,10/17/17,Jedidiah Crandall,NM,University of New Mexico,Standard Grant,Darleen Fisher,8/31/19,"$1,400,000.00 ","Stephanie Forrest, Michalis Faloutsos",crandall@cs.unm.edu,"1700 Lomas Blvd. NE, Suite 2200",Albuquerque,NM,871310001,5052774186,CSE,7363,"7925, 9150",$0.00 ,"Pressing policy questions with respect to Internet chokepoint, such as questions about Internet balkanization or the effects that a specific policy might have, are difficult to answer for two reasons.  First, the data available are typically limited to a single country or small cross-section of the Internet for a short period of time.  Second, the data do not lend themselves well to simple analysis such as averages or direct comparisons.  Research is needed that bridges the gap between collecting long-term, wide-scale data and answering the ever-evolving high level questions that policy makers have.  <br/><br/>This project develops advanced capabilities for measuring Internet chokepoints and analyzing and modeling the data in a way that informs real policy questions.  This project is carrying out Internet chokepoint measurements longitudinally and on a global scale, in multiple layers of the network stack including both TCP/IP layer filtering and filtering in social media.  The project also studies the role of tunnels and network virtualization in how different countries and organizations implement chokepoints in order to have central locations for implementing policy.  In order to accomplish these goals, the project is developing advanced TCP/IP side channel techniques for measuring TCP/IP layer filtering and understanding network topology, novel inference algorithms and natural language processing techniques to discern subtle manipulations of social media, and robust modeling techniques based on Agent Based modeling to analyze the data in a meaningful way.  <br/><br/>In terms of broader impacts, the project will provide new capabilities: (a) to inform policy makers about violations or threats to digital speech freedom, and (b) to advise democratic and anti-chokepoint efforts on how to circumvent chokepoints and limit retributions to citizens around the world.  All source code developed as part of this project will be released under a GNU Public License (GPL) license, and the datasets will be made publicly available.  A key part of the proposed project is mentorship of graduate and undergraduate researchers in all aspects of research, and recruitment and outreach to middle- and high-school students in New Mexico.  The University of New Mexico is both a Carnegie RU/VH Very High Research Activity University and a HACU Hispanic Serving Institution as well as an EPSCoR state.  This puts UNM in a unique position to involve groups that are underrepresented in computer science, and truly disadvantaged economically, in world-class Internet measurement research."
178,1534568,Correspondence Mechanisms in Visual Cognition,BCS,"Perception, Action & Cognition",8/1/15,7/21/15,Jonathan Flombaum,MD,Johns Hopkins University,Standard Grant,Michael Hout,7/31/20,"$286,782.00 ",,flombaum@jhu.edu,1101 E 33rd St,Baltimore,MD,212182686,4439971898,SBE,7252,"7252, 9178, 9179",$0.00 ,"Visual information is initially received by the eyes; however, the experience of vision itself is the result of complex computations carried out deep within the brain. Though many aspects of human vision are well-understood, the mechanisms by which the visual system maps recent memory (what happened a moment ago) onto incoming visual signals (what is happening now) remain a critical question for cognitive science. As events unfold in real time, there needs to be a mechanism for continually linking the current visual information with what just happened. For example, tracking a moving object requires mapping very recent visual memory signals onto the visual signals currently entering from the retina and tying this information together. The same holds true when trying to detect changes in a scene, or when trying to remember the positions of groups of objects, or even when trying to link landmarks on a map to landmarks in real space. This research project aims to characterize the correspondence mechanisms that enable this mapping between the recent and the current. Understanding how the human visual system accomplishes correspondence mappings is critical for understanding complex visual activities and the development of specialized visual skills such those involved in driving, radar control, video analysis, satallite imagery analysis, and baggage screening. Understanding correspondence mechanisms could also inform the design of artificial vision systems. <br/><br/>The research achieves its goals by combining eye tracking methodology and probabilistic models derived from computer vision algorithms, along with behavioral tasks that engage motion tracking, spatial working memory, and visual working memory. Eye tracking is a crucial component of the project because, in humans, the quality of received visual signals depends heavily on a source's distance from an observer's fixation (eccentricity). The research project therefore begins with experiments that compare observers' performance as a function of their fixations as well as simulations by computational models that adopt those empirically obtained fixations. Subsequent experiments then investigate methods for facilitating and training fixation to improve observer performance. A major implication of the research is that fixation selection places tremendous constraints on the accuracy of probabilistic correspondence algorithms, and as a result, on the ability to effectively obtain, store, and retrieve visual information."
179,1513188,CI-P: Community Infrastructure to Catalyze Research in Automata Computing,CNS,"CCRI-CISE Cmnty Rsrch Infrstrc, Software & Hardware Foundation",6/1/15,9/18/17,Kevin Skadron,VA,University of Virginia Main Campus,Standard Grant,Almadena Chtchelkanova,9/30/18,"$115,814.00 ",Mircea Stan,skadron@cs.virginia.edu,P.O.  BOX 400195,CHARLOTTESVILLE,VA,229044195,4349244270,CSE,"7359, 7798","7359, 7941, 9251",$0.00 ,"Micron Technology has announced a new hardware accelerator, the Automata Processor, and helped establish a center at the University of Virginia to facilitate training for potential users, help create a research community around this paradigm, and catalyze new collaborations.  The Automata Processor is designed to enable massively parallel pattern recognition.  It is claimed that speedups of 10-1000X or greater are possible on a wide range of applications involving complex pattern matching, such as in bioinformatics, cybersecurity, data mining, and natural language processing. While the potential impact of automata computing technology is significant, no automata computing infrastructure currently exists to catalyze new research opportunities around this technology.<br/><br/>This planning project will bring together a broad-based community of Computer and Information Science and Engineering researchers who can develop new applications of, and advance the capabilities of automata computing, with the goal of defining the needs of a community infrastructure involving automata technology, and defining the needs for a future, full Community Infrastructure-New proposal. Researchers from diverse institutions will be invited to participate in this planning project. These researchers and their students will learn about the technical capabilities of the Automata Processor and new models for heterogeneous computing, and will have the opportunity to engage with an emerging research community as well as to begin initial research and educational projects involving automata technology. Because of its widespread commercial potential, new technologies developed by users of the community infrastructure are likely to have high value as communal research artifacts, with numerous licensing possibilities or the potential to form spin-off companies."
180,1554480,RAPID: Collaborative Research: Employees' Response to OPM Data Breaches: Decision Making in the Context of Anxiety and Fatigue,SES,Secure &Trustworthy Cyberspace,9/1/15,8/25/15,Rui Chen,IN,Ball State University,Standard Grant,Sara Kiesler,8/31/17,"$100,001.00 ",,ruichen@iastate.edu,2000 West University Avenue,Muncie,IN,473060155,7652851600,SBE,8060,"7434, 7914",$0.00 ,"Nontechnical Description<br/>According to recent reports in the press, the Office of Personnel Management (OPM) was hit hard in two recent cyber-attacks (OPM 2015). In April 2015, OPM discovered that personal data (e.g., Social Security Numbers, full name, and birth date) of 4.2 million current and former Federal government employees had been stolen (referred to as personnel records incident hereafter). Later in June 2015, OPM discovered that around 21.5 million employees - current and former Federal employees and contractors - were affected as their personal information such as Social Security Numbers, fingerprints, and background investigation records were compromised (referred to as background investigation records incident hereafter). Unlike other information, sensitive data such as the background investigation records, which include personal histories, relationships, and biometrics, reveal employees' personal lives are difficult to be re-issued. Typical protection such as a few months of credit monitoring may be insufficient in protecting victims from determined attackers.  To date, little is known about how and why people decide and act in the aftermath of breaches involving their personal data. In particular, the role of data breach fatigue, manifested by insensitivity to data breaches and low estimate of fraud loss, in affecting people's decisions and actions is unknown. The existing research has also been silent on employees' decision making and reactions in response to data breaches. To fill this research gap, in this proposal, we plan to conduct a study that reveals the key decision factors, response actions, and the potential effect of data breach fatigue in the context of anxiety over the possible outcomes of the breach. Findings of the study will help in understanding employee reactions towards data breaches. New knowledge will help industry and policy makers develop intervention strategies that avert the effect of breach fatigue<br/><br/>Technical Description<br/>This proposal will explore the crucial issues that influence employees' responses in the context of the recent two OPM data breach incidents. This proposed research will compare these two different incidents and their impacts on different types of victims, employees who receive notification of the personnel records incident (now), employees who receive notification of both incidents (future), and employees who only receive notification of the background investigation records incident (future). We will also survey employees who have not received any notification, as a control group. In addition to self-reported data through surveys, we shall extend this study by capturing organic Twitter messages related to the two breach incidents in the respective time periods in 2015 to study how people coped with breach incidents. Utilizing natural language processing, we intend to (1) explore patterns of discourses associated with the data breach fatigue, (2) extract coping mechanisms from the discourses, and (3) compare coping mechanisms of employees, identified from the survey and those derived from the data mining."
181,1554373,RAPID: Collaborative Research: Employees' Response to OPM Data Breaches: Decision Making in the Context of Anxiety and Fatigue,SES,Secure &Trustworthy Cyberspace,9/1/15,8/25/15,H. Raghav Rao,NY,SUNY at Buffalo,Standard Grant,mo wang,9/30/16,"$99,894.00 ",,hr.rao@utsa.edu,520 Lee Entrance,Buffalo,NY,142282567,7166452634,SBE,8060,"7434, 7914",$0.00 ,"Nontechnical Description<br/>According to recent reports in the press, the Office of Personnel Management (OPM) was hit hard in two recent cyber-attacks (OPM 2015). In April 2015, OPM discovered that personal data (e.g., Social Security Numbers, full name, and birth date) of 4.2 million current and former Federal government employees had been stolen (referred to as personnel records incident hereafter). Later in June 2015, OPM discovered that around 21.5 million employees - current and former Federal employees and contractors - were affected as their personal information such as Social Security Numbers, fingerprints, and background investigation records were compromised (referred to as background investigation records incident hereafter). Unlike other information, sensitive data such as the background investigation records, which include personal histories, relationships, and biometrics, reveal employees' personal lives are difficult to be re-issued. Typical protection such as a few months of credit monitoring may be insufficient in protecting victims from determined attackers.  To date, little is known about how and why people decide and act in the aftermath of breaches involving their personal data. In particular, the role of data breach fatigue, manifested by insensitivity to data breaches and low estimate of fraud loss, in affecting people's decisions and actions is unknown. The existing research has also been silent on employees' decision making and reactions in response to data breaches. To fill this research gap, in this proposal, we plan to conduct a study that reveals the key decision factors, response actions, and the potential effect of data breach fatigue in the context of anxiety over the possible outcomes of the breach. Findings of the study will help in understanding employee reactions towards data breaches. New knowledge will help industry and policy makers develop intervention strategies that avert the effect of breach fatigue<br/><br/>Technical Description<br/>This proposal will explore the crucial issues that influence employees' responses in the context of the recent two OPM data breach incidents. This proposed research will compare these two different incidents and their impacts on different types of victims, employees who receive notification of the personnel records incident (now), employees who receive notification of both incidents (future), and employees who only receive notification of the background investigation records incident (future). We will also survey employees who have not received any notification, as a control group. In addition to self-reported data through surveys, we shall extend this study by capturing organic Twitter messages related to the two breach incidents in the respective time periods in 2015 to study how people coped with breach incidents. Utilizing natural language processing, we intend to (1) explore patterns of discourses associated with the data breach fatigue, (2) extract coping mechanisms from the discourses, and (3) compare coping mechanisms of employees, identified from the survey and those derived from the data mining."
182,1523285,Student Support at the North American Association for Computational Linguistics Workshop on Computational Methods for Analysis of Narrative,IIS,"INFORMATION TECHNOLOGY RESEARC, ROBUST INTELLIGENCE",5/1/15,4/23/15,Anna Feldman,NJ,Montclair State University,Standard Grant,Tatiana Korelsky,4/30/16,"$6,000.00 ",,feldmana@mail.montclair.edu,1 Normal Avenue,Montclair,NJ,70431624,9736556923,CSE,"1640, 7495","7495, 7556",$0.00 ,"The main goal of the CLfL 2015 workshop is to strengthen research in the field of Natural Language Processing (NLP) by bringing together researchers working on computational approaches to narrative. Narrative data differ from texts in more technical expository genres strongly enough that NLP methods might not be directly applicable. The analysis of narrative poses new, interesting problems. The readers treat texts as a form of creative expression, and they expect more than the typical information-seeking tools. For example, a person may be interested in reading recommendations, analyzing narrative of a certain period or sub-genre or finding examples of certain narrative devices. The workshop is held in conjunction with NAACL 2015 conference, in Denver, Colorado. This award subsidizes travel and accommodation expenses for students actively participating in the workshop. These travel grants support geographic and other minorities by giving them an opportunity to exchange ideas, get into contact with key persons in the field, and gain invaluable feedback from the senior participants.<br/><br/>This workshop contributes to the maintenance and development of a research community in computational linguistics for narrative, with a geographical focus on North America. The workshop strengthens research in individual, punctual efforts on different aspects of computational approaches to narrative by bringing together people from both computer science and humanities.  It also fosters collaboration between theoreticians and practitioners working in academic or industrial settings, across disciplines."
183,1519525,SBIR Phase I: Workforce Creative thinking Assessment,IIP,SMALL BUSINESS PHASE I,7/1/15,12/10/15,Farzad Eskafi,CA,Sparkting,Standard Grant,Glenn H. Larsen,6/30/16,"$172,500.00 ",,feskafi@gmail.com,1501 ROSE ST STE 203,Berkeley,CA,947031008,5108212610,ENG,5371,"163E, 5371, 8031, 8032, 8039",$0.00 ,"This SBIR Phase I project focuses on designing exercises and developing a set of computational methods to measure and enhance semantic-based creativity in the workforce. When hiring a prospect or training an existing employee, there is no efficient way to measure one's problem solving abilities. In order to be an efficient problem solver, an employee must possess the knowledge (e.g., an employee is given a problem whose solution she has witnessed or contributed to previously) and the employee needs to think creatively (e.g., the employee encounters a novel problem; in most scenarios, creative employees use brainstorming methods to generate a number of possible solutions and test possible solutions to arrive at the best answer). This project's unique approach is the design of models, where creativity and divergent thinking is measured and a CQ index is generated.<br/><br/>Using advanced techniques in Natural Language Processing (NLP) and psychometric approach, the project evaluates the responses and generates an index called ""Creativity Quotient,"" which represents users' creativity. The responses are evaluated based three factors: fluency (# of unique and relevant ideas), flexibility (# of categories of responses) & originality (infrequency and uniqueness of responses). The technique deploys a word or phrase-based associative weight where the algorithm measures the uniqueness of associations between words and phrases. Unique associations receive higher weighting."
184,1550757,EAGER: The Exploration of Geometric and Non-Geometric Structure in Data,IIS,Robust Intelligence,9/1/15,7/23/15,Mikhail Belkin,OH,Ohio State University,Standard Grant,Weng-keen Wong,2/28/17,"$150,000.00 ","Yusu Wang, Jihun Hamm",mbelkin@cse.ohio-state.edu,Office of Sponsored Programs,Columbus,OH,432101016,6146888735,CSE,7495,"7495, 7916",$0.00 ,"The goal of machine learning is to extract useful information from data.  While the amount of data available to researchers for analysis is ever increasing, much of the data are unlabeled, meaning that the data come without labels indicating their associations with specific learning tasks. Thus understanding unsupervised inference is one of the key problems in machine learning.  In addition, data annotated for a certain task may be difficult to use even for tasks only slightly different.  This is known as the problem of transfer learning in the literature.  To make the most of the available information, machine learning algorithms need to to obtain, analyze and use realistic structural assumptions about the data based on rigorous mathematical models. The proposed work offers students working on this project an opportunity to be exposed to a broad spectrum of topics including machine learning, statistics, geometry and applied mathematics. Students will learn a combination of theory and algorithm development skills in machine learning and data analysis. The results of this work will be disseminated to the broad scientific community through publications in journals, conferences, presentations in various venues, including tutorials and course notes.  The material related to this project will be incorporated in PI?s and co-PI's courses. The PIs will also create summer research and practice opportunities for interested undergraduate students in research related to the project. <br/><br/>In this EAGER project an exploration of two types of structural assumptions on the data will be started.  Geometric structures in data will be explored, such as hierarchical structure of clusters and density. The use of partial orders for non-geometric data will be explored, based on probabilistic models for partial rankings an orders for problems such as zero-shot learning and transfer learning. By approaching the problem of inference from data within these frameworks, output of this project will be a stepping stone to the challenges of machine learning and to developing efficient algorithms to advance the state-of-the-art both in theory and practice.  It is argued argue that these models and the proposed mathematical/algorithmic machinery are amenable to theoretical analysis  and  will provide insight into  properties of real data. Results from the proposed work will broaden the scope of machine learning methods to analyze more complex data in a theoretically well-founded manner."
185,1446912,CPS: TTP Option: Synergy: Collaborative Research: Calibration of Personal Air Quality Sensors in the Field -  Coping with Noise and Extending Capabilities,CNS,CPS-Cyber-Physical Systems,1/1/15,2/29/16,William Griswold,CA,University of California-San Diego,Standard Grant,Sylvia Spengler,12/31/19,"$1,125,985.00 ","Sanjoy Dasgupta, Tajana Rosing, Kevin Patrick",wgg@cs.ucsd.edu,Office of Contract & Grant Admin,La Jolla,CA,920930621,8585344896,CSE,7918,"7918, 8235, 9251",$0.00 ,"All cyber-physical systems (CPS) depend on properly calibrated sensors to sense the surrounding environment. Unfortunately, the current state of the art is that calibration is often a manual and expensive operation; moreover, many types of sensors, especially economical ones, must be recalibrated often. This is typically costly, performed in a lab environment, requiring that sensors be removed from service.  MetaSense will reduce the cost and management burden of calibrating sensors. The basic idea is that if two sensors are co-located, then they should report similar values; if they do not, the least-recently-calibrated sensor is suspect.  Building on this idea, this project will provide an autonomous system and a set of algorithms that will automate the detection of calibration issues and preform recalibration of sensors in the field, removing the need to take sensors offline and send them to a laboratory for calibration. The outcome of this project will transform the way sensors are engineered and deployed, increasing the scale of sensor network deployment. This in turn will increase the availability of environmental data for research, medical, personal, and business use. MetaSense researchers will leverage this new data to provide early warning for factors that could negatively affect health.  In addition, graduate student engagement in the research will help to maintain the STEM pipeline.<br/><br/>This project will leverage large networks of mobile sensors connected to the cloud. The cloud will enable using large data repositories and computational power to cross-reference data from different sensors and detect loss of calibration. The theory of calibration will go beyond classical models for computation and physics of CPS. The project will combine big data, machine learning, and analysis of the physics of sensors to calculate two factors that will be used in the calibration. First, MetaSense researchers will identify measurement transformations that, applied in software after the data collection, will generate calibrated results. Second, the researchers will compute the input for an on-board signal-conditioning circuit that will enable improving the sensitivity of the physical measurement. The project will contribute research results in multiple disciplines. In the field of software engineering, the project will contribute a new theory of service reconfiguration that will support new architecture and workflow languages.  New technologies are needed because the recalibration will happen when the machine learning algorithms discover calibration errors, after the data has already been collected and processed. These technologies will support modifying not only the raw data in the database by applying new calibration corrections, but also the results of calculations that used the data. In the field of machine learning, the project will provide new algorithms for dealing with spatiotemporal maps of noisy sensor readings. In particular, the algorithms will work with Gaussian processes and the results of the research will provide more meaningful confidence intervals for these processes, substantially increasing the effectiveness of MetaSense models compared to the current state of the art. In the field of pervasive computing, the project will build on the existing techniques for context-aware sensing to increase the amount of information available to the machine learning algorithms for inferring calibration parameters. Adding information about the sensing context is paramount to achieve correct calibration results. For example, a sensor that measures air pollution inside a car on a highway will get very different readings if the car window is open or closed. Finally, the project will contribute innovations in sensor calibration hardware. Here, the project will contribute innovative signal-conditioning circuits that will interact with the cloud system and receive remote calibration parameters identified by the machine learning algorithms. This will be a substantial advance over current circuits based on simple feedback loops because it will have to account for the cloud and machine learning algorithms in the loop and will have to perform this more complex calibration with power and bandwidth constraints.  Inclusion of graduate students in the research helps to maintain the STEM pipeline."
186,1549981,INSPIRE: Not Unbiased: The Implications of Human-Algorithm Interaction on Training Data and Algorithm Performance,IIS,"Information Technology Researc, Perception, Action & Cognition, Info Integration & Informatics, INSPIRE",10/1/15,6/10/16,Olfa Nasraoui,KY,University of Louisville Research Foundation Inc,Standard Grant,Wei Ding,9/30/20,"$829,222.00 ",Patrick Shafto,olfa.nasraoui@louisville.edu,Atria Support Center,Louisville,KY,402021959,5028523788,CSE,"1640, 7252, 7364, 8078","1640, 7252, 7364, 8653, 9102, 9150, 9251",$0.00 ,"This INSPIRE award is partially funded by the Information Integration and Informatics program in the Division of Information and Intelligent Systems in the Directorate for Computer & Information Science & Engineering, the Perception, Action & Cognition program in the Division of Behavioral and Cognitive Sciences in the Directorate for Social, Behavioral & Economic Sciences, and the Office of Integrative Activities in the Office of the Director.<br/><br/>One of the most common uses of machine learning is to learn to replicate human decisions, a common example is recommender systems. In these systems, computers are trained to replicate the recommendation a collaboration of hundreds or thousands of humans would give, if that were possible.  Most of the data used to train these systems are not from a controlled random sample, but are obtained from users based on outputs of algorithms (e.g., which search engine results do users click on?), which introduces bias into the process and ultimately impacts the quality of the results.  This project addresses this problem by examining how the human decision process that creates these data in the first place is affected by the data coming from machine algorithms, how this in turn impacts the algorithms themselves, and how to ultimately adjust for human bias in the machine learning process.  Specific areas tackled are filtering (e.g., web search) and recommender systems.  The deep research into how the human decision process affects machine learning, and how machine learning impacts the human decision process, can provide significant advances in the accuracy and utility of systems using machine learning.<br/><br/>The project builds on analysis of machine learning algorithms based on Hidden Markov Models (HMMs).  The formal analysis initially looks at ""blind spots"" - the impact of bias from users not getting complete (or a random sample) of data.  Further analysis will be based on the outcome of two human experiments:  Two category recommendation (labeling items, with items to be labelled chosen by random, active learning, and filter-based algorithms), and movie recommendation.  The results will be used to develop improved machine learning approaches based on antidotes (altering learned models to reduce bias) and reactive learning (active learning that takes into account the human and machine biases).  The PIs also have plans to capitalize on the lessons learned by providing examples of the use of cognitive science in a Web Mining course, and of the impact of machine learning in Data Science for Psychologists courses."
187,1531491,"MRI:  Acquisition of PI2, a CAVE2-Inspired Display for Discovery Science, Creativity, and Education",CNS,"Major Research Instrumentation, Special Projects - CNS",9/1/15,3/21/19,Jian Chen,MD,University of Maryland Baltimore County,Standard Grant,Rita Rodriguez,8/31/19,"$377,994.00 ","Craig Saper, Damian Doyle, Karl Steiner, Michael Summers, Penny Rheingans",chen.8028@osu.edu,1000 Hilltop Circle,Baltimore,MD,212500002,4104553140,CSE,"1189, 1714","1189, 9251",$0.00 ,"A hybrid-reality environment, named PI2, is expected to become an integral and vital part of a long-term vision for complex data analysis at this institution, in effect, a human-computer symbiosis in which humans guide computers to identify features of potential interest that the computer then locates and displays. Developing this vision requires advances in multiple areas, including semi-automatic feature detection, visual representations, and interaction, where traditional display modalities limit what can be displayed and perceived. Moreover, the instrument also facilitates broad interdisciplinary research and provides an innovative teaching and research environment for a diverse student population. It contributes to train future generations of researchers in state-of-the-art interactive visual computing for data analysis and enables broader activities and courses and expands research to outreach new applications (e.g., digital humanity for the American Indian population by working with the Smithsonian Museums). Expectations include:<br/>- Advancing multiple avenues of creative inquiry currently blocked or severely restricted will advance rapidly. (The instrument encourages visual thinking among researchers in sciences, healthcare, biomedicine, national security, humanities, and education); <br/>- Establishing appropriate levels of technologies needed for different classes of knowledge discovery analysis; and<br/>- Assembling a set of research projects to investigate the use of the instrument with the expectation of creating a novel, demonstrably useful, rich, and expressive set of techniques for many cyber-physical and cyber-human systems. <br/><br/>PI2, a hybrid-reality environment, aims to support research in interactive computing and digital humanities. The omni-stereo and mono-modalities of the instrument breaks the traditional barriers between virtual reality (VR) and tiled wall displays. The ability of PI2 to synthesize, capture, create, and analyze visual information in unprecedented detail can transform the way analysts interact with visual information. Leveraging many important characteristics: immersion, hybrid reality, high resolution, large field of view, large space and size, body-centric human-computer interaction, and support for heterogeneous data fusion, it benefits multiple projects in research areas (e.g., brain connectome, woodland ecology, interpersonal experiences, biomedicine, universal access, engineering physics, simulations, systems biology, education, digital humanities, green technologies, and unmanned-vehicle studies). The instrument brings together disparate fields: natural language processing, wearable computing, visualization, data mining, and interaction."
188,1550635,EAGER: Computational Models of Essay Rewritings,IIS,"Robust Intelligence, Cyberlearn & Future Learn Tech",9/1/15,4/7/16,Rebecca Hwa,PA,University of Pittsburgh,Standard Grant,Tatiana Korelsky,8/31/18,"$307,912.00 ",Diane Litman,hwa@cs.pitt.edu,300 Murdoch Building,Pittsburgh,PA,152603203,4126247400,CSE,"7495, 8020","7495, 7916, 8045, 9251",$0.00 ,"Natural language processing (NLP) is an integral part of an intelligent tutoring system for writing; it allows the system to automatically analyze student writings and provide feedback to help students to learn. For example, methods have been developed to automatically detect and correct grammar usage errors and to assess aspects of student writing. However, current technology does not offer enough support for teaching students to revise their writings. Unlike mechanical error corrections, the rationales behind revisions are harder to determine. There may be multiple possible changes for an unclear passage in a draft; conversely, one specific writing change might be due to several possible underlying reasons. This EAGER award investigates whether NLP methods can help students to learn to make a more concrete connection between the abstract principles of rewriting (e.g., ""A paper should have a clear thesis"") and the particular contexts in which the revision is carried out. The success of this project would enable educational applications that benefit the society.<br/> <br/>This project evaluates the viability of revision as a pedagogical technique by determining whether student interactions with the revision assistant enables them to learn to write better -- that is, whether certain forms of the feedback (in terms of the perceived purposes and scopes of changes) encourage students to learn to make more effective revisions. More specifically, the project works toward three objectives: <br/>(1) Define a schema for characterizing the types of changes that occur at different levels of the rewriting.  For example, the writer might add one or more sentences to provide evidence to support a thesis; or the writer might add just one or two words to make a phrase more precise. <br/>(2) Based on the schema, design a computational model for recognizing the purpose and scope of each change within a revision. One application of such a model is a revision assistant that serves as a sounding board for students as they experiment with different revision alternatives. <br/>(3) Conduct experiments to study the interactions between students and the revision writing environment in which variations of idealized computational models are simulated. The findings of the experiments pave the way for developing better technologies to support for student learning."
189,1645959,"ABI Development: Global Names Discovery, Indexing and Reconciliation Services",DBI,ADVANCES IN BIO INFORMATICS,11/16/15,7/19/16,Dmitry Mozzherin,IL,University of Illinois at Urbana-Champaign,Continuing Grant,Peter McCartney,7/31/19,"$428,770.00 ",,mozzheri@illinois.edu,1901 South First Street,Champaign,IL,618207406,2173332187,BIO,1165,9178,$0.00 ,"In the mid-18th century biology as a science was revolutionized by Carl Linneaus and the use of binomial scientific names. Such names consist of two words in Latin - genus name and specific epithet - for example, Homo sapiens for humans. For higher classifications Linneaus adopted names consisting of a single Latin word. This nomenclatural system dramatically simplified handling species information, and gave scientists from different countries a standard way of communication. This system has been so successful it has persisted for over 250 years and is still in very active use today. An enormous amount of scientific and popular science literature uses scientific names. In the electronic age scientific names enable the effective organization of biodiversity information from various sources, as demonstrated by the Encyclopedia of Life (http://eol.org) and Biodiversity Heritage Library (http://bhl.org). However the naming system introduced by Linnaeus is far from perfect. For example, if scholars decide to move a particular species to different genus - the name changes, and finding information about it becomes much more difficult. For example, a very prominent model organism of modern science Drosophila melanogaster (fruit fly) is now recommended to be placed in the genus Sophophora, changing its name to Sophophora melanogaster. Such changes are inevitable in Linnaean nomenclature, and they create a significant amount of confusion. The project aims to create a system which would allow a user to enter a name, and find out if it is a known scientific name, if another name should be used instead, if there are other historical synonyms, and if this name is a misspelling of a known scientific name. The system will also display which data sources contain information related to a name, and will provide a list of relevant web-pages (URLs). This project aims to make significant strides in removing ambiguity and confusion from biodiversity data. Teaching activities are planned with college students at Arizona State and there are opportunities for participation in Google Summer of Code and Biodiversity informatics training courses developed at MBL.<br/><br/>The first stage of the research was also funded by NSF and it allowed the development of a prototype and 'proof of concept' algorithms for finding and verifying scientific names in texts, images, species lists. This second phase aims to make production version of the algorithms, make them precise, fast, and scale them up to serve the global biological community.  The task of scientific name disambiguation is not a simple one. It requires cooperation of many researchers. Discovery of scientific names in texts uses natural language processing algorithms which we plan to improve. The project aims to collect all known spellings and renderings of scientific names (20 million are currently collected) and organizes them into groups belonging to the same sets of organisms. It also catalogues where each spelling was used and stores information as a global name index. The database is powered by a search engine which uses fuzzy matching algorithms, and name parsing algorithms to find that names like 'Carek scirpoidea Michx. var. convoluta Kük.' and 'Carex scirpoidea subsp. convoluta (Kukenth) D. A. Dunlop' refer to the same<br/>subspecies. The project's URL is: http://globalnames.org/"
190,1447413,BIGDATA: F: DKA: Collaborative Research: Structured Nearest Neighbor Search in High Dimensions,IIS,Big Data Science &Engineering,9/15/15,9/10/14,Pedro Felzenszwalb,RI,Brown University,Standard Grant,Sylvia Spengler,8/31/18,"$250,000.00 ",,pff@brown.edu,BOX 1929,Providence,RI,29129002,4018632777,CSE,8083,"7433, 8083, 9150",$0.00 ,"A fundamental problem in the analysis of large datasets consists of finding one or more data items that are as similar as possible to an input query. This situation occurs, for example, when a user wants to identify a product captured in a photo. The corresponding computational problem, called Nearest Neighbor (NN) Search, has attracted a large body of research, with several algorithms having significant impact. Yet the state of the art in NN suffers from important theoretical and practical limitations. In particular, it does not provide a natural way to exploit data *structure* that is present in many applications. For example, although the identity of a depicted object does not change when one varies the lighting or the position of the object, the current NN algorithms will treat the resulting images as completely different from each other and thus will mis-identify the object. To overcome this difficulty, in this project the PIs will develop new efficient algorithms that incorporate problem structure into NN search. The PIs expect that such methods will produce substantially better results for many massive data analysis tasks.<br/><br/>To ensure that the work is grounded in an important application, the PIs will focus on computer vision, an area where Internet-scale datasets are having a substantial impact. NN search is vital for computer vision, and in fact many senior computer vision researchers view improved NN techniques as their top algorithmic priority. Image and video have significant structure, often spatial in nature, which algorithmic techniques such as graph cuts have been able to exploit with considerable success. The proposed work will formulate new variants of NN search that make use of additional structure, and will design efficient algorithms to solve these problems over large datasets. In particular, the PIs will investigate three structured NN problem formulations. Simultaneous nearest-neighbor queries involves multiple queries where the answers should be compatible with each other. Nearest-neighbor under transformations considers distances that are invariant to a variety of image transformations. Nearest-neighbors for subspaces involves searching a set of linear or affine subspaces for the one that comes closest to a query point.  Broader impacts of the project include graduate training in both algorithms and image processing.<br/><br/>For further information see the project web site at: http://cs.brown.edu/~pff/SNN/"
191,1535967,AitF: FULL: From Worst-Case to Realistic-Case Analysis for Large Scale Machine Learning Algorithms,CCF,Algorithms in the Field,9/1/15,10/5/17,Maria-Florina Balcan,PA,Carnegie-Mellon University,Standard Grant,A. Funda Ergun,8/31/21,"$719,986.00 ","Tom Mitchell, Avrim Blum",ninamf@cs.cmu.edu,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,CSE,7239,012Z,$0.00 ,"The aim of this project is to develop mathematical models, analysis, and algorithms that will advance both the design and understanding of large-scale machine learning systems.  In recent years, machine learning has come into widespread use across a range of applications, and we have also seen significant advances in the theoretical understanding of learning processes. Yet despite these successes, there remains a gulf between theory and application.  For example, applications often demonstrate success on problems that theory tells us are intractable in the worst case.  Furthermore, as modern machine learning applications scale up from learning of single tasks to learning many tasks simultaneously, new theory is needed to analyze these larger scale multi-task learning settings.  This project aims to bridge this gap by developing and applying theory targeted toward realistic-case analysis of learning problems, which capture the structures that enable applications to succeed even when theoretical analyses show the impossibility of doing so in the worst case.  This work will be guided by problems at the core of NELL and InMind, two current learning systems that address large-scale multi-task machine learning problems, for reading the web and providing highly personalized electronic assistants to hundreds of interconnected mobile phone users.<br/><br/>More specifically, this project has three main components:<br/><br/>(1) To develop computationally efficient algorithms for clustering, constrained optimization, and related optimization tasks crucial to large-scale machine learning, with provable guarantees under natural, realistic non-worst-case analysis models.<br/><br/>(2) To develop foundations and practical algorithms for multi-task and life-long learning that exploit explicit and implicit structure to minimize key resources including computation time and human labeling effort, as well as address key constraints such as privacy.<br/><br/>(3) To apply the algorithms developed to solve key challenges in two current large-scale learning systems, NELL and InMind.<br/><br/>The proposed work will aid the development of large-scale machine learning applications, as well as create important connections between multiple areas of significant importance in modern machine learning and theoretical computer science. In addition to advising students on topics connected to this project, research progress (on multi-task learning, life-long learning, and clustering) will be integrated in the curricula of several courses at CMU and course materials will be made available on the world wide web. Course projects based on this research will be available to students in the introductory machine learning course at CMU, which enrolls over 600 students each year. In addition, students seeking topics for undergraduate thesis or independent study may also pursue research affiliated with this project."
192,1508131,CDS&E: In Situ Data Analysis and Scalable Machine Learning for Exascale Scientific Simulations,OAC,CDS&E,9/1/15,9/11/15,Ke-Thia Yao,CA,University of Southern California,Standard Grant,Tevfik Kosar,8/31/16,"$150,000.00 ","Aiichiro Nakano, Ke-Thia Yao, Jacqueline Chame",kyao@isi.edu,University Park,Los Angeles,CA,900890001,2137407762,CSE,8084,"7433, 8084",$0.00 ,"Large scale scientific simulations are used in a range of application domains including materials science, climate modeling, combustion and others. These simulations are often limited by the hardware on which they run, including the capacity of computational platforms and storage systems. The goals of these simulations may include finding rare but interesting events in simulation output, discovering common sequences of events, and discovering causality among events. Often, these scientific simulations consume all available computational resources on a high performance computing platform during a simulation run, and be forced to only sample data  techniques to decrease the size of the simulation so as to make it possible to store, transfer and post-process the output data. Such data sampling reduces the quality of science results, since not all available data are utilized during analysis. This project aims to greatly improve the scale and quality of scientific simulation results using innovative ""in situ"" algorithms and machine learning techniques for rare event detection. This research will be validated using a large-scale materials science simulation, that of self-healing nanomaterial system capable of sensing and repairing damage in harsh chemical environments and in high temperature/high pressure operating conditions. Self-healing is of significance since it can improve the reliability and lifetime of materials while reducing the cost of manufacturing, monitoring and maintenance of high-temperature turbines, wind, solar energy and lighting systems. The research can be generalized to a range of scientific simulation domains that share the common goals of discovering rare and interesting events, sequences of events and causality among events. Finally, the research concepts and results will be incorporated into graduate level courses taught by the research team.<br/><br/>The goal of the project is to demonstrate the feasibility, performance and scalability of the research approaches in greatly improving the quality of exascale scientific simulations using in situ machine learning algorithms within a well-defined, reusable in situ software framework. The scope of the project includes: selecting a simplified, but representative, long-time material process suitable for super-state parallel replica dynamics (SPRD); developing in situ machine learning algorithms for rare event detection of super-state transitions; and studying library-based approaches to support the high performance coupling of exascale simulations with in situ machine learning algorithms. To accomplish the project goals, the following three objectives are defined: 1) Prove the feasibility, performance and scalability of in situ SPRD simulation for predicting long-time material processes; 2) Prove the feasibility, performance and scalability of in situ machine learning algorithms for rare-event detection of super-state transitions; and 3) Prove the feasibility, performance and scalability of in situ library-based approaches to coupling exascale simulations and machine learning algorithms."
193,1547880,INSPIRE: Teaming Citizen Science with Machine Learning to Deepen LIGO's View of the Cosmos,IIS,"LIGO RESEARCH SUPPORT, OFFICE OF MULTIDISCIPLINARY AC, Information Technology Researc, COMPUTATIONAL PHYSICS, HCC-Human-Centered Computing, INSPIRE",10/1/15,12/22/17,Vassiliki Kalogera,IL,Northwestern University,Continuing Grant,William Bainbridge,9/30/19,"$1,015,663.00 ","Aggelos Katsaggelos, Kevin Crowston, Shane Larson, Laura Trouille, Joshua Smith, Laura Whyte",vicky@northwestern.edu,750 N. Lake Shore Drive,Chicago,IL,606114579,3125037955,CSE,"1252, 1253, 1640, 7244, 7367, 8078","1252, 7367, 8653, 9251",$0.00 ,"This INSPIRE award is partially funded by the Cyber-Human Systems Program in the Division of Information and Intelligent Systems in the Directorate for Computer Science and Engineering, the Gravitational Physics Program in the Division of Physics in the Directorate for Mathematical and Physical Sciences, and the Office of Integrative Activities.<br/><br/>This innovative project will develop a citizen science system to support the Advanced Laser Interferometer Gravitational wave Observatory (aLIGO), the most complicated experiment ever undertaken in gravitational physics. Before the end of this decade it will open up the window of gravitational wave observations on the Universe. However, the high detector sensitivity needed for astrophysical discoveries makes aLIGO very susceptible to noncosmic artifacts and noise that must be identified and separated from cosmic signals. Teaching computers to identify and morphologically classify these artifacts in detector data is exceedingly difficult. Human eyesight is a proven tool for classification, but the aLIGO data streams from approximately 30,000 sensors and monitors easily overwhelm a single human. This research will address these problems by coupling human classification with a machine learning model that learns from the citizen scientists and also guides how information is provided to participants. A novel feature of this system will be its reliance on volunteers to discover new glitch classes, not just use existing ones. The project includes research on the human-centered computing aspects of this sociocomputational system, and thus can inspire future citizen science projects that do not merely exploit the labor of volunteers but engage them as partners in scientific discovery.  Therefore, the project will have substantial educational benefits for the volunteers, who will gain a good understanding on how science works, and will be a part of the excitement of opening up a new window on the universe.<br/><br/>This is an innovative, interdisciplinary collaboration between the existing LIGO, at the time it is being technically enhanced, and Zooniverse, which has fielded a workable crowdsourcing model, currently involving over a million people on 30 projects.  The work will help aLIGO to quickly identify noise and artifacts in the science data stream, separating out legitimate astrophysical events, and allowing those events to be distributed to other observatories for more detailed source identification and study. This project will also build and evaluate an interface between machine learning and human learning that will itself be an advance on current methods. It can be depicted as a loop: (1) By sifting through enormous amounts of aLIGO data, the citizen scientists will produce a robust ""gold standard"" glitch dataset that can be used to seed and train machine learning algorithms that will aid in the identification task. (2) The machine learning protocols that select and classify glitch events will be developed to maximize the potential of the citizen scientists by organizing and passing the data to them in more effective ways. The project will experiment with the task design and workflow organization (leveraging previous Zooniverse experience) to build a system that takes advantage of the distinctive strengths of the machines (ability to process large amounts of data systematically) and the humans (ability to identify patterns and spot discrepancies), and then using the model to enable high quality aLIGO detector characterization and gravitational wave searches."
194,1525919,III: Small: Collaborative Research: Towards Interpretable Machine Learning,IIS,Info Integration & Informatics,9/1/15,6/15/20,Kilian Weinberger,NY,Cornell University,Standard Grant,Wei Ding,8/31/21,"$250,000.00 ",,kilianweinberger@cornell.edu,373 Pine Tree Road,Ithaca,NY,148502820,6072555014,CSE,7364,"7364, 7923",$0.00 ,"This research project investigates the design and development of machine learning algorithms that make decisions that are interpretable by humans.  As predictions of machine learning models are increasingly used in making decisions with critical consequences (e.g., in medicine or economics), it is important that decision makers understand the rationale behind these predictions.  The project defines interpretable algorithms through three key properties; Simplicity: intuitively comprehensible by users who are not experts in machine learning, Verifiability: a clear relationship between input features and model output, and Actionability: For a given input and desired output, the user should be able to identify changes to the input features that transform the model prediction to the desired output.  The project investigates how to design distance metrics supporting simplicity and verifiability, as well as algorithms to identify input changes to change outputs.  The project will be evaluated in a medical context, addressing the problem of early detection of hospital patients at risk of sudden deterioration.<br/><br/>This work builds on the well-understood k-Nearest-Neighbor classifier, which would inherently seem to provide simplicity and verifiability.  The challenge is in high dimensions, e.g., when used for document classification; differences are spread across more dimensions than are humanly comprehensible.  The project uses novel dimensionality reduction approaches to create dissimilarity metrics that are interpretable and accurate.  Visualization techniques to present this data will be explored, including techniques supporting more complex classification approaches such as ensembles.  The project investigates novel methods for delivering actionability in machine learning algorithms by identifying changes that can truly transform an entity's class membership - a problem that has recently been identified as surprisingly difficult.  A secondary outcome will be improvements in classifier robustness, as small changes that change class membership are a good indication of non-robustness."
195,1526012,III: Small: Collaborative Research: Towards Interpretable Machine Learning,IIS,Info Integration & Informatics,9/1/15,8/19/15,Yixin Chen,MO,Washington University,Standard Grant,Sylvia Spengler,8/31/19,"$248,790.00 ",,chen@cse.wustl.edu,CAMPUS BOX 1054,Saint Louis,MO,631304862,3147474134,CSE,7364,"7364, 7923, 9150",$0.00 ,"This research project investigates the design and development of machine learning algorithms that make decisions that are interpretable by humans.  As predictions of machine learning models are increasingly used in making decisions with critical consequences (e.g., in medicine or economics), it is important that decision makers understand the rationale behind these predictions.  The project defines interpretable algorithms through three key properties; Simplicity: intuitively comprehensible by users who are not experts in machine learning, Verifiability: a clear relationship between input features and model output, and Actionability: For a given input and desired output, the user should be able to identify changes to the input features that transform the model prediction to the desired output.  The project investigates how to design distance metrics supporting simplicity and verifiability, as well as algorithms to identify input changes to change outputs.  The project will be evaluated in a medical context, addressing the problem of early detection of hospital patients at risk of sudden deterioration.<br/><br/>This work builds on the well-understood k-Nearest-Neighbor classifier, which would inherently seem to provide simplicity and verifiability.  The challenge is in high dimensions, e.g., when used for document classification; differences are spread across more dimensions than are humanly comprehensible.  The project uses novel dimensionality reduction approaches to create dissimilarity metrics that are interpretable and accurate.  Visualization techniques to present this data will be explored, including techniques supporting more complex classification approaches such as ensembles.  The project investigates novel methods for delivering actionability in machine learning algorithms by identifying changes that can truly transform an entity's class membership - a problem that has recently been identified as surprisingly difficult.  A secondary outcome will be improvements in classifier robustness, as small changes that change class membership are a good indication of non-robustness."
196,1447473,BIGDATA: F: DKA: Collaborative Research: Structured Nearest Neighbor Search in High Dimensions,IIS,Big Data Science &Engineering,9/15/15,9/10/14,Ramin Zabih,NY,Cornell University,Standard Grant,Sylvia Spengler,8/31/20,"$500,000.00 ",Robert Kleinberg,rdz@cs.cornell.edu,373 Pine Tree Road,Ithaca,NY,148502820,6072555014,CSE,8083,"7433, 8083",$0.00 ,"A fundamental problem in the analysis of large datasets consists of finding one or more data items that are as similar as possible to an input query. This situation occurs, for example, when a user wants to identify a product captured in a photo. The corresponding computational problem, called Nearest Neighbor (NN) Search, has attracted a large body of research, with several algorithms having significant impact. Yet the state of the art in NN suffers from important theoretical and practical limitations. In particular, it does not provide a natural way to exploit data *structure* that is present in many applications. For example, although the identity of a depicted object does not change when one varies the lighting or the position of the object, the current NN algorithms will treat the resulting images as completely different from each other and thus will mis-identify the object. To overcome this difficulty, in this project the PIs will develop new efficient algorithms that incorporate problem structure into NN search. The PIs expect that such methods will produce substantially better results for many massive data analysis tasks.<br/><br/>To ensure that the work is grounded in an important application, the PIs will focus on computer vision, an area where Internet-scale datasets are having a substantial impact. NN search is vital for computer vision, and in fact many senior computer vision researchers view improved NN techniques as their top algorithmic priority. Image and video have significant structure, often spatial in nature, which algorithmic techniques such as graph cuts have been able to exploit with considerable success. The proposed work will formulate new variants of NN search that make use of additional structure, and will design efficient algorithms to solve these problems over large datasets. In particular, the PIs will investigate three structured NN problem formulations. Simultaneous nearest-neighbor queries involves multiple queries where the answers should be compatible with each other. Nearest-neighbor under transformations considers distances that are invariant to a variety of image transformations. Nearest-neighbors for subspaces involves searching a set of linear or affine subspaces for the one that comes closest to a query point.  Broader impacts of the project include graduate training in both algorithms and image processing.<br/><br/>For further information see the project web site at: http://cs.brown.edu/~pff/SNN/"
197,1447476,BIGDATA: F: DKA: Collaborative Research: Structured Nearest Neighbor Search in High Dimensions,IIS,Big Data Science &Engineering,9/15/15,9/10/14,Piotr Indyk,MA,Massachusetts Institute of Technology,Standard Grant,Sylvia Spengler,8/31/19,"$500,000.00 ",William Freeman,indyk@mit.edu,77 MASSACHUSETTS AVE,Cambridge,MA,21394301,6172531000,CSE,8083,"7433, 8083",$0.00 ,"A fundamental problem in the analysis of large datasets consists of finding one or more data items that are as similar as possible to an input query. This situation occurs, for example, when a user wants to identify a product captured in a photo. The corresponding computational problem, called Nearest Neighbor (NN) Search, has attracted a large body of research, with several algorithms having significant impact. Yet the state of the art in NN suffers from important theoretical and practical limitations. In particular, it does not provide a natural way to exploit data *structure* that is present in many applications. For example, although the identity of a depicted object does not change when one varies the lighting or the position of the object, the current NN algorithms will treat the resulting images as completely different from each other and thus will mis-identify the object. To overcome this difficulty, in this project the PIs will develop new efficient algorithms that incorporate problem structure into NN search. The PIs expect that such methods will produce substantially better results for many massive data analysis tasks.<br/><br/>To ensure that the work is grounded in an important application, the PIs will focus on computer vision, an area where Internet-scale datasets are having a substantial impact. NN search is vital for computer vision, and in fact many senior computer vision researchers view improved NN techniques as their top algorithmic priority. Image and video have significant structure, often spatial in nature, which algorithmic techniques such as graph cuts have been able to exploit with considerable success. The proposed work will formulate new variants of NN search that make use of additional structure, and will design efficient algorithms to solve these problems over large datasets. In particular, the PIs will investigate three structured NN problem formulations. Simultaneous nearest-neighbor queries involves multiple queries where the answers should be compatible with each other. Nearest-neighbor under transformations considers distances that are invariant to a variety of image transformations. Nearest-neighbors for subspaces involves searching a set of linear or affine subspaces for the one that comes closest to a query point.  Broader impacts of the project include graduate training in both algorithms and image processing.<br/><br/>For further information see the project web site at: http://cs.brown.edu/~pff/SNN/"
198,1446899,CPS: TTP Option: Synergy:  Collaborative Research: Calibration of Personal Air Quality Sensors in the Field - Coping with Noise and Extending Capabilities,CNS,CPS-Cyber-Physical Systems,1/1/15,8/27/14,Michael Hannigan,CO,University of Colorado at Boulder,Standard Grant,Sylvia Spengler,12/31/19,"$290,007.00 ",,hannigan@colorado.edu,"3100 Marine Street, Room 481",Boulder,CO,803031058,3034926221,CSE,7918,"7918, 8235",$0.00 ,"All cyber-physical systems (CPS) depend on properly calibrated sensors to sense the surrounding environment. Unfortunately, the current state of the art is that calibration is often a manual and expensive operation; moreover, many types of sensors, especially economical ones, must be recalibrated often. This is typically costly, performed in a lab environment, requiring that sensors be removed from service.  MetaSense will reduce the cost and management burden of calibrating sensors. The basic idea is that if two sensors are co-located, then they should report similar values; if they do not, the least-recently-calibrated sensor is suspect.  Building on this idea, this project will provide an autonomous system and a set of algorithms that will automate the detection of calibration issues and preform recalibration of sensors in the field, removing the need to take sensors offline and send them to a laboratory for calibration. The outcome of this project will transform the way sensors are engineered and deployed, increasing the scale of sensor network deployment. This in turn will increase the availability of environmental data for research, medical, personal, and business use. MetaSense researchers will leverage this new data to provide early warning for factors that could negatively affect health.  In addition, graduate student engagement in the research will help to maintain the STEM pipeline.<br/><br/>This project will leverage large networks of mobile sensors connected to the cloud. The cloud will enable using large data repositories and computational power to cross-reference data from different sensors and detect loss of calibration. The theory of calibration will go beyond classical models for computation and physics of CPS. The project will combine big data, machine learning, and analysis of the physics of sensors to calculate two factors that will be used in the calibration. First, MetaSense researchers will identify measurement transformations that, applied in software after the data collection, will generate calibrated results. Second, the researchers will compute the input for an on-board signal-conditioning circuit that will enable improving the sensitivity of the physical measurement. The project will contribute research results in multiple disciplines. In the field of software engineering, the project will contribute a new theory of service reconfiguration that will support new architecture and workflow languages.  New technologies are needed because the recalibration will happen when the machine learning algorithms discover calibration errors, after the data has already been collected and processed. These technologies will support modifying not only the raw data in the database by applying new calibration corrections, but also the results of calculations that used the data. In the field of machine learning, the project will provide new algorithms for dealing with spatiotemporal maps of noisy sensor readings. In particular, the algorithms will work with Gaussian processes and the results of the research will provide more meaningful confidence intervals for these processes, substantially increasing the effectiveness of MetaSense models compared to the current state of the art. In the field of pervasive computing, the project will build on the existing techniques for context-aware sensing to increase the amount of information available to the machine learning algorithms for inferring calibration parameters. Adding information about the sensing context is paramount to achieve correct calibration results. For example, a sensor that measures air pollution inside a car on a highway will get very different readings if the car window is open or closed. Finally, the project will contribute innovations in sensor calibration hardware. Here, the project will contribute innovative signal-conditioning circuits that will interact with the cloud system and receive remote calibration parameters identified by the machine learning algorithms. This will be a substantial advance over current circuits based on simple feedback loops because it will have to account for the cloud and machine learning algorithms in the loop and will have to perform this more complex calibration with power and bandwidth constraints.  Inclusion of graduate students in the research helps to maintain the STEM pipeline."
199,1551614,EAGER: Scaling Up Machine Learning with Virtual Memory,IIS,"Info Integration & Informatics, Software & Hardware Foundation",10/1/15,9/14/15,Duen Horng Chau,GA,Georgia Tech Research Corporation,Standard Grant,Aidong Zhang,9/30/17,"$184,904.00 ",Richard Vuduc,polo@gatech.edu,Office of Sponsored Programs,Atlanta,GA,303320420,4048944819,CSE,"7364, 7798","7364, 7916, 7943",$0.00 ,"Large datasets in terabytes or petabytes are increasingly common, calling for new kinds of scalable machine learning approaches. While state-of-the-art techniques often use complex designs, specialized methods to store and work with large datasets, this project proposes a minimalist approach that forgoes such complexities, by leveraging the fundamental virtual memory capability found on all modern operating systems, to load into the virtual memory space the large datasets that are otherwise too large to fit in the computer's main memory. This main idea will allow developers to easily work with large datasets as if they were in-memory data, enabling them to create machine learning software that is significantly easier to develop and maintain, yet faster and more scalable. Developers will achieve higher work efficiency and make fewer programming errors; companies will reduce operating costs; and researchers will innovate methodology without getting bogged down by implementation details and scalability concerns. The proposed ideas could make a far-reaching impact on industry and academia, in science, education, and technology, as they face increasing challenges in applying machine learning on large datasets. The proposed ideas will also help train the next generation of scientists and engineers by allowing students to learn to work with large datasets in significantly simpler ways. As virtual memory is universally available on modern devices and operating systems, the proposed ideas will also work on mobile, low-power devices, enabling them to perform computation at unprecedented scales and speed.<br/><br/>This project investigates a fundamental, radical way to scale up machine learning algorithms based on virtual memory, one that may be easier to code and maintain, but currently under-utilized in by both single-machine and multi-machine distributed approaches. This research aims to develop deep understanding of this radical idea, its benefits and limitations, and to what extent these results apply in various settings, with respect to datasets, memory sizes, page sizes (e.g., from the default 4KB to the jumbo 2MB pages that enable terabyes of virtual memory space), and architectures (e.g., testing on distributed shared memory file systems like Lustre that support paging and virtual memory over large computer clusters). The researchers will build on their preliminary work on graph algorithms that already demonstrates significant speed-up over state-of-the-art approaches; they will extend their approach to a wide range of machine learning and data mining algorithms. They will also develop mathematical models and systematic approaches to profile and predict algorithm performance and energy usage based on extensive evaluation across platforms, datasets, and languages. <br/><br/>For further information, see the project web site at: http://poloclub.gatech.edu/mmap/."
200,1523346,32nd International Conference on Machine Learning (ICML 2015),IIS,"Information Technology Researc, Info Integration & Informatics, Robust Intelligence",6/1/15,5/28/15,Kilian Weinberger,NY,Cornell University,Standard Grant,Weng-keen Wong,5/31/17,"$35,000.00 ",,kilianweinberger@cornell.edu,373 Pine Tree Road,Ithaca,NY,148502820,6072555014,CSE,"1640, 7364, 7495","7495, 7556",$0.00 ,"This award provides partial support for student authors from US Institutions to travel to ICML 2015.  The award will support roughly 40 students (who have authored accepted submissions) to travel to the conference and present their research.  ICML is one of the premier Machine Learning conferences in the world, and a highly competitive venue.  The student scholarships funded under this award creates opportunities for students to present their work to an international audience at a top conference --- a significant professional enhancement for graduate research students.  The award thus contributes to the education and training of the next generation of researchers and educatiors in the increasingly important area of Machine Learning.  <br/><br/>Special attention is paid to broadening participation of students from groups traditionally underrepresented in Computer Science in general, and in Machine Learning in particular.  The selection of students to be funded is based on a selection committee review of their authorship and financial needs. The student scholarships are very important for encouraging student participation and shaping the future of the field. Special attention will be paid to broadening the participation of students from groups that are traditionally underrepresented in Computer Science in general and in Machine Learning research in particular, and students from institutions with limited Machine Learning expertise who would benefit from the opportunity to interact with researchers from around the world."
201,1561462,NIPS 2015 Workshop on Machine Learning For Healthcare,IIS,"Robust Intelligence, Smart and Connected Health",11/1/15,10/28/15,David Sontag,NY,New York University,Standard Grant,Hector Munoz-Avila,4/30/16,"$6,000.00 ",,dsontag@mit.edu,70 WASHINGTON SQUARE S,NEW YORK,NY,100121019,2129982121,CSE,"7495, 8018","7495, 7556, 8018",$0.00 ,"The rise in availability and size of clinical data presents rich opportunities to develop and apply machine learning methods to solve problems faced by clinicians and usher in new forms of medical practice (such as personalized medicine) that would otherwise be infeasible.  This workshop, focused on machine learning for healthcare, will be co-located with the annual Neural Information Processing Systems (NIPS) conference in Montreal, the foremost international meeting for machine learning research.  The workshop will i) help build the community of machine learning researchers focused on solutions to healthcare problems, and ii) foster discussions between machine learning and clinical researchers to leverage machine learning in service of healthcare.  The workshop includes industry and academic participants.  It includes a ""healthcare challenges"" session in which clinical researchers will bring new problems to the statistical machine learning community.<br/><br/>This award supports Ph.D student attendance at the workshop, and hence will influence the future workforce in applications of statistical learning to healthcare.  The students will be introduced to key open questions and interact with established machine learning and clinical researchers, thus encouraging their investment into this emerging domain."
202,1544917,CPS/Synergy/Collaborative Research: Smart Calibration Through Deep Learning for High-Confidence and Interoperable Cyber-Physical Additive Manufacturing Systems,CMMI,CPS-Cyber-Physical Systems,9/1/15,8/21/15,Qiang Huang,CA,University of Southern California,Standard Grant,Bruce Kramer,8/31/20,"$350,000.00 ",,qiang.huang@usc.edu,University Park,Los Angeles,CA,900890001,2137407762,ENG,7918,"082E, 083E, 7918, 8235",$0.00 ,"Additive Manufacturing holds the promise of revolutionizing manufacturing. One important trend is the emergence of cyber additive manufacturing communities for innovative design and fabrication. However, due to variations in materials and processes, design and computational algorithms currently have limited adaptability and scalability across different additive manufacturing systems. This award will establish the scientific foundation and engineering principles needed to achieve adaptability, extensibility, and system scalability in cyber-physical additive manufacturing systems, resulting in high efficiency and accuracy fabrication. The research will facilitate the evolution of existing isolated and loosely-connected additive manufacturing facilities into fully functioning cyber-physical additive manufacturing systems with increased capabilities. The application-based, smart interfacing infrastructure will complement existing cyber additive communities and enhance partnerships between academia, industry, and the general public. The research will contribute to the technology and engineering of Cyber-physical Systems and the economic competitiveness of US manufacturing. This interdisciplinary research will generate new curricular materials and help educate a new generation of cybermanufacturing workforce. <br/><br/>The research will establish smart and dynamic system calibration methods and algorithms through deep learning that will enable high-confidence and interoperable cyber-physical additive manufacturing systems. The dynamic calibration and re-calibration algorithms will provide a smart interfacing layer of infrastructure between design models and physical additive manufacturing systems. Specific research tasks include: (1) Establishing smart and fast calibration algorithms to make physical additive manufacturing machines adaptable to design models; (2) Deriving prescriptive compensation algorithms to achieve extensible design models; (3) Dynamic recalibration through deep learning for improved predictive modeling and compensation; and (4) Developing a smart calibration server and APP prototype test bed for scalable additive cyberinfrastructures."
203,1518703,SHF: Large: General-Purpose Approximate Computing Across the System Stack,CCF,"Special Projects - CCF, Software & Hardware Foundation",7/1/15,9/17/18,Luis Ceze,WA,University of Washington,Continuing Grant,Yuanyuan Yang,6/30/21,"$2,399,764.00 ","Mark Oskin, Daniel Grossman, Emina Torlak",luisceze@cs.washington.edu,4333 Brooklyn Ave NE,Seattle,WA,981950001,2065434043,CSE,"2878, 7798","7798, 7925, 7941",$0.00 ,"Energy efficiency is a fundamental challenge facing the IT industry. Benefits go beyond reduced power demands in data centers and longer battery life in mobile devices. It is a fundamental enabler of future systems as we approach the limits of silicon device scaling. Therefore, providing a novel and holistic approach to energy efficiency in computer systems can have a transformative effect on IT and society. Many important applications---e.g., computer vision, novel user interfaces, signal processing, web search, augmented reality, and big-data analytics---can inherently tolerate some forms of inaccurate computation at various levels. With approximate computing, this fact can be exploited for fundamentally more efficient computing systems. This is a direct analog to Daniel Kahneman's model of how our brains work: they do cheap and quick reasoning (using System 1) in an approximate way, and when required, they do more expensive (and tiring) detailed thinking (using System 2). This research project will develop a analogous model for computer systems, from hardware to programming tools. <br/><br/>Taking advantage of approximate computing requires significant innovation: programming models, tools for testing and debugging, and system support with quality guarantees. This project will develop a comprehensive solution across the system stack, from programming language to hardware. To demonstrate the potentials, prototypes of compelling applications amenable to approximate computing (e.g., computer vision) will be created. The project involves work on systems, programming languages, formal methods, and architecture, matching the inter-disciplinary expertise of the PI team. In addition to research papers, the project scope also includes releasing tools, benchmarks, and general infrastructure to the academic and industrial communities. The PIs have a history of inclusion of minorities and undergraduate students in their research efforts."
204,1446801,CPS: Breakthrough: Programming and Execution Environment for Geo-Distributed Latency-Sensitive Applications,CNS,"Information Technology Researc, Special Projects - CNS",1/1/15,1/13/20,Umakishore Ramachandran,GA,Georgia Tech Research Corporation,Standard Grant,David Corman,3/31/20,"$472,754.00 ",,rama@cc.gatech.edu,Office of Sponsored Programs,Atlanta,GA,303320420,4048944819,CSE,"1640, 1714","8234, 9251",$0.00 ,"The confluence of new networked sensing technologies (e.g., cameras), distributed computational resources (e.g., cloud computing), and algorithmic advances (e.g., computer vision) are offering new and exciting opportunities for solving a variety of new problems that are of societal importance including emergency response, disaster recovery, surveillance, and transportation.  Solutions to this new class of problems, referred to as ""situation awareness"" applications, include surveillance via large-scale distributed camera networks and personalized traffic alerts in vehicular networks using road and traffic sensing.   A breakthrough in system software technology is needed to meet the challenges posed by these applications since they are latency-sensitive, data intensive, involve heavy-duty processing, and must run 24x7 while dealing with the vagaries of the physical world.  This project aims to make such a breakthrough, through new distributed programming idioms and resource allocation strategies.  To better identify the challenges posed by situation awareness applications, the project includes experimental deployment of the new technologies in partnership with the City of Baton Rouge, Louisiana. <br/><br/>The central activity is to develop appropriate system abstractions for design of situation awareness applications and encapsulate them in distributed programming idioms for domain experts (e.g., vision researchers).  The resulting programming framework allows association of critical attributes such as location, time, and mobility with sensed data to reason about causal events along these axes.  To meet the latency constraints of these applications, the project develops geospatial resource allocation mechanisms that complement and support the distributed programming idioms, extending the utility-computing model of cloud computing to the edge of the network.  Since the applications often have to work with inexact knowledge of what is happening in the physical environment, owing to limitations of the distributed sensing sources, the project also investigates system support for application-specific information fusion and spatio-temporal analyses to increase the quality of results. Efforts toward development of a future cyber-physical systems workforce include creation of a new multidisciplinary curriculum around situation awareness, exploration of new immersive learning pedagogical styles, and mentoring and providing research experience to undergraduate students through research experiences and internships aimed at increasing participation of women and minorities."
205,1513816,AF: Medium: Collaborative Research: Algorithmic Foundations for Trajectory Collection Analysis,CCF,Algorithmic Foundations,6/1/15,5/30/17,Pankaj Agarwal,NC,Duke University,Continuing Grant,Joseph Maurice Rojas,5/31/20,"$807,999.00 ",Carlo Tomasi,pankaj@cs.duke.edu,"2200 W. Main St, Suite 710",Durham,NC,277054010,9196843030,CSE,7796,"7924, 7929, 9251",$0.00 ,"This project engages experts in computational geometry, optimization, and computer vision from Duke and Stanford to develop a theoretical and algorithmic framework for analyzing large collections of trajectory data from sensors or simulations. Trajectories are functions from a time interval to a multi-dimensional space that arise in the description of any system that evolves over time.<br/><br/>Trajectory data is being recorded or inferred from hundreds of millions of sensors nowadays, from traffic monitoring systems and GPS sensors on cell phones to cameras in surveillance systems or those embedded in smart phones, in helmets of soldiers in the field, or in medical devices, as well as from scientific experiments and simulations, such as molecular dynamics computations in biology. Algorithms for trajectory-data analysis can lead to video retrieval systems, activity recognition, facility monitoring and surveillance, medical investigation, traffic navigation aids, military analysis and deployment tools, entertainment, and much more. Many of these application fields intersect areas of national security, as well as domains of broader societal benefit.<br/><br/>This project pursues a transformational approach that combines the geometry of individual trajectories with the information that an entire collection of trajectories provides about its members. Emphasis is on simple and fast algorithms that scale well with size and dimension, can handle uncertainty in the data, and accommodate streams of noisy and non-uniformly sampled measurements.<br/><br/>The investigators have a long track record of collaboration with applied scientists in many disciplines, and will continue to  transfer their new research to these scientific fields through joint publications and research seminars, also in collaboration with several industrial partners. This project will heavily rely on the participation of graduate and undergraduate students. Participating undergraduates will supplement their education with directed projects, software development, and field studies. Data sets used and acquired for this project will be made available to the community through online repositories. Software developed will also be made publicly available.<br/><br/>Understanding trajectory data sets, and extracting meaningful information from them, entails many computational challenges. Part of the problem has to do with the huge scale of the available data, which is constantly growing, but there are several others as well. Trajectory data sets are marred by sensing uncertainty and heterogeneity in their quality, format, and temporal support. At the same time, individual trajectories can have complex shapes, and even small nuances can make big differences in their semantics.<br/><br/>A  major tension in understanding trajectory data is thus between the need to capture the fine details of individual trajectories and the ability to exploit the wisdom of the collection, i.e., to take advantage of the information embedded in a large collection of trajectories but missing in any individual trajectory. This emphasis on the wisdom of the collection is one of the main themes of the project, and leads to a multitude of important problems in computational geometry, combinatorial and numerical optimization, and computer vision. Another theme of  the project is to learn and exploit both continuous and discrete modes of variability in trajectory data.<br/><br/>Deterministic and probabilistic representations will be developed to summarize collections of trajectories that capture commonalities and differences between them, and efficient algorithms will be designed to compute these representations. Based on these summaries, methods will be developed to estimate a trajectory from a given collection, compare trajectories to each other in the context of a collection, and retrieve trajectories from a collection in response to a query. Trajectory collections will also be used to infer information about the environment and the mobile entities involved in these motions."
206,1514305,AF: Medium: Collaborative Research: Algorithmic Foundations for Trajectory Collection Analysis,CCF,Algorithmic Foundations,6/1/15,5/30/17,Leonidas Guibas,CA,Stanford University,Continuing grant,Joseph Maurice Rojas,5/31/19,"$400,000.00 ",,guibas@cs.stanford.edu,450 Jane Stanford Way,Stanford,CA,943052004,6507232300,CSE,7796,"7924, 7929",$0.00 ,"This project engages experts in computational geometry, optimization, and computer vision from Duke and Stanford to develop a theoretical and algorithmic framework for analyzing large collections of trajectory data from sensors or simulations. Trajectories are functions from a time interval to a multi-dimensional space that arise in the description of any system that evolves over time.<br/><br/>Trajectory data is being recorded or inferred from hundreds of millions of sensors nowadays, from traffic monitoring systems and GPS sensors on cell phones to cameras in surveillance systems or those embedded in smart phones, in helmets of soldiers in the field, or in medical devices, as well as from scientific experiments and simulations, such as molecular dynamics computations in biology. Algorithms for trajectory-data analysis can lead to video retrieval systems, activity recognition, facility monitoring and surveillance, medical investigation, traffic navigation aids, military analysis and deployment tools, entertainment, and much more. Many of these application fields intersect areas of national security, as well as domains of broader societal benefit.<br/><br/>This project pursues a transformational approach that combines the geometry of individual trajectories with the information that an entire collection of trajectories provides about its members. Emphasis is on simple and fast algorithms that scale well with size and dimension, can handle uncertainty in the data, and accommodate streams of noisy and non-uniformly sampled measurements.<br/><br/>The investigators have a long track record of collaboration with applied scientists in many disciplines, and will continue to  transfer their new research to these scientific fields through joint publications and research seminars, also in collaboration with several industrial partners. This project will heavily rely on the participation of graduate and undergraduate students. Participating undergraduates will supplement their education with directed projects, software development, and field studies. Data sets used and acquired for this project will be made available to the community through online repositories. Software developed will also be made publicly available.<br/><br/>Understanding trajectory data sets, and extracting meaningful information from them, entails many computational challenges. Part of the problem has to do with the huge scale of the available data, which is constantly growing, but there are several others as well. Trajectory data sets are marred by sensing uncertainty and heterogeneity in their quality, format, and temporal support. At the same time, individual trajectories can have complex shapes, and even small nuances can make big differences in their semantics.<br/><br/>A  major tension in understanding trajectory data is thus between the need to capture the fine details of individual trajectories and the ability to exploit the wisdom of the collection, i.e., to take advantage of the information embedded in a large collection of trajectories but missing in any individual trajectory. This emphasis on the wisdom of the collection is one of the main themes of the project, and leads to a multitude of important problems in computational geometry, combinatorial and numerical optimization, and computer vision. Another theme of  the project is to learn and exploit both continuous and discrete modes of variability in trajectory data.<br/><br/>Deterministic and probabilistic representations will be developed to summarize collections of trajectories that capture commonalities and differences between them, and efficient algorithms will be designed to compute these representations. Based on these summaries, methods will be developed to estimate a trajectory from a given collection, compare trajectories to each other in the context of a collection, and retrieve trajectories from a collection in response to a query. Trajectory collections will also be used to infer information about the environment and the mobile entities involved in these motions."
207,1544841,CPS/Synergy/Collaborative Research: Smart Calibration Through Deep Learning for High-Confidence and Interoperable Cyber-Physical Additive Manufacturing Systems,CMMI,CPS-Cyber-Physical Systems,9/1/15,8/21/15,Arman Sabbaghi,IN,Purdue University,Standard Grant,Bruce Kramer,8/31/20,"$299,952.00 ",,sabbaghi@purdue.edu,Young Hall,West Lafayette,IN,479072114,7654941055,ENG,7918,"082E, 083E, 7918, 8235",$0.00 ,"Additive Manufacturing holds the promise of revolutionizing manufacturing. One important trend is the emergence of cyber additive manufacturing communities for innovative design and fabrication. However, due to variations in materials and processes, design and computational algorithms currently have limited adaptability and scalability across different additive manufacturing systems. This award will establish the scientific foundation and engineering principles needed to achieve adaptability, extensibility, and system scalability in cyber-physical additive manufacturing systems, resulting in high efficiency and accuracy fabrication. The research will facilitate the evolution of existing isolated and loosely-connected additive manufacturing facilities into fully functioning cyber-physical additive manufacturing systems with increased capabilities. The application-based, smart interfacing infrastructure will complement existing cyber additive communities and enhance partnerships between academia, industry, and the general public. The research will contribute to the technology and engineering of Cyber-physical Systems and the economic competitiveness of US manufacturing. This interdisciplinary research will generate new curricular materials and help educate a new generation of cybermanufacturing workforce. <br/><br/>The research will establish smart and dynamic system calibration methods and algorithms through deep learning that will enable high-confidence and interoperable cyber-physical additive manufacturing systems. The dynamic calibration and re-calibration algorithms will provide a smart interfacing layer of infrastructure between design models and physical additive manufacturing systems. Specific research tasks include: (1) Establishing smart and fast calibration algorithms to make physical additive manufacturing machines adaptable to design models; (2) Deriving prescriptive compensation algorithms to achieve extensible design models; (3) Dynamic recalibration through deep learning for improved predictive modeling and compensation; and (4) Developing a smart calibration server and APP prototype test bed for scalable additive cyberinfractures."
208,1446765,CPS: Synergy: Autonomous Vision-based Construction Progress Monitoring and Activity Analysis for Building and Infrastructure Projects,CMMI,CPS-Cyber-Physical Systems,1/1/15,8/6/14,Mani Golparvar-Fard,IL,University of Illinois at Urbana-Champaign,Standard Grant,Bruce Kramer,12/31/19,"$999,935.00 ","Timothy Bretl, Derek Hoiem",mgolpar@illinois.edu,1901 South First Street,Champaign,IL,618207406,2173332187,ENG,7918,"029E, 071E, 078E, 079E, 6840, 7397, 8235",$0.00 ,"This Cyber-Physical Systems (CPS)  award supports research to enable the automated monitoring of building and infrastructure construction projects. The purpose of construction monitoring is to provide developers, contractors, subcontractors, and tradesmen with the information they need to easily and quickly make project control decisions. These decisions have a direct impact on the overall efficiency of a construction project. Given that construction is a $800 billion industry, gains in efficiency could lead to enormous cost savings, benefiting both the U.S. economy and society. In particular, both construction cost and delivery time could be significantly reduced by automated tools to assess progress towards completion (progress monitoring) and how construction resources are being utilized (activity monitoring). These tools will be provided by advances in the disciplines of computer vision, robotics, and construction management. The interdisciplinary nature of this project will create synergy among these disciplines and will positively influence engineering education. Partnerships with industry will also ensure that these advances have a positive impact on construction practice.<br/><br/>The process of construction monitoring involves data collection, analysis, and reporting. Research will address the existing scientific challenges to automating these three activities. Data collection will be automated by recording video with aerial robots and a network of cameras. Key research objectives are to derive planning algorithms that guarantee complete coverage of a construction site and to derive vision-based control algorithms that enable robust placement and retrieval of cameras. Analysis will be automated with a digital building information model with respect to which construction resources can be tracked. Key research objectives are to improve the efficiency and reliability of image-based reconstruction, to recognize material properties as well as geometry, to establish a formal language for representing construction activities, and to extend a parts-based approach for automated activity recognition. Reporting will be automated with a ubiquitous display of the digital building information model. Key research objectives are to formalize a constraint construction ontology with associated classification mechanisms and allow for systematic earned value analysis of construction progress. Experimental validation will focus on monitoring construction of substructure and superstructure skeletal elements in buildings and infrastructure systems as well as the associated earth-moving, concrete placement, and steel erection activities that are common in construction projects."
209,1507830,Extracting Knowledge from 100 years of Microstructural Images: Using Machine Vision and Machine Learning to Address the Microstructural Big Data Challenge,DMR,"CONDENSED MATTER & MAT THEORY, CDS&E",9/15/15,7/11/17,Elizabeth Holm,PA,Carnegie-Mellon University,Continuing Grant,Daryl Hess,8/31/18,"$400,000.00 ",,eaholm@andrew.cmu.edu,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,MPS,"1765, 8084","7433, 8084, 9216",$0.00 ,"The Division of Materials Research; the Civil, Mechanical, and Manufacturing Innovation Division; and the Division of Advanced Cyberinfrastructure contribute funds to this award. It supports research and education to collect, analyze, and compare data on materials from vast sources. All solid objects - from an airplane wing to a frying pan - have a microscopic structure that is usually not visible to the naked eye. This structure determines the properties of the material as a whole - whether it is strong or weak, for example. For the past century, materials scientists have studied these structures by using microscopes to take pictures (called micographs) of them. They then measure the important features seen in the microscopic images and relate those measurements to the properties of the material.<br/>Just as in personal photography, digital cameras have enabled materials scientists to take more pictures and do more with them than ever before. Moreover, older micrographs have been scanned in to digital archives. Materials scientists are now confronted with a set of images that is too large and too diverse to analyze manually. Fortunately, computer scientists have developed ""machine vision"" computer programs that identify similarities in large sets of images by in a sense mimicking how humans see objects. This project will gather micrographs from many sources into an open archive and use machine vision programs to search, sort, and classify them automatically without significant human intervention.<br/>By synthesizing microscopic image data at a previously impossible scale, this project creates a foundation for discovering new connections between microscopic structures and materials properties. The results will help improve current materials and even develop new ones. The data will be made available to the broader community.<br/><br/>The Division of Materials Research; the Civil, Mechanical, and Manufacturing Innovation Division; and the Division of Advanced Cyberinfrastructure contribute funds to this award. It supports research and education to collect, analyze, and compare data on materials from vast sources. Over the past 100 years, materials scientists have made great progress in acquiring, analyzing, and comparing microstructural images. Much of this effort has been directed toward deep understanding of particular materials systems or classes of microstructures. When the catalog of possible microstructural features is known, imaging techniques can take advantage of well-defined feature characteristics to analyze microstructures with high precision. However, when the features of interest are not known a priori, these methods become intractable, inaccurate, or fail completely. Thus, typically, they are applied only to a pre-selected set of micrographs, chosen by a human expert. In contrast, the goal of this effort is to develop a general method to find useful relationships between micrographs without assumptions about what features may be present. Such an approach can leverage the explosion in digital data over the past two decades to survey the breadth of available microstructures efficiently and without significant human intervention.<br/>Capitalizing on recent advances in computer science, this project applies a subset of data science concepts - including data harvesting, machine vision, and machine learning - to advance the science of microstructure. The result will be a framework for finding connections between microstructural images within and across material systems, which will support outcomes ranging from computational tools to discovery science, including:<br/>- New open source tools for extracting micrographs and associated metadata from various digital archives, including the internet, PDF documents, and local storage media.<br/>- A comprehensive database of publicly available micrographs with traditional text-based search and novel image-based search functions. <br/>- Optimized, high throughput, automatic machine vision techniques to identify microstructural features that are salient to image analysis and microstructural science.<br/>- Automatic and objective machine learning systems that find relationships between microstructures in order to discover new structure-property and structure-performance connections.<br/>The goal of microstructural science is to understand the connection between microstructural features and materials properties. By developing an open-access, automatic, and objective machine learning system for finding relationships between microstructural images, this project creates a foundation for discovering new connections that may inspire deeper understanding or predictive capabilities."
210,1464539,Collaborative Research:  Planning Grant: I/UCRC for Advanced Electronics through Machine Learning,IIP,IUCRC-Indust-Univ Coop Res Ctr,4/15/15,4/13/15,Madhavan Swaminathan,GA,Georgia Tech Research Corporation,Standard Grant,Thyagarajan Nandagopal,3/31/16,"$11,500.00 ",,madhavan.swaminathan@ece.gatech.edu,Office of Sponsored Programs,Atlanta,GA,303320420,4048944819,ENG,5761,5761,$0.00 ,"The accepted engineering design methodology requires that mass scale manufacturing of a new product not commence until a prototype of the product is tested and found to meet its performance specifications. It is not unusual for a product to go through multiple design iterations before it can satisfy all the design requirements. Modern electronic products, which range from a single integrated circuit to a smart phone to an aircraft instrumentation system, are so complex and contain so many components - billions in the case of an integrated circuit - that it is infeasible to construct hardware prototypes for each design iteration, from the points of view of both cost and time. Instead, a mathematical representation of the product must be developed, i.e. a virtual prototype, and its behavior then simulated. Each of the components that constitute the product would be represented by a model. Behavioral models of the components are most desirable; a behavioral model represents the terminal response of a component in response to an outside stimulus or signal, without concern to the inner workings of the component. Behavioral models are computationally efficient and have the benefit of obscuring intellectual property. However, despite many years of significant effort by the electronic design automation community, there is not a general, systematic method to generate accurate and comprehensive behavioral models, in part because of the non-linear, complex and multi-port nature of the components being modeled. The proposing team will utilize the planning grant to establish a research center that will overcome these modeling challenges through the development and application of novel machine-learning methods and algorithms.<br/><br/>Machine-learning algorithms are used to extract a model of a component or system from input-output data, despite the presence of uncertainty and noise. In this center, the input-output data are obtained either from measurements of a component or by running detailed simulations of a component. The emphasis is on models that balance good predictive ability against computational complexity. The center will pioneer the application of machine learning to electronics modeling. It will develop a methodology to use prior knowledge, i.e., physical constraints and domain knowledge provided by designers, to speed up the learning process. Novel methods of incorporating component variability, including that due to semiconductor process variations, will be developed."
211,1464544,Collaborative Research:  Planning Grant: I/UCRC for Advanced Electronics through Machine Learning,IIP,IUCRC-Indust-Univ Coop Res Ctr,4/15/15,4/13/15,Elyse Rosenbaum,IL,University of Illinois at Urbana-Champaign,Standard Grant,Thyagarajan Nandagopal,3/31/16,"$16,258.00 ",,elyse@uiuc.edu,1901 South First Street,Champaign,IL,618207406,2173332187,ENG,5761,5761,$0.00 ,"The accepted engineering design methodology requires that mass scale manufacturing of a new product not commence until a prototype of the product is tested and found to meet its performance specifications. It is not unusual for a product to go through multiple design iterations before it can satisfy all the design requirements. Modern electronic products, which range from a single integrated circuit to a smart phone to an aircraft instrumentation system, are so complex and contain so many components - billions in the case of an integrated circuit - that it is infeasible to construct hardware prototypes for each design iteration, from the points of view of both cost and time. Instead, a mathematical representation of the product must be developed, i.e. a virtual prototype, and its behavior then simulated. Each of the components that constitute the product would be represented by a model. Behavioral models of the components are most desirable; a behavioral model represents the terminal response of a component in response to an outside stimulus or signal, without concern to the inner workings of the component. Behavioral models are computationally efficient and have the benefit of obscuring intellectual property. However, despite many years of significant effort by the electronic design automation community, there is not a general, systematic method to generate accurate and comprehensive behavioral models, in part because of the non-linear, complex and multi-port nature of the components being modeled. The proposing team will utilize the planning grant to establish a research center that will overcome these modeling challenges through the development and application of novel machine-learning methods and algorithms.<br/><br/>Machine-learning algorithms are used to extract a model of a component or system from input-output data, despite the presence of uncertainty and noise. In this center, the input-output data are obtained either from measurements of a component or by running detailed simulations of a component. The emphasis is on models that balance good predictive ability against computational complexity. The center will pioneer the application of machine learning to electronics modeling. It will develop a methodology to use prior knowledge, i.e., physical constraints and domain knowledge provided by designers, to speed up the learning process. Novel methods of incorporating component variability, including that due to semiconductor process variations, will be developed."
212,1513692,III: Medium: Machine Learning with Humans in the Loop,IIS,Info Integration & Informatics,8/1/15,5/29/18,Thorsten Joachims,NY,Cornell University,Continuing Grant,Sylvia Spengler,7/31/20,"$1,016,000.00 ",Arpita Ghosh,tj@cs.cornell.edu,373 Pine Tree Road,Ithaca,NY,148502820,6072555014,CSE,7364,"7364, 7924, 9251",$0.00 ,"Machine learning is increasingly used in systems common in daily life -- from search engines to online education to smart homes. Through interactions with their users, these systems learn about the world to improve efficiency and performance. Building effective and robust Human-Interactive Learning (HIL) systems, however, requires a framework that simultaneously integrates models of human behavior with the design of machine learning algorithms, because user data is only an indirect mapping, mediated through the human decision-making process, of the knowledge the system aims to elicit.  This project takes an interdisciplinary approach to developing effective techniques to design human-interactive learning systems.<br/><br/>This project explores how humans provide data and how learning algorithms use this data in an integrated framework that encompasses three aspects. First, the project develops models of the user decision process, formalizing how observable user actions map to the underlying knowledge the system aims to acquire. Second, these models inform the design of the interface that connects the user and the learning algorithm to suitably trade off the quantity and quality of the data acquired. Third, the user model and interface motivate new machine learning settings and algorithms to maximize learning efficiency.  By developing an integrated framework for the three interconnected components for building Human Interactive Learning Systems -- human decision models, information-elicitation interfaces, and learning algorithms -- this project will impact future designs of widely-used systems such as non-web information search, recommendation, and online education."
213,1464632,Collaborative Research:  Planning Grant: I/UCRC for Advanced Electronics through Machine Learning,IIP,IUCRC-Indust-Univ Coop Res Ctr,4/15/15,4/13/15,Paul Franzon,NC,North Carolina State University,Standard Grant,Thyagarajan Nandagopal,3/31/16,"$11,499.00 ",,paulf@ncsu.edu,2701 Sullivan DR STE 240,Raleigh,NC,276950001,9195152444,ENG,5761,5761,$0.00 ,"The accepted engineering design methodology requires that mass scale manufacturing of a new product not commence until a prototype of the product is tested and found to meet its performance specifications. It is not unusual for a product to go through multiple design iterations before it can satisfy all the design requirements. Modern electronic products, which range from a single integrated circuit to a smart phone to an aircraft instrumentation system, are so complex and contain so many components - billions in the case of an integrated circuit - that it is infeasible to construct hardware prototypes for each design iteration, from the points of view of both cost and time. Instead, a mathematical representation of the product must be developed, i.e. a virtual prototype, and its behavior then simulated. Each of the components that constitute the product would be represented by a model. Behavioral models of the components are most desirable; a behavioral model represents the terminal response of a component in response to an outside stimulus or signal, without concern to the inner workings of the component. Behavioral models are computationally efficient and have the benefit of obscuring intellectual property. However, despite many years of significant effort by the electronic design automation community, there is not a general, systematic method to generate accurate and comprehensive behavioral models, in part because of the non-linear, complex and multi-port nature of the components being modeled. The proposing team will utilize the planning grant to establish a research center that will overcome these modeling challenges through the development and application of novel machine-learning methods and algorithms.<br/><br/>Machine-learning algorithms are used to extract a model of a component or system from input-output data, despite the presence of uncertainty and noise. In this center, the input-output data are obtained either from measurements of a component or by running detailed simulations of a component. The emphasis is on models that balance good predictive ability against computational complexity. The center will pioneer the application of machine learning to electronics modeling. It will develop a methodology to use prior knowledge, i.e., physical constraints and domain knowledge provided by designers, to speed up the learning process. Novel methods of incorporating component variability, including that due to semiconductor process variations, will be developed."
214,1453261,CAREER: Algorithmic Aspects of Machine Learning,CCF,Algorithmic Foundations,7/1/15,8/13/19,Ankur Moitra,MA,Massachusetts Institute of Technology,Continuing Grant,A. Funda Ergun,6/30/21,"$500,000.00 ",,moitra@MIT.EDU,77 MASSACHUSETTS AVE,Cambridge,MA,21394301,6172531000,CSE,7796,"1045, 7926",$0.00 ,"Algorithms and complexity are the theoretical foundation and backbone of machine learning. Yet over the past few decades an uncomfortable truth has set in that worst-case analysis is not the right framework to study it: every model that is interesting enough to use in practice leads to computationally hard problems. The goal of the PI's research agenda is to move beyond worst-case analysis. This involves formalizing when and why heuristics -- such as alternating minimization and Gibbs sampling -- work as well as designing fundamentally new algorithms for some of the basic tasks in machine learning. This project has already had a number of successes such as provable algorithms for nonnegative matrix factorization, topic modeling and learning mixture models.<br/><br/>The PI will investigate several new directions in topics such as sparse coding, inference in graphical models, inverse problems for tensors, and semi-random models. These projects will leverage a wide range of modern tools to give new provable algorithms in each of these settings, and will involve making new connections between alternating minimization and approximate gradient descent, analyzing Gibbs sampling through correlation decay and coupling, connecting tensor completion and quantum complexity and rethinking the standard distributional models used in machine learning. These projects cut across several areas of computer science and applied mathematics and will build new bridges between them, as well as expanding the reach of theory into a number of domains where there is a serious gap in our current understanding. Bridging theory and practice has significant broader impact.  The PI will continue to mentor undergraduate and graduate students."
215,1453304,CAREER: Machine Learning through the Lens of Economics (And Vice Versa),IIS,Robust Intelligence,2/1/15,2/14/18,Jacob Abernethy,MI,Regents of the University of Michigan - Ann Arbor,Continuing Grant,Weng-keen Wong,7/31/18,"$403,469.00 ",,prof@gatech.edu,3003 South State St. Room 1062,Ann Arbor,MI,481091274,7347636438,CSE,7495,"1045, 7495",$0.00 ,"Machine Learning (ML) is the study of leveraging data and computational resources to obtain prediction and decision-making algorithms that function well in the presence of uncertainty. The techniques employed to design and study ML algorithms typically involve concepts and tools from probability, statistics, and optimization; the language of economics, on the other hand, is conspicuously absent. It is rare to encounter terms such as marginal price, utility, equilibrium, risk aversion, and such, in the ML research literature. This gap is significant and belies the reality that the broad interest in Machine Learning, and its sudden growth spurt as a research field, can be ascribed to its potential for generating economic value across many segments of society. This NSF CAREER projectadvances an already-emerging relationship between Machine Learning and the fields of microeconomic theory and finance. This will begin with the development of mathematical tools that enable a semantic correspondence between learning-theoretic objects and economic abstractions. For example, the project shows that many algorithms can be viewed as implementing a market economy, where learning parameters are associated with prices, parameter updates are viewed as transactions, and under certain conditions learned hypotheses can be extracted as market-clearing price equilibria. In addition to developing this link, the project research raises a number of intriguing questions and explores several surprising and novel applications with benefits to computer science more broadly. <br/><br/>Among several such applications stemming from the new theoretical connections are:<br/>1. Developing new models for distributed computing for learning and estimation tasks: The economic lens gives new insights into a robust and effective model for decentralization of data-focused tasks.<br/>2. Designing new techniques for crowdsourcing and labor decentralization via collaborative mechanisms involving financial payment schemes: This builds off of the success of platforms like Amazon's Mechanical Turk as well as the Netflix Prize and the prediction challenge company Kaggle.<br/>3. Developing a market-oriented model for data brokerage and financially-efficient learning: As information is increasingly traded in market environments, we aim to answer questions such as ""what is the marginal value of a unit of data?""<br/><br/>The project will also develop the Michigan Prediction Team, a data-science focused program for formulating and solving prediction and learning challenges that develop from across the University of Michigan as well as externally. The group primarily targets undergraduates with graduate student mentors, and Team has a strong interdisciplinary focus."
216,1533983,PFI:BIC Human-Centered Smart-Integration of Mobile Imaging and Sensing Tools with Machine Learning for Ubiquitous Quantification of Waterborne and Airborne Nanoparticles,IIP,"PFI-Partnrships for Innovation, BioP-Biophotonics",10/1/15,6/15/17,Aydogan Ozcan,CA,University of California-Los Angeles,Standard Grant,Jesus Soriano Molla,9/30/19,"$1,018,160.00 ",Mihaela van der Schaar,ozcan@ee.ucla.edu,10889 Wilshire Boulevard,LOS ANGELES,CA,900951406,3107940102,ENG,"1662, 7236","005E, 116E, 1662, 9251",$0.00 ,"This Partnerships for Innovation: Building Innovation Capacity (PFI:BIC) project focuses on the creation of a human-centered smart toolset and service system for on-site and ubiquitous quantification and automated charaterization/classification of nanosize objects. Nanoparticles are being used in more and more commercial and industrial products while their health and environmental implications are still under debate. The toxicity of nanomaterials not only varies among different materials, but is also highly dependent on the dose of exposure. Developing a sensitive method to detect the release and spatio-temporal distribution of nanoparticles in the environment as well as in daily lives is a high priority before their toxicity effects are fully understood via long-term toxicological studies. Despite this urgent need for widespread detection and quantification of nanoparticle distributions, current technologies are lacking appropriate features for ubiquitous and cost-effective mapping and quantification of nanoparticle contamination. This project aims to create a transformative and human-centered toolset for on-site and ubiquitous quantification and automated characterization of nanomaterials found in houses, workplaces and the environment based on the cost-effective integration of computational imaging and mobile sensing techniques with big data based dynamic machine learning algorithms. <br/><br/>The central challenge in this project is to translate the bulky and expensive laboratory equipment currently used for nanoparticle quantification and characterization to field-portable, easy-to-use, cost-effective, and rapid analysis devices and smart service systems aiming to be massively used by consumers in their daily routines. To solve this challenge, highly sensitive optical imaging systems will be developed based on mass-produced Complementary Metal-Oxide Semiconductor (CMOS) sensor chips embedded in mobile phones with extraordinary signal to noise ratios (SNR) and large fields-of-view for high-throughput machine learning based automated nanoparticle analysis and classification. One approach this will take is to combine computational microscopy with self-assembled nanolenses around nanoparticles that significantly enhance imaging SNR and contrast. The aim of this approach is to enable automated detection and sizing of individual nanoparticles, mono-dispersed samples, and complex poly-dispersed mixtures, where the sample concentrations can span ~5 orders-of-magnitude and particle sizes can range from 40 nm to millimeter-scale, which provide unmatched performance metrics compared to existing nanoparticle sizing approaches. Another approach that will be implemented is the development of highly sensitive multi-modal (e.g. fluorescence plus dark-field) mobile phone based microscopy platforms for distributed nanoparticle imaging and sensing. Furthermore, in terms of big data analysis and machine learning tools, the techniques in this project can adaptively learn ""semantic"" similarities that can be used for more accurate data classification. These techniques are unlike existing techniques developed so far in the literature.  The extant technologies are based only on signal similarities, which do not work well on multi-modality data. The smart and adaptive methods of this project are the first in the literature that come with confidence bounds, that is, they not only have the capability to accurately classify the information, but they also provide guarantees about the accuracy of this classification, which is quite important for self-learning smart service systems. Through these field-portable devices that are integrated with adaptive big data based decision analytics and quantification algorithms, spatio-temporal maps of nanoparticle concentrations and size distributions in various consumer samples will be created for public or personal monitoring (e.g., measurements of waterborne/airborne particles at home, workplace, or airborne particles along a freeway, etc.).<br/><br/>The broader impacts of this transformative research include (1) The development of these nanoparticle sensing and quantification platforms and smart service systems will extend the boundaries of current optical metrology science, resulting in new advances in the fields of nanophotonics and optical microscopy (2) These devices will also be easy to translate into various biomedical, chemical and material science applications, significantly impacting the use and regulations of nanotechnologies in consumer market and related products. (3) This project would deliver a paradigm-shift by ubiquitous quantification and spatiotemporal mapping/monitoring of nanoparticle contamination and exposure even in non-laboratory settings, assisting in the revelation and better understanding of various cause-effect relationships at the consumer level that have remained unidentified so far due to the limitations of existing nano-imaging, detection and quantification technologies, also providing maps of potential health risks. (4) This project will also establish a complementary educational outreach program based in California.<br/><br/>The lead institution and primary partners included in this cross-organizational interdisciplinary project are: Lead Academic Institution: University of California, Los Angeles, CA, School of Engineering, Electrical and Bioengineering Departments; Primary Industrial Partner: Holomic LLC (Small Business located in Los Angeles, CA); Other Industrial Partner: Google Inc. (Large Business located in Mountain View, CA)."
217,1545481,NRT-DESE LUCID: A project-focused cross-disciplinary graduate training program for data-enabled research in human and machine learning and teaching,DGE,"RES IN DISABILITIES ED, NSF Research Traineeship (NRT)",9/1/15,8/12/19,Tim Rogers,WI,University of Wisconsin-Madison,Standard Grant,John Weishampel,8/31/21,"$3,058,515.00 ","Martina Rau, Xiaojin Zhu, Martha Alibali, Robert Nowak",ttrogers@wisc.edu,21 North Park Street,MADISON,WI,537151218,6082623822,EHR,"1545, 1997","7433, 9179, SMET",$0.00 ,"NRT DESE: Learning, understanding, cognition, intelligence, and data science (LUCID)<br/><br/>In modern life there are many situations requiring people to interact with computers, either so that they may learn from the machine or so that the machine may learn from them. The applications in education, industry, health, robotics, and national security hint at the enormous societal and economic benefits arising from research into the technologies that promote learning in both people and computers. Yet the potential has been difficult to realize because such research requires scientists with expertise in quite different fields of study. While computer scientists receive training in complex computational ideas and methods, they know little about how people learn and behave. This National Science Foundation Research Traineeship (NRT) award to the University of Wisconsin-Madison will prepare trainees with data-enabled science and engineering training to simultaneously understand computational theory and methods, the mechanisms that support human learning and behavior, and the ways these mechanisms behave in complex real-world situations. The traineeship anticipates equipping forty (40) doctoral students with the skills and expertise necessary to advance our understanding of human and machine learning and teaching, through a new training program that focuses on learning, understanding, cognition, intelligence, and data science.<br/><br/>This project will train doctoral students from computer science, engineering, cognitive psychology, and education sciences, with the goal of promoting a common knowledge base that allows these scientists to work productively across traditional boundaries on both basic research questions and practical, real-world problems. The traineeship will include several graduate training innovations: (1) a project-focused ""prof-and-peer"" mentoring system where scientists work in cross-disciplinary teams to address a shared research problem, (2) close involvement of partners in industry, government, and non-profit sectors to develop research problems with real-world application, (3) an information outreach effort that trains scientists to communicate with the public, industry, and policy-makers through traditional and new media outlets, (4) a flexible development plan that allows each trainee to garner the cross-disciplinary expertise needed to advance a particular research focus, and (5) new mechanisms for recruiting and retaining under-represented groups in STEM research. This training will prepare US scientists to compete globally at the highest levels for positions in science, industry, and government, in a growth sector of the 21st century knowledge economy. <br/><br/>The NSF Research Traineeship (NRT) Program is designed to encourage the development and implementation of bold, new, potentially transformative, and scalable models for STEM graduate education training.  The Traineeship Track is dedicated to effective training of STEM graduate students in high priority interdisciplinary research areas, through the comprehensive traineeship model that is innovative, evidence-based, and aligned with changing workforce and research needs.<br/><br/>This award is supported, in part, by the EHR Core Research (ECR) program, specifically the ECR Research in Disabilities Education (RDE) area of special interest.  ECR emphasizes fundamental STEM education research that generates foundational knowledge in the field.  Investments are made in critical areas that are essential, broad and enduring: STEM learning and STEM learning environments, broadening participation in STEM, and STEM workforce development."
218,1452851,CAREER: Situated Recognition: Learning to understand our local visual environment,IIS,Robust Intelligence,3/1/15,3/3/15,Alexander Berg,NC,University of North Carolina at Chapel Hill,Standard Grant,Jie Yang,2/29/20,"$509,965.00 ",,aberg@cs.unc.edu,104 AIRPORT DR STE 2200,CHAPEL HILL,NC,275991350,9199663411,CSE,7495,"1045, 7495",$0.00 ,"This project develops computer vision technologies for recognizing objects in our daily lives.  For recognizing visual content around us, where cameras can record multiple images over a period of time, there is an opportunity to take advantage of context that is not available for internet images.  This project pursues new representations and computational strategies exploiting this context efficiently to achieve high-quality visual recognition in our environment.  Balanced against the opportunity of using this context is the challenge of making recognition work in any particular environment, in the face of clutter, occlusion, non-canonical views, and idiosyncratic appearance variation.  The methods developed can be a core part of developing technology to help computer vision systems scale to recognize everything in our daily world. The research leads to automated systems for better understanding and monitoring of our daily environment, improved human-computer interaction, and encourages more research in this area.<br/><br/>This research direction is different from the majority of work in recognition that has focused on internet images collected from the web. The biases of such web-collected may lead to models that do not generalize to a particular environment. Situated recognition allows exploiting local context, including human interaction and spoken language, to build models specific to an environment and furthermore to the parts of an environment that are important to people. The project collects multiple datasets stressing multi-view imagery and long-term observation of environments while sampling a wide variety of settings. The research team develops algorithms to parse and detect objects by exploiting context, efficient re-use, and context-dependent saliency; and uses situated natural language to drive automatic learning of visual recognition models.<br/><br/>Project Webpage: http://acberg.com"
219,1544969,CPS: Synergy: Collaborative Research: Extracting Time-Critical Situational Awareness from Resource Constrained Networks,CNS,CPS-Cyber-Physical Systems,10/1/15,9/16/15,Amit Roy Chowdhury,CA,University of California-Riverside,Standard Grant,David Corman,9/30/20,"$576,000.00 ","Eamonn Keogh, Srikanth Krishnamurthy",amitrc@ece.ucr.edu,Research & Economic Development,RIVERSIDE,CA,925210217,9518275535,CSE,7918,"7918, 8235",$0.00 ,"The goal of this project is to facilitate timely retrieval of dynamic situational awareness information from field-deployed nodes by an operational center in resource-constrained uncertain environments, such as those encountered in disaster recovery or search and rescue missions.  This is an important cyber physical system problem with perspectives drawn at a system and platform level, as well as at the system of systems level.  Technology advances allow the deployment of field nodes capable of returning rich content (e.g., video/images) that can significantly aid rescue and recovery. However, development of techniques for acquisition, processing and extraction of the content that is relevant to the operation under resource constraints poses significant interdisciplinary challenges, which this project will address. The focus of the project will be on the fundamental science behind these tasks, facilitated by validation via both in house experimentation, and field tests orchestrated based on input from domain experts.<br/><br/>In order to realize the vision of this project, a set of algorithms and protocols will be developed to: (a) intelligently activate field sensors and acquire and process the data to extract semantically relevant information; (b) formulate expressive and effective queries that enable the near-real-time retrieval of relevant situational awareness information while adhering to resource constraints;  and, (c) impose a network structure that facilitates cost-effective query propagation and response retrieval. The research brings together multiple sub-disciplines in computing sciences including computer vision, data mining, databases and networking, and understanding the scientific principles behind information management with compromised computation/communication resources. The project will have a significant broader impact in the delivery of effective situational awareness in applications like disaster response. The recent :World Disaster Report"" states that there were more than 1 million deaths and $1.5 trillion in damage from disasters within the past decade; the research has the potential to drastically reduce these numbers.  Other possible applications are law enforcement and environmental monitoring.  The project will facilitate a strong inter-disciplinary education program and provide both undergraduate and graduate students experience with experimentation and prototype development.  There will be a strong emphasis on engaging the broader community and partnering with programs that target under-represented students and minorities."
220,1457024,Cortical Motion Coding and Gaze Control in Natural Vision,IOS,"PHYSICS OF LIVING SYSTEMS, Cross-BIO Activities, Activation",9/15/15,9/18/18,Leslie Osborne,IL,University of Chicago,Continuing Grant,Sridhar Raghavachari,3/31/19,"$897,513.00 ",,leslie.osborne@duke.edu,6054 South Drexel Avenue,Chicago,IL,606372612,7737028669,BIO,"7246, 7275, 7713","8007, 8091, 9178, 9179",$0.00 ,"The human eye sends information to the brain at an estimated rate of approximately 10 megabits per second, roughly the speed of an ethernet connection. Processing such a large bandwidth stream of visual information on behaviorally relevant time scales requires the brain to extract and represent information from visual signals efficiently, i.e. represent the most information for the least cost in time, hardware and energy. In essence, the brain needs to compress the visual stream in much the same way that software compresses the digital representation of a movie. This coding enhancement might arise because the brain has evolved coding strategies that specifically account for the fact that because of both object and eye movements, the visual input to the eye may be correlated in space and time. As a result, the visual signals to the brain from the eye and retina may be quite predictable. One of the primary questions in current sensory-motor systems research is to what extent the brain utilizes prediction to compensate for the fact that it takes a finite amount of time to process information even though the visual scene might change in the interim. This proposal focuses on neural representation of visual motion and gaze behavior for natural motion videos and uses a novel video game environment to simplify the analysis of gaze. The project will also create a publicly available database of natural gaze recordings, analyze the statistics of natural retinal image motion, characterize the representation of naturally correlated motion stimuli in cortical neurons, and to articulate the strategy underlying gaze control. This database will benefit neuroscience, computer vision, media design, and other fields.<br/><br/>The experimental approach combines cortical physiology in non-human primates with high-resolution eye movement recording in both humans and monkeys. The PI proposes to use high-resolution videos of natural moving scenes as visual stimuli while recording neural activity in motion-sensitive visual cortex.  By carefully degrading the movies to make them increasingly less natural and measuring the impact on neural responses, the experiments will determine what features of the moving visual scene are represented most precisely.  A second set of experiments will study the interactions between the visual scene and eye movements. The PI will develop an innovative Pong-like video game that actively engages the viewers and creates a common viewing purpose (scoring points) while simplifying the identification of the target of interest to aid analysis, thereby controlling the cognitive state of the viewer. The interdisciplinary nature of the work will provide training opportunities for undergraduate and graduate students crossing over from mathematics and physics to neurobiology, and for students with a biology background to gain skills in computational analysis."
221,1545071,CPS: Synergy: Collaborative Research: Extracting Time-Critical Situational Awareness from Resource Constrained Networks,CNS,CPS-Cyber-Physical Systems,10/1/15,9/16/15,Sharad Mehrotra,CA,University of California-Irvine,Standard Grant,David Corman,9/30/20,"$224,000.00 ",,sharad@ics.uci.edu,"141 Innovation Drive, Ste 250",Irvine,CA,926173213,9498247295,CSE,7918,"7918, 8235",$0.00 ,"he goal of this project is to facilitate timely retrieval of dynamic situational awareness information from field-deployed nodes by an operational center in resource-constrained uncertain environments, such as those encountered in disaster recovery or search and rescue missions. This is an important cyber physical system problem with perspectives drawn at a system and platform level, as well as at the system of systems level. Technology advances allow the deployment of field nodes capable of returning rich content (e.g., video/images) that can significantly aid rescue and recovery. However, development of techniques for acquisition, processing and extraction of the content that is relevant to the operation under resource constraints poses significant interdisciplinary challenges, which this project will address. The focus of the project will be on the fundamental science behind these tasks, facilitated by validation via both in house experimentation, and field tests orchestrated based on input from domain experts.<br/><br/>In order to realize the vision of this project, a set of algorithms and protocols will be developed to: (a) intelligently activate field sensors and acquire and process the data to extract semantically relevant information; (b) formulate expressive and effective queries that enable the near-real-time retrieval of relevant situational awareness information while adhering to resource constraints; and, (c) impose a network structure that facilitates cost-effective query propagation and response retrieval. The research brings together multiple sub-disciplines in computing sciences including computer vision, data mining, databases and networking, and understanding the scientific principles behind information management with compromised computation/communication resources. The project will have a significant broader impact in the delivery of effective situational awareness in applications like disaster response. The recent :World Disaster Report"" states that there were more than 1 million deaths and $1.5 trillion in damage from disasters within the past decade; the research has the potential to drastically reduce these numbers. Other possible applications are law enforcement and environmental monitoring. The project will facilitate a strong inter-disciplinary education program and provide both undergraduate and graduate students experience with experimentation and prototype development. There will be a strong emphasis on engaging the broader community and partnering with programs that target under-represented students and minorities."
222,1527208,NRI: Collaborative Research: Task Dependent Semantic Modeling for Robot Perception,IIS,NRI-National Robotics Initiati,9/1/15,8/12/15,Jana Kosecka,VA,George Mason University,Standard Grant,Jie Yang,8/31/19,"$267,486.00 ",,kosecka@gmu.edu,4400 UNIVERSITY DR,FAIRFAX,VA,220304422,7039932295,CSE,8013,8086,$0.00 ,"The research in this project enables robots to better deal with the complex cluttered environments around us, ranging from open scenes to cluttered table-top settings and to perform the basic mapping, navigation, object search so as to enable fetch and delivery tasks most commonly required in service co-robotics applications.  The key contribution of the project is to develop visual perception systems for robots that can understand the semantic labels of the visual world at multiple levels of specificity as required by particular robot tasks or human-robot interaction.  In addition, the project enables robot perception systems to better understand new, previously unseen, environments through automatically adapting existing learned models, and by actively choosing how to best explore and recognize novel visual spaces and objects.  The datasets and benchmarks, as well as the developed models, form basis for more rapid progress on semantic visual perception for robotics.<br/><br/>The development of methodologies for learning compositional representations which enable active learning and efficient inference is a long standing problem in computer vision and robot perception. Guided by the constraints of indoors and outdoors environments, we plan to exploit large amounts of data, strong geometric and semantic priors and develop novel representations of objects and scenes. The developed representations are captured by compositional structured probabilistic models including deep convolutional networks. Doing this rapidly is required to support active visual exploration to improve semantic parsing of a space.  Furthermore the project team collects and disseminates a large dataset of densely sampled RGBD imagery to support offline evaluation and benchmarking of active vision for semantic parsing.  The project can result in advances in active hierarchical semantic vision for robot tasks including exploration, search, manipulation, programming by example, and generally for human-robot interaction."
223,1451604,"INT: Project Learning with Automated, Networked Supports (PLANS)",IIS,"Discovery Research K-12, Cyberlearn & Future Learn Tech",5/1/15,10/24/17,Marcia Linn,CA,University of California-Berkeley,Standard Grant,Amy Baylor,4/30/20,"$2,290,222.00 ","Michael Heilman, Nitin Madnani, Elizabeth Gerard",mclinn@berkeley.edu,Sponsored Projects Office,BERKELEY,CA,947101749,5106433891,CSE,"7645, 8020","7645, 8045, 8233",$0.00 ,"Project Learning with Automated, Networked Supports (PLANS) will contribute to science and engineering education in middle schools.  This software technology will support deep learning as students carry out project-based work.  Specifically, PLANS will connect tools for brainstorming, planning, modeling, sketching, data gathering, graphing, designing, and testing, with technology that automatically assesses use of the tools and generates guidance that supports learning.  This project is funded by the Cyberlearning and Future Learning Technologies program, which integrates opportunities offered by emerging technologies with advances in what is known about how people learn.<br/><br/>PLANS will define a genre of technology-enabled learning resources that combine investigation tools within a sequence of student-led projects, and analytics technologies that guide students toward deeper understanding. The researchers will explore an instance of this genre applied to middle school physical science with a focus on energy content, science and engineering practices, and cross cutting themes. By implementing and researching a half-semester sequence of projects, the researchers will examine synergies between investigation and analytic technologies that result in substantial learning gains. The researchers will collaborate with five middle schools serving diverse student populations. Through three iterations of improvements in the technologies and project materials, the project will optimize and document the educational value of this approach."
224,1532591,NCS-FO: Algorithmically explicit neural representation of visual memorability,ECCS,"Engineering of Biomed Systems, IntgStrat Undst Neurl&Cogn Sys",8/1/15,10/5/15,Antonio Torralba,MA,Massachusetts Institute of Technology,Standard Grant,Shubhra Gangopadhyay,7/31/19,"$805,000.00 ","Antonio Torralba, Dimitrios Pantazis",torralba@csail.mit.edu,77 MASSACHUSETTS AVE,Cambridge,MA,21394301,6172531000,ENG,"5345, 8624","004E, 8089, 8091, 8551",$0.00 ,"As Lewis Carroll famously wrote in Alice in Wonderland - It's a poor sort of memory that only works backwards-. On this side of the mirror, we cannot remember visual events before they happen; however, our work will help predict what people remember, as they see an image or an event. Our team of investigators in cognitive science, human neuroscience and computer vision bring the synergetic expertise to determine how visual memories are encoded in the human brain at milliseconds and millimeters-resolution. Cognitive-level algorithms of memory would be a game changer for society, ranging from accurate diagnostic tools to human-computer interfaces that will foresee the needs of humans and compensate when cognition fails. <br/> <br/> <br/>The project capitalizes on the spatiotemporal dynamics of encoding memories while providing a computational framework for determining the representations formed from perception to memory along the scale of the whole human brain. A fundamental function of cognition is the encoding of information, a dynamic and complex process underlying much of our successful interaction with the external environment. Here, we propose to combine three technologies to predict what makes an image memorable or forgettable: neuro-imaging technologies recording where encoding happens in the human brain (spatial scale), when it happens (temporal scale), and what types of computation are performed at the different stages of storage (computational scale). Characterizing the spatiotemporal dynamics of visual memorability, and determining the type of computation and representation a successful memorability system performs is a crucial endeavor for both basic and applied sciences."
225,1513126,"II-New: A Research Platform for Heterogeneous, Massively Parallel Computing",CNS,CCRI-CISE Cmnty Rsrch Infrstrc,7/1/15,6/22/15,Yicheng Tu,FL,University of South Florida,Standard Grant,Aidong Zhang,6/30/19,"$679,798.00 ","Sudeep Sarkar, Jay Ligatti, Sagar Pandit, Swaroop Ghosh",tuy@mail.usf.edu,4019 E. Fowler Avenue,Tampa,FL,336172008,8139742897,CSE,7359,7359,$0.00 ,"The world of computing has entered the multi-core age. In addition to multi-core CPUs, co-processors containing thousands of computing cores in a single chip have become popular platforms for general-purpose computing. With the aggregated computing capabilities increasing at a steep rate, computing communities are still in an early stage in developing software systems, frameworks and applications to take full advantage of these new platforms. The co-existence of several different multi-core systems, including the Graphics Processing Units (GPUs), Intel?s Many Integrated Core (MIC) cards, and Accelerated Processing Units (APUs), further complicates the issue. This, on the other hand, provides opportunities for interesting research that spans different layers of the software stack.  This infrastructure will support multiple, coordinated research projects that will develop frameworks and software systems enabling a new class of applications requiring high-performance computing capabilities.<br/><br/>The main goal of this project is to build a computer cluster with heterogeneous, massive parallel computing capabilities to accelerate existing research and enable ground-breaking new research that shares the same need for intensive computation at the University of South Florida (USF). This project brings together eight USF investigators with research projects in several core disciplines of computer science and engineering: big data management, scientific computing, system security, hardware design, data mining, computer vision and pattern recognition. Specifically, the requested cluster supports research in: <br/>(1) design and optimization of a novel data stream management system architecture in a heterogeneous many-core hardware environment; <br/>(2) coarse-grained molecular simulation approach that allows accurate simulation of large-scale atomistic systems; <br/>(3) new system to deploy security policies that excel in both policy composition and runtime performance; <br/>(4) efficient modeling and design of energy-efficient and secure hardware systems; <br/>(5) automated interpretation of activities using pattern theory; <br/>(6) fast large scale clustering; and <br/>(7) pattern identification from biomedical image data.<br/>The intellectual merit of this project derives from the innovations of the individual projects and from the potential cross-disciplinary ideas it can germinate in the future. The infrastructure is expected to facilitate collaboration and cross-pollination of algorithms, models, representations, and data sets across individual project areas, building a collaborative network across the investigators.  Furthermore, the cluster is expected to impact over a dozen application domains via on-going and planned research projects among the investigators and their collaborators throughout the USF system.<br/><br/>Direct benefits to education and research will also be extended to the larger community through the applied aspects of projects, teaching and training. Project results and media content of the cluster will be showcased in the popular USF Engineering EXPO event, which seeks to educate and motivate K-12 students on math, science, engineering, and technology subjects."
226,1527181,"RI: Small: Time Resolved Imaging: New Methods for Capture, Analysis and Applications",IIS,Robust Intelligence,9/1/15,12/13/19,Ramesh Raskar,MA,Massachusetts Institute of Technology,Continuing Grant,Jie Yang,8/31/20,"$460,000.00 ",,raskar@media.mit.edu,77 MASSACHUSETTS AVE,Cambridge,MA,21394301,6172531000,CSE,7495,"7495, 7923",$0.00 ,"This project fundamentally combines the emerging time of flight imaging techniques with computational methods to redefine a camera and also go beyond the conventional barriers in scientific imaging.  Imaging has transformed science and technology in many fields.  Time-aware ultrafast imaging can bring further radical new innovations in coming years.  Recently, there has been a significant commercial interest in converting time-aware sensors into low cost consumer solutions.  Going forward, solving time-based forward and inverse transport problems can impact new fundamental research in biology, physics, optics, computer science, engineering, and mathematics, with broad applications in health, robotics, defense, and mobility. They have high potential to stimulate economic investment and entrepreneurship using modern imaging solutions. <br/><br/>Emerging image sensors with picosecond (ps) time resolution provide new ways to capture and understand the world. For scene analysis, typical computational imaging techniques exploit sensor parameters such as spatial resolution, wavelength, and polarization.  However, they are far slower than light speed and are consequently limited in their ability to model the complex dynamics of light propagation.  Time-resolved (or transient) sensors overcome this limitation, but their integration with computational methods has not been realized yet.  Therefore, with the recent spurt in commercial time-of-flight (ToF) systems, new research in transient computational imaging is well-timed. Beyond ToF depth information, this research explores the capture and analysis of per-pixel time profiles at ps scales. This leads to joint re-examination of fundamental inverse problems and solutions in scientific, industrial and consumer applications. Specifically the project builds computer vision algorithms for seeing objects beyond the line of sight, behind diffusive layers and inside turbid media. This provides novel applications in medical imaging. With the development of the theoretical foundation and enabling tools, the project accelerates research and commercialization of this new field."
227,1522654,Acceleration Techniques for Lower-Order Algorithms in Nonlinear Optimization,DMS,COMPUTATIONAL MATHEMATICS,8/1/15,7/28/15,Hongchao Zhang,LA,Louisiana State University,Standard Grant,Leland Jameson,7/31/19,"$177,771.00 ",,hozhang@math.lsu.edu,202 Himes Hall,Baton Rouge,LA,708032701,2255782760,MPS,1271,"9150, 9263",$0.00 ,"This project focuses on developing efficient innovative acceleration techniques and their underlying theories for the algorithms in nonlinear optimization. The acceleration techniques and algorithms developed in this project will have broad impact in many areas of computational science, including imaging/signal processing, optimal control, computer vision, petroleum engineering, topology optimization, and electronic structure computations. The algorithms developed in this research will be made publicly available on the web and will be applied in solving various computational problems. In addition, the student involved in this project will have excellent opportunities to participate in interdisciplinary research.<br/><br/>The research will include developing subspace techniques for nonlinear conjugate gradient method and accelerated nonlinear conjugate gradient methods with theoretically guaranteed optimal global complexity. A framework of inexact alternating direction method of multipliers (ADMM) will also be developed, in which multiple steps are allowed to solve the subproblem to an adaptive accuracy, while still maintaining global convergence even when the problem has more than two blocks. The project will also study acceleration strategies for gradient based stochastic optimization. In particular, adaptive strategies for choosing sample points and extracting quasi-Newton information based on the obtained stochastic information will be explored. In addition, a novel dual active set approach will be developed for solving smooth large-scale nonlinear optimization. For example, in projection on polyhedra, an algorithm can be developed to approximately identify the active linear constraints, while an asymptotically faster algorithm can be used to compute a high accuracy solution."
228,1514268,RI: Medium: Deep Understanding: Integrating Neural and Symbolic Models of Meaning,IIS,Robust Intelligence,6/1/15,6/14/17,Daniel Jurafsky,CA,Stanford University,Continuing grant,Tatiana Korelsky,5/31/19,"$1,100,000.00 ","Christopher Manning, Percy Liang",jurafsky@stanford.edu,450 Jane Stanford Way,Stanford,CA,943052004,6507232300,CSE,7495,"7495, 7924",$0.00 ,"Natural language understanding, automatically computing the meaning of text, is key for allowing citizens to deal intelligently with the vast amount of digital information surrounding us, from the fine print on credit cards to science textbook chapters or online instructional material. The goal of this project is to develop systems that can build richer understandings of text than current systems. Humans have an incredible ability to integrate the structure of  meaning --- how the meanings of sentences can be built up from the meanings of words --- with statistical knowledge about how words occur together with other words. Humans also effortlessly integrate meaning with 'reference', knowing which people or events in the world the text is talking about. But these tasks are quite difficult for computational systems. This project builds new computational models that integrate deep neural networks --- computational models with great power for representing word meaning in a statistical way --- with computational methods from logic and semantics. These new models allow word meanings to be combined together to build sentence meanings and also allow meanings to be linked with entities and events in the world. The resulting representations should help enable such societally important language understanding applications like question answering or tutorial software.<br/><br/>This project develops compositional forms of deep learning that bridge between lexical and compositional semantics. This includes new kinds of embeddings that can be used to perform better meaning composition, computing for example that a student with a plaster cast is similar to an injured person just as earlier embeddings computed that injured is similar to hurt, and  extending the virtues (such as lexical coverage) of embeddings to represent the denotations of logical predicates. Another focus is  enriching models of meaning with models of reference, building entity-based models that can resolve coreference in texts to handle problems like bridging anaphora or verb and event coreference, with algorithms for entity-based coreference based on tensors that capture similarity of reference rather than similarity of lexical meaning.  And it includes developing vector space lexicons that represent both natural language dependency tree fragments and logical fragments in a shared vector space, and representing meaning as general programs that can model the effects of events and processes on resources in the world.  The new models are brought to bear on the end-to-end task of learning semantic parsers that map text to a semantic denotation."
229,1540890,"International Symposium on Computational Geometry (SOCG) 2015, Eindhoven, The Netherlands, June 22-25, 2015",CCF,"INFORMATION TECHNOLOGY RESEARC, ALGORITHMIC FOUNDATIONS",6/1/15,5/1/15,Joseph S. Mitchell,NY,SUNY at Stony Brook,Standard Grant,jack snoeyink,12/31/16,"$15,000.00 ",,joseph.mitchell@stonybrook.edu,WEST 5510 FRK MEL LIB,Stony Brook,NY,117940001,6316329949,CSE,"1640, 7796","7556, 7929",$0.00 ,"This award supports participation in the 31st International Symposium on Computational Geometry, Jun 22-25, 2015.  SOCG is a focused conference that considers how to develop algorithms to solve problems best stated in geometric form.   Research presented at previous SOCG conferences has had significant impact on the US economy in diverse areas that include algorithms for geometric data processing, computer vision, geographic information systems, manufacturing, graphics and visualization, optimization, computer-aided design, and networks.<br/><br/>This award aims to increase the impact of this conference on students, particularly those<br/>from under-represented groups, by encouraging and enabling their participation, especially in cases<br/>where travel expenses would otherwise preclude their attendance. Participation in a top-level event<br/>such as SOCG can be educating, motivating, and useful for networking, both with other students<br/>and with more senior scientists. This support dovetails nicely with the Young Researchers<br/>Forum, a successful satellite event added to the conference in 2012 to encourage and promote<br/>computational geometry research by students and postdocs."
230,1526367,NRI: Collaborative Research: Task Dependent Semantic Modeling for Robot Perception,IIS,NRI-National Robotics Initiati,9/1/15,8/12/15,Alexander Berg,NC,University of North Carolina at Chapel Hill,Standard Grant,Jie Yang,8/31/20,"$269,480.00 ",,aberg@cs.unc.edu,104 AIRPORT DR STE 2200,CHAPEL HILL,NC,275991350,9199663411,CSE,8013,8086,$0.00 ,"The research in this project enables robots to better deal with the complex cluttered environments around us, ranging from open scenes to cluttered table-top settings and to perform the basic mapping, navigation, object search so as to enable fetch and delivery tasks most commonly required in service co-robotics applications.  The key contribution of the project is to develop visual perception systems for robots that can understand the semantic labels of the visual world at multiple levels of specificity as required by particular robot tasks or human-robot interaction.  In addition, the project enables robot perception systems to better understand new, previously unseen, environments through automatically adapting existing learned models, and by actively choosing how to best explore and recognize novel visual spaces and objects.  The datasets and benchmarks, as well as the developed models, form basis for more rapid progress on semantic visual perception for robotics.<br/><br/>The development of methodologies for learning compositional representations which enable active learning and efficient inference is a long standing problem in computer vision and robot perception. Guided by the constraints of indoors and outdoors environments, we plan to exploit large amounts of data, strong geometric and semantic priors and develop novel representations of objects and scenes. The developed representations are captured by compositional structured probabilistic models including deep convolutional networks. Doing this rapidly is required to support active visual exploration to improve semantic parsing of a space.  Furthermore the project team collects and disseminates a large dataset of densely sampled RGBD imagery to support offline evaluation and benchmarking of active vision for semantic parsing.  The project can result in advances in active hierarchical semantic vision for robot tasks including exploration, search, manipulation, programming by example, and generally for human-robot interaction."
231,1544741,CPS: Synergy: Collaborative Research: Learning control sharing strategies for assistive cyber-physical systems,CNS,CPS-Cyber-Physical Systems,10/1/15,3/17/20,Brenna Argall,IL,Rehabilitation Institute of Chicago,Standard Grant,Wendy Nilsen,9/30/20,"$363,937.00 ",,brenna.argall@northwestern.edu,355 East Erie Street,Chicago,IL,606113167,3122385195,CSE,7918,"7918, 8235",$0.00 ,"CPS: Synergy: Collaborative Research: Learning control sharing strategies for assistive cyber-physical systems<br/><br/>Assistive machines - like powered wheelchairs, myoelectric prostheses and robotic arms - promote independence and ability in those with severe motor impairments. As the state- of-the-art in these assistive Cyber-Physical Systems (CPSs) advances, more dexterous and capable machines hold the promise to revolutionize ways in which those with motor impairments can interact within society and with their loved ones, and to care for themselves with independence. However, as these machines become more capable, they often also become more complex. Which raises the question: how to control this added complexity? A new paradigm is proposed for controlling complex assistive Cyber-Physical Systems (CPSs), like robotic arms mounted on wheelchairs, via simple low-dimensional control interfaces that are accessible to persons with severe motor impairments, like 2-D joysticks or 1-D Sip-N-Puff interfaces. Traditional interfaces cover only a portion of the control space, and during teleoperation it is necessary to switch between different control modes to access the full control space. Robotics automation may be leveraged to anticipate when to switch between different control modes. This approach is a departure from the majority of control sharing approaches within assistive domains, which either partition the control space and allocate different portions to the robot and human, or augment the human's control signals to bridge the dimensionality gap. How to best share control within assistive domains remains an open question, and an appealing characteristic of this approach is that the user is kept maximally in control since their signals are not altered or augmented. The public health impact is significant, by increasing the independence of those with severe motor impairments and/or paralysis. Multiple efforts will facilitate large-scale deployment of our results, including a collaboration with Kinova, a manufacturer of assistive robotic arms, and a partnership with Rehabilitation Institute of Chicago. <br/><br/>The proposal introduces a formalism for assistive mode-switching that is grounded in hybrid dynamical systems theory, and aims to ease the burden of teleoperating high-dimensional assistive robots. By modeling this CPS as a hybrid dynamical system, assistance can be modeled as optimization over a desired cost function. The system's uncertainty over the user's goals can be modeled via a Partially Observable Markov Decision Processes. This model provides the natural scaffolding for learning user preferences. Through user studies, this project aims to address the following research questions: (Q1)  Expense: How expensive is mode-switching? (Q2)  Customization Need: Do we need to learn mode-switching from specific users? (Q3)  Learning Assistance: How can we learn mode-switching paradigms from a user? (Q4)  Goal Uncertainty: How should the assistance act under goal uncertainty? How will users respond? The proposal leverages the teams shared expertise in manipulation, algorithm development, and deploying real-world robotic systems. The proposal also leverages the teams complementary strengths on deploying advanced manipulation platforms, robotic motion planning and manipulation, and human-robot comanipulation, and on robot learning from human demonstration, control policy adaptation, and human rehabilitation. The proposed work targets the easier operation of robotic arms by severely paralyzed users. The need to control many degrees of freedom (DoF) gives rise to mode-switching during teleoperation. The switching itself can be cumbersome even with 2- and 3-axis joysticks, and becomes prohibitively so with more limited (1-D) interfaces. Easing the operation of switching not only lowers this burden on those already able to operate robotic arms, but may open use to populations to whom assistive robotic arms are currently inaccessible. This work is clearly synergistic: at the intersection of robotic manipulation, human rehabilitation, control theory, machine learning, human-robot interaction and clinical studies. The project addresses the science of CPS by developing new models of the interaction dynamics between the system and the user, the technology of CPS by developing new interfaces and interaction modalities with strong theoretical foundations, and the engineering of CPS by deploying our algorithms on real robot hardware and extensive studies with able-bodied and users with sprinal cord injuries."
232,1528159,"CIF: Small: Collaborative Research: Inference of Information Measures on Large Alphabets: Fundamental Limits, Fast Algorithms, and Applications",CCF,Comm & Information Foundations,9/1/15,6/24/15,Itschak(Tsachy) Weissman,CA,Stanford University,Standard Grant,Phillip Regalia,8/31/18,"$250,000.00 ",,tsachy@stanford.edu,450 Jane Stanford Way,Stanford,CA,943052004,6507232300,CSE,7797,"7923, 7935",$0.00 ,"A key task in information theory is to characterize fundamental performance limits in compression, communication, and more general operational problems involving the storage, transmission and processing of information. Such characterizations are usually in terms of information measures, among the most fundamental of which are the Shannon entropy and the mutual information. In addition to their prominent operational roles in the traditional realms of information theory, information measures have found numerous applications in many statistical modeling and machine learning tasks. Various modern data-analytic applications deal with data sets naturally viewed as samples from a probability distribution over a large domain. Due to the typically large alphabet size and resource constraints, the practitioner contends with the difficulty of undersampling in applications ranging from corpus linguistics to neuroscience. One of the main goals of this project is the development of a general theory based on a new set of mathematical tools that will facilitate the construction and analysis of optimal estimation of information measures on large alphabets. The other major facet of this project is the incorporation of the new theoretical methodologies into machine learning algorithms, thereby significantly impacting current real-world learning practices.  Successful completion of this project will result in enabling technologies and practical schemes - in applications ranging from analysis of neural response data to learning graphical models - that are provably much closer to attaining the fundamental performance limits than existing ones. The findings of this project will enrich existing big data-analytic curricula.  A new course dedicated to high-dimensional statistical inference that addresses estimation for large-alphabet data in depth will be created and offered. Workshops on the themes and findings of this project will be organized and held at Stanford and UIUC. <br/><br/>A comprehensive approximation-theoretic approach to estimating functionals of distributions on large alphabets will be developed via computationally efficient procedures based on best polynomial approximation, with provable essential optimality guarantees. Rooted in the high-dimensional statistics literature, our key observation is that while estimating the distribution itself requires the sample size to scale linearly with the alphabet size, it is possible to accurately estimate functionals of the distribution, such as entropy or mutual information, with sub-linear sample complexity. This requires going beyond the conventional wisdom by developing more sophisticated approaches than maximal likelihood (?plug-in?) estimation. The other major facet of this project is translating the new theoretical methodologies into highly scalable and efficient machine learning algorithms, thereby significantly impacting current real-world learning practices and significantly boosting the performance in several of the most prevalent machine learning applications, such as learning graphical models, that rely on mutual information estimation."
233,1523815,RI: Small: Efficient Projection-Free Algorithms for Optimization and Online Machine Learning,IIS,Robust Intelligence,9/1/15,8/5/15,Elad Hazan,NJ,Princeton University,Standard Grant,Weng-keen Wong,8/31/18,"$500,000.00 ",,ehazan@cs.princeton.edu,Off. of Research & Proj. Admin.,Princeton,NJ,85442020,6092583090,CSE,7495,"7495, 7923",$0.00 ,"The advent of the Internet gives rise to an exponential growth in data collection, availability and complexity. With it increases our need for more efficient data analysis algorithms. Over super-scale datasets, the only feasible data analysis techniques are iterative linear-time first-order optimization methods. <br/><br/>The computational bottleneck in applying these state-of-the-art iterative methods to machine learning and data analysis is often the so-called ""projection step"". This project addresses the need to design projection-free optimization algorithms that replace projections by more efficient linear optimization steps. A key contribution of the project is the continual dissemination and transfer of this technology. The open-source software releases will continue to enable large-scale machine learning applications in science and engineering. The broader impact goals of the project, beyond theory and algorithms, include the development of a textbook on efficient optimization techniques in machine learning, as well as the development of a new curriculum focused on preparing students for the scientific and engineering needs in this field."
234,1527105,"CIF: Small: Collaborative Research: Inference of Information Measures on Large Alphabets: Fundamental Limits, Fast Algorithims, and Applications",CCF,COMM & INFORMATION FOUNDATIONS,9/1/15,6/24/15,Yihong Wu,IL,University of Illinois at Urbana-Champaign,Standard Grant,Richard Brown,9/30/17,"$250,000.00 ",,yihong.wu@yale.edu,1901 South First Street,Champaign,IL,618207406,2173332187,CSE,7797,"7923, 7935",$0.00 ,"A key task in information theory is to characterize fundamental performance limits in compression, communication, and more general operational problems involving the storage, transmission and processing of information. Such characterizations are usually in terms of information measures, among the most fundamental of which are the Shannon entropy and the mutual information. In addition to their prominent operational roles in the traditional realms of information theory, information measures have found numerous applications in many statistical modeling and machine learning tasks. Various modern data-analytic applications deal with data sets naturally viewed as samples from a probability distribution over a large domain. Due to the typically large alphabet size and resource constraints, the practitioner contends with the difficulty of undersampling in applications ranging from corpus linguistics to neuroscience. One of the main goals of this project is the development of a general theory based on a new set of mathematical tools that will facilitate the construction and analysis of optimal estimation of information measures on large alphabets. The other major facet of this project is the incorporation of the new theoretical methodologies into machine learning algorithms, thereby significantly impacting current real-world learning practices.  Successful completion of this project will result in enabling technologies and practical schemes - in applications ranging from analysis of neural response data to learning graphical models - that are provably much closer to attaining the fundamental performance limits than existing ones. The findings of this project will enrich existing big data-analytic curricula.  A new course dedicated to high-dimensional statistical inference that addresses estimation for large-alphabet data in depth will be created and offered. Workshops on the themes and findings of this project will be organized and held at Stanford and UIUC. <br/><br/>A comprehensive approximation-theoretic approach to estimating functionals of distributions on large alphabets will be developed via computationally efficient procedures based on best polynomial approximation, with provable essential optimality guarantees. Rooted in the high-dimensional statistics literature, our key observation is that while estimating the distribution itself requires the sample size to scale linearly with the alphabet size, it is possible to accurately estimate functionals of the distribution, such as entropy or mutual information, with sub-linear sample complexity. This requires going beyond the conventional wisdom by developing more sophisticated approaches than maximal likelihood (?plug-in?) estimation. The other major facet of this project is translating the new theoretical methodologies into highly scalable and efficient machine learning algorithms, thereby significantly impacting current real-world learning practices and significantly boosting the performance in several of the most prevalent machine learning applications, such as learning graphical models, that rely on mutual information estimation."
235,1449815,NRT-DESE: Flexibility in Language Processes and Technology: Human- and Global-Scale,DGE,"Linguistics, NSF Research Traineeship (NRT)",4/1/15,8/16/18,Colin Phillips,MD,University of Maryland College Park,Standard Grant,John Weishampel,3/31/21,"$3,117,101.00 ","William Idsardi, Hal Daume, Robert DeKeyser, Rochelle Newman",colin@umd.edu,3112 LEE BLDG 7809 Regents Drive,COLLEGE PARK,MD,207425141,3014056269,EHR,"1311, 1997","1311, 7433, 9179, OTHR, SMET",$0.00 ,"Language learning, in humans and machines, has far-reaching relevance to global technology, commerce, education, health, and national security.  This National Science Foundation Research Traineeship (NRT) award prepares doctoral students at the University of Maryland, College Park with tools to advance language technology and language learning.  The program provides trainees with an interdisciplinary understanding of learning models from cross-training in linguistics, computer science, and psychological and neural sciences, and with the tools to work with multi-scale language data.  The training program contributes to the public understanding of science through a policy internship program that engages trainees with federal agencies and Washington-area professional organizations.  Moreover, by contributing to the development of a free public digital linguistic tool, Langscape, it will provide a valuable resource for researchers, the public, the government, and nongovernmental agencies to discover geographical and linguistic information about languages of the world. <br/><br/>Flexible and efficient language learning, in humans and machines, is the research focus of this NRT program. The research hypothesis is that improvements in learning in machines and in humans will come from the ability to use training data more efficiently at multiple scales. Through interdisciplinary team approaches, trainees will explore efficient use of language data, with a focus on the informativity of data to human and machine learning.  Through a suite of training activities that includes intensive summer research workshops, engagement with undergraduates and K-12 schools, and policy internships, trainees will become flexible communicators in writing and speaking and also learn to apply their research to diverse contexts."
236,1526379,RI: Small: Robust Optimization of Loss Functions with Application to Active Learning,IIS,Robust Intelligence,9/1/15,8/7/15,Brian Ziebart,IL,University of Illinois at Chicago,Standard Grant,Weng-keen Wong,8/31/18,"$500,000.00 ",Lev Reyzin,bziebart@uic.edu,809 S. Marshfield Avenue,Chicago,IL,606124305,3129962862,CSE,7495,"7495, 7923",$0.00 ,"The goal of this project is to develop machine learning techniques that produce better predictions in a broad range of application domains where the usefulness of predictions is measured by application-specific performance measures. Existing machine learning methods are frequently forced to approximate these performance measures so that the search for a good predictor using the approximated measure will be efficient. This can produce inappropriate predictions for data in which the approximation to the performance measure is loose, even for the most fundamental performance measure: accuracy. The approach of this project instead approximates the training data and optimizes the exact performance measure to obtain a good predictor. Approximation takes the form of an ""adversary"" in a zero-sum game that chooses how the predicted variables are distributed for evaluation in a way that minimizes performance, but also matches properties of the dataset that are measured from training data. Many performance measures that are intractable to directly optimize become tractable when adversarially optimized.  Resulting predictors are designed for the worst case and must perform at least as well when the adversary is replaced by real data with high probability.<br/><br/>The societal impact of better aligning machine learning methods to a significantly wider range of performance measures is substantial.  All classification and regression tasks that are currently solved using methods that approximate the desired performance measure, such as support vector machines or AdaBoost, could potentially be improved by the proposed approach.  The project specifically investigates cost-sensitive classification, in which different mistakes incur penalties that are based on the implications of the prediction on real-world applications, F-measure maximization, which is a preferred performance measure balancing precision and recall in information retrieval tasks, and active learning, where the approach produces predictions that are robust to sample selection bias.  Additional broader impacts of this project include developing new curriculum that will enable a wide range of data-driven practitioners to apply these improved methods to important application areas, including public policy, medical decision support, and epidemiology. Further, the PIs are committed to advising students from underrepresented groups at the University of Illinois at Chicago, which is an urban school with a diverse student population."
237,1453658,CAREER: Optimizing Learning Models for Interpretation of Heterogeneous Biological Data,IIS,Info Integration & Informatics,2/1/15,2/8/18,Tomasz Arodz,VA,Virginia Commonwealth University,Continuing Grant,Sylvia Spengler,1/31/21,"$435,653.00 ",,tarodz@vcu.edu,P.O. Box 980568,RICHMOND,VA,232980568,8048286772,CSE,7364,"1045, 7364",$0.00 ,"The gap between techniques for rapid gathering of data and the ability to navigate the resulting large, heterogeneous datasets and formulate novel hypotheses is widening. In molecular biology, this limits basic discoveries and slows the translation of results from laboratory to clinic. Bridging this gap will necessarily involve intelligent algorithms. Yet, despite significant advances, machine-learning algorithms struggle with taking the calculated risk that underpins promising hypotheses. A machine can generate de novo a number of hypotheses that match experimental data, but are they plausible given our understanding of biology? An algorithm can use the data to score which of the known biological pathways is dysregulated in the disease, but can it propose a novel explanation of the observations? This project will address the limitation of current methods through novel algorithms that will be able to integrate existing knowledge into data-driven analysis and modeling. Following rigorous validation involving real and synthetic datasets, the proposed algorithms will form a transformative computational tool set for biologists, greatly enhancing the capabilities for understanding normal and pathological behavior of molecular systems involving heterogeneous elements interacting in complex ways. The work on the algorithms will involve undergraduate and graduate students. Through participation in multidisciplinary research, they will learn how to overcome barriers in cross-discipline communication, which is crucial in the world where computer methods permeate many other areas. Students will also benefit from a new course on Graph Theory and Machine Learning. The PI will make the learning modules from the course available online to instructors at other institutions. Also, annually, the PI will organize a high-school programming contest that will serve as a hands-on demonstration of how algorithms can help solve societal and scientific problems. All activities at high-school, undergraduate and graduate levels will have emphasis on supporting women and other underrepresented groups in their exploration of computer science.<br/><br/>The project's main objective of creating algorithms for training classifiers that are accurate and interpretable will be achieved through graph-regularized machine learning that ties together ensembles, submodular set functions, and techniques for non-smooth function optimization. Submodular regularizers have recently received attention as a powerful way for equipping linear models with information about structures in the feature space, but extending the approach to non-linear classifiers is hampered by the nonlinearity of the relationship between measured features and the predicted outcome. This research will move past this obstacle by designing innovative ensemble-based methods that incorporate submodular regularizers, thus leading to improved accuracy, reduced overtraining and increased interpretability of models. These novel methods will be applicable to any classification problem that involves features connected by a network. Such problems are not confined to biology; the methods will be of broader use, for example to image analysis or text categorization. For biological applications, which will be the main focus of the work, a unified meta-network that links molecular elements at genetic, epigenetic, transcriptomic, proteomic and metabolomics levels will be constructed, and a new approach for dealing with missing data will be designed using concepts from spectral graph theory."
238,1544797,CPS: Synergy: Collaborative Research: Learning control sharing strategies for assistive cyber-physical systems,CNS,CPS-Cyber-Physical Systems,10/1/15,9/14/15,Siddhartha Srinivasa,PA,Carnegie-Mellon University,Standard Grant,Wendy Nilsen,10/31/17,"$435,928.00 ",,siddhartha.srinivasa@gmail.com,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,CSE,7918,"7918, 8235",$0.00 ,"CPS: Synergy: Collaborative Research: Learning control sharing strategies for assistive cyber-physical systems<br/><br/>Assistive machines - like powered wheelchairs, myoelectric prostheses and robotic arms - promote independence and ability in those with severe motor impairments. As the state- of-the-art in these assistive Cyber-Physical Systems (CPSs) advances, more dexterous and capable machines hold the promise to revolutionize ways in which those with motor impairments can interact within society and with their loved ones, and to care for themselves with independence. However, as these machines become more capable, they often also become more complex. Which raises the question: how to control this added complexity? A new paradigm is proposed for controlling complex assistive Cyber-Physical Systems (CPSs), like robotic arms mounted on wheelchairs, via simple low-dimensional control interfaces that are accessible to persons with severe motor impairments, like 2-D joysticks or 1-D Sip-N-Puff interfaces. Traditional interfaces cover only a portion of the control space, and during teleoperation it is necessary to switch between different control modes to access the full control space. Robotics automation may be leveraged to anticipate when to switch between different control modes. This approach is a departure from the majority of control sharing approaches within assistive domains, which either partition the control space and allocate different portions to the robot and human, or augment the human's control signals to bridge the dimensionality gap. How to best share control within assistive domains remains an open question, and an appealing characteristic of this approach is that the user is kept maximally in control since their signals are not altered or augmented. The public health impact is significant, by increasing the independence of those with severe motor impairments and/or paralysis. Multiple efforts will facilitate large-scale deployment of our results, including a collaboration with Kinova, a manufacturer of assistive robotic arms, and a partnership with Rehabilitation Institute of Chicago. <br/><br/>The proposal introduces a formalism for assistive mode-switching that is grounded in hybrid dynamical systems theory, and aims to ease the burden of teleoperating high-dimensional assistive robots. By modeling this CPS as a hybrid dynamical system, assistance can be modeled as optimization over a desired cost function. The system's uncertainty over the user's goals can be modeled via a Partially Observable Markov Decision Processes. This model provides the natural scaffolding for learning user preferences. Through user studies, this project aims to address the following research questions: (Q1)  Expense: How expensive is mode-switching? (Q2)  Customization Need: Do we need to learn mode-switching from specific users? (Q3)  Learning Assistance: How can we learn mode-switching paradigms from a user? (Q4)  Goal Uncertainty: How should the assistance act under goal uncertainty? How will users respond? The proposal leverages the teams shared expertise in manipulation, algorithm development, and deploying real-world robotic systems. The proposal also leverages the teams complementary strengths on deploying advanced manipulation platforms, robotic motion planning and manipulation, and human-robot comanipulation, and on robot learning from human demonstration, control policy adaptation, and human rehabilitation. The proposed work targets the easier operation of robotic arms by severely paralyzed users. The need to control many degrees of freedom (DoF) gives rise to mode-switching during teleoperation. The switching itself can be cumbersome even with 2- and 3-axis joysticks, and becomes prohibitively so with more limited (1-D) interfaces. Easing the operation of switching not only lowers this burden on those already able to operate robotic arms, but may open use to populations to whom assistive robotic arms are currently inaccessible. This work is clearly synergistic: at the intersection of robotic manipulation, human rehabilitation, control theory, machine learning, human-robot interaction and clinical studies. The project addresses the science of CPS by developing new models of the interaction dynamics between the system and the user, the technology of CPS by developing new interfaces and interaction modalities with strong theoretical foundations, and the engineering of CPS by deploying our algorithms on real robot hardware and extensive studies with able-bodied and users with sprinal cord injuries."
239,1448572,SBIR Phase I:  Virtual Footwear Fitting System,IIP,SBIR Phase I,1/1/15,12/18/14,Alex Villanueva,CA,"Eclo, Inc.",Standard Grant,Rajesh Mehta,9/30/15,"$149,197.00 ",,alexv@vt.edu,277 Castro St,Mountain View,CA,940411203,4802541536,ENG,5371,"5371, 8029, 8037",$0.00 ,"The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is in addressing one of the most common problems associated with ordering clothing and shoes online, which is the inability to know how things will fit. The technology being developed is a virtual footwear fitting system (VFFS) utilizing a user's own foot geometry. The virtual fitting experience will give size recommendations based on comfort levels and support levels for different shoe models. This will reduce the 35% return rate of online footwear retailers and thereby reduce their estimated $1.5 billion in annual losses in the US alone. Reducing the risk factor associated with online ordering will help people benefit fully from the convenience and large selection of online retailers. The reduction in return rate will reduce the carbon footprint of some 30 million annual returns. The VFFS will be available to the masses and will create a large exposure for 3D modeling fields such as computer aided design, finite element analysis and computer vision. 3D scanning with smartphones has endless applications, and the virtual fitting technology will help make computer vision principles more adapted for those applications. Virtual fitting opens the doors for mass customization, which is the future of footwear and garments. The technology will help bring our society closer to a world where things are tailored for us, convenient and affordable.<br/><br/><br/><br/>This project focuses on the development of a virtual footwear fitting system to reduce the return rate of online retail. The system consists of a foot scanner, shoe scanner and fitting algorithm. Online shoppers will use their smartphones to scan their feet and obtain a 3D model. A photogrammetry algorithm is being developed for this specific application. It utilizes feature recognition of the foot along with smartphone acceleration outputs to construct a 3D model with 1 mm accuracy in different image capturing environments. An adaptive inner volume (AIV) scanner is being developed to scan the inner volume of shoes that is combined with an outer scan to give a 3D shoe model with 0.5 mm accuracy along with material stiffness. The fitting algorithm will compare foot and shoe scans to perform a comprehensive fitting analysis giving online shoppers a way to know what shoe size they need and how comfortable their potential purchase will be. By the end of this project, a basic virtual fitting platform will be developed. It will allow a user to scan his or her foot with an iPhone and obtain a size recommendation for a specific shoe within ±0.5 size of his or her personal choice based on comfort levels, support levels and fit preferences."
240,1448453,SBIR Phase I: Georeferenced Augmented Reality for Knowledge-Based Excavator Control,IIP,SMALL BUSINESS PHASE I,1/1/15,11/2/15,Melora Goosey,MI,PERCEPTION ANALYTICS & ROBOTICS LLC,Standard Grant,Muralidharan S. Nair,6/30/16,"$149,994.00 ",,pearlsbirpi@umich.edu,3239 Kilburn Park Cir,Ann Arbor,MI,481054125,7348465650,ENG,5371,"5371, 6840, 8035, 9139, HPCC",$0.00 ,"The broader impact/commercial potential of this project stems from its ability to transform excavator operation and control from a primarily skill-based activity to a knowledge-based practice, leading to significant increases in productivity and safety. This is turn will help realize enormous cost savings and reduction of potential hazards to the public, improve competitiveness of U.S. construction industry, and reduce life cycle costs of civil infrastructure. Such benefits will also accrue in fields such as manufacturing, transportation, mining, and ship-building where the transition from skill-based to knowledge-based processes is of value. In the long-run, the accidental utility strike warning capabilities of the proposed solution also have the potential to save millions of dollars in liability and opportunity costs for customers, and avoid disruptions to life and commerce, and prevent physical danger to workers and the public. The societal benefit and commercial impact of the project are thus expected to be the significant reductions in construction and underground infrastructure costs that will be possible through safe and efficient excavation. <br/><br/>This Small Business Innovation Research (SBIR) Phase I project will translate fundamental computer-vision based motion-tracking technology into a transformative solution for tracking an excavator end-effector (bucket) in field conditions, and demonstrate the capability to meet target market performance demands. Excavation is a quintessential construction activity where every operator faces two major problems: 1) The need to maintain precise grade control; and 2) The need to avoid accidental utility strikes. This project will overcome these pain points and present operators with a visualization of excavation job plans, target grade profiles, and the evolving grade profile in real-time using augmented reality, allowing operators to achieve target grades with high precision, improved productivity, and safety. The disruptive innovation is the use of inexpensive computer-vision based tracking to: 1) track the position of an excavator bucket in a local coordinate system; and 2) track the position of a cabin-mounted camera in the same coordinate system to visualize buried utility locations in augmented reality. Extensive field testing has demonstrated that this technology tracks markers relative to a camera with an uncertainty of less than one inch, offering significant advantages over current global positioning system based methods that are expensive and unreliable for the pursued application."
241,1411229,Theoretical Approaches to Multi-Scale Complex Systems,DMR,CONDENSED MATTER & MAT THEORY,3/15/15,2/23/17,Zohar Nussinov,MO,Washington University,Continuing Grant,Daryl Hess,2/28/19,"$279,000.00 ",,zohar@wuphys.wustl.edu,CAMPUS BOX 1054,Saint Louis,MO,631304862,3147474134,MPS,1765,"7203, 7237, 8084, 8091",$0.00 ,"NONTECHNICAL SUMMARY<br/>This award supports theoretical and data-centric computational research and education with an aim to advance the understanding of complex materials. In a perfect crystal, periodically repeating a fundamental structural unit of atoms can generate the entire atomic structure of the material. The theoretical description of such a structure of atoms leads to the elegant explanation of many properties of crystalline or near crystalline materials. By contrast, in complex materials such as glasses, additional rich structures appear on length scales between the atomic and the macroscopic scales. These features make understanding a wide range of materials from ordinary window glass to high temperature superconductors which can conduct electricity without resistance challenging.<br/><br/>A goal of this project is to seek and quantify important structures in glassy materials - not by looking for particular known patterns- but rather by an unbiased analysis of structural data while invoking general physical principles. To address this challenge, the PI will broaden network analysis and computer vision methods to pinpoint and quantify the salient features of complex materials across different scales of length and time. This project will introduce new mathematical techniques for studying complex heterogeneous dynamics of both classical and quantum systems. The research will be done in close contact with experimental research groups around the world. The theoretical and data intensive computational study, in combination with experimental observations, is expected to lead to a deeper understanding of the structure of complex materials and systems including neural circuits, and enable progress in other disciplines. The techniques developed in this award will enhance cross-fertilization between condensed matter physics and other areas with multi-scale architectures such as medical imaging, bioinformatics, and communication networks. In collaboration with experimental neuroscientists, the PI aims to apply theoretical and data-centric computational tools developed in the course of research on materials to the visual neural circuits of the brain.  <br/><br/>This project supports training graduate students as well as further developing courses on advanced statistical mechanics and quantum information. The PI will also engage high school and undergraduate students to design software packages that illustrate the use of some of the methods developed during the course of the research.<br/><br/>TECHNICAL SUMMARY<br/>This award supports theoretical and data-centric computational research and education in theoretical condensed matter physics of materials with complex structure. The PI aims to develop and explore a new framework involving empirical data combined with ideas from data mining and network theory, Fokker-Planck, and other methods, to systematically uncover natural structures across a broad range of spatial and temporal scales. Analysis of experimental data of structural glasses and complex electronic materials will be performed with the aid of methods developed in the course of the project. Research will, specifically, be conducted along several directions: (1) The PI aims to employ ideas from statistical mechanics and network theory to unveil natural building blocks in disparate complex materials. The analysis will be performed on both experimental and simulation results in various materials systems including metallic glass alloys and electronic systems. As a byproduct, new concepts and tools will be developed and applied to optimization as well as imaging problems of wide interest. In collaboration with neuroscientists, the PI plans to apply developed tools to the study of the structure of the circuitry of the visual system of the brain. (2) The project will probe for low temperature quantum dynamical heterogeneities in cuprate superconductors and other electronic systems through a direct analysis of experimental data. Novel quantum effects at high temperatures will be further investigated.  (3) Fokker-Planck methods will be applied to the study of structure and evolution of viscous systems. (4) The PI will investigate the mechanical properties of complex systems, in particular how they may respond to external shear, and ascertain associated length scales.<br/><br/>This project provides a multidisciplinary research environment and an opportunity for students to learn a multitude of valuable techniques, including: molecular dynamics, image and network analysis, and data mining, in the context of close connection to experiments. The PI aims to review and communicate current ideas in network science, statistical mechanics, and condensed matter physics to graduate students through new courses. He also plans to engage high school and undergraduate students to design software packages that illustrate the use of some of the methods developed during the course of the research."
242,1526842,RI: Small: Heuristic Search Algorithms for Probabilistic Graphical Models,IIS,Robust Intelligence,9/1/15,8/5/15,Rina Dechter,CA,University of California-Irvine,Standard Grant,Rebecca Hwa,8/31/19,"$499,955.00 ",Alexander Ihler,dechter@uci.edu,"141 Innovation Drive, Ste 250",Irvine,CA,926173213,9498247295,CSE,7495,"7495, 7923",$0.00 ,"Probabilistic graphical models are employed throughout science and engineering to solve difficult problems, including automated reasoning and decision making, computer vision, computational biology and genetics, and data mining.  However, exact inference is often computationally intractable, necessitating approximations or bounds.  While significant progress has been made, many real-world problems remain out of reach. Many techniques require a set of problem-specific customizations and choices that must be made in advance, with little guidance or automation.<br/><br/>The goal of this research is to develop the next generation of approximate, anytime inference algorithms for graphical models. The PIs will empower search algorithms by strengthening their guiding heuristic functions using variational bounds that are both pre-compiled as well as re-computed dynamically during search. The new algorithms ensure compact search spaces by exploiting the problems' decomposition (using AND/OR search), equivalence (by caching) and pruning irrelevant subspaces using the power of their bounding heuristics. These frameworks will additionally provide automated guidance for selecting parameters to optimize the inherent trade-offs between complexity and accuracy to provide meaningful any-time bounds, while tuning those decisions to the respective benchmarks and instances."
243,1602118,I-Corps: Virtual 3D reconstruction of hollow organs from white light endoscopy,IIP,I-Corps,11/1/15,10/27/15,Audrey Bowden,CA,Stanford University,Standard Grant,Steven Konsek,4/30/16,"$50,000.00 ",,a.bowden@vanderbilt.edu,450 Jane Stanford Way,Stanford,CA,943052004,6507232300,ENG,8023,,$0.00 ,"White light endoscope (WLE) is a medical device that allows physicians to conduct real-time video explorations of the interior of organs to detect disease or guide surgery. Unfortunately, the WLE video data are too cumbersome in their native form to review post-treatment; hence, the information is often reduced to handwritten notes or a few still-image frames for inclusion in medical records. The loss of this visually rich information limits the ability of WLE to inform clinical decisions about treatment and surgery. In particular, diseases like bladder cancer, which holds the unfortunate distinction as the 4th most common cancer in men and the highest treatment cost per patient-lifetime of all cancers, would benefit from novel ways to review WLE video data to facilitate early detection of tumors and to better track changes in the bladder wall of patients likely to experience recurrence (> 50%). The goal of this project is to develop new technology to produce 3D visualizations of the interior of hollow organs such as the bladder from WLE videos. The availability of such technology will provide physicians with new tools to make better informed decisions about treatment and surgery, as well as provide researchers with new technology to enable novel studies on disease progression, ultimately leading to better health outcomes and lower treatment costs for diseases like cancer.<br/><br/>The proposed novel algorithm applies state-of-the-art techniques in computer vision to the problem of 3D reconstruction of the shape and surface appearance of hollow organs. A key innovation in the proposed approach is that the algorithm is suitable to reconstruct a full, 3D model of an organ from endoscopic video captured with standard clinical hardware and requires only minor modifications to the standard clinical workflow. The ability to create these reconstructions from standard equipment and workflows arises from careful design decisions regarding (1) the protocol for endoscopic video collection, (2) necessary image pre-processing steps and (3) the particular combination of state-of-the-art techniques developed in the computer vision community into an end-to-end pipeline unique for our application. In brief, the overall algorithm involves the following steps: down-sample raw image data, process selected frames, determine camera poses, extrapolate the organ surface as a mesh and apply image-based texture to the finalized mesh. The team's initial application is for reconstructing urinary bladder using standard rigid cystoscopy data; to date the team has validated the ability to perform 3D reconstructions using standard clinical cystoscopy videos of > 30 human patients. As the proposed method can powerfully augment the visual medical record of internal organ appearance, it is broadly applicable to endoscopy and represents a significant advance in monitoring the appearance of a patient's organ over time, as may be well suited for applications such as cancer surveillance."
244,1535902,AitF: EXPL: Collaborative Research: Approximate Discrete Programming for Real-Time Systems,CCF,"Algorithms in the Field, Algorithmic Foundations",9/1/15,4/20/18,Thomas Goldstein,MD,University of Maryland College Park,Standard Grant,Tracy Kimbrel,8/31/19,"$216,000.00 ",,tomg@cs.umd.edu,3112 LEE BLDG 7809 Regents Drive,COLLEGE PARK,MD,207425141,3014056269,CSE,"7239, 7796","013Z, 7926, 9251",$0.00 ,"Discrete programming (DP) deals with optimization problems involving variables that range over a discrete (e.g., integer-valued) solution space. DP is an important tool in a variety of practical applications including digital communications, operations research, power grid optimization, and computer vision. While discrete programs are typically solved offline by sophisticated software using powerful computers, DP has recently emerged as an important tool in applications requiring real-time processing in embedded systems with stringent area, cost, and power constraints. Since existing DP solvers entail prohibitive complexity and power consumption when implemented on existing embedded hardware, novel algorithms and hardware architectures are necessary to unlock the potential of DP in real-time applications. This project fuses optimization theory, numerical methods, and circuit design to develop fast algorithms and suitable hardware architectures for real-time DP in embedded systems. Besides a thorough theoretical analysis of the proposed methods, the project includes extensive software and hardware benchmarking to reveal the efficacy of real-time DP in practice. To bridge the ever-growing gap between recent advances in numerical optimization and hardware design, the project also includes the development of undergraduate and graduate courses that build upon the vertically-integrated research approach of this project, in addition to offering summer research internships (REUs) to introduce young scientists to the field of discrete programming.<br/><br/>The project develops a set of computationally efficient and hardware-aware algorithms and corresponding dedicated very-large scale integration (VLSI) architectures that enable DP for real-time embedded systems.  The proposed DP algorithms rely on a variety of algorithmic transformations, ranging from semidefinite and infinity-norm-based relaxations to exact variable-splitting methods and non-convex approximations. These disparate approaches offer a wide range of tradeoffs between solution quality and hardware implementation complexity. The project studies these fundamental tradeoffs, as well as the effects of finite-precision arithmetic in VLSI, from both a theoretical and practical perspective. To carry out this investigation, three dedicated VLSI architectures will be developed that exploit the inherent parallelism of the proposed algorithms. These architectures target (i) data detection in multi-antenna (MIMO) wireless systems that is the key bottleneck in next-generation communication systems, (ii) signal recovery problems in hyperspectral imaging, and (iii) phase retrieval problems from x-ray crystallography. By investigating the domain-specific performance and complexity of various numerical solvers in a variety of conditions and hardware configurations, the project will reveal the efficacy and limits of DP for a broad range of real-time applications beyond the ones studied in this project."
245,1453771,CAREER: Adaptive Tactile Picture Books for Blind Children during Emergent Literacy,IIS,HCC-Human-Centered Computing,2/1/15,2/15/19,Tom Yeh,CO,University of Colorado at Boulder,Continuing Grant,Ephraim Glinert,1/31/21,"$581,999.00 ",,tom.yeh@colorado.edu,"3100 Marine Street, Room 481",Boulder,CO,803031058,3034926221,CSE,7367,"1045, 7367, 9251",$0.00 ,"This research is motivated by the importance of a child's early experiences with picture books during emergent literacy, which is the period when a child gradually learns to read.  Picture books connect visual literacy, cultural literacy, and print literacy, which is crucial for a child's literacy development.  But only a limited number of children's books have tactile pictures accessible to blind children, and these typically cannot be modified to suit a child's particular needs.  With this in mind, the PI will investigate 3D printing as the technical mechanism to physically realize a variation of an adaptive tactile picture that is optimized for a specific child and to ""make"" just one unique copy of it, something which is not possible with conventional printing approaches geared toward mass production of a uniform design.  Achieving this vision will change the way tactile pictures are made available to visually impaired children; a parent or a teacher will be able to download a digital 3D model of a tactile picture book from a website, adapt the model for the child, 3D print the model, and read the resulting ""one-of-a-kind"" book together with the child.  Research products will include a website for sharing 3D tactile pictures and open source tools for creating them.<br/><br/>In this project, which integrates science, design, and engineering, the PI will exploit 3D printing to overcome the lack of adaptation in conventional one-size-fits-all tactile pictures.  Specific Aim 1 will establish a new experimental framework, based on large-scale dissemination to families and video-based behavioral observations of children, for scientifically studying 3D-printed tactile pictures.  Specific Aim 2 will uncover the principles to elicit preferences and abilities from a visually impaired child, and the adaptive transformations necessary to optimize the design of a tactile picture specifically for that child.  Specific Aim 3 will introduce technical innovations to optimize the 3D printing of tactile pictures, to simplify 3D modeling software to empower more people to model tactile pictures, and to automate certain repetitive and laborious steps using computer vision.  To achieve these goals, the PI will partner with the Anchor Center for Blind Children, the National Center on Severe and Sensory Disabilities, the Colorado Talking Book Library, and the National Braille Press, all of which are organizations with a shared interest in promoting emergent literacy development of visually impaired children.  Project outcomes will advance our understanding of tactile pictures geared toward visually impaired children, and of the tactilization process for storybook pictures."
246,1542860,I-Corps: Computer Vision for Tracking People in Different Scenarios,IIP,I-Corps,6/1/15,5/21/15,Davi Geiger,NY,New York University,Standard Grant,Steven Konsek,2/29/16,"$50,000.00 ",,geiger@cs.nyu.edu,70 WASHINGTON SQUARE S,NEW YORK,NY,100121019,2129982121,ENG,8023,,$0.00 ,"A large retail chain with numerous stores in several shopping malls (e.g., Bed, Bath & Beyond) may devise advertising campaigns in which they display new items or feature promotional items within each store, and then endeavor to evaluate the success of each campaign. In addition to tracking their revenues, the retailer may seek to measure traffic outside each store, the percentage of people who enter the store, how people move throughout the store, time spent in each location throughout the store, etc. While there are tools and technologies currently available to assist with these efforts, they provide very crude data, resulting in an unmet market need. Today, infrared cameras are used to address the needs described above, but are crude in their estimates. Other existing solutions include mounted cameras in specific locations to spot people in a given part of the store (e.g., hot zones or entrances to count people entering). They are also used to distinguish different types of people (kids versus adults). But these existing methods and technologies are unreliable and unadaptable (e.g., adjusting to occlusions or changes in natural light over time to get an accurate count), and therefore fail to deliver accurate results to the retail chains.<br/><br/>This I-Corps team is developing technology that addresses this market need in new and innovative ways, using technology not present in products today and providing more accurate and higher fidelity data than existing solutions on the market. The solution being developed will be easy to use, low cost, and reliable. The proposed technology is applicable to both indoor and outdoor data (accounting for light variations). To develop the solution and business model, the team plans to establish relationships with large retail stores to further assess their needs and where the technology will be best applied. Beyond the retail market, it is believed that the proposed technology will have commercial impact in airports, train/bus stations, and all surveillance activities. Clearly, security is an area where this technology of accurate measurement of people's movements is necessary. With the technology this I-Corps team is developing, retailers will receive more accurate and reliable data about the activities happening in and around their stores (such as the result of a window marketing campaign). This will enable them to improve store operations and provide better experiences to customers."
247,1535897,AitF: EXPL: Collaborative Research: Approximate Discrete Programming for Real-Time Systems,CCF,Algorithms in the Field,9/1/15,7/30/15,Christoph Studer,NY,Cornell University,Standard Grant,Tracy Kimbrel,8/31/19,"$200,000.00 ",,studer@cornell.edu,373 Pine Tree Road,Ithaca,NY,148502820,6072555014,CSE,7239,013Z,$0.00 ,"Discrete programming (DP) deals with optimization problems involving variables that range over a discrete (e.g., integer-valued) solution space. DP is an important tool in a variety of practical applications including digital communications, operations research, power grid optimization, and computer vision. While discrete programs are typically solved offline by sophisticated software using powerful computers, DP has recently emerged as an important tool in applications requiring real-time processing in embedded systems with stringent area, cost, and power constraints. Since existing DP solvers entail prohibitive complexity and power consumption when implemented on existing embedded hardware, novel algorithms and hardware architectures are necessary to unlock the potential of DP in real-time applications. This project fuses optimization theory, numerical methods, and circuit design to develop fast algorithms and suitable hardware architectures for real-time DP in embedded systems. Besides a thorough theoretical analysis of the proposed methods, the project includes extensive software and hardware benchmarking to reveal the efficacy of real-time DP in practice. To bridge the ever-growing gap between recent advances in numerical optimization and hardware design, the project also includes the development of undergraduate and graduate courses that build upon the vertically-integrated research approach of this project, in addition to offering summer research internships (REUs) to introduce young scientists to the field of discrete programming.<br/><br/>The project develops a set of computationally efficient and hardware-aware algorithms and corresponding dedicated very-large scale integration (VLSI) architectures that enable DP for real-time embedded systems.  The proposed DP algorithms rely on a variety of algorithmic transformations, ranging from semidefinite and infinity-norm-based relaxations to exact variable-splitting methods and non-convex approximations. These disparate approaches offer a wide range of tradeoffs between solution quality and hardware implementation complexity. The project studies these fundamental tradeoffs, as well as the effects of finite-precision arithmetic in VLSI, from both a theoretical and practical perspective. To carry out this investigation, three dedicated VLSI architectures will be developed that exploit the inherent parallelism of the proposed algorithms. These architectures target (i) data detection in multi-antenna (MIMO) wireless systems that is the key bottleneck in next-generation communication systems, (ii) signal recovery problems in hyperspectral imaging, and (iii) phase retrieval problems from x-ray crystallography. By investigating the domain-specific performance and complexity of various numerical solvers in a variety of conditions and hardware configurations, the project will reveal the efficacy and limits of DP for a broad range of real-time applications beyond the ones studied in this project."
248,1528145,NRI: Robust and Low-Cost Smart Skin with Active Sensing Network for Enhancing Human-Robot Interaction,IIS,NRI-National Robotics Initiati,10/1/15,8/27/15,Fu-Kuo Chang,CA,Stanford University,Standard Grant,David Miller,6/30/20,"$900,000.00 ",Marco Pavone,fkchang@stanford.edu,450 Jane Stanford Way,Stanford,CA,943052004,6507232300,CSE,8013,8086,$0.00 ,"Tactile sensing is ubiquitous in nature - arguably even more essential than vision. Most animals have thousands of cutaneous sensors over their bodies for touch, temperature, etc. But even the most sophisticated robots have relatively few tactile sensors and, after 30 years of research, tactile sensing lags behind computer vision. This project aims at the development of a novel artificial skin mimicking the human skin that can be fitted into any robotic hand providing information-rich ""sense of touch."" This technology leads to the development of extremely sensitive robotic skins with unprecedented tactile sensing capabilities. As such, this work enables a plethora of robotic applications where tactile sensing is of utmost importance, ranging from robotic caregivers to medical robotics and autonomous exploration. The methodology in this project may revolutionize the way future robots are designed enabling their broad applicability. This effort represents a major milestone in endowing robots with the sensory information required to carry out tasks in human-centered environments.<br/><br/>The advent of microprocessors for touch sensing, spurred by the smart phone industry, has helped to address the wiring problem with local processing of information and communication. However, there remain the critical problems of fabricating a multi-functional artificial skin that can conformally cover arbitrary surfaces, diagnosing in real-time, the contact state, and gathering a large amount of data for high-resolution tactile sensing, while minimizing power consumption. Overall, this effort addresses tactile sensing from a system-level point of view. The approach involves the development of advanced manufacturing technologies from leveraging nonstandard CMOS/MEMS/NEMS fabrication processes to produce a low-cost and robust artificial skin outfitted with multi-modal micro-sensors. The tactile sense of touch is achieved via an innovative micro-contact sensing technique based on ultrasound waves generated from embedded sensors to identify local contact/slip conditions. Finally, the validation and performance evaluation is demonstrated through a series of graded tactile sensing experiments."
249,1514053,"III: Medium: Constructing Knowledge Bases by Extracting Entity-Relations and Meanings from Natural Language via ""Universal Schema""",IIS,Info Integration & Informatics,9/1/15,9/17/15,Andrew McCallum,MA,University of Massachusetts Amherst,Continuing Grant,Maria Zemankova,8/31/20,"$1,000,000.00 ",,mccallum@cs.umass.edu,Research Administration Building,Hadley,MA,10359450,4135450698,CSE,7364,"7364, 7924",$0.00 ,"Automated knowledge base (KB) construction from natural language is of fundamental importance to (a) scientists (for example, there has been long-standing interest in building KBs of genes and proteins), (b) social scientists (for example, building social networks from textual data), and (c) national defense (where network analysis of criminals and terrorists have proven useful). The core of a knowledge base is its objects (""entities"", such as proteins, people, organizations and locations) and its connections between these objects (""relations"", such as one protein increasing production of another, or a person working for an organization). This project aims to greatly increase the accuracy with which entity-relations can be extracted from text, as well as increase the fidelity which many subtle distinctions among types of relations can be represented. The project's technical approach -- which we call ""universal schema"" -- is a markedly novel departure from traditional methods, based on representing all of the input relation expressions as positions in a common multi-dimensional space, with nearby relations having similar meanings. Broader impacts will include collaboration with industry on applications of economic importance, collaboration with academic non-computer-scientists on a multidisciplinary application, creating and publicly releasing new data sets for benchmark evaluation by ourselves and others (enabling scientific progress through improved performance comparisons), creating and publicly releasing an open-source implementation of our methods (enabling further scientific research, easy large-scale use, rapid commercialization and third-party enhancements). Education impacts include creating and teaching a new course on knowledge base construction for the sciences, organizing a research workshop on embeddings, extraction and knowledge representation, and training multiple undergraduates and graduate students. <br/><br/>Most previous research in relation extraction falls into one of two categories. In the first, one must define a pre-fixed schema of relation types (such as lives-in, employed-by and a handful of others), which limits expressivity and hides language ambiguities. Training machine learning models here either relies on labeled training data (which is scarce and expensive), or uses lightly-supervised self-training procedures (which are often brittle and wander farther from the truth with additional iterations). In the second category, one extracts into an ""open"" schema based on language strings themselves (lacking ability to generalize among them), or attempts to gain generalization with unsupervised clustering of these strings (suffering from clusters that fail to capture reliable synonyms, or even find the desired semantics at all). This project proposes research in relation extraction of ""universal schema"", where we learn a generalizing model of the union of all input schemas, including multiple available pre-structured KBs as well as all the observed natural language surface forms. The approach thus embraces the diversity and ambiguity of original language surface forms (not trying to force relations into pre-defined boxes), yet also successfully generalizes by learning non-symmetric implicature among explicit and implicit relations using new extensions to the probabilistic matrix factorization and vector embedding methods that were so successful in the NetFlix prize competition. Universal schema provide for a nearly limitless diversity of relation types (due to surface forms), and support convenient semi-supervised learning through integration with existing structured data (i.e., the relation types of existing databases). In preliminary experiments, the approach already surpassed by a wide margin the previous state-of-the-art relation extraction methods on a benchmark task. New proposed research includes new training processes, new representations that include multiple-senses for the same surface form as well as embeddings with variances, new methods of incorporating constraints, joint inference between entity- and relation-types, new models of non-binary and higher-order relations, and scalability through parallel distribution. The project web site (http://www.iesl.cs.umass.edu/projects/NSF_USchema.html) will include information on the project and provide access to data sets, source code and documentation, teaching and workshop materials, and publications. In addition, datasets will be disseminated via UCI Machine Learning Repository (or other similar archive location for machine learning data) to facilitate sharing with other researchers and ensure long-term availability, and GitHub will be used to facilitate release, sharing, and archiving of code."
250,1539608,CyberSEES: Type 1: A Novel Machine Learning Framework for Urban Heat Island Causal Analysis: a Fusion of Observations and Physical Models,CCF,CyberSEES,9/1/15,8/27/15,Yan Liu,CA,University of Southern California,Standard Grant,Phillip Regalia,8/31/19,"$400,000.00 ",George Ban-Weiss,yanliu.cs@usc.edu,University Park,Los Angeles,CA,900890001,2137407762,CSE,8211,"8207, 8231",$0.00 ,"The urban heat island is a phenomenon whereby urban areas have higher surface and atmospheric temperatures than surrounding suburban and rural regions. These higher temperatures lead to increases in building cooling energy use during summer, and can exacerbate urban air pollution, human thermal comfort, and heat waves. This project unites expertise in three fields---machine learning, civil and environmental engineering, and earth science---to develop a novel framework that integrates data-driven approaches and physics-based climate simulation models to better understand the physical process drivers of urban heat islands around the world. Better understanding the main drivers of urban heat islands in cities is critical for identifying the appropriate engineering solutions for mitigating urban warming from heat islands. This research will have broad societal impact by changing the ways that people participate in scientific data analysis tasks, and will build a stronger body of research in computational sustainability. <br/><br/>The proposed framework will integrate data-driven approaches and physically based models in one discovery process. It consists of three important steps: (i) latent feature discovery, which aims to automatically infer high-level feature representations from large scale observational data via deep networks. These latent features capture the complex nonlinear transformation of observed variables as a metaphor of latent physical processes; (ii) latent feature interpretation, which generates candidates of urban heat island drivers from the latent features via a compiled dataset. It provides insights into how these latent features are associated with physical processes; and (iii) urban heat island driver identification, which designs effective experiments to identify the causes of heat islands by varying observed variables from simulation models. The project is expected to advance the knowledge of key physical process drivers causing urban heat islands in cities. In addition, the developed framework has the potential to advance machine learning, including feature learning, causal analysis with confounders, and causal experiment design. The source code for computational tools and the data sets collected through the project will be freely disseminated to the broader research and educational community."
251,1525971,AF: Small: New Directions in Learning Theory,CCF,Algorithmic Foundations,6/15/15,6/11/15,Avrim Blum,PA,Carnegie-Mellon University,Standard Grant,Tracy Kimbrel,11/30/17,"$450,000.00 ",,avrim@ttic.edu,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,CSE,7796,"7923, 7926",$0.00 ,"This project is to develop core principles and technologies for systems that learn from observation and experience in order to better help their users.  While there is already a significant body of work in the area of machine learning, today's interconnected world provides both new challenges and new opportunities that classic methods are not able to address or take advantage of.  This project has three main thrusts.  The first involves development of methods that can extract useful information from auxiliary sources in addition to traditional labeled data.  This includes methods for quickly learning multiple related tasks by taking advantage of ways in which they relate to each other.  The second involves approaches for learning about what different users or agents want by observing the results of their interactions.  Finally, the third thrust involves development of new rigorous methods for quickly estimating the amount of resources that would be needed to solve a given learning task.  Broader impacts of the project include the training of a diverse set of graduate students, improving undergraduate curricula with respect to machine learning technology, and developing a new book for advanced undergraduates on algorithms and analysis for data science.<br/><br/>More specifically, the first main thrust of this work involves a combination of unsupervised, semi-supervised, and multi-task learning. This work will investigate problems of estimating error rates from unlabeled data, unifying co-training and topic models, learning multiple related tasks from limited supervision, and learning new representations of data using tools from high-dimensional geometry.  The second main thrust will focus on reconstructing estimates of agent utilities from observing the outcomes of economic mechanisms such as combinatorial auctions.  This thrust also includes problems of learning the rules of unknown mechanisms from experimentation.  Finally, the last thrust focuses on development of the theory of property testing for machine learning problems, with the goal of quickly estimating natural formal measures of complexity of a given learning task."
252,1551875,EAGER: Research in the Interface of Algorithmic Game Theory and Learning,CCF,ALGORITHMIC FOUNDATIONS,9/1/15,8/25/15,Constantinos Daskalakis,MA,Massachusetts Institute of Technology,Standard Grant,Tracy J. Kimbrel,8/31/16,"$225,000.00 ",,costis@csail.mit.edu,77 MASSACHUSETTS AVE,Cambridge,MA,21394301,6172531000,CSE,7796,"7916, 7932",$0.00 ,"Recent years have seen tremendous advances in Machine Learning and in the interface between Computer Science and Economics. Progress in Machine Learning has been driven by the vast amounts of data that humanity is generating and collecting. It is now widely accepted that scientific innovation necessitates the development of computational methodology to process this data and use it for inference and prediction. This has resulted in remarkable progress at the interface of Algorithms, Machine Learning and Statistics. At the same time, much of the world's economic activity has been transferred to the Internet via old markets that obtained online presence as well as new markets that are directly inspired and enabled by online activity, such as sponsored search and ad auctions. Driven by the increasing importance of online economic activity there has been much interest in investigating its joint computational and economic characteristics through research at the interface of Computer Science and Economics, which includes Algorithmic Game Theory.<br/><br/>The PI and his group at MIT have made several contributions to both Learning and Algorithmic Game Theory. The goal of the proposed research is to push the research front in the interface between these two fields.<br/><br/>The PI and his team plan to pursue 4 goals, as follows. Goal (1) is to advance understanding of learning dynamics in games. Goal (2) is to design ""learning mechanisms'' to solve learning and inference tasks when the only access to data is through strategic data providers with a cost for producing good data. Besides online learning, the team expects that advances in (1) will have implications to fundamental problems in Algorithmic Game Theory, particularly in goal (3): improving the state-of-the-art in algorithms for the computation of approximate Nash equilibria. Progress in (2) will have immediate applications in crowd-sourcing, but the team also plans to investigate another application, motivated by the tremendous growth of Massive Online Open Courses: goal (4) is to develop good peer grading schemes."
253,1542265,"Workshop:  Learning,  Perception and Control in Robotics and Humans",IIS,ROBUST INTELLIGENCE,8/1/15,7/21/15,Evangelos Theodorou,GA,Georgia Tech Research Corporation,Standard Grant,jeffrey trinkle,2/29/16,"$88,211.00 ",,evangelos.theodorou@ae.gatech.edu,Office of Sponsored Programs,Atlanta,GA,303320420,4048944819,CSE,7495,"7495, 7556",$0.00 ,"Autonomy and robust intelligence are important research areas due to the plethora of applications from space exploration, manufacturing, robotics and transportation to medicine and biology. In aerospace applications, autonomy becomes more and more important due to the need for deep space exploration. In manufacturing, the development of technologies for safe human-machine and human-robot interaction have the potential to improve the competitiveness of existing industrial processes and create economic growth. In medicine and biology, the use of intelligent systems can improve health care and minimize risk factors. In the area of disaster response, there is the need for autonomous systems to operate in remote and dangerous for the human environments. Given the importance of autonomy and robust intelligence in the aforementioned areas, this workshop aims to identify fundamental scientific questions and encourage new research directions at the confluence of aerospace, medicine, transportation, manufacturing, disaster response, space exploration, and biology.<br/><br/>Learning, perception and control are fundamental modalities necessary for autonomous systems to operate in dynamic, uncertain and remote environments. Autonomous systems should be able to robustly walk, navigate, e&#64259;ciently explore, quickly learn new motor skills and generalize these skills to unseen conditions. This workshop brings together scientists from di&#64256;erent areas of sciences and engineering to brainstorm on two questions related to the representation of sensory information and data, and generalization of decision and control mechanisms in robotics and autonomous systems. The aforementioned topics are investigated at the intersection of planning and control, information theory, machine learning, neuroscience and perception. The emphasis of the workshop will be on the mathematical interdependencies and interconnections of these areas based on mathematical concepts that include but they are not limited to di&#64256;erential geometry and topology. The goal for the workshop is to determine future research directions and identify open questions across the disciplines of control theory, machine learning, perception and cognitive sciences."
254,1513031,II-New: Enabling Analysis of Time-Dependent 3D Transparent Flows via Light Field Imaging and Displays,CNS,CCRI-CISE Cmnty Rsrch Infrstrc,6/15/15,6/10/15,Jingyi Yu,DE,University of Delaware,Standard Grant,Jie Yang,12/31/18,"$174,445.00 ",Lian-Ping Wang,yu@cis.udel.edu,210 Hullihen Hall,Newark,DE,197160099,3028312136,CSE,7359,"7359, 9150",$0.00 ,"This project supports construction of a novel computational imaging system for efficient and robust acquisition and reconstruction of multi-scale flows. Since transparent flows do not have their own images but borrow appearance from nearby objects, the solution developed here directly acquires light paths off/through the flow wavefront/volume and then infers the physical properties of the flow. The acquisition system is portable and non-intrusive, to support on-site acquisition of various types of flows. Time-dependent 3D transparent flows such as fluid wavefronts, gas-liquid interfacial flows, and turbulent flows are ubiquitous in chemical, biological and environmental engineering. Accurate reconstruction of such flows under experimental setups have a broader impact on these areas and their applications, e.g., studying coal combustors and predicting natural phenomena such as warm rain and hurricane.<br/><br/>The new 3D flow acquisition system leverages emerging light field (LF) cameras and displays. An LF collects all rays emitting from a 3D scene. The research team explores two unique properties of LFs, ray sampling and multi-view imaging. A reconfigurable LF camera-display system is first developed to directly capture how the flow changes the light paths. Tailored computer vision algorithms are then developed to infer physical properties of the flow (density, velocity, vorticity, etc). Compared with state-of-the-art solutions, the new acquisition system is portable and can be re-configured to acquire various flow types ranging from completely specular wavefronts to transparent gas volumes and to turbulent flows laden with solid particles. For validation, direct (with all scales resolved) simulations of turbulent channel flows are conducted, without and with solid particles, against the experimental measurements using a mesoscopic approach known as the lattice Boltzmann (LB) method."
255,1546689,EARS: Machine Learning and Social Protocols for Enhancing Spectrum Access for Wireless Communications,CNS,EARS,10/1/15,9/15/15,Janne Lindqvist,NJ,Rutgers University New Brunswick,Standard Grant,Alexander Sprintson,9/30/19,"$300,000.00 ",,janne.lindqvist@rutgers.edu,33 Knightsbridge Road,Piscataway,NJ,88543925,8489320150,CSE,7976,7976,$0.00 ,"Radio spectrum is a scarce resource that needs to be managed. Towards this end, this project designs, implements and studies novel protocols for enhancing access to radio spectrum by taking advantage of the fundamentals of human behavior. It produces technologies, theories, and guidelines for protocols that are expected to significantly improve the efficiency of spectrum access. This may lead to substantial societal impact by allowing more work to be done with the same resources. Success will also benefit the environment by limiting the infrastructure needed for the required data traffic; both energy and infrastructure investment can be minimized. Further, the project creates a model of human behavior in a specific technical context that can serve as a basis for similar projects in other technology areas involving resource optimization.  The project creates anonymized datasets, and tools to analyze human and wireless behavior, all of which will be distributed to the general public. This project has important educational and training benefits at high school, undergraduate and graduate levels, including undergraduate research prototyping projects, and new graduate-level courses and seminars. <br/><br/>This interdisciplinary project applies and develops expertise from areas of social computing, machine learning, wireless technology, security engineering, physical analogs, mobile systems, and user-centered design. The project implements and deploys a system that enables efficient bandwidth sharing with machine learning and social protocols that goes beyond what is possible with technology alone. Social protocols are cooperative yet discretionary methods that allow to users distribute access more fairly using inherently natural decision-making processes as opposed to externally imposed ones. The project consists of three major activities: (1) study the main approaches to social protocols, including persuasive computing, clinical behavior change theories, and micro-tasks; (2) develop a system that observes users trying to access the network and facilitates control rules that allow maximum value to users in the most transparent way; and (3) deploy these social protocols and the system in live networks to evaluate the approaches in real-life settings."
256,1527232,NRI: Deep Learning Unmanned Aircraft Systems for High-Throughput Agricultural Disease Phenotyping,IIS,NRI-National Robotics Initiati,8/1/15,10/20/15,Michael Gore,NY,Cornell University,Standard Grant,David Miller,7/31/19,"$1,149,273.00 ","Rebecca Nelson, Michael Gore",mag87@cornell.edu,373 Pine Tree Road,Ithaca,NY,148502820,6072555014,CSE,8013,8086,$0.00 ,"An estimated 13% of crops are lost globally to plant diseases. Disease detection, identification, and tracking is performed today by crop scouts, a process that is expensive, slow and difficult, and is impractical to expand to cover all crops. The project involves developing AI-drones that work side-by-side with farmers and identify specific diseases and assess their progress.  The use of intelligent drones for crop monitoring will allow farmers to respond quickly to emergent diseases, nutrient stress, and other potentially devastating damages without the prohibitive expense of hiring a crop scout. This ability could increase productivity, and may also help predict, track and respond to epidemics for national and global food security. In addition, this technology will also be used for ongoing collection of precise plant performance data for breeding resistance.<br/><br/>The central hypothesis of this proposal is that drones equipped with trained Convolutional Neural Networks can provide a transformative increase in actionable crop disease identification. A secondary hypothesis is that the proposed phenotyping at the individual plant level will also provide unprecedented resolution of data for future modeling, breeding, and data-driven yield optimization. In order to test this hypothesis, we will develop a UAS platform to collect images over university owned experimental crops, and aim to develop AI to identify pathologies at an accuracy that is on par with human experts. The UASs will consult human experts in ambiguous cases and gradually learn to make decisions autonomously. The key challenge will be development of AI that can reliably diagnose disease with a good accuracy of detection to false alarms. Automatic identification of disease is a challenging machine vision task given the complexity of images, exacerbated by variable lighting and weather conditions, and navigation/stability control."
257,1507928,CDS&E: Formalisms and Tools for Data-enabled Turbulence Modeling,CBET,CDS&E,9/1/15,6/27/15,Karthikeyan Duraisamy,MI,Regents of the University of Michigan - Ann Arbor,Standard Grant,Ron Joslin,8/31/20,"$399,986.00 ",Paul Durbin,kdur@umich.edu,3003 South State St. Room 1062,Ann Arbor,MI,481091274,7347636438,ENG,8084,"058E, 7433, 8084",$0.00 ,"The goal of the proposed research is to devise rigorous mathematical techniques that utilize large databases obtained by simulations to develop predictive models of turbulent flow. Even though the proposal is focused on turbulence, the tools to be developed will be of general applicability for data-driven modeling  in other areas of science and engineering. <br/><br/>Predictive models have not yet taken full advantage of the massive amounts of data being generated by the fluids community. New strategies are needed to extract information and modeling knowledge from data. Adjoint-driven inverse problems will be invoked to extract relevant modeling information from data. An important aspect of this approach is that the data is processed in the context in which it is needed for prediction. Domain-specific machine learning techniques will be used to convert information to modeling knowledge. In essence, the inverse solution infers functional deficiencies in the model and machine learning is used to reconstruct the missing functional form. The co-PIs plan to investigate how to identify and formulate a properly-posed data-driven-turbulence-modeling problem, the implications that these approaches have in more general data-driven computational physics applications, and the most effective ways to use machine learning in a predictive physics setting. Applications to be explored include transition to turbulence, thermal transport, and near-wall turbulent stress closures. The proposed work is expected to result in improved closure models for Reynolds-Averaged as well as hybrid Reynolds-Averaged/Large Eddy simulations. While the focus of the proposed work is on turbulent flow applications, several aspects of the formulation and tools will be of more general value to the field of data-driven physical modeling. Educational activities that would integrate machine learning into fluid dynamics courses are proposed. Tools and technologies will be shared with the community and the industry"
258,1516527,US-German Data Sharing: Integrating Distributed Data Resources to Enable New Research Approaches in Neuroscience,IIS,"CYBERINFRASTRUCTURE, CRCNS-Computation Neuroscience",12/1/15,9/1/15,Friedrich Sommer,CA,University of California-Berkeley,Standard Grant,Kenneth Whang,11/30/19,"$522,884.00 ",,fsommer@berkeley.edu,Sponsored Projects Office,BERKELEY,CA,947101749,5106433891,CSE,"7231, 7327","7327, 8089, 8091",$0.00 ,"This project seeks to develop new methods of describing and managing neuroscience data in order to accelerate scientific progress in many fields of neuroscience and deepen understanding of the brain.  The project will produce software tools to enable annotation and integration of distributed data and help leverage the wealth of data emerging from current large-scale projects such as the Human Brain Project in Europe and the BRAIN Initiative in the US.  These results will impact medical application areas such as brain machine interfaces, devices for sensor prosthetics and also application areas such as computer vision.  Further, the methods developed might be generalizable to other domains of biology and medicine where traditional rigid approaches for organizing data are inapplicable. This could lead to the discovery of causes and treatments of diseases that would not have been made otherwise.<br/><br/>Neurophysiology data, which contain recordings of brain activity, are becoming more commonly shared on the web but they are still very hard to use. To improve the usability of shared neurophysiology data sets, a standardized and expandable system will be developed for annotating the data with metadata required for their understanding. Furthermore, semantic web technology will be employed to represent, index, and integrate data and metadata, across distributed locations on the web. Improving the organization of metadata for shared neurophysiology data will be key for enabling studies that integrate across data sets, such as new types of meta-analyses or data mining methods.  This project builds on existing online resources for neurophysiology data created by the project partners, CRCNS.org and G-NODE.org, and on pervious work by the INCF neurophysiology data sharing task force. Training of international students and researchers in annual summer courses at UC Berkeley and LMU Munich will improve career opportunities that allow individuals across disciplines to make discoveries and advancements in neuroscience. <br/><br/>A companion project is being funded by the Federal Ministry of Education and Research, Germany (BMBF)."
259,1535987,AitF: FULL: Collaborative Research: PEARL: Perceptual Adaptive Representation Learning in the Wild,CCF,Algorithms in the Field,9/1/15,8/14/15,Mehryar Mohri,NY,New York University,Standard Grant,A. Funda Ergun,8/31/21,"$399,983.00 ",,mohri@cims.nyu.edu,70 WASHINGTON SQUARE S,NEW YORK,NY,100121019,2129982121,CSE,7239,012Z,$0.00 ,"Vast amounts of digitized images and videos are now commonly available, and the advent of search engines has further facilitated their access. This has created an exceptional opportunity for the application of machine learning techniques to model human visual perception. However, the data often does not conform to the core assumption of machine learning that training and test images are drawn from exactly the same distribution, or ""domain."" In practice, the training and test distributions are often somewhat dissimilar, and distributions may even drift with time. For example, a ""dog"" detector trained on Flickr may be tested on images from a wearable camera, where dogs are seen in different viewpoints and lighting conditions. The problem of compensating for these changes--the domain adaptation problem--must therefore be addressed both in theory and in practice for algorithms to be effective. This problem is not just a second-order effect and its solution does not constitute a small increase in performance.  Ignoring it can lead to dramatically poor results for algorithms ""in the field.""<br/><br/>This project will develop a core suite of theory and algorithms for PErceptual Adaptive Representation Learning (PEARL), which, when given a new task domain, and previous experience with related tasks and domains, will provide a learning architecture likely to achieve optimal generalization on the new task. We expect PEARL to have a significant impact on the research community by providing a much-needed theoretical and computational framework that takes steps toward unifying the subfields of domain adaptation theory and domain adaptation practice. Our theoretical and practical advancements will impact many application areas by allowing the use of pre-trained perceptual models (visual and otherwise) in new situations and across space and time. For example, in mobile technology and robotics, PEARL will help personal assistants and robots better adapt their perceptual interfaces to individual users and particular situated environments.  At the core of this project are three main research thrusts: 1) making theoretical advances for domain adaptation by developing generalized discrepancy distance minimization; 2) using the theoretical guarantees of generalized discrepancy distance to develop algorithms for key adaptation scenarios of deep perceptual representation learning, domain adaptation with active learning, and time-dependent adaptation; 3) advancing the theory and developing algorithms for the multiple-source adaptation scenario. In addition to our core aims, we plan to implement our algorithms within a scalable open-source framework, and evaluate our algorithms on large-scale visual data sets."
260,1528163,SHF: Small: Detecting and Repairing Presentation Failures in Web Applications,CCF,Software & Hardware Foundation,8/1/15,6/29/15,William Halfond,CA,University of Southern California,Standard Grant,Sol Greenspan,7/31/20,"$375,000.00 ",,halfond@usc.edu,University Park,Los Angeles,CA,900890001,2137407762,CSE,7798,"7923, 7944",$0.00 ,"The user interfaces (UIs) of a web application serve an important role<br/>in the success of online companies.  Studies have shown that the<br/>aesthetics of an application?s UIs can strongly influence users?<br/>overall evaluation of a company; in particular impressions of<br/>trustworthiness and usability. Presentation failures ? a discrepancy<br/>between the actual appearance of a web site and its intended<br/>appearance ? can undermine this effort and negatively impact end<br/>users? perception of the quality of the site, the services it<br/>delivers, and affect the branding a company is trying to achieve.<br/>Debugging presentation failures is typically a manual task where<br/>testers must identify when the layout and style of an app?s pages is<br/>incorrect and developers must carry out repairs. This process is both<br/>labor intensive and error prone: testers can easily miss presentation<br/>failures, since the process is based on the testers? visual comparison<br/>of a page and an oracle; and it can be difficult for developers to<br/>identify the responsible HTML elements or properties once a failure is<br/>detected, since HTML and CSS interact in complex ways. In this<br/>research effort the PIs will investigate automated techniques to help<br/>testers more accurately detect presentation failures and efficiently<br/>identify the faulty code.  Given the widespread deployment of web<br/>applications, this work will impact both end users and software<br/>developers by reducing errors in these applications and making the<br/>debugging process more effective and efficient.  The results of this<br/>research will also impact educational efforts through the training of<br/>future software engineering in new techniques to improve the quality<br/>of web applications.<br/><br/>The technical research will encompass two general areas.  The first<br/>area of research will advance techniques for detecting presentation<br/>failures.  The primary mechanism for doing this will be to apply<br/>computer vision techniques to automatically identify discrepancies<br/>between the actual rendered appearance of a UI and its intended<br/>appearance.  The developed techniques will be widely applicable to a<br/>range of UIs, including those with dynamic content and those using<br/>different specification mechanisms for UI oracles.  The second area of<br/>research will use the identified differences to identify the specific<br/>elements in the UI code that are responsible for the observed<br/>failures.  The research efforts will focus on using artificial<br/>intelligence techniques, such as search-based and statistical-based<br/>learning, on the UI?s layout models to reason about the UI?s possible<br/>faults and guide the developers to identify the faulty HTML elements<br/>and CSS properties."
261,1550881,EAGER-NEON: Image-Based Ecological Information System (IBEIS) for Animal Sighting Data for NEON,EF,MacroSysBIO & NEON-Enabled Sci,9/1/15,8/7/15,Daniel Rubenstein,NJ,Princeton University,Standard Grant,Michelle Elekonich,8/31/18,"$50,000.00 ",,dir@princeton.edu,Off. of Research & Proj. Admin.,Princeton,NJ,85442020,6092583090,BIO,7959,"7916, 7959, 9178, 9179",$0.00 ,"The National Ecological Observatory Network (NEON) is coming online and will provide atmospheric and ecological data locally, regionally and continent wide. At the same time, images are rapidly becoming the most abundant, widely available, and cheapest source of information about the natural world, especially about animals. This project will extend NEON's data, scientific, and citizen science capacity with image-based animal sighting data to scalably collect, manage, and analyze data for individually identifiable wildlife using the Image-Based Ecological Information System (IBEIS) prototype recently developed under another NSF award. Combined with other ecological data, the image data offer the promise of addressing big questions about animal ecology, behavior, and conservation - who? where? when? what? and why? - at high resolution and at fine-grained scale, across landscapes and ecosystems, from an individual animal to regional and global systems. As part of this project, undergraduate and graduate students from ecology and computer science at four institutions will produce and test the application interface, and will develop a suite of companion applications and training tools to allow greater involvement of citizen scientists.<br/><br/>These tools will allow NEON to connect its database to data derived from large volumes of animal photographic images. Although this is primarily a proof of concept proposal focused on connecting whale shark images to NEONs atmospheric data, it will provide the means to be able to apply IBEIS algorithms and databases on images of distinctly marked North American species such as tortoises, monarch butterflies, salamanders, spotted skunk, bobcat, lynx, and humpback whales, thereby connecting these to NEON?s other data streams related to organisms, land use, hydrology and biogeochemistry. The proposed suite of tools includes: 1. an infrastructure and a mechanism for collecting images from scientists, automated remote cameras, citizen scientists and other sources; 2. a data management system for storing, accessing and manipulating images and derived data; 3. computer vision techniques for extracting information from the images about the identity of species and individual animals, as well as techniques for combining that information with other relevant data to derive information about ecological units such as animals, populations, species, and habitats; 4. a software application-program interface integrating the image and derived data with and within NEON; 5. a framework for engaging citizen scientists in data collection, derived science, and interaction with nature. Previous funding from NSF allowed building and testing of an IBEIS prototype.  This project will focus on the detection and identification methods for the identifiable US species, on integrating the system with NEON, and on scaling the system to many thousands of daily images from a variety of sources."
262,1547991,I-Corps: Automating the Development Process of Mobile Applications,IIP,I-Corps,7/15/15,7/13/15,Christoph Csallner,TX,University of Texas at Arlington,Standard Grant,Steven Konsek,12/31/16,"$50,000.00 ",,Csallner@uta.edu,"701 S Nedderman Dr, Box 19145",Arlington,TX,760190145,8172722105,ENG,8023,8023,$0.00 ,"Developing a professional mobile phone application is cumbersome and expensive in practice. To create for such an application an optimized user interface, professional graphic artists (i.e., non-programmers) design each screen of the user interface, for example, as pixel-based designs in Photoshop. Programmers then manually convert these screen designs to application source code. This conversion is labor-intensive and typically needs to be done several times during a project, as during a project user interface designs are often optimized iteratively. This project explores the commercialization of the first technique to automatically perform this conversion from pixel-based screen design drawing to working user interface source code. The project is expected to benefit both professional mobile application developers and students. The tool may save professional developers time and teach students how screen elements can be implemented in source code.<br/><br/>To satisfy consumer demand for customized user interface (UI) designs, mobile application UIs often deviate from standard UI components and provide their own UI designs. While many graphical UI builder tools exist, they are not well suited for such custom UI designs and expert graphic designers prefer pixel-based design tools. On a given screen design graphic, the project team's prototype tool identifies UI elements such as images, text, containers, and lists, via computer vision and optical character recognition (OCR) techniques. The prototype further infers a suitable UI hierarchy and exports the results as source code that is ready for compilation and execution on mobile computing devices. The generated UI closely mimics the UI of a corresponding real application, both in terms of pixel-by-pixel screenshot similarity and by the similarity of their runtime UI code structures. During the I-Corps program, the team will focus on contacting potential customers, gathering their feedback, and using it to further enhance the proposed prototype."
263,1453580,CAREER: A Data-Driven Network Inference Framework for Context-Conditioned Protein Interaction Graphs,IIS,Info Integration & Informatics,8/15/15,6/7/17,Yanjun Qi,VA,University of Virginia Main Campus,Continuing Grant,Sylvia Spengler,7/31/20,"$496,556.00 ",,yq2h@virginia.edu,P.O.  BOX 400195,CHARLOTTESVILLE,VA,229044195,4349244270,CSE,7364,"1045, 7364, 9102",$0.00 ,"Biological systems can be studied as graphs, where nodes represent entities (e.g., proteins) and edges represent interactions (e.g., physical binding, functional dependency). The identification of important protein interaction networks enables new insights into principles of life, evolution change, disease study, and drug development. The network wiring and function of a protein interaction graph is determined by context: genetics, environment, and small molecules such as drugs. However, almost all protein interaction networks, to date, have been examined under a single static condition, due to limitations of biotechnologies for graph data collection. Therefore, the research objective of this proposal is to design novel and efficient machine-learning algorithms to identify context-specific protein interaction graphs. Identifying context-specific protein networks has biomedical applications of social importance, such as studying cellular developments across multiple cell stages or investigating cellular changes with different drug treatments in the context of leukemia. Both applications will be explored as evaluation components of the project through collaborating with the Center for Public Health Genomics and the Emily Couric Cancer Center at UVA School of Medicine. The proposed research is expected to impact other domains as well, for instance, social-network discovery and condition-specific network inference for brain connectivity. The proposed career plan will result in educational and outreach initiatives that build on the interdisciplinary nature of the research. These plans include: (a) designing new course projects that work on real-life network-inference problems and data; (b) developing novel instructional techniques to train graduate students professional skills such as ""how to teach'' or ""how to do research"" using state-of-the-art structural learning problems as sample projects; (c) involving undergraduates in network learning research through UVA undergraduate capstone projects; (d) increasing awareness of graph-learning research among K-12 students through presentations at the UVA Introduction to Engineering (ITE) Program involving high school students; and (e) enhancing interactions with the UVA Medical School Community, especially through public release and tutorials of computational tools created from this project.<br/><br/>The past decade has seen a revolution in genomic technologies that enable the simultaneous measurement of thousands of molecular entities (e.g., genes or proteins). The flood of genome-wide data generated by next-generation sequencing technologies has provided an unprecedented coverage of large-scale, context-conditioned signatures of relevant gene products that have great potential to infer network connectivity and function in each context.  The proposal will develop a suite of novel machine-learning methods for inference of context-specific networks from multi-context molecular signature datasets that are high dimensional, heterogeneous and noisy. Aiming to overcome these data challenges, the proposed research includes the following three related tasks: (i) develop new and scalable structural learning algorithms to estimate multiple different but related sparse Gaussian Graphical Models (sGGMs) from data samples aggregated across multiple distinct conditions, (ii) develop novel learning strategies for modeling and detecting modules (i.e., multi-protein groups) within the framework of multitasking sGGMs, (iii) extend the above structural learning models to non-Gaussian cases, semi-supervised settings considering partial-observed networks and supervised disease diagnosis settings. <br/><br/>Additional information about the project, including the publications, open-source implementations of algorithms, data sets and educational materials will be shared through the project website: http://www.cs.virginia.edu/yanjun/context_graph/"
264,1550880,EAGER-NEON: Image-Based Ecological Information System (IBEIS) for Animal Sighting Data for NEON,EF,MacroSysBIO & NEON-Enabled Sci,9/1/15,8/7/15,Charles Stewart,NY,Rensselaer Polytechnic Institute,Standard Grant,Michelle Elekonich,8/31/17,"$105,999.00 ",,stewart@cs.rpi.edu,110 8TH ST,Troy,NY,121803522,5182766000,BIO,7959,"7916, 7959, 9178, 9179",$0.00 ,"The National Ecological Observatory Network (NEON) is coming online and will provide atmospheric and ecological data locally, regionally and continent wide. At the same time, images are rapidly becoming the most abundant, widely available, and cheapest source of information about the natural world, especially about animals. This project will extend NEON's data, scientific, and citizen science capacity with image-based animal sighting data to scalably collect, manage, and analyze data for individually identifiable wildlife using the Image-Based Ecological Information System (IBEIS) prototype recently developed under another NSF award. Combined with other ecological data, the image data offer the promise of addressing big questions about animal ecology, behavior, and conservation - who? where? when? what? and why? - at high resolution and at fine-grained scale, across landscapes and ecosystems, from an individual animal to regional and global systems. As part of this project, undergraduate and graduate students from ecology and computer science at four institutions will produce and test the application interface, and will develop a suite of companion applications and training tools to allow greater involvement of citizen scientists.<br/><br/>These tools will allow NEON to connect its database to data derived from large volumes of animal photographic images. Although this is primarily a proof of concept proposal focused on connecting whale shark images to NEONs atmospheric data, it will provide the means to be able to apply IBEIS algorithms and databases on images of distinctly marked North American species such as tortoises, monarch butterflies, salamanders, spotted skunk, bobcat, lynx, and humpback whales, thereby connecting these to NEON?s other data streams related to organisms, land use, hydrology and biogeochemistry. The proposed suite of tools includes: 1. an infrastructure and a mechanism for collecting images from scientists, automated remote cameras, citizen scientists and other sources; 2. a data management system for storing, accessing and manipulating images and derived data; 3. computer vision techniques for extracting information from the images about the identity of species and individual animals, as well as techniques for combining that information with other relevant data to derive information about ecological units such as animals, populations, species, and habitats; 4. a software application-program interface integrating the image and derived data with and within NEON; 5. a framework for engaging citizen scientists in data collection, derived science, and interaction with nature. Previous funding from NSF allowed building and testing of an IBEIS prototype.  This project will focus on the detection and identification methods for the identifiable US species, on integrating the system with NEON, and on scaling the system to many thousands of daily images from a variety of sources."
265,1536003,AitF: FULL: Collaborative Research: PEARL: Perceptual Adaptive Representation Learning in the Wild,CCF,Algorithms in the Field,9/1/15,8/14/15,Trevor Darrell,CA,University of California-Berkeley,Standard Grant,A. Funda Ergun,8/31/19,"$200,000.00 ",,trevor@eecs.berkeley.edu,Sponsored Projects Office,BERKELEY,CA,947101749,5106433891,CSE,7239,012Z,$0.00 ,"Vast amounts of digitized images and videos are now commonly available, and the advent of search engines has further facilitated their access. This has created an exceptional opportunity for the application of machine learning techniques to model human visual perception. However, the data often does not conform to the core assumption of machine learning that training and test images are drawn from exactly the same distribution, or ""domain."" In practice, the training and test distributions are often somewhat dissimilar, and distributions may even drift with time. For example, a ""dog"" detector trained on Flickr may be tested on images from a wearable camera, where dogs are seen in different viewpoints and lighting conditions. The problem of compensating for these changes--the domain adaptation problem--must therefore be addressed both in theory and in practice for algorithms to be effective. This problem is not just a second-order effect and its solution does not constitute a small increase in performance.  Ignoring it can lead to dramatically poor results for algorithms ""in the field.""<br/><br/>This project will develop a core suite of theory and algorithms for PErceptual Adaptive Representation Learning (PEARL), which, when given a new task domain, and previous experience with related tasks and domains, will provide a learning architecture likely to achieve optimal generalization on the new task. We expect PEARL to have a significant impact on the research community by providing a much-needed theoretical and computational framework that takes steps toward unifying the subfields of domain adaptation theory and domain adaptation practice. Our theoretical and practical advancements will impact many application areas by allowing the use of pre-trained perceptual models (visual and otherwise) in new situations and across space and time. For example, in mobile technology and robotics, PEARL will help personal assistants and robots better adapt their perceptual interfaces to individual users and particular situated environments.  At the core of this project are three main research thrusts: 1) making theoretical advances for domain adaptation by developing generalized discrepancy distance minimization; 2) using the theoretical guarantees of generalized discrepancy distance to develop algorithms for key adaptation scenarios of deep perceptual representation learning, domain adaptation with active learning, and time-dependent adaptation; 3) advancing the theory and developing algorithms for the multiple-source adaptation scenario. In addition to our core aims, we plan to implement our algorithms within a scalable open-source framework, and evaluate our algorithms on large-scale visual data sets."
266,1551290,"CAREER:  Discriminative Spatiotemporal Models for Recognizing Humans, Objects, and their Interactions",IIS,Robust Intelligence,9/1/15,9/21/15,Deva Ramanan,PA,Carnegie-Mellon University,Continuing grant,Jie Yang,5/31/16,"$106,011.00 ",,deva@cs.cmu.edu,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,CSE,7495,"1045, 1187",$0.00 ,"One of the goals of computer vision is to build a system that can see people and recognize their activities. Human actions are rarely performed in isolation -- the surrounding environment, nearby objects, and nearby humans affect the nature of the performed activity.<br/>Examples include actions such as ""eating"" and ""shaking hands."" The research goal of this project is to approach human performance in understanding videos of activities defined by human-object and human-human interactions.<br/><br/>This project makes use of structured, contextual representations to make predictions given spatiotemporal data. It does so by extending recent successful work on object recognition to the space-time domain, introducing extensions for spatiotemporal grouping and contextual modeling. Video enables the extraction of additional dynamic cues absent in static images, but this poses additional computational burdens that are addressed through algorithmic innovations for approximate parsing and large-scale discriminative learning.<br/><br/>To place activity recognition on firm quantitative ground, the proposed models are evaluated using concrete metrics based on activities of daily living (ADL) and human proxemic models from the medical and anthropological communities. Examples include systems for automated monitoring of stroke patients interacting with everyday objects and automated analysis of crisis response team interactions during emergency drills. This project produces non-scripted, real-world, labeled action recognition datasets, of benefit to the research community as a whole."
267,1546993,EAGER: Cybermanufacturing: Defending Side Channel Attacks in Cyber-Physical Additive Layer Manufacturing Systems,CNS,"CYBER-PHYSICAL SYSTEMS (CPS), ENG INTERDISC RES (IDR)",10/1/15,8/28/15,Mohammad Al Faruque,CA,University of California-Irvine,Standard Grant,David Corman,9/30/18,"$200,000.00 ",,alfaruqu@uci.edu,"141 Innovation Drive, Ste 250",Irvine,CA,926173213,9498247295,CSE,"7918, 7951","7916, 7918",$0.00 ,"Cyber-physical additive layer manufacturing, e.g., 3D printing, has become a promising technology for providing cost, time, and space effective solution by reducing the gap between designers and manufacturers. However, the concern for the protection of intellectual property is arising in conjunction with the capabilities of supporting massive innovative designs and rapid prototyping. Intellectual property in the additive layer manufacturing system consists of: i) geometric design of an object; ii) attributes of an object; iii) process information; and iv) machine information. This EArly-concept Grant for Exploratory Research (EAGER) project seeks to develop defense mechanisms for detecting malware and counterfeit articles using a variety of signals that are observed during the manufacturing process including acoustic, temperature, power, and others.  The project is an EAGER because both the uniqueness of the observed signal signatures, and their utilization in securing the manufacturing process are high risk with potential for high reward in thwarting attacks.<br/><br/>This project will demonstrate that during the life-cycle of the additive layer manufacturing system, the intellectual property information contained in the cyber domain can be recovered/reconstructed through attacks occurring during the manufacturing process in the physical domain through various non-intrusive techniques. It will then focus on creating both machine-dependent and machine-independent defense mechanisms for avoiding such an attack. This project will significantly impact US competitiveness over technology-oriented manufacturing. The attack model will provide feedback to 3D printer manufacturers and CAD tool designers to build defenses against these new types of attack. Moreover, it will have a significant societal impact to the explosively growing maker and crowd-sourcing community in protecting their intellectual property. In addition, the project's approach can be used in other manufacturing systems, e.g., CNC machines, manufacturing robots, etc. This is possibly the very first approach to create defense for additive layer manufacturing mechanisms against such attacks occurring in the physical domain to get access to information of the cyber domain.  This project has three specific objectives: 1) It will demonstrate a proof of concept by presenting a novel attack model constructed using a combination of machine learning, signal processing, and pattern recognition techniques that utilize the side-channel information (power, temperature, acoustic, electromagnetic emission) obtained during the manufacturing process. 2) It will develop a machine-specific defense mechanism against the attack model for the 3D printer. New techniques to add additional physical process encryption, e.g. adding extra information to the G-code to obfuscate the printing process from the attack model between the G-code and the physical manufacturing process, will be demonstrated. 3) It will create a new security-aware 3D-printing algorithm for the machine-independent CAD tools that can protect against such side channel attacks. The 3D-printing algorithm will slice the STL and generate layer description language (e.g. G-code) randomly so that for the same 3D object, different instructions will be sent to the 3D printer and eventually different physical features will be extracted by the attackers."
268,1545995,BIGDATA: F: New Algorithms of Online Machine Learning for Big Data,IIS,Big Data Science &Engineering,9/1/15,9/8/15,Tianbao Yang,IA,University of Iowa,Standard Grant,Wei Ding,8/31/19,"$712,401.00 ",Padmini Srinivasan,tianbao-yang@uiowa.edu,2 GILMORE HALL,IOWA CITY,IA,522421320,3193352123,CSE,8083,"7433, 8083, 9150",$0.00 ,"This project is developing innovative, theoretically rigorous algorithms to learn from continuously arriving (streaming) data.  Specific challenges addressed are class imbalance (one of the concepts to be learned is very rare, as in disease detection), cost constraints on both obtaining features (e.g., computationally expensive image processing), and cost constraints on obtaining class labels (e.g., human annotation.)  The algorithms developed in this project make it possible to effectively address big data challenges in streaming data due to increased complexities in various aspects such as heavily imbalanced data distributions, ultrahigh dimensional features, a large number of labels, highly complex constraints, etc.  The project will also contribute to training future professionals in big data analytics, including participation in the University of Iowa's undergraduate summer research program and high school student training program.<br/><br/>Most work devoted to online learning algorithms and their analysis were developed with the goal of minimizing a symmetric measure (e.g., the classification error) and without considering practical constraints arising in big data.  This project addresses imbalanced data by developing online learning algorithms for minimizing asymmetric measures including F-score, area under the ROC curve, and area under precision and recall curve.  Convex or non-convex surrogate loss functions that well-approximate these asymmetric measures are constructed and minimized in an online fashion. The project also develops online algorithms under three types of constraints arising in big data context namely constraints on computing costs, on query costs, and complex inequality constraints, by exploring techniques in randomized algorithms, active learning and convex optimization.  The developed algorithms are being evaluated in real applications including biomedical semantic indexing, social media mining, and image annotation."
269,1534780,SBIR Phase II: A Student Centered Adaptive Learning Engine,IIP,SBIR Phase II,9/1/15,5/23/19,Mary Blink,PA,"TutorGen, Inc.",Standard Grant,Rajesh Mehta,9/30/19,"$1,195,999.00 ",,mjblink@tutorgen.com,505 Linden Ct,Mars,PA,160467167,7046992541,ENG,5373,"115E, 116E, 1654, 165E, 169E, 5373, 7218, 8031, 8032, 8039, 8240, 9102, 9150, 9231, 9251",$0.00 ,"This SBIR Phase II project represents a revolutionary advance in adaptive educational technology systems by using data collected from previous exercises to automatically generate hints and feedback for students. This work addresses the SBIR Educational Technologies and Applications subtopic EA5 - Learning and Assessment by making adaptive learning widely available and by providing tools to assess student performance in order to make interventions as early as possible and help students succeed. While it is well known that adaptive Computer Based Training (CBT) is more efficient and more effective, it has traditionally been cost prohibitive to produce in most domains. By leveraging the latest research in Big Data analytics and Educational Data Mining (EDM), this project will produce adaptive capabilities automatically, dramatically reducing the costs of producing more effective training and making such training widely available. The core customers for this technology will be providers of training systems. This includes publishing organizations, developers of software tools for education, and providers of corporate and government training. Institutions that are struggling to educate students, particularly across STEM (Science, Technology, Engineering, and Mathematics) fields, will be able to use this technology in their existing teaching systems and thus improve student engagement and performance.<br/><br/>The final outcome of this project will be an integrated set of software tools that collect data from existing computer/web based training software, and automatically generate adaptive capabilities to create a personalized learning environment for students. It does this by using novel EDM and machine learning techniques to build and organize student and problem models that improve over time as more data is collected. It also provides for the tracking of student progress on specific concepts or skills (knowledge tracing), allowing for easy assessment at any point in time. The system also dynamically selects the students' next problems to maximize student learning and minimize time needed to master a set of skills (problem selection). For complex multi-step problems, this system will provide context-specific, just-in-time hints to help students as they learn. The final product will include data adapters that allow developers of existing software to seamlessly connect with this system. Finally, a main differentiator of this system is the transparent process of data curation and the related visualization tools that expose the problem- and student-model generation process. This combination of human input and machine learning will provide researchers, developers, and educators tools to explore student data and allow for new insights into how students learn."
270,1538389,Remote Infrastructure Monitoring Assessment via Multispectral Imaging of Surface Coatings,CMMI,"Dynamics, Control and System D",9/1/15,8/26/15,Ivan Bartoli,PA,Drexel University,Standard Grant,Jordan Berg,8/31/20,"$430,000.00 ","Antonios Kontsos, Matthew McCarthy",ib77@drexel.edu,"1505 Race St, 10th Floor",Philadelphia,PA,191021119,2158955849,ENG,7569,"030E, 031E, 032E, 033E, 034E, 035E, 7237, 8024",$0.00 ,"Our country's aging and deteriorating infrastructure creates the need for transformative science and technology that can assist in novel management and maintenance approaches. This award supports fundamental research on monitoring, measurement, analysis and computations in support of system diagnostics, applicable to a civil infrastructure. Examples include bridges, railroads, masonry buildings, networks of pipelines, powerlines, dams and more. This award investigates multispectral imaging through novel coatings that act as selectively responsive sensing units when deposited on actual structural components and monitored by appropriate imaging devices. The results of this award could be used to implement unprecedented structural health monitoring procedures which combined with the use of unmanned aerial systems developed to perform such measurements in highly dynamic environments have the potential to radically reduce downtime associated with lengthy and costly maintenance operations. In addition, they may offer the capability of both earlier and more quantitative identification of deterioration and damage. Therefore, results of this research are expected to benefit the U.S economy and society as it targets major improvements in manufacturing, monitoring and analysis methods with applications in urban systems, sustainability and resiliency of infrastructure. The broader impact of this research focuses in enhancing the efforts for cross-cutting progress in restoration and improvement of urban infrastructure. In this context of civic engagement, the research will leverage resources within the PIs' institution co-operative educational program to transfer related knowledge in educational curricula and train the next generation of engineers working in the area of system diagnostics.<br/><br/>Multimodal remote sensing instrumentation platform coupled with scalable manufacturing and autonomous aerial vehicles will create a framework to rapidly assess the condition of infrastructure systems. Three research objectives in this project are targeted: the use of novel manufacturing procedures in a context of a materials-by-design and advances in metamaterials for the creation of unprecedented opportunities for scalable, tunable, multimodal, multispectral and embedded coatings for automated inspection and deformation quantification; the use of unmanned aerial systems equipped with multi-spectral sensors capable to measure deformation through interrogation of the coatings; the development of methodologies to combine multispectral image point clouds, as well as other sources of information with multiscale and multimodal modeling for state awareness and remaining useful life predictions. Specifically, novel manufacturing procedures will be adopted to design surface coatings and embed them in structural components. The properties of the coatings will be tailored to guarantee their rapid detection by multispectral sensing capturing ultraviolet and infrared radiation as well as visible light. The relative position of the coating elements will be tracked using dedicated algorithms based on computer vision and photogrammetry used to quantify local and global position coordinates of the monitored structures."
271,1533768,XPS: FULL: DSD: A Parallel Tensor Infrastructure (ParTI!) for Data Analysis,CCF,Exploiting Parallel&Scalabilty,9/1/15,6/23/20,Richard Vuduc,GA,Georgia Tech Research Corporation,Standard Grant,Wei Ding,8/31/21,"$750,000.00 ","Jimeng Sun, Chao Zhang",richie@cc.gatech.edu,Office of Sponsored Programs,Atlanta,GA,303320420,4048944819,CSE,8283,,$0.00 ,"This project concerns efficient parallel algorithms and software for<br/>emerging and future data analysis and mining applications, based on an<br/>emerging class of techniques known as tensor networks. Tensors, which<br/>are higher-dimensional generalizations of matrices, are finding<br/>applications in signal and image processing, computer vision,<br/>healthcare analytics, and neuroscience, to name just a few. Yet<br/>despite this demand, there is no comprehensive, high-performance<br/>software infrastructure targeting server systems that may have many<br/>parallel processors. Thus, the overarching research goal of this<br/>project is to design the first such infrastructure. The resulting<br/>prototype will be an open-source package, called the Parallel Tensor<br/>Infrastructure, or ParTI! The broader impact of the ParTI! project is<br/>to make the use of tensors, in a variety of data processing domains,<br/>much easier to do and more widespread.<br/><br/>The ParTI! project will focus specifically on algorithmic and software<br/>support for sparse tensors on single-node multi- and many-core<br/>accelerated platforms. The technical approach relies on a specific way<br/>of representing tensors, referred to as tensor networks.  A tensor<br/>network is an efficient approach for representing the structure of a<br/>high-order tensor or tensor factorization. It can used by the data<br/>analyst as a simple, high-level way to express the specific structure<br/>or relationships he or she seeks in the data that the tensor<br/>represents. However, a tensor network is not just a tool for the<br/>analyst; it is also an abstract intermediate form, from which it is<br/>possible to derive algorithms, express and manage parallelism, and<br/>semi-automatically generate tensor processing software. This insight,<br/>combined with well-known data layout and communication-avoiding<br/>parallelization techniques from high-performance sparse linear<br/>algebra, is what will enable a ParTI! for tensor-based data<br/>analysis. The project will show the utility of this approach by<br/>evaluating the ParTI! prototype on real data sets and systems, through<br/>collaborations with government research laboratory and industry<br/>partners.<br/><br/>For further information see the project web site at: parti-project.org"
272,1550853,EAGER-NEON: Image-Based Ecological Information System (IBEIS) for Animal Sighting Data for NEON,EF,MacroSysBIO & NEON-Enabled Sci,9/1/15,8/7/15,Tanya Berger-Wolf,IL,University of Illinois at Chicago,Standard Grant,Michelle Elekonich,8/31/18,"$144,001.00 ",,berger-wolf.1@osu.edu,809 S. Marshfield Avenue,Chicago,IL,606124305,3129962862,BIO,7959,"7916, 7959, 9178, 9179",$0.00 ,"The National Ecological Observatory Network (NEON) is coming online and will provide atmospheric and ecological data locally, regionally and continent wide. At the same time, images are rapidly becoming the most abundant, widely available, and cheapest source of information about the natural world, especially about animals. This project will extend NEON's data, scientific, and citizen science capacity with image-based animal sighting data to scalably collect, manage, and analyze data for individually identifiable wildlife using the Image-Based Ecological Information System (IBEIS) prototype recently developed under another NSF award. Combined with other ecological data, the image data offer the promise of addressing big questions about animal ecology, behavior, and conservation - who? where? when? what? and why? - at high resolution and at fine-grained scale, across landscapes and ecosystems, from an individual animal to regional and global systems. As part of this project, undergraduate and graduate students from ecology and computer science at four institutions will produce and test the application interface, and will develop a suite of companion applications and training tools to allow greater involvement of citizen scientists.<br/><br/>These tools will allow NEON to connect its database to data derived from large volumes of animal photographic images. Although this is primarily a proof of concept proposal focused on connecting whale shark images to NEONs atmospheric data, it will provide the means to be able to apply IBEIS algorithms and databases on images of distinctly marked North American species such as tortoises, monarch butterflies, salamanders, spotted skunk, bobcat, lynx, and humpback whales, thereby connecting these to NEON?s other data streams related to organisms, land use, hydrology and biogeochemistry. The proposed suite of tools includes: 1. an infrastructure and a mechanism for collecting images from scientists, automated remote cameras, citizen scientists and other sources; 2. a data management system for storing, accessing and manipulating images and derived data; 3. computer vision techniques for extracting information from the images about the identity of species and individual animals, as well as techniques for combining that information with other relevant data to derive information about ecological units such as animals, populations, species, and habitats; 4. a software application-program interface integrating the image and derived data with and within NEON; 5. a framework for engaging citizen scientists in data collection, derived science, and interaction with nature. Previous funding from NSF allowed building and testing of an IBEIS prototype.  This project will focus on the detection and identification methods for the identifiable US species, on integrating the system with NEON, and on scaling the system to many thousands of daily images from a variety of sources."
273,1535797,AitF:  FULL: Collaborative Research:   PEARL: Perceptual Adaptive Representation Learning in the Wild,CCF,Algorithms in the Field,9/1/15,8/14/15,Kate Saenko,MA,University of Massachusetts Lowell,Standard Grant,Tracy Kimbrel,4/30/17,"$200,000.00 ",,saenko@bu.edu,Office of Research Admin.,Lowell,MA,18543692,9789344170,CSE,7239,012Z,$0.00 ,"Vast amounts of digitized images and videos are now commonly available, and the advent of search engines has further facilitated their access. This has created an exceptional opportunity for the application of machine learning techniques to model human visual perception. However, the data often does not conform to the core assumption of machine learning that training and test images are drawn from exactly the same distribution, or ""domain."" In practice, the training and test distributions are often somewhat dissimilar, and distributions may even drift with time. For example, a ""dog"" detector trained on Flickr may be tested on images from a wearable camera, where dogs are seen in different viewpoints and lighting conditions. The problem of compensating for these changes--the domain adaptation problem--must therefore be addressed both in theory and in practice for algorithms to be effective. This problem is not just a second-order effect and its solution does not constitute a small increase in performance.  Ignoring it can lead to dramatically poor results for algorithms ""in the field.""<br/><br/>This project will develop a core suite of theory and algorithms for PErceptual Adaptive Representation Learning (PEARL), which, when given a new task domain, and previous experience with related tasks and domains, will provide a learning architecture likely to achieve optimal generalization on the new task. We expect PEARL to have a significant impact on the research community by providing a much-needed theoretical and computational framework that takes steps toward unifying the subfields of domain adaptation theory and domain adaptation practice. Our theoretical and practical advancements will impact many application areas by allowing the use of pre-trained perceptual models (visual and otherwise) in new situations and across space and time. For example, in mobile technology and robotics, PEARL will help personal assistants and robots better adapt their perceptual interfaces to individual users and particular situated environments.  At the core of this project are three main research thrusts: 1) making theoretical advances for domain adaptation by developing generalized discrepancy distance minimization; 2) using the theoretical guarantees of generalized discrepancy distance to develop algorithms for key adaptation scenarios of deep perceptual representation learning, domain adaptation with active learning, and time-dependent adaptation; 3) advancing the theory and developing algorithms for the multiple-source adaptation scenario. In addition to our core aims, we plan to implement our algorithms within a scalable open-source framework, and evaluate our algorithms on large-scale visual data sets."
274,1456793,WiFiUS: Collaborative Research: Sequential Inference and Learning for Agile Spectrum Use,CNS,"Special Projects - CNS, Special Projects - CCF",3/15/15,3/11/15,Harold Vincent Poor,NJ,Princeton University,Standard Grant,Monisha Ghosh,2/28/18,"$135,000.00 ",,poor@princeton.edu,Off. of Research & Proj. Admin.,Princeton,NJ,85442020,6092583090,CSE,"1714, 2878","7363, 8229",$0.00 ,"A key imperative to expanding future wireless services is to overcome the spectral crunch. At present, static allocation and rigid regulation lead to underutilization of available spectral resources. Flexible spectrum use aims at exploiting under-utilized spectrum. Available spectrum opportunities may be non-contiguous, scattered over a large bandwidth, and are available locally and for a limited period of time due to the highly dynamic nature of wireless transmissions. This fuels the need to understand how to discover, assess and utilize the time-frequency-location varying spectral resources efficiently and with minimal delay. Moreover, it is critical to access identified idle spectrum in an agile manner.<br/><br/>This project will design sequential inference and learning algorithms for agile spectrum access when the state of the spectrum varies rapidly. The key advantage of sequential algorithms, as compared to block-wise algorithms, is that they typically lead to significantly reduced decision delays. The overarching goal of this project is to design sequential inference and learning algorithms for agile spectrum utilization. In particular, this project will employ advanced sequential inference and learning methods for the following three interconnected yet increasingly sophisticated and demanding tasks: 1) to employ sequential reinforcement learning and sequential inference algorithms to design sensing policies for rapid spectrum opportunities discovery; 2) to design sequential algorithms for fast and accurate spectrum quality assessment; and 3) to build, maintain and exploit an interference map of the area where our network operates and represent it as a spatial potential field. The proposed research is expected to make substantial contributions to both applications and theory. On the application level, the proposed research has the potential to substantially improve spectral efficiency by introducing novel tools from sequential analysis, machine learning and statistical inference for the design of spectrum discovery, assessment and exploitation policies. On the theoretical level, the proposed project will advance the state of the art in sequential analysis and contribute new approaches to the general methodological base for optimal stopping, control and machine learning problems. Furthermore, new methods and theory of modeling and exploiting knowledge of interference using spatial potential fields, sequential statistics and advanced propagation modeling will be developed."
275,1545974,"SHF: Conference: Hardware and Algorithms for Learning On-a-chip; November 5, 2015; Austin, TX",CCF,SOFTWARE & HARDWARE FOUNDATION,7/15/15,7/14/15,Yu Cao,AZ,Arizona State University,Standard Grant,Sankar Basu,6/30/16,"$15,000.00 ",,Yu.Cao@asu.edu,ORSPA,TEMPE,AZ,852816011,4809655479,CSE,7798,7945,$0.00 ,"Support for student participation in a workshop to establish a forum for discussion of the current practices, as well as future research needs in three interrelated subareas of brain related sciences is provided through this grant. The three specific subareas that the workshop will discuss are: hardware acceleration of on-chip learning; machine learning algorithms and neuro-inspired information processing; and nano-electronic design for learning, with the focus on performance and energy efficiency. The workshop will consist of three sessions and will be followed by a panel discussion, and will end with a poster session to present student work. The conference will not only provide a venue for researchers to address problems but also a platform for graduate students and others to learn about cutting-edge research by attending invited talks, presentations, tutorials in the general area.<br/><br/>The outcome of this workshop include a webpage hosted by the Arizona State University to disseminate the program and discussions, as well as a report that clearly identifies major research challenges and needs in computational neurosciences, machine learning, and hardware design. The papers and posters will be published through the workshop proceedings. These results will illustrate new and exciting inter-disciplinary research problems of interest to the related communities, and articulate a vision of conducting further research in this area. The final workshop report will also be submitted to NSF as an annual report and will  be released online for sharing with the research community in general."
276,1457076,WiFiUS: Collaborative Research: Sequential Inference and Learning for Agile Spectrum Use,CNS,"SPECIAL PROJECTS - CISE, SPECIAL PROJECTS - CCF",3/15/15,3/11/15,Lifeng Lai,MA,Worcester Polytechnic Institute,Standard Grant,Wenjing Lou,10/31/16,"$135,000.00 ",,lflai@ucdavis.edu,100 INSTITUTE RD,WORCESTER,MA,16092247,5088315000,CSE,"1714, 2878","7363, 8229",$0.00 ,"A key imperative to expanding future wireless services is to overcome the spectral crunch. At present, static allocation and rigid regulation lead to under utilization of available spectral resources. Flexible spectrum use aims at exploiting under-utilized spectrum. Available spectrum opportunities may be non-contiguous, scattered over a large bandwidth, and are available locally and for a limited period of time due to the highly dynamic nature of wireless transmissions. This fuels the need to understand how to discover, assess and utilize the time-frequency-location varying spectral resources efficiently and with minimal delay. Moreover, it is critical to access identified idle spectrum in an agile manner.<br/><br/>This project will design sequential inference and learning algorithms for agile spectrum access when the state of the spectrum varies rapidly. The key advantage of sequential algorithms, as compared to block-wise algorithms, is that they typically lead to significantly reduced decision delays. The overarching goal of this project is to design sequential inference and learning algorithms for agile spectrum utilization. In particular, this project will employ advanced sequential inference and learning methods for the following three interconnected yet increasingly sophisticated and demanding tasks: 1) to employ sequential reinforcement learning and sequential inference algorithms to design sensing policies for rapid spectrum opportunities discovery; 2) to design sequential algorithms for fast and accurate spectrum quality assessment; and 3) to build, maintain and exploit an interference map of the area where our network operates and represent it as a spatial potential field. The proposed research is expected to make substantial contributions to both applications and theory. On the application level, the proposed research has the potential to substantially improve spectral efficiency by introducing novel tools from sequential analysis, machine learning and statistical inference for the design of spectrum discovery, assessment and exploitation policies. On the theoretical level, the proposed project will advance the state of the art in sequential analysis and contribute new approaches to the general methodological base for optimal stopping, control and machine learning problems. Furthermore, new methods and theory of modeling and exploiting knowledge of interference using spatial potential fields, sequential statistics and advanced propagation modeling will be developed."
277,1539099,VEC: Small: Collaborative Research: Scene Understanding from RGB-D Images,IIS,"INFORMATION TECHNOLOGY RESEARC, IIS SPECIAL PROJECTS",9/1/15,8/23/16,Jitendra Malik,CA,University of California-Berkeley,Continuing grant,Jie Yang,8/31/18,"$160,662.00 ",Alexei Efros,malik@cs.berkeley.edu,Sponsored Projects Office,BERKELEY,CA,947101749,5106433891,CSE,"1640, 7484",002Z,$0.00 ,"This project exploits the benefits of RGB-D (color and depth) image collections with extra depth information to significantly advance the state-of-the-art in visual scene understanding, and makes computer vision techniques become usable in practical applications. Recent advance in affordable depth sensors has made depth acquisition significantly easier for ordinary users. These depth cameras are becoming very common in digital devices and help automatic scene understanding. The research team develops technologies to take advantage of depth information. Besides the published research results, the research team plans to distribute source code and benchmark data sets that could benefit researchers in a variety of disciplines. This project is integrated with educational programs, such as interdisciplinary workshops and courses at the graduate, undergraduate, and professional levels and diversity enhancement programs that promote opportunities for disadvantaged groups. The research team is closely collaborating with the industrial partner (Intel), involving interns and technology transfer in real products. The project is also applying the developed algorithms to the assistive technology for the blind and visually impaired.<br/><br/>This research develops algorithms required to perform real-time segmentation, labeling, and recognition of RGB-D images, videos, and 3D scans of indoor environments. Specifically, the PIs develop methods to: (1) acquire large labeled RGB-D datasets for training and evaluation, (2) study algorithms to recognize objects and estimate detailed 3D knowledge about the scene, (3) exploit the object-to-object contextual relationships in 3D, and (4) demonstrate applications to benefit the general public, including household robotics and assistive technologies for the blind."
278,1527340,"RI: Small: Object Detection, Pose Estimation, and Semantic Segmentation Using 3D Wireframe Models",IIS,Robust Intelligence,9/1/15,4/13/18,Rene Vidal,MD,Johns Hopkins University,Continuing Grant,Jie Yang,8/31/19,"$458,322.00 ",,rvidal@cis.jhu.edu,1101 E 33rd St,Baltimore,MD,212182686,4439971898,CSE,7495,"7495, 7923, 9251",$0.00 ,"Humans can recognize a wide variety of object categories in an image, even in the midst of occlusion and clutter, and in spite of significant variability in the objects' shape, size, appearance, and pose. While automatic object recognition systems in computer vision have seen great progress over the past decade, state-of-the-art methods rely primarily on 2D representations of object categories, which cannot capture large variations in the objects' shape and pose. This project develops new 3D representations of objects that are general enough to describe the shape of a wide variety of object categories, yet simple enough so that they can be learned efficiently from a collection of 2D images, and used for detecting and segmenting objects in new images. This project can impact many research areas, including image search, autonomous navigation, medical diagnostic tools, surveillance and robotics.<br/><br/>This project develops a new class of 3D wireframe models for view-invariant object detection, fine-grained pose estimation, and semantic segmentation. A wireframe model is a sparse collection of 3D points, edges and surface normals defined only at a few points on the boundaries of the 3D object. The project develops methods for learning wireframe models from 2D images of multiple object categories, methods for learning deformable wireframe models that capture intra-class shape variations across object categories, methods for integrating appearance information into wireframe models, and methods for semantic segmentation based on wireframe models."
279,1539014,VEC: Small: Collaborative Research: Scene Understanding from RGB-D Images,IIS,Information Technology Researc,9/1/15,5/26/16,Jianxiong Xiao,NJ,Princeton University,Continuing Grant,Jie Yang,8/31/19,"$135,000.00 ",Thomas Funkhouser,xj@princeton.edu,Off. of Research & Proj. Admin.,Princeton,NJ,85442020,6092583090,CSE,1640,002Z,$0.00 ,"This project exploits the benefits of RGB-D (color and depth) image collections with extra depth information to significantly advance the state-of-the-art in visual scene understanding, and makes computer vision techniques become usable in practical applications. Recent advance in affordable depth sensors has made depth acquisition significantly easier for ordinary users. These depth cameras are becoming very common in digital devices and help automatic scene understanding. The research team develops technologies to take advantage of depth information. Besides the published research results, the research team plans to distribute source code and benchmark data sets that could benefit researchers in a variety of disciplines. This project is integrated with educational programs, such as interdisciplinary workshops and courses at the graduate, undergraduate, and professional levels and diversity enhancement programs that promote opportunities for disadvantaged groups. The research team is closely collaborating with the industrial partner (Intel), involving interns and technology transfer in real products. The project is also applying the developed algorithms to the assistive technology for the blind and visually impaired.<br/><br/>This research develops algorithms required to perform real-time segmentation, labeling, and recognition of RGB-D images, videos, and 3D scans of indoor environments. Specifically, the PIs develop methods to: (1) acquire large labeled RGB-D datasets for training and evaluation, (2) study algorithms to recognize objects and estimate detailed 3D knowledge about the scene, (3) exploit the object-to-object contextual relationships in 3D, and (4) demonstrate applications to benefit the general public, including household robotics and assistive technologies for the blind."
280,1512945,Collaborative Research: Generalized Fiducial Inference for Massive Data and High Dimensional Problems,DMS,STATISTICS,9/1/15,8/16/17,Thomas Chun Man Lee,CA,University of California-Davis,Continuing Grant,Gabor Szekely,8/31/19,"$150,000.00 ",,tcmlee@ucdavis.edu,OR/Sponsored Programs,Davis,CA,956186134,5307547700,MPS,1269,,$0.00 ,"R. A. Fisher, the father of modern statistics, proposed the idea of Fiducial Inference in the 1930s. While his proposal led to some interesting methods for quantifying uncertainty, other prominent statisticians of the time did not accept Fisher's approach because it went against the ideas of statistical inference of the time. Beginning around the year 2000, the PIs and collaborators started to re-investigate the ideas of fiducial inference and discovered that Fisher's approach, when properly generalized, would open doors to solve many important and difficult problems of uncertainty quantification. The PIs termed their generalization of Fisher's ideas as generalized fiducial inference. After many years of preliminary investigations, the PIs developed a coherent, well thought out plan for a systematic research program in this area. A large part of this project develops practical solutions for different modeling problems that have natural applications in diverse fields. Finance (volatility estimation) and measurement science (calibration of measurements from different government labs, for example, US NIST) are two primary examples, while others include gene expression data, climate problems, recommender systems, and computer vision. <br/><br/>This project is motivated by the success of generalized fiducial inference (GFI) as introduced by the PIs as a generalization of Fisher's fiducial argument. The PIs are now working towards scaling up their GFI methodology to handle big data problems and other difficult problems that have emerged due to our ability to collect massive amounts of data rapidly. In particular the PIs plan to conduct research into the following topics: (i) a thorough investigation of fundamental issues of GFI including connection with Approximate Bayesian Calculations and higher order asymptotics; (ii) sparse covariance estimation using GFI in the ""large p small n"" context; (iii) development of the idea of Fiducial Selector so that a sparsity of the fiducial distribution is induced as a natural outcome of a minimization problem; (iv) uncertainty quantification for the matrix completion problem using GFI, and (v) applications of GFI to a wide variety of practical problems, such as volatility estimation in finance and international key comparison experiments in measurement science."
281,1501499,A Simulation-Based Curriculum to Accelerate Math Remediation and Improve Degree Completion for STEM Majors,DUE,Advanced Tech Education Prog,5/1/15,4/3/20,Kathleen Offenholley,NY,CUNY Borough of Manhattan Community College,Standard Grant,Virginia Carter,4/30/21,"$875,794.00 ","Ching-Song Wei, Francesco Crocco",koffenholley@bmcc.cuny.edu,199 Chambers Street,New York,NY,100071079,2122208010,EHR,7412,"1032, 9178, SMET",$0.00 ,"Success in basic algebra is a major stumbling block for students seeking technical careers. Although it is often assumed that STEM majors start with calculus as their first college math course, this is not the case for many urban and minority community college students. This project is addressing the national problem of mathematics remediation for STEM majors by creating a game- and simulation-based algebra and trigonometry curriculum. The curriculum features three to five video games that place math content within real-world GIS scenarios.  The curriculum is used in a summer immersion program for in-coming Geographic Information Science (GIS) majors at the Borough of Manhattan Community College (BMCC). The project will impact STEM education at a national level by providing all materials free and open-source to secondary and post-secondary institutions via a project website with downloadable curricula, game software, video tutorials, professional development materials for faculty and staff, and support forums. The summer immersion program pairs MAT 056, the most common remedial math course for BMCC STEM majors, with GEO 100, the first course in the GIS sequence.  Students in the program are recruited from NYC high school graduates participating in BMCC's Science Technology Entry Program. After completing the summer program, students will receive special support services and internships in GIS to advance them into an articulating baccalaureate program at CUNY or in related industries. This project targets BMCC's large population of minority students (over 90%) and women students (nearly 60%). <br/><br/>Evidence-based research has shown gaming in the mathematics curriculum has the power to transform mathematics education from an intimidating and negative experience to one that is fun, engaging, and successful.  While there are many promising NSF-funded science games that are engaging and promote deep learning, there are very few math games that do so, and even fewer that specifically cover algebra topics in a way that can be appreciated by adult learners.  Success in basic algebra is a major stumbling block for students seeking technical careers.  This project will build on: 1) a previous NSF grant award to develop a GIS major at BMCC and 2) a Department of Education Title V grant for Hispanic-serving institutions to enhance e-learning initiatives at BMCC, including game-based learning.  Games and simulations will feature real-world applications that focus on skills and contexts necessary for a career in GIS. Results of this project will be disseminated through conference presentations and publications. Project outcomes will be assessed through in-game analytics, traditional quizzes and midterm exams, and the common department final exam for MAT 056. Department final exam scores and course retention rates will be compared with rates for the college as a whole, using a logistic regression analysis of statistically matched cohorts. In addition, differences by gender and ethnicity will be examined. Student opinion surveys and focus groups will further gauge the effectiveness of the games and simulations on student attitudes toward math and technical fields. Finally, the program will be examined using longitudinal data on student enrollment, two-year retention rates, and progress toward degree or transfer."
282,1509739,Asymptotics and concentration in spectral estimation for large matrices,DMS,STATISTICS,7/1/15,6/19/15,Vladimir Koltchinskii,GA,Georgia Tech Research Corporation,Standard Grant,Gabor Szekely,6/30/19,"$289,425.00 ",,vlad@math.gatech.edu,Office of Sponsored Programs,Atlanta,GA,303320420,4048944819,MPS,1269,,$0.00 ,"Estimation of large matrices and their spectral characteristics is crucial in a variety of problems of science and engineering that deal with large high-dimensional data sets. Spectral methods are of the utmost importance in kernel machine learning, manifold learning, functional data analysis, community detection in large networks, quantum statistics and quantum information, and many other applications. The purpose of the project is to develop new mathematical tools needed in analysis of high-dimensional and infinite-dimensional matrix estimation problems that could potentially lead to more powerful methods of statistical inference for complex, high-dimensional data. The project also includes a number of activities with an impact on graduate education and on research collaborations between statistics, computer science and other areas.<br/><br/>The focus of the project is on concentration properties and asymptotics of spectral characteristics (eigenvalues, eigenvectors, spectral projectors) of several important classes of random matrices and operators playing crucial role in high dimensional and infinite dimensional statistical inference and in machine learning. They include sample covariance operators, kernel matrices in machine learning, empirical heat kernels and Laplacians for manifold data, matrices involved in spectral clustering problems on graphs, matrix estimators in trace regression problems (such as matrix completion and quantum state tomography). The main goal is to study the problems where there exists an operator norm consistent estimator of the target matrix (operator), but it converges at a slow rate and to develop a broad range of concentration bounds and asymptotic results for specific functionals of the underlying random matrix (operator) such as its eigenvalues, bilinear forms of its spectral projection operators, norms of deviations of empirical spectral projection operators from their true counterparts. The solution of these problems relies on further development of the methods of high-dimensional probability, such as concentration inequalities, generic chaining bounds for Gaussian, empirical and related classes of stochastic processes, non-asymptotic bounds for random matrices."
283,1512893,Collaborative Research:  Generalized Fiducial Inference for Massive Data and High Dimensional Problems,DMS,STATISTICS,9/1/15,8/16/17,Jan Hannig,NC,University of North Carolina at Chapel Hill,Continuing Grant,Gabor Szekely,8/31/19,"$150,000.00 ",,jan.hannig@unc.edu,104 AIRPORT DR STE 2200,CHAPEL HILL,NC,275991350,9199663411,MPS,1269,,$0.00 ,"R. A. Fisher, the father of modern statistics, proposed the idea of Fiducial Inference in the 1930s.  While his proposal led to some interesting methods for quantifying uncertainty, other prominent statisticians of the time did not accept Fisher's approach because it went against the ideas of statistical inference of the time.  Beginning around the year 2000, the PIs and collaborators started to re-investigate the ideas of fiducial inference and discovered that Fisher's approach, when properly generalized, would open doors to solve many important and difficult problems of uncertainty quantification.  The PIs termed their generalization of Fisher's ideas as generalized fiducial inference.  After many years of preliminary investigations, the PIs developed a coherent, well thought out plan for a systematic research program in this area.  A large part of this project develops practical solutions for different modeling problems that have natural applications in diverse fields.  Finance (volatility estimation) and measurement science (calibration of measurements from different government labs, for example, US NIST) are two primary examples, while others include gene expression data, climate problems, recommender systems, and computer vision.<br/><br/>This project is motivated by the success of generalized fiducial inference (GFI) as introduced by the PIs as a generalization of Fisher's fiducial argument.  The PIs are now working towards scaling up their GFI methodology to handle big data problems and other difficult problems that have emerged due to our ability to collect massive amounts of data rapidly.  In particular the PIs plan to conduct research into the following topics: (i) a thorough investigation of fundamental issues of GFI including connection with Approximate Bayesian Calculations and higher order asymptotics; (ii) sparse covariance estimation using GFI in the ""large p small n"" context; (iii) development of the idea of Fiducial Selector so that a sparsity of the fiducial distribution is induced as a natural outcome of a minimization problem; (iv) uncertainty quantification for the matrix completion problem using GFI, and (v) applications of GFI to a wide variety of practical problems, such as volatility estimation in finance and international key comparison experiments in measurement science."
284,1527827,III: Small: New Machine Learning Approaches for Modeling Time-to-Event Data,IIS,Info Integration & Informatics,9/1/15,8/19/15,Chandan Reddy,MI,Wayne State University,Standard Grant,Aidong Zhang,12/31/16,"$310,542.00 ",,reddy@cs.vt.edu,5057 Woodward,Detroit,MI,482023622,3135772424,CSE,7364,"7364, 7923",$0.00 ,"Due to the advancements in recent data collection technologies, different disciplines have attained the ability to not only accumulate a wide variety of data but also to monitor observations over longer periods of time. In many real-world applications, the primary goal of monitoring these observations is to better estimate the time for a particular event of interest to occur. Examples of these events include disease recurrence in healthcare, time to default in finance, device failure in engineering, etc. A major challenge with such time-to-event data is that it is often incomplete; some data instances are either removed or become unobservable over a period of time before the event occurs. Due to this missing piece of information, standard statistical and machine learning tools cannot readily be applied to analyze such data. Survival analysis methods, primarily developed by the statistics community, aim to model time-to-event data and are usually more effective compared to the standard prediction algorithms as they directly model the probability of occurrence of an event in contrast to assigning a nominal label to the data instance. More importantly, they can implicitly handle missing data. However, in many practical scenarios, the missing data challenges are compounded by several other related complexities such as the presence of correlations within the data, temporal dependencies across multiple instances (collected over a period of time), lack of available information from a single source, and difficulty in acquiring sufficient event data in a reasonable amount of time. Such data poses unique challenges to the field of predictive analytics and thus creates opportunities to develop new algorithms to tackle these issues.  This project provides innovative computational methods to assist novel scientific discoveries and bring practical transformational impact to the analysis and exploration of various time-to-event datasets and applications.  The proposed methods are primarily being evaluated in the context of biomedical data, but are applicable to various other forms of time-to-event data that is often seen in other disciplines such as social science, engineering, finance, and economics.<br/><br/>This project builds novel computational and analytical algorithms that can efficiently and accurately capture the underlying predictive patterns in time-to-event data. The project aims at building new algorithms for longitudinal data analysis, integrate multiple sources while building time-to-event models, and predict temporal events with limited amount of training data. Specifically, the research objectives are to develop the following: (i) Latent feature models that can capture the longitudinal dependencies underlying multiple outcomes over a period of time. Multiple independent regression models for various missing data time windows are learned and then unified into a multi-output regression model over the diverse output space using sparsity regularizers. (ii) Multi-source time-to-event models that can effectively integrate multiple sources of information and make predictions by incorporating prior knowledge about the instances and their relationships. (iii) Bayesian methods for early-stage event prediction to tackle the problem of lack of sufficient training data on events at early stages of studies (which is a common problem in such time-to-event data). All the methods proposed in this project are evaluated using real-world biomedical data including high-dimensional genomic data and heterogeneous electronic health records. In addition, the algorithms developed in this project will also be used to tackle the problem of student retention."
285,1545888,RAPID: Deploying Unmanned Aerial Vehicles for In-situ Studies of Collective Migration,IOS,Animal Behavior,9/1/15,8/27/15,Andrew Berdahl,NM,Santa Fe Institute,Standard Grant,Karen Mabry,8/31/18,"$176,854.00 ",,berdahl@uw.edu,1399 HYDE PARK ROAD,SANTA FE,NM,875018943,5059462727,BIO,7659,"1228, 7659, 7914, 9150, 9179",$0.00 ,"Throughout the natural world, animals make long distance migrations that involve astounding feats of navigation. Collective behavior such as schooling, flocking, and herding is ubiquitous in these migratory species. A growing body of research predicts that by migrating in groups, these animals may be increasing their ability to find their way. However, despite the strong connection between collective navigation and migration, tests for such effects in nature are rare. Migratory caribou, seeking safe passage across treacherous sea ice, are not only of significant ecological and social importance, but also represent an excellent system in which to explore collective navigation in nature. In this project, the researchers will leverage recent advances in Unmanned Aerial Vehicles (drones) and computer vision technology, to obtain movement trajectories of individual caribou within a herd.  Using these trajectories they will determine the role of social interactions on the migratory dynamics of caribou, and investigate the risks of climate change and increased shipping traffic on these animals and their migration. RAPID funding will allow the team to join a government-led caribou survey, in November 2015, which would provide crucial logistical support and essential data. Through this collaboration, results will be disseminated directly to policy makers, improving caribou conservation and the well-being of the Aboriginal people relying on these animals for nutrition and traditional lifestyle. The technology developed will be widely applicable to other systems and provide a significant advance for the study of in situ animal behavior. Outreach associated with the research will provide extracurricular STEM education for underrepresented individuals in both Nunavut and New Mexico by engaging their imagination through the application of cutting-edge technology to important scientific questions.<br/><br/>Despite the clear ecological importance of animal movement, little is known about how animal groups collectively make movement decisions in their natural habitats. The purpose of this research is to prototype a holistic, state-of-the-art, open-source, remote sensing system with the capacity to capture high-definition video of animal groups in the wild and convert this footage into trajectory data, and deploy this technology to study social migration dynamics in caribou. The researchers will use the trajectory data to test the hypothesis that caribou benefit from collective navigation during their migration and uncover the specific mechanisms driving such effects. In addition to providing a tool useful for other groups studying in situ animal behavior, this research will help broaden the focus of research on navigation and migration, from the individual, to a view that explicitly includes the social context. Trajectory data will be posted on Dryad and the publicly accessible MoveBank website (http://movebank.org/)."
286,1515257,EAPSI: Reconstructing 3D models from 2D images,OISE,EAPSI,6/1/15,6/24/15,Audrey Cheong,TX,Cheong                  Audrey         L,Fellowship,Anne Emig,5/31/16,$70.00 ,,,,Sugar Land,TX,774982142,,O/D,7316,"5924, 5978, 7316",$0.00 ,"This award supports research aimed at simplifying the process of obtaining 3-dimensional models through enhanced software and algorithm development, enabling 3D model creation on everyday hardware such as a basic camera phone or digital camera. This will allow small projects to perform 3D quantitative studies without having to locate or invest in a 3D scanner. This project will be conducted in collaboration with Dr. Shang-Hong Lai of National Tsing-Hua University in Taiwan. Dr. Lai, an expert in the field of computer vision, has created 3D face models from a single face image. Similar principles can be applied to reconstruct 3D torso models to enhance breast cancer research. Performing surface analysis on the 3D models will facilitate the quantitative analysis of the breast morphology, help improve plastic surgery techniques to adjust shape and volume, and provide 3D visualization of possible surgical outcomes to patients during their pre-surgical consultation. In the broader scheme, a cell phone app could potentially create a 3D model from captured photos on the scene, which will lead to a greater accessibility to 3D data. <br/><br/>An algorithm on reconstructing 3D models from multi-view images will be developed for rendering different objects, which will allow for 3D shape analysis. The objectives are to calibrate the images to determine camera positions, construct a coarse depth map of the target object, and modify the parameters of the modeling algorithm to achieve photo-consistency. Multiple photographs of a small toy object will be taken from various positions and these positions will be recorded for reference. Some challenges are differentiating shadow boundaries from structural edges and determining the directions of the light sources. The graph cut method will give the general outline of the 3D model based on the silhouettes from the images. Other methods utilizing illumination and shading will be used to further refine the 3D model. This NSF EAPSI award supports the research of a U.S. graduate student and is funded in collaboration with the Ministry of Science and Technology of Taiwan."
287,1632865,EAGER: Nonlinear and Data-Adaptive Compressive Sampling for Big Data Processing,ECCS,CCSS-Comms Circuits & Sens Sys,12/1/15,2/29/16,Konstantinos Slavakis,NY,SUNY at Buffalo,Standard Grant,chengshan xiao,8/31/16,"$114,204.00 ",,kslavaki@buffalo.edu,520 Lee Entrance,Buffalo,NY,142282567,7166452634,ENG,7564,"154E, 7916, 8084",$0.00 ,"As pervasive sensors continuously collect and record massive amounts of<br/>high-dimensional data from communication, social, and biological networks,<br/>and growing storage as well as processing capacities of modern computers<br/>have provided new and powerful ways to dig into such huge quantities of<br/>information, the need for novel analytic tools to comb through these ""big<br/>data"" becomes imperative. The objective of this project is to develop a<br/>novel framework for nonlinear, data-adaptive (de)compression algorithms to<br/>learn the latent structure within large-scale, incomplete or corrupted<br/>datasets for compressing and storing only the essential information, for<br/>running analytics in real time, inferring missing pieces of a dataset, and<br/>for reconstructing the original data from their compressed renditions. <br/><br/>The intellectual merit lies in the exploration of the fertile but largely<br/>unexplored areas of manifold learning, nonlinear dimensionality reduction,<br/>and sparsity-aware techniques for compression and recovery of missing and<br/>compromised measurements. Capitalizing on recent advances in machine<br/>learning and signal processing, differential geometry, sparsity, and<br/>dictionary learning are envisioned as key enablers. Effort will be put also<br/>into developing online and distributed (non)linear dimensionality reduction<br/>algorithms to allow for streaming analytics of sequential measurements<br/>using parallel processors. <br/><br/>The broader impact is to contribute to the development of novel <br/>computational methods and tools useful for data inference, cleansing, <br/>forecasting, and collaborative filtering, with direct impact to <br/>statistical signal processing and machine learning applications<br/>to large-scale data analysis, including communication, social, and<br/>biological networks."
288,1420600,Collaborative Research: Cognitive and Neural Indicators of School-based Improvements in Spatial Problem Solving,DRL,"REAL, ECR-EHR Core Research",1/1/15,7/30/14,Robert Kolvoord,VA,James Madison University,Standard Grant,Gregg Solomon,12/31/19,"$449,784.00 ",,kolvoora@jmu.edu,MSC 5728,HARRISONBURG,VA,228077000,5405686872,EHR,"7625, 7980",,$0.00 ,"This project, to be conducted by James Madison University, Georgetown University, and Northwestern University, will examine learning spatial thinking skills by high school students (who are studying geoscience), looking at educational outcomes as well as behavioral and neurological measures.  There is already considerable evidence linking spatial ability and future STEM (science, technology, engineering, and mathematics) attainment.  The project will focus on a high school course, the Geospatial Semester, that is designed to improve spatial thinking.  The project will look for changes in patterns of brain activity as a result of the course, as well as relations among the educational, behavioral, and neurological measures.  Prior research indicates that spatial training in the laboratory may reduce gender differences in performance; this project will seek to measure this effect in real world high school spatial learning, and to identify neural mechanisms that help explain how and why the gender gap closes.  This project will advance the work of the EHR (Education & Human Resources) Directorate in studying the cognitive and neural basis of STEM learning.<br/><br/>The overall goal of the project is to develop a mechanistic theory of change for spatial STEM education at the behavioral and neural levels.  To achieve this goal, the project will measure a combination of outcomes: educational (e.g., coursework), behavioral (e.g., core spatial ability on standard tests of mental rotation and embedded figure identification, use of spatial language), and neurological (e.g., neural efficiency and grey matter volume in brain regions that support spatial thinking ability, interconnectivity networks across brain regions).  Students in the spatially-based Geospatial Semester will be compared to peers receiving standard (non-spatially-based) STEM education in other advanced science courses.  Neurological measures will be obtained by functional and structural magnetic resonance imaging performed at pre- and post-test time points.  In general, the project will address the issue of brain plasticity in a high school STEM education context.  The project will use machine learning techniques to discern neurological effects of spatial learning in both hypothesis-driven and data-driven ways, and examine gender differences in the effect of spatial STEM learning on cognition and the brain.  Analyses will focus on relating changes across neural, behavioral, and educational levels toward an integrated understanding of the mechanisms that make spatial STEM learning effective."
289,1452099,CAREER: New Frontiers in Sequential Decision Making with a View Towards Mobile Health Applications,IIS,Robust Intelligence,9/1/15,6/14/19,Ambuj Tewari,MI,Regents of the University of Michigan - Ann Arbor,Continuing Grant,Rebecca Hwa,8/31/20,"$499,939.00 ",,tewaria@umich.edu,3003 South State St. Room 1062,Ann Arbor,MI,481091274,7347636438,CSE,7495,"1045, 7495",$0.00 ,"This NSF CAREER project uses machine learning to advance the science of health interventions delivered via mobile devices. The field of mobile health (mhealth) is advancing rapidly spurred by the widespread use of mobile devices across the world. Mhealth researchers are using mobile devices to diagnose, manage, and treat a variety of health conditions including, but by no means limited to, alcohol abuse, depression, obesity, and smoking. The project creates new learning algorithms that learn to personalize mhealth interventions to the changing needs of individual users. It also develops techniques that use the social network among users to intervene so that the entire network is steered towards positive health outcomes. The project will give undergraduate researchers the opportunity to work on cutting edge data science driven by socially relevant problems. It will develop new graduate level courses on sequential decision making. Software including simulations specific to health domains, implementations of learning algorithms, and mobile apps that deliver interventions will be publicly released. Research results will be communicated to, and feedback solicited from, the general public under the auspices of the newly formed Michigan Institute for Data Science (MIDAS). Involvement of under-represented minorities will be encouraged via the NextProf Science workshop and Faculty Allies for Diversity program at the University of Michigan.<br/><br/>The project involves solving three technical challenges. First, existing algorithms for sequential decision making, such as contextual bandit algorithms, need to be redesigned to make them suitable for adoption in mhealth. The project will develop actor-critic contextual bandit algorithms that separate the representation of the policy from the representation of the reward function. Having an interpretable, low dimensional policy space is crucial for interpretability of the learned policy by the domain scientist. Second, most of the literature on bandit problems and reinforcement learning assumes stationarity, an unreasonable assumption in mhealth. Using recent advances in the field of no-regret learning, the project will develop learning algorithms that can deal with non-stationarity. Third, the science of network interventions is in its infancy. The project will serve a catalyst for its development by synthesizing recent advances in high dimensional statistical learning and control of complex networks to design learning algorithms that intervene at a small number of nodes in a time evolving network to achieve a desired long term objective."
290,1546482,BIGDATA: Collaborative Research: F: Stochastic Approximation for Subspace and Multiview Representation Learning,IIS,Big Data Science &Engineering,9/1/15,9/3/15,Raman Arora,MD,Johns Hopkins University,Standard Grant,Sylvia Spengler,8/31/20,"$704,672.00 ",,arora@cs.jhu.edu,1101 E 33rd St,Baltimore,MD,212182686,4439971898,CSE,8083,"7433, 8083",$0.00 ,"Unsupervised learning of useful features, or representations, is one of the most basic challenges of machine learning. Unsupervised representation learning techniques capitalize on unlabeled data which is often cheap and abundant and sometimes virtually unlimited. The goal of these ubiquitous techniques is to learn a representation that reveals intrinsic low-dimensional structure in data, disentangles underlying factors of variation by incorporating universal AI priors such as smoothness and sparsity, and is useful across multiple tasks and domains. <br/><br/>This project aims to develop new theory and methods for representation learning that can easily scale to large datasets. In particular, this project is concerned with methods for large-scale unsupervised feature learning, including Principal Component Analysis (PCA) and Partial Least Squares (PLS). To capitalize on massive amounts of unlabeled data, this project will develop appropriate computational approaches and study them in the ""data-laden"" regime. Therefore, instead of viewing representation learning as dimensionality reduction techniques and focusing on an empirical objective on finite data, these methods are studied with the goal of optimizing a population objective based on sample. This view suggests using Stochastic Approximation approaches, such as Stochastic Gradient Descent (SGD) and Stochastic Mirror Descent, that are incremental in nature and process each new sample with a computationally cheap update. Furthermore, this view enables a rigorous analysis of benefits of stochastic approximation algorithms over traditional finite-data methods. The project aims to develop stochastic approximation approaches to PCA and PLS and related problems and extensions, including deep, and sparse variants, and analyze these problems in the data-laden regime."
291,1420481,Collaborative Research: Cognitive and Neural Indicators of School-based Improvements in Spatial Problem Solving,DRL,"REAL, ECR-EHR Core Research",1/1/15,7/30/14,Adam Green,DC,Georgetown University,Standard Grant,Gregg Solomon,12/31/18,"$625,971.00 ",,aeg58@Georgetown.edu,37th & O St N W,Washington,DC,200571789,2026250100,EHR,"7625, 7980",,$0.00 ,"This project, to be conducted by James Madison University, Georgetown University, and Northwestern University, will examine learning spatial thinking skills by high school students (who are studying geoscience), looking at educational outcomes as well as behavioral and neurological measures.  There is already considerable evidence linking spatial ability and future STEM (science, technology, engineering, and mathematics) attainment.  The project will focus on a high school course, the Geospatial Semester, that is designed to improve spatial thinking.  The project will look for changes in patterns of brain activity as a result of the course, as well as relations among the educational, behavioral, and neurological measures.  Prior research indicates that spatial training in the laboratory may reduce gender differences in performance; this project will seek to measure this effect in real world high school spatial learning, and to identify neural mechanisms that help explain how and why the gender gap closes.  This project will advance the work of the EHR (Education & Human Resources) Directorate in studying the cognitive and neural basis of STEM learning.<br/><br/>The overall goal of the project is to develop a mechanistic theory of change for spatial STEM education at the behavioral and neural levels.  To achieve this goal, the project will measure a combination of outcomes: educational (e.g., coursework), behavioral (e.g., core spatial ability on standard tests of mental rotation and embedded figure identification, use of spatial language), and neurological (e.g., neural efficiency and grey matter volume in brain regions that support spatial thinking ability, interconnectivity networks across brain regions).  Students in the spatially-based Geospatial Semester will be compared to peers receiving standard (non-spatially-based) STEM education in other advanced science courses.  Neurological measures will be obtained by functional and structural magnetic resonance imaging performed at pre- and post-test time points.  In general, the project will address the issue of brain plasticity in a high school STEM education context.  The project will use machine learning techniques to discern neurological effects of spatial learning in both hypothesis-driven and data-driven ways, and examine gender differences in the effect of spatial STEM learning on cognition and the brain.  Analyses will focus on relating changes across neural, behavioral, and educational levels toward an integrated understanding of the mechanisms that make spatial STEM learning effective."
292,1533623,NCS-FO: The role of noise in mental exploration for learning,BCS,IntgStrat Undst Neurl&Cogn Sys,8/15/15,8/10/15,Joshua Gold,PA,University of Pennsylvania,Standard Grant,Kurt Thoroughman,7/31/19,"$1,019,916.00 ",Joseph Kable,jigold@mail.med.upenn.edu,Research Services,Philadelphia,PA,191046205,2158987293,SBE,8624,"8089, 8091, 8551",$0.00 ,"What makes people behave so differently from one another? Consider how we make decisions. Some people are quick and decisive but overly rigid, unable to adapt effectively to new opportunities or threats. In contrast, others may be more deliberative and less confident, making their decisions less predictable but more adaptable to changing circumstances. With funding from the National Science Foundation, Drs. Joshua Gold and Joseph Kable of the University of Pennsylvania are investigating a new theory that in the real world, there is a fundamental tradeoff between these two extremes. The theory includes a novel proposal that what has previously been dismissed by researchers as random variability in human behavior might instead reflect uncertain, adaptable decision-making linked with norepinephrine, a neurochemical implicated in learning and arousal. Does this characteristic explain other aspects of human personality and behavior? Can norepinephrine levels in the brain be manipulated to affect complex learning and decision-making behaviors? In answering these questions, this work will establish foundational, basic knowledge that, in the long term, will help to guide the development of new tools to diagnose and counteract conditions associated with abnormal learning and decision-making, including attention deficit hyperactivity disorder (ADHD), anxiety, depression, and schizophrenia. This knowledge about individual differences in learning will also inform how to best tailor educational and learning practices, as well as how to design computer programs that learn adaptively from experience. Other benefits of this work are resources that will assist research and education in cognitive and neural systems, including publically available datasets, computer code and machine learning algorithms; increased participation of underrepresented groups in this kind of integrative research, via summer research experiences for high school and undergraduate students; and an increased public awareness of neuroscience via public lectures, Brain Awareness Week activities, and contributions to a website that explains brain research in laymen's terms.<br/><br/>The work is based on a novel hypothesis about brain mechanisms that are responsible for certain idiosyncratic learning and decision processes. Specifically, in our unpredictable world, decision-makers face an inherent trade-off: higher certainty leads to more precise and accurate choices when the world is stable but an inability to adjust to change, whereas less certainty can lead to greater adaptability but also more variable and imprecise decisions.  The investigators propose that this trade-off is regulated by interactions between arousal and cortical systems. To test this hypothesis, they use an interdisciplinary and integrative set of approaches with three primary objectives: 1) develop a theoretical framework describing inherent trade-offs between output stability and learning in hierarchical, probabilistic inference processes in unpredictable environments; 2) identify behavioral, physiological, and neural correlates of variability in how individuals navigate these trade-offs while making choices in unpredictable environments; and 3) identify causal influences of the brainstem nucleus locus coeruleus, a key component of the arousal system, on the variability in adaptive inference. The work forges meaningful connections across theory and experiment, spanning multiple spatial and temporal scales and levels of abstraction, to identify computational and physiological underpinnings of individual differences in learning."
293,1537987,"Collaborative Research: Active Statistical Learning: Ensembles, Manifolds, and Optimal Experimental Design",CMMI,OE Operations Engineering,9/1/15,8/7/15,Enrique Del Castillo,PA,Pennsylvania State Univ University Park,Standard Grant,Georgia-Ann Klutke,8/31/19,"$175,000.00 ",,exd13@psu.edu,110 Technology Center Building,UNIVERSITY PARK,PA,168027000,8148651372,ENG,006Y,"071E, 078E, 9102",$0.00 ,"In numerous industries such as manufacturing, health care or energy production, current sensor technology can generate enormous quantities of measurements of an object at low cost. Each measurement consists of several instances of interrelated variables, and the goal is to use the data to build a computer model that permits one to predict the class of an object (such as the health condition of a patient or the quality of a manufactured part). Along with the sensor data, the class labels for some objects are needed to train the computer model. While the sensor variables can frequently be obtained rapidly and inexpensively (e.g., medical images or chemical analyses) the class label associated with each object might require human effort that is time-consuming and expensive. Therefore, care should be taken to select the objects to label that are most informative for building the predictive computer model. Often one selects objects iteratively, where the class labels from the previously selected batch guides the next batch of objects to label. This is the purpose of a so-called active learning strategy. The purpose of this research is to find new active learning methods that accelerate model building and provide better predictions in systems where large datasets of attribute measurements are available. This will result in more efficient and productive systems that will benefit the U.S. economy and society.<br/><br/>Existing active learning methods are often based on strong assumptions for the joint input/output distribution or use a distance-based approach. These methods are susceptible to noise in the input space, assume numerical inputs only, and often work poorly in high dimensions. In applications, data sets are often large, noisy, contain missing values and mixed variable types. In this research, a non-parametric approach to the active learning problem is planned to address these challenges. The algorithm is based on a batch diversification strategy applied to an ensemble of decision trees. A novel active learning strategy that considers the geometric structure of the manifold where the unlabeled data resides will also be considered. The geometric properties of the data space may result in more informative active learning solutions. This is a collaborative effort between Arizona State University, Pennsylvania State University, and Intel Corporation with complementary expertise in machine learning and optimal design. The participation of Intel will help ensure the successful dissemination and broad applicability of the results."
294,1552454,EAGER: Social Networks Based Concept Learning in Images,IIS,Info Integration & Informatics,9/1/15,8/31/15,Bir Bhanu,CA,University of California-Riverside,Standard Grant,Maria Zemankova,8/31/18,"$200,000.00 ",,bhanu@ece.ucr.edu,Research & Economic Development,RIVERSIDE,CA,925210217,9518275535,CSE,7364,"7364, 7916",$0.00 ,"The need for easy, quick, and intuitive search of visual data at the conceptual level is universal. This project will explore a novel network analysis-based approach to searching visual data, ultimately leading to visual search engines that would make searching images as easy as searching by keywords is today. Accomplishing this requires new approaches to machine learning of visual concepts. This project proposes a formal framework that unifies the ideas from social networks and semantic concept learning so multiple semantic concepts can be learned with high confidence. Specifically, the approach utilizes hierarchical co-occurrence correlation among concepts as cues to help the detection of individual visual concepts. The sources for robustness are the learning of co-occurrence patterns, similar to community structures in a social network, and their refinement over time. The success of concept co-occurrence detection will simplify management of personal image data with automatic tagging. With semantically organized personal content, the preferences of a user can be learned to provide personalization of various contents that he/she consumes online. This will have a broad impact for diverse applications ranging from information technology to physical, life and social sciences to intelligence organizations to news bureaus. Forensics analysis of digital data would be greatly speeded up as humans do not have to sift through large amount of data. Extension of the techniques to video, music, and multi-modal data would also provide similar ease for content consumption. The research team provides an environment integrating education and workforce development with research, and with recruiting and retaining a diverse group of students. Complementing the research activities will be new initiatives in education and public outreach. <br/><br/>The project develops a transformative approach to explore the acquisition and refinement of semantic visual concepts systematically. First, it discovers the hierarchical co-occurrence patterns of concepts as underlying community structures in the co-occurrence network. The co-occurrence patterns play roles similar to underlying scene concepts at a higher level of semantics. Second, it proposes an approach for selecting visually-consistent-semantic concepts. Since concepts vary in their visual complexity, visual-semantic relatedness of each concept is investigated by quantitatively measuring the within-concept visual variability and the visual distances to the other concepts such that they can be modeled more reliably and detected more easily. Third, the project introduces a novel image content descriptor called concept signature that can record both the semantic concept and the corresponding confidence value inferred from low-level features. Finally, the project proposes techniques for scalability to handle and evaluate the performance on large databases by developing open source techniques and software tools. The results will be broadly disseminated through the project website (http://vislab.ucr.edu/RESEARCH/VisualSemanticConcepts/VSC.php), via regular releases of software tools and offering tutorials/workshops at major IEEE/ACM conferences."
295,1462191,EAGER-DynamicData: Subspace Learning From Binary Sensing,ECCS,"Big Data Science &Engineering, ",9/1/15,9/1/15,Yuejie Chi,OH,Ohio State University,Standard Grant,Akbar Sayeed,9/30/18,"$200,000.00 ",,yuejiechi@cmu.edu,Office of Sponsored Programs,Columbus,OH,432101016,6146888735,ENG,"8083, O395","153E, 7916",$0.00 ,"Decentralized sensing systems play an increasingly critical role in everyday life, including wireless sensor networks, mobile crowd-sensing with internet-of-things, and crowdsourcing with human workers, with applications in network analysis, distributed wideband spectrum sensing, target tracking, environmental monitoring, and advertisement prediction. Despite the promise, however, efficient inference is extremely challenging due to processing large amounts of data at the typically resource-starved sensor nodes. This project develops efficient feature extraction and dimensionality reduction tools for decentralized sensing systems with minimal computation, storage and communication requirements of each sensor node to make sense of the surrounding dynamic environments. Students on this program will develop multi-disciplinary expertise in signal processing, machine learning, optimization, and statistics. New graduate-level courses on high-dimensional data analysis will be developed by the PI at Ohio State University. <br/><br/>More specifically, this project offers an integrated approach for subspace learning from bits, where the sampling strategy explicitly accounts for the communication burden by only requesting a single bit from each sensor node. This project opens up opportunities to develop a theory of principal component analysis (or subspace learning) based on binary sensing, where noisy data samples are synthesized into coarse yet high-fidelity binary measurements that are more amenable for communication and inference. The consideration of binary measurements is well-motivated, as in practice, measurements are either mapped to bits from a finite alphabet before computation, or available naturally in the quantized form, such as comparison outcomes from human as sensors; constraints in storage and communication are often expressed in terms of the number of bits rather than the number of real measurements; finally, binary measurements are also more robust against unknown, nonlinear and heterogeneous distortions from different sensors compared with real measurements. Unfortunately, none of the existing subspace learning frameworks is tailored to acquire and process quantized measurements, and will yield highly sub-optimal results if naive quantization is applied. This project addresses the above challenge and highlights a novel interplay between the quantity, precision, and fidelity of measurements in sensing for estimating and tracking a low-dimensional subspace in a dynamic environment. Decentralized and online inference algorithms for subspace learning are developed together with adaptive sensing schemes to speed up convergence."
296,1545814,I-Corps:  AutoEEG-enhancing productivity by autoscanning EEG signals,IIP,I-Corps,7/15/15,7/3/15,Iyad Obeid,PA,Temple University,Standard Grant,lydia mcclure,8/31/16,"$50,000.00 ",Joseph Picone,iobeid@temple.edu,1801 N. Broad Street,Philadelphia,PA,191226003,2157077547,ENG,8023,,$0.00 ,"Electroencephalograms (EEGs) are the most pervasive neural diagnostic tool; they require a highly trained neurologist to analyze them. Long-term EEG monitoring, used to diagnose rare events such as epileptic seizures, is difficult or impossible to scan manually without decision software support.  Development of portable standalone diagnostic tools, which can address emerging markets such as contact sports, is highly difficult; and smaller or stand-alone medical practices often lack expertise to conduct diagnostics on-site and accordingly lose revenue. At present, innovation in commercial clinical decision support tools is minimal, whereas the global market for rapidly diagnosing brain-related injury and disease is growing. The proposed AutoEEGTM is a software tool that enhances productivity by auto-scanning EEG signals and flagging sections of the signal that need further review by a clinician. The proposed tool reduces the amount of data needing manual review by two orders of magnitude, offering substantial productivity gains in a clinical setting.<br/><br/>The proposed clinical decision support tool is based on proven, advanced, deep learning technology. It reduces time to diagnosis, reduces error and is sufficiently lightweight to run on portable standalone platforms. This technology is able to identify EEG events in the signal and subsequently to provide a report that summarizes its findings based on the event detected. The transcribed EEG signals can be viewed from any portable computing device. It also has the ability to learn from data, helping in future decision making, providing real-time feedback to aid in diagnosis, and, for patients undergoing long-term monitoring, creating an alert when abnormal signals are identified. This market-leading product will (1) Enable clinical neurologists employing a volume-based business mode to decrease the time spent analyzing an EEG and thereby increase billing; (2) Allow pharmas to assess changes quantitatively in neural activation during clinical trials; (3) Allow neurologists to order and bill for substantially more long-term monitoring tests based on this proven decision support tool; and (4) Add value to the commodity EEG headsets currently entering the market by providing meaningful, real-time signal analysis. This research project has two key components: (1) a detailed analysis of the market to understand various opportunities such as licensing to equipment manufacturers and off-line analysis for contract research organizations; and (2) usability design and engineering to understand the analytics and user interface issues that bring most value to potential users such as clinicians and primary care physicians. The outcomes of this research will be used to harden the technology and guide integration into existing EEG products."
297,1463864,CRII: CHS: Scaling Up Online Peer Tutoring of Computer Programming,IIS,"CRII CISE Research Initiation, HCC-Human-Centered Computing",9/1/15,8/1/16,Philip Guo,NY,University of Rochester,Continuing Grant,William Bainbridge,11/30/16,"$207,000.00 ",,pjguo@ucsd.edu,"518 HYLAN, RC BOX 270140",Rochester,NY,146270140,5852754031,CSE,"026Y, 7367","7367, 8228, 9251",$0.00 ,"This project will develop innovative software to broaden access to free one-on-one tutoring, starting in the domain of computer programming, which is crucial for many kinds of 21st-century jobs. Learning is one of the most important and fundamental lifelong endeavors. A well-educated public is crucial for maintaining a healthy, prosperous, and innovative society. Regardless of subject, one-on-one tutoring is the most personal and effective way to learn. Although recent efforts such as MOOCs (Massive Open Online Courses) are scaling up access to lectures and other educational materials, it is hard to scale up one-on-one tutoring to online settings because there are far more learners in the world than qualified expert tutors. Access to free online tutors will especially benefit lower-income learners, who are more likely to be from rural areas or underrepresented minority groups.<br/><br/>The technical objective of this project is to investigate how to scale up peer tutoring of computer programming through user interfaces and algorithms for matching learners with tutors, hosting tutoring sessions within real-time code visualizations, and reviewing archived sessions. The central idea is to draw from the large pool of learners concurrently accessing online educational resources to serve as peer tutors for one another. Although peers lack the deep expertise of experts, many are effective tutors since they recently learned the material and can empathize better with learners. This project will build upon Online Python Tutor, a Web-based educational tool that the researcher created to visualize the execution of computer code, now one of the most popular websites for learning programming in the Python language. This project's specific aims are to build a peer tutoring system atop the Online Python Tutor website and to use it to study online peer tutoring interactions. The system has two main components: 1.) PythonLive is a real-time tutoring interface for computer programming that enables multiple users to concurrently write, execute, visualize, and chat about code, a single tutor to effectively handle multiple simultaneous tutees, and offline learning by reviewing archived tutoring sessions. 2.) TutorMatch is an interface that uses crowdsourcing and machine learning to connect learners with appropriately-skilled peer tutors in real-time. In sum, this research will contribute new software-based techniques to facilitate peer tutoring of computer programming, which draws upon and contributes to the fields of human-computer interaction, social computing, and computer-supported cooperative learning."
298,1453432,CAREER: Privacy-preserving learning for distributed data,CCF,"Special Projects - CCF, Comm & Information Foundations, Secure &Trustworthy Cyberspace",7/1/15,7/17/18,Anand Sarwate,NJ,Rutgers University New Brunswick,Continuing Grant,Phillip Regalia,6/30/21,"$556,200.00 ",,anand.sarwate@rutgers.edu,33 Knightsbridge Road,Piscataway,NJ,88543925,8489320150,CSE,"2878, 7797, 8060","1045, 7434, 7797, 7935, 9251",$0.00 ,"Medical technologies such as imaging and sequencing make it possible to gather massive amounts of information at increasingly lower cost. Sharing data from studies can advance scientific understanding and improve healthcare outcomes. Concern about patient privacy, however, can preclude open data sharing, thus hampering progress in understanding stigmatized conditions such as mental health disorders.  This research seeks to understand how to analyze and learn from sensitive data held at different sites (such as medical centers) in a way that quantifiably and rigorously protects the privacy of the data.  <br/><br/>The framework used in this research is differential privacy, a recently-proposed model for measuring privacy risk in data sharing.  Differentially private algorithms provide approximate (noisy) answers to protect sensitive data, involving a tradeoff between privacy and utility.  This research studies how to combine private approximations from different sites to improve the overall quality or utility of the result. The main goals of this research are to understand the fundamental limits of private data sharing, to design algorithms for making private approximations and rules for combining them, and to understand the consequences of sites having more complex privacy and sharing restrictions.  The methods used to address these problems are a mix of mathematical techniques from statistics, computer science, and electrical engineering.<br/><br/>The educational component of this research will involve designing introductory university courses and material on data science, undergraduate research projects, curricular materials for graduate courses, and outreach to the growing data-hacker community via presentations, tutorial materials, and open-source software. <br/><br/>The primary aim of this research is bridge the gap between theory and practice by developing algorithmic principles for practical privacy-preserving algorithms. These algorithms will be validated on neuroimaging data used to understand and diagnose mental health disorders. Implementing the results of this research will create a blueprint for building practical privacy-preserving learning for research in healthcare and other fields.  The tradeoffs between privacy and utility in distributed systems lead naturally to more general questions of cost-benefit tradeoffs for learning problems, and the same algorithmic principles will shed light on information processing and machine learning in general distributed systems where messages may be noisy or corrupted."
299,1527148,"NRI: Real Time Observation, Inference and Intervention of Co-Robot Systems Towards Individually Customized Performance Feedback Based on Students' Affective States",IIS,NRI-National Robotics Initiati,9/1/15,8/11/15,Conrad Tucker,PA,Pennsylvania State Univ University Park,Standard Grant,David Haury,8/31/18,"$342,574.00 ",Timothy Brick,conradt@andrew.cmu.edu,110 Technology Center Building,UNIVERSITY PARK,PA,168027000,8148651372,CSE,8013,8086,$0.00 ,"This NSF National Robotics Initiative project will investigate the potential of a cycle of observation, inference and intervention by co-robot systems to enhance students' affective states and improve their performance on engineering laboratory tasks. Co-robots are robots that work side-by-side with humans, assisting them and adapting to their needs. The two-way exchange of knowledge between students and co-robots creates a reciprocal relationship, in which each party learns from the other in service of a common goal. Affective states, such as frustration and engagement, play a major role in students' performance on everyday learning tasks. A student who is overly stressed or distracted may commit errors that would be otherwise easy to avoid. A co-robot system that is cognizant of students' affective states can intervene to prevent these errors. The results of this project may provide a template for skill-based instruction on topics well beyond engineering. Currently, such learning requires extensive interactions between a student and an instructor, with the instructor providing intensive feedback at all times. In many cases, personality mismatches or other issues between instructor and student can lead to frustration, learning difficulties, and eventual dropout. Furthermore, one-on-one learning is limited by scalability challenges, as an increase in the number of students, without a proportional increase in trained instructors, can result in decreases in quality and quantity of instructor time allocated to each student. Co-robot learning systems will be able to mitigate these challenges by providing both real time and scalable feedback systems that adapt to the individual needs of students and help to minimize the amount of human instructor time required by each student. <br/><br/>This research will acquire facial, auditory, and body gesture data from students using the integrated visual, audio and depth sensory system of the co-robot.  The system will make statistical inferences of students' affective states, based on machine learning classification of facial and body language data.  Visual feedback will be used to present students with interventions (visual instructions and commentary) intended to enhance their affective state and improve their performance on laboratory tasks.  The project will assess the impact of co-robots' ability to improve students' affective states and enhance students' performance on laboratory tasks over repeated iterations of learning and testing.  This project will lead to a better understanding of how students interact and function during potentially stressful laboratory activities. The co-robot systems proposed in this work will help discover the correlations that exists between students' affect and task performance. Co-robots will actively adapt to the manner in which students learn complex engineering tasks and the affective states that accompany that learning. Co-robot systems that predict the effectiveness of specific intervention strategies for each student and situation will lead to individually-tailored student feedback that serves both students and instructors towards enhancing student performance over time. This proposal advances the impact of co-robots into educational research and practice and extends knowledge of how to succinctly represent the complexities of human behavior in digital form."
300,1529079,Collaborative Research: Adiabatic Quantum Computing and Statistics,DMS,CISE-MPS QIS Faculty Program,9/1/15,7/21/15,Daniel Lidar,CA,University of Southern California,Standard Grant,Edward Taylor,8/31/17,"$3,300.00 ",,lidar@usc.edu,University Park,Los Angeles,CA,900890001,2137407762,MPS,8082,7203,$0.00 ,"For decades computer power has doubled for constant cost roughly once every two years according to the so-called Moore's law. This dream run is realized through technological advances in the fabrication of computer hardware, making electronic devices smaller and smaller. However, as the sizes of the computer electronic devices get close to the atomic scale, quantum effects are starting to interfere in their functioning, and thus conventional computer technology approaches run up against fundamental difficulties of size limit. This research project concerns quantum computing, the development of computer technology dependent on the principles of quantum physics, as opposed to the electronic devices following laws of classical physics used by classical computers. This revolutionary field will enable a range of exotic new devices, and in particular it will likely lead to the creation of powerful quantum computers. The research project is among the frontier research endeavors where quantum technologies are being developed and quantum devices are being built with capabilities exceeding those of classical computational devices. <br/><br/>Quantum computation and quantum information science more generally concern the preparation and control of the quantum states of physical systems to manipulate and transmit information. A quantum system usually has complexity exponentially increasing with its size. As a result, it takes an exponential number of bits of memory on a classical computer to store the state of a quantum system, and simulations of quantum systems via classical computers face great computational challenge. On the other hand, since quantum systems are able to store and track an exponential number of complex numbers and perform data manipulations and calculations as the systems evolve, quantum systems hold great promise as computational tools.  Quantum information science grapples with understanding how to take advantage of the enormous amount of information hidden in the quantum systems and to harness the immense potential computational power of atoms and photons for the purpose of information processing and computation. This cross-disciplinary research project addresses questions in quantum information science on the interface between quantum computing and machine learning and between quantum tomography and compressed sensing. The collaborative research aims to explore the power of adiabatic quantum computation and its impact on computer science and statistics in general and machine learning and Monte Carlo sampling in particular. The work investigates the use of leading techniques from machine learning in computer science and statistics, compressed sensing in applied mathematics, statistics, and engineering, and quantum tomography in quantum physics for adiabatic quantum computing. The research activities promote collaborations among investigators with different disciplinary backgrounds and stimulate novel ideas for possible breakthrough, transformative research."
301,1546500,BIGDATA: Collaborative Research: F: Stochastic Approximation for Subspace and Multiview Representation Learning,IIS,Big Data Science &Engineering,9/1/15,7/15/19,Nathan Srebro,IL,Toyota Technological Institute at Chicago,Standard Grant,Sylvia Spengler,8/31/20,"$402,518.00 ",,nati@ttic.edu,6045 S Kenwood Ave,Chicago,IL,606372803,7738340409,CSE,8083,"7433, 8083, 9251",$0.00 ,"Unsupervised learning of useful features, or representations, is one of the most basic challenges of machine learning. Unsupervised representation learning techniques capitalize on unlabeled data which is often cheap and abundant and sometimes virtually unlimited. The goal of these ubiquitous techniques is to learn a representation that reveals intrinsic low-dimensional structure in data, disentangles underlying factors of variation by incorporating universal AI priors such as smoothness and sparsity, and is useful across multiple tasks and domains. <br/><br/>This project aims to develop new theory and methods for representation learning that can easily scale to large datasets. In particular, this project is concerned with methods for large-scale unsupervised feature learning, including Principal Component Analysis (PCA) and Partial Least Squares (PLS). To capitalize on massive amounts of unlabeled data, this project will develop appropriate computational approaches and study them in the ?data laden? regime. Therefore, instead of viewing representation learning as dimensionality reduction techniques and focusing on an empirical objective on finite data, these methods are studied with the goal of optimizing a population objective based on sample. This view suggests using Stochastic Approximation approaches, such as Stochastic Gradient Descent (SGD) and Stochastic Mirror Descent, that are incremental in nature and process each new sample with a computationally cheap update. Furthermore, this view enables a rigorous analysis of benefits of stochastic approximation algorithms over traditional finite-data methods. The project aims to develop stochastic approximation approaches to PCA and PLS and related problems and extensions, including deep, and sparse variants, and analyze these problems in the data-laden regime."
302,1528735,Collaborative Research:  Adiabatic Quantum Computing and Statistics,DMS,"Chem Thry, Mdls & Cmptnl Mthds, CISE-MPS QIS Faculty Program",9/1/15,8/8/19,Yazhen Wang,WI,University of Wisconsin-Madison,Continuing Grant,Andrew Pollington,8/31/20,"$246,650.00 ",,yzwang@stat.wisc.edu,21 North Park Street,MADISON,WI,537151218,6082623822,MPS,"6881, 8082",7203,$0.00 ,"For decades computer power has doubled for constant cost roughly once every two years according to the so-called Moore's law. This dream run is realized through technological advances in the fabrication of computer hardware, making electronic devices smaller and smaller. However, as the sizes of the computer electronic devices get close to the atomic scale, quantum effects are starting to interfere in their functioning, and thus conventional computer technology approaches run up against fundamental difficulties of size limit. This research project concerns quantum computing, the development of computer technology dependent on the principles of quantum physics, as opposed to the electronic devices following laws of classical physics used by classical computers. This revolutionary field will enable a range of exotic new devices, and in particular it will likely lead to the creation of powerful quantum computers. The research project is among the frontier research endeavors where quantum technologies are being developed and quantum devices are being built with capabilities exceeding those of classical computational devices. <br/><br/>Quantum computation and quantum information science more generally concern the preparation and control of the quantum states of physical systems to manipulate and transmit information. A quantum system usually has complexity exponentially increasing with its size. As a result, it takes an exponential number of bits of memory on a classical computer to store the state of a quantum system, and simulations of quantum systems via classical computers face great computational challenge. On the other hand, since quantum systems are able to store and track an exponential number of complex numbers and perform data manipulations and calculations as the systems evolve, quantum systems hold great promise as computational tools.  Quantum information science grapples with understanding how to take advantage of the enormous amount of information hidden in the quantum systems and to harness the immense potential computational power of atoms and photons for the purpose of information processing and computation. This cross-disciplinary research project addresses questions in quantum information science on the interface between quantum computing and machine learning and between quantum tomography and compressed sensing. The collaborative research aims to explore the power of adiabatic quantum computation and its impact on computer science and statistics in general and machine learning and Monte Carlo sampling in particular. The work investigates the use of leading techniques from machine learning in computer science and statistics, compressed sensing in applied mathematics, statistics, and engineering, and quantum tomography in quantum physics for adiabatic quantum computing. The research activities promote collaborations among investigators with different disciplinary backgrounds and stimulate novel ideas for possible breakthrough, transformative research."
303,1525936,SHF:  Small:   Driving Learning for Program Verification,CCF,Software & Hardware Foundation,9/1/15,6/24/15,Aarti Gupta,NJ,Princeton University,Standard Grant,Nina Amla,8/31/19,"$463,706.00 ",,aartig@princeton.edu,Off. of Research & Proj. Admin.,Princeton,NJ,85442020,6092583090,CSE,7798,"7923, 8206",$0.00 ,"Program verification has broad applications, from ensuring safety of mission-critical software to improving program robustness and programmer productivity. Automatic program verification techniques employ various forms of learning to enhance scalability on large programs. These include deductive learning in modern logic-based solvers and learning from counterexamples in abstraction refinement procedures. Modular verification is essential for scaling verification to large software, and concurrent program verification is critical due to the wide prevalence of multi-core hardware. <br/><br/>This project develops techniques for learning inductive invariants for modular verification in a teacher-learner setting. The research objectives include studying suitable languages of invariants at procedure boundaries, identifying requirements for progress in learning, and developing effective techniques for guiding the learner. The project also addresses verification of concurrent programs, where learning over different event sequences is performed by dynamic and predictive analysis over program traces. The goal is to drive the learning toward unexplored program behaviors by automatically generating test inputs. The research objectives include studying new trace abstractions and coverage metrics for concurrent programs, and developing techniques for coverage-guided test generation. The methods for driving learning include directed testing to target specific scenarios relevant for learning. Beyond these specific contributions, the results will provide insights on applying machine learning techniques in combination with static and dynamic analysis for advancing program verification. The project includes development of educational material, tools, and benchmarks that will be made publicly available."
304,1462254,EAGER-DynamicData: Collaborative Research: Data-driven morphing of parsimonious models for the description of transient dynamics in complex systems,ECCS,"Big Data Science &Engineering, ",9/15/15,9/11/15,Themistoklis Sapsis,MA,Massachusetts Institute of Technology,Standard Grant,akbar sayeed,8/31/17,"$75,000.00 ",,sapsis@mit.edu,77 MASSACHUSETTS AVE,Cambridge,MA,21394301,6172531000,ENG,"8083, O395","153E, 5384, 7916",$0.00 ,"Predicting and quantifying the behavior of complex systems in engineering and science is a topic of critical importance for many areas such as design, optimization, and safety. Even more critical is the forecast of extreme responses of these systems. Rare responses that can lead to catastrophic events manifest themselves in a wide range of systems such as geophysical phenomena, power and communication networks, and epileptic incidents in brain activity, just to mention a few. In all of these cases, accurate predictions are hampered by the fact that the exact dynamics of the system in nature is often poorly understood. This poor understanding is due to the large number of essentially coupled mechanisms that operate at different temporal and spatial scales. Although it is not always essential to predict the system with high accuracy over all these different levels, it is important to understand and model the effect of the unresolved mechanisms to the degrees of freedom we want to predict. This requires a reliable knowledge of the descriptive laws for these mechanisms as well as their coupling to the degrees-of-freedom of interest and this is clearly not always (well) done. This incomplete modeling of the dynamics leads to inevitable model error that is essential to be taken into account for reliable predictions. An obvious way that this can be done is by the utilization of available and dynamically incoming data. The goal of this work is the development of methods and algorithms to extend the capability for data-driven morphing (that is, data-driven adaptation) of parsimonious models. These will be able to adequately capture the instantaneously most significant dynamics of the system and utilize them to inexpensively perform informative prediction and uncertainty quantification. Such a development will be of critical importance for many fields where the modeling and predictive capacity is limited by the inadequate understanding of the underlying physical mechanisms.<br/><br/>The aim of this proposal is to link machine learning with model reduction in a data-stream-driven environment, in order to formulate fundamentally novel methods for the probabilistic forecast of complex stochastic systems. Of particular interest is the quantification and prediction of extreme responses, by relying exclusively on the utilization of available data and with the minimum use of equations (or high fidelity solvers), if these are available. The effort is driven by the presence of serious obstacles associated with the prediction of such features in complex dynamical systems: non-negligible model error in the descriptive laws (if these are available), prohibitive cost for real time computations, sparse data or data with non-negligible error, and transient dynamics. These difficulties are manifest at a time when there is a great need for understanding and prediction of extreme responses in contexts such as climate dynamics, nonlinear waves, and networks of high dimensionality. The aim is to address several of theses challenges by constructing new stochastic prediction methods that will extend the existing state-of-the-art for data-driven modeling and prediction through the implementation (and appropriate extension) of machine learning / data mining techniques and the combination with stochastic order reduction and uncertainty quantification methods that dynamically adapt the reduced order subspace according to the dynamics. These efforts will be guided by a proof-of-concept application involving prediction of extreme, localized events in nonlinear waves. Adaptive reduced order models driven by data will be a key element for the inference of critical dynamical properties, which are otherwise ""buried"" in the complex responses. By linking machine learning techniques to adaptive reduced order models our research will catalyze new domains of numerical/mathematical analysis and it will extend the reach of more conventional mathematics-assisted modeling beyond some of its current limits."
305,1540541,BSF: 2014414: New Challenges and Perspectives in Online Algorithms,CCF,Special Projects - CCF,9/1/15,8/6/15,Anupam Gupta,PA,Carnegie-Mellon University,Standard Grant,Nina Amla,8/31/19,"$40,000.00 ",,anupamg@cs.cmu.edu,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,CSE,2878,2878,$0.00 ,"While the traditional design and analysis of algorithms assumes that complete knowledge of the entire input is available to the algorithm, the area of online algorithms deals with the case where the input is revealed in parts, and the online algorithm is required to respond to each new part immediately upon arrival, without knowledge of the future. Previous decisions of the online algorithm cannot be revoked. Thus, the main issue in online computation is obtaining good performance in the face of uncertainty, since the future is unknown to the algorithm. The problems in this setting arise in all of computer science, as well in much of sequential decision-making, machine learning, and many other areas.<br/><br/>The proposed research is focused on a deeper investigation of the primal-dual approach to online algorithm design. The topics investigated in this project are (a) extending the success of linear optimization to the convex case, (b) relaxing monotonicity of the variables and developing principled approaches for algorithms with preemption, and (c) understanding the connection of online primal-dual approaches and online machine learning algorithms. As part of the broader impact, the research is likely to lead to better algorithms for a variety of problems both in traditional algorithm design and in other areas like machine learning and algorithmic game theory."
306,1420599,Collaborative Research: Cognitive and Neural Indicators of School-based Improvements in Spatial Problem Solving,DRL,"REAL, ECR-EHR Core Research",1/1/15,7/30/14,David Uttal,IL,Northwestern University,Standard Grant,Gregg Solomon,12/31/18,"$427,606.00 ",,duttal@northwestern.edu,750 N. Lake Shore Drive,Chicago,IL,606114579,3125037955,EHR,"7625, 7980",,$0.00 ,"This project, to be conducted by James Madison University, Georgetown University, and Northwestern University, will examine learning spatial thinking skills by high school students (who are studying geoscience), looking at educational outcomes as well as behavioral and neurological measures.  There is already considerable evidence linking spatial ability and future STEM (science, technology, engineering, and mathematics) attainment.  The project will focus on a high school course, the Geospatial Semester, that is designed to improve spatial thinking.  The project will look for changes in patterns of brain activity as a result of the course, as well as relations among the educational, behavioral, and neurological measures.  Prior research indicates that spatial training in the laboratory may reduce gender differences in performance; this project will seek to measure this effect in real world high school spatial learning, and to identify neural mechanisms that help explain how and why the gender gap closes.  This project will advance the work of the EHR (Education & Human Resources) Directorate in studying the cognitive and neural basis of STEM learning.<br/><br/>The overall goal of the project is to develop a mechanistic theory of change for spatial STEM education at the behavioral and neural levels.  To achieve this goal, the project will measure a combination of outcomes: educational (e.g., coursework), behavioral (e.g., core spatial ability on standard tests of mental rotation and embedded figure identification, use of spatial language), and neurological (e.g., neural efficiency and grey matter volume in brain regions that support spatial thinking ability, interconnectivity networks across brain regions).  Students in the spatially-based Geospatial Semester will be compared to peers receiving standard (non-spatially-based) STEM education in other advanced science courses.  Neurological measures will be obtained by functional and structural magnetic resonance imaging performed at pre- and post-test time points.  In general, the project will address the issue of brain plasticity in a high school STEM education context.  The project will use machine learning techniques to discern neurological effects of spatial learning in both hypothesis-driven and data-driven ways, and examine gender differences in the effect of spatial STEM learning on cognition and the brain.  Analyses will focus on relating changes across neural, behavioral, and educational levels toward an integrated understanding of the mechanisms that make spatial STEM learning effective."
307,1564765,III: Small: Collaborative Research: Probabilistic Models using Generalized Exponential Families,IIS,Info Integration & Informatics,9/1/15,9/24/15,Vishwanathan Swaminathan,CA,University of California-Santa Cruz,Standard Grant,Sylvia Spengler,12/31/16,"$114,649.00 ",,vishy@ucsc.edu,1156 High Street,Santa Cruz,CA,950641077,8314595278,CSE,7364,"7364, 7923",$0.00 ,"III: Small: Collaborative Research: Probabilistic Models using Generalized Exponential Families<br/>Swaminathan Vishwanathan, Purdue University; Manfred Warmuth, University of California, Santa Cruz<br/><br/>Machine learning is currently indispensible for building predictive models from massive data sets. A large majority of widely used machine learning algorithms  are based on minimizing a convex loss function. A fundamental problem with all such models is that they are not robust to outliers. To address this limitation, this project develops probabilistic models based on a parametric family of distributions, namely, the t-exponential family, that lead to quasi-convex loss functions and yield models that  are robust to outliers. <br/><br/>The key challenge when working with the t-exponential family of distributions, as in the case of the exponential family, is to compute the log-partition function and perform inference efficiently. The project addresses this challenge in two specific cases.  For problems with small number of classes exact iterative schemes are being developed. For problems where the number of classes is exponentially large, approximate inference techniques are being developed by extending variational methods. <br/><br/>In partnership with Google, some of the data mining algorithms resulting from this project are being applied to a challenging real-world problem of recognizing text in photos (the PhotoOCR problem). The project offers opportunities for research-based advanced training of graduate students as well as research opportuinities for undergraduates in machine learning and data mining. Algorithms for constructing predictive models from data that are robust in the presence of outliers are likely to find use in a broad range of applications. Open source implementions of algorithms, publications, and data sets resulting from the project are being made available through the project web page at: http://learning.stat.purdue.edu/wiki/tentropy/start"
308,1525276,CIF: Small: Active data screening for efficient feature learning,CCF,Comm & Information Foundations,9/1/15,8/18/15,Waheed Bajwa,NJ,Rutgers University New Brunswick,Standard Grant,John Cozzens,8/31/17,"$160,000.00 ",Anand Sarwate,waheed.bajwa@rutgers.edu,33 Knightsbridge Road,Piscataway,NJ,88543925,8489320150,CSE,7797,"7923, 7936",$0.00 ,"Advances in sensing and data acquisition technologies make it easy to generate vast quantities of data that must be stored, communicated, processed, and understood. This data may be of variable quality and the nature of the data may vary over time -- this variability can cause difficulties for existing approaches to efficiently represent the data. Current methods use economical representations of the data in terms of a smaller number of properties, or features, of the raw data. Standard feature representations such as Fourier and wavelet representations may not be efficient at representing the data from these new acquisition technologies. One paradigm to overcome this mismatch is the data-driven approach, in which an algorithm processes the data to learn a novel and efficient feature representation for the given data. While these are more useful, such approaches may not scale well to massive data sets.<br/><br/>This work designs new methods for data-driven feature learning that are scalable and robust to noisy, time-varying data. It proposes an ""active screening"" approach to learning new features; the data processing algorithm uses a low-complexity criterion to screen for useful and informative points for feature learning. Advantages of active screening include a reduction in the computational and storage overhead as well as the ability to reject outliers or other spurious and misleading data. The investigators develop active screening methods for consistent estimation under generative models for the data, analyze the tradeoff between representation and classification in active screening for discriminative dictionary learning, and extend the active screening analysis to distributed settings for distributed dictionary learning. They investigate the promise of these methods on two large-scale electrocardiography (ECG) datasets of 170+ patients. This work combines ideas from statistics (feature screening) and machine learning (active learning and selective sampling) to design efficient representations of complex signals from massive data sets and may inform the design of new data acquisition technologies by incorporating screening ideas into the technologies themselves."
309,1537898,"Collaborative Research: Active Statistical Learning: Ensembles, Manifolds, and Optimal Experimental Design",CMMI,OE Operations Engineering,9/1/15,8/7/15,George Runger,AZ,Arizona State University,Standard Grant,Georgia-Ann Klutke,8/31/18,"$175,000.00 ",Eugene Tuv,george.runger@asu.edu,ORSPA,TEMPE,AZ,852816011,4809655479,ENG,006Y,"071E, 078E",$0.00 ,"In numerous industries such as manufacturing, health care or energy production, current sensor technology can generate enormous quantities of measurements of an object at low cost. Each measurement consists of several instances of interrelated variables, and the goal is to use the data to build a computer model that permits one to predict the class of an object (such as the health condition of a patient or the quality of a manufactured part). Along with the sensor data, the class labels for some objects are needed to train the computer model. While the sensor variables can frequently be obtained rapidly and inexpensively (e.g., medical images or chemical analyses) the class label associated with each object might require human effort that is time-consuming and expensive. Therefore, care should be taken to select the objects to label that are most informative for building the predictive computer model. Often one selects objects iteratively, where the class labels from the previously selected batch guides the next batch of objects to label. This is the purpose of a so-called active learning strategy. The purpose of this research is to find new active learning methods that accelerate model building and provide better predictions in systems where large datasets of attribute measurements are available. This will result in more efficient and productive systems that will benefit the U.S. economy and society.<br/><br/>Existing active learning methods are often based on strong assumptions for the joint input/output distribution or use a distance-based approach. These methods are susceptible to noise in the input space, assume numerical inputs only, and often work poorly in high dimensions. In applications, data sets are often large, noisy, contain missing values and mixed variable types. In this research, a non-parametric approach to the active learning problem is proposed to address these challenges. The algorithm is based on a batch diversification strategy applied to an ensemble of decision trees. A novel active learning strategy that considers the geometric structure of the manifold where the unlabeled data resides will also be considered. The geometric properties of the data space may result in more informative active learning solutions. This is a collaborative effort between Arizona State University, Pennsylvania State University, and Intel Corporation with complementary expertise in machine learning and optimal design. The participation of Intel will help ensure the successful dissemination and broad applicability of the results."
310,1447954,SBIR Phase I:  Platform to Coordinate Personalized Learning Between Third Party Mobile Educational Apps to Improve School Readiness,IIP,SMALL BUSINESS PHASE I,1/1/15,12/3/14,Prasanna Krishnan,PA,SmartyPAL,Standard Grant,Glenn H. Larsen,6/30/15,"$150,000.00 ",,praskrish@gmail.com,631 Pine street,philadelphia,PA,191064108,6503536724,ENG,5371,"5371, 8031, 8032, 9177",$0.00 ,"This SBIR Phase I project proposes to develop a personalized adaptive learning platform for mobile, game-based education that will help improve school-readiness outcomes for young children. The use of mobile devices by children is widespread and increasing, but the existing market for learning apps falls short in providing both evidence-based educational content and engaging experiences. This project attempts to address both of these problems by building a mobile app platform consisting of expertly curated learning apps that are enhanced by adaptive personalization technology. The need for an effective solution to the challenge of preparing our youngest children for school has never been greater, particularly in light of recent findings that emphasize the impact of early education on long-term socioeconomic outcomes. One promising solution to this challenge is personalization, which is known to have strong, positive effects in education. The impact of this project will be to apply personalization in early education, thereby elevating the quality of children's educational apps and improving school-readiness by making learning fun and more effective for children. Given the growing demand for age-appropriate mobile content for children, this project has the potential to generate significant returns as a commercial enterprise and, importantly, through the economic gains resulting from improvements in educational outcomes.<br/><br/>This project's core technological innovation consists of an analytic engine, developed specifically to inconspicuously measure children's interaction with the proposed platform's educational content and dynamically adjust game-play to suit each user's individual skills, interests and educational needs. This adaptive learning technology will deliver personalized educational activities that are neither too difficult nor too easy for users, maximizing user engagement and educational outcomes. While personalization has been applied with widespread success in other markets (e.g., e-commerce, advertising, media), it has rarely been applied in early education. The proposed project will combine the personalization techniques used in these markets (collaborative filtering, machine learning) with statistical methods of psychometric assessment (item response theory) and an expert-developed curricular framework to build a mobile platform that not only recommends educational games, but also adjusts the difficulty of play within each game. As children use the platform, data will be unobtrusively gathered on their interests and proficiencies, providing evidence-based assessment of the platform?s efficacy. By observing educational results over time, the platform will ultimately serve to validate the merits of personalization technology in early education and help improve school-readiness outcomes for young children."
311,1546462,BIGDATA: Collaborative Research: F: Stochastic Approximation for Subspace and Multiview Representation Learning,IIS,"CDS&E-MSS, Big Data Science &Engineering",9/1/15,9/3/15,Han Liu,NJ,Princeton University,Standard Grant,Sylvia Spengler,8/31/18,"$400,810.00 ",,hanliu@northwestern.edu,Off. of Research & Proj. Admin.,Princeton,NJ,85442020,6092583090,CSE,"8069, 8083","7433, 8083, 8084, 8251",$0.00 ,"Unsupervised learning of useful features, or representations, is one of the most basic challenges of machine learning. Unsupervised representation learning techniques capitalize on unlabeled data which is often cheap and abundant and sometimes virtually unlimited. The goal of these ubiquitous techniques is to learn a representation that reveals intrinsic low-dimensional structure in data, disentangles underlying factors of variation by incorporating universal AI priors such as smoothness and sparsity, and is useful across multiple tasks and domains. <br/><br/>This project aims to develop new theory and methods for representation learning that can easily scale to large datasets. In particular, this project is concerned with methods for large-scale unsupervised feature learning, including Principal Component Analysis (PCA) and Partial Least Squares (PLS). To capitalize on massive amounts of unlabeled data, this project will develop appropriate computational approaches and study them in the ?data laden? regime. Therefore, instead of viewing representation learning as dimensionality reduction techniques and focusing on an empirical objective on finite data, these methods are studied with the goal of optimizing a population objective based on sample. This view suggests using Stochastic Approximation approaches, such as Stochastic Gradient Descent (SGD) and Stochastic Mirror Descent, that are incremental in nature and process each new sample with a computationally cheap update. Furthermore, this view enables a rigorous analysis of benefits of stochastic approximation algorithms over traditional finite-data methods. The project aims to develop stochastic approximation approaches to PCA and PLS and related problems and extensions, including deep, and sparse variants, and analyze these problems in the data-laden regime."
312,1458652,ABI Innovation: New methods for multiple sequence alignment with improved accuracy and scalability,DBI,ADVANCES IN BIO INFORMATICS,8/15/15,3/9/20,Tandy Warnow,IL,University of Illinois at Urbana-Champaign,Standard Grant,Peter McCartney,7/31/21,"$861,625.00 ",,warnow@illinois.edu,1901 South First Street,Champaign,IL,618207406,2173332187,BIO,1165,9251,$0.00 ,"Multiple sequence alignment (MSA) is one of the most basic bioinformatics steps, in which a set of molecular sequences (i.e., DNA, RNA, or amino acid sequences) are arranged inside a matrix to identify corresponding positions. MSA calculation is a fundamental first step in many biological analyses. Because of its broad applicability and importance, many MSA methods have been developed and are in wide use today.  Unfortunately, many real world biological datasets have features (large size and fragmentary sequences, for example) that make accurate MSA calculation very difficult.  Because poorly estimated alignments result in errors in downstream biological analyses, new MSA techniques are needed that can produce accurate alignments on difficult datasets. This project will develop MSA methods with greatly improved accuracy, and that can analyze the large and heterogeneous sequence datasets being assembled in different biology projects nationally. The project also has a substantial outreach component to women's colleges and minority serving institutions, and summer software schools to train biologists in the use of the project software.<br/><br/>Multiple sequence alignment (MSA) and phylogeny estimation are two very basic bioinformatics problems, which sit at the intersection of machine learning, statistical estimation, and evolutionary and structural biology. MSA has particular importance in constructing evolutionary trees, understanding the function and structure of proteins, detecting interactions between proteins, and even genome assembly. Large-scale MSA and phylogeny estimation also require high performance computing and parallel algorithms, in order to provide adequate scalability. The team will develop new machine learning techniques to greatly improve MSA methods, and hence also phylogeny estimation, since it depends on accurate multiple sequence alignments. The core of this project is algorithm development, utilizing a variety of machine learning techniques (including Hidden Markov Models), statistical estimation methods (especially Bayesian MCMC and maximum likelihood), and novel algorithmic strategies, all focused on improving scalability and accuracy. More information about the project can be found at: http://tandy.cs.illinois.edu/MSAproject.html"
313,1516677,Variational Problems on Random Structures: Analysis and Applications to Data Science,DMS,APPLIED MATHEMATICS,9/1/15,8/25/15,Dejan Slepcev,PA,Carnegie-Mellon University,Standard Grant,Victor Roytburd,8/31/19,"$181,107.00 ",,slepcev@math.cmu.edu,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,MPS,1266,,$0.00 ,"This research project is concerned with applying modern tools from mathematical analysis to the study of currently important topics in data science, including massive data analysis (data clouds) and machine learning.  Modern data-acquisition techniques produce a wealth of data about the world we live in.  Extracting the information from the data leads to machine learning/statistics tasks such as clustering, classification, low-dimensional embedding, and others.  This project introduces new mathematical tools for understanding of some of the state-of-art approaches to important data analysis tasks.  The conclusions gained will improve their reliability, robustness, speed, and scalability.  The insights of the analysis are expected to lay the foundation to create new models for data analysis and new approaches to the pertinent tasks.<br/><br/>This project will investigate variational problems that arise in data analysis and machine learning.  It will do so by considering variational descriptions of these problems in which the answer is obtained by minimizing an objective functional.  The project will develop a mathematical framework suitable for studies of asymptotic properties of variational problems posed on random samples and related random geometries.  In particular, it will investigate the relationship between variational problems on random discrete structures, such as a neighborhood graph of a data cloud, and continuum variational problems.  It combines the techniques of calculus of variations, applied analysis, optimal transportation, probability, and statistics to gain insights about discrete variational problems in a random setting."
314,1462241,EAGER-DynamicData: Collaborative Research: Data-driven morphing of parsimonious models for the description of transient dynamics in complex systems,ECCS,"Big Data Science &Engineering, ",9/15/15,9/11/15,Yannis Kevrekidis,NJ,Princeton University,Standard Grant,Anthony Kuh,8/31/17,"$75,000.00 ",,yannis@princeton.edu,Off. of Research & Proj. Admin.,Princeton,NJ,85442020,6092583090,ENG,"8083, O395","153E, 5384, 7916",$0.00 ,"Predicting and quantifying the behavior of complex systems in engineering and science is a topic of critical importance for many areas such as design, optimization, and safety. Even more critical is the forecast of extreme responses of these systems. Rare responses that can lead to catastrophic events manifest themselves in a wide range of systems such as geophysical phenomena, power and communication networks, and epileptic incidents in brain activity, just to mention a few. In all of these cases, accurate predictions are hampered by the fact that the exact dynamics of the system in nature is often poorly understood. This poor understanding is due to the large number of essentially coupled mechanisms that operate at different temporal and spatial scales. Although it is not always essential to predict the system with high accuracy over all these different levels, it is important to understand and model the effect of the unresolved mechanisms to the degrees of freedom we want to predict. This requires a reliable knowledge of the descriptive laws for these mechanisms as well as their coupling to the degrees-of-freedom of interest and this is clearly not always (well) done. This incomplete modeling of the dynamics leads to inevitable model error that is essential to be taken into account for reliable predictions. An obvious way that this can be done is by the utilization of available and dynamically incoming data. The goal of this work is the development of methods and algorithms to extend the capability for data-driven morphing (that is, data-driven adaptation) of parsimonious models. These will be able to adequately capture the instantaneously most significant dynamics of the system and utilize them to inexpensively perform informative prediction and uncertainty quantification. Such a development will be of critical importance for many fields where the modeling and predictive capacity is limited by the inadequate understanding of the underlying physical mechanisms.<br/><br/>The aim of this proposal is to link machine learning with model reduction in a data-stream-driven environment, in order to formulate fundamentally novel methods for the probabilistic forecast of complex stochastic systems. Of particular interest is the quantification and prediction of extreme responses, by relying exclusively on the utilization of available data and with the minimum use of equations (or high fidelity solvers), if these are available. The effort is driven by the presence of serious obstacles associated with the prediction of such features in complex dynamical systems: non-negligible model error in the descriptive laws (if these are available), prohibitive cost for real time computations, sparse data or data with non-negligible error, and transient dynamics. These difficulties are manifest at a time when there is a great need for understanding and prediction of extreme responses in contexts such as climate dynamics, nonlinear waves, and networks of high dimensionality. The aim is to address several of theses challenges by constructing new stochastic prediction methods that will extend the existing state-of-the-art for data-driven modeling and prediction through the implementation (and appropriate extension) of machine learning / data mining techniques and the combination with stochastic order reduction and uncertainty quantification methods that dynamically adapt the reduced order subspace according to the dynamics. These efforts will be guided by a proof-of-concept application involving prediction of extreme, localized events in nonlinear waves. Adaptive reduced order models driven by data will be a key element for the inference of critical dynamical properties, which are otherwise ""buried"" in the complex responses. By linking machine learning techniques to adaptive reduced order models our research will catalyze new domains of numerical/mathematical analysis and it will extend the reach of more conventional mathematics-assisted modeling beyond some of its current limits."
315,1523374,"EAGER: Models, analytics, and algorithms for data driven applications",ECCS,CCSS-Comms Circuits & Sens Sys,7/15/15,1/21/15,Zhengdao Wang,IA,Iowa State University,Standard Grant,akbar sayeed,9/30/17,"$180,000.00 ",,zhengdao@iastate.edu,1138 Pearson,AMES,IA,500112207,5152945225,ENG,7564,"153E, 7916, 9150",$0.00 ,"Data driven applications are becoming increasingly prevalent as our ability to collect data continue to increase and the cost continues to decrease. Examples of such applications include social networks (twitter, facebook, etc.), bioinformatics (gene regulatory networks, genomic sequence analysis), and environmental monitoring using wireless sensor networks. As more data<br/>become available, how to convert data to actionable information to guide decision is a pending problem that will have many applications. In addition to being ""big"", the data and underlying phenomena also exhibit time-variations which add to the<br/>difficulty of gleaning useful information from data. To cope with these challenges, this project will develop  an flexible algorithmic framework using Dynamic Bayesian Networks,  which can sufficiently describe the physical reality, and at the <br/>same time are expressive enough to allow for machine based learning, optimization, inference and adaptation to dynamical <br/>data. Such a computational framework can assimilate data continuously, adaptively, and reduce the data to information in<br/>a format that is logical, intuitive, and amenable to human interactions.Such features are particularly needed for large-scale,<br/>data-intensive engineering systems and networks such as structural health monitoring, surveillance and disease discovery <br/>using gene regulatory networks.  <br/><br/><br/>Intellectual Merit:<br/><br/>The proposed project will build on existing work on graphical models and explore the largely open area of learning dynamical graphical models based on measured data. Algorithms will be developed based on convex optimization formulation for online<br/>adaptation of the graphical models. Partially known structural information, and issues with noisy, incomplete, corrupted, <br/>or missing data will be examined. The proposed research will advance the status of the art of learning from dynamical data,<br/>and explore the many under-explored connections between signal processing, and information theory, statistics,<br/>and machine learning.<br/><br/>Broader Impact: The proposed research will develop generic models and analytical tools for data based applications. <br/>It will also generate low-complexity algorithms that can be adapted to a large number of applications such as<br/>wireless sensor networks, gene-regulatory networks, and genomic sequence analysis. The research will allow data<br/>that are made available through digital technologies to be converted to information automatically by computers,<br/>which can then be understood and acted upon by humans. The educationalgoal of this proposed project is to efficientl<br/>integrate research with educational activities and to train both undergraduate and graduate students in interdisciplinary<br/>areas to produce next-generation engineers. Efforts will be made to invite women, underpresented, and minority undergraduate<br/>students to participate in the proposed research."
316,1537427,Parametric Cost Function Approximations for Robust Energy Systems Planning,CMMI,OE Operations Engineering,9/1/15,8/23/15,Warren Powell,NJ,Princeton University,Standard Grant,Georgia-Ann Klutke,8/31/18,"$250,000.00 ",,powell@princeton.edu,Off. of Research & Proj. Admin.,Princeton,NJ,85442020,6092583090,ENG,006Y,"076E, 078E, 8023",$0.00 ,"The transition to a grid where an increasing portion of our energy comes from wind and solar energy is requiring that the grid operators develop a planning process that can handle a much higher level of uncertainty than is encountered today.  Current tools have been criticized as ad-hoc procedures that consist of optimizing around a deterministic forecast, leaving the grid vulnerable to unexpected variations.  Proposed ""stochastic models"" are much harder to solve, and grid operators still struggle to solve their existing deterministic models.  Existing industry practice, while ad-hoc, actually represents a simple example of a powerful class of policies called ""parametric cost function approximations."" that has received virtually no attention from the academic literature.  This research builds on the core strategy in use today, but blends the capabilities of existing commercial optimization solvers for deterministic models with the power of machine learning algorithms.  The work will formalize existing industry practice, providing an implementable path to handling uncertainty which will provide a way for grid operators to naturally handle the steady increase in energy from wind and solar.<br/><br/>The research proposes an algorithmic strategy that represents a fundamental departure from the field of stochastic programming, which handles uncertainty through the solution of stochastic look-ahead policies where the future is represented using a set of sampled realizations (scenarios).  This research introduces a new class of policies called parametric cost function approximations.  These are parametrically modified deterministic optimization problems which are optimized to minimize cost and risk, just as any machine learning model is optimized to fit data.  This strategy blends the power of high-dimensional statistical learning with math programming and stochastic gradient methods, which form the basis of a feedback learning algorithm for identifying the best objective function to achieve the objectives of the ISO.  Both offline and online versions of the algorithm will be developed, so policies can be tuned in a simulator (offline) but then continually adapted in production (online)."
317,1456416,SBIR Phase II:  Mobile Indoor Localization and Navigation System Using Sensory Data with Data Mining and Machine Learning Techniques,IIP,SBIR Phase II,3/1/15,5/18/15,Benjamin Balaguer,CA,Intelligent Computer Programming Labs Inc.,Standard Grant,Peter Atherton,2/28/17,"$716,141.00 ",,bbalaguer@icplabs.com,413 Saddle Rock Lane,Rio Vista,CA,945710000,4152460889,ENG,5373,"5373, 8032, 8039",$0.00 ,"The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project will result from a revolution in the way buildings are used. If successfully implemented, the technology offers a solution to both end-users and companies. Users inside buildings will have access to floorplans, points of interest, location-based information, and turn-by-turn directions directly on their smartphones. Companies will be able to offer new experiences to customers, analyze their movements, and provide them with targeted information or advertisements when and where they need them. Other applications of the technology will provide considerable societal benefits: (i) first responders will be able to accurately localize victims, thus reducing response times and saving lives; (ii) building managers will be able to condition rooms in real-time based on their occupancies, substantially reducing energy consumption; (iii) people with disabilities will be able to obtain assistance by finding wheelchair-accessible routes; (iv) warehouse managers will be able to reduce order fulfillment time; (v) service providers (e.g., hospitals, military, IT) will be able to track, dispatch, and more efficiently manage critical workforce personnel (e.g. doctors and technicians).  Indoor localization is expected to be, in the near future, as pervasive as GPS is today.<br/><br/>This Small Business Innovation Research (SBIR) Phase II project will further develop the company's indoor localization technology and deploy it to mobile devices. By analyzing and processing smartphones' accelerometer, cellular, magnetometer, orientation, and WiFi sensor data, a building's sensory blueprint can be created. The building's sensory blueprint can then be exploited to localize people holding smartphones, by means of a combination of machine learning, data mining, sensor fusion, and statistical, tracking, and path planning algorithms. The project aims to develop and implement the following software services: (i) a mapping service that converts a smartphone's sensor readings into a sensory blueprint; (ii) a localization service that allows end-users to view their location inside buildings on their smart mobile devices; (iii) a navigation service that provides paths and turn-by-turn directions to points of interest; (iv) a location-based service that presents interesting information in the user's vicinity; (v) a behavior analytics engine that displays statistical information about a user's movement or building's utilization; (vi) a software package that facilitates the technology's distribution. These services will be deployed within a server-client framework, alleviating the space, memory, and computational constraints imposed by mobile devices."
318,1521972,SCH: INT: Collaborative Research: Diagnostic Driving: Real Time Driver Condition Detection Through Analysis of Driving Behavior,IIS,Smart and Connected Health,9/1/15,5/16/19,Avelino Gonzalez,FL,The University of Central Florida Board of Trustees,Standard Grant,Wendy Nilsen,8/31/20,"$330,011.00 ",Annie Wu,gonzalez@ucf.edu,4000 CNTRL FLORIDA BLVD,Orlando,FL,328168005,4078230387,CSE,8018,"8018, 8062, 9251",$0.00 ,"The automobile presents a great opportunity for healthcare monitoring. For one, most Americans engage in daily driving, and patient's time spent in vehicles is a missed opportunity to monitor their condition and general wellbeing. The goal of this project is to develop and evaluate technology for automatic in-vehicle monitoring of early symptoms of medical conditions and disrupted medications of patients, and to provide preventive care. Specifically, in this project we will focus on Attention-Deficit/Hyperactivity disorder (ADHD) in teenagers and young adults, a prevalent chronic medical condition which when uncontrolled has the potential for known negative health and quality of life consequences. The approach of using driving behavior to monitor ADHD symptoms could be applied to many other medical conditions (such as diabetes, failing eyesight, intoxication, fatigue or heart attacks) thereby transforming medical management into real-time sensing and management. Identification of all these conditions from driving behavior and alerting the proper agent could transform how we think about health monitoring and result in saved lives and reduced injuries.<br/><br/>The main goal of this project is to leverage the large amounts of health data that can be collected while driving via machine learning, in order to detect subtle changes in behavior due to out-of-control ADHD symptoms that can, for example, indicate the onset of episodes of inattention before they happen. Via lab-based driving simulator as well as on-road studies, the research team will investigate the individualized behaviors and patterns in vehicle control behaviors that are characteristic of ADHD patients under various states of medication usage. The team will develop a machine learning framework based on case-based and context-based reasoning to match the current driving behavior of the patient with previously recorded driving behavior corresponding to different ADHD symptoms. The key machine learning challenge is to define appropriate similarity measures to compare driving behavior that take into account the key distinctive features of ADHD driving behavior identified during our study. The team will evaluate the accuracy with which the proposed approach can identify and distinguish between different out-of-control ADHD symptoms, which are the implications for long-term handling of ADHD patients, via driving simulator experiments as well as using instrumented cars with real patients."
319,1521943,SCH: INT: Collaborative Research: Diagnostic Driving: Real Time Driver Condition Detection Through Analysis of Driving Behavior,IIS,Smart and Connected Health,9/1/15,5/31/18,Santiago Ontanon,PA,Drexel University,Standard Grant,Wendy Nilsen,8/31/20,"$491,940.00 ",,santi@cs.drexel.edu,"1505 Race St, 10th Floor",Philadelphia,PA,191021119,2158955849,CSE,8018,"8018, 8062, 9251",$0.00 ,"The automobile presents a great opportunity for healthcare monitoring. For one, most Americans engage in daily driving, and patient's time spent in vehicles is a missed opportunity to monitor their condition and general wellbeing. The goal of this project is to develop and evaluate technology for automatic in-vehicle monitoring of early symptoms of medical conditions and disrupted medications of patients, and to provide preventive care. Specifically, in this project we will focus on Attention-Deficit/Hyperactivity disorder (ADHD) in teenagers and young adults, a prevalent chronic medical condition which when uncontrolled has the potential for known negative health and quality of life consequences. The approach of using driving behavior to monitor ADHD symptoms could be applied to many other medical conditions (such as diabetes, failing eyesight, intoxication, fatigue or heart attacks) thereby transforming medical management into real-time sensing and management. Identification of all these conditions from driving behavior and alerting the proper agent could transform how we think about health monitoring and result in saved lives and reduced injuries.<br/><br/>The main goal of this project is to leverage the large amounts of health data that can be collected while driving via machine learning, in order to detect subtle changes in behavior due to out-of-control ADHD symptoms that can, for example, indicate the onset of episodes of inattention before they happen. Via lab-based driving simulator as well as on-road studies, the research team will investigate the individualized behaviors and patterns in vehicle control behaviors that are characteristic of ADHD patients under various states of medication usage. The team will develop a machine learning framework based on case-based and context-based reasoning to match the current driving behavior of the patient with previously recorded driving behavior corresponding to different ADHD symptoms. The key machine learning challenge is to define appropriate similarity measures to compare driving behavior that take into account the key distinctive features of ADHD driving behavior identified during our study. The team will evaluate the accuracy with which the proposed approach can identify and distinguish between different out-of-control ADHD symptoms, which are the implications for long-term handling of ADHD patients, via driving simulator experiments as well as using instrumented cars with real patients."
320,1539622,CyberSEES: Type 2: Collaborative Research: Cyber-infrastructure and Technologies to Support Large-Scale Wildlife Monitoring and Research for Wildlife and Ecology Sustainability,CCF,CyberSEES,11/1/15,5/22/18,Roland Kays,NC,Friends of the North Carolina State Museum of Natural Sciences,Standard Grant,David Corman,10/31/19,"$309,982.00 ",,rwkays@ncsu.edu,11 West Jones Stree,Raleigh,NC,276011029,9197079847,CSE,8211,"8208, 8231",$0.00 ,"Minimizing the impact of human actions on wildlife is a priority for conservation biology.  Distributed motion-sensitive cameras, or camera traps, are popular tools for monitoring wildlife populations.  Recent work has shown that camera trap surveys can be expanded to large scales by crowdsourcing through citizen science, producing big data sets needed to evaluate the effect of sustainability strategies on wildlife populations.  However, these large-scale surveys create millions of photographs that create new challenges for data processing and quality control. <br/><br/>This project seeks to develop advanced computing technologies for cloud-based large-scale data sensing, analysis, annotation, management, and preservation for wildlife and ecological sustainability to inform effective resource management, decision-making, and polices on human actions to protect wildlife and natural resources. Specifically, the project aims to: (1) explore a citizen scientist-based approach and system for large-scale sustainable data collection; (2) develop deep-learning based fine-grain animal species recognition from large data sets and automated content annotation; (3) study cloud-based computing with thin-client access and resource allocation for scalable deployment and easy access by citizen scientists; and (4) develop a comprehensive data annotation quality monitoring and control framework with tightly coupled computer annotation, crowd-sourcing, and expert review to ensure a high scientific standard of data quality. These tools will be integrated into the eMammal infrastructure to study three questions on wildlife sustainability: energy development, housing development, and wildlife harvest.  These tools will also be available to other wildlife researchers using the eMammal system, enabling an improved understanding how humans can live sustainably with wildlife. <br/><br/>Collaborative wildlife monitoring and tracking at large geographical and time scales will contribute to the understanding of complex dynamics of wildlife systems, and provide important scientific evidence for informed decisions and effective solutions to sustainability issues in wildlife environments. This project will provide unique, exciting, and interdisciplinary opportunities for mentoring graduate students and involving K-12 and undergraduate students into professionally guided research.  The citizen science approach used in this project should accommodate hundreds of students in research."
321,1510741,RI: Medium: Machine Learning for Agricultural and Medical Entomology,IIS,Robust Intelligence,10/1/15,7/29/15,Eamonn Keogh,CA,University of California-Riverside,Standard Grant,Rebecca Hwa,9/30/19,"$1,100,000.00 ","Christian Shelton, Anupama Dahanukar",eamonn@cs.ucr.edu,Research & Economic Development,RIVERSIDE,CA,925210217,9518275535,CSE,7495,"7495, 7924",$0.00 ,"This award will enable a team of computer scientists and entomologists at the University of California-Riverside to develop sensors and software that will allow the classification of flying insects. The ability to automatically and accurately classify flying insects has the potential to have significant impact human affairs, because insects spread disease, feed on crops and livestock, and ruin food stores, at a combined annual cost of billions of dollars and incalculable human suffering.<br/> <br/>The intellectual merit of the project is in producing algorithms, devices, and procedures that will radically expand the ability to conduct insect surveillance. Recent advances in sensor technology and machine learning and the ongoing revolution in Big Data are just beginning to enable development of advanced algorithms that will help usher in a new era of computational entomology. The investigators will build inexpensive devices that can detect and classify flying insects. For at least some genera of insects the resulting classification labels will go beyond species-level to predict sex and physiological states (such as virgin vs. gravid and newly emerged vs. mature) of individual insects.  The investigators will create computational devices that can determine the origin of selected insects, and selectively capture targeted insects for downstream molecular diagnostic analysis. Producing such information will both accelerate basic research in entomology and will allow more effective vector control. The broader impacts of the project are inherent in the potential to significantly improve the quality and volume of insect surveillance, thus allowing more effective Integrated Vector Management. In the case of mosquitoes, more effective interventions are known to directly save lives. Resulting algorithms will allow the creation of systems to provide actionable information on multiple scales, from informing a policy committee to instructing an agricultural robot to open a valve. The projects comprehensive educational and outreach activities have already been piloted on a small scale and include detailed plans to reach out to underserved communities at the K-12 and college levels."
322,1521959,SCH: INT: Collaborative Research: Diagnostic Driving: Real Time Driver Condition Detection Through Analysis of Driving Behavior,IIS,Smart and Connected Health,9/1/15,8/13/15,Yi-Ching Lee,PA,The Children's Hospital of Philadelphia,Standard Grant,diwakar gupta,10/31/16,"$1,021,302.00 ",Flaura Winston,ylee65@gmu.edu,Research Administration,Philadelphia,PA,191044318,2674260122,CSE,8018,"8018, 8062",$0.00 ,"The automobile presents a great opportunity for healthcare monitoring. For one, most Americans engage in daily driving, and patient's time spent in vehicles is a missed opportunity to monitor their condition and general wellbeing. The goal of this project is to develop and evaluate technology for automatic in-vehicle monitoring of early symptoms of medical conditions and disrupted medications of patients, and to provide preventive care. Specifically, in this project we will focus on Attention-Deficit/Hyperactivity disorder (ADHD) in teenagers and young adults, a prevalent chronic medical condition which when uncontrolled has the potential for known negative health and quality of life consequences. The approach of using driving behavior to monitor ADHD symptoms could be applied to many other medical conditions (such as diabetes, failing eyesight, intoxication, fatigue or heart attacks) thereby transforming medical management into real-time sensing and management. Identification of all these conditions from driving behavior and alerting the proper agent could transform how we think about health monitoring and result in saved lives and reduced injuries.<br/><br/>The main goal of this project is to leverage the large amounts of health data that can be collected while driving via machine learning, in order to detect subtle changes in behavior due to out-of-control ADHD symptoms that can, for example, indicate the onset of episodes of inattention before they happen. Via lab-based driving simulator as well as on-road studies, the research team will investigate the individualized behaviors and patterns in vehicle control behaviors that are characteristic of ADHD patients under various states of medication usage. The team will develop a machine learning framework based on case-based and context-based reasoning to match the current driving behavior of the patient with previously recorded driving behavior corresponding to different ADHD symptoms. The key machine learning challenge is to define appropriate similarity measures to compare driving behavior that take into account the key distinctive features of ADHD driving behavior identified during our study. The team will evaluate the accuracy with which the proposed approach can identify and distinguish between different out-of-control ADHD symptoms, which are the implications for long-term handling of ADHD patients, via driving simulator experiments as well as using instrumented cars with real patients."
323,1458626,"Bilateral BBSRC-NSF/BIO: Mining of imaging flow cytometry data for label-free, single cell analysis",DBI,ADVANCES IN BIO INFORMATICS,7/15/15,7/16/15,Anne Carpenter,MA,"Broad Institute, Inc.",Standard Grant,Peter H. McCartney,6/30/18,"$394,349.00 ",Paul Rees,anne@broadinstitute.org,415 Main Street,Cambridge,MA,21421401,6177147000,BIO,1165,,$0.00 ,"The goal of this project is to develop and demonstrate software to mine data from imaging flow cytometers. These instruments can capture thousands of images of cells per second. The images can in theory be analyzed to precisely measure hundreds of features related to a cell's appearance (""morphology""); this project is to develop advanced machine-learning software to accomplish this, unlocking the otherwise hidden information within the images. The software will be developed, improved, and validated in several demonstration experiments involving the cell cycle, the component cells of primary blood, immune cell activation, and stem cell identity. The goal will be to use as few or indeed no fluorescent biomarkers, eliminating the need to perturb cells. The resulting open-source software will be freely available to scientists worldwide for both applied and clinical research, and will be accompanied by user-friendly training materials and in-person workshops. The project is collaborative and interdisciplinary and includes training early career-stage scientists in computational biology, via the existing Scientists without Borders program.<br/><br/>The project involves close collaboration with researchers using imaging flow cytometers and builds on successful interdisciplinary work in biological data mining. In order to devise the novel software and methodology to mine the large datasets acquired using imaging flow cytometry, the team will develop algorithms to seamlessly import data from an imaging cytometer, robustly segment cells, quality-filter them (e.g., for debris and blur), and quantify morphological parameters (usually hundreds) for each cell (usually thousands), including various measures of size, shape, and texture. Using these features, trained machine-learning algorithms will identify cell phenotypes of interest or otherwise characterize the state of cell in driving biological projects from project partners who use imaging flow cytometry in a host of biological research studies. The goal will be to use as few or indeed no fluorescent biomarkers, eliminating the need to perturb cells. The project will give the scientific community a validated, open-source software toolbox of image processing and machine learning algorithms readily usable by biologists. More information about the project can be found at: http://www.broadinstitute.org/~anne/Carpenter_NSF_ImagingFlowCytometry.html"
324,1462773,Distinguishing Between Human Activities in Real-Time Based on Wearable Sensor Data Using a Low-dimensional Model of Human Movement,CMMI,"Dynamics, Control and System D",5/1/15,4/24/15,Prashant Mehta,IL,University of Illinois at Urbana-Champaign,Standard Grant,Irina Dolinskaya,4/30/20,"$346,551.00 ",,mehtapg@illinois.edu,1901 South First Street,Champaign,IL,618207406,2173332187,ENG,7569,"030E, 031E, 032E, 033E, 034E, 035E, 099E, 8024",$0.00 ,"Human Activity Recognition is the process of inferring human activity from motion sensors such as accelerometer and gyroscopes worn on the human body.  These motion sensors are embedded in physical activity tracking devices and SmartWatches.  Accurate inference of human activity offers many benefits for health monitoring and for promoting overall wellness of an individual.  There are many machine learning-based approaches to human activity recognition from sensor data.  However, in practice, the current approaches often suffer from poor accuracy on account of uncertainty due to wide range of human body types, sensor locations on the body and real-time sources of uncertainty such as changes in activity speed and intensity, sensor noise, packet drops, sensor failure etc.  This award supports fundamental research to provide needed knowledge for the development of robust activity recognition methodology and algorithms.  It is projected that there will be a trillion embedded sensors in connected people and devices by 2020. This project's algorithmic and software tools can potentially be directly applied to fitness monitoring, eldercare support, long-term preventive and chronic care, and rehabilitation.  Therefore, results from this research will benefit the US economy and society.  To promote transitions, several educational initiatives are planned that seek to engage undergraduate students in entrepreneurship.<br/>  <br/>A major objective of the research concerns development of methods and algorithms to mitigate uncertainty in machine learning problems, such as the activity recognition problem, involving dynamic data sets.  A control-theoretic framework is planned to not only address the robustness issues due to uncertainty, but also enable certain unified architectures for learning patterns from sensor data.  If successful, the work can lead to novel algorithmic approaches to represent, learn and recognize hidden low order patterns in unstructured dynamic data sets.  Besides methodological developments, this project will engineer other more tangible outcomes such as the development of algorithms and software for feedback particle filter, enunciation of control architectures and algorithms for representation of complex patterns in data, and development of software tools for the human activity recognition system."
325,1539389,CyberSEES: Type 2: Collaborative Research: Cyber-infrastructure and Technologies to Support Large-Scale Wildlife Monitoring and Research for Wildlife and Ecology Sustainability,CCF,CyberSEES,11/1/15,8/25/15,Zhihai He,MO,University of Missouri-Columbia,Standard Grant,David Corman,10/31/20,"$699,489.00 ","Tony Han, Joshua Millspaugh",hezhi@missouri.edu,115 Business Loop 70 W,COLUMBIA,MO,652110001,5738827560,CSE,8211,"8208, 8231, 9150",$0.00 ,"Minimizing the impact of human actions on wildlife is a priority for conservation biology.  Distributed motion-sensitive cameras, or camera traps, are popular tools for monitoring wildlife populations.  Recent work has shown that camera trap surveys can be expanded to large scales by crowdsourcing through citizen science, producing big data sets needed to evaluate the effect of sustainability strategies on wildlife populations.  However, these large-scale surveys create millions of photographs that create new challenges for data processing and quality control. <br/><br/>This project seeks to develop advanced computing technologies for cloud-based large-scale data sensing, analysis, annotation, management, and preservation for wildlife and ecological sustainability to inform effective resource management, decision-making, and polices on human actions to protect wildlife and natural resources. Specifically, the project aims to: (1) explore a citizen scientist-based approach and system for large-scale sustainable data collection; (2) develop deep-learning based fine-grain animal species recognition from large data sets and automated content annotation; (3) study cloud-based computing with thin-client access and resource allocation for scalable deployment and easy access by citizen scientists; and (4) develop a comprehensive data annotation quality monitoring and control framework with tightly coupled computer annotation, crowd-sourcing, and expert review to ensure a high scientific standard of data quality. These tools will be integrated into the eMammal infrastructure to study three questions on wildlife sustainability: energy development, housing development, and wildlife harvest.  These tools will also be available to other wildlife researchers using the eMammal system, enabling an improved understanding how humans can live sustainably with wildlife. <br/><br/>Collaborative wildlife monitoring and tracking at large geographical and time scales will contribute to the understanding of complex dynamics of wildlife systems, and provide important scientific evidence for informed decisions and effective solutions to sustainability issues in wildlife environments. This project will provide unique, exciting, and interdisciplinary opportunities for mentoring graduate students and involving K-12 and undergraduate students into professionally guided research.  The citizen science approach used in this project should accommodate hundreds of students in research."
326,1533917,XPS: EXPL: DSD: Portal: A Language and Compiler for Parallel N-body Computations,CCF,"Software & Hardware Foundation, Exploiting Parallel&Scalabilty",7/1/15,4/28/17,Aparna Chandramowlishwaran,CA,University of California-Irvine,Standard Grant,Anindya Banerjee,6/30/19,"$316,000.00 ",,amowli@uci.edu,"141 Innovation Drive, Ste 250",Irvine,CA,926173213,9498247295,CSE,"7798, 8283","7943, 9251",$0.00 ,"Title: XPS: EXPL: DSD: Portal: A Language and Compiler for Parallel N-body Computations<br/><br/>Modern machines are becoming increasingly more complex resulting in even the most advanced compilers failing to generate the best optimized code. As a result, there is a big gap between the algorithm one designs on paper and the code that runs efficiently on a billion-core system. This research project aims to bridge this gap by developing Portal, a new high-performance domain-specific language(DSL) and compiler, for the domain of N-body problems. Such problems have applications in various areas ranging from scientific computing simulations in molecular dynamics, astrophysics, acoustics, fluid dynamics all the way to big data problems. In Portal, domain scientists can write programs at a high level while obtaining the performance of highly tuned and optimized low level code written by experts on modern massively parallel machines. The intellectual merit is to show how a DSL with a high-level formulation can lead directly to both asymptotically fast algorithms and their efficient parallel implementations on a variety of distinct architectures. The project's broader significance and importance are freely available software to enable domain scientists to harness the performance power of parallel computing and enabling scientific discovery not only in scientific computing and machine learning but also in a number of related problems in domains such as statistics, computer graphics, computational geometry, and applied mathematics. These problems can be expressed in Portal to obtain an out-of-the-box parallel optimized implementation.<br/><br/>The goals of Portal are three-fold: (a) to implement scalable, fast, and asymptotically optimal tree-based N-body algorithms, (b) to design an intuitive language and API to enable rapid implementations of a variety of algorithms, and (c) to enable parallel large-scale problems to run on current and future exascale machines. More importantly, the language and intermediate algorithm representation are independent of the architecture, making this approach portable and easily extensible to different platforms from ARM/x86 CPUs to GPUs. The project aims to solve important software issues that will allow for interoperability and scalability of N-body problems on massive datasets."
327,1464219,CRII: RI: Semiparametric Approaches to Learning Robot Dynamics,IIS,"Robust Intelligence, NRI-National Robotics Initiati",7/1/15,5/27/15,Byron Boots,GA,Georgia Tech Research Corporation,Standard Grant,Reid Simmons,6/30/16,"$75,000.00 ",,bboots@cs.washington.edu,Office of Sponsored Programs,Atlanta,GA,303320420,4048944819,CSE,"7495, 8013","7495, 8086, 8228",$0.00 ,"Robotics is revolutionizing quality of life in a wide range of domains from healthcare to automobile safety. Parametric modeling techniques are fundamental to robotic prediction and control in these domains, employed with great success when a robot's interaction with an environment can be precisely characterized by Newtonian physics. As complex robotic technology moves into natural and human environments, it is becoming more difficult to robustly characterize these interactions a priori with parametric models. As a result, machine learning is an increasingly important tool: models of complicated and noisy dynamics can be directly learned from a robot's interaction with its environment. In particular, nonparametric learning has shown exceptional promise, often outperforming parameterized, physics-based models when applied to difficult modeling problems.  However, nonparametric approaches also have practical drawbacks: they do not incorporate prior knowledge such as physics-based insights and constraints, they are data-intensive, and they often incur significant computational cost. <br/><br/>This is an exploratory investigation of how nonparametric statistical models can be better integrated with parametric physics-based models for robot prediction and control. The focus of this project is on developing new semiparametric models that elegantly compose parametric and nonparametric components for accurate, robust modeling."
328,1552288,"EAGER: BIGDATA: SMART Data - Academic Success Made Affordable, Rapid, and Timely through Integrated Data Analytics",IIS,S-STEM-Schlr Sci Tech Eng&Math,10/1/15,9/14/15,Krishna Madhavan,IN,Purdue University,Standard Grant,John Cherniavsky,9/30/17,"$300,000.00 ","Matthew Ohland, Heidi Diefes-Dux, Mihaela Vorvoreanu, Michael Zentner",cm@purdue.edu,Young Hall,West Lafayette,IN,479072114,7654941055,CSE,1536,"7433, 7916, 8083",$0.00 ,"Academic Success Made Affordable, Rapid, and Timely through Integrated Data Analytics<br/><br/>Data science techniques have revolutionized many academic fields and led to terrific gains in the commercial sector. They have to date been underutilized in solving critical problems in the US educational system, particularly in understanding Science, Technology, Engineering and Mathematics (STEM) learning and learning environments, broadening participation in STEM, and increasing retention for students traditionally underserved in STEM. The goals of the Directorate for Education and Human Resources through the Critical Techniques and Technologies for Advancing Foundations and Applications of Big Data Science & Engineering (BIGDATA) program are to advance fundamental research aimed at understanding and solving these critical problems, and to catalyze the use of data science in Education Research. This Early Concept Grant for Exploratory Research (EAGER) will seek to understand the background and backbone of systems of data processing and analytics that can provide insights using standard types of data that colleges and universities already have (for example, from their Learning Management Systems, administrative data systems, and advising systems) to provide predictions and recommendations to students and instructors to increase the percentage of students who succeed in college and graduate on time. This has terrific potential to increase graduation rates at two- and four-year institutions, which can lower college costs for families and students. In addition, it has terrific potential to increase the attraction and retention of underrepresented minorities in STEM fields, as it will identify barriers for all students to graduating successfully and on time.<br/><br/>The team vision is to eventually build a data platform that integrates key data sources and provides tools that leverage important insights from these sources. The types of data sources are: 1) longitudinal data from undergraduate students on their academic trajectories in STEM majors; 2) Learning management system (LMS) records of student activity, and 3) text data from a variety of sources, such as advisors. The tools will include a 1) GUI for students, teachers and administrators to see student academic pathways and 2) a portal for communication between students, faculty and advisors. This system will help students, teachers and admistrators engage in Data Driven Decision Making (D3M) around academic pathways to successful and on time graduation from college. This Early Concept Grant for Exploratory Research (EAGER) will solve many challenges that must be undertaken to achieve this vision and provide the solutions to those challenges to relevant communities in a variety of ways.  The team has four goals for this early concept project. The first is to examine existing data and predictive models for understanding student success and retention and build a catalog of applications and data types available for decision making around pathways to these outcomes. The second goal is to describe design specifications, including data types, algorithms, and machine learning techniques that are needed to build the data platform. The third is to pilot research on critical elements of a D3M training program for undergraduate students, instructors and administrators. The final goal is to develop a design framework that builds ethics, privacy and security in from the ground up. All products will be made available as publications, curricula, or software shared openly on common forums such as GitHub."
329,1514204,III: Medium: High-Dimensional Interaction Analysis in Bio-Data Sets,IIS,"Info Integration & Informatics, Unallocated Program Costs",8/15/15,2/15/19,Jing Gao,NY,SUNY at Buffalo,Standard Grant,Sylvia Spengler,4/30/19,"$1,031,252.00 ","Murali Ramanathan, Alan Hutson, Jo Freudenheim",jing@buffalo.edu,520 Lee Entrance,Buffalo,NY,142282567,7166452634,CSE,"7364, 9199","7364, 7924",$0.00 ,"Discovering interactions between the attributes in a data set provides insight into the underlying structure of the data and explains the relationships between the attributes. This project develops multi-disciplinary approaches that integrate computer science, statistics, and epidemiology techniques to mine interaction relationships among attributes and phenotypes (traits or class labels) in biological data sets. Specifically, this project develops innovative and statistically sound methodologies for mining novel interactions within attributes or between attributes and phenotypes to help identify critical factors in biological applications.  In particular, the novel analysis methods can enable the genetic and environmental interactions underlying a range of complex diseases to be delineated. The research activities of this project can also promote the integration of biology, computer science, and statistics, which is highly significant to many applications. <br/><br/>The project will formulate various metrics that enable efficient pruning and searching in the multi-dimensional combinatorial space for identifying significant interaction relationships. This enables highly effective approaches that build search-based trees or identify highly correlated subspaces to detect meaningful local interactions that may not be significant considering the whole data sets but are strongly interacted with traits on a subset of data. This enables comparison of data from multiple different groups such as based on age, race, or other properties. It is important to find both common and different interactions in different groups so that effective methods can be developed for targeted groups. The methods developed will detect complex interactions between attributes in multiple groups simultaneously by capturing both their commonalities and differences in joint matrix factorization or deep learning models. These approaches are remarkably powerful for biological applications, such as detecting gene-gene interactions and gene-environmental interactions that lead to breast cancer. The concept of interaction is also ubiquitous and important in many scientific disciplines ranging from economics, sociology and physics, to the pharmaceutical sciences. The novel approaches and analysis tools developed in this project are useful for finding out any interaction relationships between attributes associated with phenotype labels or without phenotype labels. These approaches and tools are general and are applicable to a variety of applications."
330,1505388,I-Corps:  WorkReadyGrad application,IIP,I-Corps,2/1/15,1/21/15,Caitlin Dooley,GA,"Georgia State University Research Foundation, Inc.",Standard Grant,Rathindra DasGupta,7/31/15,"$50,000.00 ",,cdooley@gsu.edu,58 Edgewood Avenue,Atlanta,GA,303032921,4044133570,ENG,8023,,$0.00 ,"Economists and educators are grappling with how to best prepare students for a rapidly changing economic landscape.  Research suggests that changing workforce needs will require students to develop transferable knowledge and skills through deep learning connections between school, work, and life. This I-Corps team will explore the launch of an application called WorkReadyGrad.com which seeks to close the skills gap by elevating the quality and frequency of dialogue between students and potential employers. The proposed work is a web-based social networking and electronic-portfolio tool that facilitates this connection, and connects students to professionals for career and educational mentoring in STEM fields. Made for students in middle school through college and professionals across a wide range of careers, the tool can connect students with information, resources, and potential employers. Complementing this student service, the proposed tool will enable employers to communicate future hiring needs and to groom new talent. <br/><br/>Specific to this I-Corps application, WorkReadyGrad intends to engage in customer discovery by engaging companies and civic institutions to build a more diverse pool of STEM workers. By launching in Atlanta, WorkReadyGrad is able to connect colleges and employers with an underserved, under-utilized workforce that is more than 50% African American.. Short-term success will be gauged by skill sets and certifications that students obtain and the growth of their professional networks. Short-term success will also be gauged by the number of professional mentors enrolled from public and private employers and the quality of their engagements within the WorkReadyGrad environment. Long-term success will be measured by enrollment and completion of post-secondary education and by the quantity and quality of job placements from post-secondary institutions. It is anticipated that the time horizon to short-term impact is approximately 3-12 months and to long-term impact is approximately 2-10 years."
331,1547102,EAGER:   Cybermanufacturing: Predictive Analytics Models and Techniques for Intelligent Cybermanufacturing,CMMI,SSA-Special Studies & Analysis,9/1/15,8/14/15,Yan Wang,GA,Georgia Tech Research Corporation,Standard Grant,Bruce Kramer,8/31/17,"$100,000.00 ",Ling Liu,yan.wang@me.gatech.edu,Office of Sponsored Programs,Atlanta,GA,303320420,4048944819,ENG,1385,"082E, 083E, 152E, 7916, 8083, 9146, MANU",$0.00 ,"Enabling information infrastructure is the major element of the emerging service-oriented cybermanufacturing paradigm. Quality and cost are the major issues that 3D printing service providers (i.e. manufacturers) are facing as they are the critical measures for keeping their services competitive and affordable. The bidding price from a manufacturer or 3D printing service provider for a submitted job is crucial in market competition. This EArly-concept Grant for Exploratory Research (EAGER) project is the development of three data analytics approaches in a pilot prototype system to provide accurate cost estimation of 3D printing jobs. They are shape mining, user-product usage based analytics, and a hybrid approach combining shape mining with statistical mining of user-oriented data for enhanced cost prediction. <br/><br/>The data analytics models and service-oriented framework will provide a generic and systematic approach to improve the efficiency of next-generation cybermanufacturing infrastructure, enabling manufacturers to make timely and sound decisions based on evidence and insight derived from deep learning from big data. This data science-based approach will enable better understanding of the power and the potential of cybermanufacturing in manufacturing efficiency enhancement and decision making."
332,1534790,SBIR Phase II:  Personalized Reading Instruction,IIP,SBIR Phase II,9/1/15,9/22/17,Jay Goyal,WA,Actively Learn Inc.,Standard Grant,Rajesh Mehta,2/28/19,"$1,010,000.00 ",,jay@activelylearn.com,220 2nd Ave S,Seattle,WA,981042617,8575406670,ENG,5373,"165E, 5373, 8031, 8032, 8040, 8240, 9177",$0.00 ,"This SBIR Phase II project proposes to discover digital methods to personalize reading instruction such that students understand more when they read, retain knowledge, and build lasting skills. The academic research on reading supports the claim that active reading strategies that incorporate quality instruction can benefit students. However, instruction is usually not personalized to meet the needs of specific students, and even when an educator works 1:1 with a student they can only interpret a limited number of signals from a student to help guide instruction. The objective of the project is to take in several inputs when students read digitally and investigate whether personalized reading instruction can effectively be created and delivered such that students get extra help when they struggle and are challenged when they can succeed on their own. Two-thirds of students in the U.S. are struggling readers; they cannot understand the main idea when they read. These students are four times more likely to drop out of school. People who read critically have more success in school, obtain high quality jobs, and are able to contribute more to expand social resources. Researchers and educators have been trying to solve the ""reading gap"" for decades, but only now does the technology exist to make this possible.<br/><br/>This SBIR Phase II project proposes to use unique machine learning techniques to personalize reading instruction. The algorithms to personalize instruction will ensure that extra help, or scaffolding, is allocated to the students who need it, and removed when they no longer need it or when it threatens to become a crutch. This approach is different than other machine learning algorithms, which are built to minimize the overall error or maximize the overall reward. However, what is required for personalized reading instruction is different. The algorithm must learn how much to help a student not so they perform better with help, but so they perform better without it because the goal is for students to become better readers in the long term, not become reliant on scaffolding to read. The objective of the research is to fully develop and commercialize this personalized reading system and will involve data science, application development, and content authoring."
333,1513957,SBE: Medium: Towards Personalized Privacy Assistants,SES,Secure &Trustworthy Cyberspace,9/15/15,8/12/19,Norman Sadeh,PA,Carnegie-Mellon University,Standard Grant,Mark Hurwitz,8/31/20,"$499,189.00 ",Yuvraj Agarwal,sadeh@cs.cmu.edu,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,SBE,8060,"7434, 7924, 9178, 9179, 9251",$0.00 ,"Whether it is on their smartphones, in their browsers or on social networks, people are confronted with an increasingly unmanageable number of privacy settings. What is needed is a new, more scalable paradigm that empowers them to regain control over the collection and use of their data. This is particularly the case for mobile apps people download on their smartphones. These apps have been shown to collect and share a wide variety of sensitive data, with users unable to keep up.  If all users felt the same way about the data collection and sharing practices of mobile apps, it would easy to have the apps pre-configured to only allow for those practices with which they are comfortable. Unfortunately, prior research has shown that this is not the case. This project is studying novel technology intended to significantly simplify the process of configuring privacy settings such as those associated with mobile apps. With close to 200 million US smartphone users, each with an average of nearly 50 mobile apps on their devices, this project could have a significant impact on the privacy of everyday Americans. <br/><br/>Specifically, this research harnesses recent advances in privacy preference modeling, machine learning and dialogue technologies to develop personalized privacy assistants capable of learning people's privacy preferences and of semi-automatically configuring many privacy settings on their behalf.  The researchers are evaluating different configurations of personalized privacy assistants, focusing in particular on human subject experiments intended to evaluate their impact on privacy decision making and user behavior. Configurations being evaluated differ in the style and frequency of dialogues with users, the way in which machine learning is used to drive these dialogues and the level of automation in configuring privacy settings. Human subject experiments look at factors that include the impact of different configurations of the technologies on the level of comfort users have with their privacy settings, their overall awareness and sense of control, and both short-term and long-term behavioral effects. Other important factors include user burden, frequency of interruptions and overall user satisfaction."
334,1642608,"IMA SUMMER SCHOOL ON MODERN APPLICATIONS OF REPRESENTATION THEORY (SUPPLEMENTARY FUNDING), July 20 - August 6, 2014",DMS,"ALGEBRA,NUMBER THEORY,AND COM, COMM & INFORMATION FOUNDATIONS",12/1/15,6/2/16,Imre Kondor,PA,Pennsylvania State Univ University Park,Standard Grant,James Matthew Douglass,8/31/17,"$24,711.00 ",,risi@cs.uchicago.edu,110 Technology Center Building,UNIVERSITY PARK,PA,168027000,8148651372,MPS,"1264, 7797",7556,$0.00 ,"A three-week long summer school for graduate students in ""Modern Applications of Representation Theory"" will take place at the University of Chicago from July 20 to August 6, 2014 as part of the Institute for Mathematics and Applications (IMA) Graduate Students Summer Programs.  In mathematics symmetries are described by abstract objects called groups.  Representation theory, originating in the work of mathematicians such as Issai Schur and Ferdinand Frobenius at the end of the 19th century, studies how the structure of groups can be captured by simpler, linear objects, in particular, matrices.  Apart from making groups more concrete, it has long been realized that this is critical for understanding how groups act on other objects.  For example, in physics one is interested in how the symmetry groups of nature, such as translations and rotations, act on physical systems.  This is why, starting in the 1920's, the representation theory of unitary groups, in particular, has been central to the development of quantum mechanics and particle physics.   Recently, surprising connections have also been uncovered between representation theory and problems of a seemingly very different character, such as finding the number of operations required on a computer to carry out arithmetic computations, aligning a large number of noisy images of the same molecule taken by a certain type of electron-microscope, and ranking problems in statistical machine learning.  The aim of the summer school is to engage graduate students from across the sciences in research in these exciting new areas.<br/><br/>In recent years representation theory has found new applications in a range of domains from cryo-electron microscopy, through machine learning, to holographic algorithms.  The fact that the subject is usually taught for an audience in pure mathematics makes it challenging for graduate students in more applied disciplines to learn the material. Conversely, mathematicians are often not aware of the new, exciting applications of representation theory.  This summer school attempts to bridge this gap by starting with a short introduction to representation theory, followed by a series of mini-courses on some of the most recent work on using representation theoretical ideas in imaging, signal processing, holographic algorithms, quantum computing, algebraic and geometric computational complexity theory, non-commutative Fourier transforms, and other areas.  Each of these subjects is presented by one of the leading experts in the area.  The website for the summer school can be found here:  <br/><br/>www.ima.umn.edu/event/index.php?event_id=PISG7.20-8.6.14"
335,1507631,I-Corps:  Faster than Light Big Data Analytics,IIP,I-Corps,1/1/15,1/5/15,Inderjit Dhillon,TX,University of Texas at Austin,Standard Grant,Steven Konsek,6/30/16,"$50,000.00 ",,inderjit@cs.utexas.edu,"3925 W Braker Lane, Ste 3.340",Austin,TX,787595316,5124716424,ENG,8023,,$0.00 ,"Data-driven decision making is changing society. Political candidates now collate and analyze databases to identify possible campaign supporters. Couples now meet online through matchmaking websites that pair individuals based on profiles and questionnaires. Marketers now fine-tune their messages based on fine-grain market segmentation provided by extensive web and social network signals. Companies<br/>now develop products based on A/B testing or measured customer behavior. However, the diffusion of the basic software platforms and algorithms for analytics has been uneven. Large firms can afford to hire programmers and data scientists to run sophisticated analytics, but most organizations cannot afford the large capital and operational expenses to maintain analytics infrastructure. This leaves them<br/>out of the potentially transformative impact of data-driven decision making. This proposal expands the space of high-performance machine learning by harnessing a more general and more efficient parallel programming platform.<br/><br/>The large capital expense stems from the lack of a turnkey solution for predictive analytics. Applications are ad-hoc, developed anew based on specific organizational requirements. The large operational expense arises from the data demand of these applications, which requires many machine-hours to produce results. The team has developed a reusable software platform that can parallelize analytics applications with orders of magnitude performance improvements and can run with a fraction of the hardware resources compared to commonly deployed commercial products. On top of this platform, the team has begun to develop a suite of state-of-the-art massively scalable machine learning algorithms. Through the course of this project, the team intends to broadly engage with potential customers of data analytics to help understand possible business models for the proposed technology."
336,1511655,I-Corps:  Honest Signals-Machine Learning for Job Candidate Assessment,IIP,I-Corps,1/1/15,1/5/15,David Cox,MA,Harvard University,Standard Grant,Rathindra DasGupta,6/30/15,"$50,000.00 ",,davidcox@fas.harvard.edu,1033 MASSACHUSETTS AVE,Cambridge,MA,21385369,6174955501,ENG,8023,,$0.00 ,"A recent study on hiring indicated that interviewers felt that 80 to 85 percent of the job candidates they interviewed were poorly prepared. This has led to a remarkable rise in career coaching and employment websites.  At the present time, the American unemployment rate is 6.2%, representing 9.7 million unemployed persons currently seeking work. Extending beyond this figure, the underemployment rate, or the overall rate of those seeking full-time work, is estimated to be as high as 15.1% for the same period.   The vast majority of these job seekers lacks experience in branding themselves successfully and does not possess an extensive professional network from which to learn about job opportunities; this means that companies could easily be missing out on large numbers of qualified candidates. The proposed technology provides digital tools to help candidates hone their interviewing skills and professionalism for no charge, while simultaneously providing companies with an inexpensive platform to apply video analytics to the task of pre-screening candidates. This will allow companies to gain the high-value information normally only obtainable at high cost during a live interview for a much broader set of candidates.<br/><br/>This I-Corps team envisions a free service where job candidates are able to try the proposed concept ""Honest Signals"" video analytics features before subscribing and gaining access to a network of potential employers. Companies who use the Honest Signals service for employee screening will have access both to a network of users who have asked to have their videos shared with employers and to custom analytics tools which will allow them to screen candidates based on their own unique needs, thus streamlining human resources operations for challenging hiring scenarios such as low-end/unskilled labor, retail associates, and overseas labor. The proposed work will bring fundamentally new kinds of information to job candidate assessment and will help pioneer automated methods to exploit these new data sources. The use of these vastly richer forms of information has the potential to transform the practice of hiring through the use of innovative techniques drawn from social psychology."
337,1534781,SBIR Phase II:  System for Patient Risk Stratification through Electronic Health Record Analytics,IIP,SBIR Phase II,9/15/15,1/24/20,Thaddeus Fulford-Jones,MA,"Radial Analytics, Inc.",Standard Grant,Alastair Monk,8/31/20,"$1,496,989.00 ",,thaddeus@radialanalytics.com,50 Beharrell Street Suite A,Concord,MA,17420000,6178558214,ENG,5373,"019Z, 116E, 165E, 169E, 5373, 8018, 8023, 8032, 8042, 8240, 9102, 9231, 9251",$0.00 ,"The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project is significant; transitions of care impact millions of Americans every year. The healthcare system bears substantial cost and inefficiency&#8232;on account of suboptimal care transitions and overspending. This Phase II project will support progress towards a ""learning healthcare system"" and will extend the capabilities of data mining and machine learning in healthcare<br/><br/>The proposed project seeks to improve data mining technologies for healthcare decision support. This project will focus on the analysis of a broad variety&#8232;of data types that are common in healthcare settings. The anticipated improvements would allow frontline care staff, operational managers, and healthcare executives to assess and make stronger evidence-driven decisions regarding quality, cost, and access as patients move through the healthcare system. The enhanced data mining system would utilize state-of-the-art pattern recognition and machine learning techniques to dynamically process and interpret clinical, claims, and other types of healthcare data. If successful, this research will impact the state-of-the-art in healthcare analytics."
338,1557761,QuBBD: Classification and clustering of medical time series data: the example of syncope,DMS,,9/15/15,9/11/15,Pierre Gremaud,NC,North Carolina State University,Standard Grant,Nandini Kannan,8/31/18,"$92,000.00 ",Mette Olufsen,gremaud@math.ncsu.edu,2701 Sullivan DR STE 240,Raleigh,NC,276950001,9195152444,MPS,O470,,$0.00 ,"Syncope or, more colloquially, fainting, is a surprisingly common and little understood condition.  The long term goal of the proposed line of work is the identification of the root causes of syncope.  Fainting can result from the failure of one or more internal control mechanisms.  There are currently no clear causal links between these controls and the observed symptoms.  In order to understand the involved mechanisms, this project will start by analyzing clinical data to determine the number and characterization of the different types of syncope.  A better understanding hinges on the analysis of clinical data, here time series, and the ability to infer from these, patient classification.  Various scenarios will be tested through mathematical modeling to confirm both the soundness of the obtained classification and the nature and source of the pathology for each identified class.  The ability to identify subjects as members of a class or group also makes it possible to leverage information about the other members of that group for individual diagnosis purposes.  The methodology developed here will contribute to the implementation of this approach, sometimes referred to as ""bringing cohort studies to the bedside"".  This approach will also be applicable to the study of other diseases where similar clinical data are being collected such as epilepsy.<br/><br/>Time series data are ubiquitous.  In fact, the development of an ever increasing number of applications depends on their analysis, from stock market and economics to weather predictions.  Practical issues include signal matching, classification, pattern detection and early prediction.  Signals of interest are often high dimensional and noisy and the underlying dynamics are generally unknown.  This award supports initiation of a collaborative research project that addresses all the above issues in the context of medical times series data and, more specifically, for syncope.  To do so, a novel paradigm combining non-parametric statistics, machine learning, and applied mathematics is proposed.  The first objective is to design and compute features in multivariate time series that are well adapted to classification and clustering.  The proposed approach relies on recent advances in the evaluation of variable importance for scattered data.  The second objective is to facilitate calibration of mathematical models by extending machine-learning concepts to this new realm. The goal is to determine how much data is necessary to ""learn"" biomathematics models and increasing their predictive power.  This award is supported by the National Institutes of Health Big Data to Knowledge (BD2K) Initiative in partnership with the National Science Foundation Division of Mathematical Sciences."
339,1557466,QuBBD: Estimating drug-drug and drug-disease interactions for nursing homes residents,DMS,,9/15/15,9/11/15,Roee Gutman,RI,Brown University,Standard Grant,Swatee Naik,8/31/17,"$99,998.00 ",,Roee_Gutman@brown.edu,BOX 1929,Providence,RI,29129002,4018632777,MPS,O470,9150,$0.00 ,"Drug-drug interaction (DDI) is a situation in which one drug affects the activity of another.  Drug-disease interaction (DDSI) is a situation where prescribed medication has the potential to exacerbate a preexisting disease.  Both DDI and DDSI are major causes of adverse drug effects (ADEs) and represent a considerable threat to public health.  Frail elderly patients have a higher prevalence of multiple morbidities and they are often prescribed a wide range of drugs to treat these morbidities.  Thus, they are at increased risk for adverse effects from DDI and DDSI.  Other than hospitalization, re-hospitalization and emergency room visits, little is known about the contribution of DDI and DDSI to other important outcomes, such as functional status, institutionalization, and mortality.  In addition, some DDIs and DDSIs are unknown and require evidence-based updates.  This award supports initiation of a collaborative research project with the long-term goal of developing methods that efficiently identify ADEs caused by DDIs and DDSIs by combining electronic health records with precise clinical data (such as gene sequencing).  These findings will enable the updating of current guidelines, generate alerts, identify possible chemical compounds that may interact and result in ADEs, and propose new genetic drug pathways.  The current objective, which is a step toward the long-term goal, is composed of two aims.  The first aim is to combine current methods in machine learning and statistical causal inference to accurately identify DDI and DDSI.  The second aim is to develop statistically valid methods that will impute missing patient level data in large detailed datasets. <br/><br/>The project will exploit the availability of current datasets that could be augmented in the future to include precise clinical data.  The Minimum Data Set is part of the U.S. federally mandated process for clinical assessment of all residents in Medicare or Medicaid certified nursing homes.  It includes data on cognitive and functional abilities, disease diagnosis, psychological well-being, and medication use.  The dataset comprises millions of patients, thus enabling the discovery of DDIs and DDSIs that occur in a small fraction of the population.  The first aim comprises a combination of state-of-the-art supervised learning approaches such as LASSO, random forests, SVM, and Ensemble methods with efficient statistical causal inference approaches such as optimal matching, weighting, targeted learning, and multiple imputation.  The approach is innovative because it combines causal inference methods with supervised learning methods to generate accurate and precise methods that have statistically valid operating characteristics when providing estimates for a specific patient.  The second aim includes development of methods that rely on dimension reduction of the observed covariates to impute missing data.  When imputing missing data, the rule of thumb is to use all of the available data, so that the missing at random assumption (MAR) is plausible.  Dimension reduction aims to reduce the number of covariates that are used for prediction.  Thus, development of an imputation algorithm with many missing covariates requires striking a balance between dimension reduction and plausibility of the MAR assumption.  This award is supported by the National Institutes of Health Big Data to Knowledge (BD2K) Initiative in partnership with the National Science Foundation Division of Mathematical Sciences."
340,1509040,ECCS-EPCN: Stochastic Power Control and Learning for Energy Grids,ECCS,EPCN-Energy-Power-Ctrl-Netwrks,8/1/15,8/5/15,Georgios Giannakis,MN,University of Minnesota-Twin Cities,Standard Grant,Radhakisan Baheti,7/31/20,"$300,000.00 ",Vassilis Kekatos,georgios@umn.edu,200 OAK ST SE,Minneapolis,MN,554552070,6126245599,ENG,7607,"092E, 155E",$0.00 ,"This project investigates a computational framework to deal with the stochastic, dynamic, and spatio-temporally distributed nature of forthcoming power systems. The envisioned advances in adaptivity, awareness, and scalability aim at engaging currently inactive electricity consumers in a sustainable power system. Smooth integration of photovoltaics, wind, storage systems, and electric vehicles, will promote innovation and development, in terms of markedly advancing the resource allocation, learning, and monitoring infrastructure. <br/> <br/>The proposed research aims for broad socio-technical advances in energy networks. Successful completion of the project will offer cyber innovations to enable systematic integration of stochastic renewable generation while improving end-user satisfaction. Given the universality of the research tools and methodologies, the utility of the proposed research goes well beyond the envisioned application area to the broader fields of optimization, stochastic processes, control systems, machine learning, statistical signal processing, and cyber security. Broader transformative impact will result from pragmatic test cases proposed for validation, involvement of undergraduates in research, and outreach activities."
341,1550163,EAGER: Collaborative Research: Lighthouse: A User-Centered Web System for High-Performance Software Development,CCF,Software & Hardware Foundation,8/1/15,4/28/17,Elizabeth Jessup,CO,University of Colorado at Boulder,Standard Grant,Almadena Chtchelkanova,7/31/18,"$166,000.00 ",,jessup@cs.colorado.edu,"3100 Marine Street, Room 481",Boulder,CO,803031058,3034926221,CSE,7798,"7916, 7942, 9251",$0.00 ,"The role of computing in science and engineering has been growing steadily in recent years, leading to ever larger and more complex problems.  Their solution requires high-performance resources, which has resulted in many efforts to produce new mathematical approaches, programming models and languages, software libraries, and runtime environments for advanced parallel computers. This vast and growing number of solution methods presents a challenge to scientists who do not want to invest the time needed to locate and learn new computational tools. <br/>The PIs are developing Lighthouse, a taxonomy-based system, that equips practitioners with easy discovery and use of high-performance software solutions to a variety of common problems arising in scientific and engineering applications.  Lighthouse also integrates information about the performance of multiple implementations of numerical algorithms.  It thus allows users to readily find and start using the best method for addressing a particular challenge with the computers available to them. Less time spent in learning numerical packages and faster scientific simulations directly increases scientific productivity.<br/>Lighthouse is a web-based framework that offers a solution to two main research challenges.  First, it aids in the creation of complex high-performance applications that leverage the latest advances in applied mathematics and computer science research.  Second, it facilitates effective intra- and cross-domain communication for computer scientists, applied mathematicians, computational scientists, and students as they tackle scientific and engineering computing problems. Lighthouse incorporates an extensible taxonomy that enables different types of searches for numerical solutions depending on a user?s level of expertise.  It then provides a mechanism for generating code templates based on the search results.  Lighthouse thereby reduces the substantial learning curves associated with using state-of-the art high-performance software libraries. The Lighthouse system integrates taxonomy information for an expanding number of high-performance numerical libraries that presently comprises the linear algebra packages LAPACK, PETSc, and SLEPc.  Lighthouse employs machine learning methods to automatically classify routines based on their performance attributes.  In this way, it helps to provide more accurate search results that take into account not only functionality but also problem scale, performance, and available computational resource requirements. Finally, Lighthouse improves communication within and between communities by providing an accessible web-based interface to taxonomies of numerical packages that includes documentation, references, and performance information."
342,1523767,NRI: Learning to Plan for New Robot Manipulation Tasks,IIS,NRI-National Robotics Initiati,9/1/15,9/17/15,Tomas Lozano-Perez,MA,Massachusetts Institute of Technology,Continuing Grant,James Donlon,8/31/19,"$900,000.00 ",Leslie Kaelbling,tlp@csail.mit.edu,77 MASSACHUSETTS AVE,Cambridge,MA,21394301,6172531000,CSE,8013,8086,$0.00 ,"Robots have great potential societal benefits, especially working with humans in tasks such as manufacturing, disaster relief and elder care. Robots are however very difficult to program to perform new tasks: non-programmers can teach relatively stereotyped action sequences and expert programmers can generate more elaborate action strategies through long programming and debugging processes. Part of the difficulty stems from trying to teach the robot at the level of actions, since the actions to achieve a desired effect depend strongly on details of the environment.  Instead, this project focuses on teaching the robot models of the environment.  The robot can then use these models to plan its actions automatically.  This approach leads to more adaptable behavior.  Models are also easier to extend and re-use than action sequences, thereby reducing the burden for teaching subsequent tasks. The project involves a thorough integration of research and education. Graduate and undergraduate students are involved in all aspects of the research. Furthermore, the research in this project will become part of an undergraduate subject on robot algorithms at MIT.<br/><br/>This project will develop techniques to teach a robot to perform long-horizon tasks in complex, uncertain domains, in a way that equips the robot with knowledge it can re-use and re-combine with previous knowledge to solve not just the task it was taught, but a broad array of additional tasks.  Furthermore, the robot will be aware of its own knowledge and lack of knowledge, and will be able to plan to take actions, including performing experiments and asking humans for further information, to improve its own knowledge about how to behave in its environment.  The project will develop a set of machine learning tools that will allow humans to, relatively quickly and straightforwardly, teach the basic ideas of a new domain to the robot, and then enable to robot to continue to improve its knowledge as it gains experience in the domain. This project will build on a new hierarchical framework for integrating robot motion planning, symbolic planning, purposive perception and decision-theoretic reasoning.  The framework, as it stands, supports planning and execution to achieve pick-and-place tasks in complex domains that may require moving objects out of the way, using real, noisy, robot perception and actuation.  However, it requires a specification of the domain it is to operate in.  In our existing implementation, the domain description was written by hand, by experts, through a long period of trial-and-error. The concrete objective of the project is to develop methods enabling a robot to learn to perform high-level tasks in new domains by acquiring new domain models through human-provided examples and advice.  These methods will be evaluated in three domains using a Willow Garage PR2 mobile manipulation robot.  The overriding objective will be to develop methods that apply broadly and can be used to instruct robots to perform a wide variety of tasks."
343,1539527,CyberSEES: Type 2: Human-centered systems for cyber-enabled sustainable buildings,CCF,CyberSEES,9/1/15,8/27/15,Panagiota Karava,IN,Purdue University,Standard Grant,Bruce Hamilton,8/31/19,"$1,200,000.00 ","Robert Proctor, Jianghai Hu, Athanasios Tzempelikos, Ilias Bilionis",pkarava@purdue.edu,Young Hall,West Lafayette,IN,479072114,7654941055,CSE,8211,"8208, 8231",$0.00 ,"In the U.S., the building sector accounts for about 40% of primary energy usage, 71% of electricity and 38% of carbon dioxide emissions. For this reason, development of efficient solutions to reduce energy consumption and environmental impact of buildings is of critical societal importance. Occupants play a significant role in energy use of office buildings, affecting up to 30% of the energy use. To manage occupants' energy impact due to their presence and behavior, environmental control systems (e.g., HVAC, shading, lighting) have been automated based on the use of ""widely acceptable"" visual and thermal comfort metrics. However, occupants have a strong preference for customized indoor climate, and there is a strong relationship between occupants' perception of control over their environment and productivity, health and well-being. To address the challenge of customized control of the environment, the objective of this project is to realize a new paradigm for human-centered sustainable buildings, enabled by conducting research with computing innovations in probabilistic methods and machine learning, linked to sustainability, and with broader impacts in multiple domains of science and engineering. Broader impacts are: (1) New computing methods and algorithms on probabilistic classification, inference and optimal control that may impact a number of scientific communities, including Architectural, Mechanical, Electrical, Computer and Industrial/Human Factors Engineering, Computer and Psychological Sciences. Potential application areas include genomics, traffic flow prediction, infrastructure systems including power, transportation, etc. (2) Integration of the project's modeling, simulation, and experimental platforms into new teaching modules and experiential learning activities that support the curriculum and workforce development in four engineering schools (Civil, Mechanical, Electrical and Computer, Industrial Engineering) and the Department of Psychological Sciences. (3) Dissemination of research outcomes to the academic community and to the industry through publications, workshops, conferences and a customized external evaluation process. (4) Creation of outreach and engagement initiatives for K-12 teachers and students in STEM learning and research. <br/><br/>This project takes a multidisciplinary approach that is grounded in (1) new algorithms for automated identification of the relevant human perception-attributes of buildings; and (2) new concepts for intelligent and self-tuned comfort delivery systems for customized thermal and visual environments in buildings. The research includes: (1) Laboratory and field studies with human test-subjects that map indoor environment conditions, thermal and visual perception and comfort, occupant-building interactions and control actions, as well as corresponding space performance for perimeter building zones. (2) Probabilistic classification of human perception, comfort, and satisfaction profiles for a typical population. (3) Computationally-efficient inference algorithms for online learning of individual and population-level human preferences. (4) Optimal control algorithms and simulation tools for implementation in building management systems. The research outcomes will be integrated into a new cyber-enabled technological solution for self-tuned comfort delivery devices (thermostats, shading and lighting actuators). The experimental prototypes and field demonstrations will achieve improved performance with quantified building energy use reduction and occupant satisfaction, as well as robustness to uncertainty due to the reduction in the frequency of overrides."
344,1527388,CIF: Small: Learning Signal Representations for Multiple Inference Tasks,CCF,Comm & Information Foundations,8/1/15,7/27/15,Maxim Raginsky,IL,University of Illinois at Urbana-Champaign,Standard Grant,Phillip Regalia,7/31/20,"$500,000.00 ",Pierre Moulin,maxim@illinois.edu,1901 South First Street,Champaign,IL,618207406,2173332187,CSE,7797,"7923, 7936",$0.00 ,"Rapid advances in high-performance computing and widespread availability of massive datasets are bringing about a paradigm shift in the theory and practice of signal representations, geared towards inference and learning. A signal representation is a compressed summary that only retains those features of the signal that are salient for a class of inference tasks. This project provides a comprehensive theoretical and algorithmic framework for signal representations, which is sufficiently broad to cover both the traditional types of signal representations, such as vector quantization and sparse codes, and the more modern types, inspired by recent advances in machine learning and signal processing for Big Data. <br/><br/>Under this framework, the statistical performance and the computational complexity of signal representations are addressed in a unified manner by imposing structural constraints on the encoding map, the decoding map, and the model space of the representation, while simultaneously tailoring these objects to the class of tasks of interest. This unification leads to new theoretical and algorithmic insights into highly structured internal representations that are a key factor in recent spectacular success of deep neural networks on challenging tasks in visual, audio, and speech analytics."
345,1546098,BIGDATA: F: Learning Big Bayesian Networks,IIS,Big Data Science &Engineering,10/1/15,9/15/15,Qing Zhou,CA,University of California-Los Angeles,Standard Grant,Sylvia Spengler,9/30/20,"$919,305.00 ",Arash Amini,zhou@stat.ucla.edu,10889 Wilshire Boulevard,LOS ANGELES,CA,900951406,3107940102,CSE,8083,"7433, 8083",$0.00 ,"A fundamental problem in analyzing big data is to extract and represent the relations among the huge number of variables in a dataset. For example, in a genomic dataset, one may want to find out the dependence among a large number of genetic variations and various disease states. The Bayesian network is a commonly used class of mathematical models to represent such complex relations among a collection of variables, with wide applications in many scientific fields, ranging from the biomedical sciences to the social sciences. The goal of this project is to develop statistical and machine learning methods to construct Bayesian networks from big data, where the datasets may contain thousands to millions of variables. This is a challenging problem, particularly for large networks, as seen from the fact that state-of-the-art methods can barely handle thousands of variables. In this project, a novel divide-and-conquer approach will be developed and implemented as open-source packages for public use. The PIs will also study the theoretical properties of key components of this approach.  Through seminar organization and educational activities in both graduate and undergraduate training, the cutting-edge research in this project will be communicated immediately to a much broader audience.<br/><br/>The proposed approach consists of three main components: Partition, Estimation and Fusion (PEF). In the partition stage, spectral clustering will be embedded into an iterative subsampling approach to efficiently group variables into clusters. In the estimation stage, a few new methods will be developed to estimate the structure of a Bayesian network for each cluster of nodes, which serves as a subgraph of the big network. These methods include convex relaxations for permutations, fast algorithms for large-scale regularized estimation of the parameters of a Bayesian network, and novel formulations for discrete data. The final fusion stage will merge subgraphs into one big Bayesian network via a new method based on multiple-response sparse regression. Rigorous analysis of the PEF learning strategy for Bayesian networks under high-dimensional scaling will be conducted to provide theoretical guarantees for the methods and the algorithms."
346,1550202,EAGER: Collaborative Research: Lighthouse: A User- Centered Web System for High-Performance Software Development,CCF,Software & Hardware Foundation,8/1/15,6/10/16,Boyana Norris,OR,University of Oregon Eugene,Standard Grant,Almadena Chtchelkanova,7/31/18,"$157,973.00 ",,norris@cs.uoregon.edu,5219 UNIVERSITY OF OREGON,Eugene,OR,974035219,5413465131,CSE,7798,"7916, 7942, 9251",$0.00 ,"The role of computing in science and engineering has been growing steadily in recent years, leading to ever larger and more complex problems.  Their solution requires high-performance resources, which has resulted in many efforts to produce new mathematical approaches, programming models and languages, software libraries, and runtime environments for advanced parallel computers. This vast and growing number of solution methods presents a challenge to scientists who do not want to invest the time needed to locate and learn new computational tools. <br/>The PIs are developing Lighthouse, a taxonomy-based system, that equips practitioners with easy discovery and use of high-performance software solutions to a variety of common problems arising in scientific and engineering applications.  Lighthouse also integrates information about the performance of multiple implementations of numerical algorithms.  It thus allows users to readily find and start using the best method for addressing a particular challenge with the computers available to them. Less time spent in learning numerical packages and faster scientific simulations directly increases scientific productivity.<br/>Lighthouse is a web-based framework that offers a solution to two main research challenges.  First, it aids in the creation of complex high-performance applications that leverage the latest advances in applied mathematics and computer science research.  Second, it facilitates effective intra- and cross-domain communication for computer scientists, applied mathematicians, computational scientists, and students as they tackle scientific and engineering computing problems. Lighthouse incorporates an extensible taxonomy that enables different types of searches for numerical solutions depending on a user?s level of expertise.  It then provides a mechanism for generating code templates based on the search results.  Lighthouse thereby reduces the substantial learning curves associated with using state-of-the art high-performance software libraries. The Lighthouse system integrates taxonomy information for an expanding number of high-performance numerical libraries that presently comprises the linear algebra packages LAPACK, PETSc, and SLEPc.  Lighthouse employs machine learning methods to automatically classify routines based on their performance attributes.  In this way, it helps to provide more accurate search results that take into account not only functionality but also problem scale, performance, and available computational resource requirements. Finally, Lighthouse improves communication within and between communities by providing an accessible web-based interface to taxonomies of numerical packages that includes documentation, references, and performance information."
347,1527460,SHF: Small: On-Die Learning: A Pathway to Post-Deployment  Robustness and Trustworthiness of Analog/RF ICs,CCF,Software & Hardware Foundation,7/15/15,7/8/15,Yiorgos Makris,TX,University of Texas at Dallas,Standard Grant,Sankar Basu,6/30/19,"$350,000.00 ",,yiorgos.makris@utdallas.edu,"800 W. Campbell Rd., AD15",Richardson,TX,750803021,9728832313,CSE,7798,"7923, 7945",$0.00 ,"Towards enabling reliable computing and promoting technology trustworthiness, this project seeks to facilitate cost-effective realization of robust and trusted integrated circuits (ICs), with particular emphasis in the analog/RF domain. While manufactured ICs are subjected to extensive scrutiny in order to weed out defective or suspicious parts prior to their deployment, a variety of reasons such as silicon aging and adverse operational or environmental conditions might cause the performances of a previously healthy analog/RF IC to fail its design specifications. Similarly, field-activated triggers of hidden capabilities might cause a previously trusted analog/RF IC to exhibit malicious functionality. With analog/RF ICs now prevalent in most electronic systems, due to the rapid growth of wireless communications, sensor applications, and the Internet of Things (IoT), equipping them with robustness and trustworthiness evaluation mechanisms becomes paramount to the applications wherein they are deployed. The proposed research is complemented by educational and outreach activities, including development of a new educational module, by providing opportunities for graduate and undergraduate research in interdisciplinary projects spanning Electrical Engineering, Computer Science and Applied Mathematics, as well as exposure of local community members to the topics of self-test, self-calibration, and post-deployment trust evaluation of analog/RF ICs through tutorials and seminars organized by the Texas Analog Center of Excellence (TxACE) at the University of Texas at Dallas. <br/><br/>More specifically, this project seeks to enhance post-deployment robustness and trustworthiness of analog/RF ICs by integrating machine learning-based on-die monitoring and calibration capabilities. The key focus of this project is the cost-effective integration of on-die learning capabilities, which can be trained to (i) determine whether an analog/RF IC complies with its specifications, (ii) calibrate its performances, and/or (iii) detect the activation of potentially malicious circuitry, based on simple measurements from on-chip sensors. Through design, fabrication and characterization of two different analog/RF ICs, this project seeks to demonstrate that on-die intelligence can be integrated to provide post-deployment self-test, calibration and trust evaluation capabilities."
348,1527490,RI: Small: Inference with Incomplete Data,IIS,Robust Intelligence,9/1/15,8/4/15,Judea Pearl,CA,University of California-Los Angeles,Standard Grant,Weng-keen Wong,8/31/18,"$473,159.00 ",,judea@cs.ucla.edu,10889 Wilshire Boulevard,LOS ANGELES,CA,900951406,3107940102,CSE,7495,"7495, 7923",$0.00 ,"Missing data is a problem that plagues every branch of empirical science. Sensors do not always work reliably, respondents do not fill out every question in the questionnaire, and medical patients are often unable to recall episodes, treatments or outcomes. This project attempts to recover information of interest from partially observed data by reasoning about the process that could have caused some data to be missing and others to be observed. Recent advances in graphical models and causal inference permit us to describe such processes formally and identify conditions under which recovery from missing data would be feasible and, if so, how. This project will focus on conditions in which recovery is deemed infeasible by available techniques. By learning to manage such conditions this research will benefit empirical research in a variety of fields, including machine learning, big data, epidemiology, statistics, economics, social science and medicine.<br/><br/>The aim of the proposed research is to develop computer systems capable of learning from incomplete data by encoding assumptions in a graphical causal model. Using such models we will identify conditions that facilitate (or prohibit) inference and learning. In particular, this research will develop effective procedures for determining whether unbiased estimates of statistical and causal relationships can be computed given incomplete data and whether assumptions that facilitate such estimability have testable implications. Additionally, it will generate bounds for those relationships that are proved to be inestimable. These will in turn lead to a theoretical understanding of what is possible and impossible under missing data conditions. Given the ubiquity of the missing data problem we believe this research will create new and important tools for all data-intensive sciences.<br/><br/>"
349,1520588,SBIR Phase I:  Aerial Weed Scout with Robust Adaptive Control for Site-Specific Weed Control,IIP,SBIR Phase I,7/1/15,6/22/15,Gregory Rose,IL,"Intelinair, Inc.",Standard Grant,Ruth Shuman,12/31/15,"$150,000.00 ",,greg@intelinair.com,1807 S. Neil St.,Champaign,IL,618207215,6268172110,ENG,5371,"5371, 8038",$0.00 ,"The broader impact/commercial potential of this Small Business Innovation Research (SBIR) project is to provide the farmer with the ability, on demand, to use aerial remote sensing via an unmanned aerial vehicle (UAV) to detect and identify weeds in the field.  Presently, weeds are increasingly becoming herbicide-resistant.  Farmers are required to spend more money and apply greater amounts of chemicals to their field leading to less profit and environmental degradation.  In the meantime, UAVs have greatly decreased in cost while the FAA is increasingly loosening regulations to fly.  UAVs bring powerful benefits:  They can be operated on-demand (as opposed to long lead times for imaging via manned flight); and they also provide much higher resolution as compared with satellite alternatives.  However, farmers needs more than images; they need actionable intelligence so decisions can be made.  With the high resolution images provided by the UAV, and the advances in analytics, machine learning, and signature matching, these technologies will be used to help farmers efficiently manage their herbicide applications and improve their yield.<br/><br/>This SBIR Phase I project proposes to demonstrate the feasibility of an aerial weed scout by developing and testing the onboard image processing algorithm and L1 robust adaptive control for accurate, efficient weed identification and mapping.  Multiple technologies must come together in an integrative fashion in order to deliver this as a complete solution.  A stable platform capable of following flight paths with extraordinary precision regardless of wind and other disturbances is necessary to avoid the ""garbage-in /garbage-out"" phenomenon that can occur with inaccurate data collection - this is especially critical for machine vision algorithms as crisp images greatly improve accuracy.  Regarding image processing and machine vision, one critical aspect is designing these such that they can be efficiently executed given the resources available on-board, and speedily executed in the time available.  The goal is to present the results of these algorithms to the farmer in a user friendly way so that they can quickly confirm or reject (as appropriate) the results and take action."
350,1550397,EAGER: Volition Based Anticipatory Control for Time-Critical Brain-Prosthetic Interaction,IIS,HCC-Human-Centered Computing,8/15/15,8/5/15,Gil Weinberg,GA,Georgia Tech Research Corporation,Standard Grant,Ephraim Glinert,7/31/17,"$178,761.00 ",,gil.weinberg@coa.gatech.edu,Office of Sponsored Programs,Atlanta,GA,303320420,4048944819,CSE,7367,"7367, 7916, 8089",$0.00 ,"This exploratory project focuses on developing algorithms that will allow the PI's previously implemented prototype drumming prosthesis, which was developed in an effort to help an injured teen, to anticipate human physical actions based on an analysis of EEG signals so that it can respond mechanically in a timely manner.  The goal is to enable the enhanced prosthesis to detect volition, the cognitive process by which an individual decides on and commits to a particular course of action hundreds of milliseconds before the action actually takes place, in order to foresee the drummer's actions and achieve sub-second synchronization between artificial and biological limbs, thereby leading to improved performance in a time-sensitive domain where asynchronous operations of more than a few milliseconds are noticeable by listeners.  Project outcomes will include cognitive models and technical approaches that will be of great value for improving efficiency and fluency in a wide range of human-robot and human-prosthesis interaction scenarios, from construction tasks where humans and robots collaborate to achieve common goals, to time-critical tasks such as in hospital operating rooms or space stations where humans operate artificial robotic limbs.  The work will also lead to creation of a volition trials database that will be documented and shared with the broad community of brain scholars and brain-machine interface researches.  And the project will have additional broad impact by supporting students in the Robotic Musicianship group at Georgia Tech as it transitions from its previous focus on robotic musicianship into the fields of prosthetic and human augmentation.<br/><br/>Prior studies of volition have shown that across multiple repetitions of (real or imagined) motor activity one can derive the Event-Related-Potential (ERP) associated with the intent to move the hand, up to a few seconds prior to the generation of the movement.  Additionally, studies of mirror neurons have shown that observing a motor activity can trigger sets of cells in the brain that replicate the activity depicted when a subject is engaged in the action itself.  In this project the PI will build on such findings to develop new pattern recognition algorithms for EEG signal analysis in an effort to identify volition and design new anticipatory algorithms for brain-machine interfaces that reduce latency and allow for synchronization at the millisecond level.  The work will be carried out in stages.  The PI will first collect EEG data from a large number of experimental trials where participants are engaged in a voluntary motor action.  The data will be studied to detect patterns indicative of volition activity from electrodes monitoring both the motor and pre-motor cortices (SMA and pre-SMA), and also to isolate the neural correlates of imagined vs. real movement.  A variety of general purpose machine learning classifiers, as well as music focused feature extraction techniques, will be used to distinguish between anticipatory patterns of activity preluding an action (volition) and patterns generated when the action is indeed manifested.  As part of the analysis the PI will attempt to acquire an understanding of the delta times between volition and action under different conditions, and he will develop a repeatability / reliability matrix to be utilized for synchronization in the next stage of the work, in which the PI will develop a ""latency compensation engine"" that generates robotic drum hits at the exact anticipated action time, compensating for mechanical latencies while taking into account the projected delta time between volition and action.  Multi-modal integration with data from other sensors (EMG, microphones, proximity, etc.) will be exploited to correct errors is detection and classification.  Finally, success of the new algorithms will be evaluated using both objective and subjective measures by having the amputee drummer perform a series of musical tasks with the robotic arm."
351,1462404,Collaborative Research: EAGER-DynamicData: Machine Intelligence for Dynamic Data-Driven Morphing of Nodal Demand in Smart Energy Systems,ECCS,"Big Data Science &Engineering, ",9/1/15,9/4/15,Nikolaos Gatsis,TX,University of Texas at San Antonio,Standard Grant,Jenshan Lin,8/31/17,"$73,303.00 ",,nikolaos.gatsis@utsa.edu,One UTSA Circle,San Antonio,TX,782491644,2104584340,ENG,"8083, O395","153E, 155E, 7916",$0.00 ,"The electric power grid is the indispensable infrastructure for power delivery and distribution. It is a system of high complexity and heterogeneity comprised of a variety of interconnected systems, subsystems, generators, and loads. In addition, it is a dynamic system with evolving characteristics that suffers from several infrastructure limitations, which if not handled properly, may lead to instabilities with severe consequences including costly brownouts and blackouts. However, advancements in information and data-driven technologies offer the necessary ground for developing tools that efficiently monitor grid infrastructure and manage electricity flows in ways that achieve and maintain high performance and reliability in grid operations. Towards that end, coupling power systems with information systems converts traditional electric energy delivery infrastructures into interconnected hybrid energy-data systems called smart energy systems, where power flow is controlled via information signals. Dynamic data available in smart energy systems includes, but is not limited to, hourly user energy consumption measurements from smart meters, electricity pricing signals, system voltage readings from GPS-synchronized measuring units scattered throughout the network that can take hundreds of readings per second, and data from weather stations. Thus, due to grid complexity, a tremendous amount of information is not only generated but also transferred throughout the grid, and grid participants, such as customers, utility companies, and grid operators, are exposed to multiple heterogeneous data streams coming from various sources. In this data intensive environment, participants are being engaged to make fast real-time decisions regarding morphing of their load demand and consumption behavior patterns. Nodal load forecasting is identified as a key point for developing future smart energy systems and electricity markets. The principal theme of this research is the fast and optimal nodal load morphing in smart energy systems that takes into account big volumes of dynamically varying data.<br/><br/>In particular, this research addresses the problem of management and processing of big data within the framework of Dynamic Data Driven Systems (DDDS) as applied to nodal load morphing. The focus of this study will be the development of a set of new intelligent and self-adaptive algorithms for online big data processing and fast real-time decision-making in smart energy infrastructures. The main feature of the current research is the integration of machine learning DDDS with dynamic optimization methods to solve the computational problem of forecasting optimal or near-optimal shapes of a load in a timely manner accounting for multiple streams of continuously incoming data and their inherent uncertainty. Emphasis will be given in handling and processing incentive signals and more particularly electricity pricing signals as a major factor in load morphing. Furthermore, extensive testing and verification of the developed algorithms will be performed on real-time simulated scenarios obtained with the GridLAB-D software simulator. In short, the proposed research for nodal load morphing will enable a new and transformative approach towards efficient, inexpensive, and fast processing of big data as applied to smart energy systems."
352,1517510,Collaborative Research:  RUI:  New Insights from a Systematic Approach to Quasar Variability,AST,EXTRAGALACTIC ASTRON & COSMOLO,7/15/15,7/6/15,Eilat Glikman,VT,Middlebury College,Standard Grant,Matthew Benacquista,6/30/18,"$49,230.00 ",,eglikman@middlebury.edu,14 OLD CHAPEL ROAD,MIDDLEBURY,VT,57536000,8024435000,MPS,1217,"1207, 9150",$0.00 ,"A longstanding problem in astrophysics is to understand how galaxies form and develop throughout their lifetimes.  Such understanding is necessary to uncover how our Universe evolved and to gain insight into the origin of our own Milky Way Galaxy.  One important aspect of understanding galaxy formation and evolution is to study quasars and other active galactic nuclei.  They are also interesting astrophysical phenomena in their own right and serve as a probe of relativistic physics.  They co-evolve with their host galaxies and trace the evolution of cosmic structure.  However, most of the methods for quasar discovery are based on the properties of their broadband spectral energy distributions, and nearly all known quasars and quasar candidates come from samples that use some type of flux ratios or just the presence of a non-thermal emission.  Variability offers a spectrum-independent method for quasar discovery.  Although variability has been much studied on the basis of individual or a few objects, variability-based quasar surveys have so far been limited to small dedicated regions of sky with at most a few thousand objects and/or poor time resolution.  This study of quasar variability employs the Catalina Real-time Transient Survey (CRTS) data set, which covers about 80\% of the sky over a baseline greater than 9 years. Its 500 million object data set currently holds about 250,000 known quasars, 500,000 photometric quasar candidates and an estimated 1,000,000 new variability-selected quasars.  This will form the largest quasar data set to date.  In keeping with the CRTS Open Data policy, all quasars (and other classified objects) identified in this project will be released to the community.  This will form a major new resource for both quasar and more general variability studies.  The statistical methods to be used are also applicable to any irregular-sampled time series, and the combination of these with machine-learning techniques is a case study for data-intensive science.<br/><br/>This project is a collaboration with a primarily undergraduate institution and directly enhances the STEM education of two undergraduates.  Working with scientists at the Center for Data-Driven Discovery (CD3) at Caltech, they will be exposed to cutting-edge techniques in data science, including high level usage of data mining and extracting meaningful results from these large data sets.  Data products from this project also form the basis for student projects at the joint US-Chile-funded La Serena School for Data Science, training the next generation in applied tools for handling big astronomical data.<br/><br/>In particular, this project will focus on (i) the correlation of quasar variability features, particularly characteristic timescales, with physical parameters, such as luminosity, black hole mass, and the Eddington ratio; (ii) periodic variability as possible evidence for supermassive black hole binaries; (iii) variability as a probe of obscuration in young dust-enshrouded red quasars; and (iv) quantifying wavelength dependencies of variability to improve quasar selection and constrain different models of physical processes.  The study will employ modern statistical techniques that can work naturally with irregularly-sampled gappy time series without the need for reprojection or smoothing.  In combination with machine-learning methods, these will produce optimal ensemble-based results, such as new variability-polychromatic methods for quasar selection.  This project will be a key study on optical quasar variability well into the LSST era.  It is at least two orders of magnitude larger than any previous study in terms of sky coverage and number of quasars and an order of magnitude better in terms of time resolution (number of observations / baseline).  It will also substantially increase the number of high likelihood quasar candidates known, particularly in the regions of the sky not covered by SDSS."
353,1463316,Methods for Dynamic Network Identification with Application to the Control of Smart Buildings,CMMI,"GOALI-Grnt Opp Acad Lia wIndus, Dynamics, Control and System D",6/1/15,6/26/20,Prabir Barooah,FL,University of Florida,Standard Grant,Robert Landers,7/31/20,"$354,895.00 ",,pbarooah@ufl.edu,1 UNIVERSITY OF FLORIDA,GAINESVILLE,FL,326112002,3523923516,ENG,"1504, 7569","019Z, 030E, 031E, 032E, 033E, 034E, 035E, 099E, 8024",$0.00 ,"A dynamic network consists of interacting dynamic sub-systems. Such networks occur in many domains: living cells, financial markets, the Internet and the power grid are some examples. Heating, ventilation and air conditioning (HAVC) systems in buildings can also be modeled through dynamic networks since each room's climate depends on that of nearby spaces. Knowledge of such dynamic network models is essential to design and deploy control strategies devoted to the improvement of energy efficiency and occupant comfort. Yet, in practice the structure and dynamics of these networks are either unknown or imprecisely known. For instance, information on the thermal interaction among rooms is difficult to obtain from laws of physics due to the complexity of the physical processes involved. The goal of this project is to formulate algorithms for the identification of dynamic sparse network models from measured data. The research results will support the study of advanced controls for HVAC systems to reduce their energy use and to provide demand-side flexibility to the power grid. Since buildings consume 75% of the nation's electricity, improvement of energy efficiency through smart building control systems will contribute to the sustainability of the nation's energy system.<br/> <br/>Although 'dynamic system identification' is a well-developed field, the field of identification of dynamic networks is not at all well-developed. Traditional dynamic system identification techniques cannot exploit the inherent sparseness of the network identification problem, while traditional machine learning techniques are mostly applicable to only static networks. In this project we combine ideas from traditional dynamic system identification, L1 optimization for sparse vector recovery (from compressed sensing), and graphical modeling from machine learning to address the challenges in dynamic network identification. If successful, the research will (1) provide fundamental contribution to the nascent field of dynamic network identification through new algorithms, and (2) enable speedy deployment of 'smart building' technologies in commercial buildings. In addition, the project will support a number of educational innovations for attracting students from under-represented groups to engineering and generating excitement about engineering."
354,1533737,XPS: FULL: CCA: Collaborative Research: Automatically Scalable Computation,CCF,Exploiting Parallel&Scalabilty,8/1/15,7/27/15,Margo Seltzer,MA,Harvard University,Standard Grant,Marilyn McClure,7/31/20,"$525,000.00 ","David Brooks, Ryan Adams",margo@eecs.harvard.edu,1033 MASSACHUSETTS AVE,Cambridge,MA,21385369,6174955501,CSE,8283,,$0.00 ,"For over thirty years, each generation of computers has been faster than the one that preceded it. This exponential scaling transformed the way we communicate, navigate, purchase, and conduct science. More recently, this dramatic growth in single processor performance has stopped and has been replaced by new generations of computers with more processors on them; for example, even the cell phones we carry have multiple processors in them.  Writing software that effectively leverages multiple processing elements is difficult, and rewriting the decades of accumulated software is both difficult and costly. This research takes a different approach -- rather than converting sequential software into parallel software, this project develops ways to store and reuse computation. Imagine computing only when computer time and energy are cheap and plentiful, storing that computation, and then using it later, when computation might be limited or expensive.  The approach used involves making informed predictions about computation likely to happen in the future, proactively executing likely computations in parallel with the actual computation, and then ""jumping forward in time"" if the actual execution arrives at any of the predicted computations that have already been completed.  This research touches many areas within Computer Science, architecture, compilers, machine learning, systems, and theory.  Additionally, exploiting massively parallel computation will produce immediate returns in multiple scientific fields that rely on computation.<br/><br/>The approach used in this research views computational execution as moving a system through the enormously high dimensional space represented by its registers and memory of a conventional single-threaded processor.  It uses machine learning algorithms to observe execution patterns and make predictions about likely future states of the computation.  Based on these predictions, the system launches potentially large numbers of speculative threads to execute from these likely computations, while the actual computation proceeds serially.  At strategically chosen points, the main computation queries the speculative executions to determine if any of the completed computation is useful; if it is, the main thread uses the speculative computation to immediately begin execution where the speculative computation left off, achieving a speed-up over the serial execution.  This approach has the potential to be extremely scalable: the more cores, memory, and communication bandwidth available, the greater the potential for performance improvement. The approach also scales across programs -- if the program running today happens upon a state encountered by a program running yesterday, the program can reuse yesterday's computation. This project has the potential to break new ground for research in many areas in Computer Science touched by it."
355,1534910,DMREF: Deblurring our View of Atomic Arrangements in Complex Materials for Advanced Technologies,DMR,"OFFICE OF MULTIDISCIPLINARY AC, APPLIED MATHEMATICS, DMREF",10/1/15,8/7/18,Simon  J. L. Billinge,NY,Columbia University,Standard Grant,John Schlueter,3/31/21,"$1,082,786.00 ","Daniel Hsu, Qiang Du",sb2896@columbia.edu,2960 Broadway,NEW YORK,NY,100276902,2128546851,MPS,"1253, 1266, 8292","054Z, 8251, 8400",$0.00 ,"DMREF: Deblurring our View of Atomic Arrangements in Complex Materials for Advanced Technologies<br/><br/>Non-technical Description: As we try and find new technologies to solve some of mankind's toughest challenges such as abundant sustainable energy, environmental remediation, and health, we are increasingly seeking more and more complex materials.  We already have devices that turn sunlight into electricity and use sunlight to split water into precious hydrogen fuel, but issues such as device efficiency and cost mean that the current technologies cannot be taken to the vast scale needed for our modern needs. This puzzle may be solved by the use of advanced materials that perform their tasks - energy conversion, cancer cell killer, or whatever it may be - with greater efficiency.  This is inevitably leading us towards more complicated materials that consist of many different chemical elements and have engineered structures on multiple different length-scales from the atomic to the nano- and meso-scales all the way to macroscopic scales.  The problem is that, because of their complexity, it becomes very difficult to even characterize these materials when we have made them, let alone design and engineer them at the nanoscale.  Our usual tools based on the scattering of x-rays by crystals stop working for such nanoscale structures.  The problem is not that we lack powerful enough x-ray beams.  The problem is that the x-ray scattering signal from these complicated materials doesn't contain enough information to allow us to find a unique structure solution.  It is as if we are looking at complex patterns of atomic arrangements through blurry, steamed up glasses.  This project will bring greater clarity to this situation by marrying together advances in applied mathematics from diverse areas such as image recognition, information theory and machine learning, which are having transformative impacts in commerce, law enforcement and so on, and applying them to the problem of recognizing atomic arrangements in materials of the highest complexity.<br/><br/>Technical Description: The approach will to solve multi-scale structures of materials by marrying together the latest advances in the processing of x-ray scattering data from nanomaterials, such as atomic pair distribution function (PDF) analysis, with other sources of input information such as small angle scattering, EXAFS and other spectroscopies, as well as inputs from first principle theory such as DFT, but place them in a rigorous mathematical framework and a robust computational framework such that the information content in the data may be utilized to the greatest extent possible whilst taking into account uncertainties from statistical and systematic uncertainties.  The mathematical framework will utilize the latest developments in stochastic optimization, uncertainty quantification including function-space Bayesian methods, machine learning and image recognition."
356,1537504,Collaborative Research: Decision Model for Patient-Specific Motion Management in Radiation Therapy Planning,CMMI,OE Operations Engineering,9/1/15,8/31/15,Shouyi Wang,TX,University of Texas at Arlington,Standard Grant,Georgia-Ann Klutke,8/31/19,"$65,237.00 ",,shouyiw@uta.edu,"701 S Nedderman Dr, Box 19145",Arlington,TX,760190145,8172722105,ENG,006Y,"076E, 078E, 8023",$0.00 ,"A significant challenge in lung cancer radiation therapy (RT) is respiration-induced tumor motion, which hinders sufficient delivery of curative doses to target volumes. Although modern tumor motion management strategies for positron emission tomography/computed tomography (PET/CT)-guided RT are becoming more available, those techniques have yet to be fully incorporated into clinical practice. This is mainly because not every patient will benefit from a costly and lengthy motion-managed PET/CT scan due to high intra-patient and inter-patient variability of respiratory patterns. The objective of this project is to bridge the knowledge gap of which motion management method would best benefit an individual patient. This project will develop a new decision-making paradigm, in which machine learning techniques will be developed to characterize respiratory motion patterns and combine them with other diagnostic factors to predict the benefits from motion management methods for each individual patient. A decision-analytic cohort model will be developed to compare and evaluate the cost-effectiveness of the new decision paradigm and the traditional population-based radiation oncology practice of motion management based on our existing database of respiratory traces from more than 3,000 patients. While specifically applied to decisions surrounding respiratory motion management, the developed decision paradigm can be generalized and applied to other real life decision analysis problems.<br/><br/>This award supports fundamental research in data mining/machine learning and decision analysis, which will provide needed knowledge for the development of tools for effective management of patient-specific tumor motion. The modeling effort in this project will 1) establish a new mathematical foundation for supervised multivariate sparse variable selection and prediction to discover complicated multivariate relationships among high-dimensional variables; 2) construct a general integrated validation framework to rigorously test the cost-effectiveness of patient-specific health interventions. The new multivariate sparse variable selection and prediction approach can be used to build an interpretable prediction model, handle high-dimensional data with a low sample size, avoid under-shrinkage effect, and incorporate structured group selection. The cost-effectiveness analysis framework integrates the outcome of prediction model, the treatment effect and survival outcome model. This modeling aims to quantitatively estimate long-term cancer survival outcomes from improvement in patient-specific planning of radiation dosing by selective motion control."
357,1533663,XPS: FULL: CCA: Collaborative Research: Automatically Scalable Computation,CCF,Exploiting Parallel&Scalabilty,8/1/15,7/27/15,Steven Homer,MA,Trustees of Boston University,Standard Grant,Marilyn McClure,7/31/19,"$350,000.00 ","Jonathan Appavoo, Ajay Joshi",homer@bu.edu,881 COMMONWEALTH AVE,BOSTON,MA,22151300,6173534365,CSE,8283,,$0.00 ,"For over thirty years, each generation of computers has been faster than the one that preceded it. This exponential scaling transformed the way we communicate, navigate, purchase, and conduct science. More recently, this dramatic growth in single processor performance has stopped and has been replaced by new generations of computers with more processors on them; for example, even the cell phones we carry have multiple processors in them.  Writing software that effectively leverages multiple processing elements is difficult, and rewriting the decades of accumulated software is both difficult and costly. This research takes a different approach -- rather than converting sequential software into parallel software, this project develops ways to store and reuse computation. Imagine computing only when computer time and energy are cheap and plentiful, storing that computation, and then using it later, when computation might be limited or expensive.  The approach used involves making informed predictions about computation likely to happen in the future, proactively executing likely computations in parallel with the actual computation, and then ""jumping forward in time"" if the actual execution arrives at any of the predicted computations that have already been completed.  This research touches many areas within Computer Science, architecture, compilers, machine learning, systems, and theory.  Additionally, exploiting massively parallel computation will produce immediate returns in multiple scientific fields that rely on computation.<br/><br/>The approach used in this research views computational execution as moving a system through the enormously high dimensional space represented by its registers and memory of a conventional single-threaded processor.  It uses machine learning algorithms to observe execution patterns and make predictions about likely future states of the computation.  Based on these predictions, the system launches potentially large numbers of speculative threads to execute from these likely computations, while the actual computation proceeds serially.  At strategically chosen points, the main computation queries the speculative executions to determine if any of the completed computation is useful; if it is, the main thread uses the speculative computation to immediately begin execution where the speculative computation left off, achieving a speed-up over the serial execution.  This approach has the potential to be extremely scalable: the more cores, memory, and communication bandwidth available, the greater the potential for performance improvement. The approach also scales across programs -- if the program running today happens upon a state encountered by a program running yesterday, the program can reuse yesterday's computation. This project has the potential to break new ground for research in many areas in Computer Science touched by it."
358,1526952,AF: Small: Fundamental Connections in Randomness and Complexity,CCF,Algorithmic Foundations,9/1/15,8/12/15,David Zuckerman,TX,University of Texas at Austin,Standard Grant,Tracy Kimbrel,8/31/18,"$400,000.00 ",,diz@utexas.edu,"3925 W Braker Lane, Ste 3.340",Austin,TX,787595316,5124716424,CSE,7796,"7923, 7927",$0.00 ,"Major advances in computer science have come from finding connections between different areas and then exploiting them in nontrivial ways. In this project, the PI seeks new connections and plans to capitalize on these and known connections among several areas of randomness and computational complexity. Computational complexity explores which problems are computationally intractable and why, as well as tradeoffs between computational resources such as time, space, and randomness.<br/><br/>Randomness is extremely useful in computer science and widely used in practice.  When simulating complex phenomena, such as the weather or the economy, it is standard to include random components.  Computer security is impossible without randomness.  Yet while randomness is provably necessary for computer security, it is not known whether it is provably necessary for algorithms. A major question in computing is to understand the power of randomness and whether it is really necessary for algorithms. Research addressing this question often focuses on two fundamental objects: pseudorandom generators and randomness extractors. A pseudorandom generator is a deterministic algorithm that expands a small number of random bits into a large number of pseudorandom bits, where algorithms using these pseudorandom bits behave similarly to algorithms using perfectly random bits. A randomness extractor is a deterministic algorithm that converts a large amount of low-quality randomness into a smaller, but still large, amount of high-quality randomness.<br/><br/>This project has several themes. How do pseudorandom generators and randomness extractors relate to lower bounds for computational problems? How do pseudorandom generators and randomness extractors relate to cryptography, the mathematical foundations of computer security? How do randomness extractors relate to error-correcting codes, which enable reliable transmission over noisy media? How do codes relate to machine learning? How do pseudorandom generators relate to computational biology? By finding and exploiting these connections, we can greatly advance knowledge in the underlying areas and increase the chances of breakthroughs. Several application areas are important to society. Understanding molecular structure could impact biology, medicine, and drug design. Improvements in cryptography could lead to improved computer security. Machine learning addresses the omnipresent big data."
359,1557572,QuBBD: Collaborative Research: Personalized Predictive Neuromarkers for Stress-Related Health Risks,DMS,,9/15/15,3/17/16,Aarti Singh,PA,Carnegie-Mellon University,Standard Grant,Nandini Kannan,8/31/16,"$91,165.00 ",Timothy Verstynen,aartisingh@cmu.edu,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,MPS,O470,,$0.00 ,"Coronary heart disease (CHD) remains the leading cause of premature death among adults in the U.S. and other postindustrial nations.  Predicting CHD risk - especially for a particular individual - remains a fundamental challenge.  This project will investigate the application of statistical machine learning approaches to multimodal brain imaging, behavioral, biological, and related data to enhance the prediction of CHD risk.  It specifically addresses the question of whether particular patterns of human brain activity during psychological stress reliably predict known risk markers of CHD; namely, stress-related rises in blood pressure and arterial morphology.  From a basic science perspective, this research will advance our mechanistic understanding of how the brain relates to our physical health.  From a public health perspective, this research will help to identify markers of brain activity that could be objectively identified and possibly targeted for modification in otherwise healthy people at risk for future CHD. <br/><br/>The key challenge with mapping neuroimaging data to CHD risk lies in being able to very precisely regress observed psychological stress reactions on the time-series of brain activity recorded in thousands of voxels, and identify which brain regions are most relevant for the regression.  Conventional analytic approaches involve forming coarse temporal summaries by committing to specific parametric models, such as a generalized linear model and a fixed model for hemodynamic response, that result in poor accuracy.  This award supports initiation of a collaborative research project that brings together a highly cross-disciplinary team of statistical machine learning, neuroimaging and health psychology researchers to tackle the following two goals: 1) identify a generalizable model and neuromarkers that predict individual differences in cardiovascular risk factors based on neural dynamics under psychological stress.  This will be enabled through novel methods for functions-to-real and functions-to-function lasso regression;  2) characterize how neural patterns can be integrated with other physiological and anthropometric factors to personalize individual risk scores and neuro-biomarkers.  This award is supported by the National Institutes of Health Big Data to Knowledge (BD2K) Initiative in partnership with the National Science Foundation Division of Mathematical Sciences."
360,1534412,Collaborative Research: Record Linkage and Privacy-Preserving Methods for Big Data,SES,"METHOD, MEASURE & STATS, DATANET, SCIENCE RESOURCES STATISTICS, ",9/15/15,9/11/15,Rebecca Steorts,NC,Duke University,Standard Grant,Cheryl Eavey,8/31/18,"$265,579.00 ",,beka@stat.duke.edu,"2200 W. Main St, Suite 710",Durham,NC,277054010,9196843030,SBE,"1333, 7726, 8800, O499","7433, 7434, 9102",$0.00 ,"This research project will develop sound statistical and machine learning techniques for preserving privacy with linked data.  Social entities and their patterns of behavior is a crucial topic in the social sciences.  Research in this area has been invigorated by the growth of the modern information infrastructure, ease of data collection and storage, and the development of novel computational data analyses techniques.  However, in many application areas relevant and sensitive information is commonly located across multiple databases.  Data analysis is inherently impossible without merging databases, but at the cost of increasing the risk of a privacy violation.  This research will address the problem of how to perform valid statistical inference in the presence of multiple data sources, data sharing, and privacy in the age of ""big data.""  The investigators' new modeling construct for inference and uncertainty quantification will contribute to both statistics and the many disciplines for which statistics is a principal tool.  The methods will have a wide range of applications in the social, economic, and behavioral sciences, including medicine, genetics, official statistics, and human rights violations.  The investigators will collaborate with post-doctoral researcher and with graduate and undergraduate students.  The statistical methods will be encapsulated in open-source software packages, allowing off-the-shelf use by practitioners while facilitating more detailed control and extensions.<br/><br/>This interdisciplinary research project will improve upon methods in record linkage and privacy using state-of-the-art techniques from statistics and machine learning.  Record linkage is the process of merging possible noisy databases with the goal of removing duplicate entries.  Privacy-preserving record linkage (PPRL) tries to identify records that refer to the same entities from multiple databases without compromising the privacy of the entities represented by these records.  The research will focus on three aims: (1) development of new Bayesian methods for PPRL, where the error can be propagated exactly across the entire linkage process and into statistical inference, including new privacy measures to capture a tradeoff between utility and risk of any individual risk in a linked database; (2) development of new robust methods for realizing synthetic data releases post-linkage with differential privacy guarantees and its relaxations to address additional layers of privacy and support broader data sharing; and (3) exploration of ""big data"" methods such as variational inference to address scalability and latent cluster exchangeability issues existing within linkage and privacy, such that the new methods can scale to multiple and large databases.  The new methods will be scalable and assess uncertainty throughout the entire linkage and privacy process and can be evaluated using Bayesian disclosure risk and Bayesian differential privacy.  The project is supported by the Methodology, Measurement, and Statistics Program and a consortium of federal statistical agencies as part of a joint activity to support research on survey and statistical methodology."
361,1549608,EAGER: Advancing the Engineering of Complex Systems through Automated Mechanism Design for Complex Network Formation,CMMI,Systems Science (SYS),8/15/15,7/31/15,Mario Ventresca,IN,Purdue University,Standard Grant,Rich Malak,7/31/18,"$172,396.00 ",,mventresca@purdue.edu,Young Hall,West Lafayette,IN,479072114,7654941055,ENG,8085,"067E, 068E, 073E, 7916",$0.00 ,"Traditional engineering methodologies have proven highly effective for designing complicated systems, such as automobiles and computers, where the sum of the parts equals the whole. However, these methodologies are not very useful for designing complex systems that are greater than the sum of their parts, such as global supply chains, the Internet of Things, social networks and smart power grids. The rapidly increasing and critical role such systems play in U.S. social, industrial and economic infrastructures necessitates new principles for designing complex systems in an inherently efficient, robust and sustainable fashion. This EArly-concept Grant for Exploratory Research (EAGER) award supports fundamental research to provide engineering principles for designing such complex systems. Knowledge from several disciplines including computer science, game theory, optimization and machine learning is integral to this research. Moreover, the emphasis on complex systems will help broaden interest in engineering research and positively impact engineering education. <br/><br/>The highly interdependent nature of many physical, virtual and cyber-physical complex systems and our increasing reliance upon them, demand a sound basis upon which to base their design. This research will provide principles for designing complex systems by considering the problem as one of automated mechanism design within the context of strategic Bayesian network formation games to automatically devise incentives such that multiple global design objectives are achieved. The local and stochastic nature inherent to many complex systems motivates a behavior-based perspective on network formation rules, permitting the consideration of more realistic scenarios and may provide deeper insight into network formation itself. The research team will perform simulations, devise models and perform analytical derivations under different system design objectives in order to establish hidden relationships, vary complex system size to better understand adaptation and evolution, consider fixed system interactions to reveal the role of legacy or backbone infrastructure on future systems and mechanisms, and integrate online machine learning as a means for providing feedback thus allowing for adaptive mechanisms."
362,1462393,Collaborative Research: EAGER-DynamicData: Machine Intelligence for Dynamic Data-Driven Morphing of Nodal Demand in Smart Energy Systems,ECCS,"Big Data Science &Engineering, ",9/1/15,9/4/15,Lefteri Tsoukalas,IN,Purdue University,Standard Grant,Akbar Sayeed,8/31/17,"$86,696.00 ",Miltiadis Alamaniotis,tsoukala@ecn.purdue.edu,Young Hall,West Lafayette,IN,479072114,7654941055,ENG,"8083, O395","153E, 155E, 7916",$0.00 ,"The electric power grid is the indispensable infrastructure for power delivery and distribution. It is a system of high complexity and heterogeneity comprised of a variety of interconnected systems, subsystems, generators, and loads. In addition, it is a dynamic system with evolving characteristics that suffers from several infrastructure limitations, which if not handled properly, may lead to instabilities with severe consequences including costly brownouts and blackouts. However, advancements in information and data-driven technologies offer the necessary ground for developing tools that efficiently monitor grid infrastructure and manage electricity flows in ways that achieve and maintain high performance and reliability in grid operations. Towards that end, coupling power systems with information systems converts traditional electric energy delivery infrastructures into interconnected hybrid energy-data systems called smart energy systems, where power flow is controlled via information signals. Dynamic data available in smart energy systems includes, but is not limited to, hourly user energy consumption measurements from smart meters, electricity pricing signals, system voltage readings from GPS-synchronized measuring units scattered throughout the network that can take hundreds of readings per second, and data from weather stations. Thus, due to grid complexity, a tremendous amount of information is not only generated but also transferred throughout the grid, and grid participants, such as customers, utility companies, and grid operators, are exposed to multiple heterogeneous data streams coming from various sources. In this data intensive environment, participants are being engaged to make fast real-time decisions regarding morphing of their load demand and consumption behavior patterns. Nodal load forecasting is identified as a key point for developing future smart energy systems and electricity markets. The principal theme of this research is the fast and optimal nodal load morphing in smart energy systems that takes into account big volumes of dynamically varying data.<br/><br/>In particular, this research addresses the problem of management and processing of big data within the framework of Dynamic Data Driven Systems (DDDS) as applied to nodal load morphing. The focus of this study will be the development of a set of new intelligent and self-adaptive algorithms for online big data processing and fast real-time decision-making in smart energy infrastructures. The main feature of the current research is the integration of machine learning DDDS with dynamic optimization methods to solve the computational problem of forecasting optimal or near-optimal shapes of a load in a timely manner accounting for multiple streams of continuously incoming data and their inherent uncertainty. Emphasis will be given in handling and processing incentive signals and more particularly electricity pricing signals as a major factor in load morphing. Furthermore, extensive testing and verification of the developed algorithms will be performed on real-time simulated scenarios obtained with the GridLAB-D software simulator. In short, the proposed research for nodal load morphing will enable a new and transformative approach towards efficient, inexpensive, and fast processing of big data as applied to smart energy systems."
363,1509789,"Conic optimization methods for control, system identification, and signal processing",ECCS,EPCN-Energy-Power-Ctrl-Netwrks,9/15/15,7/30/18,Lieven Vandenberghe,CA,University of California-Los Angeles,Standard Grant,Radhakisan Baheti,8/31/20,"$343,483.00 ",,vandenbe@ee.ucla.edu,10889 Wilshire Boulevard,LOS ANGELES,CA,900951406,3107940102,ENG,7607,"092E, 155E, 9251",$0.00 ,"Convex optimization methods are important in control, signal processing, machine learning, and many other fields of engineering and applied science. Advances in algorithms for convex optimization over the last twenty-five years have resulted in reliable and user-friendly software tools that are widely used in academic research and industry. The most popular software packages are based on a separation between a modeling front-end and a general-purpose solver for semidefinite optimization (a matrix extension of classical linear programming, and a special case of conic linear optimization). The task of the modeling front-end is to translate the optimization problem into the canonical form required by the semidefinite optimization solver. The techniques used in this translation step are the outcome of extensive research on how to represent nonlinear convex constraints in the semidefinite optimization format. The algorithms used in the solvers are primal-dual interior-point methods for semidefinite optimization, which reached a high level of maturity in the early 2000s. Each of these two layers brings a limit on scalability. The reduction to semidefinite optimization in the modeling step often requires the introduction of auxiliary variables and constraints, which can increase the size of the optimization problem considerably. In addition, the semidefinite optimization algorithms that are commonly used in convex optimization solvers are second-order methods and require in each iteration the solution of large, often dense, sets of linear equations.  This further limits the size of the problems that can be solved. This proposal is motivated by the increasing demand for large-scale convex optimization algorithms in control, signal processing, and system identification. The project focuses on developing specialized methods for two types of constraints that underlie some of the most important convex optimization applications in these areas, and that have recently found new applications in statistical signal processing and machine learning.    <br/><br/>The first class of problems consists of convex optimization problems involving convex cones of nonnegative Popov functions. This includes nonnegative matrix polynomials and trigonometric polynomials, and is of fundamental importance in linear system theory, control, and signal processing. The second class includes system identification methods based on minimizing the nuclear norm (trace norm) of structured matrices. The focus on these two problem classes is motivated by several reasons: first, their central position in system theory and signal processing; second, well-known difficulties in solving them using general-purpose semidefinite optimization software;  and, third, their importance in recently discovered techniques that extend 1-norm optimization methods for sparse signal recovery to sparse signal recovery problems over continuous domains and to matrix rank minimization problems. Two algorithmic approaches will be considered for each of the two problem classes: interior-point methods for non-symmetric conic optimization, that handle the constraints directly without embedding them in a much larger semidefinite optimization problem, and first-order proximal algorithms based on operator splitting and decomposition techniques."
364,1526353,"TWC: Small: Online tracking: Threat Detection, Measurement and Response",CNS,Secure &Trustworthy Cyberspace,7/1/15,8/14/15,Arvind Narayanan,NJ,Princeton University,Standard Grant,Dan Cosley,6/30/18,"$500,000.00 ",,arvindn@princeton.edu,Off. of Research & Proj. Admin.,Princeton,NJ,85442020,6092583090,CSE,8060,"7434, 7923",$0.00 ,"The project develops new technologies for continual, web-scale measurement and rapid defenses against emerging threats to web privacy and security arising from third-party tracking. It draws from the fields of web security, systems, measurement, statistics, and machine learning. The outputs of this project will enable website administrators to find and fix a large class of privacy and security problems. They will help improve existing browser privacy tools. Finally, they will aid regulatory enforcement and contribute to better-informed press reporting on online privacy and security. <br/><br/>This project is driven by two fundamental questions: (i) which privacy and security threats are created or exacerbated by the presence of embedded third party elements? and (ii) how can we automatically detect these problems and develop defensive tools for users and site administrators? The project uses automated web browsing and measurement to generate ""third-party threat profiles"" for websites, maintain a ""web privacy census,"" and develop a new class of browser privacy tools using machine learning. The project brings transparency to the third-party ecosystem and enables oversight, giving users and developers the upper hand in blocking unwanted tracking and supporting enforcement actions when companies violate established privacy rules."
365,1534433,Collaborative Research: Record Linkage and Privacy-Preserving Methods for Big Data,SES,"Methodology, Measuremt & Stats, Data Cyberinfrastructure, Secure &Trustworthy Cyberspace, SCIENCE RESOURCES STATISTICS",9/15/15,9/11/15,Aleksandra Slavkovic,PA,Pennsylvania State Univ University Park,Standard Grant,Cheryl Eavey,8/31/19,"$334,243.00 ",,sesa@psu.edu,110 Technology Center Building,UNIVERSITY PARK,PA,168027000,8148651372,SBE,"1333, 7726, 8060, 8800","7433, 7434, 9102",$0.00 ,"This research project will develop sound statistical and machine learning techniques for preserving privacy with linked data.  Social entities and their patterns of behavior is a crucial topic in the social sciences.  Research in this area has been invigorated by the growth of the modern information infrastructure, ease of data collection and storage, and the development of novel computational data analyses techniques.  However, in many application areas relevant and sensitive information is commonly located across multiple databases.  Data analysis is inherently impossible without merging databases, but at the cost of increasing the risk of a privacy violation.  This research will address the problem of how to perform valid statistical inference in the presence of multiple data sources, data sharing, and privacy in the age of ""big data.""  The investigators' new modeling construct for inference and uncertainty quantification will contribute to both statistics and the many disciplines for which statistics is a principal tool.  The methods will have a wide range of applications in the social, economic, and behavioral sciences, including medicine, genetics, official statistics, and human rights violations.  The investigators will collaborate with post-doctoral researcher and with graduate and undergraduate students.  The statistical methods will be encapsulated in open-source software packages, allowing off-the-shelf use by practitioners while facilitating more detailed control and extensions.<br/><br/>This interdisciplinary research project will improve upon methods in record linkage and privacy using state-of-the-art techniques from statistics and machine learning.  Record linkage is the process of merging possible noisy databases with the goal of removing duplicate entries.  Privacy-preserving record linkage (PPRL) tries to identify records that refer to the same entities from multiple databases without compromising the privacy of the entities represented by these records.  The research will focus on three aims: (1) development of new Bayesian methods for PPRL, where the error can be propagated exactly across the entire linkage process and into statistical inference, including new privacy measures to capture a tradeoff between utility and risk of any individual risk in a linked database; (2) development of new robust methods for realizing synthetic data releases post-linkage with differential privacy guarantees and its relaxations to address additional layers of privacy and support broader data sharing; and (3) exploration of ""big data"" methods such as variational inference to address scalability and latent cluster exchangeability issues existing within linkage and privacy, such that the new methods can scale to multiple and large databases.  The new methods will be scalable and assess uncertainty throughout the entire linkage and privacy process and can be evaluated using Bayesian disclosure risk and Bayesian differential privacy.  The project is supported by the Methodology, Measurement, and Statistics Program and a consortium of federal statistical agencies as part of a joint activity to support research on survey and statistical methodology."
366,1547464,Collaborative Research: EAGER: Automating HERD Reporting Using Machine Learning and Administrative Data,SMA,STAR Metrics,9/1/15,12/14/15,Rodolfo Torres,KS,University of Kansas Center for Research Inc,Standard Grant,Cassidy Sugimoto,8/31/18,"$176,071.00 ",Jun Huan,torres@ku.edu,2385 IRVING HILL RD,Lawrence,KS,660457568,7858643441,SBE,8022,"7626, 7916, 9150",$0.00 ,"The National Center for Science and Engineering Statistics (NCSES) Higher Education Research and Development Survey (HERD) data are collected through a survey instrument sent to approximately 900 universities and colleges annually. Each of these institutions collects data to respond to the survey in their own way. Most rely on highly labor-intensive processes to gather and classify information about expenditures and research projects in terms of the character of the work, funding sources and fields of science.  The ad-hoc expenditure classification methods employed are dependent on the individuals carrying out the task as they develop the necessary evaluation skills over time.  There is potential for a lack of consistency over time and across institutions.  <br/><br/>This research develop the tools necessary to leverage university administrative data to automate the essential and time-consuming step of classifying projects by science areas, purpose and sponsor type  required to respond to the NCSES HERD Survey.  The results will provide a better understanding of the similarities/differences in the data reported for HERD and STAR METRICS® and provide suggestions about how data collection for each source might be improved.<br/>"
367,1462530,EAGER-DynamicData: A Hierarchical Approach to Dynamic Big Data Analysis in Power Infrastructure Security,ECCS,"Big Data Science &Engineering, ",9/1/15,9/4/15,Amir-Hamed Mohsenian-Rad,CA,University of California-Riverside,Standard Grant,Akbar Sayeed,8/31/18,"$185,000.00 ","Christian Shelton, Fabio Pasqualetti",hamed@ee.ucr.edu,Research & Economic Development,RIVERSIDE,CA,925210217,9518275535,ENG,"8083, O395","155E, 5384, 7916",$0.00 ,"This research will address a dynamic big data problem that is of urgent national interest: the need for efficient methods to diagnose faults and attacks in critical interconnected infrastructures, such as electricity power networks. Additionally, this project will investigate new methodologies to extract knowledge from the complex streams of data that come from various sensors in infrastructure systems and the models of their behavior. Results and findings in this project will be validated via industry-accredited power system simulators, and will be useful to the power industry in enhancing the safety, stability, and security of essential power infrastructure. This project will promote multi-disciplinary research involving expertise in big data analysis, machine learning, security, power systems, and control systems. This research will provide a powerful bridge between theory and real-world applications while serving as a training platform for a diverse new generation of engineers at the University of California, Riverside, one of America's most ethnically diverse research-intensive institutions.<br/> <br/>This project will foster the use of multi-resolution data-driven methods for the detection and classification of anomalies in critical dynamical infrastructures, with focus on power networks. This project has three novel, innovative, and potentially transformative technical elements: (1) A comprehensive statistical model, as an alternative to existing physics-based models, using Dynamic Bayesian Networks and Conditional Random Fields to model complex infrastructures subject to failures and attacks; (2) A hierarchical detection and classification method based upon machine learning concepts to tame and leverage the vast amount and diversity of dynamic multi-resolution data collected by spatially distributed sensors; (3) A systematic method to train and inform data-driven methodologies from model-based and analytical knowledge that come from power systems and control theory to build scalable and performing detection and classification mechanisms in power infrastructure security."
368,1536407,Collaborative Research: Decision Model for Patient-Specific Motion Management in Radiation Therapy Planning,CMMI,OE Operations Engineering,9/1/15,8/31/15,Wanpracha Chaovalitwongse,WA,University of Washington,Standard Grant,Georgia-Ann Klutke,5/31/17,"$184,758.00 ","Paul Kinahan, George Sandison, Shan Liu, Stephen Bowen",artchao@uark.edu,4333 Brooklyn Ave NE,Seattle,WA,981950001,2065434043,ENG,006Y,"076E, 078E, 8023",$0.00 ,"A significant challenge in lung cancer radiation therapy (RT) is respiration-induced tumor motion, which hinders sufficient delivery of curative doses to target volumes. Although modern tumor motion management strategies for positron emission tomography/computed tomography (PET/CT)-guided RT are becoming more available, those techniques have yet to be fully incorporated into clinical practice. This is mainly because not every patient will benefit from a costly and lengthy motion-managed PET/CT scan due to high intra-patient and inter-patient variability of respiratory patterns. The objective of this project is to bridge the knowledge gap of which motion management method would best benefit an individual patient. This project will develop a new decision-making paradigm, in which machine learning techniques will be developed to characterize respiratory motion patterns and combine them with other diagnostic factors to predict the benefits from motion management methods for each individual patient. A decision-analytic cohort model will be developed to compare and evaluate the cost-effectiveness of the new decision paradigm and the traditional population-based radiation oncology practice of motion management based on our existing database of respiratory traces from more than 3,000 patients. While specifically applied to decisions surrounding respiratory motion management, the developed decision paradigm can be generalized and applied to other real life decision analysis problems.<br/><br/>This award supports fundamental research in data mining/machine learning and decision analysis, which will provide needed knowledge for the development of tools for effective management of patient-specific tumor motion. The modeling effort in this project will 1) establish a new mathematical foundation for supervised multivariate sparse variable selection and prediction to discover complicated multivariate relationships among high-dimensional variables; 2) construct a general integrated validation framework to rigorously test the cost-effectiveness of patient-specific health interventions. The new multivariate sparse variable selection and prediction approach can be used to build an interpretable prediction model, handle high-dimensional data with a low sample size, avoid under-shrinkage effect, and incorporate structured group selection. The cost-effectiveness analysis framework integrates the outcome of prediction model, the treatment effect and survival outcome model. This modeling aims to quantitatively estimate long-term cancer survival outcomes from improvement in patient-specific planning of radiation dosing by selective motion control."
369,1444214,US-Belgium workshop: Atomic Switch Networks for Neuromorphic Reservoir Computing; Late Fall-2015/Early Spring 2016; University of Ghent-Belgium.,OISE,Catalyzing New Intl Collab,9/15/15,9/11/15,Adam Stieg,CA,University of California-Los Angeles,Standard Grant,Anne Emig,2/28/17,"$49,117.00 ",James Gimzewski,stieg@cnsi.ucla.edu,10889 Wilshire Boulevard,LOS ANGELES,CA,900951406,3107940102,O/D,7299,"5947, 5979, 5980, 7556, 8091",$0.00 ,"NSF CNIC proposal #1444214<br/>US-Belgium Workshop: Atomic Switch Networks for Neuromorphic Reservoir Computing<br/><br/>Part 1:<br/>The human brain outperforms digital computers in a number of tasks such as image, motion tracking and sound recognition and decision making in complex and often noisy and error prone environments. Digital computers are by their nature poorly-suited for tasks such as autonomous control (navigation, robotics), pattern recognition (speech, vision) or prediction (weather, financial markets). A biologically inspired approach to computing, called Reservoir Computing (RC), on the other hand, has demonstrated the potential to perform complex tasks efficiently. To perform RC, a newly developed hardware platform called the Atomic Switch Network (ASN) uses nanotechnology to create billions of synthetic synapses wired up in a fashion similar to that of the neocortex in the human brain. The implementation of a functioning RC-ASN system requires the collaborative expertise from recognized world leaders in RC methods at Ghent University, Belgium and the UCLA team who have developed the ASN device. UCLA has proposed a participant-driven workshop involving invited lectures, hands-on tutorials with hardware and software and breakout discussions with the goal to accelerate realization of this new form of computers system. This workshop will provide international research opportunities to 5 US students and early career researchers, while also promoting team-building skills, student-driven collaboration, and cultural exchange. By combining concepts from nanoscience, neuroscience, and machine learning, this proposal seeks to leverage the collective expertise of all parties to advance this next-generation cognitive technology. The successful outcomes of this research will also benefit the BRAIN Initiative, which is a priority research area of the U.S.<br/>  <br/>Part 2:<br/>Atomic Switch Networks (ASN) are a unique class of biologically inspired computing architectures designed to produce a complex, dynamical system through the collective interactions of functional nanoscale materials. These self-organized devices retain the intrinsic memory capacity of their component resistive switching elements while generating a class of emergent behaviors commonly associated with biological cognition. Their capacity for non-linear transformation of input information, which is processed and stored in a distributed fashion, generates patterns of dynamic spatiotemporal activity that can be used as the basis for a computational platform. Recent efforts to model, simulate, and measure the operational dynamics of ASNs toward hardware implementation of reservoir computing (RC), a burgeoning field that investigates the computational aptitude of complex biologically inspired systems to address problems in which data is constantly changing, incomplete, or subject to errors, indicate the necessity to establish a collaboration with experts in the field of machine learning. The combined expertise of proposed workshop participants will focus on a critical assessment of how to best utilize ASN devices to overcome current operational limits on real-time signal processing in the RC paradigm such as speed, network density, and scalability. Beyond lectures and discussion sections, tutorial workshops delivered by participants from the US and EU will be utilized to disseminate/demonstrate the current status of (1) modeling/simulation of ASNs, (2) physical implementations of ASNs, and (3) physical implementations of other hardware systems (memristors, optoelectronics, etc.). Targeted outcomes include identification of specific areas for near-term collaboration and follow-on funding within existing Core programs at the NSF.  This new collaboration will provide a tremendous opportunity to explore the best-case scenario resulting from the world's leading RC research with a potentially groundbreaking platform for hardware-based RC to contribute to novel approaches in real-time information processing and computation."
370,1557482,QuBBD: Collaborative Research: Personalized Predictive Neuromarkers for Stress-Related Health Risks,DMS,,9/15/15,9/10/15,Peter Gianaros,PA,University of Pittsburgh,Standard Grant,Nandini Kannan,8/31/16,"$8,835.00 ",,gianaros@pitt.edu,300 Murdoch Building,Pittsburgh,PA,152603203,4126247400,MPS,O470,,$0.00 ,"Coronary heart disease (CHD) remains the leading cause of premature death among adults in the U.S. and other postindustrial nations.  Predicting CHD risk - especially for a particular individual - remains a fundamental challenge.  This project will investigate the application of statistical machine learning approaches to multimodal brain imaging, behavioral, biological, and related data to enhance the prediction of CHD risk.  It specifically addresses the question of whether particular patterns of human brain activity during psychological stress reliably predict known risk markers of CHD; namely, stress-related rises in blood pressure and arterial morphology.  From a basic science perspective, this research will advance our mechanistic understanding of how the brain relates to our physical health.  From a public health perspective, this research will help to identify markers of brain activity that could be objectively identified and possibly targeted for modification in otherwise healthy people at risk for future CHD. <br/><br/>The key challenge with mapping neuroimaging data to CHD risk lies in being able to very precisely regress observed psychological stress reactions on the time-series of brain activity recorded in thousands of voxels, and identify which brain regions are most relevant for the regression.  Conventional analytic approaches involve forming coarse temporal summaries by committing to specific parametric models, such as a generalized linear model and a fixed model for hemodynamic response, that result in poor accuracy.  This award supports initiation of a collaborative research project that brings together a highly cross-disciplinary team of statistical machine learning, neuroimaging and health psychology researchers to tackle the following two goals: 1) identify a generalizable model and neuromarkers that predict individual differences in cardiovascular risk factors based on neural dynamics under psychological stress.  This will be enabled through novel methods for functions-to-real and functions-to-function lasso regression;  2) characterize how neural patterns can be integrated with other physiological and anthropometric factors to personalize individual risk scores and neuro-biomarkers.  This award is supported by the National Institutes of Health Big Data to Knowledge (BD2K) Initiative in partnership with the National Science Foundation Division of Mathematical Sciences."
371,1540657,BSF: 2014324: Streaming Algorithms for Fundamental Computations in Numerical Linear Algebra,CCF,Special Projects - CCF,9/1/15,8/6/15,Michael Mahoney,CA,International Computer Science Institute,Standard Grant,Tracy Kimbrel,8/31/19,"$40,000.00 ",,mmahoney@icsi.berkeley.edu,1947 CENTER ST STE 600,Berkeley,CA,947044115,5106662900,CSE,2878,2878,$0.00 ,"Streaming algorithms that use every input datum once (single-pass) or scan the input a small number of times (multiple passes) are gaining importance due to the increasing volumes of data that are available for business, scientific, and security applications. Performing large-scale data analysis and machine learning often requires addressing numerical linear algebra primitives, such as least squares regression, singular value decompositions, least absolute deviations regression, and canonical correlation analysis.  In this proposal, the PIs aim to improve significantly the theory and practice of streaming algorithms for these fundamental linear algebra kernels.  The new algorithms will provide faster and more accurate kernels for the ubiquitous big data applications, reducing resource use (hardware and energy) of machine learning applications, and will make security applications that rely critically on accuracy provably trustworthy.  In addition, they will enable improved exploitation of data in physical, chemical, and biomedical applications.<br/><br/>The computations that will be considered are performed either using inexact incremental single-pass algorithms, or by expensive multi-pass algorithms. Although existing inexact algorithms often work well enough in practice, the worst-case behavior of applications relying on these building blocks has not been characterized. This is especially troubling in the security and anomaly-detection areas, where a malicious party could conceivably exploit such inexactness. The PIs will develop a set of provably-accurate single-pass algorithms for numerical linear algebra. They will also explore alternative algorithmic routes, mainly multi-pass randomized algorithms, both for the core problems (least squares regression regression and singular value decomposition) and for the more challenging ones (least absolute deviations regression and canonical correlations). They will characterize the accuracy/performance tradeoffs associated with these computations, where performance refers mostly to the number of passes but also to the total computational effort (including communication). The PIs will carry out this investigation using benchmarks from significant applications, as well as theoretical lower bounds on single-pass algorithms."
372,1557605,QuBBD: Mathematical models for a molecular genetic understanding of population variation in risk of cardiovascular disease,DMS,"INFRASTRUCTURE PROGRAM, ",9/15/15,9/11/15,Stephen Ramsey,OR,Oregon State University,Standard Grant,Nandini Kannan,8/31/16,"$99,579.00 ",Harold Bae,stephen.ramsey@oregonstate.edu,OREGON STATE UNIVERSITY,Corvallis,OR,973318507,5417374933,MPS,"1260, O470",8083,$0.00 ,"Genome-wide association studies (GWAS) are a powerful approach for mapping the regions of the genome containing single-nucleotide differences in the population (single-nucleotide variants or SNVs) that are associated with trait variation. Narrowing down from GWAS-identified genomic regions to the individual SNVs that are responsible for trait variation is particularly challenging for regions of the genome that are in-between genes.  A second challenge in GWAS is that the large number of SNVs necessitates a high level of statistical stringency, and thus, many biologically relevant SNVs are missed.  GWAS is used for many human disease traits (such as coronary artery disease or CAD), and thus, addressing these two challenges would have broad significance in biology and biomedical research.  This award supports initiation of a collaborative research project that will address these two challenges by developing mathematical models that integrate a variety of types of measurements and information derived from cells and population studies, in order to pinpoint SNVs in-between genes that affect trait variation, and to improve the statistical power of GWAS.  CAD is a high-significance application for improving GWAS because of CAD's prevalence (15 million in the U.S.).<br/><br/>The objectives of this project are to (1) create and evaluate an integrative statistical model for improving power for GWAS analysis and for discovering novel gene-trait associations and (2) create and evaluate a machine-learning model for identifying regulatory variants within intergenic GWAS regions.  The models would incorporate features from large-scale datasets from the Framingham Heart Study SHARe database, the ENCODE project, the GTEx project, and CARDIOGRAMplusC4D. The models' performance would be benchmarked against previously published models.  The project's significant outcomes would be: (1) the first analytic statistical model for integrative GWAS analysis that would provide a readily interpretable significance score; (2) a quantitatively validated and interpretable machine-learning model for combining genomic information types to predict regulatory variants; (3) feature importance scores for the genomic features that are integrated within the model; and (4) identification of new GWAS loci for population variation in CAD risk, and, for regulatory variants within the loci, the genes and transcription factors (and ultimately, the gene functional annotations) with which they are associated.  Software implementations of the methods will be shared in an open-source software repository.  This award is supported by the National Institutes of Health Big Data to Knowledge (BD2K) Initiative in partnership with the National Science Foundation Division of Mathematical Sciences."
373,1547513,Collaborative Research:  EAGER: Automating HERD Reporting Using Machine Learning and Administrative Data,SMA,STAR Metrics,9/1/15,8/26/15,Joshua Rosenbloom,MA,National Bureau of Economic Research Inc,Standard Grant,Cassidy Sugimoto,8/31/18,"$37,357.00 ",,jlrosenb@iastate.edu,1050 Massachusetts Avenue,Cambridge,MA,21385398,6178683900,SBE,8022,"7626, 7916",$0.00 ,"The National Center for Science and Engineering Statistics (NCSES) Higher Education Research and Development Survey (HERD) data are collected through a survey instrument sent to approximately 900 universities and colleges annually. Each of these institutions collects data to respond to the survey in their own way. Most rely on highly labor-intensive processes to gather and classify information about expenditures and research projects in terms of the character of the work, funding sources and fields of science.  The ad-hoc expenditure classification methods employed are dependent on the individuals carrying out the task as they develop the necessary evaluation skills over time.  There is potential for a lack of consistency over time and across institutions.  <br/><br/>This research develop the tools necessary to leverage university administrative data to automate the essential and time-consuming step of classifying projects by science areas, purpose and sponsor type  required to respond to the NCSES HERD Survey.  The results will provide a better understanding of the similarities/differences in the data reported for HERD and STAR METRICS® and provide suggestions about how data collection for each source might be improved.<br/>"
374,1518308,Collaborative Research: New Insights from a Systematic Approach to Quasar Variability,AST,EXTRAGALACTIC ASTRON & COSMOLO,7/15/15,7/6/15,Stanislav Djorgovski,CA,California Institute of Technology,Standard Grant,Richard E. Barvainis,6/30/18,"$289,745.00 ",Matthew Graham,george@astro.caltech.edu,1200 E California Blvd,PASADENA,CA,911250600,6263956219,MPS,1217,1207,$0.00 ,"A longstanding problem in astrophysics is to understand how galaxies form and develop throughout their lifetimes.  Such understanding is necessary to uncover how our Universe evolved and to gain insight into the origin of our own Milky Way Galaxy.  One important aspect of understanding galaxy formation and evolution is to study quasars and other active galactic nuclei.  They are also interesting astrophysical phenomena in their own right and serve as a probe of relativistic physics.  They co-evolve with their host galaxies and trace the evolution of cosmic structure.  However, most of the methods for quasar discovery are based on the properties of their broadband spectral energy distributions, and nearly all known quasars and quasar candidates come from samples that use some type of flux ratios or just the presence of a non-thermal emission.  Variability offers a spectrum-independent method for quasar discovery.  Although variability has been much studied on the basis of individual or a few objects, variability-based quasar surveys have so far been limited to small dedicated regions of sky with at most a few thousand objects and/or poor time resolution.  This study of quasar variability employs the Catalina Real-time Transient Survey (CRTS) data set, which covers about 80\% of the sky over a baseline greater than 9 years. Its 500 million object data set currently holds about 250,000 known quasars, 500,000 photometric quasar candidates and an estimated 1,000,000 new variability-selected quasars.  This will form the largest quasar data set to date.  In keeping with the CRTS Open Data policy, all quasars (and other classified objects) identified in this project will be released to the community.  This will form a major new resource for both quasar and more general variability studies.  The statistical methods to be used are also applicable to any irregular-sampled time series, and the combination of these with machine-learning techniques is a case study for data-intensive science.<br/><br/>This project is a collaboration with a primarily undergraduate institution and directly enhances the STEM education of two undergraduates.  Working with scientists at the Center for Data-Driven Discovery (CD3) at Caltech, they will be exposed to cutting-edge techniques in data science, including high level usage of data mining and extracting meaningful results from these large data sets.  Data products from this project also form the basis for student projects at the joint US-Chile-funded La Serena School for Data Science, training the next generation in applied tools for handling big astronomical data.<br/><br/>In particular, this project will focus on (i) the correlation of quasar variability features, particularly characteristic timescales, with physical parameters, such as luminosity, black hole mass, and the Eddington ratio; (ii) periodic variability as possible evidence for supermassive black hole binaries; (iii) variability as a probe of obscuration in young dust-enshrouded red quasars; and (iv) quantifying wavelength dependencies of variability to improve quasar selection and constrain different models of physical processes.  The study will employ modern statistical techniques that can work naturally with irregularly-sampled gappy time series without the need for reprojection or smoothing.  In combination with machine-learning methods, these will produce optimal ensemble-based results, such as new variability-polychromatic methods for quasar selection.  This project will be a key study on optical quasar variability well into the LSST era.  It is at least two orders of magnitude larger than any previous study in terms of sky coverage and number of quasars and an order of magnitude better in terms of time resolution (number of observations / baseline).  It will also substantially increase the number of high likelihood quasar candidates known, particularly in the regions of the sky not covered by SDSS."
375,1457262,Collaborative Research: WiFiUS: Heterogeneous Resource Allocation for Hierarchical Software-Defined Radio Access Networks at the Edge,CNS,"SPECIAL PROJECTS - CISE, SPECIAL PROJECTS - CCF",3/15/15,3/11/15,Guoliang Xue,AZ,Arizona State University,Standard Grant,Richard Brown,2/28/17,"$140,000.00 ",,xue@asu.edu,ORSPA,TEMPE,AZ,852816011,4809655479,CSE,"1714, 2878","7363, 8229",$0.00 ,"Wireless cellular infrastructure is becoming dense and heterogeneous. Software-defined networking has brought new opportunities for enhancing network planning and administration. When applied in geographically large-scale radio access networks, software-defined networking suffers from scalability and survivability problems due to high centralization.  In this project, a hierarchical software-defined radio access network architecture is designed. In contrast to a flat version of software defined radio access networks, which have large control overhead and system latency for large scaled networks, the proposed scheme offers reduced control overhead and system latency by exploration locality, thereby enhancing system scalability. In addition, the tree structure of the hierarchical design can easily deal with single points of failure. The plan is to first investigate the architecture design of the hierarchical software-defined radio access network, the protocol design for the control network, survivability, and locality and optimization, followed by large-scale elastic approaches, machine learning based approaches, and matching theory based approaches for network-wide resource allocation. The intellectual merit originates from the interdisciplinary fusion of different technologies including software-defined networking, wireless communications, software-defined radio, machine learning, and game theory. The research plan addresses challenging issues of coexistence of interfering clusters and elastic resource allocation for a large-scale radio access networks, while developing theoretically novel frameworks to tackle these challenges. This research will lead to simpler and more efficient resource allocation schemes for wireless networks by exploring the power of software-defined networks. The transformative and interdisciplinary project involves a complementary mix of network architecture design, theoretical modeling and analysis, and experimental simulations quantifying performance benefits. <br/><br/>The outcomes will be made available to the research community through high quality journal articles and conference presentations which may be used by industrial entities for network development and future industrial standardization. This project will strengthen collaboration in the research field of wireless communication between the United States and Finland. The proposed research activities will complement and enrich the growing curriculum on game theory and optimization at the University of Houston and Arizona State University through course development and special topic seminars. Highly skilled personnel in related areas will be trained in carrying out the proposed research tasks. Special efforts will be made to engage minority and underrepresented groups, including African-American, Hispanic and female graduate students. K-12 outreach is also planned so as to motivate high school students in science and engineering."
376,1456921,Collaborative Research: WiFiUS: Heterogeneous Resource Allocation for Hierarchical Software-Defined Radio Access Networks at the Edge,CNS,"SPECIAL PROJECTS - CISE, SPECIAL PROJECTS - CCF",3/15/15,3/11/15,Zhu Han,TX,University of Houston,Standard Grant,David Corman,2/28/17,"$140,000.00 ",,zhan2@uh.edu,4800 Calhoun Boulevard,Houston,TX,772042015,7137435773,CSE,"1714, 2878","7363, 8229",$0.00 ,"Wireless cellular infrastructure is becoming dense and heterogeneous. Software-defined networking has brought new opportunities for enhancing network planning and administration. When applied in geographically large-scale radio access networks, software-defined networking suffers from scalability and survivability problems due to high centralization.  In this project, a hierarchical software-defined radio access network architecture is designed. In contrast to a flat version of software defined radio access networks, which have large control overhead and system latency for large scaled networks, the proposed scheme offers reduced control overhead and system latency by exploration locality, thereby enhancing system scalability. In addition, the tree structure of the hierarchical design can easily deal with single points of failure. The plan is to first investigate the architecture design of the hierarchical software-defined radio access network, the protocol design for the control network, survivability, and locality and optimization, followed by large-scale elastic approaches, machine learning based approaches, and matching theory based approaches for network-wide resource allocation. The intellectual merit originates from the interdisciplinary fusion of different technologies including software-defined networking, wireless communications, software-defined radio, machine learning, and game theory. The research plan addresses challenging issues of coexistence of interfering clusters and elastic resource allocation for a large-scale radio access networks, while developing theoretically novel frameworks to tackle these challenges. This research will lead to simpler and more efficient resource allocation schemes for wireless networks by exploring the power of software-defined networks. The transformative and interdisciplinary project involves a complementary mix of network architecture design, theoretical modeling and analysis, and experimental simulations quantifying performance benefits. <br/><br/>The outcomes will be made available to the research community through high quality journal articles and conference presentations which may be used by industrial entities for network development and future industrial standardization. This project will strengthen collaboration in the research field of wireless communication between the United States and Finland. The proposed research activities will complement and enrich the growing curriculum on game theory and optimization at the University of Houston and Arizona State University through course development and special topic seminars. Highly skilled personnel in related areas will be trained in carrying out the proposed research tasks. Special efforts will be made to engage minority and underrepresented groups, including African-American, Hispanic and female graduate students. K-12 outreach is also planned so as to motivate high school students in science and engineering."
377,1534545,Empirical Process and Modern Statistical Decision Theory,DMS,STATISTICS,5/1/15,4/28/15,Huibin Zhou,CT,Yale University,Standard Grant,Gabor J. Szekely,4/30/16,"$21,000.00 ",,huibin.zhou@yale.edu,Office of Sponsored Projects,New Haven,CT,65208327,2037854689,MPS,1269,7556,$0.00 ,"The workshop titled ""Empirical Process and Modern Statistical Decision Theory"" will be held at Yale University on May 7-9, 2015. The era of big data is fundamentally changing every aspect of our life through discoveries in science, medicine, and engineering. Many innovative and intuitively appealing methodologies have been proposed to make novel and significant discoveries by analysis of complex and big data. It is timely and critically important for statisticians to develop deep, broad, and formal statistical theory to understand and justify why and when certain methodologies would work or not work, to guide statistical practice to make valid and influential contributions to our society. This workshop will bring together some of authorities in statistical decision theory and empirical Process to review the most important and influential advances in the past, to report their most recent exciting research, and to discuss their view of future developments. The workshop will provide a venue for promising young researchers to interact with these leaders of the field and each other, leading to future collaborations and discoveries. A potential outcome of this workshop will provide a guidance to researchers and professors on how and what to teach in empirical process to train our students in understanding and developing modern statistical decision theory.<br/><br/>Empirical process has been playing a key role in developing modern statistical decision theory for a wide range of important models and significant methodologies, such as providing a unified framework for many high dimensional linear models by Gaussian width, establishing asymptotic equivalence theory of Le Cam for various statistical models by the KMT construction, and justifying the effectiveness of important algorithms in machine learning including SVM by VC dimension. In the last ten to fifteen years, the statistics research has witnessed  a tremendous successes of empirical process theory in Bayesian nonparametrics, shape constrained estimation, robust estimation, minimax regret, lasso, and sparse principal component analysis. This workshop will bring together some of authorities in statistical decision theory including Lawrence Brown and Iain Johnstone, and in empirical processes including Richard Dudley and Evarist Gine, as well as some of the the most prominent researchers in high dimensional estimation, Bayesian nonparametrics, robust estimation, machine learning, and shape constrained estimation, to celebrate the most significant advances in the past and to discuss exciting future developments."
378,1454688,CAREER: Ligand Engineering of Structure and Electronic Function in Complex Metal Oxyfluorides,DMR,CONDENSED MATTER & MAT THEORY,6/1/15,5/13/19,James Rondinelli,IL,Northwestern University,Continuing Grant,Daryl Hess,5/31/21,"$500,000.00 ",,jrondinelli@northwestern.edu,750 N. Lake Shore Drive,Chicago,IL,606114579,3125037955,MPS,1765,"1045, 7569, 7752, 8609",$0.00 ,"NON-TECHNICAL SUMMARY<br/><br/>The CAREER project supports computational materials science research and education aimed at understanding and designing electronic properties of materials, including electrical resistivity and optical behavior, by control over material structure at the level of atoms. The specific compounds of interest include oxides with transition metal cations, and focus on the effect that fluorine, which can substitute for oxygen in the materials, has on the electronic functionality. Previous research on transition metal oxides has established the importance of atomic structure engineering of electronic responses in many crystal families. Most conventional property-by-design routes rely, however, on changing the transition metal cation in oxides. This project seeks to apply another route - tuning the interactions in the crystal through anion (fluorine and oxygen) substitution. Changes in the geometry, arrangement and composition of the anion atoms will be explored through a confluence of materials theory techniques based on symmetry analyses, materials informatics (machine learning), and quantum mechanical calculations. The PI has ongoing collaborations with leading experts in synthesis and characterization of such materials composed of transition metals, oxygen, and fluorine; understanding derived here will stimulate experimental methods and vice versa. Continued investigation of known materials, while valuable, is inadequate to formulate strategies for deterministic property control. Knowledge obtained here will facilitate the selection and design of materials with tunable electronic states. It will benefit society by advancing the repertoire of structure-based design strategies to control electronic structure, which could lead to the discovery of new functional materials, e.g. for better performing energy storage and conversion systems, materials for transparent electronics, and optical technologies relying on laser generated light.<br/><br/>This project will further the educational opportunities of students at Northwestern University and other academic institutions, precollege students, and contribute to the professional development of high school teachers. The PI will implement an educational plan to foster awareness, understanding, and appreciation of advanced technology materials and data-driven scientific methods through two main tasks. First, he will create Engineering Design Modules capable of engaging students in grades 9 through 12 by fostering model building skills to analyze and communicate concepts taught in secondary chemistry and physics courses that underpin many modern technologies, and support teachers preparing for the recently adopted Next Generation Science Standards. Second, he will create a Materials Informatics Curriculum to engage university students in modern informatics-based science problem-solving methods. All contextual learning activities will build knowledge, promote scientific and engineering literacy, and provide greater insight into the societal needs for engineering solutions, fostering cognitive skills through an emphasis on cause/effect relationships that are axiomatic to the research objectives and vital to the next-generation workforce. Assessment of the proposed educational activities and broad dissemination through multiple platforms will determine the efficacy of the educational activities, improve their implementation, and maximize impact.<br/><br/><br/>TECHNICAL SUMMARY<br/><br/>This CAREER award supports synergistic research, education, and outreach activities which focus on the design of functional electronic behavior in transition metal oxyfluorides using control over the ligand sublattice by oxygen/fluorine substitution and ordering. Conventional routes to direct the responses in transition metal oxides primarily rely on cation substitution and interfacial effects in thin films and superlattices, which offer limited control owing to a single (oxygen) anion - this makes materials discovery challenging. Remarkably, ligand (anion) engineering with mixed anion polyhedral building blocks remains to be fully exploited for property control and design, especially in these materials which already find use in energy generation and storage, phosphors, and catalysis.<br/> <br/>Combinations of applied group theory, informatics, and density functional theory calculations will be applied to achieve the main research objectives, which include (1) Advancing new theoretical methods to establish structure-function axioms for how anion order can be used to direct crystal structure and properties; (2) Formulating a quantitative theory of structure stability based on understanding the ligand sublattice symmetry and local bonding interactions; and (3) Understanding the consequences of mixed-anion polyhedral topologies on electronic properties. Structure-property axioms will be extracted by studying the consequences of anion substitution using oxyfluoride building blocks on physical properties in cryolite and elpasolite structures. With that knowledge, quantitative guidelines will be constructed to tailor electronic structure and properties in new oxyfluorides: metal-insulator transitions, electronic band gaps, and large non-linear optical responses. Success in this research will produce new knowledge underlying crystal stability, chemical bonding, and electronic behavior. It will articulate predictive rules for selecting new oxyfluorides, accelerate discovery, and enable an unprecedented expansion of compounds with varying electronic functions. Ultimately, interactions with experimental groups will lead to the discovery of functional properties in structurally and chemically more complex (hybrid) organic and inorganic materials than those proposed. Such collaborations will ensure that the virtual predictions translate into realistic models and new materials, which may transform the discovery process for materials deployed in glasses, phosphors, fuel-conversion, and Li-ion batteries technologies.<br/><br/>This project will further the educational opportunities of students at Northwestern University and other academic institutions, precollege students, and contribute to the professional development of high school teachers. The PI will implement an educational plan to foster awareness, understanding, and appreciation of advanced technology materials and data-driven scientific methods through two main tasks. First, he will create Engineering Design Modules capable of engaging students in grades 9 through 12 by fostering model building skills to analyze and communicate concepts taught in secondary chemistry and physics courses that underpin many modern technologies, and support teachers preparing for the recently adopted Next Generation Science Standards. Second, he will create a Materials Informatics Curriculum to engage university students in modern informatics-based science problem-solving methods. All contextual learning activities will build knowledge, promote scientific and engineering literacy, and provide greater insight into the societal needs for engineering solutions, fostering cognitive skills through an emphasis on cause/effect relationships that are axiomatic to the research objectives and vital to the next-generation workforce. Assessment of the proposed educational activities and broad dissemination through multiple platforms will determine the efficacy of the educational activities, improve their implementation, and maximize impact."
379,1514174,"III: Medium: Collaborative Research: Computational Tools for Extracting Individual, Dyadic, and Network Behavior from Remotely Sensed Data",IIS,Info Integration & Informatics,9/1/15,7/5/16,Margaret Crofoot,CA,University of California-Davis,Standard Grant,Maria Zemankova,8/31/19,"$409,937.00 ",,mccrofoot@ucdavis.edu,OR/Sponsored Programs,Davis,CA,956186134,5307547700,CSE,7364,"7364, 7924, 9102, 9251",$0.00 ,"Recent technological advances in location tracking, video and photo capture, accelerometers, and other mobile sensors provide massive amounts of low-level data on the behavior of animals and humans.  Analysis of this data can teach us much about individual and group behavior, but analytical techniques that lead to insight about that behavior are still in their infancy.  In particular, these new data can provide an unprecedented window into the lives of wild animals, augmenting the traditional time-consuming first-hand observations from field biologists.  Unfortunately, the interpretation of low-level (i.e., unprocessed) data from animal-borne electronic sensors still poses a significant bottleneck in leveraging all of the available data to better understand the individual, pairwise, and group behavior of animal populations.  This project will develop tools for scaling the expert knowledge needed to interpret high-level behaviors from low-level sensor data using tools from statistical machine learning and network analysis.  These data and analytical tools promise to fundamentally change our understanding why animals do what they do, at high resolution and across multiple scales, from individuals to entire populations.  The results of the project will be applicable in many settings where massive sensor data is overwhelming traditional insight derived from observational approaches.  As part of the project, unique data on primate behavior that will bridge the low-level data and expert knowledge will be collected at Mpala Research Centre, Kenya. Undergraduate, graduate, and postdoctoral students from computer science and animal behavior will collaborate across continental and disciplinary boundaries. <br/><br/>The technical aims of this project include developing structured prediction methods that improve behavior recognition at multiple levels (individual, pair-wise, and group), using network properties to improve the identification of group activities, and advancing active learning in the structured prediction setting so that ""expensive"" expert knowledge and supplemental data collection will be judiciously utilized for maximum benefit in learning behavior recognition models.  Recognizing animal behavior from low-level sensor data is hierarchical in this approach, with individual activities recognized directly from data and the context of these data, the inferred individual activities informing pair-wise behavior recognition, and inferred pair-wise behavior informing group-level activity recognition. The benefits of improving the accuracy of individual and pair-wise behavior for recognizing group-level behavior will enable expert annotations to be requested that improve behavior recognition the most across all levels.  These advances will enable field-biologists to investigate new hypotheses about fundamental evolutionary, ecological, and population processes at scale without the burdens of complete manual annotation of collected data.  The methods will be applicable beyond field biology to understanding the hierarchy of behavior from individual entities to groups, from humans to cells, in scientific, educational, and business contexts. The team will leverage the interdisciplinary and international nature of the project to continue its ongoing work to increase participation of women and minorities in STEM research at undergraduate and graduate levels."
380,1446592,CPS: Frontier: Collaborative Research: BioCPS for Engineering Living Cells,CNS,"S&AS - Smart & Autonomous Syst, Special Projects - CNS, CPS-Cyber-Physical Systems",5/1/15,9/15/17,R. Vijay Kumar,PA,University of Pennsylvania,Continuing Grant,Ralph Wachter,4/30/20,"$1,425,147.00 ",,Kumar@seas.upenn.edu,Research Services,Philadelphia,PA,191046205,2158987293,CSE,"039Y, 1714, 7918","7918, 8236, 9251",$0.00 ,"Recent developments in nanotechnology and synthetic biology have enabled a new direction in biological engineering: synthesis of collective behaviors and spatio-temporal patterns in multi-cellular bacterial and mammalian systems. This will have a dramatic impact in such areas as amorphous computing, nano-fabrication, and, in particular, tissue engineering, where patterns can be used to differentiate stem cells into tissues and organs. While recent technologies such as tissue- and organoid on-a-chip have the potential to produce a paradigm shift in tissue engineering and drug development, the synthesis of user-specified, emergent behaviors in cell populations is a key step to unlock this potential and remains a challenging, unsolved problem. <br/><br/>This project brings together synthetic biology and micron-scale mobile robotics to define the basis of a next-generation cyber-physical system (CPS) called biological CPS (bioCPS). Synthetic gene circuits for decision making and local communication among the cells are automatically synthesized using a Bio-Design Automation (BDA) workflow. A Robot Assistant for Communication, Sensing, and Control in Cellular Networks (RA), which is designed and built as part of this project, is used to generate desired patterns in networks of engineered cells. In RA, the engineered cells interact with a set of micro-robots that implement control, sensing, and long-range communication strategies needed to achieve the desired global behavior. The micro-robots include both living and non-living matter (engineered cells attached to inorganic substrates that can be controlled using externally applied fields). This technology is applied to test the formation of various patterns in living cells. <br/><br/>The project has a rich education and outreach plan, which includes nationwide activities for CPS education of high-school students, lab tours and competitions for high-school and undergraduate students, workshops, seminars, and courses for graduate students, as well as specific initiatives for under-represented groups. Central to the project is the development of theory and computational tools that will significantly advance that state of the art in CPS at large. A novel, formal methods approach is proposed for synthesis of emergent, global behaviors in large collections of locally interacting agents. In particular, a new logic whose formulas can be efficiently learned from quad-tree representations of partitioned images is developed. The quantitative semantics of the logic maps the synthesis of local control and communication protocols to an optimization problem. The project contributes to the nascent area of temporal logic inference by developing a machine learning method to learn temporal logic classifiers from large amounts of data. Novel abstraction and verification techniques for stochastic dynamical systems are defined and used to verify the correctness of the gene circuits in the BDA workflow."
381,1526275,CHS: Small: Narrative-Based Training Simulations Using Theory of Mind,IIS,HCC-Human-Centered Computing,9/1/15,8/19/15,Stacy Marsella,MA,Northeastern University,Standard Grant,William Bainbridge,8/31/19,"$493,048.00 ",Magy Seif El-Nasr,stacymarsella@gmail.com,360 HUNTINGTON AVE,BOSTON,MA,21155005,6173733004,CSE,7367,"7367, 7923",$0.00 ,"This project will create a framework for simulation-based training that supports a learner's exploration and replay, and will exercise theory of mind skills, in order to deliver the full promise of social skills training. The term Theory of Mind (ToM) refers to the human capacity to use beliefs about the mental processes and states of others. In order to train social skills, there has been a rapid growth in narrative-based simulations that allow learners to role-play social interactions. However, the design of these systems often constrains the learner's ability to explore different behaviors and their consequences. Attempts to support more generative experiences face a combinatorial explosion of alternative paths through the interaction, presenting an overwhelming challenge for developers to create content for all the alternatives. Rather, training systems are often designed around exercising specific behaviors in specific situations, hampering the learning of more general skills in using ToM.  This research seeks to solve this problem through three contributions: (1) a new model for conceptualizing narrative and role-play experiences that addresses generativity, (2) new methods that facilitate content creation for those generative experiences, and (3) an approach that embeds theory of mind training in the experience to allow for better learning outcomes. This research is applicable to complex social skill training across a range of situations: in schools, communities, the military, police, homeland security, and ethnic conflict.<br/> <br/>This research begins with a paradigm shift that re-conceptualizes social skills simulation as the learner rehearsing a role instead of performing a role. This shift will exploit Stanislavsky's Active Analysis (AA), a performance rehearsal technique that explicitly exercises Theory of Mind skills. Further, AA's  decomposition into short rehearsal scenes can break the combinatorial explosion over long narrative arcs that exacerbates content creation for social training systems. The research will then explore using behavior fitting and machine learning techniques on crowd sourced data as way to semi-automate the development of multi-agent simulations for social training.  The research will assess quantitatively and qualitatively the ability of this approach (a) to provide experiences that support exploration and foster ToM use and (b) to support acquiring crowd sourced data that can be used to craft those experiences using automatic methods.  This project is unique in combining cutting edge work in modeling theory of mind, interactive environments, performance rehearsal and crowd sourcing. This multidisciplinary collaboration will enable development of a methodology for creating interactive experiences that pushes the boundaries of the current state of the art in social skill training. Reliance on crowd sourcing provides an additional benefit of being able to elicit culturally specific behavior patterns by selecting the relevant crowd, allowing for both culture-specific and cross-cultural training content."
382,1461491,"""Improved Methodologies for Field Experiments:  Maximizing Statistical Power While Promoting Replication.""",SES,Economics,9/1/15,9/3/15,Michael Anderson,CA,University of California-Berkeley,Standard Grant,Nancy Lutz,8/31/19,"$459,953.00 ",Jeremy Magruder,mlanderson@berkeley.edu,Sponsored Projects Office,BERKELEY,CA,947101749,5106433891,SBE,1320,9179,$0.00 ,"This project proposes new methods for analyzing data from randomized controlled trials (RCTs), which are now increasingly used in some areas of economics and other social sciences.  Researchers often want to use an RCT to test multiple different hypotheses.  However, carrying out this kind of data analysis requires careful consideration of statistical issues; in particular, there is a chance that using the same data to run multiple different statistical analyses will increase the chance of false positives.  As a result, economists conducting RCTs are increasingly specifying pre-analysis plans; that is, they are specifying how they plan to analyze the data before beginning the analysis.  These plans, however, do limit researchers' ability to reuse data and learn from the data before beginning the statistical analysis.  The research team will develop a potential mechanism for researchers to learn from the data without specifying every hypothesis in advance.  They will use concepts from machine-learning as well as probability theory to provide a better framework for how to maximize power within a given RCT experiment.  The results could therefore improve data analysis in important ways.<br/><br/>The research team has evidence that in practice preanalysis plans include tests for many, even hundreds, of hypotheses.  As a result current preanalysis plan designs are likely to be underpowered for many effects of economic interest.  The team will extend and apply techniques from biostatistics, including gatekeeping, sequential testing, and FDR control to the design of preanalysis plans. They will also develop a split-sample approach in which researchers conduct exploratory analysis on one part of the data set, and, using estimates from that part of the data combined with theory over likely mechanisms, refine their hypotheses before registering a subset of hypotheses to be tested on the remaining part of the data."
383,1446474,CPS: Frontier: Collaborative Research: BioCPS for Engineering Living Cells,CNS,"Special Projects - CNS, CPS-Cyber-Physical Systems",5/1/15,5/21/18,Ron Weiss,MA,Massachusetts Institute of Technology,Continuing grant,Ralph Wachter,4/30/19,"$1,200,000.00 ",,rweiss@mit.edu,77 MASSACHUSETTS AVE,Cambridge,MA,21394301,6172531000,CSE,"1714, 7918","7918, 8236",$0.00 ,"Recent developments in nanotechnology and synthetic biology have enabled a new direction in biological engineering: synthesis of collective behaviors and spatio-temporal patterns in multi-cellular bacterial and mammalian systems. This will have a dramatic impact in such areas as amorphous computing, nano-fabrication, and, in particular, tissue engineering, where patterns can be used to differentiate stem cells into tissues and organs. While recent technologies such as tissue- and organoid on-a-chip have the potential to produce a paradigm shift in tissue engineering and drug development, the synthesis of user-specified, emergent behaviors in cell populations is a key step to unlock this potential and remains a challenging, unsolved problem. <br/><br/>This project brings together synthetic biology and micron-scale mobile robotics to define the basis of a next-generation cyber-physical system (CPS) called biological CPS (bioCPS). Synthetic gene circuits for decision making and local communication among the cells are automatically synthesized using a Bio-Design Automation (BDA) workflow. A Robot Assistant for Communication, Sensing, and Control in Cellular Networks (RA), which is designed and built as part of this project, is used to generate desired patterns in networks of engineered cells. In RA, the engineered cells interact with a set of micro-robots that implement control, sensing, and long-range communication strategies needed to achieve the desired global behavior. The micro-robots include both living and non-living matter (engineered cells attached to inorganic substrates that can be controlled using externally applied fields). This technology is applied to test the formation of various patterns in living cells. <br/><br/>The project has a rich education and outreach plan, which includes nationwide activities for CPS education of high-school students, lab tours and competitions for high-school and undergraduate students, workshops, seminars, and courses for graduate students, as well as specific initiatives for under-represented groups. Central to the project is the development of theory and computational tools that will significantly advance that state of the art in CPS at large. A novel, formal methods approach is proposed for synthesis of emergent, global behaviors in large collections of locally interacting agents. In particular, a new logic whose formulas can be efficiently learned from quad-tree representations of partitioned images is developed. The quantitative semantics of the logic maps the synthesis of local control and communication protocols to an optimization problem. The project contributes to the nascent area of temporal logic inference by developing a machine learning method to learn temporal logic classifiers from large amounts of data. Novel abstraction and verification techniques for stochastic dynamical systems are defined and used to verify the correctness of the gene circuits in the BDA workflow."
384,1446607,CPS: Frontier: Collaborative Research: BioCPS for Engineering Living Cells,CNS,CPS-Cyber-Physical Systems,5/1/15,8/31/17,Calin Belta,MA,Trustees of Boston University,Continuing Grant,Ralph Wachter,4/30/21,"$1,882,852.00 ",Douglas Densmore,cbelta@bu.edu,881 COMMONWEALTH AVE,BOSTON,MA,22151300,6173534365,CSE,7918,"7918, 8236",$0.00 ,"Recent developments in nanotechnology and synthetic biology have enabled a new direction in biological engineering: synthesis of collective behaviors and spatio-temporal patterns in multi-cellular bacterial and mammalian systems. This will have a dramatic impact in such areas as amorphous computing, nano-fabrication, and, in particular, tissue engineering, where patterns can be used to differentiate stem cells into tissues and organs. While recent technologies such as tissue- and organoid on-a-chip have the potential to produce a paradigm shift in tissue engineering and drug development, the synthesis of user-specified, emergent behaviors in cell populations is a key step to unlock this potential and remains a challenging, unsolved problem. <br/><br/>This project brings together synthetic biology and micron-scale mobile robotics to define the basis of a next-generation cyber-physical system (CPS) called biological CPS (bioCPS). Synthetic gene circuits for decision making and local communication among the cells are automatically synthesized using a Bio-Design Automation (BDA) workflow. A Robot Assistant for Communication, Sensing, and Control in Cellular Networks (RA), which is designed and built as part of this project, is used to generate desired patterns in networks of engineered cells. In RA, the engineered cells interact with a set of micro-robots that implement control, sensing, and long-range communication strategies needed to achieve the desired global behavior. The micro-robots include both living and non-living matter (engineered cells attached to inorganic substrates that can be controlled using externally applied fields). This technology is applied to test the formation of various patterns in living cells. <br/><br/>The project has a rich education and outreach plan, which includes nationwide activities for CPS education of high-school students, lab tours and competitions for high-school and undergraduate students, workshops, seminars, and courses for graduate students, as well as specific initiatives for under-represented groups. Central to the project is the development of theory and computational tools that will significantly advance that state of the art in CPS at large. A novel, formal methods approach is proposed for synthesis of emergent, global behaviors in large collections of locally interacting agents. In particular, a new logic whose formulas can be efficiently learned from quad-tree representations of partitioned images is developed. The quantitative semantics of the logic maps the synthesis of local control and communication protocols to an optimization problem. The project contributes to the nascent area of temporal logic inference by developing a machine learning method to learn temporal logic classifiers from large amounts of data. Novel abstraction and verification techniques for stochastic dynamical systems are defined and used to verify the correctness of the gene circuits in the BDA workflow."
385,1514126,"III: Medium: Collaborative Research: Computational Tools for Extracting Individual, Dyadic, and Network Behavior from Remotely Sensed Data",IIS,Info Integration & Informatics,9/1/15,8/25/15,Brian Ziebart,IL,University of Illinois at Chicago,Standard Grant,Maria Zemankova,8/31/19,"$554,348.00 ",Tanya Berger-Wolf,bziebart@uic.edu,809 S. Marshfield Avenue,Chicago,IL,606124305,3129962862,CSE,7364,"7364, 7924",$0.00 ,"Recent technological advances in location tracking, video and photo capture, accelerometers, and other mobile sensors provide massive amounts of low-level data on the behavior of animals and humans.  Analysis of this data can teach us much about individual and group behavior, but analytical techniques that lead to insight about that behavior are still in their infancy.  In particular, these new data can provide an unprecedented window into the lives of wild animals, augmenting the traditional time-consuming first-hand observations from field biologists.  Unfortunately, the interpretation of low-level (i.e., unprocessed) data from animal-borne electronic sensors still poses a significant bottleneck in leveraging all of the available data to better understand the individual, pairwise, and group behavior of animal populations.  This project will develop tools for scaling the expert knowledge needed to interpret high-level behaviors from low-level sensor data using tools from statistical machine learning and network analysis.  These data and analytical tools promise to fundamentally change our understanding why animals do what they do, at high resolution and across multiple scales, from individuals to entire populations.  The results of the project will be applicable in many settings where massive sensor data is overwhelming traditional insight derived from observational approaches.  As part of the project, unique data on primate behavior that will bridge the low-level data and expert knowledge will be collected at Mpala Research Centre, Kenya. Undergraduate, graduate, and postdoctoral students from computer science and animal behavior will collaborate across continental and disciplinary boundaries. <br/><br/>The technical aims of this project include developing structured prediction methods that improve behavior recognition at multiple levels (individual, pair-wise, and group), using network properties to improve the identification of group activities, and advancing active learning in the structured prediction setting so that ""expensive"" expert knowledge and supplemental data collection will be judiciously utilized for maximum benefit in learning behavior recognition models.  Recognizing animal behavior from low-level sensor data is hierarchical in this approach, with individual activities recognized directly from data and the context of these data, the inferred individual activities informing pair-wise behavior recognition, and inferred pair-wise behavior informing group-level activity recognition. The benefits of improving the accuracy of individual and pair-wise behavior for recognizing group-level behavior will enable expert annotations to be requested that improve behavior recognition the most across all levels.  These advances will enable field-biologists to investigate new hypotheses about fundamental evolutionary, ecological, and population processes at scale without the burdens of complete manual annotation of collected data.  The methods will be applicable beyond field biology to understanding the hierarchy of behavior from individual entities to groups, from humans to cells, in scientific, educational, and business contexts. The team will leverage the interdisciplinary and international nature of the project to continue its ongoing work to increase participation of women and minorities in STEM research at undergraduate and graduate levels."
386,1527636,CIF: Small: Collaborative Research: Ordinal Data Compression,CCF,Comm & Information Foundations,9/1/15,8/11/15,Olgica Milenkovic,IL,University of Illinois at Urbana-Champaign,Standard Grant,Phillip Regalia,8/31/19,"$250,000.00 ",,milenkov@uiuc.edu,1901 South First Street,Champaign,IL,618207406,2173332187,CSE,7797,"7923, 7935",$0.00 ,"With the emergence of Big Data platforms in social and life sciences, it is becoming of paramount importance to develop efficient lossless and lossy data compression methods catering to the need of such information systems. Although many near-optimal compression methods exist for classical text, image and video data, they tend to perform poorly on data which naturally appears in fragmented or ordered form. This is especially the case for so called ordinal data, arising in crowd-voting, recommender systems, and genome rearrangement studies. There, information is represented with respect to a ?relative,? rather than ?absolute? scale, and the particular constraints of the ordering cannot be properly captured via simple dictionary constructions. This project seeks to improve the operational performance of a number of data management, cloud computing and communication systems by developing theoretical, algorithmic and software solutions for ordinal data compaction.<br/><br/>The main goal of the project is to develop the first general and comprehensive theoretical framework for ordinal compression. In particular, the investigators propose to investigate new distortion measures for ordinal data and rate-distortion functions for lossy ordinal compression; rank aggregation and learning methods for probabilistic ordinal models, used for ordinal clustering and quantization; and smooth compression and compressive computing in the ordinal domain. The proposed analytical framework will also allow for addressing algorithmic challenges arising in the context of compressing complete, partial and weak rankings. The accompanying software solutions are expected to find broad applications in areas as diverse as theoretical computer science (sorting, searching and selection), machine learning (clustering and learning to rank), and gene prioritization and phylogeny (reconstruction of lists of influential genes and ancestral genomes, respectively)."
387,1534798,SBIR Phase II:  Translational Information Management for Industry,IIP,SMALL BUSINESS PHASE II,9/15/15,8/9/17,Bruce Buchanan,TX,i2k Connect LLC,Standard Grant,Peter Atherton,2/28/18,"$898,990.00 ",,buchanan@cs.pitt.edu,10419 Ten Point Lane,Missouri City,TX,774592996,7134137880,ENG,5373,"116E, 169E, 5373, 8032, 8240, 9231, 9251",$0.00 ,"The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project will be more effective and efficient use of unstructured data.  Limiting analysis to structured data ignores the massive amount of information in reports, memos, articles, and other written documents.  Workers require information on past work and ongoing projects, best practices, current events and competitor and customer activities.  However, companies with thousands of workers have millions of documents.  It is prohibitively expensive to index them manually so that they can be found, analyzed and acted on.  Moreover, overloaded workers do not have the time or training required to take on the task.  The problem is exacerbated by mergers and acquisitions.  In addition, over years, it is common for companies to accumulate large numbers of duplicate and out-of-date documents that workers do not take the time to rationalize and delete.  The result is inflated storage costs and reduced productivity as workers struggle to find the relevant, up-to-date information.  Inconsistent information governance also puts organizations at risk - litigation (retaining documents without legal or business value), safety (using out-of-date process safety management procedures) and operational (not leveraging best practices and lessons learned across the enterprise and beyond).<br/><br/>This Small Business Innovation Research (SBIR) Phase II project addresses the problem that text documents - especially those internal to an organization - are very difficult to locate and analyze unless they are classified and tagged.  But manual classification and tagging are too expensive and inconsistent for large collections.  Large companies store many millions of documents.  And there is even more relevant information on the Web.  The objective of the proposed research to is to provide software assistants that classify documents into pre-specified categories, add tags to describe what each document is about, and the entities named in the documents (e.g., oilfields).  The assistants identify relevant documents and help people to learn of new developments by sending alerts when new documents of interest appear on the web or in the company's computers.  The primary technical result will be a suite of software assistants that companies can adopt singly or as an ensemble to help manage information sustainably.  These assistants build upon the proposed research to develop and integrate novel approaches to unsupervised machine learning, concept identification, and ontology construction.  They will enable companies to overcome major problems, including overload, finding relevant, up-to-date information, analyzing unstructured information, and identifying unneeded documents for elimination."
388,1526763,CIF: Small: Collaborative Research: Ordinal Data Compression,CCF,COMM & INFORMATION FOUNDATIONS,9/1/15,8/11/15,Arya Mazumdar,MN,University of Minnesota-Twin Cities,Standard Grant,Richard Brown,6/30/16,"$249,973.00 ",,arya@cs.umass.edu,200 OAK ST SE,Minneapolis,MN,554552070,6126245599,CSE,7797,"7923, 7935",$0.00 ,"With the emergence of Big Data platforms in social and life sciences, it is becoming of paramount importance to develop efficient lossless and lossy data compression methods catering to the need of such information systems. Although many near-optimal compression methods exist for classical text, image and video data, they tend to perform poorly on data which naturally appears in fragmented or ordered form. This is especially the case for so called ordinal data, arising in crowd-voting, recommender systems, and genome rearrangement studies. There, information is represented with respect to a ?relative,? rather than ?absolute? scale, and the particular constraints of the ordering cannot be properly captured via simple dictionary constructions. This project seeks to improve the operational performance of a number of data management, cloud computing and communication systems by developing theoretical, algorithmic and software solutions for ordinal data compaction.<br/><br/>The main goal of the project is to develop the first general and comprehensive theoretical framework for ordinal compression. In particular, the investigators propose to investigate new distortion measures for ordinal data and rate-distortion functions for lossy ordinal compression; rank aggregation and learning methods for probabilistic ordinal models, used for ordinal clustering and quantization; and smooth compression and compressive computing in the ordinal domain. The proposed analytical framework will also allow for addressing algorithmic challenges arising in the context of compressing complete, partial and weak rankings. The accompanying software solutions are expected to find broad applications in areas as diverse as theoretical computer science (sorting, searching and selection), machine learning (clustering and learning to rank), and gene prioritization and phylogeny (reconstruction of lists of influential genes and ancestral genomes, respectively)."
389,1547092,I-Corps L:  TheDesignExchange- Nuturing a National Design Innovation Ecosystem,DUE,I-Corps,8/1/15,7/22/15,Alice Agogino,CA,University of California-Berkeley,Standard Grant,Karen Crosby,7/31/16,"$50,000.00 ",,agogino@berkeley.edu,Sponsored Projects Office,BERKELEY,CA,947101749,5106433891,EHR,8023,007Z,$0.00 ,"Through the NSF Innovations Corps for Learning Program (I-Corps L), this project will transform TheDesignExchange into a high impact sustainable enterprise, nurturing a national design innovation ecosystem. Human-centered design is emerging as an important approach to innovation and product development. Although there are many programs to introduce the principles of human-centered design to interested individuals, progressing from these programs to becoming an expert is poorly supported. This is especially true for those that focus on the research portion of the human-centered process, those that design and conduct research studies with potential customers and users that inform innovation and product development efforts. TheDesignExchange aims to support individuals as they progress through the levels of design mastery, while encouraging the development of best practices for conducting design research and applying human-centered design methods.  TheDesignExchange  will match design methods to design problems in order to help designers and design researchers to make informed decisions about when and how to apply methods, and facilitates discussions between practitioners in an exchange of professional support. Another goal is to leverage knowledge gained from theDesignExchange to identify and teach the underlying skills associated with the various methods, providing recent graduates a more fluid understanding of how, why, and when to apply specific design research methods. TheDesignExchange has the potential to produce broad impacts as a source of information on innovation and the design process for the design community, and for those interested in joining these fields, while acting as an online meeting place for community discussions. <br/><br/>TheDesignExchange team has created a taxonomy of design methods collected from an in-depth analysis of 82 design process models, and the collection of over 300 design methods. Ongoing work falls into four categories: (1) refinement of the taxonomy through a series of workshops held with design practitioners; (2) collection of relevant design process case studies analyzed for the use of design methods; and (3) development of a design method recommendation system, using machine learning algorithms; (4) development of an interactive and intelligent web portal. By analyzing whether methods frequently co-occur with one another or not, it was found that predictions based on method covariance have higher precision-recall performance than predictions based on problem content; and by using spectral clustering on the method covariance data, methods can be automatically divided into expert-given groupings with 92% accuracy."
390,1533753,XPS: FULL: DSD: Scalable High Performance with Halide and Simit Domain Specific Languages,CCF,Exploiting Parallel&Scalabilty,8/1/15,7/20/15,Saman Amarasinghe,MA,Massachusetts Institute of Technology,Standard Grant,Anindya Banerjee,7/31/20,"$845,000.00 ","Wojciech Matusik, Armando Solar-Lezama, Fredo Durand",saman@mit.edu,77 MASSACHUSETTS AVE,Cambridge,MA,21394301,6172531000,CSE,8283,,$0.00 ,"Title: XPS: FULL: DSD: Scalable High Performance with Halide and Simit Domain Specific Languages<br/><br/>Today, getting scalable parallel performance requires heroic programming by experts. In this proposal we are developing a methodology based on Domain Specific Languages (DSLs) to simplify the programmer effort required to harness the power of scalable parallelism. The intellectual merits of this proposal are to show that DSLs can provide a way for programmers to take advantage of scalable performance without a heroic effort. Having a simple path for scalable parallel performance will have a broader significance and importance on areas such as climate modeling and other simulations of large-scale science, by enabling them to efficiently utilize large-scale machines and the cloud.<br/><br/>This proposal aims to radically simplify high performance DSL construction. First, it will introduce a unified transformation framework where complex program transformations are described by example. Using synthesis technology, combinations of localized rewriting rules will be extracted and applied, simplifying the implementation while providing correctness guarantees. Second, it will build a unified parallel low-level intermediate representation by extending LLVM. With the new parallel backend, DSLs only have to expose algorithmic parallelism and the backend will do all architecture-specific mapping of parallelism to vector, non-uniform memory access (NUMA), graphics processing unit (GPU) and distributed parallel components. Third, it will develop a unified auto-tuning framework. Effectiveness of frontend transformations depends on the ability of backends to exploit them. The unified auto-tuning framework will completely eliminate this complexity by offloading transformation selection to the auto-tuner which will use sophisticated machine learning techniques to empirically select transformations that yield the best scalable parallel performance.  The ideas introduced will be demonstrated through two important domain-specific languages ? the Halide DSL for image processing pipelines and the Simit DSL for physical simulations."
391,1545089,CPS: Synergy: Collaborative Research: Adaptive Intelligence for Cyber-Physical Automotive Active Safety - System Design and Evaluation,CNS,CYBER-PHYSICAL SYSTEMS (CPS),9/15/15,9/16/15,Laurent Itti,CA,University of Southern California,Standard Grant,Ralph Wachter,8/31/18,"$240,000.00 ",,itti@pollux.usc.edu,University Park,Los Angeles,CA,900890001,2137407762,CSE,7918,"7918, 8235",$0.00 ,"The automotive industry finds itself at a cross-roads. Current advances in MEMS sensor technology, the emergence of embedded control software, the rapid progress in computer technology, digital image processing, machine learning and control algorithms, along with an ever increasing investment in vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) technologies, are about to revolutionize the way we use vehicles and commute in everyday life. Automotive active safety systems, in particular, have been used with enormous success in the past 50 years and have helped keep traffic accidents in check. Still, more than 30,000 deaths and 2,000,000 injuries occur each year in the US alone, and many more worldwide. The impact of traffic accidents on the economy is estimated to be as high as $300B/yr in the US alone. Further improvement in terms of driving safety (and comfort) necessitates that the next generation of active safety systems are more proactive (as opposed to reactive) and can comprehend and interpret driver intent. Future active safety systems will have to account for the diversity of drivers' skills, the behavior of drivers in traffic, and the overall traffic conditions.<br/><br/>This research aims at improving the current capabilities of automotive active safety control systems (ASCS) by taking into account the interactions between the driver, the vehicle, the ASCS and the environment. Beyond solving a fundamental problem in automotive industry, this research will have ramifications in other cyber-physical domains, where humans manually control vehicles or equipment including: flying, operation of heavy machinery, mining, tele-robotics, and robotic medicine. Making autonomous/automated systems that feel and behave ""naturally"" to human operators is not always easy. As these systems and machines participate more in everyday interactions with humans, the need to make them operate in a predictable manner is more urgent than ever.<br/><br/>To achieve the goals of the proposed research, this project will use the estimation of the driver's cognitive state to adapt the ASCS accordingly, in order to achieve a seamless operation with the driver. Specifically, new methodologies will be developed to infer long-term and short-term behavior of drivers via the use of Bayesian networks and neuromorphic algorithms to estimate the driver's skills and current state of attention from eye movement data, together with dynamic motion cues obtained from steering and pedal inputs. This information will be injected into the ASCS operation in order to enhance its performance by taking advantage of recent results from the theory of adaptive and real-time, model-predictive optimal control. The correct level of autonomy and workload distribution between the driver and ASCS will ensure that no conflicts arise between the driver and the control system, and the safety and passenger comfort are not compromised. A comprehensive plan will be used to test and validate the developed theory by collecting measurements from several human subjects while operating a virtual reality-driving simulator."
392,1453171,CAREER: Query Compilation Techniques for Complex Analytics on Enterprise Clusters,IIS,Info Integration & Informatics,6/1/15,7/9/19,Tim Kraska,RI,Brown University,Continuing Grant,Wei Ding,5/31/21,"$550,000.00 ",,kraska@mit.edu,BOX 1929,Providence,RI,29129002,4018632777,CSE,7364,"1045, 7364, 9150",$0.00 ,"Big data and the evolving field of Data Science are fundamentally shifting the meaning of analytics. Highly complex computations have come to define the typical workload, with jobs ranging from machine learning to large-scale visualization. However, there is a fundamental discrepancy between the availability of analytical tools for big Internet companies and those for non-tech enterprises. Current analytics frameworks, like Spark or Hadoop, are designed to meet the needs of giant Internet companies; that is, they are built to process petabytes of data in cloud deployments consisting of thousands of cheap commodity machines. Yet non-tech companies like banks and retailers - or even the typical data scientist - seldom operate deployments of that size, instead preferring smaller clusters, aka Enterprise clusters, with more reliable hardware. In fact, recent industry surveys reported that the median Hadoop cluster was fewer than 10 nodes, and over 65% of users operate clusters smaller than 50 nodes. Targeting complex analytics workloads on smaller clusters, however, fundamentally changes the way we should design analytics tools. Most current systems focus on the major challenges associated with large cloud deployments, where network and disk I/O are the primary bottleneck and failures are common, where the next generation of analytics frameworks should optimize specifically for the computation bottleneck. As part of this project, the PIs will systematically design a new analytical open-source engine, called Tupleware, build specifically for the infrastructure of non-tech companies. Tupleware will make complex analytics more accessible and push the boundaries of what computations are possible.<br/><br/>Specifically, the PIs will design, implement and evaluate various program synthesis, i.e., query compilation techniques, for complex analytics on enterprise clusters with fast interconnects and considerable available memory. Existing query compilation techniques focus on SQL and are not designed for workloads where UDFs and iterations dominate the computation, nor do they target distributed setups; all issues the PIs will address in this proposal. Furthermore, the PIs aim to combine high-level query optimization with compiler technology to holistically optimize complex analytical workflows by considering statistics about the data (e.g., the selectivity of predicates) with low-level statistics about the UDFs (e.g., the number of used registers). Finally, all the results will be integrated into the Tupleware system and thus, made accessible for a broader range of users.<br/><br/>For further information see the project web site at: http://tupleware.cs.brown.edu/"
393,1544814,CPS: Synergy: Collaborative Research: Adaptive Intelligence for Cyber-Physical Automotive Active Safety - System Design and Evaluation,CNS,"Special Projects - CNS, CPS-Cyber-Physical Systems",9/15/15,5/22/20,Panagiotis Tsiotras,GA,Georgia Tech Research Corporation,Standard Grant,Ralph Wachter,3/31/21,"$576,001.00 ",Karen Feigh,p.tsiotras@ae.gatech.edu,Office of Sponsored Programs,Atlanta,GA,303320420,4048944819,CSE,"1714, 7918","1714, 7918, 8235, 9251",$0.00 ,"The automotive industry finds itself at a cross-roads. Current advances in MEMS sensor technology, the emergence of embedded control software, the rapid progress in computer technology, digital image processing, machine learning and control algorithms, along with an ever increasing investment in vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) technologies, are about to revolutionize the way we use vehicles and commute in everyday life. Automotive active safety systems, in particular, have been used with enormous success in the past 50 years and have helped keep traffic accidents in check. Still, more than 30,000 deaths and 2,000,000 injuries occur each year in the US alone, and many more worldwide. The impact of traffic accidents on the economy is estimated to be as high as $300B/yr in the US alone. Further improvement in terms of driving safety (and comfort) necessitates that the next generation of active safety systems are more proactive (as opposed to reactive) and can comprehend and interpret driver intent. Future active safety systems will have to account for the diversity of drivers' skills, the behavior of drivers in traffic, and the overall traffic conditions.<br/><br/>This research aims at improving the current capabilities of automotive active safety control systems (ASCS) by taking into account the interactions between the driver, the vehicle, the ASCS and the environment. Beyond solving a fundamental problem in automotive industry, this research will have ramifications in other cyber-physical domains, where humans manually control vehicles or equipment including: flying, operation of heavy machinery, mining, tele-robotics, and robotic medicine. Making autonomous/automated systems that feel and behave ""naturally"" to human operators is not always easy. As these systems and machines participate more in everyday interactions with humans, the need to make them operate in a predictable manner is more urgent than ever.<br/><br/>To achieve the goals of the proposed research, this project will use the estimation of the driver's cognitive state to adapt the ASCS accordingly, in order to achieve a seamless operation with the driver. Specifically, new methodologies will be developed to infer long-term and short-term behavior of drivers via the use of Bayesian networks and neuromorphic algorithms to estimate the driver's skills and current state of attention from eye movement data, together with dynamic motion cues obtained from steering and pedal inputs. This information will be injected into the ASCS operation in order to enhance its performance by taking advantage of recent results from the theory of adaptive and real-time, model-predictive optimal control. The correct level of autonomy and workload distribution between the driver and ASCS will ensure that no conflicts arise between the driver and the control system, and the safety and passenger comfort are not compromised. A comprehensive plan will be used to test and validate the developed theory by collecting measurements from several human subjects while operating a virtual reality-driving simulator."
394,1539975,I/UCRC Phase I:  VT Site Addition to the Center for UAS,CNS,"IUCRC-Indust-Univ Coop Res Ctr, , , , , , ",9/15/15,9/13/16,Craig Woolsey,VA,Virginia Polytechnic Institute and State University,Continuing Grant,Behrooz Shirazi,8/31/20,"$262,285.00 ",Kevin Kochersberger,cwoolsey@vt.edu,Sponsored Programs 0170,BLACKSBURG,VA,240610001,5402315281,CSE,"5761, P309, P310, P311, P404, p277, p278","170E, 5761, 8039, 8237, 9251",$0.00 ,"The project will add an I/UCRC site at Virginia Tech, as a new member of Center for Unmanned Aircraft Systems (C-UAS). C-UAS is currently the only NSF-sponsored unmanned aircraft research center. It was established in 2012, with BYU as the lead institution and UC-Boulder as a member. VT faculty will share responsibility for managing research projects with BYU and CU. The participating members of the VT site will also collaborate with members of the current sites to ensure technical success and to foster multi-corporate relationships among industry partners.  The Site will focus on the following application areas:  agriculture (e.g., methods to monitor plant pathogen transport), civil infrastructure (e.g., bridge inspection), and geographic information systems. It will make an impact on society by developing: (1) pre-competitive research products and intellectual property that will advance UAS technology for commerce and security, (2) curriculum enhancements to support the special technical training needs of students pursuing engineering careers in the UAS domain, and (3) a diverse, strongly connected human network of forward-thinking, technically excellent professionals who will invent the future for the UAS commercial sector.<br/><br/>Virginia Tech faculty will make contributions in the broad areas of (1) advanced flight control (e.g., methods to construct robust, secure, and mathematically certified control algorithms), (2) airworthiness, cybersecurity, and reliability (e.g., reliability prediction methods that incorporate imprecise uncertainties intrinsic to small UAS); (3) machine vision and machine learning (e.g., software for real-time, on-board intelligent image processing), (4) multiphysics design optimization (e.g., aircraft design optimization tools that incorporate unsteady fluid/structure interaction), and , and (5) wireless communication (e.g., the use of software defined radio and intelligent communication protocols that adapt the information flow to the environmental conditions)."
395,1505989,Using the Higgs Boson to Probe and Test the Standard Model at ATLAS,PHY,HEP-High Energy Physics,7/15/15,5/31/17,Jahred Adelman,IL,Northern Illinois University,Continuing Grant,Saul Gonzalez,6/30/19,"$419,994.00 ",,jahred.adelman@niu.edu,301 Lowden Hall,De Kalb,IL,601152828,8157531581,MPS,1221,7483,$0.00 ,"This award will provide support for a group with one PI, a postdoc and two graduate students to work on the ATLAS experiment at the Large Hadron Collider (LHC) at CERN, a particle physics laboratory in Geneva, Switzerland.  The LHC is a large, complex machine that accelerates protons to unprecedented energies, allowing for discovery of elementary particles more massive than any yet observed.  One of the LHC's primary objectives was to find the Higgs Boson, the last particle in the historically successful ""Standard Model"" (SM) that accounts for much of the interactions of particles forming the visible matter in the universe.  Though doubts linger, many physicists believe the new particle is the Higgs Boson long predicted by the SM.  Nevertheless, more careful measurements of its rarer decay modes need to be made.  Surprises are possible, and could result from effects beyond the SM that are yet to be discovered.  This project will focus on efforts to do this by looking at events that pair a Higgs boson with another particle.  Searches for extremely rare processes as will be sought in this effort require the use of sophisticated software and computing tools, modern machine learning techniques, data mining and the use of complex statistical tool kits. These skills are useful not only in particle physics and other STEM fields, but also outside of academia and the research world as companies, non-profits and governmental organizations across the country look to the large data sets newly available from social media and computing databases to provide better products and more useful services to customers and<br/>citizens.<br/><br/>The proposal supports work to be done in Run2.  This is about to begin at the full energy potential of the LHC of 13 TeV, almost twice that in Run1 and twice the energy ever studied with precision before.  This group will focus on events with a Higgs Boson accompanied by either a t quark, a t-tbar pair or another Higgs.  The PI has expertise in such analyses in Run1 that may allow him some advantage in dealing with different conditions likely to occur in Run2.  The group will be working on the ATLAS fast tracker (FTK) that will allow new triggers that might allow the events of interest to be found more efficiently."
396,1548517,EAGER: Developing a Mathematical Framework to Enable Bi-Directional Interactions of Humans with Smart Engineered Systems Using Relational Elements,CMMI,EFRI Research Projects,9/1/15,9/4/15,Burcin Becerik-Gerber,CA,University of Southern California,Standard Grant,Alexandra Medina-Borja,12/31/17,"$250,000.00 ",Jonathan Gratch,becerik@usc.edu,University Park,Los Angeles,CA,900890001,2137407762,ENG,7633,"030E, 033E, 7916, 9102",$0.00 ,"Buildings consume a staggering 38 percent of our nation's total energy use. Existing automation approaches to address this problem focus either on buildings, allowing them to better sense and respond to the behavior of their occupants, or focus on occupants, seeking to educate them on making more efficient energy choices. In contrast, this EArly-concept Grant for Exploratory Research (EAGER) project considers ways to enhance the interaction between buildings and occupants. The research team hypothesizes that user-building interactions will be most effective when building users establish a relationship of trust with building automation. By developing mathematical models and theory that amplify user capabilities through relational features, users are empowered to improve individual performance as well as building performance, while also improving societal well-being. To do so, the work draws on theories from the behavioral sciences to mathematically model when and how a building should interact with a user and how these interactions should be framed. The results will change the way we perceive and experience today's built environments, leading what could become the creation of unprecedented built environments that are attentive and have an identity. The project will enhance infrastructure for research and education by making the models and data available via a free research license, incorporating research findings into the engineering curriculum, disseminating research findings via publications, and national and international presentations. <br/><br/>The modeling framework for user-virtual human agent interactions is the key contribution to smart engineered systems modeling and design and occurs at the intersection of engineering, the behavioral sciences and computational modeling. If successful, the mathematical framework will be used to design smart buildings that have two-way interactions with people. The research objectives contribute to the ultimate goal of enabling cyber-physical systems to interact and collaborate with humans.  This project integrates experimental data into the mathematical models, testing the inclusion of relational elements embedded in the personification of a building. The models will predict which response is the most suitable for a building-user interaction. This model will also be informed and constrained by existing theoretical work on persuasion. The model will account for various contextual, temporal and personal factors as well as the changes in user response due to continuous interactions with the building. The multiple-step modeling methodology incorporates a combination of machine learning techniques, mathematical projections for the classification problem, and statistical models such as Markov model, and autoregressive moving average models. In particular, the contributions are twofold: (1) modeling user-building interactions using virtual human agents personifying buildings; and (2) performing fundamental research on how theories of human interpersonal trust and influence can inform the design of automation. The research will contribute to the fundamental understanding of human-machine teamwork, including elucidating theories of why and how people build connections with automated systems and advance our general understanding of how automation exhibiting relational features can facilitate behavior change in the population served by those systems."
397,1464618,Planning Grant: I/UCRC Center for UAS Site Addition (Virginia Tech),IIP,IUCRC-Indust-Univ Coop Res Ctr,3/15/15,3/24/15,Craig Woolsey,VA,Virginia Polytechnic Institute and State University,Standard Grant,Raffaella Montelli,2/29/16,"$10,000.00 ",Kevin Kochersberger,cwoolsey@vt.edu,Sponsored Programs 0170,BLACKSBURG,VA,240610001,5402315281,ENG,5761,"5761, 8039",$0.00 ,"The Center for Unmanned Aircraft Systems (C-UAS), an Industry/University Cooperative Research Center (I/UCRC) established by Brigham Young University and the University of Colorado in 2012, provides innovative solutions to key technical challenges facing the UAS industry and trains a diverse, highly qualified cadre of leaders for this emerging field. In seeking to join the C-UAS, as a third university site, Virginia Tech brings expertise in a variety of critical and complementary research areas. Virginia Tech also leads one of six FAA-designated unmanned aircraft system test sites, which presents an opportunity for C-UAS partners to support the FAA's regulatory policy development efforts while establishing a foothold in emerging commercial markets. This planning grant supports Virginia Tech's effort to develop a viable, self-sustaining Site Addition to the C-UAS.<br/><br/>The specific research agenda to be addressed within this C-UAS Site Addition will ultimately be determined by the Center's Industry Advisory Board (IAB), including new representatives who commit to join the Center along with Virginia Tech. Likely focus areas for the expanded C-UAS research program include advanced flight control (e.g., methods to construct robust, secure, and mathematically certified control algorithms), agriculture (e.g., methods to monitor plant pathogen transport), machine vision and machine learning (e.g., software for real-time, on-board intelligent image processing), multiphysics design optimization (e.g., aircraft design optimization tools that incorporate unsteady fluid/structure interaction), safety and airworthiness (e.g., reliability prediction methods that incorporate imprecise uncertainties intrinsic to small UAS), and wireless communications (e.g., the use of software defined radio and intelligent communication protocols that adapt the information flow to the environmental conditions). Proposals for specific research tasks will be developed in coordination with prospective industry and government partners to ensure that those partners are fully engaged in the process and committed to the research program."
398,1411343,CDS&E: Collaborative Research: Computational Design of Topological Superconductors and Weyl - Dirac Semimetals,DMR,CONDENSED MATTER & MAT THEORY,6/15/15,7/5/17,Ashvin Vishwanath,CA,University of California-Berkeley,Continuing Grant,Daryl Hess,5/31/18,"$300,509.00 ",,avishwanath@g.harvard.edu,Sponsored Projects Office,BERKELEY,CA,947101749,5106433891,MPS,1765,"7433, 8084",$0.00 ,"NONTECHNICAL SUMMARY<br/>The Division of Materials Research and the Division of Advanced Cyberinfrastructure contribute funds to this award. It supports a close interaction of theoretical and computational research to develop novel theoretical and computational methods and tools for calculating and predicting materials properties, and to use them to discover new materials with novel functionalities. The PIs will develop methods that combine a predictive computational method based on density functional theory with methods from the quantum mechanical theories of many interacting particles and methods from computer science. The PIs will focus on the discovery of new states of electrons that are predicted to exist in materials and involve new ways for electrons organize themselves. The organization obeys rules governed by topology, a mathematical theory that focuses on properties of objects that remain unchanged by deformation. While subtle, topological phases are robust being able to survive materials deformations and imperfections. These new topological states include new kinds of insulators, metals and superconductors. The new tools will enable the PIs and the community to predict specific materials with new electronic topological states that may arise in materials such as topological semimetals and superconductors. <br/>This research effort includes developing and disseminating a new software tool, TOP STUDIO, which will enhance and simplify research on material specific studies of new states of matter. Experimentalists, materials scientists and engineers in the US and in other countries will be able to use the user friendly interface of TOP STUDIO to calculate properties of compounds. The software will enable education on topological properties of electrons in solids at advanced undergraduate and graduate levels.  The project will involve and train graduate students and postdocs who will receive a unique interdisciplinary training in computational and theoretical condensed matter physics and materials. Providing well-written objected oriented modern software, using a standardized interface will allow for broader participation of the community in this research area and for educating the next generation.<br/><br/>TECHICAL SUMMARY <br/>The Division of Materials Research and the Division of Advanced Cyberinfrastructure contribute funds to this award. It supports development of new computational methods combining robust electronic structure methods with an advanced many-body theory and machine learning algorithms. The main objective of this project is to develop and implement new methods for the search and discovery of advanced quantum materials with novel magnetic, superconducting and transport characteristics that rely on topologically protected states. The search includes materials that are Weyl-Dirac semimetals and topological superconductors. <br/>The research nurtures the close interaction between theory and computation. The computational approach is based on density functional theory, which is able to predict some properties of many materials including metals and semiconductors, combined with dynamical mean field theory, which includes some effects of strong correlation. To tackle the variety of interactions needed to discover various topological phases in real materials new theoretical methods, powerful algorithms, and computer programs will be developed. Linear response theory will be utilized in order to predict full wave vector and frequency dependent interactions controlling topological superconductivity phenomena. Floquet theory will be used to study topological phases induced by time-dependent fields. The resulting software will contribute to the tools used to search, predict, and discover new materials with topologically protected states of electrons.<br/>The new TOP STUDIO software will be created with a user-friendly interface designed to allow materials exploration by non-experts, by materials scientists and engineers and by theoretical solid-state physicists. TOP STUDIO will promote teaching, training and learning with an educational mode, which can be used to teach students about topological states of quantum matter to students using visualization techniques."
399,1454471,"CAREER: When Energy Harvesting Meets ""Big Data"": Designing Smart Energy Harvesting Wireless Sensor Networks",ECCS,CCSS-Comms Circuits & Sens Sys,3/1/15,1/20/15,Jing Yang,AR,University of Arkansas,Standard Grant,chengshan xiao,9/30/16,"$500,000.00 ",,yangjing@psu.edu,1125 W. Maple Street,Fayetteville,AR,727013124,4795753845,ENG,7564,"1045, 153E, 9102, 9150",$0.00 ,"In order to build a self-sustainable wireless sensor network, powering sensor nodes with energy harvesting devices becomes a natural and feasible solution, thanks to the recent progress on energy harvesting technology. However, wireless sensor networks usually need to collect and transmit vast amounts of data. Utilizing the random, non-uniform, and scarce harvested energy intelligently to meet the energy demand caused by ""big data"" is extremely challenging. The real-time latency requirement for data delivery makes the problem even more complicated. This project overcomes these challenges through strategically allocating the stochastic energy resources to collect and transmit the most important sensor data, providing a reliable pipeline for data collection, transmission and analysis in energy harvesting sensor networks. The project is expected to have direct impact on the design and wide deployment of energy harvesting wireless sensor networks, with applications in radio spectrum management, environment monitoring, healthcare, surveillance, disaster relief, etc. Furthermore, the proposed work has widespread potential applications far beyond energy harvesting wireless sensor networks, such as automatic residential energy consumption scheduling in smart grid, micro-grid planning with renewable energy inputs, information extraction from astronomical amounts of data collected from large-scale participatory sensing, etc. The proposed project integrates a research agenda with a strong educational component and will serve the following educational purposes: 1) the PI will use the proposed project to stimulate and maintain undergraduate students' interests in engineering via undergraduate research, senior project design and project based learning; 2) the requested funding will be used to train graduate students via curriculum development, proper research mentoring and industry collaboration; and 3) the proposed project will be used to attract potential engineering students through various outreach activities. <br/><br/><br/>The goal of this project is to construct a new paradigm of sensing and transmission schemes in data-intensive energy harvesting wireless sensor networks to intelligently utilize the random, non-uniform, and scarce harvested energy with analytically provable sensing, transmission and inference performance guarantees. Two different but closely coupled approaches are proposed to achieve this goal. One is an energy-driven approach and the other is a data-driven approach. For the energy-driven approach, the statistics of the energy harvesting process are exploited to construct online sensing and transmission schemes. Two main tasks addressed in this research thrust are objective-oriented cooperative sensing scheduling policies to cope with the non-uniform energy supply in large-scale sensor networks, and delay-constrained data transmission schemes to meet the real-time latency requirement. The data-driven approach utilizes the characteristics of the underlying sensing phenomena to adaptively and strategically allocate scarce energy resources for the collection and transmission of the most important sensor data. Two specific tasks in this research thrust include a Gaussian process based framework to systematically utilize the spatial-temporal correlations in sensing fields, and adaptive sensing strategies that exploit the structured sparsity of underlying sensing signals, both under the stochastic energy constraints at sensors. The project promises to build intelligent energy harvesting wireless sensor networks with superb sensing, transmission and inference performances on a solid analytical foundation. The delay-constrained information theoretic analysis for energy harvesting communications will integrate a new set of analytical tools from renewal theory with tools in information theory, and create synergies between them. The proposed data-driven adaptive sensing approach will develop synergies between stochastic queueing control and high-dimensional data analysis. The interdisciplinary nature of the research allows us to utilize techniques from stochastic queueing control, renewal theory, information theory, machine learning and is expected to advance the understanding of those areas."
400,1526237,CSR: Small: Collaborative Research: EDS: Systems and Algorithmic Support for Managing Complexity in Sensorized Distributed Systems,CNS,CSR-Computer Systems Research,10/1/15,9/8/15,Yuvraj Agarwal,PA,Carnegie-Mellon University,Standard Grant,Marilyn McClure,9/30/19,"$300,000.00 ",,Yuvraj.Agarwal@cs.cmu.edu,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,CSE,7354,7923,$0.00 ,"Commercial buildings, the energy grid and transportation systems are examples of emerging distributed systems that are beginning to be instrumented with a large number of sensors and actuators for sensing ambient environmental conditions, user occupancy, state of energy use etc. The goal of such instrumentation is to improve safety, utility and reduce costs. This is a hard problem due to interaction of humans, devices and networks in an operating environment with uncertainties regarding veracity, timeliness, meaning and value of sensor data. A large number of sensors must be provisioned, monitored and maintained by system operators. This is currently a manual and error prone task. Deploying, managing and adapting a sensorized system at scale become nearly impossible. In the micro-grid testbed of networked buildings used by this project, there are over a hundred thousand alarms raised per day by the first fifty buildings under observation. In reality, despite thousands of reported sensors there are only a few hundred distinct types of sensors. The key is to reduce the complexity of sensorized distributed systems using automated or semi-automated methods to characterize sensors, determine their type based on the sensor data streams and make inferences about the quality of sensor data with minimal operator effort. This project will apply advances in unsupervised machine learning methods to compose, aggregate and interpret sensory data spatially and over time in order to enable robust derivation of semantically useful sensory information for applications and users resulting in better-utilized and robust systems. <br/><br/>The intellectual merit of the project lies in building an information flow model, with a systematic capture and use of sensor meta-data that enables algorithmic approaches to data composition and building inferences. Using the proposed learning based automation approach along with programming and runtime support, the project will devise a data-to-decision flow for distributed systems operating across timing and reliability constraints. The project outlines smart buildings as an application driver for the envisioned sensorized distributed system with a working real-life testbed. This research will directly contribute to methods for discovery of tele-connections, such as dependence and causal relationships, between various sensory data streams which are crucial for devising effective control of devices connected to these distributed systems.<br/><br/>The broader impacts of the project include advances in the design, deployment, management and programming methodologies for a new class of distributed computing systems that can deal with changing characteristics and topologies of the underlying sensor network. The particular testbed will demonstrate, how such methods can create energy-efficient, sustainable, and comfortable buildings for occupants. A number of educational and outreach activities have been planned to train the next generation talent for the emerging area of a data-driven internet of things. For the broader research community, the project will make available, SensorDepot, an open-source extensible architecture for implementing applications for sensorized distributed systems."
401,1564044,CIF: Small: Collaborative Research: Communicating While Computing: Mobile Fog Computing Over Wireless Heterogeneous Networks,CCF,Comm & Information Foundations,9/1/15,9/17/15,Gesualdo Scutari,IN,Purdue University,Standard Grant,Phillip Regalia,8/31/19,"$224,829.00 ",,gscutari@purdue.edu,Young Hall,West Lafayette,IN,479072114,7654941055,CSE,7797,"7923, 7935",$0.00 ,"An increasing number of applications, including surveillance, medical monitoring, automatic translations and gaming, rely on the capability of mobile wireless devices to carry out computation-intensive tasks in a timely manner. This requirement conflicts with the expectation that mobile devices should run on a battery without needing frequent recharging. A promising solution to this challenge is mobile cloud computing, that is, the offloading of computation-intensive tasks to a cloud service provider by means of wireless transmission. However, the energy and latency entailed by wireless transmission may offset the potential gains of mobile cloud computing. This project proposes to tackle the outlined problem via the development of effective, low-complexity, scalable and flexible offloading strategies that operate over a mobile fog computing architecture, in which small-cell base stations are endowed with computing capabilities to offer proximate wireless access and computing. The insights gained from the successful completion of this project will be beneficial for a gamut of other exciting problem domains that require large-scale optimization, including big data mining, signal processing, machine learning, and smart grid. The research agenda is complemented by a multidisciplinary educational plan that targets both undergraduate and graduate students via hands-on learning and experimentation activities. Industrial collaboration is also envisaged through internship and co-op opportunities. <br/><br/>The inter-layer optimization of the computation and communication resources in a mobile fog computing network yields unstructured nonconvex mixed-integer problems, which are unexplored and challenging, and whose formulation depends on whether the mobile applications are splittable, i.e., divisible into subtasks that can be individually offloaded, or not. Since the problems at hand do not lend themselves to the application of existing iterative optimization techniques, such as Difference-of-Convex programming, a class of scalable and flexible solution methods with controllable convergence, complexity and overhead is introduced based on a novel successive convex approximation framework. In the case of splittable applications, the analytical and algorithmic framework is augmented by the application of message passing strategies that leverage the call graph representation of the mobile applications."
402,1532843,2015 Fifth Annual Mid-Atlantic Regional Mathematics Student Conference,DMS,COMPUTATIONAL MATHEMATICS,4/1/15,8/11/15,Maria Emelianenko,VA,George Mason University,Standard Grant,Leland Jameson,3/31/16,"$19,602.00 ","Lizette Zietsman, Luis Melara",memelian@gmu.edu,4400 UNIVERSITY DR,FAIRFAX,VA,220304422,7039932295,MPS,1271,"7556, 9263",$0.00 ,"This award supports participation in the Fifth Annual Mid-Atlantic Regional Joint Mathematics Conference, held at George Mason University, March 20-21, 2015. The goals of the conference are to (a) bring together students and experts in industry and academia; (b) provide undergraduate and graduate students as well as post-doctoral associates and new doctoral degree holders an opportunity to present their research in a professional conference setting; (c) foster interdisciplinary collaboration among research groups from both academia and industry; (d) encourage networking between students and industry professionals and; (e) increase student awareness of, and interest in, U.S. government and private industry careers.  This conference will strengthen the connection between mathematical advances in industry, government agencies, and academia. Professionals from industry and government agencies, academicians, and students will benefit from the diverse collection of talks, focusing on real world problems as well as recent theoretical advances. The core participating universities for this conference are George Mason University, Virginia Polytechnic Institute, and Shippensburg University.<br/><br/>The conference will focus on recent advances in applied mathematics, including optimal control theory, partial differential equations and the finite element method, computational topology, probability theory and stochastic processes, fluid dynamics, the modeling of materials, and machine learning.  Additionally, the conference will feature talks concerning traditionally pure and applied areas of mathematics with emphasis on computational aspects of problem solving as well as education. The conference will help to prepare mathematical sciences students for careers in business, industry, and government.  Industry participants will benefit from learning about cutting-edge academic advances and from access to highly-trained problem solvers, and students will benefit from exposure to real world problems, thus giving context to their areas of study.  The broader impact of the conference is the increased collaboration between industry, government and academia. By bringing these diverse groups together, the meeting will foster the exchange of ideas on how to tackle new, complex problems. Additionally, the conference provides young academic researchers a chance to network with industry professionals.  <br/><br/>Conference web page:  sites.google.com/site/siammidatlantic"
403,1522231,2015 Gene Golub SIAM Summer School (G2S3): Randomization in Numerical Linear Algebra (RandNLA),DMS,COMPUTATIONAL MATHEMATICS,6/15/15,6/15/15,Ilse C.F. Ipsen,NC,North Carolina State University,Standard Grant,Junping Wang,5/31/16,"$25,000.00 ",,ipsen@ncsu.edu,2701 Sullivan DR STE 240,Raleigh,NC,276950001,9195152444,MPS,1271,"7556, 9263",$0.00 ,"This project supports the participation of about 25 US based Ph.D. students at the Gene Golub SIAM Summer School. The Summer School will take place 15-26 June 2015, at the European Cultural Centre of Delphi, in Greece, and the topic is ""Randomization in Numerical Linear Algebra (RandNLA)"". Randomization, when it comes down to it, amounts to playing dice and taking chances. At first sight it appears to be an unconscionable approach for a scientific investigation. However, randomization goes back to Los Alamos National Laboratory in 1946, where John von Neumann, Stanislaw Ulam and Nicholas Metropolis designed Monte Carlo methods to model the behavior of neutrons for the purpose of radiation shielding. When applied judiciously and with great statistical care, randomized sampling can reduce a data deluge to a manageable volume.<br/><br/>RandNLA, specifically, is a new interdisciplinary area that targets big data applications in data mining, information retrieval, and internet modeling,  as well as more traditional areas like genetics and astronomy. The difficulty is guaranteeing reliability and predictable behaviour in the face of randomization. The development and analysis of RandNLA methods requires innovative tools from applied mathematics, statistics, computer science, and optimization. The summer school will give the students an interdisciplinary education in an area that is too fresh to have produced textbooks. They will learn the relevant theoretical foundations in linear algebra, probability theory, numerical methods, theory of algorithms, and convex optimization; and they will be exposed to applications in data analysis and machine learning. The students will also interactively participate in projects that combine algorithm design, numerical implementations, and empirical evaluations on real data. More information about this summer school can be found at http://scgroup19.ceid.upatras.gr/g2s32015/."
404,1411336,CDS&E: Collaborative Research: Computational Design of Topological Superconductors and Weyl - Dirac Semimetals,DMR,"OFFICE OF MULTIDISCIPLINARY AC, CONDENSED MATTER & MAT THEORY, CDS&E",6/15/15,6/29/17,Sergey Savrasov,CA,University of California-Davis,Continuing grant,Daryl Hess,8/31/18,"$294,000.00 ",,savrasov@physics.ucdavis.edu,OR/Sponsored Programs,Davis,CA,956186134,5307547700,MPS,"1253, 1765, 8084","7433, 8084",$0.00 ,"NONTECHNICAL SUMMARY<br/>The Division of Materials Research and the Division of Advanced Cyberinfrastructure contribute funds to this award. It supports a close interaction of theoretical and computational research to develop novel theoretical and computational methods and tools for calculating and predicting materials properties, and to use them to discover new materials with novel functionalities. The PIs will develop methods that combine a predictive computational method based on density functional theory with methods from the quantum mechanical theories of many interacting particles and methods from computer science. The PIs will focus on the discovery of new states of electrons that are predicted to exist in materials and involve new ways for electrons organize themselves. The organization obeys rules governed by topology, a mathematical theory that focuses on properties of objects that remain unchanged by deformation. While subtle, topological phases are robust being able to survive materials deformations and imperfections. These new topological states include new kinds of insulators, metals and superconductors. The new tools will enable the PIs and the community to predict specific materials with new electronic topological states that may arise in materials such as topological semimetals and superconductors. <br/>This research effort includes developing and disseminating a new software tool, TOP STUDIO, which will enhance and simplify research on material specific studies of new states of matter. Experimentalists, materials scientists and engineers in the US and in other countries will be able to use the user friendly interface of TOP STUDIO to calculate properties of compounds. The software will enable education on topological properties of electrons in solids at advanced undergraduate and graduate levels.  The project will involve and train graduate students and postdocs who will receive a unique interdisciplinary training in computational and theoretical condensed matter physics and materials. Providing well-written objected oriented modern software, using a standardized interface will allow for broader participation of the community in this research area and for educating the next generation.<br/><br/>TECHICAL SUMMARY <br/>The Division of Materials Research and the Division of Advanced Cyberinfrastructure contribute funds to this award. It supports development of new computational methods combining robust electronic structure methods with an advanced many-body theory and machine learning algorithms. The main objective of this project is to develop and implement new methods for the search and discovery of advanced quantum materials with novel magnetic, superconducting and transport characteristics that rely on topologically protected states. The search includes materials that are Weyl-Dirac semimetals and topological superconductors. <br/>The research nurtures the close interaction between theory and computation. The computational approach is based on density functional theory, which is able to predict some properties of many materials including metals and semiconductors, combined with dynamical mean field theory, which includes some effects of strong correlation. To tackle the variety of interactions needed to discover various topological phases in real materials new theoretical methods, powerful algorithms, and computer programs will be developed. Linear response theory will be utilized in order to predict full wave vector and frequency dependent interactions controlling topological superconductivity phenomena. Floquet theory will be used to study topological phases induced by time-dependent fields. The resulting software will contribute to the tools used to search, predict, and discover new materials with topologically protected states of electrons.<br/>The new TOP STUDIO software will be created with a user-friendly interface designed to allow materials exploration by non-experts, by materials scientists and engineers and by theoretical solid-state physicists. TOP STUDIO will promote teaching, training and learning with an educational mode, which can be used to teach students about topological states of quantum matter to students using visualization techniques."
405,1643006,CAREER:The Symbiosis of Graphical Models and Games,IIS,Robust Intelligence,9/1/15,7/31/16,Luis Ortiz,MI,Regents of the University of Michigan - Ann Arbor,Continuing Grant,James Donlon,5/31/18,"$214,341.00 ",,leortiz@umich.edu,3003 South State St. Room 1062,Ann Arbor,MI,481091274,7347636438,CSE,7495,"1045, 1187",$0.00 ,"Many natural, social and engineered systems exhibit or facilitate complex behavior. Such behavior often results from the deliberate actions of, and interactions between, a large number of individuals. The need to study behavior in complex systems of network-structured interactions in large populations promotes interest in computational game-theoretic models.<br/><br/>Graphical games build on classical models in game theory, as well as compact, structured representations in probabilistic graphical models. The result is a practical and computationally amenable model to handle networked large-population systems.<br/><br/>The creation of technology for scientists and policy makers to study and work with large real-world complex systems of (strategic) interactions requires further advances in graphical games and models. The project seeks to fill knowledge gaps by advancing computational aspects of game theory, graphical models and machine learning, and laying the foundation for a systematic two-way knowledge transfer between computational game theory and graphical models.<br/><br/>The research program strengthens the connection between graphical models and game theory by casting probabilistic inference problems as equilibrium computation, creating algorithms to learn games from behavioral data, and characterizing equilibrium structure and computation.<br/><br/>The educational program includes the infusion of research results into general education, at all levels, via development of new courses and integration into existing ones; and a concerted effort to bridge the Departments of Economics and Computer Science at Stony Brook. Collaborations through the Center for Game Theory in Economics and the International Summer Festival on Game Theory, held annually at Stony Brook, serve as conduits for outreach and dissemination."
406,1449828,NRT-DESE:  Graduate Training in Data-Enabled Research into Human Behavior and its Cognitive and Neural Mechanisms,DGE,NSF Research Traineeship (NRT),4/1/15,5/25/18,Mohammed Hoque,NY,University of Rochester,Standard Grant,John Weishampel,3/31/21,"$2,965,341.00 ","Robert Jacobs, Gregory DeAngelis, Mohammed Hoque, Ajay Anand",mehoque@cs.rochester.edu,"518 HYLAN, RC BOX 270140",Rochester,NY,146270140,5852754031,EHR,1997,"7433, 8089, 9179, SMET",$0.00 ,"Understanding the cognitive and neural basis of human behavior is one of the most fundamental areas of scientific inquiry for the 21st Century. It will impact almost every facet of human existence, including commerce, education, health care, and national security, as well as basic science. This National Science Foundation Research Traineeship (NRT) award prepares Ph.D. students at the University of Rochester to advance discoveries at the intersection of computer science, brain and cognitive sciences, and neuroscience. Trainees will be prepared to harness the burgeoning power of data science to make novel inroads into understanding the neural foundations of human behavior.  Trainees will learn to be equally comfortable applying these skills in industrial and academic settings.  By emphasizing both practical applications and basic science, this program will prepare trainees to develop research solutions relevant to today?s societal needs as well as develop research approaches of critical importance to future needs.<br/><br/>Focusing on understanding the nature of intelligence, this program will provide students with skills to blend expertise in data science and computer science with a deep understanding of experimental approaches to collecting and analyzing neural and behavioral data. The program will use theories and methods from data science including machine learning and statistics to provide students with a foundation for theory development, computational modeling, and data analysis. This foundation will serve as a conceptual and methodological framework unifying their studies of artificial and biological intelligence. The hands-on, project-oriented nature of this program will provide students with the capabilities needed to conceptualize, design, and implement large-scale research projects from beginning to end. This traineeship provides a novel model for structuring interdisciplinary education, based on a modular cross-training course followed by an interdisciplinary practicum course, which can be replicated in many fields and universities."
407,1550507,I-Corps:  D-Ready Mobile Guide,IIP,I-Corps,8/1/15,12/21/15,Ethan Bueno de Mesquita,IL,University of Chicago,Standard Grant,Steven Konsek,1/31/17,"$50,000.00 ",Paula Worthington,bdm@uchicago.edu,6054 South Drexel Avenue,Chicago,IL,606372612,7737028669,ENG,8023,,$0.00 ,"This project focuses on techniques for survey analysis that can inform decision making -- potentially in a broad range of areas.<br/><br/><br/>D-Ready will use contemporary machine learning techniques to process and filter the data the team will collate regarding such areas as choice preferences. The proposed technology will be deployed with two goals in mind.  A survey design methodology will be used to help ensure that decision-selection guidelines are unbiased for individual user. This is important because the more unbiased the decision support product, the more widely it will be trusted and used. The team will also use cursor-tracking technology to develop an in-depth statistical picture of how individual users view decision-support site-apps. This will allow for the team to use nonlinear regression and classification methods to segment users into distinct types. The insights learned from these analyses will be valuable to support decision makers in a broad range of applications. Applying the customer discovery process during the I-Corps curriculum, the team intends to come up with a more complete value proposition that it will provide and refine the product features that will be most useful.<br/><br/><br/>D-Ready is envisioned as an online and mobile decision support guide that creates customized information for users that is simple, convenient and unbiased. The proposed technology has potentially broad applications in several areas including marketing, for example, where a three-step strategy of 1) defining an attraction for a certain population, 2) harvesting detailed interactive behavior data while users react to that attraction, and finally 3) analyzing that data for the purpose of market segmentation, represents a powerful new form of survey collection."
408,1527625,CIF: Small: Collaborative Research: Communicating While Computing: Mobile Fog Computing Over Wireless Heterogeneous Networks,CCF,Comm & Information Foundations,9/1/15,8/11/15,Gesualdo Scutari,NY,SUNY at Buffalo,Standard Grant,Phillip Regalia,10/31/15,"$224,829.00 ",,gscutari@purdue.edu,520 Lee Entrance,Buffalo,NY,142282567,7166452634,CSE,7797,"7923, 7935",$0.00 ,"An increasing number of applications, including surveillance, medical monitoring, automatic translations and gaming, rely on the capability of mobile wireless devices to carry out computation-intensive tasks in a timely manner. This requirement conflicts with the expectation that mobile devices should run on a battery without needing frequent recharging. A promising solution to this challenge is mobile cloud computing, that is, the offloading of computation-intensive tasks to a cloud service provider by means of wireless transmission. However, the energy and latency entailed by wireless transmission may offset the potential gains of mobile cloud computing. This project proposes to tackle the outlined problem via the development of effective, low-complexity, scalable and flexible offloading strategies that operate over a mobile fog computing architecture, in which small-cell base stations are endowed with computing capabilities to offer proximate wireless access and computing. The insights gained from the successful completion of this project will be beneficial for a gamut of other exciting problem domains that require large-scale optimization, including big data mining, signal processing, machine learning, and smart grid. The research agenda is complemented by a multidisciplinary educational plan that targets both undergraduate and graduate students via hands-on learning and experimentation activities. Industrial collaboration is also envisaged through internship and co-op opportunities. <br/><br/>The inter-layer optimization of the computation and communication resources in a mobile fog computing network yields unstructured nonconvex mixed-integer problems, which are unexplored and challenging, and whose formulation depends on whether the mobile applications are splittable, i.e., divisible into subtasks that can be individually offloaded, or not. Since the problems at hand do not lend themselves to the application of existing iterative optimization techniques, such as Difference-of-Convex programming, a class of scalable and flexible solution methods with controllable convergence, complexity and overhead is introduced based on a novel successive convex approximation framework. In the case of splittable applications, the analytical and algorithmic framework is augmented by the application of message passing strategies that leverage the call graph representation of the mobile applications."
409,1450848,EAGER: A Graphical Approach for Choice Modeling,CMMI,MANFG ENTERPRISE SYSTEMS,1/1/15,7/19/14,Sewoong Oh,IL,University of Illinois at Urbana-Champaign,Standard Grant,diwakar gupta,12/31/15,"$87,937.00 ",,sewoong@cs.washington.edu,1901 South First Street,Champaign,IL,618207406,2173332187,ENG,1786,"076E, 078E, 7916, 8023",$0.00 ,"A central problem of interest to operations managers is how to use historical sales data to accurately predict revenues when offering a particular assortment of products. Such predictions are subsequently used in making important business decisions such as assortment planning, new product development, and demand estimation. Choice models are widely used in modeling the underlying customer behavior. Traditional choice models are either too simple to accurately reflect the nature of how people make decisions, or so complex that it is either computationally intractable to fit the model to historical data or to subsequently use it to make business decisions. This EArly-concept Grant for Exploratory Research (EAGER) project studies innovative and novel choice models that are designed to be computationally efficient for the decision problems of interest in revenue management and at the same time have strong predictive power. The results of the research will enable improved business decisions to be made while simultaneously reducing operational costs. It will provide key technologies in important business applications of assortment planning, new product development, brand value evaluation, demand estimation, optimal pricing, and revenue management. It will generate collaborations among the disciplines of operations management, economics, cognitive psychology, and machine learning. The research component is tightly integrated with the education plan, including a graduate course on probabilistic graphical models. Both undergraduate and graduate students will benefit from the research activities by engaging in research and applying the knowledge to solve real world problems.<br/><br/>The suboptimal tradeoff of traditional choice models is due to the fact that these models are designed without computational efficiency in mind. In this era of tremendous increase in the scale of data being generated, computational efficiency is of primary concern. This project will build on graph-based probabilistic models such as random walks on graphs and probabilistic graphical models, and will lead to (a) development of new graph-based models for choice modeling designed for computational efficiency; (b) development of new methodologies for learning these models from historical purchase data; (c) development of novel inference algorithms for predicting the customer preferences from these models; and (d) development of new methodologies for solving optimization problems in revenue management with these models. The research will lay foundations of a new graph-based modeling approach for revenue management. The significance and novelty of the work lie in the fact that the design objective of the choice modeling is critically different from the traditional criteria used by economists and cognitive psychologists (such as describing the functional form of the underlying rational decision processes), which does not consider the computational efficiency of solving decision problems in revenue management. In contrast to this, choice models for making decisions based on massive modern datasets should have computational efficiency embedded into the models by design. The theory and models developed in this project will bring together ideas and techniques from probability theory and graph theory to jointly reason about uncertainty and complexity (such as probabilistic graphical models and random walks on graphs) as well as insights and tools from recent advances in revenue management (such as choice modeling using Markov chains). The research has a potential to advance our fundamental understanding in how people make decisions when presented with many options."
410,1453346,CAREER: Using Multiple Gaits and Inherent Dynamics for Legged Robots With Improved Mobility,CMMI,"Dynamics, Control and System D",3/1/15,1/6/15,C. David Remy,MI,Regents of the University of Michigan - Ann Arbor,Standard Grant,Irina Dolinskaya,2/29/20,"$500,000.00 ",,cdremy@umich.edu,3003 South State St. Room 1062,Ann Arbor,MI,481091274,7347636438,ENG,7569,"034E, 035E, 1045, 8024",$0.00 ,"The goal of this Faculty Early Career Development (CAREER) Program project is to investigate legged robots that are faster and energetically more efficient.  The project capitalizes on the currently untapped possibility of using different gaits at different locomotion speeds.  This idea is inspired by nature.  Humans, for example, switch from walking to running as they increase speed; horses transit from walking to trotting and galloping.  Switching gaits is analogous to switching gears in a car.  It increases versatility and reduces energy consumption.  Additionally, in nature the choice of gaits is strongly coupled to an animal's morphology.  A massive elephant moves differently than a filigree gazelle.  This project will investigate this complex relation of gaits, motions, and morphologies, and will transfer the underlying principles to robotic systems.  The work will be conducted in simulation studies and with actual robots.  In the long term, this CAREER plan aims at the development of robots that reach and even exceed the agility of humans and animals.  It will enable us to build robots that can run as fast as a cheetah and as enduring as a husky, while mastering the same terrain as a mountain goat.  Moreover, it will provide us with novel designs for active prosthetics and exoskeletons.  The project will also leverage the fascinating topic of legged locomotion to spark the interest of K-12 students, underrepresented groups, and a broader audience for science, technology, and engineering.<br/><br/>This project seeks to understand the fundamental principles of designing, building, and controlling legged robots that embrace and exploit their inherent mechanical dynamics.  The underlying premise is that locomotion can emerge in great part passively from the interaction of inertia, gravity, and elastic oscillations.  The goal of this project is to identify systems in which such dynamics can be excited in a variety of different modes.  Different modes would correspond to different gaits, and would enable efficient motion in different operational conditions.  The work will extend optimal control and machine-learning techniques, such as multiple shooting, direct collocation, or probabilistic direct policy learning methods, to allow the simultaneous generation of gaits, motions, and morphological parameters (including stiffness values, mass distributions, or actuator sizes).  Since performance criteria such as speed, efficiency, robustness, and agility only fully manifest themselves in actual hardware implementations, the research team will study these concepts not only in simulation, but also with hardware prototypes.  In particular, the methodology will be employed to examine the benefits of flexible spines in quadrupeds and of articulated ankles in bipeds, as well as to compare series elastic actuation and parallel elastic actuation concepts."
411,1527364,TWC: Small: Statistical Models for Opinion Spam Detection Leveraging Linguistic and Behavioral Cues,CNS,Secure &Trustworthy Cyberspace,8/1/15,7/29/15,Arjun Mukherjee,TX,University of Houston,Standard Grant,Shannon Beck,7/31/19,"$499,695.00 ",,amukher6@Central.UH.EDU,4800 Calhoun Boulevard,Houston,TX,772042015,7137435773,CSE,8060,"7434, 7923",$0.00 ,"Online opinions now play a pivotal role in decision making and influence a wide spectrum of our lives. Choices of restaurants at which to dine, places to stay, universities to attend, books to read, doctors to consult, and even political candidates to vote for, are largely influenced by crowdsourced opinions. However, it is estimated that up to 30% of reviews on websites are fake. As a larger part of the US economy is becoming driven by social opinions, it poses a serious risk to the general public (e.g., by getting mislead to invest on low quality products, services or doctors). The Federal Trade Commission Opinion may soon consider online fraud as unlawful and a legal offense. Detecting fake online opinions is an urgent research area. Otherwise, online social media might continue to progress undetected. This project aims to develop novel deception detection algorithms in order to identify fraudulent behavior. It synergistically integrates techniques from computational linguistics, behavioral modeling and statistical machine learning in order to advance knowledge in this area. <br/><br/>The project consists of a four-pronged research effort: 1) novel methods to learn deception classifiers from large-scale noisy crowd data and small-scale domain expert coded data, 2) unsupervised models that treat ""spamicity"" of reviewers as latent with observed behavioral footprints, 3) a relational architecture for jointly modeling reviews, reviewers, and their linguistic and behavioral patterns leveraging inherent reinforcement relations, and 4) an ensemble scoring mechanism blending cues from of all approaches, and an end-to-end validation framework. The techniques developed in the project can (1) reduce the marketing, consumer, and economic risk in e-commerce; (2) improve user profiling, detecting online harassment, bigotry, trolls, and other social media fraud that are of major relevance to national security; and (3) transition techniques developed to courses/tutorials and attract underrepresented students, including minorities and women. The result is a suite of novel, principled, and scalable techniques to filter opinion spam at large scale."
412,1534340,DMREF: Collaborative Research: The Synthesis Genome: Data Mining for Synthesis of New Materials,DMR,"DMR SHORT TERM SUPPORT, DMREF",10/1/15,7/30/18,Elsa Olivetti,MA,Massachusetts Institute of Technology,Standard Grant,John Schlueter,9/30/19,"$792,585.00 ",Gerbrand Ceder,elsao@mit.edu,77 MASSACHUSETTS AVE,Cambridge,MA,21394301,6172531000,MPS,"1712, 8292","024E, 054Z, 8400, 9102",$0.00 ,"NON-TECHNICAL:<br/><br/>Development of new materials is the key to addressing many of the technical challenges our society faces from energy storage to water treatment and purification. To offer just a few examples:  in the oil industry, new materials are needed to withstand aggressive conditions, where failure comes with tremendous cost; electrified vehicle drive trains will be advanced by higher performing battery electrodes; carbon dioxide capture requires inexpensive new materials with the proper thermodynamic and kinetic behavior towards absorption and release. The rapid design of novel materials has been transformed by approaches where properties for many tens of thousands of materials can be predicted or inferred by a computer. The pace of commercially-realized advanced materials seems now to be limited by trial-and-error synthesis techniques. In other words, researchers have accelerated the process of knowing what to make such that the bottleneck is now how to make the structures. This research will learn from existing knowledge to develop insight on the synthesis of inorganic compounds.  The analytical foundation of these activities stems from advances in machine learning that has allowed computers to excel in typically ""human"" tasks such as health care diagnoses and game show participation. This research will further accelerate the goals of efforts such as the Materials Genome Initiative for Global Competitiveness by enabling efficient synthesis of novel materials thereby speeding up evaluation of newly suggested materials.<br/><br/><br/>TECHNICAL:<br/><br/>Materials are a key bottleneck in many technological advances such as efficient catalysis, clean energy generation, and water filtration. Materials Genome Initiative-style efforts have produced several examples of computationally designed materials in the fields of energy storage, catalysis, thermoelectrics, and hydrogen storage, as well as large data resources that can be used to screen for potentially transformative compounds. These successes in accelerated materials design have moved the bottleneck in materials development towards the synthesis of novel compounds, and much of the momentum and efficiency gained in the design process becomes gated by trial-and-error synthesis techniques. This research will do for solid state advanced materials synthesis what modern computational methods are doing for materials properties: Build predictive tools for synthesis so that targeted compounds can be synthesized more rapidly. This work will combine knowledge regarding synthesis, first principles modeling, and data mining to suggest synthesis routes for novel compounds."
413,1461914,"BDD: Efficient and Scalable Collection, Analytics and Processing of Big Data for Disaster Applications",CNS,"Special Projects - CNS, Special Projects - CCF, Big Data Science &Engineering",4/1/15,3/18/20,Sanjay Madria,MO,Missouri University of Science and Technology,Standard Grant,Sylvia Spengler,9/30/20,"$346,632.00 ",,madrias@mst.edu,300 W 12th Street,Rolla,MO,654096506,5733414134,CSE,"1714, 2878, 8083","022Z, 5921, 7363, 8230, 9150, 9251",$0.00 ,"The outcomes from this project is to assist human operators in their disaster management coordination and planning, such as directing a medical physician's team to their nearest cluster of affected people in a region to administer medications as necessary, or finding a safe route for evacuation of affected people. Sensor data integrated with microblogs such as Tweets help identifying some local events and people's sentiments, which are significantly useful in handling/understanding disaster situations. It will also benefit other applications such as real-time tracking of road/driving conditions in vehicular networks.<br/><br/>This research is conducted jointly with Osaka University in Japan, to benefit both the universities in enhancing not only their knowledge but also to learn global perspective in solving important problems. The research team is designing schemes for dynamic and collaborative data compression and multi-streams compression of multi-dimensional sensor data with error correction and recovery for addressing the energy efficiency and bandwidth limitation issues. Compression schemes exploit temporal locality and delta compression to provide better bandwidth utilization. Different methods for measuring error are designed and compared for the compressibility and actual error for variations in methods of utilizing the error tolerance. In addition, the team is developing algorithms for highly scalable indexing schemes for efficient data retrieval involving mainly range queries, top-k query, ranked-based searches and snapshot queries for multi-dimensional sensor data from different data sources to address the issue of timely dissemination. Hilbert Curve based linearization technique integrated with an overlay network is designed to (1) map multidimensional attributes onto a single dimension while preserving its data locality, and (2) to create a balanced network by associating only one node with each leaf of the virtual tree and then partition the multidimensional search space into subspaces and assign each node to a unique subspace. This allows an overlay network to start from a predefined prefix to handle data skewness. This research is also designing a scheme for using microblog messages as social sensors for efficient integration with other sensor data. We are using machine-learning techniques to match each message with its associate location based on the characteristics of the message. The results will be validated and evaluated using the sensor cloud test-bed available at Missouri S&T."
414,1456299,SBIR Phase II:  Cloud-based Acoustic Simulation Service,IIP,SBIR Phase II,3/1/15,2/29/16,Anish Chandak,NC,"Impulsonic, Inc.",Standard Grant,Peter Atherton,8/31/17,"$840,750.00 ",,achandak@impulsonic.com,"605 W. Main St, Suite 105",Carrboro,NC,275101693,9193603095,ENG,5373,"077E, 169E, 5373, 8032, 8039, 8240",$0.00 ,"The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project results from the proposed technology touching many aspects of architectural design. The company's cloud-based acoustic simulation service (CBASS) is aimed at architects and acoustical consultants. There are over 105,000 architects, 25,000 architecture students, and 2,000 acoustical consultants in the US. Acoustic simulation is a $450 million market. CBASS will be used to understand a building's acoustics before it is built, reducing the time and money spent on fixing acoustic issues. CBASS will check for acoustic issues in a design and automatically suggest ways of fixing them, allowing architects to be more certain of their designs, and saving on remodeling costs by getting the acoustics right initially. CBASS will also speed up and simplify the process by which architects and acoustical consultants collaborate, leading to reduced costs and better acoustics. The company will offer low-cost subscriptions to students, to help train the next generation of architects to think about acoustics earlier in the design phase. Increased use of acoustic simulation will lead to better sounding spaces for everyone: classrooms that are more conducive to learning, hospitals that more conducive to healing, more enjoyable theaters, and quieter homes.<br/><br/>This Small Business Innovation Research (SBIR) Phase II project aims to develop new ways of using cloud computing to understand and improve the acoustics of buildings. Reliably modeling the behavior of sound waves is a hard problem. Researchers have recently developed an algorithm called Adaptive Rectangular Decomposition (ARD), which can accurately model acoustics. To make it work on complex, real-world data, the company has developed algorithms for running ARD on multiple computers at the same time, in the cloud. In this project the company will develop new algorithms that use acoustic modeling and machine learning to automatically check for compliance with standard acoustic guidelines, identify acoustic issues, and suggest changes and improvements to a design that are likely to improve its acoustics. When running ARD in the cloud, individual computers must keep sending data to each other, slowing down the overall simulation. The company will develop improved ways of managing this data exchange, allowing much faster and more reliable acoustic simulation. To let teams of architects and acoustical consultants collaborate on a project, the company will also develop ways of allowing the same project data and analyses to be accessed and reviewed by multiple people at the same time."
415,1534431,DMREF: Collaborative Research: The Synthesis Genome: Data Mining for Synthesis of New Materials,DMR,"DMR SHORT TERM SUPPORT, DMREF",10/1/15,8/13/15,Andrew McCallum,MA,University of Massachusetts Amherst,Standard Grant,John Schlueter,6/30/19,"$363,945.00 ",,mccallum@cs.umass.edu,Research Administration Building,Hadley,MA,10359450,4135450698,MPS,"1712, 8292","024E, 8400, 9102",$0.00 ,"NON-TECHNICAL:<br/><br/>Development of new materials is the key to addressing many of the technical challenges our society faces from energy storage to water treatment and purification. To offer just a few examples:  in the oil industry, new materials are needed to withstand aggressive conditions, where failure comes with tremendous cost; electrified vehicle drive trains will be advanced by higher performing battery electrodes; carbon dioxide capture requires inexpensive new materials with the proper thermodynamic and kinetic behavior towards absorption and release. The rapid design of novel materials has been transformed by approaches where properties for many tens of thousands of materials can be predicted or inferred by a computer. The pace of commercially-realized advanced materials seems now to be limited by trial-and-error synthesis techniques. In other words, researchers have accelerated the process of knowing what to make such that the bottleneck is now how to make the structures. This research will learn from existing knowledge to develop insight on the synthesis of inorganic compounds.  The analytical foundation of these activities stems from advances in machine learning that has allowed computers to excel in typically ""human"" tasks such as health care diagnoses and game show participation. This research will further accelerate the goals of efforts such as the Materials Genome Initiative for Global Competitiveness by enabling efficient synthesis of novel materials thereby speeding up evaluation of newly suggested materials.<br/><br/><br/>TECHNICAL:<br/><br/>Materials are a key bottleneck in many technological advances such as efficient catalysis, clean energy generation, and water filtration. Materials Genome Initiative-style efforts have produced several examples of computationally designed materials in the fields of energy storage, catalysis, thermoelectrics, and hydrogen storage, as well as large data resources that can be used to screen for potentially transformative compounds. These successes in accelerated materials design have moved the bottleneck in materials development towards the synthesis of novel compounds, and much of the momentum and efficiency gained in the design process becomes gated by trial-and-error synthesis techniques. This research will do for solid state advanced materials synthesis what modern computational methods are doing for materials properties: Build predictive tools for synthesis so that targeted compounds can be synthesized more rapidly. This work will combine knowledge regarding synthesis, first principles modeling, and data mining to suggest synthesis routes for novel compounds."
416,1514491,III: Medium: 20/20: A System for Human-in-the-Loop Data Exploration,IIS,Info Integration & Informatics,9/1/15,7/26/16,Stanley Zdonik,RI,Brown University,Continuing Grant,Maria Zemankova,8/31/19,"$1,000,000.00 ","Andries van Dam, Ugur Cetintemel, Tim Kraska",sbz@cs.brown.edu,BOX 1929,Providence,RI,29129002,4018632777,CSE,7364,"7364, 7924, 9150",$0.00 ,"Explorative data analysis plays a key role in data-driven discovery in a wide range of domains including science, engineering and business. In order for data analysis to become a commodity during a period when their user base is continually expanding and diversifying, human productivity and ease-of-use must become first-class design considerations for any database system. Unfortunately, data tools that are user friendly and designed to improve human productivity are still sorely lacking. This project will enable users at different skill levels to interact with and explore their large datasets far easier and faster than they do today. Rather than spending a lot of precious time to build complex analytics tasks, this work will offer a more agile, responsive and user-friendly system based on direct manipulation of the visual representations (e.g., charts, graphs, maps) of the data sets and analysis results. The system can also be used as a learning tool: e.g., a teacher could walk students through a complex dataset to verify specific hypothesis. This project will make large-scale data exploration more accessible to more users. Overall, it will accelerate discovery and breakthroughs in many domains such as e-commerce, finance and science. This research will be incorporated in undergraduate and graduate coursework. The outreach activities include special research and education­focused programs that are geared towards undergraduates and high school girls.<br/><br/>This project will build a new class of database systems designed for Human-In-the-Loop (HIL) operation. The work targets an ever-growing set of data-centric applications in which users directly manipulate, analyze and explore large data sets, often using complex analytics and machine learning techniques. Traditional database technologies are ill suited to serve this purpose. Historically, databases assumed (1) text-based input (e.g., SQL) and output, (2) a point (i.e., stateless) query-response paradigm, (3) batch results, and (4) simple analytics. The project team will drop these fundamental assumptions and build a system that instead supports visual input and output, ""conversational"" interaction, early and progressive results, and complex analytics. Building a system that integrates these features requires a complete rethinking of the full data stack, from the visual interface to the ""core"", as well as incorporating pertinent algorithms. The primary research challenges revolve around developing algorithms and optimizations that leverage the unique characteristics of HIL workloads to speed up analysis over large data collections. The team will build a proof-of-concept HIL database called 20/20 that will tightly integrate and significantly extend two existing technologies built at Brown: PanoramicData is a touch and pen data visualization system and will serve as the front-end. The second building block is the Tupleware main-memory analytics system, which compiles complex analytics pipelines into executables. Tupleware will serve as the back­end analytics component. The team expects that the end result will offer a substantial speed­up (50% or more) over the state-of-the-art solutions for common analytics workloads. The project web site (http://database.cs.brown.edu/projects/20-20/) will include information on the project, publications, public datasets and code."
417,1526383,TWC: TTP Option: Small: Automating Attack Strategy Recognition to Enhance Cyber Threat Prediction,CNS,Secure &Trustworthy Cyberspace,10/1/15,4/28/16,Shanchieh Yang,NY,Rochester Institute of Tech,Standard Grant,Kevin Thompson,9/30/19,"$666,960.00 ","Michael Kuhl, Daryl Johnson, Esa Rantanen, Bill Stackpole",sjyeec@rit.edu,1 LOMB MEMORIAL DR,ROCHESTER,NY,146235603,5854757987,CSE,8060,"7434, 7923",$0.00 ,"Network attacks are increasingly complex and fast-evolving.  A single attack may use multiple reconnaissance, exploit, and obfuscation techniques.  This project investigates how to extract critical attack attributes, synthesize novel attack sequences, and reveal potential threats to critical assets in a timely manner.  The project uses machine learning techniques to simultaneously identify new attack types and observed events that could identify those attacks.  The Transition-to-Practice component in the project includes a three-phase plan to provide a positively reinforced and measurable cycle to develop, evaluate, and refine a prototype system in real-world environments. This significantly broadens the engagement of security practitioners and student teams, who will be planning and executing attacks to test the prototype system. The outcome of this research will provide timely comprehension and anticipation of critical attack strategies, offering the practitioners a solution to level the playing field against sophisticated attackers.<br/><br/>Specifically, this work develops an online semi-supervised learning framework to capture both spatial and temporal features of attack strategies. An attack behavior model is a collection of feature probability distributions.  The attack features are used to synthesize attack sequences via Monte-Carlo simulation. The attack sequences along with an ensemble prediction are then used to reveal potential threats to critical assets in the network.  The project will be evaluated on real-world attack data as well as synthetic network attacks.  An extensive outreach plan includes course module development, a mid-project workshop to engage security researchers and practitioners, and a summary panel in an international conference."
418,1525629,CIF: Small: Collaborative Research: Communicating While Computing: Mobile Fog Computing Over Wireless Heterogeneous Networks,CCF,Comm & Information Foundations,9/1/15,8/11/15,Osvaldo Simeone,NJ,New Jersey Institute of Technology,Standard Grant,Phillip Regalia,8/31/19,"$249,848.00 ",,osvaldo.simeone@njit.edu,University Heights,Newark,NJ,71021982,9735965275,CSE,7797,"7923, 7935",$0.00 ,"An increasing number of applications, including surveillance, medical monitoring, automatic translations and gaming, rely on the capability of mobile wireless devices to carry out computation-intensive tasks in a timely manner. This requirement conflicts with the expectation that mobile devices should run on a battery without needing frequent recharging. A promising solution to this challenge is mobile cloud computing, that is, the offloading of computation-intensive tasks to a cloud service provider by means of wireless transmission. However, the energy and latency entailed by wireless transmission may offset the potential gains of mobile cloud computing. This project proposes to tackle the outlined problem via the development of effective, low-complexity, scalable and flexible offloading strategies that operate over a mobile fog computing architecture, in which small-cell base stations are endowed with computing capabilities to offer proximate wireless access and computing. The insights gained from the successful completion of this project will be beneficial for a gamut of other exciting problem domains that require large-scale optimization, including big data mining, signal processing, machine learning, and smart grid. The research agenda is complemented by a multidisciplinary educational plan that targets both undergraduate and graduate students via hands-on learning and experimentation activities. Industrial collaboration is also envisaged through internship and co-op opportunities. <br/><br/>The inter-layer optimization of the computation and communication resources in a mobile fog computing network yields unstructured nonconvex mixed-integer problems, which are unexplored and challenging, and whose formulation depends on whether the mobile applications are splittable, i.e., divisible into subtasks that can be individually offloaded, or not. Since the problems at hand do not lend themselves to the application of existing iterative optimization techniques, such as Difference-of-Convex programming, a class of scalable and flexible solution methods with controllable convergence, complexity and overhead is introduced based on a novel successive convex approximation framework. In the case of splittable applications, the analytical and algorithmic framework is augmented by the application of message passing strategies that leverage the call graph representation of the mobile applications."
419,1540016,Planning Grant: I/UCRC for Center for Dynamic Data Analytics Site at the University of Virginia,ITR,IUCRC-Indust-Univ Coop Res Ctr,9/15/15,9/9/15,Peter Beling,VA,University of Virginia Main Campus,Standard Grant,Thyagarajan Nandagopal,8/31/16,"$14,740.00 ","William Scherer, Abhi Shelat",pb3a@virginia.edu,P.O.  BOX 400195,CHARLOTTESVILLE,VA,229044195,4349244270,CSE,5761,"5761, 8039",$0.00 ,"The University of Virginia (UVA) is planning to join the Center for Dynamic Data Analytics, an existing multi-university Industry/University Cooperative Research Center (I/UCRC) which is currently comprised of Rutgers University (lead institution) and Stony Brook University. This award provides UVA with the support from the NSF for exploring the feasibility of establishing this new site. The planning grant will be used to plan a joint industry and university research agenda, to organize and attend the meetings with potential industry members and the existing sites, and to host a planning meeting that will be the center point of the effort to articulate the vision for the site to potential members. The principal industry domains for UVA/CDDA research are: (1) Finance and banking, including financial markets and electronic trading, systemic risk, and data analytics for regulators; (2) Internet analytics, including marketing and the Internet of Things; (3) Advanced manufacturing, including data-driven approaches to automation, human factors, process improvement, and prognostics; (4) Cyberphysical systems, including cybersecurity of physical systems and autonomous systems. Together these domains constitute a portion of the economy that is both substantial and increasing. <br/><br/>The primary mission of the UVA site of CDDA is to perform research on data analytics to support decision making in industry and government. The focus is on analysis of data sets distinguished by their velocity, variety, volume, complexity, and other features commonly associated with Big Data. The UVA site of CDDA will pursue a research agenda that complements that of the existing sites and features an integration of control, decision, and systems modeling concepts with statistical methods, machine learning and pattern recognition. Areas of concentration include behavioral and preference modeling, state-based models, reinforcement learning, secure computation and data privacy, and data-driven approaches to cybersecurity. A principal distinguishing characteristic of the UVA site research agenda is an emphasis on prescriptive modeling and understanding the collection and use of data in the context of decision making in complex systems."
420,1460682,Neural mechanisms of age-related changes in perceptual and memory decisions,BCS,Cognitive Neuroscience,3/15/15,1/21/15,Mark Wheeler,GA,Georgia Tech Research Corporation,Standard Grant,Kurt Thoroughman,2/28/19,"$571,099.00 ",,mark.wheeler@psych.gatech.edu,Office of Sponsored Programs,Atlanta,GA,303320420,4048944819,SBE,1699,,$0.00 ,"Decision-making abilities change as we age. While some changes are beneficial, such as improvements arising from accumulated experience, others are not. For example, older people are typically more cautious than is optimal in their decision-making, and are less apt to altar their decision strategies even when it would be advantageous. These changes can affect our health and quality of life. Despite the importance of decision making to healthy living, the neural mechanisms of the decision-making process, from perception to action, and how those mechanisms change in aging, are poorly understood. With the support of the National Science Foundation, Dr. Mark Wheeler and his research team at the Georgia Institute of Technology will use functional magnetic resonance imaging (fMRI) to identify how age-related differences in decisions about what we perceive and remember correspond with changes in brain function. Dr. Wheeler's research strategy is to investigate decision making as a multi-stage process in which incoming information is analyzed, evidence relevant to choices are extracted from that information and accumulated over time, and a decision is reached when there is enough evidence. Different stages of the decision process have been found to have different fMRI activation profiles, making it possible to identify whether specific stages change in healthy aging. It is predicted that, as we age, certain aspects of this multi-stage process change while others do not, and that these changes will be related to functional changes in the brain. For example, the increased caution found in many older individuals should be associated with a greater accumulation of fMRI activation prior to a decision, while a decrease in the fidelity of information processing in memory and perception should be associated with slower rates of accumulating fMRI activation. The research will also use machine learning approaches to test the degree to which the timing and amount of fMRI activity predict decision outcome. By investigating changes in neural activity as decisions develop in time, this project will identify which neural processes are most important in reaching a decision, and which change the most as we age. Such knowledge will be useful in the development of strategies or methods to improve decision-making abilities in later years. <br/><br/>This project will have broader impacts on health, science training, and outreach. The research will address issues related to human health and well-being, such as how decision-making abilities in people who are in good health differ from people with Alzheimer's disease. The research program emphasizes mentoring of high school, undergraduate, and graduate students in learning the scientific process, including hands-on training in hypothesis generation, experimental design, data collection, processing, analysis, and interpretation, and public speaking/presentation. Undergraduate researchers with advanced projects are encouraged to present their findings at local and international conferences. Research findings are presented in scientific and public formats, including scientific meetings and publications, web sites, classes, and community seminars and forums."
421,1526841,CSR:Small:Collaborative Research:EDS: Systems and Algorithmic Support for Managing Complexity in Sensorized Distributed Systems,CNS,Computer Systems Research (CSR,10/1/15,9/8/15,Rajesh Gupta,CA,University of California-San Diego,Standard Grant,Marilyn McClure,9/30/18,"$200,000.00 ",,gupta@cs.ucsd.edu,Office of Contract & Grant Admin,La Jolla,CA,920930621,8585344896,CSE,7354,7923,$0.00 ,"Commercial buildings, the energy grid and transportation systems are examples of emerging distributed systems that are beginning to be instrumented with a large number of sensors and actuators for sensing ambient environmental conditions, user occupancy, state of energy use etc. The goal of such instrumentation is to improve safety, utility and reduce costs. This is a hard problem due to interaction of humans, devices and networks in an operating environment with uncertainties regarding veracity, timeliness, meaning and value of sensor data. A large number of sensors must be provisioned, monitored and maintained by system operators. This is currently a manual and error prone task. Deploying, managing and adapting a sensorized system at scale become nearly impossible. In the micro-grid testbed of networked buildings used by this project, there are over a hundred thousand alarms raised per day by the first fifty buildings under observation. In reality, despite thousands of reported sensors there are only a few hundred distinct types of sensors. The key is to reduce the complexity of sensorized distributed systems using automated or semi-automated methods to characterize sensors, determine their type based on the sensor data streams and make inferences about the quality of sensor data with minimal operator effort. This project will apply advances in unsupervised machine learning methods to compose, aggregate and interpret sensory data spatially and over time in order to enable robust derivation of semantically useful sensory information for applications and users resulting in better-utilized and robust systems. <br/><br/>The intellectual merit of the project lies in building an information flow model, with a systematic capture and use of sensor meta-data that enables algorithmic approaches to data composition and building inferences. Using the proposed learning based automation approach along with programming and runtime support, the project will devise a data-to-decision flow for distributed systems operating across timing and reliability constraints. The project outlines smart buildings as an application driver for the envisioned sensorized distributed system with a working real-life testbed. This research will directly contribute to methods for discovery of tele-connections, such as dependence and causal relationships, between various sensory data streams which are crucial for devising effective control of devices connected to these distributed systems.<br/><br/>The broader impacts of the project include advances in the design, deployment, management and programming methodologies for a new class of distributed computing systems that can deal with changing characteristics and topologies of the underlying sensor network. The particular testbed will demonstrate, how such methods can create energy-efficient, sustainable, and comfortable buildings for occupants. A number of educational and outreach activities have been planned to train the next generation talent for the emerging area of a data-driven internet of things. For the broader research community, the project will make available, SensorDepot, an open-source extensible architecture for implementing applications for sensorized distributed systems."
422,1533767,XPS: FULL: CCA: Cymric: A Flexible Processor-Near-Memory System Architecture,CCF,Exploiting Parallel&Scalabilty,9/1/15,8/12/15,Hyesoon Kim,GA,Georgia Tech Research Corporation,Standard Grant,Yuanyuan Yang,8/31/19,"$750,000.00 ","Sudhakar Yalamanchili, Saibal Mukhopadhyay",hyesoon@cc.gatech.edu,Office of Sponsored Programs,Atlanta,GA,303320420,4048944819,CSE,8283,,$0.00 ,"Emerging software applications in data analytics, graph analysis, and machine learning must process increasing volumes of data. Such data-intensive applications stress the communication and storage parts of a computer system more than the computation parts. Yet most of today?s systems are ?compute-centric? in the sense that their designs focus on improving the efficiency of computation. Computing near data or processing-near-memory can significantly reduce data communication between memory and processor providing potential for significant improvement in energy-efficiency of next generation computing systems.  <br/><br/>This project investigates and addresses the challenges facing the design of the next generation of memory-centric processors ? specifically processing-near-memory (PNM) architectures.  The project will characterize the energy and time behavior of future applications and use this understanding to assess how to best architect new systems that combine memory and compute in close proximity. Specifically, the activities will address i) programming models, ii) processing-near-memory architectures, and iii) power & thermal managements with the development of cross-cutting data movement, compiler and microarchitectural optimizations. The results of this work can influence the design of future enterprise and high performance computing systems. The major principles, insights, and outcomes of this project will be integrated as modules into mainstream courses in Computer Architecture. The investigators will also continue their participation in Institute outreach programs that have the goal of promoting participation of undergraduate, minority, and female students in engineering research and higher education."
423,1505103,Collaborative Research: High-Throughput Quantification of Solid State Electrochemistry for Next Generation Energy Technologies,DMR,"OFFICE OF MULTIDISCIPLINARY AC, DMR SHORT TERM SUPPORT, CERAMICS",7/1/15,3/20/20,Sossina Haile,IL,Northwestern University,Continuing Grant,Lynnette Madsen,2/28/21,"$626,849.00 ",,sossina.haile@northwestern.edu,750 N. Lake Shore Drive,Chicago,IL,606114579,3125037955,MPS,"1253, 1712, 1774","1253, 1515, 7237, 8396, 8399, 8614",$0.00 ,"NON-TECHNICAL DESCRIPTION:<br/>The goal of this research is to advance the fundamental understanding of the behavior of oxide electrodes used in fuel cells, electrolysis cells, batteries, and other energy technologies. The approach combines high-throughput synthesis of libraries of material structures, with advanced high-throughput characterization and high-throughput data analysis. By making use of structures with well-defined geometric features, it is possible to directly interpret the electrochemical data. The insight afforded in turn enables deliberate engineering of structures to achieve exceptional performance. It also provides chemical guidance on how to create next generation materials. The performance enhancements can ultimately advance goals in sustainable energy. A broad cross-section  of students at all levels are incorporated into the research and training goals of this effort via internships for high school and undergraduate students, as well as doctoral research opportunities for graduate students. Outreach efforts include engaging local K-12 students in science and engineering.<br/><br/>TECHNICAL DESCRIPTION:<br/>This work aims to dramatically advance the understanding of electrochemical reaction pathways by making use of geometrically well-defined systems. Typical electrochemical structures incorporate random, high-surface area features to maximize overall performance and are not well-suited to extraction of fundamental behavior. In contrast, geometrically well-defined systems enable determination of properties such as length-specific triple-phase boundary activity, bulk chemical diffusion coefficient, area-specific surface activity, and much more. These are essential parameters for the deliberate engineering of high-performance structures. The painstaking nature of acquiring such data using individually prepared samples has, however, limited the study of geometrically well-defined electrochemical systems to a few important examples, despite growing recognition of its value. In this project, advanced fabrication tools are utilized to create libraries of electrode structures on electrolyte substrates and rapidly measure the entire contents of each library using an in-house constructed, unique scanning electrochemical probe system. Computational tools are developed to handle the massive quantities of data generated, including data mining and machine learning capabilities to create efficiencies in data acquisition and analysis. Libraries of geometrically graded microdot electrodes are complemented with selected compositionally-graded libraries, with the compositional space identified to further elucidate rate-limiting steps. Electrochemical studies are complemented with a broad suite of physical and chemical characterization methods to provide a comprehensive picture of material behavior as relevant to electrocatalysis. Generation of new insights into electrochemical reaction pathways is an essential step in the creation of next-generation electrochemical energy storage and conversion devices and as such has an important role in a sustainable energy future."
424,1564477,CAREER: Toward Cooperative Interference Mitigation for Heterogeneous Multi-Hop MIMO Wireless Networks,CNS,Networking Technology and Syst,8/1/15,7/16/18,Ming Li,AZ,University of Arizona,Continuing Grant,Alexander Sprintson,6/30/21,"$445,768.00 ",,lim@email.arizona.edu,888 N Euclid Ave,Tucson,AZ,857194824,5206266000,CSE,7363,"1045, 9150",$0.00 ,"The ever-growing number of wireless systems and the scarcity for available spectrum necessitates highly efficient spectrum sharing among disparate wireless networks. Many of them are heterogeneous in hardware capabilities, wireless technologies, or protocol standards. The resulting cross-technology interference (CTI) can be detrimental to the performance of co-locating networks if not properly mitigated. Current interference management approaches mostly follow the interference-avoidance paradigm, where transmissions are separated in frequency, time, or space to enable spectrum sharing, rather than to reduce or eliminate interference. This project explores cooperative interference mitigation (CIM), a new coexistence paradigm among heterogeneous multi-hop wireless networks. By exploiting recent advances in multi-input multi-output (MIMO) interference cancellation (IC) techniques, the proposed approach allows disparate networks to cooperatively cancel/mitigate their CTI to enhance everyone?s performance. This research focuses on the following objectives: 1) Develop tractable models/frameworks to analyze the theoretical limits and performance bounds of CIM for heterogeneous multi-hop networks, considering various forms of network heterogeneity; 2) Study the incentives of CIM through a novel game theoretic framework, that characterizes the conditions of mutual cooperation and thwarts selfish or malicious behavior; 3) Design distributed performance-approaching algorithms to achieve CIM and integrate them into practical network/MAC layer protocols, by exploiting machine learning tools and implicit inter-system communications. The expected outcomes also include the development of various simulation toolkits and system prototypes for experimental validation. <br/><br/>The integrated education plan includes cross-discipline curriculum development, student mentoring and outreach. The proposed research will have broad impacts on unplanned heterogeneous multi-hop networks that share spectrum resources, such as current and future networks in unlicensed bands, and secondary networks in TV white spaces. Applications will benefit multiple domains including healthcare, energy, emergency services and military etc. Major results will be disseminated via conference and journal publications, software packages, talks and tutorials."
425,1518732,SHF:Large:Collaborative Research: Inferring Software Specifications from Open Source Repositories by Leveraging Data and Collective Community Expertise,CCF,Software & Hardware Foundation,7/1/15,6/17/15,Vasant Honavar,PA,Pennsylvania State Univ University Park,Standard Grant,Sol Greenspan,6/30/20,"$319,511.00 ",,vhonavar@ist.psu.edu,110 Technology Center Building,UNIVERSITY PARK,PA,168027000,8148651372,CSE,7798,"7925, 7944",$0.00 ,"Today individuals, society, and the nation critically depend on software to manage critical infrastructures for power, banking and finance, air traffic control, telecommunication, transportation, national defense, and healthcare. Specifications are critical for communicating the intended behavior of software systems to software developers and users and to make it possible for automated tools to verify whether a given piece of software indeed behaves as intended. Safety critical applications have traditionally enjoyed the benefits of such specifications, but at a great cost.  Because producing useful, non-trivial specifications from scratch is too hard, time consuming, and requires expertise that is not broadly available, such specifications are largely unavailable. The lack of specifications for core libraries and widely used frameworks makes specifying applications that use them even more difficult. The absence of precise, comprehensible, and efficiently verifiable specifications is a major hurdle to developing software systems that are reliable, secure, and easy to maintain and reuse. <br/><br/>This project brings together an interdisciplinary team of researchers with complementary expertise in formal methods, software engineering, machine learning and big data analytics to develop automated or semi-automated methods for inferring the specifications from code. The resulting methods and tools combine analytics over large open source code repositories to augment and improve upon specifications by program analysis-based specification inference through synergistic advances across both these areas. <br/><br/>The broader impacts of the project include: transformative advances in specification inference and synthesis, with the potential to dramatically reduce, the cost of developing and maintaining high assurance software; enhanced interdisciplinary expertise at the intersection of formal methods software engineering, and big data analytics; Contributions to research-based training of a cadre of scientists and engineers with expertise in high assurance software."
426,1527453,CHS: Small: Supporting Crowdsourced Sensemaking in Big Data with Dynamic Context Slices,IIS,HCC-Human-Centered Computing,10/1/15,6/17/19,Kurt Luther,VA,Virginia Polytechnic Institute and State University,Continuing Grant,William Bainbridge,12/31/19,"$532,000.00 ",Christopher North,kluther@vt.edu,Sponsored Programs 0170,BLACKSBURG,VA,240610001,5402315281,CSE,7367,"7367, 7923, 9251",$0.00 ,"This research will investigate how crowdsourcing and computational techniques can be combined to support the efforts of an individual analyst engaged in a complex sensemaking task, such as identifying a threat to national security or determining the names of people and places in a photograph. Currently, such complex tasks are beyond the capabilities of the most advanced machine learning techniques or crowdsourcing workflows, and even trained experts struggle to perform them.  Huge quantities of data are now available online, but making sense of them is challenging because human cognition, while remarkably powerful, is nevertheless a limited resource. Visual analytics tools seek to overcome this limitation by leveraging the complementary strengths of information visualization and data mining, but these tools generally assist with low-level tasks, requiring significant effort on the part of users. Crowdsourcing has emerged as a promising technique for applying human intelligence to problems computers cannot easily solve, but for crowds to assist individuals with complex sensemaking tasks, two significant challenges must be addressed. First, we must understand when crowds versus computation are more useful at each phase in the sensemaking loop. Second, we must overcome the limited time and expertise of most crowd workers to sustain deep, complex lines of inquiry.<br/><br/>This research addresses both of these challenges through a series of four experiments. First, it will conduct a laboratory study where individuals perform complex sensemaking tasks to understand what types and amounts of context they use to make decisions, and how the sensemaking loop might be decomposed into subtasks. Second, it will conduct a series of experiments comparing crowdsourcing to automated techniques for each of the most promising sensemaking subtasks. Third, it will experiment with different crowd workflows to develop a revised sensemaking loop, optimized for the relative strengths of crowds and computation, and develop a software prototype based on this approach. At the core of the software design is the novel concept of ""context slices,"" an innovative technique for addressing the transience of crowd workers by giving them only the information they need to complete their assigned task, allowing complex investigations to be pursued across multiple workers. The fourth experiment will evaluate this approach by comparing performance with the software to the baselines established in the first study."
427,1541233,"An Undergraduate Workshop on ""Big Data, Human Health and Statistics""",DMS,STATISTICS,6/15/15,6/6/15,Bhramar Mukherjee,MI,Regents of the University of Michigan - Ann Arbor,Standard Grant,Gabor Szekely,5/31/16,"$15,000.00 ",,bhramar@umich.edu,3003 South State St. Room 1062,Ann Arbor,MI,481091274,7347636438,MPS,1269,7556,$0.00 ,"This workshop is a concluding event to an Undergraduate Summer Institute on ""Big Data, Human Health and Statistics"" held at the Department of Biostatistics, University of Michigan, Ann Arbor from June 1-26 and is intended to be an event focusing on the undergraduates. The training of the next generation of quantitative scientists needs to change to meet the demands of the data. We define ""Big Data"" as datasets of enormous size and complexity (either in number of observations, and/or in the number/nature of predictors/outcomes). Classical theory, computation and intuition often fail for such irregular, sparse data sets of vast size. More training in data management, data storage, visualization, high dimensional statistics, optimization, causal methods, modeling sparse data, machine learning are needed to equip students to tackle these big data challenges. It is expected that the knowledge obtained from these massive heterogeneous data sources will inform prevention, screening, prognosis, and treatment of human diseases and play a major role in biology, medicine, and public health in the coming decade. This workshop lies in the intersection of Big Data,  Human Health and Statistics. <br/><br/>In this two-day symposium, the first day will feature talks by distinguished researchers in areas of relevance to Big Data, including mobile health, precision medicine, genomics, data visualization. There will be a poster session by undergraduate attendees and an oral presentation session by the undergraduate participants of the summer institute. The second day will be a professional development workshop for undergraduate attendees.  Registration will be open to anyone interested (with a maximum limit of 120 participants). The grant will support participation cost of selected undergraduate attendees, faculty mentors and outside speakers."
428,1536444,Optimal Design of Biomarker-Based Screening Strategies for Early Detection of Chronic Diseases,CMMI,OE Operations Engineering,9/1/15,7/6/16,Brian Denton,MI,Regents of the University of Michigan - Ann Arbor,Standard Grant,Georgia-Ann Klutke,8/31/18,"$205,000.00 ",John Wei,btdenton@umich.edu,3003 South State St. Room 1062,Ann Arbor,MI,481091274,7347636438,ENG,006Y,"076E, 078E, 116E, 8023, 9178, 9231, 9251",$0.00 ,"Chronic diseases are the leading cause of death in the United States. Early detection of diseases like cancer can extend life and improve quality of life as well as reduce cost to health systems, freeing resources for other purposes. Recent discoveries of many new biomarkers are helping physicians identify early signs of chronic diseases, such as cancer. At the same time, these advances have made clinical decision making difficult because the available tests are not 100 percent reliable and sometimes cause false positive or false negative results. False positives lead to anxiety and unnecessary referral of patients for expensive and invasive tests, such as biopsies and radiological imaging; false negatives cause a disease to go undetected and potentially progress to a life threatening stage. Since no single biomarker on its own is considered satisfactory, attention is turning to ways to combine biomarkers into composite tests with better predictive characteristics. This project will develop mathematical models for investigating which tests to use, when to use them, and how to combine them to screen for diseases in a way that balances the benefits of early detection with the harms of false negative test results.<br/> <br/>The aim of this project is to create stochastic programming models and partially observable Markov decision processes that integrate screening, diagnosis, and treatment decisions over the complete lifecycle of a chronic disease to optimize population screening. New data-driven models will be created for optimal design of (a) one-time composite screening tests; (b) personalized dynamic protocols for screening over a patient's lifetime to optimally balance the competing goals of early disease detection and minimal cost and harm from screening.  These problems are challenging because of their stochastic and combinatorial nature, the partially observable nature of early stage chronic diseases, and the fact that there are multiple stakeholders (patients, physicians, insurers) and therefore multiple criteria. Theoretical properties that provide insight into optimal screening strategies will be analyzed and used to design efficient algorithms and approximation methods for solving these problems using a combination of stochastic optimization and machine learning. The discoveries from this project will be used to investigate the optimal design of screening strategies that weigh the benefits of early detection with the potential harms of screening. Finally, these models will be used to determine ideal characteristics of new biomarkers that make them worthy of costly clinical investigation."
429,1539722,III: Small: Collaborative Research: Functional Network Discovery for Brain Connectivity,IIS,Info Integration & Informatics,1/1/15,3/9/15,Jieping Ye,MI,Regents of the University of Michigan - Ann Arbor,Standard Grant,Sylvia Spengler,7/31/18,"$200,000.00 ",,jpye@umich.edu,3003 South State St. Room 1062,Ann Arbor,MI,481091274,7347636438,CSE,7364,"7364, 7923",$0.00 ,"Neuroscience is at a moment in history where mapping the connectivity of the human brain non- invasively and in vivo has just begun with many unanswered questions.  While the anatomical structures in the brain have been well known for decades, how they are used in combination to form task specific networks has still not been completely explored. Understanding what these networks are, and how they develop, deteriorate, and vary across individuals will provide a range of benefits from disease diagnosis, to understanding the neural basis of creativity, and even in the very long term to brain augmentation. Though machine learning and data mining has made significant inroads into real world practical applications in industry and the sciences, most existing work focuses on lower-level tasks such as predicting labels, clustering and dimension reduction. This requires the practitioner to shoe-horn their more complex tasks, such as network discovery, into the algorithm's settings. <br/><br/>The focus of this grant is a transition to more complex higher-level discovery tasks and in particular, eliciting networks from spatio-temporal data represented as a tensor. Here the spatio-temporal data is an fMRI scan of a person represented as a four dimensional tensor with each entry in the tensor being a data point that indicates the brain activity at that time and location. The overall problem focus is to simplify this data into a cognitive network consisting of identifying active regions of the brains and the interactions that occur between them. The work will consist of three intertwined tasks as follows: i) Supervised and Semi-supervised Network Discovery, ii) Complex Network Discovery and iii) Network Discovery in Populations. In the supervised/semi-supervised setting, the networks discovered involves coordinated activity among some combination of anatomical structures Since all or some of the structures are given along with their boundaries, this is termed a supervised (or semi-supervised) problem. With complex network discovery the team will move beyond finding a single network of coordinated activity to finding multiple networks with complex (beyond coordinates) relationships between the structures/regions. Finally with network discovery in populations , the previous work that studies an individual scan will be expanded to a population of scans.  A population may be a collection of individuals performing the same task or a single individual's scans collected over time. Studying such populations allows addressing innovative questions such as: ""How does one individual's network change over the course of development, aging, or disease?"" and ""How do the networks differ for one group of individuals to that of another group?"""
430,1454816,CAREER:   Probing the Demographics of Supermassive Black Holes with Time-Domain Observations of Tidal Disruption Events,AST,CAREER: FACULTY EARLY CAR DEV,5/1/15,9/14/17,Suvi Gezari,MD,University of Maryland College Park,Standard Grant,Sarah Higdon,4/30/21,"$808,004.00 ",,suvi@astro.umd.edu,3112 LEE BLDG 7809 Regents Drive,COLLEGE PARK,MD,207425141,3014056269,MPS,1045,"1045, 1206, 1207",$0.00 ,"Supermassive black holes (SMBHs) are not just rare and exotic; recent observations of nearby galaxies have demonstrated that SMBHs are a central component of almost all galaxies.  Based on these observations, the best evidence suggests that both the SMBHs and the galaxy grow in mass over time.   How does this happen?   The researcher has a plan of observations that will show the connection between these central beasts (SMBHs) and their host galaxies. However the observations are difficult.  Most SMBHs in the Universe are hidden from view: too distant to measure the gravitational pull on the stars by the SMBH, or too dim from the lack of gas falling onto the SMBH. <br/><br/>The researcher plans to use optical telescopes, which look at the sky automatically and repeatedly every night, to find rare transient events: when stars pass too close to SMBHs.  A 'starved' (dormant) black hole will reveal itself when an unlucky star passes close enough to be torn apart by tremendous tides expected near the SMBH.   Based on the brightness of the flare, when the Tidal Disruption Event (TDE) happens, they will be able to weigh the central SMBH, estimate the black hole spin, as well as constrain the mass, radius, and internal structure of the disrupted star.<br/><br/>The project has several major intellectual merits:  1) The researcher clearly lays out the status of measuring intermediate-mass and supermassive black hole properties with tidal disruption events and convincingly details how to find thousands of TDE candidates.  3) They plan to increase the sensitivity of Pan-STARRS 1 (PS1) Medium Deep Survey with stacking analysis and using the dataset to classify transients of all flavors is the step towards efficiently filtering observations with the new Large Synoptic Survey Telescope.  3) The researcher will help other scientists' research by sharing their machine-learning algorithms.<br/><br/>The researcher will enhance the Maryland GRAD-MAP program, which encourages women and under-represented groups in STEM fields.  She will leverage established success of GRAD-MAP in improving retention rate among physics majors at local minority serving institutions. Her previous experience in GRAD-MAP is significant. PS1 database offers a useful source of data for short research projects as part of the winter workshop. Their experience with PS1 will ensure that the use of this database is maximized. The GRAD-MAP program has evidence of success with five of seven participants following through with research programs during the summer. There is the potential to take 5 success stories and turn them into many more new success stories."
431,1521672,Collaborative Research: CompSustNet: Expanding the Horizons of Computational Sustainability,CCF,Expeditions in Computing,12/15/15,2/11/20,Douglas Fisher,TN,Vanderbilt University,Continuing Grant,Sylvia Spengler,11/30/20,"$190,000.00 ",,douglas.h.fisher@vanderbilt.edu,Sponsored Programs Administratio,Nashville,TN,372350002,6153222631,CSE,7723,"7723, 9150",$0.00 ,"Poverty, saving species, repowering the world with renewable energy, lifting people up to live better lives - there are no easy answers to guiding our planet on the path toward sustainability. Complex problems require sophisticated solutions. They involve intricacy beyond human capabilities, the kind of big-data processing and analysis that only advanced large-scale computing can provide. This NSF Expedition in Computing launches CompSustNet (http://www.compsust.net), a vast research network powered by the nation's recognized university computer science programs, charged with applying the emerging field of computational sustainability to solving the world's seemingly unsolvable resource problems. Put simply, the project will enlist some of the top talents in computing, social science, conservation, physics, materials science, and engineering to unlock sustainable solutions that safeguard our planet's future.<br/><br/>Computational Sustainability is, at its core, the belief that with sufficiently advanced computational techniques, we can devise sustainable solutions that meet the environmental, societal, and economic needs of today while providing for future generations. In much the same way IBM's supercomputer Watson could defeat any challenger in Jeopardy!, computational sustainability posits that a computer-engineered solution can be applied to world's difficult and challenging problems - from helping farmers and herders in Africa survive severe droughts to developing a smart power grid fueled entirely by renewable energy. CompSustNet is a large national and international multi-institutional research network led by Cornell University and including 11 other US academic institutions: Bowdoin, Caltech, CMU, Georgia Tech, Howard University, Oregon State, Princeton, Stanford, UMass, University of South California, and Vanderbilt University, as well as collaborations with several international universities. But CompSustNet is not just an academic enterprise, as it also includes key governmental and non-governmental organizations that specialize in conservation, poverty mitigation, and renewable energy, such as The Nature Conservancy, The World Wildlife Fund, The International Livestock Research Institute, The Trans-African Hydro-Meteorological Observatory, and the National Institute of Standards and Technology.<br/><br/>CompSustNet's core mission is to significantly expand the horizons of computational sustainability and foster the advancement of state-of-the-art computer science to achieve the scale to tackle global problems. Research will focus on cross-cutting computational topics such as optimization, dynamical models, simulation, big data, machine learning, and citizen science, applied to sustainability challenges.  For example, computational sustainability is being put to work to resolve the problem of providing wetlands for shorebirds that migrate from the Arctic through California during a time of drought. As California gets drier, the shorebirds have nowhere to stop, rest, and refuel by eating wetland invertebrates. Scientists are developing new dynamic precision conservation techniques that use complex, big-data models to tackle the problem with NASA satellite imagery, meteorological forecasts, and citizen science in the form of thousands of bird location sightings from the Cornell Lab of Ornithology's eBird checklisting app for birdwatchers. Through partnership with The Nature Conservancy, the program forecasts when and where wetland habitat would be needed for shorebirds, and the Conservancy pays Central Valley rice farmers to flood their fields at opportune times - providing benefits for birds and farmers at a time when extreme drought is making life tough for both. In similar ways, computational sustainability projects will also be hard at work innovating automated monitoring networks to protect endangered elephant population from poachers, promoting the discovery of novel ways to harvest energy from sun light, and designing algorithms to manage the generation and storage of renewable energy in the power grid. <br/><br/>Advancements in computational sustainability will lead to novel, low-cost, high-efficiency strategies for saving endangered species, helping indigenous peoples improve their way of life, and scaling renewables up to meet 21st century energy demand. CompSustNet is like the seed, the venture capital, to help the field of computational sustainability achieve what's possible."
432,1545949,PIRE: GROWTH: Global Relay of Observatories Watching Transients Happen,OISE,PIRE- Prtnrshps Inter Res & Ed,10/1/15,7/29/19,Mansi Kasliwal,CA,California Institute of Technology,Continuing Grant,Maija Kukla,9/30/20,"$4,518,826.00 ","Thomas Prince, Shrinivas Kulkarni",mansi@astro.caltech.edu,1200 E California Blvd,PASADENA,CA,911250600,6263956219,O/D,"1798, 7742","1207, 5905, 5919, 5921, 5924, 5936, 7566",$0.00 ,"The Global Relay of Observatories Watching Transients Happen (GROWTH) program is an international collaborative network of astronomers and telescopes dedicated to the study of short-lived cosmic transients and near-earth asteroids. Cosmic transients are energetic flashes of light that are millions to billions of times the brightness of the sun, e.g. explosive deaths of massive stars, white dwarf detonations, exotic neutron star mergers and tidal disruption by black holes. Key follow-up observations of fast-fading or fast-moving events must occur at night promptly after discovery but before the sun rises. A relay of telescopes spanning multiple longitudes on earth will pass the baton amongst each other to effectively extend the night-time darkness. GROWTH will enable detailed monitoring of events that would otherwise vanish before the next night?s observations could begin at any single location. The torrent of data thus generated shall be used to improve both astronomical observing techniques and big data management and analysis (such as machine learning algorithms, database design and statistics). In addition, the intrinsically international context of the GROWTH program shall foster the development of relationships and social capital required to navigate effectively among diverse science cultures from around the world. Therefore, participating students will have the quantitative, computational and interpersonal skills to assume leadership roles in the knowledge economy. <br/><br/>GROWTH will address several frontier questions in time domain astronomy systematically. GROWTH aims to characterize the electromagnetic emission from binary neutron star mergers, thereby localizing the prime source of gravitational waves expected to be detected by the Advanced Laser Interferometer Gravitational-wave Observatory (LIGO). Timely spectroscopy may identify the long sought cosmic location of heavy element production. GROWTH would track small near-earth asteroids of the type that may pose hazards, to characterize their orbit, size, type and rotation properties. GROWTH shall have a front-row seat to observe the end points of stellar evolution by routinely obtaining data in the first 24 hours of a new-born supernova and directly probing the chemistry and history of the progenitor star. The GROWTH education program will train students and postdocs and encompass undergraduate course development, international internships, annual workshops and a research conference. GROWTH will thus help the US and international community prepare for the era of the Large Synoptic Survey Telescope (starting 2022), NSF's highest priority ground-based astronomy project for the next decade."
433,1526424,NRI: Collaborative Research: Versatile Locomotion: From Walking to Dexterous Climbing With a Human-Scale Robot,IIS,NRI-National Robotics Initiati,9/1/15,8/6/15,Katie Byl,CA,University of California-Santa Barbara,Standard Grant,David Miller,8/31/19,"$454,868.00 ",,katiebyl@ece.ucsb.edu,Office of Research,Santa Barbara,CA,931062050,8058934188,CSE,8013,8086,$0.00 ,"The project aims to give legged robots the skills to navigate a wide variety of terrain.  This capability is needed to employ robots in applications such as search-and-rescue, construction, and exploration of remote environments on Earth and other planets. The multidisciplinary team, composed of researchers at Duke, Stanford, UC Santa Barbara, JPL, and Motiv Robotics, will develop a robot to climb a variety of surfaces ranging from flat ground to overhanging cliffs. Using an array of sensors, unique hands, and sophisticated algorithms, the robot will dynamically adopt walking, crawling, climbing, and swinging strategies to traverse wildly varied terrain. During the course of this research, the team hopes to achieve the milestone of the first demonstration of a human-scale rock climbing robot. The research is also expected to lead to insights into cognitive and biomechanical processes in human and animal locomotion.<br/><br/>Although rock climbing serves as an ideal proving ground for the work, this project conducts basic research to address more a general-purpose goal; namely, to provide the physical and cognitive skills for robots to adaptively navigate varied terrain. It takes a dexterous climbing approach, which uses non-gaited, coordinated sequences of contact to move the body, much as dexterous manipulation uses contact with the fingers and palm to move an object.  It will apply principles from optimization, machine learning, bioinspiration, and control theory to make intellectual contributions in several domains, such as robot hand design, planning algorithms, balance strategies, and locomotion performance measurement.  Novel grippers, sensor-based planning strategies, reactive maneuvers, and locomotion metrics will be developed during the course of this research."
434,1553192,EAGER: Language and Architecture Design for Approximation at Different Granularities,CCF,Software & Hardware Foundation,9/1/15,8/28/15,Hadi Esmaeilzadeh,GA,Georgia Tech Research Corporation,Standard Grant,tao li,8/31/17,"$160,000.00 ",,hadi@eng.ucsd.edu,Office of Sponsored Programs,Atlanta,GA,303320420,4048944819,CSE,7798,"7916, 7941, 7943",$0.00 ,"The IT industry's economic ecosystem mostly relies on continuously delivering new services and devices by exploiting continuous performance and efficiency improvements in general-purpose processing. However, as we enter the dark silicon era, the benefits from transistor scaling are diminishing and the current paradigm of processor design significantly falls short of the traditional cadence of performance improvements. These shortcomings can drastically curtail the industry's ability to continuously deliver new capabilities, breaking the backbone of its economic ecosystem. Radical departures from conventional approaches are necessary to provide large efficiency and performance gains for a wide range of applications. One such departure is general-purpose approximate computing that accepts error in computation and relaxes the traditional abstraction of ""near-perfect"" accuracy.  Approximate computing leverages the inherent error tolerance of a large body of emerging applications. These applications span a wide range of domains including vision, big data analytics, machine learning, sensor processing, cyber-physical systems, multimedia, and web search. For these diverse domains of applications, it is timely and crucial to provide architectural mechanisms and programming abstractions that enable trading quality of results for gains in both performance and efficiency. <br/><br/>This project aims to provide both architectural mechanisms and programming language constructs that make approximate computing effectively applicable to a wide range of domains of applications. Energy efficiency is the IT industry's biggest challenge. To maintain the nation's economic leadership in the IT industry, it is vital to develop solutions such as ours that provide significant gains in efficiency and performance. Many of these techniques allow approximation to permeate across the boundaries of hardware and software. Thus, it is essential to educate a workforce that not only deeply understands hardware and software but also can innovate across the boundaries of the two. This project provides a foundation for such research and education by producing benchmarks, tools, and general infrastructure for approximate computing. These artifacts will be made publicly available and will be integrated into the curriculum."
435,1522107,SCH: INT: Collaborative Research: S.E.P.S.I.S.: Sepsis Early Prediction Support Implementation System,IIS,Smart and Connected Health,10/1/15,5/8/17,Julie Ivy,NC,North Carolina State University,Standard Grant,Sylvia Spengler,9/30/19,"$850,275.00 ","Maria Mayorga, Min Chi, Osman Ozaltin",jsivy@ncsu.edu,2701 Sullivan DR STE 240,Raleigh,NC,276950001,9195152444,CSE,8018,"8018, 8062, 9102, 9251",$0.00 ,"Sepsis, infection plus systemic manifestations of infection, is the leading cause of in-hospital mortality. About 700,000 people die annually in US hospitals and 16% of them were diagnosed with sepsis (including a high prevalence of severe sepsis with major  complication). In addition to being deadly, sepsis is the most expensive condition associated with in-hospital stay, resulting in a 75% longer stay than any other condition.  The total burden of sepsis to the US healthcare system is estimated to be $20.3 billion, most of which is paid by Medicare and Medicaid. In fact, in June 2015 the Centers for Medicare & Medicaid Services (CMS) reported that sepsis accounted for over $7 billion in Medicare payments (second only to major joint replacement), a close to 10% increase from the previous year.  This pervasive drain on health care resources is due, in part, to difficulties in diagnosis and delayed treatment. For example, every one hour delay in treatment of severe sepsis/shock with antibiotics decreases a patient's survival probability by 10%. Many of these deaths could have been averted or postponed if a better system of care was in place. The goal of this research is to overcome these barriers by integrating electronic health records (EHR) and clinical expertise to provide an evidence-based framework to diagnose and accurately risk-stratify patients within the sepsis spectrum, and develop and validate intervention policies that inform sepsis treatment decisions.  The  project to bring together health care providers, researchers, educators, and students to add value to patient care by integrating machine learning, decision analytical models, human factors analysis, as well as system and process modeling to advance scientific knowledge, predict sepsis, and prevent sepsis-related health deterioration. In addition to the societal impact that clinical translation of these findings may bring, the project will provide engineering and computer science students and health services researchers with cross-disciplinary educational experience.<br/><br/>The proposed research will apply engineering and computer science methodologies to analyze patient level EHR across two large scale health care facilities, Mayo Clinic Rochester and Christiana Care Health System and to inform clinical decision making for sepsis. The multi-institutional, interdisciplinary collaboration will enable the development of health care solutions for sepsis by describing and accurately risk-stratifying hospitalized patients, and developing decision analytical models to personalize and inform diagnostic and treatment decisions considering patient outcomes and response implications. The Sepsis Early Prediction Support Implementation System (S.E.P.S.I.S.) project aims will be to: 1) Develop data-driven models to classify patients according to their clinical progression to diagnose sepsis and predict risk of deterioration, thus informing therapeutic actions. 2) Develop personalized intervention policies for patients within the sepsis spectrum.  3) Develop decision support systems (DSS) for personalized interventions focusing on resource implications and usability within a real hospital setting. The team will 1) identify important factors that uncover patient profiles based on Bayesian exponential family principal components analysis; 2) develop hidden Markov models (HMMs) and input-output HMMs to identify clusters of patients with similar progression patterns within the sepsis spectrum; 3) provide an analytical framework to support sepsis staging in clinical practice using bilevel optimization. They will 1) predict short- and long-term individual patient outcomes using multivariate statistical models and simulation; 2) develop semi-Markov decision process and partially observable semi-Markov decision process models to identify timing of therapeutic actions and diagnostic tests. Furthermore, the team will 1) predict demand for resources and develop and validate a hybrid mixed integer programming and queueing model to optimize system level allocations; 2) utilize human factors analysis and usability testing to assess the implementation of the DSS."
436,1518897,SHF: Large:Collaborative Research: Inferring Software Specifications from Open Source Repositories by Leveraging Data and Collective Community Expertise,CCF,Software & Hardware Foundation,7/1/15,4/11/19,Hridesh Rajan,IA,Iowa State University,Standard Grant,Sol Greenspan,6/30/19,"$750,125.00 ",Tien Nguyen,hridesh@iastate.edu,1138 Pearson,AMES,IA,500112207,5152945225,CSE,7798,"7925, 7944, 9150",$0.00 ,"Today individuals, society, and the nation critically depend on software to manage critical infrastructures for power, banking and finance, air traffic control, telecommunication, transportation, national defense, and healthcare. Specifications are critical for communicating the intended behavior of software systems to software developers and users and to make it possible for automated tools to verify whether a given piece of software indeed behaves as intended. Safety critical applications have traditionally enjoyed the benefits of such specifications, but at a great cost.  Because producing useful, non-trivial specifications from scratch is too hard, time consuming, and requires expertise that is not broadly available, such specifications are largely unavailable. The lack of specifications for core libraries and widely used frameworks makes specifying applications that use them even more difficult. The absence of precise, comprehensible, and efficiently verifiable specifications is a major hurdle to developing software systems that are reliable, secure, and easy to maintain and reuse. <br/><br/>This project brings together an interdisciplinary team of researchers with complementary expertise in formal methods, software engineering, machine learning and big data analytics to develop automated or semi-automated methods for inferring the specifications from code. The resulting methods and tools combine analytics over large open source code repositories to augment and improve upon specifications by program analysis-based specification inference through synergistic advances across both these areas. <br/><br/>The broader impacts of the project include: transformative advances in specification inference and synthesis, with the potential to dramatically reduce, the cost of developing and maintaining high assurance software; enhanced interdisciplinary expertise at the intersection of formal methods software engineering, and big data analytics; Contributions to research-based training of a cadre of scientists and engineers with expertise in high assurance software."
437,1458059,Collaborative Research: ABI Innovation: Computational population-genetic analysis for detection of soft selective sweeps,DBI,ADVANCES IN BIO INFORMATICS,8/1/15,7/23/15,Noah Rosenberg,CA,Stanford University,Standard Grant,Peter McCartney,7/31/19,"$574,954.00 ",Dmitri Petrov,noahr@stanford.edu,450 Jane Stanford Way,Stanford,CA,943052004,6507232300,BIO,1165,,$0.00 ,"The molecular process of adaptation-the rise in frequency of genetic variants that enable organisms to succeed in their environments-is a central process in evolutionary biology. Surmounting significant challenges such as the ability of infectious agents to evolve resistance to drugs and the ability of crop pests to defeat a diverse array of increasingly powerful insecticides requires an understanding of the nature of adaptation. Recent advances have demonstrated that adaptation often occurs via ""soft selective sweeps,"" in which an adaptive genetic variant originates multiple times or has become favored only after it has been present at a substantial frequency in the population. This project contributes to advancing knowledge of the fundamental evolutionary process of adaptation by developing new computational tools to detect and study the occurrence of adaptation by soft selective sweeps. Through the interactions of a multidisciplinary team spanning evolutionary biology and bioinformatics, the project integrates advances in evolutionary simulation with modern and efficient computational methods in order to produce progress on understanding adaptation, while simultaneously developing efficient computational tools applicable in the modern ""big-data"" era of inexpensive sequencing. In addition, its joint mentorship efforts from evolutionary and bioinformatics perspectives promote interdisciplinary training of graduate students and postdoctoral scientists. <br/><br/>The project has four objectives: (1) To design new tests for detecting selection in the case in which soft selective sweeps occur from standing genetic variation; (2) To identify haplotypes that carry a beneficial allele in genomic regions known to be experiencing positive selection; (3) To enhance new methods of analysis of natural selection to make them robust to confounding demographic scenarios; (4) To apply new selection methods in a series of data sets from multiple species, including humans, Drosophila, and Plasmodium malaria parasites. The project will use algorithmic techniques from combinatorial optimization and machine learning, and it will exploit ideas from population genetics and coalescent theory. It breaks ground on several fronts, providing a deeper understanding of the patterns in site-frequency spectra and haplotype data as a basis for selection signatures, and assisting in the design of subtyping studies for complex regions of the genome. As it becomes increasingly possible to sequence whole genomes of multiple individuals within a population, the intellectual challenge of designing tools for detecting selection to accommodate new phenomena such as soft sweeps coincides with the computational challenge of incorporating genomic data sets into selection studies. These challenges are addressed by the project, whose results will be available at http://proteomics.ucsd.edu/vbafna/research-2/nsf1458059/."
438,1515589,EAPSI: Object Recognition for the Purpose of Traffic Compliance of Autonomous Vehicles,OISE,EAPSI,6/1/15,5/26/15,Joseph Campbell,AZ,Campbell                Joseph,Fellowship Award,Anne Emig,5/31/16,"$5,070.00 ",,,,Chandler,AZ,852266097,,O/D,7316,"5927, 5978, 7316",$0.00 ,"In order for an autonomous vehicle to effectively navigate a road network, it needs to have accurate information about road elements such as traffic signs, traffic lights, and road markings so that it can comply with local traffic laws. Traditionally this information is gathered through detailed maps or visual recognition, however, these approaches can fail if there is insufficient map data available or on-board sensors are unsuccessful at locating road elements. This project proposes to build a system which can detect these road elements by analyzing the behavior of nearby road vehicles. This research will be performed under the guidance of Dr. Marcelo H. Ang Jr. at the National University of Singapore. Dr. Ang previously co-authored related research and as such will be a source of invaluable expertise, in addition to being a member of a lab with access to a cutting-edge autonomous vehicle platform.<br/><br/>Nearby vehicles will be detected by using a combination of LIDAR sensors and cameras which are mounted to an autonomous vehicle platform. Vehicle positions will be extrapolated from the sensor data in conjunction with localization data provided by the vehicle platform, from which features will be extracted and fed into a machine learning classifier. Recent research has shown great success at identifying pedestrian paths by filtering out noise in pedestrian positions via clustering then modelling with a Naive Bayes Classifier, so these techniques will be used in order to identify road elements that lie on a vehicle?s path. The results from the classifier will be used in conjunction with other traditional identification methods in order to improve the overall identification rate of road elements. This NSF EAPSI award is funded in collaboration with the National Research Foundation of Singapore."
439,1533661,NCS-FO: Collaborative Research: Understanding Individual Differences in Cognitive Performance: Joint Hierarchical Bayesian Modeling of Behavioral and Neuroimaging Data,BCS,IntgStrat Undst Neurl&Cogn Sys,8/1/15,8/10/15,Mark Steyvers,CA,University of California-Irvine,Standard Grant,Betty Tuller,7/31/19,"$301,365.00 ",,mark.steyvers@uci.edu,"141 Innovation Drive, Ste 250",Irvine,CA,926173213,9498247295,SBE,8624,"8089, 8091, 8551",$0.00 ,"Understanding the complex determinants of individual health and wellbeing is critical for the promotion and maintenance of a healthy world population.  Wellbeing may be understood not only as the absence of physical and mental illness but also as the quality of life and optimal functioning of individuals. It is well known that individuals vary tremendously in terms of cognitive abilities and dispositions, as seen from performance on high-order cognitive tasks, decision-making preferences, and emotional competencies. However, the neural underpinnings of much of this variability are poorly understood: It is unclear how individual differences in brain structure and function across tasks and processes are linked to abilities and competencies. This project explores a mathematical and computational framework for investigating a large-sample neuroimaging and behavioral dataset in order to improve our understanding of individual differences in cognitive performance. An ultimate goal of the project is to predict individual cognitive performance in novel, real-world situations based on observed (past) behavioral and neuroimaging data and contribute to the understanding of cognitive health and wellbeing of individuals. The project will also offer many training opportunities for the next generation of scientists. <br/><br/>The technical approach will build on and integrate recent advances in cognitive science, neuroscience, statistics, and machine learning. Statistical models will integrate data from both brain imaging and behavioral tests to generate predictions that otherwise may not be possible with a single source of data. The research will go beyond establishing and explaining individual differences to predicting individual cognitive performance in a variety of tasks."
440,1541818,"US-Japan Materials Genome (MG) Workshop to be held at the International Congress Center ""Epochal Tsukuba"" 2-20-3, Takezono, Tsukuba, Ibaraki, 305-0032, Japan; June 23-24, 2015",CMMI,"International Research Collab, Materials Eng. & Processing",6/15/15,6/9/15,Robert Chang,IL,Northwestern University,Standard Grant,Alexis Lewis,11/30/16,"$19,999.00 ",,r-chang@northwestern.edu,750 N. Lake Shore Drive,Chicago,IL,606114579,3125037955,ENG,"7298, 8092","022E, 1444, 5983, 7298, 7556, 8021",$0.00 ,"This award supports a workshop to facilitate the development of long-term, sustainable collaboration between US and Japan in the area of advanced structural and infrastructure materials. The workshop, entitled 'The US-Japan Materials Genome (MG) Workshop,' will promote the aims of the US Materials Genome Initiative, an initiative announced by President Obama in 2011, which aims to double the speed of discovery and deployment of new materials, at a fraction of the cost. The initiative has already had a wide impact on society and on the materials research community, spurring new collaborations and innovations resulting from integrated use of experimental and computational materials research approaches, and open access to materials data. This workshop will further these aims by establishing collaborations with materials researchers in Japan, to jointly address some of the key issues facing researchers who wish to manage, maintain, and share large amounts of materials data. Anticipated outcomes include the creation and implementation of a long term, sustainable, bilateral collaborative program, and a joint platform for materials informatics and databases.<br/><br/>Structural materials play a central role in national physical infrastructure development, and both the US and Japan have substantial efforts in developing structural materials for applications in maintaining, and improving their large-scale physical infrastructure. In addition, both nations have efforts to discover, design, and deploy advanced structural materials for dynamic applications such as the automotive, aerospace, and power generation industries. These are major undertakings that require a concerted, coordinated effort in all aspects, ranging from fundamental materials science research, to the acquisition, curation, and maintenance of high-quality data, and, ultimately, to manufacturing products that are energy efficient, environmentally sustainable, and durable. The goals of this and subsequent workshops is to bring together a diverse group of researchers from the US and Japan to discuss ways to use predictive theory and modeling, combined with machine learning, data mining, and rapid-acquisition of experimental data to produce highly efficient and low-cost manufactured products, and to accelerate materials design, discovery, and deployment through the open sharing of materials data."
441,1524750,III: Small: Collaborative Research: Reducing Classifier Bias in Social Media Studies of Public Health,IIS,Info Integration & Informatics,8/1/15,8/12/15,Sherry Emery,IL,University of Illinois at Chicago,Standard Grant,Maria Zemankova,9/30/16,"$194,526.00 ",,emery-sherry@norc.org,809 S. Marshfield Avenue,Chicago,IL,606124305,3129962862,CSE,7364,"7364, 7923, 9102",$0.00 ,"Social media creates a new opportunity for public health research, giving greater reach at lower cost than traditional survey methods.  Online content offers several potential advantages over traditional survey data; one can in real-time measure how behaviors and attitudes change in response to rare events such as legal changes, new products, and marketing campaigns.  Machine learning techniques for classification can be used to tailor interventions that improve health outcomes while minimizing costs.  However, online content is not a random sample, potentially biasing the outcomes.  This proposal develops techniques to overcome this problem, enabling effective use of publicly available social media data for public health research.  The approaches are evaluated against a traditional survey-based approach to evaluate end-to-end effectiveness in a real-world public health scenario, determining effectiveness of smoking cessation campaigns.<br/><br/>The project builds on well-grounded statistical approaches to eliminate classifier bias.  Key innovations are extending this to the high-dimensional, noisy domain of textual social media data (specifically Twitter), robustness to confounding variables, and scalable methods to identify comparison groups.  Noisy data will be addressed through advancing multiple imputation techniques.  The project will develop a model-based approach to identifying comparison groups that addresses confounding variable issues.  The methods will be evaluated in the context of an actual public health study of smoking cessation, based on historical Twitter data and traditional surveys conducted before and after a CDC campaign as well as a survey of smokers on perceived risk factors of e-cigarettes."
442,1501175,Dissertation Research: Disentangling drivers of community structure and composition of a tropical forest,DEB,POP & COMMUNITY ECOL PROG,6/1/15,3/31/15,David Wilcove,NJ,Princeton University,Standard Grant,Douglas Levey,5/31/17,"$16,277.00 ",Timothy Treuer,dwilcove@princeton.edu,Off. of Research & Proj. Admin.,Princeton,NJ,85442020,6092583090,BIO,1182,"9169, 9179, EGCH, SMET",$0.00 ,"This project seeks to disentangle and characterize drivers of plant and animal community structure and composition in one of the world's largest regenerating tropical forests.  Determining why species are able to exist in a particular time and place (particularly the role of chance vs. inevitability) is simultaneously one of the most central tasks in ecology and a deeply pressing question for conservation in a rapidly changing world.  Understanding these forces in regenerating tropical forest is a particularly timely given (1) the incredible biodiversity of tropical forests, (2) that the proportion of remaining forest classified as regenerating is high and ever-rising, and (3) such knowledge is a prerequisite for deploying ecological restoration as an effective tool for preserving threatened species and ecosystems.<br/><br/>The project team will assess three ultimate drivers of community composition and structure: soil quality, landscape composition, and initial vegetative conditions.  Using a network of vegetative plots, species composition of terrestrial mammal, bat, bird, and singing-insect assemblages will be assessed. Fundamental to this task is developing a set of tools that will allow for the rapid, cheap, and non-invasive assessment of these animal groups.  To this end, ultrasonic and audible-frequency recorders will be deployed, and the research team is developing techniques that will allow for the semi-automated analysis of recordings using machine learning.  To further assess vegetative structure and species interactions as proximate drivers of composition, arrays of synced recorders will be used to determine how key forest structural features and the distribution of some species affects the presence of others."
443,1456404,SBIR Phase II:  A Multi-Modality Sensing Approach for the Objective Assessment of Agitation and Sedation,IIP,SBIR Phase II,4/1/15,7/23/18,Behnood Gholami,NJ,Autonomous Healthcare Inc,Standard Grant,Ruth Shuman,3/31/19,"$925,978.00 ",,bgholami@autonomoushealthcare.com,132 Washington St,Hoboken,NJ,70304692,3477741617,ENG,5373,"116E, 124E, 169E, 5373, 8018, 8032, 8038, 8042, 8240, 9251",$0.00 ,"The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project involves advancements in the state-of-the-art in biomedical technology, improvement of the quality of care for intensive care unit (ICU) patients, and a reduction in healthcare costs. The proposed agitation and sedation system will assist in realizing an improved distribution of ICU workload by allowing the medical staff to focus on patients with increased agitation and in need of immediate attention.  It is expected that this technology will also improve the quality of care in military ICUs. <br/><br/>The proposed project focuses on the development of a multi-modality agitation and sedation assessment system for patients in ICUs. The primary consequence of oversedation is the prolongation of mechanical ventilation and ICU stay. Prolonged ventilation is expensive and is associated with known risks, such as inadvertent extubation, laryngo-tracheal trauma, and, most significantly, ventilator-associated pneumonia. Alternatively, agitated patients can do physical harm to themselves by dislodging vital life support and monitoring devices with excessive musculoskeletal activity. <br/><br/>Undersedation and oversedation result in excessive economic burden on the healthcare system. The annual cost of oversedation in the US is $400 million, whereas reducing mechanical ventilation in oversedated patients by just one day can result in an annual savings of $4 billion in the US alone. Current clinical practice in patient critical care requires the nursing staff to assess the patient?s agitation and sedation state and provide sedatives to ameliorate the patient?s agitation. This process relies on subjective assessments and can be influenced by personal bias. This project proposes to use machine learning algorithms to integrate data from multiple sensing modalities to objectively identify cases of undersedation and oversedation in ICU patients."
444,1524634,EAGER: Cyber-Physical Fingerprinting for Internet of Things Authentication: Accelerating IoT Research and Education Under the Global City Teams Challenge,CNS,CPS-Cyber-Physical Systems,6/15/15,6/9/15,Walid Saad,VA,Virginia Polytechnic Institute and State University,Standard Grant,David Corman,5/31/18,"$150,000.00 ",Sanjay Raman,walids@vt.edu,Sponsored Programs 0170,BLACKSBURG,VA,240610001,5402315281,CSE,7918,"7916, 7918",$0.00 ,"Device authentication and identification has been recently cited as one of the most pressing security challenges facing the Internet of things (IoT). In particular, the open-access nature of the IoT renders it highly susceptible to insider attacks. In such attacks, adversaries can capture or forge the identity of the small, resource constrained IoT devices and, thus, bypass conventional authentication methods. Such attacks are challenging to defend against due to the apparent legitimacy of the adversaries' devices. The primary goal of this research is to overcome this challenge by developing new authentication methods that supplement traditional security solutions with cyber-physical fingerprints extracted from the IoT devices' environment.<br/><br/>This project will develop a novel machine learning framework that enables the IoT to dynamically identify, classify, and authenticate devices based on their cyber-physical environment and with limited available prior data. This will result in the creation of environment-based IoT device credentials that can serve as a means of attestation, not only on the legitimacy of a device's identity, but also on the validity of the physical environment it claims to monitor and the actions it claims to be performing over time. The framework will also encompass an experimental IoT software platform that will be built to validate the proposed research. Owing to a partnership with the NIST Global City Teams Challenge (GCTC) project ""Bringing Internet of Things Know-How to High School Students"", a collaboration with IoT-DC, Arlington County, VA, and other entities, the proposed research will train high school students, STEM educators, and a broad community on a variety of research topics that will include IoT security, cyber-physical systems, and data analytics. The broader impacts will also include the creation of an interdisciplinary workforce focused on securing tomorrow's smart cities."
445,1464205,"CRII: CIF: Measure Estimation from Moments: Theory, Algorithms, and Applications",CCF,Comm & Information Foundations,5/1/15,2/2/15,Gongguo Tang,CO,Colorado School of Mines,Standard Grant,John Cozzens,4/30/17,"$175,000.00 ",,gtang@mines.edu,1500 Illinois,Golden,CO,804011887,3032733000,CSE,7797,"7936, 8228",$0.00 ,"From phase imaging in microscopy to semantic analysis of neural signals, scientific discovery relies on identifying models from massive noisy, incomplete, and corrupted data. One critical challenge for model identification is the nonlinearity inherent in most scientific models and data analysis tasks. However, as suggested by several successful approaches developed across machine learning, statistics, and optimization, nonlinear problems could become linear when ""lifted"" into higher dimensional spaces. Exploring the power and limitations of such ""linearization"" techniques plays an important role in complex model identification.<br/><br/>The research develops a unified framework of formulating nonlinear problems as infinite-dimensional linear problems using measure, a foundational concept of modern mathematics that abstracts the notion of volume and mass.  In many measure estimation problems arising in physical and information sciences, the observations are linear functions of the measure's moments and the true measure describing the system is atomic with a small support. The work first delineates the class of model identification tasks that can be formulated as measure estimation from moments. Notable examples include tensor decomposition and completion, non-negative matrix factorization, and solving high-order multi-variate polynomial equations. Second, the project develops a unified theoretical and computational approach to solve measure estimation using semidefinite programming with guaranteed performance. Finally, the project provides efficient and scalable algorithms for applications in computational optics and large-scale data analysis, where the problems of measure estimation from moments arise frequently."
446,1540285,Network Biology,CCF,SPECIAL PROJECTS - CCF,5/1/15,4/1/15,Richard Karp,CA,University of California-Berkeley,Standard Grant,Mitra Basu,4/30/16,"$25,000.00 ",,karp@cs.berkeley.edu,Sponsored Projects Office,BERKELEY,CA,947101749,5106433891,CSE,2878,"7556, 7931",$0.00 ,"Computational molecular biology has developed dramatically over the last two decades, and is by now an established discipline providing many insights into the mechanisms of disease and the functioning of living cells. Network Biology aims to understand the web of interactions among cellular components, which affect all activities and disease. Only by understanding these interactions (among genes, proteins, RNAs, complexes and other molecules) can a higher level of understanding of organismal function, dynamics and development be achieved. Further, cellular networks provide a holistic framework within which to interpret other high-throughput heterogeneous measurements (e.g., expression levels, mutations, chemical modifications, etc.) that are being collected across organisms, individuals, cell types and conditions, and increasingly at the level of individual cells. <br/><br/>It is expected that this workshop will enhance communication among biologists, medical scientists, computer scientists and mathematicians interested in network biology.<br/><br/>This workshop will bring together a diverse set of scientists focused on inferring and analyzing cellular networks, using a wide range of techniques including wet-lab experiments, graph algorithms, combinatorial optimization, machine learning and statistics. This activity can be of great benefit both to theoretical computer science, via the exposure to and infusion of new problems, and to computational biologists, who will be exposed to state of the art theoretical developments."
447,1540093,CyberSEES: Type 2: Collaborative Research: Tenable Power Distribution Networks,CCF,CyberSEES,1/1/15,3/10/15,George Michailidis,FL,University of Florida,Standard Grant,Rahul Shah,8/31/18,"$244,764.00 ",,gmichail@ufl.edu,1 UNIVERSITY OF FLORIDA,GAINESVILLE,FL,326112002,3523923516,CSE,8211,8208,$0.00 ,"This project advances modeling and computational frameworks to guarantee the sustainability of power distribution networks from environmental, economic, and social perspectives. Environmental sustainability is addressed by accounting for the increased uncertainty associated with the rapid and ad-hoc integration of renewable generation and elastic loads. With regard to economic sustainability, computational and inference foundations are put forth to ensure effective and secure market operations in the face of the imminent emergence of large-scale distribution-level electricity markets. Lastly, social sustainability is effected through cyber innovations focused on increasing economic utility and enhancing cyber security. <br/><br/>The proposed research aims for broad socio-technical advances in electricity distribution networks. Successful project completion will offer cyber innovations that enable the systematic integration of stochastic renewable generation while improving end-user satisfaction. Given the ubiquity of the suite of research tools and methodologies, the utility of the proposed research goes well beyond the envisioned application area to the broader fields of optimization, stochastic processes, control systems, machine learning, statistical signal processing, and cyber security. Broader transformative impact will result from pragmatic test cases proposed for validation, involvement of undergraduates in research, and outreach activities."
448,1538447,Mobile Gait Analysis via Low-power Piezoresponsive Wearable Sensors,CMMI,"Dynamics, Control and System D",9/1/15,5/5/16,Anton Bowden,UT,Brigham Young University,Standard Grant,Irina Dolinskaya,8/31/18,"$386,053.00 ","David Fullwood, Matthew Seeley",abowden@byu.edu,A-285 ASB,Provo,UT,846021231,8014223360,ENG,7569,"030E, 031E, 032E, 033E, 034E, 035E, 116E, 8024, 9150, 9178, 9231, 9251",$0.00 ,"The way that we move has long been recognized as a window into human health; our gait is affected by numerous pathologies, including arthritis, diabetes, and Parkinson's and Alzheimer's diseases. Having access to quantitative gait analysis could be an important tool for pathology prevention, diagnosis, and management. However to date, quantitative gait analysis has been largely confined to the laboratory, due to the cumbersome and expensive nature of multi-camera motion tracking lab equipment.  This project uses novel sensors that employ materials that emit a voltage when subjected to mechanical stress and has the objective of bringing mobile gait analysis to the general public, in their real-world environment.  These wearable sensors are inexpensive, consume little power, and can wirelessly communicate with a nearby smartphone.  In addition, the sensor technology itself promises to be transformational across a broad cross-section of applications including clothing-integrated wearable sensors, self-sensing furniture, smart mounts for vibrating equipment, advanced airbag deployment systems, integrated helmet concussion sensors and self-sensing shoes.  Students from both mechanical engineering and exercise sciences are intimately involved in all aspects of the research to provide a unique cross-disciplinary educational environment.  Furthermore, high school students can apply to receive gait analysis systems to solve interesting science and technology problems.<br/><br/>The piezoresponsive nano-composite sensors used in this project are comprised of a dielectric polymer matrix impregnated with a network of conductive nano-particles. The mechanical and electrical responses of the sensors are independently tailored to provide an optimized system for multifunctional implementation in numerous systems requiring large-deflection sensing capabilities. When connected to a voltage-sensing device (for example, a blue-tooth transmitting Arduino microcomputer) the sensors can track deformation and impact energy and relay it to a nearby smartphone. In order to effectively leverage these sensors for mobile gait analysis, several scientific barriers need to be overcome.  The piezoresponse of the sensors will be modeled using a machine-learning algorithm to enable accurate, continuous self-calibration of the sensors.  An optimized mesh network of sensors will be designed to allow mobile gait analysis and will be validated against traditional gait lab instruments.  Finally, the optimized sensors will be utilized to measure gait in a cross-section of subjects to establish a public baseline database of gait behavior in a variety of real-life environments (i.e., outside of the laboratory)."
449,1527320,SHF: Small: Design/Automation for Synthesizable and Scaling Friendly Analog/Mixed-Signal Circuits,CCF,SOFTWARE & HARDWARE FOUNDATION,6/15/15,6/15/15,Nan Sun,TX,University of Texas at Austin,Standard Grant,Sankar Basu,5/31/18,"$450,000.00 ",David Pan,nansun@mail.utexas.edu,"3925 W Braker Lane, Ste 3.340",Austin,TX,787595316,5124716424,CSE,7798,"7923, 7945",$0.00 ,"The goal of the proposed research is to tackle two critical challenges for conventional analog and mixed-signal circuits to address scaling incompatibility and low design productivity. The proposed time-domain analog mixed signal circuits are scaling compatible, and can achieve higher performance with substantially reduced chip area, power, and cost. They address the critical real-world challenge of cost effectively integrating analog-mixed-signal circuits with digital functions. They can enable emerging applications that demand ultra-low-power and ultra-low voltage solutions. The proposed research design for synthesizability and new synthesis tools also has significant impacts on enhancing design productivity, leading to reduced cost, enhanced capability, shortened time-to-market, and improved reliability. <br/><br/>The proposed novel time domain analog-mixed-signal circuits process analog information in the time or phase domain. Thus, they naturally benefit from CMOS scaling with increased transistor speed and reduced delay. Moreover, this proposal opens up a new research direction of analog-mixed-signal design for synthesizability, the objective of which is to alleviate the difficulty in analog synthesis by intentionally creating new topologies that are synthesis friendly. New analog synthesis tools will be developed, with focus on developing compact and high-fidelity models to reduce runtime and tackle process variation. Several previously untouched issues will be addressed, such as sensitive node protection, current flow constraints, and substrate noise coupling. A new machine-learning based framework will also be developed to further increase automation level."
450,1448440,SBIR Phase I:  Anomaly and malware detection using side channel analysis,IIP,SMALL BUSINESS PHASE I,1/1/15,12/11/14,Denis Foo Kune,MI,Virta Laboratories Inc.,Standard Grant,Peter Atherton,1/31/16,"$150,000.00 ",,denis@virtalabs.com,1395 Folkstone Court,Ann Arbor,MI,481052888,9176215524,ENG,5371,"5371, 8033",$0.00 ,"The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is the improvement of trustworthy software execution.  The proposed system, composed of a hardware monitoring device and a cloud-based security analytics engine, will detect process anomalies and malware in equipment where traditional anti-virus products cannot be installed.  This solution externally observes system activity by analyzing side-channel phenomena such as power consumption.  Unlike traditional anti-virus products that may interfere with normal operations and require constant updates, these side-channel measurements are independent of the software running on commercial equipment such as medical devices or point-of-sale terminals.  The proposed method processes those measurements with a continuous cloud-based machine-learning engine and integrates multiple data sources to provide IT professionals with a reliable source of timely, actionable results.  If successful, this project will help technicians quickly catch anomalous behavior, including malware, before it spreads to other devices.<br/><br/>This Small Business Innovation Research (SBIR) Phase I project explores the independent, nonintrusive detection of anomalous behavior, including malware, on high-assurance computing devices.  Many commercial appliances, such as medical devices, run commodity operating systems but cannot support traditional anti-virus programs that consume precious resources and require frequent database updates.  This incompatibility has resulted in widespread malware infections on equipment at hospitals, retailers and critical facilities.  The proposed research will involve continuously monitoring the power consumption side-channel without disrupting normal operations.  The intellectual merit of the project lies in cloud-based, high-frequency measurements and correlation of equipment behavior in order to quickly and accurately identify anomalous operations at scale.  The goal of the proposed research is to correlate side-channel outputs with system activity across geographically diverse sets of equipment in order to improve anomaly, breach, and malware detection in real-world deployments."
451,1448967,STTR Phase I:  An Intelligent Mental Health Therapy Tool,IIP,STTR PHASE I,1/1/15,12/14/15,Sherry Benton,FL,"TAO Connect, Inc.",Standard Grant,Jesus Soriano Molla,6/30/16,"$269,999.00 ",Parisa Rashidi,sherry.benton@taoconnect.org,747 SW 2nd Avenue STE 258,Gainesville,FL,326016280,3525144094,ENG,1505,"1505, 8018, 8032, 8042",$0.00 ,"The broader impact/commercial potential of this Small Business Technology Transfer, Phase I project is to help make therapy more consistent with patient preferences, beliefs, and values to maximize engagement in therapy and improve patient outcomes.  Therapy for mental health problems is highly effective, yet many patients drop out before getting the full benefit because they are not satisfied or engaged in the therapy.   Therapist Assisted Online (TAO) provides an array of tools for online psychotherapy including educational materials, homework on mobile devices, video conferencing with a therapist, and weekly monitoring of progress. <br/> <br/>The proposed project involves collecting data on all of patients? actions in the TAO system along with their ratings of each activity and their symptom improvement over time.  The research and development team will use this data to create a machine learning system that will make suggestions for best next steps in therapy based on what thousands of other users experienced.   This is the intelligent counseling system.  It will work very similarly to movie streaming services or online book sellers who recommend movies or books to you based on a person?s  past preferences and the preferences of thousands of other users."
452,1513594,"Constrained Statistical Estimation and Inference: Theory, Algorithms and Applications",DMS,STATISTICS,8/1/15,6/29/15,John Lafferty,IL,University of Chicago,Standard Grant,Gabor J. Szekely,8/31/17,"$320,000.00 ",,john.lafferty@yale.edu,6054 South Drexel Avenue,Chicago,IL,606372612,7737028669,MPS,1269,,$0.00 ,"This project lies at the boundary of statistics and machine learning. The underlying theme is to exploit constraints that are present in complex scientific data analysis problems, but that have not been thoroughly studied in traditional approaches. The project will explore theory, algorithms, and applications of statistical procedures, with constraints imposed on the storage, runtime, shape, energy or physics of the estimators and applications. The overall goal of the research is to develop theory and tools that can help scientists to conduct more effective data analysis.<br/><br/>Many statistical methods are purely ""data driven"" and only place smoothness or regularity restrictions on the underlying model. In particular, classical statistical theory studies estimators without regard to their computational requirements. In modern data analysis settings, including astronomy, cloud computing, and embedded devices, computational demands are often central. The project will develop minimax theory and algorithms for nonparametric estimation and detection problems under constraints on storage, computation, and energy. Other constraints to be studied include shape restrictions such as convexity and monotonicity for high dimensional data. The project will also investigate the incorporation of physical constraints through the use of PDEs and models of physical dynamics and mechanics, focusing on both algorithms and theoretical bounds."
453,1447397,SBIR Phase I: Monolithic CMOS-Integration of Electroplated Copper MEMS Inertial Sensors,IIP,SMALL BUSINESS PHASE I,1/1/15,12/3/14,Noureddine Tayebi,CA,InSense Inc,Standard Grant,Muralidharan S. Nair,12/31/15,"$150,000.00 ",,noureddine.tayebi@insenseinc.com,2627 Hanover St,Palo Alto,CA,943041118,6502132012,ENG,5371,"1185, 5371, 8035, 9139, HPCC",$0.00 ,"The broader impact/commercial potential of this project can lead to a revolution in the consumer<br/>electronics market (mobile handsets, tablets, game consoles and wearables), wherein high performance,<br/>low power, small footprint multisensing (not limited to inertial sensing) platforms with timing devices,<br/>are all directly microfabricated on a common ASIC substrate. Sensor fusion can produce unprecedented<br/>user experiences by using data collected from all sensors and processed using machine learning<br/>algorithms. This can further boost the sensor and timing markets that are expected to exceed $6 billion<br/>dollars by 2017. Moreover, the emergent Internet of Things (IoTs) and wearable markets are expected to<br/>reach $20 billion dollars by 2025, which can induce a rapid growth of such intelligent sensor fusion<br/>market. This can have a tremendous societal impact as wearable devices and IoT systems, interfaced with<br/>mobile platforms, can be used to monitor people?s health, safety and energy consumption. Making these<br/>solutions affordable will make it amenable to low income households not only in the US but also around<br/>the world. It will also enable researchers to attain new frontiers of knowledge such as in digital sensory<br/>systems. The long-term goals are to provide such intelligent sensor fusion solutions.<br/><br/>This Small Business Innovative Research (SBIR) Phase I project seeks to demonstrate wafer-scale<br/>microfabrication of Micro-Electro-Mechanical Systems (MEMS) inertial sensors directly on the<br/>application specific integrated circuit (ASIC) substrates, by using electroplated copper (e-Cu) as a<br/>structural material. MEMS inertial sensors, such as gyroscopes and accelerometers, are pervasively used<br/>in consumer electronics and automotive industries. Current trends are, however, requiring higher device<br/>performance with smaller footprints, wherein multi-degree-of-freedom sensors are integrated on the same<br/>package, to enable new capabilities and user experiences. These requirements can be met by<br/>monolithically fabricating inertial sensors on ASIC substrates, which is complex to achieve with silicon<br/>as a structural material. Using e-Cu, which is currently used for ASIC metal interconnects, as the<br/>structural material, can enable easier routing to implement optimized mechanical structures, smaller<br/>dimensions given the high density of copper, extremely low cost as no wafer bonding is required, smaller<br/>form factors, multiple sensors on a single die, and much smaller parasitics providing low noise and higher<br/>performance. Phase I tasks will be to wafer-scale fabricate and characterize e-Cu gyroscopes and<br/>accelerometers with optimal performance parameters and address any drift problems that could emanate<br/>from structural reliability issues associated with the MEMS elements."
454,1520278,SBIR Phase I: Making Web Applications Accessible,IIP,SMALL BUSINESS PHASE I,7/1/15,6/22/15,Yury Puzis,NY,Charmtech Labs LLC,Standard Grant,Muralidharan S. Nair,12/31/15,"$150,000.00 ",,yury.puzis@charmtechlabs.com,1500 Stony Brook Road,Stony Brook,NY,117944600,8885337884,ENG,5371,"5371, 6840, 8035, 9139, HPCC",$0.00 ,"The broader impact/commercial potential of this project include the development of a novel assistive technology that will make dynamic web applications accessible for people with vision impairments. The proposed technology will empower blind people to utilize the immensely popular web applications for social networking, email, online banking, and travel with the same ease-of-use as is experienced by sighted people. The resulting higher productivity of screen-reader users will lead to improved access to education and employment. The proposed technology will be commercialized as part of a screen-reader application for people with vision impairments, a population of up to 25 Million people in the U.S. and a quarter of a million worldwide. The proposed innovation can also be utilized in mainstream technologies that enable web automation and web crawling.<br/><br/>This Small Business Innovation Research (SBIR) Phase I project will investigate the feasibility of making widgets in dynamic web application accessible for blind people. The dynamic nature of modern day web applications has become a serious barrier for blind people who use screen readers to access the Web. Unfortunately, while screen readers can usually narrate all the individual HTML elements in a webpage, they do not recognize widgets, which are a collection of HTML elements forming discrete user interface objects with which the user can interact, e.g., HTML windows, dropdown menus, calendars, suggestion boxes, etc. Inaccessibility of widgets in a web application can make the entire web applications unusable. This project will develop the Widget Accessibility Model that will employ machine learning techniques to identify and classify dynamic widgets in web pages and supply them with generic accessible interfaces. The Model will succinctly represent an assortment of widgets, of varying degrees of complexity, as objects associated with (inheritable) properties and interaction behaviors. WAM will drive the automatic detection of widgets used in a web application and stitch together their composite behavior from their components at runtime. The resulting Model will be incorporated into a commercial screen reader."
455,1519057,SBIR Phase I: Total Holographic Characterization of Colloids Through Holographic Video Microscopy,IIP,SMALL BUSINESS PHASE I,7/1/15,12/1/15,Laura Philips,NY,"Spheryx, Inc",Standard Grant,Ben Schrag,6/30/16,"$179,999.00 ",,laphilips@gmail.com,"330 E 38th St, Apt 48J",New York,NY,100162784,6077380100,ENG,5371,"090E, 5371, 8033",$0.00 ,"This Small Business Innovation Research Phase I project will support the development of a novel approach, based on holographic video microscopy, to analyze the physical properties of colloidal dispersions. This technology will have immediate applications in industries as diverse as pharmaceuticals, cosmetics, personal care products, petrochemicals and food, all of which rely on the properties of colloidal dispersions and the microscopic particles from which they are composed. The worldwide market for particle characterization exceeded $5 billion per year in 2012. The present effort's holographic characterization technology extends the state-of-the-art in particle characterization by providing simultaneous information about both the size and the composition of individual particles in dispersion, and by building up a clear picture of the distribution of properties within a sample without relying on models or assumptions. Access to these new dimensions of information will be useful for product development, process control and quality assurance in all of the industrial sectors that rely on the properties of colloidal materials, thereby increasing opportunities for innovation, enhancing product performance, and decreasing manufacturing costs. In addition to capturing a share of the established market for particle characterization, this new product may also broaden the market by creating new application areas.<br/><br/>The intellectual merit of this project resides in transforming holographic video microscopy from an academic research tool to a powerful commercial instrument. Several innovations are required to make this revolutionary technology commercially viable. In its present incarnation, holographic characterization has been demonstrated with nearly ideal spheres, for which it yields the size to within a nanometer, the complex refractive index to within a part per thousand, and the time-resolved trajectory to within a nanometer in three dimensions. No other particle characterization technique offers such a wealth of particle-resolved information. This Phase I effort will demonstrate the feasibility of holographic particle characterization for a range of non-ideal industrial materials by applying state-of-the-art methods of machine learning to extend the technique's domain of applicability while simultaneously reducing the time per analysis from seconds to tens of milliseconds. This 100-fold acceleration, and the associated reduction in computational cost, will enable the technology to be deployed in large-volume and high-throughput applications. The resulting real-time insights into colloidal dispersions' compositions will improve manufacturing efficiency by identifying and helping to correct process deviations and failures. In so doing it will reduce product costs in all of the industrial sectors that develop and sell colloidal materials."
456,1522054,Collaborative Research: CompSustNet: Expanding the Horizons of Computational Sustainability,CCF,"Info Integration & Informatics, Expeditions in Computing",12/15/15,7/22/20,Carla Gomes,NY,Cornell University,Continuing Grant,Sylvia Spengler,11/30/20,"$7,467,645.00 ","Jon Conrad, John Hopcroft, David Shmoys, Bart Selman",gomes@cs.cornell.edu,373 Pine Tree Road,Ithaca,NY,148502820,6072555014,CSE,"7364, 7723","7364, 7556, 7723, 9102",$0.00 ,"Poverty, saving species, repowering the world with renewable energy, lifting people up to live better lives - there are no easy answers to guiding our planet on the path toward sustainability. Complex problems require sophisticated solutions. They involve intricacy beyond human capabilities, the kind of big-data processing and analysis that only advanced large-scale computing can provide. This NSF Expedition in Computing launches CompSustNet (http://www.compsust.net), a vast research network powered by the nation's recognized university computer science programs, charged with applying the emerging field of computational sustainability to solving the world's seemingly unsolvable resource problems. Put simply, the project will enlist some of the top talents in computing, social science, conservation, physics, materials science, and engineering to unlock sustainable solutions that safeguard our planet's future.<br/><br/>Computational Sustainability is, at its core, the belief that with sufficiently advanced computational techniques, we can devise sustainable solutions that meet the environmental, societal, and economic needs of today while providing for future generations. In much the same way IBM's supercomputer Watson could defeat any challenger in Jeopardy!, computational sustainability posits that a computer-engineered solution can be applied to world's difficult and challenging problems - from helping farmers and herders in Africa survive severe droughts to developing a smart power grid fueled entirely by renewable energy. CompSustNet is a large national and international multi-institutional research network led by Cornell University and including 11 other US academic institutions: Bowdoin, Caltech, CMU, Georgia Tech, Howard University, Oregon State, Princeton, Stanford, UMass, University of South California, and Vanderbilt University, as well as collaborations with several international universities. But CompSustNet is not just an academic enterprise, as it also includes key governmental and non-governmental organizations that specialize in conservation, poverty mitigation, and renewable energy, such as The Nature Conservancy, The World Wildlife Fund, The International Livestock Research Institute, The Trans-African Hydro-Meteorological Observatory, and the National Institute of Standards and Technology.<br/><br/>CompSustNet's core mission is to significantly expand the horizons of computational sustainability and foster the advancement of state-of-the-art computer science to achieve the scale to tackle global problems. Research will focus on cross-cutting computational topics such as optimization, dynamical models, simulation, big data, machine learning, and citizen science, applied to sustainability challenges.  For example, computational sustainability is being put to work to resolve the problem of providing wetlands for shorebirds that migrate from the Arctic through California during a time of drought. As California gets drier, the shorebirds have nowhere to stop, rest, and refuel by eating wetland invertebrates. Scientists are developing new dynamic precision conservation techniques that use complex, big-data models to tackle the problem with NASA satellite imagery, meteorological forecasts, and citizen science in the form of thousands of bird location sightings from the Cornell Lab of Ornithology's eBird checklisting app for birdwatchers. Through partnership with The Nature Conservancy, the program forecasts when and where wetland habitat would be needed for shorebirds, and the Conservancy pays Central Valley rice farmers to flood their fields at opportune times - providing benefits for birds and farmers at a time when extreme drought is making life tough for both. In similar ways, computational sustainability projects will also be hard at work innovating automated monitoring networks to protect endangered elephant population from poachers, promoting the discovery of novel ways to harvest energy from sun light, and designing algorithms to manage the generation and storage of renewable energy in the power grid. <br/><br/>Advancements in computational sustainability will lead to novel, low-cost, high-efficiency strategies for saving endangered species, helping indigenous peoples improve their way of life, and scaling renewables up to meet 21st century energy demand. CompSustNet is like the seed, the venture capital, to help the field of computational sustainability achieve what's possible."
457,1539007,VEC: Small: Collaborative Research: The Visual Computing Database: A Platform for Visual Data Processing and Analysis at Internet Scale,IIS,IIS Special Projects,10/1/15,9/14/17,Patrick Hanrahan,CA,Stanford University,Continuing Grant,Maria Zemankova,9/30/19,"$250,000.00 ",,hanrahan@cs.stanford.edu,450 Jane Stanford Way,Stanford,CA,943052004,6507232300,CSE,7484,002Z,$0.00 ,"This project develops a new parallel computing platform, namely Visual Computing Database, that facilitates the development of applications that require visual data analysis at massive scale. The developed system combines ideas from traditional relational database management systems (to more easily and powerfully organize and manage visual data collections) with modern graphics programming abstractions for efficiently manipulating pixel data. This project implements a prototype of the visual computing database, release it as an open source project to the community, and deploys the system at scale as a service to scientists and researchers on the Google Cloud Platform. There is strong evidence that in domains ranging from personal digital assistants that interpret one's surroundings, to management of critical infrastructure in smart cities, and to scientific data analysis, a fundamental requirement of the next generation of visual and experiential computing (VEC) applications will be the efficient analysis and mining of large repositories of visual data (images, videos, RGBD, etc.). Scaling visual data analysis applications to operate on collections such as the photos and videos on Facebook and YouTube, the traffic cameras in a city, or petabytes of images in a digital sky survey, presents significant computer science challenges due to the size of visual data representations and the computational expense of algorithms understanding and manipulating large image datasets. The difficulty of developing efficient, supercomputing scale applications from scratch inhibits the field's ability to explore advanced data-driven VEC applications. <br/><br/>A central aspect of the project is the design of a new visual data query language that integrates concepts from high performance, functional image processing languages with relational operators and spatial and temporal predicates, providing the ability to execute sequences of complex image/video analysis operations with high efficiency in the database (near the data store). Since visual analysis workloads involve tight integration of data retrieval operations and processing of the result sets (e.g., largescale machine learning, image registration/alignment, and 3D reconstruction), a key design challenge is making the results of database operations easily accessible to non-relational, supercomputing scale computations. All together the project addresses fundamental systems design questions such as: what is a good visual query language for future visual data analysis tasks? How can key operations be implemented efficiently on throughput hardware at scale? What are the appropriate benchmarks for evaluating visual data analysis systems at scale?<br/><br/>URL: http://graphics.cs.cmu.edu/projects/visualdb"
458,1514559,Smoothing Methods in Optimization,DMS,APPLIED MATHEMATICS,9/1/15,9/3/15,James Burke,WA,University of Washington,Standard Grant,Pedro Embid,6/30/20,"$186,999.00 ",,jvburke@uw.edu,4333 Brooklyn Ave NE,Seattle,WA,981950001,2065434043,MPS,1266,,$0.00 ,"This research project concerns mathematical optimization, a field that has experienced explosive growth of over the past thirty years due to its wide applicability in science, engineering, business, and medicine. Contributing factors include the advent of the internet, advances in computational power and computing architectures, the availability of very large data sets, as well as advances in science, engineering, communication, and business. These developments have created a fertile ground for the emergence of new applications and data acquisition modalities, in addition to new methods for data management, interpretation, and modeling. These are the driving forces behind big data and machine learning research. In addition, there is a greater urgency in many disciplines for addressing questions concerning design, efficiency, risk, and inference, as well as model selection, system identification, and error and uncertainty quantification. This research project aims to develop new methods in non-smooth optimization and to study the practical impact of these methods.  The research will emphasize underlying optimization tools, including model development, the design of numerical solution procedures, and the assessment and quantification of model validity, sensitivity, robustness, and uncertainty.<br/><br/>This project concerns the methods and theory associated with the use of smoothing techniques in optimization. In order to extract solutions having pre-specified properties, objective functions in modern optimization problems are often non-differentiable, with the non-differentiability being a key descriptive component. In addition, non-differentiability is present in an essential way when the problem is constrained. Problems possessing non-differentiability appear across a broad spectrum of applications. These include robust statistical modeling, regularization formulations to encode prior information, system identification, sparsity optimization, matrix completion, semi-definite programming, and any problem possessing constraints. In addition, many modern problems are very large scale.  Hence, there is a focus on the development of fast optimization algorithms for large-scale applications in the presence of non-smooth/non-convex objectives. Smoothing methods are designed to approximate these non-smooth problems, with the goal of rapidly obtaining good approximate solutions. This project is devoted to the development and understanding of new and emerging smoothing methods for non-smooth optimization, to providing a mathematical foundation for these methods, and to studying the practical impact of these methods on a range of applications. Primary objectives include (1) developing a framework for convergence analysis, (2) extending results for convex problems to non-convex problems, (3) providing a calculus for smoothing techniques, (4) developing error bounds using duality theory, (5) continued development of the level set method for optimization, and (6) consideration of parametrized optimal value functions."
459,1452068,CAREER: New methods for multivariate analysis in high dimensions,DMS,"STATISTICS, Division Co-Funding: CAREER",7/1/15,6/13/19,Adam Rothman,MN,University of Minnesota-Twin Cities,Continuing Grant,Gabor Szekely,6/30/21,"$400,000.00 ",,arothman@umn.edu,200 OAK ST SE,Minneapolis,MN,554552070,6126245599,MPS,"1269, 8048",1045,$0.00 ,"Datasets from imaging, gene microarray experiments, and many other fields often have more measured characteristics than subjects.  Analyzing these data with standard statistical methods is either impossible or inadequate.  The investigator addresses this problem by developing new statistical methods that are appropriate for such datasets.  The investigator develops theoretical justifications for these new methods and fast computational algorithms for their application.  Software that implements these algorithms will be made available to the public.  These new products will help practitioners in industry create better predictive models and will also help advance research in many other fields.  The investigator will also develop new curricula, including the creation of an undergraduate statistical computing course, an undergraduate statistical machine learning course, a Ph.D.-level topics course, and a new track within the undergraduate statistics major.<br/><br/>Building statistical models when the number of explanatory variables exceeds the sample size is an exciting area at the forefront of multivariate analysis.  Fitting these models with classical techniques is typically impossible and some constraints or penalties must be imposed.  Penalties that encourage zeros in parameter estimates have received substantial attention.  These penalties are useful because they lead to interpretable parameter estimates, but assuming that these zeros exist may be inappropriate in some applications.  The investigator develops and analyzes new methods to fit models in high dimensions that do not require that zeros are present in the parameters of interest, but still allow the practitioner to make simple interpretations of the fit in terms of the measured variables.  This includes the development of new methods to fit multiple response regression models and multinomial logistic regression models, as well as the development of new methods to shrink characteristics of inverse covariance estimates that are needed to fit predictive models.  The investigator will develop new curricula and involve Ph.D. students in the research."
460,1446804,CPS: Synergy: Collaborative Research: Cyber-Physical Approaches to Advanced Manufacturing Security,CNS,"CPS-Cyber-Physical Systems, Cybersecurity Innovation, , , ",6/15/15,9/13/17,Jaime Camelio,VA,Virginia Polytechnic Institute and State University,Cooperative Agreement,David Corman,5/31/19,"$765,247.00 ","Christopher Williams, Lee Wells",jcamelio@vt.edu,Sponsored Programs 0170,BLACKSBURG,VA,240610001,5402315281,CSE,"7918, 8027, O119, P351, Q235","7434, 7918, 8235, 8237",$0.00 ,"The evolution of manufacturing systems from loose collections of cyber and physical components into true cyber-physical systems has expanded the opportunities for cyber-attacks against manufacturing. To ensure the continued production of high-quality parts in this new environment requires the development of novel security tools that transcend both the cyber and physical worlds. Potential cyber-attacks can cause undetectable changes in a manufacturing system that can adversely affect the product's design intent, performance, quality, or perceived quality. The result of this could be financially devastating by delaying a product's launch, ruining equipment, increasing warranty costs, or losing customer trust. More importantly, these attacks pose a risk to human safety, as operators and consumers could be using faulty equipment/products. New methods for detecting and diagnosing cyber-physical attacks will be studied and evaluated through our established industrial partners. The expected results of this project will contribute significantly in further securing our nation's manufacturing infrastructure.<br/><br/>This project establishes a new vision for manufacturing cyber-security based upon modeling and understanding the correlation between cyber events that occur in a product/process development-cycle and the physical data generated during manufacturing. Specifically, the proposed research will take advantage of this correlation to characterize the relationships between cyber-attacks, process data, product quality observations, and side-channel impacts for the purpose of attack detection and diagnosis. These process characterizations will be coupled with new manufacturing specific cyber-attack taxonomies to provide a comprehensive understanding of attack surfaces for advanced manufacturing systems and their cyber-physical manifestations in manufacturing processes. This is a fundamental missing element in the manufacturing cyber-security body of knowledge. Finally, new forensic techniques, based on constraint optimization and machine learning, will be researched to differentiate process changes indicative of cyber-attacks from common variations in manufacturing due to inherent system variability."
461,1451819,The Implicit Content of Sluicing,BCS,Linguistics,6/1/15,4/14/15,Pranav Anand,CA,University of California-Santa Cruz,Standard Grant,Joan Maling,11/30/19,"$375,644.00 ","James McCloskey, Daniel Hardt",panand@ucsc.edu,1156 High Street,Santa Cruz,CA,950641077,8314595278,SBE,1311,"1311, 9251",$0.00 ,"In planning their linguistic expressions, speakers and writers are often able to leave out informationally redundant grammatical material, such as when the verb ""call"" is omitted in ""Fred called, but Sheila didn't"". This process, known as ellipsis, is widespread across the languages of the world, and is particularly common in informal language and dialogue. Among the many varieties of ellipsis is sluicing, where what is omitted is not a verb but an entire sentence. For example, a speaker may choose to leave out the understood sentence ""he called"" after ""why"" in a sentence like: ""He called, but I don't know why"".<br/><br/>Ellipsis poses challenging scientific and engineering problems. Research over the past 50 or 60 years has demonstrated that the principles permitting ellipsis involve many different kinds of information (grammatical structure, the dynamics of the discourse context, and real-world knowledge), but the precise character of these principles and their interaction is still an open question. Progress has been hampered by the lack of a crucial resource type: databases of felicitous uses of ellipsis that are large enough to validate theories against, and rich enough to form the basis for machine learning.<br/><br/>The first goal of this project is to build such a database for sluicing and to make it freely available to language scientists and engineers. This resource will be a large, curated corpus of naturally occurring ellipses, annotated at a level of sophistication that will allow a range of analytic questions to be probed quantitatively. As the curation and annotation proceed, patterns that emerge from the data will be used to investigate the interplay between grammar and context that makes ellipsis possible. Since ellipsis is a pervasive feature of human language, to better understand how it works is to better understand the nature of human linguistic behavior itself. This project should also stimulate technological innovation in an area of urgent need: in designing more sophisticated systems for interfacing with natural human conversation, which is replete with ellipses of every kind."
462,1534534,DMREF: Accelerating the Development of High Temperature Shape Memory Alloys,CMMI,DMREF,9/1/15,9/11/18,Raymundo Arroyave,TX,Texas A&M Engineering Experiment Station,Standard Grant,Mary Toney,8/31/19,"$1,667,133.00 ","Dimitris Lagoudas, Edward Dougherty, Ibrahim Karaman, Ahmed-Amine Benzerga",rarroyave@tamu.edu,400 Harvey Mitchell Pkwy S,College Station,TX,778454645,9798626777,ENG,8292,"024E, 054Z, 7433, 8021, 8400, 9102",$0.00 ,"High Temperature Shape Memory Alloys (HTSMAs) are alloys that exhibit large shape changes at high stresses and high temperatures. If the shape change behavior had be controlled and tailored, HTSMAs can be used as robust and compact solid-state actuators with performance exceeding any other current technology. Since the behavior of HTSMAs is highly dependent on chemistry and processing, tailoring of HTSMAs for specific applications using solely experimental means is unrealistic. This award supports the development of a framework that can allow for the design of chemistry and processing steps to achieve a given performance requirement in these materials. The immediate technological impact of the work is the accelerated development of high-temperature solid-state actuators for the aerospace and automotive industries. Furthermore, the award will expose seven graduate and two to four undergraduate students to a highly interdisciplinary research project, combining ideas from materials science, mechanics, computer science, machine learning and design. The work supports efforts related to the Materials Genome Initiative by integrating experimental and computational research, making digital data accessible, and training the future workforce.<br/><br/>The current investigators and their collaborators have recently discovered that nano-recipitation in NiTiHf HTSMAs leads to unprecedented cyclic stability with reversible phase transformation under significant stresses at elevated temperatures. To accelerate their development, this research team will develop a framework to prescribe the necessary initial composition and subsequent processing schedule of a NiTiHf HTSMA based on arbitrary performance requirements: A two-level physically rigorous modeling approach links chemistry and processing to performance. The first modeling level connects chemistry and processing through a precipitation model, while the second level connects microstructure to shape memory response through a thermodynamics-based micromechanics formulation. Within a Bayesian framework, models are initially calibrated using prior knowledge about the likely value of their parameters. Calibrated models are in turn used to design optimal experiments, that maximize the utility of experiments in terms of information gain or desired materials response, that then lead to enhanced model refinement and predictability. Models are in turn used to optimize shape memory response by prescribing feasible composition plus processing sets taking into account uncertainty in model parameters and heterogeneities in microstructure. The overall framework will be disseminated through conventional channels, while the models, model parameters and data generated through this research will be made available to the wider scientific community through an instance of the Materials Data Curation System developed by the National Institute of Standards and Technology."
463,1521675,Collaborative Research: CompSustNet: Expanding the Horizons of Computational Sustainability,CCF,Expeditions in Computing,12/15/15,2/11/20,Warren Powell,NJ,Princeton University,Continuing Grant,Sylvia Spengler,11/30/20,"$350,000.00 ",,powell@princeton.edu,Off. of Research & Proj. Admin.,Princeton,NJ,85442020,6092583090,CSE,7723,7723,$0.00 ,"Poverty, saving species, repowering the world with renewable energy, lifting people up to live better lives - there are no easy answers to guiding our planet on the path toward sustainability. Complex problems require sophisticated solutions. They involve intricacy beyond human capabilities, the kind of big-data processing and analysis that only advanced large-scale computing can provide. This NSF Expedition in Computing launches CompSustNet (http://www.compsust.net), a vast research network powered by the nation's recognized university computer science programs, charged with applying the emerging field of computational sustainability to solving the world's seemingly unsolvable resource problems. Put simply, the project will enlist some of the top talents in computing, social science, conservation, physics, materials science, and engineering to unlock sustainable solutions that safeguard our planet's future.<br/><br/>Computational Sustainability is, at its core, the belief that with sufficiently advanced computational techniques, we can devise sustainable solutions that meet the environmental, societal, and economic needs of today while providing for future generations. In much the same way IBM's supercomputer Watson could defeat any challenger in Jeopardy!, computational sustainability posits that a computer-engineered solution can be applied to world's difficult and challenging problems - from helping farmers and herders in Africa survive severe droughts to developing a smart power grid fueled entirely by renewable energy. CompSustNet is a large national and international multi-institutional research network led by Cornell University and including 11 other US academic institutions: Bowdoin, Caltech, CMU, Georgia Tech, Howard University, Oregon State, Princeton, Stanford, UMass, University of South California, and Vanderbilt University, as well as collaborations with several international universities. But CompSustNet is not just an academic enterprise, as it also includes key governmental and non-governmental organizations that specialize in conservation, poverty mitigation, and renewable energy, such as The Nature Conservancy, The World Wildlife Fund, The International Livestock Research Institute, The Trans-African Hydro-Meteorological Observatory, and the National Institute of Standards and Technology.<br/><br/>CompSustNet's core mission is to significantly expand the horizons of computational sustainability and foster the advancement of state-of-the-art computer science to achieve the scale to tackle global problems. Research will focus on cross-cutting computational topics such as optimization, dynamical models, simulation, big data, machine learning, and citizen science, applied to sustainability challenges.  For example, computational sustainability is being put to work to resolve the problem of providing wetlands for shorebirds that migrate from the Arctic through California during a time of drought. As California gets drier, the shorebirds have nowhere to stop, rest, and refuel by eating wetland invertebrates. Scientists are developing new dynamic precision conservation techniques that use complex, big-data models to tackle the problem with NASA satellite imagery, meteorological forecasts, and citizen science in the form of thousands of bird location sightings from the Cornell Lab of Ornithology's eBird checklisting app for birdwatchers. Through partnership with The Nature Conservancy, the program forecasts when and where wetland habitat would be needed for shorebirds, and the Conservancy pays Central Valley rice farmers to flood their fields at opportune times - providing benefits for birds and farmers at a time when extreme drought is making life tough for both. In similar ways, computational sustainability projects will also be hard at work innovating automated monitoring networks to protect endangered elephant population from poachers, promoting the discovery of novel ways to harvest energy from sun light, and designing algorithms to manage the generation and storage of renewable energy in the power grid. <br/><br/>Advancements in computational sustainability will lead to novel, low-cost, high-efficiency strategies for saving endangered species, helping indigenous peoples improve their way of life, and scaling renewables up to meet 21st century energy demand. CompSustNet is like the seed, the venture capital, to help the field of computational sustainability achieve what's possible."
464,1526799,"AF: Small: Optimizing with Submodular Set Functions: Algorithms, Integrality Gaps and Structural Results",CCF,Algorithmic Foundations,6/15/15,6/5/15,Chandra Chekuri,IL,University of Illinois at Urbana-Champaign,Standard Grant,Tracy Kimbrel,5/31/20,"$450,000.00 ",,chekuri@cs.illinois.edu,1901 South First Street,Champaign,IL,618207406,2173332187,CSE,7796,"7923, 7926",$0.00 ,"Submodular set functions capture the notion of diminishing returns in an abstract and general way. For this reason they arise in numerous scenarios of interest and have been of much importance in classical combinatorial optimization. In the recent past there has been a substantial interest in optimization problems involving these functions due to a number of applications ranging from machine learning, algorithmic game theory, and information transmission in networks. The project focuses on a some canonical classes of optimization problems where submodularity plays a role in the objective function or constraints. The goal is to design fast and near-optimal algorithms for these canonical problems. Advances in the project will lead to improved algorithms, structural results, and insights into several application areas that were mentioned. The project will support and train two PhD students in the design and analysis of algorithms at the University of Illinois at Urbana-Champaign. The PI will complete a manuscript-length survey on submodular function maximization. The PI will develop and teach a course on recent advances on submodular functions at the University of Illinois. Lecture notes and related material will be made available to the public on the university's website.<br/><br/>The technical focus of the project is to develop fast approximation algorithms for a number of fundamental problems involving submodular functions. The PI plans to build on the mathematical programming approach: relax the discrete optimization problem into a continuous optimization problem followed by appropriate rounding methods which would convert the fractional solution into an integer solution. The project will have four main thrusts.<br/><br/>(i) Constrained Submodular Function Maximization:  The goal is to obtain improved approximation algorithms for maximizing a given non-negative submodular function subject to a variety of independence (down-closed or packing) constraints. <br/><br/>(ii) Constrained Submodular Function Minimization:  The goal is to obtain improved approximation algorithms for minimizing a given non-negative submodular function subject to a variety of covering and allocation constraints.<br/><br/>(iii) Faster algorithms:  The goal is to obtain algorithms that are significantly faster than existing ones. The project will examine sequential algorithms as well as algorithms in the streaming and map-reduce models of computation.<br/><br/>(iv) Submodularity in Information Transmission:  The goal is to understand the capacity of networks for information transmission via flow-cut gaps in polymatroidal networks."
465,1557642,QuBBD: Collaborative Research: Interactive Ensemble clustering for mixed data with application to mood disorders,DMS,,9/15/15,9/11/15,Ellen Eischen,OR,University of Oregon Eugene,Standard Grant,Nandini Kannan,8/31/17,"$19,500.00 ",,eeischen@uoregon.edu,5219 UNIVERSITY OF OREGON,Eugene,OR,974035219,5413465131,MPS,O402,,$0.00 ,"The Big Data era has given rise to data of unprecedented size and complexity.  However, fully leveraging Big Data resources for knowledge and discovery is an open challenge due to the fact that conventional methods of data processing and analysis often fail or are inappropriate.  This project develops an innovative approach that utilizes Big Data to improve the classification of mood disorders for the purpose of improving diagnosis and outcomes for psychiatric patients.  Big Data issues are inherently more severe for mental disorders because of their elusive nature.  The psychiatric community has recognized the critical need for a more precise, evidence-based approach for the diagnosis and treatment of disease.  In fact, recent studies funded by the National Institute of Mental Health (NIMH) have found that psychiatric interventions were effective in less than 25% of patients presenting with an acute episode.  This low efficacy rate is especially problematic given the prevalence of mental disorders.  Mood disorders alone (e.g., depression) will be experienced by 1 in 5 adults in the United States at some point in their lives.  This project is motivated by the hypothesis that a more precise and personalized classification of mental health disease can be obtained through the development of novel clustering methods that identify clinically significant structures with these large population data sets.  However, such an approach must overcome a large number of methodological challenges introduced by the complexity of the problem and the nature of large-scale real-world electronic health data.  These challenges include, among others, complex and unknown structure, high dimensionality, heterogeneity, complex mixtures of variables, missing data, and sparsity.  <br/><br/>This award supports initiation of a collaborative research project, carried out by a team with interdisciplinary and complimentary skill sets, to develop methods for big data that address challenges inherent in the integration of biomedical data of this type.  Collective expertise of the team spans the areas of biomedical informatics, biostatistics, computer and information science, electrical and computer engineering, mathematics, and psychiatry.  A novel methodology is developed in a flexible and fully integrated framework that can be extended to other biomedical data and diseases.  Within this framework, clustering methods that capture different aspects of relatedness in the data are integrated in a rigorous way that not only accounts for model uncertainty, but also results in an interactive visualization that is accessible with strong model interpretability for the non-expert.  Specifically, the methodology will rely on novel modifications to bootstrap estimators of generalization error for the purpose of assembling a consensus over an ensemble of clusters inferred from topology-based and machine learning approaches.  The framework also supports iterative refinement of the consensus solution based on user input (via the visualization) to incorporate domain expertise.  The rigorous identification of sub-groups of individuals within heterogeneous populations will facilitate accurate and targeted diagnosis for mood disorders, and provide opportunity for personalized evidence-based interventions.   Applications focus on clustering individuals with mood disorders (bipolar disorder and major depression) from data collected in the Bipolar Disorder Research Network (BDRN).  Despite this focus, the methodology is generalizable to other diseases that face similar challenges for diagnosis and treatment.  In fact, this project supports the first steps of a long-term vision of generalizing the methods to more complex and less curated data, such as electronic health records, social media, and other sources.  This award is supported by the National Institutes of Health Big Data to Knowledge (BD2K) Initiative in partnership with the National Science Foundation Division of Mathematical Sciences."
466,1440547,SI2-SSE: Genetic Algorithm Software Package for Prediction of Novel Two-Dimensional Materials and Surface Reconstructions,OAC,"DMR SHORT TERM SUPPORT, Software Institutes",1/1/15,7/24/14,Richard Hennig,NY,Cornell University,Standard Grant,Bogdan Mihaila,12/31/18,"$344,696.00 ",,rhennig@ufl.edu,373 Pine Tree Road,Ithaca,NY,148502820,6072555014,CSE,"1712, 8004","7237, 7433, 8005, 8400, 9216",$0.00 ,"The ability to control structure and composition at the nanoscale has introduced exciting scientific and technological opportunities. Advances in the creation of nanomaterials such as single-layer materials and nanocrystals have led to improved understanding of basic structure-property relationships that, in turn, have enabled impressive progress in a broad range of nanotechnologies with applications for energy storage, catalysis and electronic devices. Yet, significant knowledge gaps persist in what single-layer materials could be synthesized and in our understanding of the nature of the surfaces of nanocrystals, particularly in the complex environment of solvents and ligands. The discovery of potentially stable novel single-layer materials and the prediction of nanocrystal surface structures are arguably among the most critical aspects of nanoscale materials. This research will provide the computational tools for the detailed prediction of the structure of two-dimensional materials and nanostructure surfaces in complex environments. This will impact the development and the design of novel nanomaterials with properties optimized for applications ranging from catalyst for chemical reactions, to energy conversion materials, to low-power and high-speed electronic devices.<br/> <br/>Progress in the field requires better computational methods for structure prediction. This project will (i) transform the Genetic Algorithm for Structure Prediction (GASP) software package developed by the PI into a sustainable scientific tool, (ii) extend its functionality to 2D materials and materials interfaces, and (iii) increase its performance by coupling to surrogate energy models that are optimized on the fly. These complementary goals will be achieved through expansion of the developer and user base, transition to portable software interfaces and data structures, and the addition of modular algorithms for functionality and performance enhancements. To enhance the functionality, the GASP algorithms will be extended to two two-dimensional materials and materials surfaces with adsorbates and ligands. To enhance the performance of the genetic algorithm, the optimization approach will be coupled to surrogate energy models such as machine-learning techniques and empirical energy models that are optimized on the fly. The publication of user tutorials, and documentation on the data structures and software interfaces will enhance the GASP codes overall utility, increase the user and developer base, and enable further extension to other data-mining and structure prediction approaches. The students involved in this project will receive extensive training and experience in algorithm development, scientific computation, and structure/property determination of complex nanomaterials. As part of the education and outreach component of the project, the PI will develop a course module on Materials Structure Predictions and widely distribute it. A weeklong workshop for students and postdocs in the third year of the project on Materials Discovery and Design will broaden the research?s impact beyond the creation of new software and the discovery of novel single-layer materials and nanocrystal surface and ligand configurations."
467,1546402,Bilateral NSF/BIO-BBSRC - Linking Cell Growth with Proliferation in the Plant Root Meristem,MCB,Genetic Mechanisms,8/1/15,7/20/15,Albrecht von Arnim,TN,University of Tennessee Knoxville,Standard Grant,Manju Hingorani,7/31/20,"$706,344.00 ",Michael Gilchrist,vonarnim@utk.edu,1331 CIR PARK DR,Knoxville,TN,379163801,8659743466,BIO,1112,"1112, 9109, 9150, 9179",$0.00 ,"The goal of this collaborative US/UK project that engages researchers at the University of Tennessee and the University of London is to better understand how plants coordinate the biosynthesis (production) of proteins with cell division as they grow. This is important because plants must carefully integrate information about available nutrients and their energy supply before deciding whether or not to invest limited resources into growth and the irreversible production of new cells. The work will lead to insights into the physiological and molecular processes that underpin the agricultural productivity of crop plants and will help to optimize plant function and crop productivity by genetic improvement. The project will develop the scientific workforce by training postdoctoral scientists, PhD students, as well as affiliated junior investigators with advanced multi-disciplinary skills at the interface of experimental and computational systems biology, skills that are highly portable in the academic and industrial sectors of the life sciences. Outreach to the general public will also be performed, in collaboration with an artist that uses plant life-related themes in her installations.<br/><br/><br/>Protein synthesis (translation) is a major sink for the carbon and nitrogen compounds that, once assimilated through photosynthesis, drive cell growth and proliferation. The project seeks to decipher how protein synthesis-driven cell growth is connected to cell proliferation in meristematic plant cells, using the root tip of the plant reference species Arabidopsis as an experimental system. An underlying hypothesis that shall be tested is that growth regulatory signaling pathways coordinate both the cell cycle and protein synthesis in response to growth stimulating signals. To this end, cell biological markers of cell proliferation will be imaged over time in strains harboring genetic lesions in key signaling pathways. Data on translational efficiency will be collected using genome-wide techniques in order to identify the targets of translational regulation in actively growing tissues. Translation data will be fitted to an emergent computational model of mRNA translation in order to derive biochemical parameters of translation such as the initiation rate. Finally, a network model will integrate new and published data to predict cell cycle transitions in response to the signals that drive cell proliferation and translation. The project is a collaboration between two experimental labs, who have complementary expertise in the regulatory processes that underpin cell division and protein translation. The two teams are joined by two computational investigators, who likewise contribute complementary expertise in machine learning, statistical modeling, pattern recognition and dynamic modeling using deterministic and probabilistic algorithms. <br/><br/>This collaborative US/UK project is supported by the US National Science Foundation and the UK Biotechnology and Biological Sciences Research Council."
468,1454817,CAREER: Maximum likelihood and nonparametric empirical Bayes methods in high dimensions,DMS,"STATISTICS, Division Co-Funding: CAREER",8/1/15,8/2/19,Lee Dicker,NJ,Rutgers University New Brunswick,Continuing Grant,Gabor Szekely,7/31/20,"$400,000.00 ",,ldicker@stat.rutgers.edu,33 Knightsbridge Road,Piscataway,NJ,88543925,8489320150,MPS,"1269, 8048",1045,$0.00 ,"The investigator is combining classical and elegant ideas from statistics (empirical Bayes, mixture models, and nonparametric maximum likelihood), with important recent breakthroughs in computing to help develop a rigorous, practical framework for many problems in modern data analysis.  Applications in genomics and other areas of biology where high-throughput data are generated form an important part of the project.  Beyond biology, the methods developed during the course of the project are expected to have applications in finance (e.g. fraud detection), machine learning (e.g. speech, text, and pattern recognition), and other fields where vast high-dimensional datasets are being rapidly generated and require accurate, incisive analysis. Another important aspect of the project addresses questions about reproducibility, which have come to the forefront in many applications involving high-dimensional data analysis.  To address these questions, the investigator is studying fundamental properties of statistical risk and risk estimation in high dimensions.  Algorithms and methods developed during the course of the project are being implemented in easy-to-use and freely available software packages.  Project research is closely integrated with education, via graduate student training and newly developed courses for graduate and undergraduate students.<br/><br/>The main objective of the project is to develop new methodologies, computational strategies, and theoretical results for the use of nonparametric maximum likelihood (NPML) techniques and empirical Bayes methods in high-dimensional data analysis.  This work is fundamentally related to the analysis of nonparametric mixture models.  Empirical Bayes methods have a long and rich history in statistics, and are particularly well-suited to high-dimensional problems.  Moreover, recent computational results and convex approximations have greatly simplified the implementation of NPML-based methods.  Leveraging these computational breakthroughs, the investigator is developing novel and scalable NPML-based methods for high-dimensional classification, high-dimensional regression, and other statistical problems.  New still-faster algorithms for computing NPML estimators, which take advantage of certain types of sparsity in the estimated mixing-measure, are also being developed.  The investigator is studying theoretical properties of the proposed methods in high-dimensional settings.  Areas of emphasis for theoretical analysis include convergence rates and frequentist risk properties of the proposed empirical Bayes methods."
469,1552163,CPS: Synergy: Sensor Network-Based Lower-Limb Prosthetic Optimization and Control,CNS,CPS-Cyber-Physical Systems,5/16/15,8/1/16,Ou Bai,FL,Florida International University,Standard Grant,Sylvia Spengler,11/30/20,"$917,925.00 ",,obai@fiu.edu,11200 SW 8TH ST,Miami,FL,331990001,3053482494,CSE,7918,"7918, 8235, 9251",$0.00 ,"More than one million people including many wounded warfighters from recent military missions are living with lower-limb amputation in the United States. This project will design wearable body area sensor systems for real-time measurement of amputee's energy expenditure and will develop computer algorithms for automatic lower-limb prosthesis optimization. The developed technology will offer a practical tool for the optimal prosthetic tuning that may maximally reduce amputee's energy expenditure during walking. Further, this project will develop user-control technology to support user's volitional control of lower-limb prostheses. The developed volitional control technology will allow the prosthesis to be adaptive to altered environments and situations such that amputees can walk as using their own biological limbs. An optimized prosthesis with user-control capability will increase equal force distribution on the intact and prosthetic limbs and decrease the risk of damage to the intact limb from the musculoskeletal imbalance or pathologies. Maintenance of health in these areas is essential for the amputee's quality of life and well-being.  Student participation is supported.<br/><br/>This research will advance Cyber-Physical Systems (CPS) science and engineering through the integration of sensor and computational technologies for the optimization and control of physical systems. This project will design body area sensor network systems which integrate spatiotemporal information from electromyography (EMG), electroencephalography (EEG) and inertia measurement unit (IMU) sensors, providing quantitative, real-time measurements of the user's physical load and mental effort for personalized prosthesis optimization. This project will design machine learning technology-based, automatic prosthesis parameter optimization technology to support in-home prosthesis optimization by users themselves. This project will also develop an EEG-based, embedded computing-supported volitional control technology to support user?s volitional control of a prosthesis in real-time by their thoughts to cope with altered situations and environments. The technical advances from this project will provide wearable and wireless body area sensing solutions for broader applications in healthcare and human-CPS interaction applications. The explored computational methods will be broadly applicable for real-time, automatic target recognition from spatiotemporal, multivariate data in CPS-related communication and control applications. This synergic project will be implemented under multidisciplinary team collaboration among computer scientists and engineers, clinicians and prosthetic industry engineers. This project will also provide interdisciplinary, CPS relevant training for both undergraduate and graduate students by integrating computational methods with sensor network, embedded processors, human physical and mental activity recognition, and prosthetic control."
470,1518789,SHF:Large:Collaborative Research: Inferring Software Specifications from Open Source Repositories by Leveraging Data and Collective Community Expertise,CCF,Software & Hardware Foundation,7/1/15,6/17/15,Gary Leavens,FL,The University of Central Florida Board of Trustees,Standard Grant,Sol Greenspan,6/30/18,"$320,000.00 ",,leavens@cs.ucf.edu,4000 CNTRL FLORIDA BLVD,Orlando,FL,328168005,4078230387,CSE,7798,"7925, 7944",$0.00 ,"Today individuals, society, and the nation critically depend on software to manage critical infrastructures for power, banking and finance, air traffic control, telecommunication, transportation, national defense, and healthcare. Specifications are critical for communicating the intended behavior of software systems to software developers and users and to make it possible for automated tools to verify whether a given piece of software indeed behaves as intended. Safety critical applications have traditionally enjoyed the benefits of such specifications, but at a great cost.  Because producing useful, non-trivial specifications from scratch is too hard, time consuming, and requires expertise that is not broadly available, such specifications are largely unavailable. The lack of specifications for core libraries and widely used frameworks makes specifying applications that use them even more difficult. The absence of precise, comprehensible, and efficiently verifiable specifications is a major hurdle to developing software systems that are reliable, secure, and easy to maintain and reuse. <br/><br/>This project brings together an interdisciplinary team of researchers with complementary expertise in formal methods, software engineering, machine learning and big data analytics to develop automated or semi-automated methods for inferring the specifications from code. The resulting methods and tools combine analytics over large open source code repositories to augment and improve upon specifications by program analysis-based specification inference through synergistic advances across both these areas. <br/><br/>The broader impacts of the project include: transformative advances in specification inference and synthesis, with the potential to dramatically reduce, the cost of developing and maintaining high assurance software; enhanced interdisciplinary expertise at the intersection of formal methods software engineering, and big data analytics; Contributions to research-based training of a cadre of scientists and engineers with expertise in high assurance software."
471,1525889,NRI: Collaborative Research: Versatile Locomotion: From Walking to Dexterous Climbing with a Human-Scale Robot,IIS,NRI-National Robotics Initiati,9/1/15,8/6/15,Mark Cutkosky,CA,Stanford University,Standard Grant,David Miller,8/31/19,"$450,000.00 ",,cutkosky@stanford.edu,450 Jane Stanford Way,Stanford,CA,943052004,6507232300,CSE,8013,8086,$0.00 ,"The project aims to give legged robots the skills to navigate a wide variety of terrain.  This capability is needed to employ robots in applications such as search-and-rescue, construction, and exploration of remote environments on Earth and other planets. The multidisciplinary team, composed of researchers at Duke, Stanford, UC Santa Barbara, JPL, and Motiv Robotics, will develop a robot to climb a variety of surfaces ranging from flat ground to overhanging cliffs. Using an array of sensors, unique hands, and sophisticated algorithms, the robot will dynamically adopt walking, crawling, climbing, and swinging strategies to traverse wildly varied terrain. During the course of this research, the team hopes to achieve the milestone of the first demonstration of a human-scale rock climbing robot. The research is also expected to lead to insights into cognitive and biomechanical processes in human and animal locomotion.<br/><br/>Although rock climbing serves as an ideal proving ground for the work, this project conducts basic research to address more a general-purpose goal; namely, to provide the physical and cognitive skills for robots to adaptively navigate varied terrain. It takes a dexterous climbing approach, which uses non-gaited, coordinated sequences of contact to move the body, much as dexterous manipulation uses contact with the fingers and palm to move an object.  It will apply principles from optimization, machine learning, bioinspiration, and control theory to make intellectual contributions in several domains, such as robot hand design, planning algorithms, balance strategies, and locomotion performance measurement.  Novel grippers, sensor-based planning strategies, reactive maneuvers, and locomotion metrics will be developed during the course of this research."
472,1551057,EAGER: NDN-Hadoop: Exploring Applicability of NDN for Big-Data Computing,CNS,"Special Projects - CNS, CSR-Computer Systems Research",8/15/15,1/26/18,Christopher Gniady,AZ,University of Arizona,Standard Grant,Marilyn McClure,7/31/18,"$232,000.00 ",Beichuan Zhang,gniady@cs.arizona.edu,888 N Euclid Ave,Tucson,AZ,857194824,5206266000,CSE,"1714, 7354","7916, 9251",$0.00 ,"Large-scale distributed processing of huge amount of data is the underpinning technology in the era of Big Data. The Apache Hadoop, an open-source software platform, has become the go-to technology for running multiple types of distributed applications such as Web indexing, data mining, business intelligence analysis, machine learning, scientific simulation, and bioinformatics research. This wide range of applications requires Hadoop to be flexible and adaptive to different network environments while providing high performance for each application, resulting in complicated protocol design, implementation, a large number of parameters to tune, and often unsatisfactory performance in real deployment. An emerging network architecture, Named Data Networking (NDN) shifts the focus from point-to-point communication to general content retrieval, a much better fit to Big Data computing than traditional TCP/IP, the communication protocol of the internet. NDN can potentially offer tremendous benefits in simplifying deployments, improving robustness and performance for Hadoop systems. <br/><br/>This project investigates the feasibility of integrating NDN and Hadoop to build an NDN-Hadoop system that will provide a robust, efficient, and scalable foundation for Big Data computing. Since Hadoop is a complicated ecosystem and NDN is being actively developed, there are many questions to be answered regarding how they may work together and how to exploit the potential benefits to the greatest extent. More specifically, this project will (1) evaluate the applicability of NDN in Hadoop infrastructure by understanding its benefits and quantifying performance gains of Hadoop running on top of NDN, (2) design novel Hadoop mechanisms to take full advantage of NDN and the information it provides for better performance and reliability, and (3) develop and test a complete system of Hadoop running on top of NDN to create a platform for future research.<br/><br/>Introducing NDN into datacenter-scale networking and computing will have profound impacts on the design of large data centers, distributed computing infrastructures, and multiple research communities. The project will produce the first data-centric Hadoop system, which will benefit all kinds of applications that use Hadoop. The results can also be translated into other Big Data computing platforms, broadening the impacts even further. It provides not only new research directions to the network and system community, but also performance enhancements to scientific computing community and data analytics. The experiences gained in this project will feedback to NDN research, contributing to the development of future Internet. The project offers great education and training opportunities for graduate and undergraduate students."
473,1507078,Collaborative Research: Smoothing Spline Semiparametric Density Models,DMS,STATISTICS,8/1/15,7/30/15,Anna Liu,MA,University of Massachusetts Amherst,Standard Grant,Gabor Szekely,7/31/19,"$129,265.00 ",,anna@math.umass.edu,Research Administration Building,Hadley,MA,10359450,4135450698,MPS,1269,,$0.00 ,"A probability density function of multiple variables describes the likelihood of different values the variables can jointly take, therefore, contains full information regarding the distribution of individual variables and their interactions. Given observed data of the random variables, density estimation is at the heart of Statistics and machine learning, where the classical problems such as regression, variable selection, clustering, and dimension reduction, can all be cast into a density estimation problem.  Advanced density estimation methods are therefore essential for the extraction of as much information as possible from the data. There has been lack of systematic research in flexible density estimation with high dimensional data or complex data such as clustered data. The overall goal of this project is to develop a smoothing spline based systematic framework that allows for flexible density model building for complex and high dimensional data. As such data arise from a wide range of applications, the results of this proposed research are useful for researchers from a wide range of fields. In particular, the proposed methods will be applied to analyze data in health and medicine, speech, environmental change, food and computer sciences, in collaboration with researchers in these areas. High-performance computing tools will be developed as a result of this research and made publicly available.<br/><br/>This project adopts a semi-parametric approach that combines advantages of parametric and nonparametric methods. Flexible and general semi-parametric density and conditional density models for independent and clustered data will be developed and studied. Regularization methods for adaptive density estimation, variable selection in high dimensional conditional density estimation and interaction selection in semi-parametric graphical models will be developed. Nonparametric components will be modeled using reproducing kernel Hilbert spaces which can deal with different density models on different domains with different penalties in a unified fashion. The semiparametric density models considered in this project contain most existing semiparametric density models as special cases as well as many new interesting models. Many methods in this project for adaptive estimation, model/variable selection, model diagnostics and inference are new. These novel methodologies constitute advances in density estimation."
474,1565233,"EAGER: Using Search Engines to Track Impact of Unsung Heroes of Big Data Revolution, Data Creators",IIS,"Info Integration & Informatics, NSF Public Access Initiative",10/1/15,8/4/16,Adam Godzik,CA,University of California-San Diego,Standard Grant,Sylvia Spengler,5/31/19,"$300,000.00 ",,adam@godziklab.org,Office of Contract & Grant Admin,La Jolla,CA,920930621,8585344896,CSE,"7364, 7414","7364, 7916",$0.00 ,"Efficient mechanisms of data exchange are increasingly central to science (and society) in the midst of the deluge of complex data. The pace of data production and its complexity mean that a large amount of data is often not adequately analyzed by the data producers, and thanks to its rapid dissemination, the broad community is participating in its analysis. This model, however, carries a risk of neglecting the input of the original data creators, as attention shifts to data integrators and analyzers. The current paradigm of information dissemination and assigning credit in science, based on peer-reviewed publications and formal acknowledgment of third-party contributions in the form of citations, is biased toward high-profile, well-known scientists and research centers who participate in the latter stages of knowledge creation and dissemination. We propose to use unbiased internet searches to identify uncredited use of datasets and resources in research literature, allowing data creators and researchers participating in early stages of data creation to claim credit for their work. Using machine-learning and text-mining techniques, the PI seeks to extract relevant information from noisy results of general-purpose search engines and develop easy-to-use interfaces for public use of such resources to supplement official bibliometric resources.<br/><br/>Acceptance and citation biases have a significant impact on careers of researchers outside the central foci of funding and publications, which are also typically places with more-diverse research forces. First, the probability of rejection in peer review is significantly biased against less-famous scientists and those at less-research-intensive institutions. These biases are less likely to affect data creation as databases typically accept data without peer review and the value of data can be measured by its use. The same biases affect number of citations, where people tend to cite more-famous, established scientists, or cite reviews that are often invited and only leading scientists would have been invited to write the review. As a result, both publications and citations are heavily biased toward already-recognized scientists that represent less-diverse populations, both in personal terms and in terms of the institutions where they work, as compared to the general population of scientists. Such biases affect careers and ability to obtaining grant funding for young scientist operating outside of the tight collaboration networks at best research institutions and creates a classical rich getting richer and poor getting poorer loop. The new internet-based information exchange paradigm has already had a profound effect on researchers? ability to getting their results in prominent, public view. This proposal aims at approaches that would further alleviate publication and citation bias, not by addressing it directly, but by developing more openways of evaluating contributions to science."
475,1526674,III: Small: Collaborative Research: Reducing Classifier Bias in Social Media Studies of Public Health,IIS,Info Integration & Informatics,8/1/15,8/12/15,Aron Culotta,IL,Illinois Institute of Technology,Standard Grant,Maria Zemankova,1/31/19,"$304,725.00 ",,culotta@cs.iit.edu,10 West 35th Street,Chicago,IL,606163717,3125673035,CSE,7364,"7364, 7923",$0.00 ,"Social media creates a new opportunity for public health research, giving greater reach at lower cost than traditional survey methods.  Online content offers several potential advantages over traditional survey data; one can in real-time measure how behaviors and attitudes change in response to rare events such as legal changes, new products, and marketing campaigns.  Machine learning techniques for classification can be used to tailor interventions that improve health outcomes while minimizing costs.  However, online content is not a random sample, potentially biasing the outcomes.  This proposal develops techniques to overcome this problem, enabling effective use of publicly available social media data for public health research.  The approaches are evaluated against a traditional survey-based approach to evaluate end-to-end effectiveness in a real-world public health scenario, determining effectiveness of smoking cessation campaigns.<br/><br/>The project builds on well-grounded statistical approaches to eliminate classifier bias.  Key innovations are extending this to the high-dimensional, noisy domain of textual social media data (specifically Twitter), robustness to confounding variables, and scalable methods to identify comparison groups.  Noisy data will be addressed through advancing multiple imputation techniques.  The project will develop a model-based approach to identifying comparison groups that addresses confounding variable issues.  The methods will be evaluated in the context of an actual public health study of smoking cessation, based on historical Twitter data and traditional surveys conducted before and after a CDC campaign as well as a survey of smokers on perceived risk factors of e-cigarettes."
476,1448289,SBIR Phase I:  Using Data Mining to Optimally Customize Therapy for Individuals with Autism,IIP,SBIR Phase I,1/1/15,6/22/15,John Nosek,PA,Guiding Technologies Corporation,Standard Grant,Jesus Soriano Molla,12/31/15,"$169,999.00 ",,johnnosek@verizon.net,1500 JFK Blvd Suite 1825 2 Penn,Philadelphia,PA,191021710,6096059273,ENG,5371,"163E, 5371, 8018, 8032, 8042, 8240",$0.00 ,"The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project includes innovations in data mining and the treatment of autism.  Applied Behavior Analysis (ABA) therapy is the gold standard in treating autism.  Applying data analytics to data from ABA therapy sessions will contribute in several important ways:  a) patterns may be discerned across individuals with autism to better understand variations in autism and create therapies to target these differences; b) patterns may be matched with other data, such as genomic data, to identify cross-patterns that may be useful in better understanding autism and ways to improve therapy; and c) the frontiers of data mining will be expanded to provide guidance in real time. This project will have the following societal impacts: 1) many more individuals with autism across the globe will receive early, quality, cost-effective treatment regimens that will enable them to live more fulfilled lives and reach their full potential; 2) families whose children are good candidates for treatment and receive it will experience reduced stress and better family life; and 3) the additional lifetime cost of not effectively treating children with autism, which is approximately ten-fold the cost of treatment, will be reduced.<br/><br/>The proposed project is to extract informative sequential patterns from trial sequences of an individual student, use them to accurately predict trial outcomes, and utilize the predictive model to provide individualized recommendations about how to modify trials and steps of student training. To achieve this goal, predictive data mining will be used. To develop accurate predictive models, the project will build on a large body of recent work in machine learning on temporal predictive modeling and sequential pattern mining, including some of the previous results of the project team. Special attention will be paid to the recent work in educational data mining and intelligent tutoring. Specific key objectives include: 1) Representation of Trial Data for Predictive Modeling: how to represent the raw sequential data in a way that is most suitable for prediction modeling; 2) Development of Models for Prediction of Trial Outcomes: which model is the most suitable for prediction of outcomes in sequential trials and how to train a prediction model from highly-dimensional multi-therapy recipient sequential data; and 3) Guiding Therapy of a Child with Autism Based on an Early Classification Model: how to adjust and extend the previously developed approach by the project team to guide trials."
477,1539069,VEC: Small: Collaborative Research: The Visual Computing Database: A Platform for Visual Data Processing and Analysis at Internet Scale,IIS,Information Technology Researc,10/1/15,9/22/15,Kayvon Fatahalian,PA,Carnegie-Mellon University,Continuing grant,Maria Zemankova,9/30/18,"$110,228.00 ",,kayvonf@cs.stanford.edu,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,CSE,1640,002Z,$0.00 ,"This project develops a new parallel computing platform, namely Visual Computing Database, that facilitates the development of applications that require visual data analysis at massive scale. The developed system combines ideas from traditional relational database management systems (to more easily and powerfully organize and manage visual data collections) with modern graphics programming abstractions for efficiently manipulating pixel data. This project implements a prototype of the visual computing database, release it as an open source project to the community, and deploys the system at scale as a service to scientists and researchers on the Google Cloud Platform. There is strong evidence that in domains ranging from personal digital assistants that interpret one's surroundings, to management of critical infrastructure in smart cities, and to scientific data analysis, a fundamental requirement of the next generation of visual and experiential computing (VEC) applications will be the efficient analysis and mining of large repositories of visual data (images, videos, RGBD, etc.). Scaling visual data analysis applications to operate on collections such as the photos and videos on Facebook and YouTube, the traffic cameras in a city, or petabytes of images in a digital sky survey, presents significant computer science challenges due to the size of visual data representations and the computational expense of algorithms understanding and manipulating large image datasets. The difficulty of developing efficient, supercomputing scale applications from scratch inhibits the field's ability to explore advanced data-driven VEC applications. <br/><br/>A central aspect of the project is the design of a new visual data query language that integrates concepts from high performance, functional image processing languages with relational operators and spatial and temporal predicates, providing the ability to execute sequences of complex image/video analysis operations with high efficiency in the database (near the data store). Since visual analysis workloads involve tight integration of data retrieval operations and processing of the result sets (e.g., largescale machine learning, image registration/alignment, and 3D reconstruction), a key design challenge is making the results of database operations easily accessible to non-relational, supercomputing scale computations. All together the project addresses fundamental systems design questions such as: what is a good visual query language for future visual data analysis tasks? How can key operations be implemented efficiently on throughput hardware at scale? What are the appropriate benchmarks for evaluating visual data analysis systems at scale?<br/><br/>URL: http://graphics.cs.cmu.edu/projects/visualdb"
478,1534850,Geometry of Convex Optimization,CMMI,"OE Operations Engineering, CDS&E-MSS",8/1/15,7/26/15,Javier Pena,PA,Carnegie-Mellon University,Standard Grant,Georgia-Ann Klutke,7/31/18,"$300,000.00 ",,jfp@andrew.cmu.edu,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,ENG,"006Y, 8069","072E, 073E, 077E, 7433, 8084, 9102, 9263",$0.00 ,"Certain geometric constructions play a central role in convex optimization - a vital computational technology for a variety of applied disciplines including data and imaging sciences.  The very definition of convexity is purely geometric: a set is convex if it contains the segment joining any two points in it.  Certain properties of convex objects provide the backbone for powerful optimization theory, and geometric constructions form the core of a variety of effective algorithmic techniques for solving optimization problems.  The overarching goal of this research project is a deeper development and tighter integration of geometric properties for convex optimization problems.  The main driving force for this project is the design of novel and more powerful algorithmic schemes.  Given the role that convex optimization plays in a variety of timely applications, this research will provide opportunities for training doctoral students and for synergies with scholars in multiple disciplines including operations research, machine learning, and mathematics.<br/><br/>A main long-term goal of this research plan is to design new algorithms for convex optimization problems via a careful blend of oracles, variable metrics, and adaptive problem preconditioning. To that end, this research project will pursue activities along four main threads.  The first thread concerns the design of new algorithms by harnessing the power of separation oracles and space dilation.  The second thread will focus on variants of the von Neumann and the Frank-Wolfe algorithms with away steps for faster convergence and sparsity. The third thread concerns refinements of conditioning for convex optimization problems.  Special emphasis will be placed on extending the existing theory to problems with flat and nearly flat geometries.  The fourth and most ambitious thread of this project will develop novel algorithms that adaptively precondition the problem at hand to rectify potential difficulties due to poor geometric structure such as those that emerge in degenerate or nearly degenerate problems. This last research thread may shed new light into one of the main open problems in computational mathematics, namely the existence of a strongly polynomial-time algorithm for linear programming."
479,1507428,10th Conference on Bayesian Nonparametrics,DMS,STATISTICS,5/1/15,4/22/15,Subhashis Ghoshal,NC,North Carolina State University,Standard Grant,Gabor Szekely,4/30/16,"$15,000.00 ",Brian Reich,subhashis_ghoshal@ncsu.edu,2701 Sullivan DR STE 240,Raleigh,NC,276950001,9195152444,MPS,1269,7556,$0.00 ,"The 10th Conference on Bayesian Nonparametrics is going to be held during June 22-26, in Raleigh, NC. Providing travel support to junior researchers who do not have access to other sources of funding to attend the conference is key to maintaining the current leadership of American institutions in this field. The conference will also provide American researchers opportunity to exchange ideas with leading researchers from elsewhere in the world such as Europe, Asia and Latin America. Many of the invited speakers including a plenary speaker are women. More women and minorities will be highly encouraged to take part in the conference by providing them local support obtained through funding from the National Science Foundation.<br/><br/>Bayesian nonparametrics has evolved as one of the fastest growing areas of research in statistics. Its application areas include genetics, finance, survival analysis, sociology, networks and machine learning. The primary objective of the conference is to bring together experts and young researchers, theoreticians and practitioners, who use Bayesian nonparametric techniques. The conference has a well-structured balanced program covering various areas of the subject. The scientific committee of the conference consists of renowned international experts on Bayesian nonparametrics and related topics. The meeting will include three overview plenary talks, twenty four invited talks, several contributed talks and two large contributed poster sessions. The poster session and some slots for contributed talks are especially reserved for young researchers. In addition, the conference will provide opportunities for young researchers to disseminate widely the results of their work by facilitating the publication of peer-reviewed papers and a proposed special issue of a leading statistics journal.  Funding from the National Science Foundation will support participation of students, post-doctoral scholars and junior faculty members working in U.S. institutions to participate in the conference."
480,1464151,CRII: CSR: Online Performance Modeling of Opaque Cloud Applications,CNS,Computer Systems Research (CSR,9/1/15,3/3/15,Anshul Gandhi,NY,SUNY at Stony Brook,Standard Grant,M. Mimi McClure,8/31/17,"$173,229.00 ",,anshul@cs.stonybrook.edu,WEST 5510 FRK MEL LIB,Stony Brook,NY,117940001,6316329949,CSE,7354,8228,$0.00 ,"Many online services are now hosted on the cloud. However, cloud adoption is still very limited when it comes to performance sensitive users. One of the primary challenges for cloud users is the lack of understanding of how cloud resource allocation relates to application performance. While Cloud Service Providers (CSPs) offer users easy access to cloud resources for their computing needs, they do not provide any guarantees on the performance of a user's deployment or any guidelines on how users should set their resource allocations. This is because user deployments are opaque: CSPs cannot control or access a user's workload or application. To make matters worse, the effective capacity of a user's cloud instance can change dynamically due to interference from other users. As a result, cloud deployments are plagued with performance issues. <br/><br/>The goal of this research is to develop novel performance models to help users and CSPs understand the dynamic resource requirements of cloud applications without requiring any extensive benchmarking or instrumentation. The research team is constructing novel workload-specific performance models that capture the relationship between cloud resource allocation and application performance. The team is also developing novel online tools based on control theory and machine learning to dynamically infer the (possibly changing) unobservable cloud parameters, thus allowing the performance models to be tuned online. The resulting models will enable users and CSPs to accurately allocate cloud resources to achieve the desired application performance. <br/><br/>Cloud-based services are becoming increasingly popular. This project helps improve cloud adoption and promotes more efficient use of the cloud. The research provides tools to reduce wastage of cloud resources, thus helping cloud users reduce their expenditure and also lowering the power consumption in CSP data centers."
481,1464627,Planning Grant: I/UCRC for Center For Advanced Research in Forensic Science,IIP,"OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL MATHEMATICS, LAW AND SOCIAL SCIENCES, Biological Anthropology, Chemical Measurement & Imaging, ",4/1/15,3/19/15,Keith Morris,WV,West Virginia University Research Corporation,Standard Grant,Rebecca Ferrell,3/31/16,"$11,500.00 ",,keith.morris@mail.wvu.edu,P.O. Box 6845,Morgantown,WV,265066845,3042933998,ENG,"1253, 1271, 1372, 1392, 6880, NX59","5761, 8819, 9150",$0.00 ,"Center for Advanced Research in Forensic Science (CARFS)<br/><br/>The mission of the proposed Center is to provide a multidisciplinary forum for advancing research and innovation in forensic sciences, enhancing education in relevant scientific disciplines, and improving the interpretation and ethical communication of forensic practitioner findings.  The establishment of the proposed center will address two main areas.  The development of new research which will develop a better understanding of methods currently used in forensic science laboratories in terms of their reliability and accuracy.  Furthermore, new ideas will be pursued to improve and develop such methods.  Secondly, the partnerships with industry will help to develop solutions to address improvements required in such areas.  A fundamental requirement of forensic evidence, irrespective of its type or nature, is a robust method for its interpretation.  Additional research will be directed to developing new methods of interpretation.  All of these areas (including specific evidence types) are noted in the NAS report on Forensic Science (2009).  The impact in the judicial process will ensure that evidence is correctly weighted, and will not support a specific proposition in an unbalanced fashion.<br/><br/>The overarching goals of the research are to evaluate the extent of problems encountered in certain areas and to develop solutions.  The evaluation of wrongful convictions in West Virginia through hair microscopy in cold cases will allow for the analyses of the specific issues related to the cases and for the designation of more appropriate techniques of analysis.  Technical evaluation of new methodologies and the interpretative nature of the evidence will be performed in projects such as the characterization and discrimination of inkjet printer inks using micro Raman spectroscopy, the unsupervised deep machine learning algorithms to predict forensic meta-information  (finger position, gender, race, and age from a latent fingerprint), and the evaluation of a portable lab for body fluid identification to ensure higher precision and accuracy of results.  A significant evaluation of forensic mass spectrometry (one of the most widely used analytical techniques in forensic laboratories) to develop new types of miniature, portable mass spectrometers will also be undertaken.  Statistical analysis of firearm identification using IBIS will be interpreted using Bayesian networks to provide a solid basis for firearms evidence interpretation.  From a social perspective and other issues rasied through the NAS report (2009), a determination of the efficacy of backlog reduction programs, assessment of the jurisdictional independence of forensic laboratories for quality assurance and cost effectiveness, and the process of attracting more women to STEM related occupations using forensic science as a model will also be undertaken.<br/><br/>This planning grant is jointly supported by NSF and the National Institute of Justice."
482,1557593,QuBBD: Collaborative Research: Interactive Ensemble clustering for mixed data with application to mood disorders,DMS,,9/15/15,9/11/15,David Gotz,NC,University of North Carolina at Chapel Hill,Standard Grant,Nandini Kannan,8/31/17,"$21,500.00 ",,gotz@unc.edu,104 AIRPORT DR STE 2200,CHAPEL HILL,NC,275991350,9199663411,MPS,O402,,$0.00 ,"The Big Data era has given rise to data of unprecedented size and complexity.  However, fully leveraging Big Data resources for knowledge and discovery is an open challenge due to the fact that conventional methods of data processing and analysis often fail or are inappropriate.  This project develops an innovative approach that utilizes Big Data to improve the classification of mood disorders for the purpose of improving diagnosis and outcomes for psychiatric patients.  Big Data issues are inherently more severe for mental disorders because of their elusive nature.  The psychiatric community has recognized the critical need for a more precise, evidence-based approach for the diagnosis and treatment of disease.  In fact, recent studies funded by the National Institute of Mental Health (NIMH) have found that psychiatric interventions were effective in less than 25% of patients presenting with an acute episode.  This low efficacy rate is especially problematic given the prevalence of mental disorders.  Mood disorders alone (e.g., depression) will be experienced by 1 in 5 adults in the United States at some point in their lives.  This project is motivated by the hypothesis that a more precise and personalized classification of mental health disease can be obtained through the development of novel clustering methods that identify clinically significant structures with these large population data sets.  However, such an approach must overcome a large number of methodological challenges introduced by the complexity of the problem and the nature of large-scale real-world electronic health data.  These challenges include, among others, complex and unknown structure, high dimensionality, heterogeneity, complex mixtures of variables, missing data, and sparsity.  <br/><br/>This award supports initiation of a collaborative research project, carried out by a team with interdisciplinary and complimentary skill sets, to develop methods for big data that address challenges inherent in the integration of biomedical data of this type.  Collective expertise of the team spans the areas of biomedical informatics, biostatistics, computer and information science, electrical and computer engineering, mathematics, and psychiatry.  A novel methodology is developed in a flexible and fully integrated framework that can be extended to other biomedical data and diseases.  Within this framework, clustering methods that capture different aspects of relatedness in the data are integrated in a rigorous way that not only accounts for model uncertainty, but also results in an interactive visualization that is accessible with strong model interpretability for the non-expert.  Specifically, the methodology will rely on novel modifications to bootstrap estimators of generalization error for the purpose of assembling a consensus over an ensemble of clusters inferred from topology-based and machine learning approaches.  The framework also supports iterative refinement of the consensus solution based on user input (via the visualization) to incorporate domain expertise.  The rigorous identification of sub-groups of individuals within heterogeneous populations will facilitate accurate and targeted diagnosis for mood disorders, and provide opportunity for personalized evidence-based interventions.   Applications focus on clustering individuals with mood disorders (bipolar disorder and major depression) from data collected in the Bipolar Disorder Research Network (BDRN).  Despite this focus, the methodology is generalizable to other diseases that face similar challenges for diagnosis and treatment.  In fact, this project supports the first steps of a long-term vision of generalizing the methods to more complex and less curated data, such as electronic health records, social media, and other sources.  This award is supported by the National Institutes of Health Big Data to Knowledge (BD2K) Initiative in partnership with the National Science Foundation Division of Mathematical Sciences."
483,1510998,EAGER: COLLABORATIVE RESEARCH: Developmental mechanisms of perception and language in the infant brain,BCS,DS -Developmental Sciences,5/15/15,5/12/15,Charles Nelson,MA,Children's Hospital Corporation,Standard Grant,david moore,4/30/17,"$84,085.00 ",Richard Aslin,charles.nelson@childrens.harvard.edu,300 LONGWOOD AVENUE,Boston,MA,21155737,6179192729,SBE,1698,"1698, 7916",$0.00 ,"Understanding what infants understand about objects and words that they encounter in the world has been an important goal in developmental science, but the field understands relatively little about how infants perform either of these two tasks.  Several neuroimaging methods have been used to determine how adult brains recognize familiar objects and words, but most of these methods are not suitable for use with infants.  The goal of this research project is to deploy two neuroimaging methods that are amenable for use with infants, as a novel way to gain insights into the fundamental brain mechanisms that enable object recognition and word understanding in 3- to 12-month-old infants. One technique, electroencephalography (EEG) involves measuring electrical activity generated by the brain from sensors on the scalp. The other, functional near-infrared spectroscopy (fNIRS), shines near-infrared light through the skull and measures how it is absorbed by the brain at each location as an index of how active that part of the brain is. Recording both these measures while infants watch and listen to stimuli will provide important insights into how the infant brain processes this information.<br/><br/>EEG has been in use with infants for many years, whereas functional near-infrared spectroscopy (fNIRS) is a relatively newer non-invasive neuroimaging technique ideally suited for use with infants. fNIRS, like fMRI, provides a signature of metabolic activity in localized regions of the brain but is more suitable for infants because it delivers light to the scalp via a tight-fitting cap.  In this project, EEG and fNIRS will be used to measure electrical and metabolic activity in the brain as infants watch objects of various categories such as vehicles or furniture, or listen to words that they know or do not know (or nonsense words).  The key analysis tool is a suite of machine-learning algorithms from the field of computer science that take the EEG or fNIRS signals to determine which components of these signals best predict, on a trial by trial basis, what the infant was seeing or hearing.<br/><br/>The outcome of the proposed research will localize in the brain where encoding of visual objects and spoken words takes place and over what time period after stimulus onset that processing occurs. These are fundamental aspects of object and language processing have eluded study in the human infant because of methodological challenges.  Establishing protocols for analyzing data generated by these two methods will contribute to analytic techniques available for future infant brain research.  Identifying normative properties of these processing mechanisms in the infant brain will set the stage for future research investigating how the developing brain is affected by variations in early experience, by compensation after injury, and by a variety of genetic anomalies."
484,1557668,QuBBD: Collaborative Proposal: Interactive Ensemble Clustering for Mixed Data with Application to Mood Disorders,DMS,,9/15/15,9/11/15,Mathews Jacob,IA,University of Iowa,Standard Grant,Nandini Kannan,8/31/16,"$19,700.00 ",,Mathews-Jacob@uiowa.edu,2 GILMORE HALL,IOWA CITY,IA,522421320,3193352123,MPS,O402,9150,$0.00 ,"The Big Data era has given rise to data of unprecedented size and complexity.  However, fully leveraging Big Data resources for knowledge and discovery is an open challenge due to the fact that conventional methods of data processing and analysis often fail or are inappropriate.  This project develops an innovative approach that utilizes Big Data to improve the classification of mood disorders for the purpose of improving diagnosis and outcomes for psychiatric patients.  Big Data issues are inherently more severe for mental disorders because of their elusive nature.  The psychiatric community has recognized the critical need for a more precise, evidence-based approach for the diagnosis and treatment of disease.  In fact, recent studies funded by the National Institute of Mental Health (NIMH) have found that psychiatric interventions were effective in less than 25% of patients presenting with an acute episode.  This low efficacy rate is especially problematic given the prevalence of mental disorders.  Mood disorders alone (e.g., depression) will be experienced by 1 in 5 adults in the United States at some point in their lives.  This project is motivated by the hypothesis that a more precise and personalized classification of mental health disease can be obtained through the development of novel clustering methods that identify clinically significant structures with these large population data sets.  However, such an approach must overcome a large number of methodological challenges introduced by the complexity of the problem and the nature of large-scale real-world electronic health data.  These challenges include, among others, complex and unknown structure, high dimensionality, heterogeneity, complex mixtures of variables, missing data, and sparsity.  <br/><br/>This award supports initiation of a collaborative research project, carried out by a team with interdisciplinary and complimentary skill sets, to develop methods for big data that address challenges inherent in the integration of biomedical data of this type.  Collective expertise of the team spans the areas of biomedical informatics, biostatistics, computer and information science, electrical and computer engineering, mathematics, and psychiatry.  A novel methodology is developed in a flexible and fully integrated framework that can be extended to other biomedical data and diseases.  Within this framework, clustering methods that capture different aspects of relatedness in the data are integrated in a rigorous way that not only accounts for model uncertainty, but also results in an interactive visualization that is accessible with strong model interpretability for the non-expert.  Specifically, the methodology will rely on novel modifications to bootstrap estimators of generalization error for the purpose of assembling a consensus over an ensemble of clusters inferred from topology-based and machine learning approaches.  The framework also supports iterative refinement of the consensus solution based on user input (via the visualization) to incorporate domain expertise.  The rigorous identification of sub-groups of individuals within heterogeneous populations will facilitate accurate and targeted diagnosis for mood disorders, and provide opportunity for personalized evidence-based interventions.   Applications focus on clustering individuals with mood disorders (bipolar disorder and major depression) from data collected in the Bipolar Disorder Research Network (BDRN).  Despite this focus, the methodology is generalizable to other diseases that face similar challenges for diagnosis and treatment.  In fact, this project supports the first steps of a long-term vision of generalizing the methods to more complex and less curated data, such as electronic health records, social media, and other sources.  This award is supported by the National Institutes of Health Big Data to Knowledge (BD2K) Initiative in partnership with the National Science Foundation Division of Mathematical Sciences."
485,1514351,EAGER:  COLLABORATIVE RESEARCH:  Developmental mechanisms of perception and language in the infant brain,BCS,DS -Developmental Sciences,5/15/15,5/12/15,Richard Aslin,NY,University of Rochester,Standard Grant,david moore,4/30/17,"$215,815.00 ",,richard.aslin@yale.edu,"518 HYLAN, RC BOX 270140",Rochester,NY,146270140,5852754031,SBE,1698,"1698, 7916",$0.00 ,"Understanding what infants understand about objects and words that they encounter in the world has been an important goal in developmental science, but the field understands relatively little about how infants perform either of these two tasks.  Several neuroimaging methods have been used to determine how adult brains recognize familiar objects and words, but most of these methods are not suitable for use with infants.  The goal of this research project is to deploy two neuroimaging methods that are amenable for use with infants, as a novel way to gain insights into the fundamental brain mechanisms that enable object recognition and word understanding in 3- to 12-month-old infants. One technique, electroencephalography (EEG) involves measuring electrical activity generated by the brain from sensors on the scalp. The other, functional near-infrared spectroscopy (fNIRS), shines near-infrared light through the skull and measures how it is absorbed by the brain at each location as an index of how active that part of the brain is. Recording both these measures while infants watch and listen to stimuli will provide important insights into how the infant brain processes this information.<br/><br/>EEG has been in use with infants for many years, whereas functional near-infrared spectroscopy (fNIRS) is a relatively newer non-invasive neuroimaging technique ideally suited for use with infants. fNIRS, like fMRI, provides a signature of metabolic activity in localized regions of the brain but is more suitable for infants because it delivers light to the scalp via a tight-fitting cap.  In this project, EEG and fNIRS will be used to measure electrical and metabolic activity in the brain as infants watch objects of various categories such as vehicles or furniture, or listen to words that they know or do not know (or nonsense words).  The key analysis tool is a suite of machine-learning algorithms from the field of computer science that take the EEG or fNIRS signals to determine which components of these signals best predict, on a trial by trial basis, what the infant was seeing or hearing.<br/><br/>The outcome of the proposed research will localize in the brain where encoding of visual objects and spoken words takes place and over what time period after stimulus onset that processing occurs. These are fundamental aspects of object and language processing have eluded study in the human infant because of methodological challenges.  Establishing protocols for analyzing data generated by these two methods will contribute to analytic techniques available for future infant brain research.  Identifying normative properties of these processing mechanisms in the infant brain will set the stage for future research investigating how the developing brain is affected by variations in early experience, by compensation after injury, and by a variety of genetic anomalies."
486,1547360,CICI: Data Provenance: Provenance-Based Trust Management for Collaborative Data Curation,OAC,Cybersecurity Innovation,9/1/15,9/8/15,Zachary Ives,PA,University of Pennsylvania,Standard Grant,Robert Beverly,8/31/19,"$500,000.00 ","Susan Davidson, Val Tannen, Sampath Kannan",zives@cis.upenn.edu,Research Services,Philadelphia,PA,191046205,2158987293,CSE,8027,"7434, 8089",$0.00 ,"Data-driven science relies not only on statistics and machine learning, but also on human expertise. As data are being collected to tackle increasingly challenging scientific and medical problems, there is need to scale up the amount of expert human input (curation and, in certain cases, annotation) accordingly.  This project addresses this need by developing collaborative data curation:  instead of relying on a small number of experts, it enables annotations to be made by communities of users of varying expertise.  Since the quality of annotations by different users will vary, novel quantitative techniques are developed to assess the trustworthiness of each user, based on their actions, and to distinguish trustworthy experts from unskilled and malicious users. Algorithms are developed to combine users' annotations based on their trustworthiness.  Collaborative data curation will greatly increase the amount of human annotated data, which will, in turn, lead to better Big Data analysis and detection algorithms for the life sciences, medicine, and beyond.<br/><br/>The central problems of collaborative data curation lie in the high variability in the quality of users' annotations, and variability in the form the data takes when they annotate it.  The proposal develops techniques to take annotations made by different users over different views of data (such as an EEG display with filters and transformations applied to the signal), to use provenance to reason about how the annotations relate to the original data, and to reason about the reliability and trustworthiness of each user's annotations over this data.  To accomplish this, the research first defines data and provenance models that capture time- and space-varying data; novel reliability calculus algorithms for computing and dynamically updating the reliability and trustworthiness of individuals, based on their annotations and how these compare to annotations from recognized experts and the broader community; and a high-level language called PAL that enables the researchers to implement and compare multiple policies.  The researchers will initially develop and validate the techniques on neuroscience and time series data, within a 900+ user public data sharing portal (with 1500+ EEG and other datasets for which annotations are required). The project team later expands the techniques to other data modalities, such as imaging and genomics"
487,1521544,Collaborative Research: Novel Computational and Statistical Approaches to Prediction and Estimation,DMS,CDS&E-MSS,8/1/15,8/10/17,Karthik Sridharan,NY,Cornell University,Continuing Grant,Christopher Stark,7/31/19,"$175,000.00 ",,sridharan@cs.cornell.edu,373 Pine Tree Road,Ithaca,NY,148502820,6072555014,MPS,8069,"7433, 8084, 9263",$0.00 ,"Terabytes of data are collected by companies and individuals every day. These data possess no value unless one can efficiently process them and use them to make decisions. The scale and the streaming nature of data pose both computational and statistical challenges. The objective of this research project is to develop novel approaches to making online, real-time decisions when data are constantly evolving and highly structured. In particular, this project focuses on online prediction problems involving multiple users in dynamic networks. The project also aims to tackle the privacy issues arising in such multi-user scenarios.<br/><br/>In recent years, it was shown that a majority of online machine learning algorithms can be viewed as solutions to approximate dynamic programming (ADP) problems that incorporate one additional datum per step. Along with directly addressing the computational concerns, the ADP framework also provides guaranteed performance on prediction problems. This project is to use and extend the ADP framework to develop prediction algorithms that simultaneously address the issues of computation, robustness, non-stationarity, privacy, and multiplicity of users."
488,1527826,NRI: Collaborative Research: Versatile Locomotion: From Walking to Dexterous Climbing with a Human-Scale Robot,IIS,NRI-National Robotics Initiati,9/1/15,5/5/17,Kris Hauser,NC,Duke University,Standard Grant,David Miller,8/31/19,"$486,512.00 ",,kkhauser@illinois.edu,"2200 W. Main St, Suite 710",Durham,NC,277054010,9196843030,CSE,8013,"8086, 9251",$0.00 ,"The project aims to give legged robots the skills to navigate a wide variety of terrain.  This capability is needed to employ robots in applications such as search-and-rescue, construction, and exploration of remote environments on Earth and other planets. The multidisciplinary team, composed of researchers at Duke, Stanford, UC Santa Barbara, JPL, and Motiv Robotics, will develop a robot to climb a variety of surfaces ranging from flat ground to overhanging cliffs. Using an array of sensors, unique hands, and sophisticated algorithms, the robot will dynamically adopt walking, crawling, climbing, and swinging strategies to traverse wildly varied terrain. During the course of this research, the team hopes to achieve the milestone of the first demonstration of a human-scale rock climbing robot. The research is also expected to lead to insights into cognitive and biomechanical processes in human and animal locomotion.<br/><br/>Although rock climbing serves as an ideal proving ground for the work, this project conducts basic research to address more a general-purpose goal; namely, to provide the physical and cognitive skills for robots to adaptively navigate varied terrain. It takes a dexterous climbing approach, which uses non-gaited, coordinated sequences of contact to move the body, much as dexterous manipulation uses contact with the fingers and palm to move an object.  It will apply principles from optimization, machine learning, bioinspiration, and control theory to make intellectual contributions in several domains, such as robot hand design, planning algorithms, balance strategies, and locomotion performance measurement.  Novel grippers, sensor-based planning strategies, reactive maneuvers, and locomotion metrics will be developed during the course of this research."
489,1509178,AF: Medium: Algorithmic Complexity in Computation and Biology,CCF,Algorithmic Foundations,7/1/15,6/9/15,Leslie Valiant,MA,Harvard University,Standard Grant,Tracy Kimbrel,6/30/21,"$900,000.00 ",,valiant@seas.harvard.edu,1033 MASSACHUSETTS AVE,Cambridge,MA,21385369,6174955501,CSE,7796,"7924, 7927",$0.00 ,"Computational complexity is the field that studies how much resources are needed for performing computational tasks. Its primary focus has been to understand how many steps and how much storage space is required for performing various important tasks on conventional computers. Biological processes, can also be viewed as computations to the extent that they consist of step-by-step processes that follow certain rules, and can then be also studied from the perspective of the theory of computational complexity. This proposal is concerned with studying both of these aspects. Its goal is that of proving, by mathematical means, upper or lower bounds on the resources needed for both general purpose and evolutionary computations.<br/> <br/>While much is understood about the complexity of computations on general purpose computers, this understanding is pivoted around a small number of critical open questions, such as the P=?NP question, the answer to which would resolve the resource requirements of numerous important tasks. The first focus of this study will be algebraic approaches to these questions, in which the limitations on computations imposed by algebraic axioms is analyzed. Holographic algorithms have over the last decade yielded novel algorithms for a variety of problems, as well as new lower bound arguments, and also new techniques for proving computational equivalence among apparently dissimilar problems. The goal of the research is to understand the inherent limits of holographic algorithms and to use this understanding to develop efficient algorithms. Darwinian evolution can be also viewed as a computational process that uses quantifiable resources, here measured in terms of numbers of generations, size of populations, and the number of experiences of individuals. Recently it was shown that the Darwinian mechanism can be viewed as a form of machine learning, the field of computer science that studies systems in which most of the information is acquired from experience and not from a programmer. The goal of the research is to understand what classes of functions, such as those occurring in protein expression networks, can so evolve using practicable resources.  Graduate students will be involved in these projects."
490,1544173,"CPS: TTP Option: Synergy: Collaborative Research: Certifiable, Scalable, and Attack-resilient Submodular Control Framework for Smart Grid Stability",CNS,"Special Projects - CNS, CPS-Cyber-Physical Systems, ",10/1/15,9/13/16,Linda Bushnell,WA,University of Washington,Continuing Grant,David Corman,9/30/19,"$893,643.00 ","Radha Poovendran, Daniel Kirschen",lb2@uw.edu,4333 Brooklyn Ave NE,Seattle,WA,981950001,2065434043,CSE,"1714, 7918, P342","7918, 8235, 8237, 9102",$0.00 ,"Exploiting inherent physical structure of the CPS domains can lead to economically viable and efficient novel algorithms for providing performance, control, synchronization and an alternate approach to CPS security that does not rely solely on cryptography. In each of these systems, regardless of the current state of the network, in the presence of disturbances or adversarial inputs, there is a need to bring the system to desired state for performance and control of the network. <br/><br/>This project presents one such novel approach by observing that the CPS applications including smartgrid, coordinating robotics, formation flights in UAV, and synchronization of biological systems including brain networks all exhibit a special physical structure, namely submodularity, with respect to the set of control actions. Submodularity is a diminishing returns property that enables the development of efficient algorithms with provable optimality guarantees and in many cases distributed versions that are locally implementable, and hence scalable. While it has been widely used in the machine learning and discrete optimization communities, the use of submodularity in the context of CPS is a fertile research area.  This project initially applies submodularity in the context of smart grid and show how it can lead to greater system stability and attack resilience. By defining suitable metrics that capture the submodular structures underlying the physical dynamics, the researchers develop algorithms that eliminate the time-consuming and computationally expensive verification of control actions through simulation. The fundamental properties of synchronization, convergence, robustness, and attack-resilience considered in this effort have crosscutting applications to multiple CPS domains, which will benefit from the submodular approach that we will research and develop."
491,1547168,EAGER: New genomic resources and models for predicting evolving vector-borne disease dynamics in a changing world,DEB,"PHYLOGENETIC SYSTEMATICS, Ecology of Infectious Diseases",7/15/15,7/6/15,Dina Fonseca,VA,Smithsonian Institution,Standard Grant,Simon Malcomber,6/30/17,"$130,000.00 ","Robert Fleischer, A. Marm Kilpatrick, Nina Fefferman, Eben Paxton",Dina.fonseca@rutgers.edu,Office of Sponsored Projects,Arlington,VA,222023709,2026337110,BIO,"1171, 7242","1171, 7242, 7916, 9169, EGCH",$0.00 ,"Global climate change has accelerated our need to understand disease dynamics. Diseases transmitted among hosts by small invertebrates such as mosquitoes or ticks (vectors) are on the rise across the world but links to climate change are unclear. Climate change can impact vector-borne disease transmission directly by shifting the occurrence of competent hosts and vectors, or a parasite, or more subtly by changing the timing or nature of their interaction. Predicting the response of vector-borne diseases to climate change requires both an understanding of how all the species involved are likely to be affected as well as new ways to identify and predict how they interact and furthermore how they and their interactions may evolve. This research will develop and test functional models of vector-borne diseases that incorporate co-evolutionary change. Specifically, the project will develop models that account for past and predict future evolution of responses to avian malaria, a mosquito borne disease, using a database of disease prevalence in the endangered Hawaiian honeycreepers. This research will answer important questions in epidemiology by measuring and integrating evolutionary changes in hosts, vectors and parasites into predictive models of disease dynamics under future climate scenarios.<br/><br/>An ideal disease system in which to develop and train such a model is avian malaria in native and introduced Hawaiian birds. The agent of avian malaria in Hawaii is a non-native Haemosporidian parasite, Plasmodium relictum, vectored by mixes of two non-native strains of the mosquito Culex quinquefasciatus. Avian malaria in Hawaii occurs as a series of replicated natural experiments in which vector and parasite prevalence vary along elevational gradients on several islands, and a parallel gradient in tolerance among some bird hosts has been reported. Although P. relictum was previously highly virulent to all Hawaiian honeycreepers, evolution of tolerance (or resistance) has been observed in several species such as the Amakihi (Hemignatus sp). Within- and among-species differences in the degree of tolerance suggest that genomic variation underlie such differences; moreover, there is geographic variation within and across islands in host species composition, host tolerance, climate (temperature and precipitation), vector abundance, vector competence, and pathogen fitness. This system has been studied for several decades; thus, many of the host and vector demographic characteristics are well understood. Specifically this project will (1) use a machine learning approach to develop ways to correlate the genetic data from the relevant species with the ecological, environmental, and epidemiological pressures that might shape their evolution; (2) analyze past and current parasite diversity using Next Generation genomics to be fed into the model. Though constructed with reference specifically to the Hawaiian Plasmodium system, if successfully validated, this model methodology will then be available for broad application into any disease system in which evolution is expected to occur in response to shifting climatological, environmental, or ecological conditions."
492,1452163,"CAREER: Role of geometry in dynamical modeling of human movement: Applications to activity quality assessment across Euclidean, non-Euclidean, and function spaces",CCF,Comm & Information Foundations,6/15/15,5/20/19,Pavan Turaga,AZ,Arizona State University,Continuing Grant,Phillip Regalia,5/31/21,"$552,000.00 ",,pavan.turaga@asu.edu,ORSPA,TEMPE,AZ,852816011,4809655479,CSE,7797,"1045, 7935, 7936, 9251",$0.00 ,"Human movement is a complex multi-dimensional dynamical process arising out of complex non-linear interactions between various muscles and joints as well as interactions with external forces and objects. These physical factors often impose certain analytical constraints on movement data obtained from external sensors, often with associated geometric representations. Examples of such representations abound in the area of human activity analysis, where several contemporary and emerging human shape and movement representations such as silhouettes, stick-figure sequences, orientation signals, and optical- flow have intricate signal spaces, often described in the language of Riemannian geometry.  The research thrusts focus on foundational methods for extracting meaningful dynamical attributes that will dovetail into the application thrust. The application centers on computational modeling of qualities of physical activities from low-fidelity sensing infrastructure, which is an important component for technology-mediated physical rehabilitation and preventive interventions.<br/> <br/>Classical vector-valued signal processing and machine learning has had a large impact in the area of understanding and modeling human activities, but their applicability is significantly limited when it comes to signal-spaces with non-Euclidean geometry. This research project aims to advance a new class of robust, non-parametric dynamical modeling approaches, which will extend from classical vector-valued observation spaces, to finite-dimensional Riemannian manifolds, as well as to infinite-dimensional function-spaces. In order to be general enough to account for various geometric spaces as mentioned here, a framework that is free of restrictive assumptions on parametric forms of dynamics is needed. Further, integrating dynamical analysis with Riemannian geometric theory allows the analysis to extend to several feature spaces including depth maps, shape sequences, stick-figures, and orientation data, without re-defining the fundamental models for activity analysis."
493,1531752,"MRI: Acquisition of Conflux, A Novel Platform for Data-Driven Computational Physics",OAC,"Major Research Instrumentation, Information Technology Researc, CYBERINFRASTRUCTURE",9/1/15,8/27/15,Karthikeyan Duraisamy,MI,Regents of the University of Michigan - Ann Arbor,Standard Grant,Stefan Robila,8/31/19,"$2,422,972.00 ","Barzan Mozafari, August Evrard, Carlos Figueroa, Krishnakumar Garikipati",kdur@umich.edu,3003 South State St. Room 1062,Ann Arbor,MI,481091274,7347636438,CSE,"1189, 1640, 7231",1189,$0.00 ,"This project develops an instrument, called ConFlux, hosted at the University of Michigan (UM), specifically designed to enable High Performance Computing (HPC) clusters to communicate seamlessly and at interactive speeds with data-intensive operations.  The project establishes a hardware and software ecosystem to enable large scale data-driven modeling of multiscale physical systems.  ConFlux will produce advances in predictive modeling in several disciplines including turbulent flows, materials physics, cosmology, climate science and cardiovascular flow modeling. <br/><br/>A wide range of phenomena exhibit emergent behavior that makes modeling very challenging. In this project, physics-constrained data-driven modeling approaches are pursued to account for the underlying complexity. These techniques require HPC applications (running on external clusters) to interact with large data sets at run time. ConFlux provides low latency communications for in- and out-of-core data, cross-platform storage, as well as high throughput interconnects and massive memory allocations. The file-system and scheduler natively handle extreme-scale machine learning and traditional HPC modules in a tightly integrated workflow---rather than in segregated operations--leading to significantly lower latencies, fewer algorithmic barriers and less data movement.  <br/><br/>Course material developed from the usage of ConFlux is being integrated into the educational curriculum via several degree and certificate programs offered by two UM institutes dedicated to computational and data sciences. Use of the ConFlux cluster will be extended to research groups outside of UM utilizing a number of Extreme Science and Engineering Discovery Environment (XSEDE) bridging tools and file-systems. Connections established through UM's Office of Outreach and Diversity are being leveraged to extend the use of ConFlux to minority serving institutions and Historically Black Colleges and Universities. Using the programs developed by the Society of Women Engineers at UM, middle and high school students will be engaged in hands-on educational modules in computing, physics and data."
494,1500124,PFI: AIR-TT: Prototype Scale-up for Traumatic Pelvic and Abdominal Injury Decision Support System (DSS),IIP,"GOALI-Grnt Opp Acad Lia wIndus, Accelerating Innovation Rsrch",4/1/15,10/16/18,Kayvan Najarian,MI,Regents of the University of Michigan - Ann Arbor,Standard Grant,Jesus Soriano Molla,12/31/18,"$308,933.00 ",,kayvan@umich.edu,3003 South State St. Room 1062,Ann Arbor,MI,481091274,7347636438,ENG,"1504, 8019","019Z, 116E, 1504, 8019, 9231, 9251",$0.00 ,"This PFI:AIR Technology Translation project focuses on translating and accelerating commercialization of a patented Traumatic Pelvic and Abdominal Injury Decision Support System (DSS) technology. This will enable physicians to quickly and accurately extract complex patient data from all relevant biomedical images, (e.g., CT scans with hundreds of image slices) trauma scores, diagnoses, treatments, demographics, and injury specifics for each patient?while integrating and analyzing the information to generate prediction, warning, and treatment recommendations at every stage of patient care. This project is important not only to help decrease medical complications and increase survival, but also to optimize resource utilization- a key to reducing the  approximately $60B medical cost each year for treating complications in pelvic and abdominal trauma cases. <br/><br/>The Traumatic Pelvic and Abdominal Injury DSS technology has the following unique capabilities which provide competitive advantages when compared to the existing state of the art for DSS tools: 1) it segments and assesses damage to major abdominal organs; 2) it provides recommendations and predictions for several specific, complex clinical decisions; 3) it is fully automated and does not require an expert?s supervision in analyzing patient data- providing an easier-to-use software interface and potentially providing higher accuracy in pertinent recommendations for trauma patient care. If the algorithms and software are successfully validated through this project, a licensing pathway has been identified to commercialize the DSS software.<br/><br/>Clinical decision making shows its true complexity when one is trying to quickly and accurately integrate complex types of patient data in an emergency setting. This project addresses several technology gaps as it translates from research discovery toward commercial application. Existing ""semi-automated"" systems use only a portion of patient data and do not analyze detailed information contained in digital images to create recommendations; conversely, current image processing technologies designed mainly to assist in analysis of CTs (or other images) are not optimized to address the needs of trauma and/or DSS tools. This project will refine and scale up the prototype, validate its clinical use, and accelerate its commercialization to assist clinicians in traumatic pelvic and abdominal injury cases. Key technical objectives are to: 1) expand the organ segmentation software module (now covering only the spleen) to include the liver, kidneys, and pancreas; 2) enhance the hemorrhage detection algorithms to find bleeding close to bones; 3) further validate and improve the system using a larger and more comprehensive dataset; and 4) rewrite the graphical user interface to match requirements for the prototype and validate its effectiveness and ease of use by clinicians. Key computational methods generated by this project include automated image processing algorithms and machine learning methods to: 1) assess a CT scan for bone fracture(s) and hemorrhage and measure their sizes; 2) segment more major organs, identify damage, and quantitatively assess level of injury; and 3) predict outcomes (survival, number of ICU days, home vs. rehab, etc.) and form recommendations for care givers at each step of the treatment. The graduate student involved in this project will gain experience in innovation and technology translation towards commercialization through development of the DDS tool, testing and validating the algorithms, and working closing with the project team, clinicians, business developers, tech transfer professionals, and a potential licensee to commercialize the technology as a viable product."
495,1527193,AF: Small: Subdivision Methods: Correctness and Complexity,CCF,Algorithmic Foundations,9/1/15,8/20/15,Michael Burr,SC,Clemson University,Standard Grant,Joseph Maurice Rojas,8/31/19,"$246,411.00 ",,burr2@clemson.edu,230 Kappa Street,CLEMSON,SC,296345701,8646562424,CSE,7796,"7923, 7933, 9150",$0.00 ,"Subdivision-based algorithms can solve problems from a wide variety of applications in mathematics, computer science, and the sciences.  For example, these types of algorithms are used in computer graphics, mathematical biology, computational geometry, mathematical modeling, robotics, machine learning, and mathematical computation.  Subdivision-based algorithms are popular because they are relatively easy to describe and implement on a computer, and they are often efficient in practice.  The work in this project is to quantify and improve the effectiveness of these types of algorithms.  By studying the efficiency and providing algorithms to approximate solutions to problems which are typically considered intractable, the results of this project provide techniques which can be applied to practical problems throughout the sciences.<br/><br/>Subdivision-based algorithms recursively and adaptively subdivide a given domain into smaller regions until, in each smaller region, the behavior of a problem-specific feature can be determined.  Subdivision-based algorithms are frequently used because they are parallelizable, recursive, and adaptive.  More precisely, they use weak local tests and perform more subdivisions only near difficult features.  These features that make subdivision-based algorithms practical, however, also make them challenging to study.  For example, local tests make global topological correctness difficult and adaptive (non-uniform) subdivisions make the number of subdivisions difficult to bound.  This project addresses both of the important questions of complexity and correctness for subdivision-based algorithms in the following two ways: (1) Using continuous amortization as a uniform method to compute the complexity of subdivision-based algorithms.  (2) Developing topologically certified subdivision-based algorithms for geometric applications on algebraic varieties.  This project extends the technique of continuous amortization to many different types of algorithms including iterative and two-dimensional subdivisions; additionally, the project develops subdivision-based algorithms to approximate previously intractable problems such as the medial axis and intersections of surfaces."
496,1533500,NCS-FO: Collaborative Research: Understanding Individual Differences in Cognitive Performance: Joint Hierarchical Bayesian Modeling of Behavioral and Neuroimaging Data,BCS,IntgStrat Undst Neurl&Cogn Sys,8/1/15,8/10/15,Zhong-Lin Lu,OH,Ohio State University,Standard Grant,Betty Tuller,7/31/19,"$452,165.00 ",Brandon Turner,zhonglin@nyu.edu,Office of Sponsored Programs,Columbus,OH,432101016,6146888735,SBE,8624,"8089, 8091, 8551",$0.00 ,"Understanding the complex determinants of individual health and wellbeing is critical for the promotion and maintenance of a healthy world population.  Wellbeing may be understood not only as the absence of physical and mental illness but also as the quality of life and optimal functioning of individuals. It is well known that individuals vary tremendously in terms of cognitive abilities and dispositions, as seen from performance on high-order cognitive tasks, decision-making preferences, and emotional competencies. However, the neural underpinnings of much of this variability are poorly understood: It is unclear how individual differences in brain structure and function across tasks and processes are linked to abilities and competencies. This project explores a mathematical and computational framework for investigating a large-sample neuroimaging and behavioral dataset in order to improve our understanding of individual differences in cognitive performance. An ultimate goal of the project is to predict individual cognitive performance in novel, real-world situations based on observed (past) behavioral and neuroimaging data and contribute to the understanding of cognitive health and wellbeing of individuals. The project will also offer many training opportunities for the next generation of scientists. <br/><br/>The technical approach will build on and integrate recent advances in cognitive science, neuroscience, statistics, and machine learning. Statistical models will integrate data from both brain imaging and behavioral tests to generate predictions that otherwise may not be possible with a single source of data. The research will go beyond establishing and explaining individual differences to predicting individual cognitive performance in a variety of tasks."
497,1521687,Collaborative Research: CompSustNet: Expanding the Horizons of Computational Sustainability,CCF,Expeditions in Computing,12/15/15,3/24/20,Thomas Dietterich,OR,Oregon State University,Continuing Grant,Sylvia Spengler,11/30/20,"$1,400,000.00 ","Alan Fern, Xiaoli Fern, Weng-Keen Wong, John Selker, Weng-Keen Wong",tgd@cs.orst.edu,OREGON STATE UNIVERSITY,Corvallis,OR,973318507,5417374933,CSE,7723,7723,$0.00 ,"Poverty, saving species, repowering the world with renewable energy, lifting people up to live better lives - there are no easy answers to guiding our planet on the path toward sustainability. Complex problems require sophisticated solutions. They involve intricacy beyond human capabilities, the kind of big-data processing and analysis that only advanced large-scale computing can provide. This NSF Expedition in Computing launches CompSustNet (http://www.compsust.net), a vast research network powered by the nation's recognized university computer science programs, charged with applying the emerging field of computational sustainability to solving the world's seemingly unsolvable resource problems. Put simply, the project will enlist some of the top talents in computing, social science, conservation, physics, materials science, and engineering to unlock sustainable solutions that safeguard our planet's future.<br/><br/>Computational Sustainability is, at its core, the belief that with sufficiently advanced computational techniques, we can devise sustainable solutions that meet the environmental, societal, and economic needs of today while providing for future generations. In much the same way IBM's supercomputer Watson could defeat any challenger in Jeopardy!, computational sustainability posits that a computer-engineered solution can be applied to world's difficult and challenging problems - from helping farmers and herders in Africa survive severe droughts to developing a smart power grid fueled entirely by renewable energy. CompSustNet is a large national and international multi-institutional research network led by Cornell University and including 11 other US academic institutions: Bowdoin, Caltech, CMU, Georgia Tech, Howard University, Oregon State, Princeton, Stanford, UMass, University of South California, and Vanderbilt University, as well as collaborations with several international universities. But CompSustNet is not just an academic enterprise, as it also includes key governmental and non-governmental organizations that specialize in conservation, poverty mitigation, and renewable energy, such as The Nature Conservancy, The World Wildlife Fund, The International Livestock Research Institute, The Trans-African Hydro-Meteorological Observatory, and the National Institute of Standards and Technology.<br/><br/>CompSustNet's core mission is to significantly expand the horizons of computational sustainability and foster the advancement of state-of-the-art computer science to achieve the scale to tackle global problems. Research will focus on cross-cutting computational topics such as optimization, dynamical models, simulation, big data, machine learning, and citizen science, applied to sustainability challenges.  For example, computational sustainability is being put to work to resolve the problem of providing wetlands for shorebirds that migrate from the Arctic through California during a time of drought. As California gets drier, the shorebirds have nowhere to stop, rest, and refuel by eating wetland invertebrates. Scientists are developing new dynamic precision conservation techniques that use complex, big-data models to tackle the problem with NASA satellite imagery, meteorological forecasts, and citizen science in the form of thousands of bird location sightings from the Cornell Lab of Ornithology's eBird checklisting app for birdwatchers. Through partnership with The Nature Conservancy, the program forecasts when and where wetland habitat would be needed for shorebirds, and the Conservancy pays Central Valley rice farmers to flood their fields at opportune times - providing benefits for birds and farmers at a time when extreme drought is making life tough for both. In similar ways, computational sustainability projects will also be hard at work innovating automated monitoring networks to protect endangered elephant population from poachers, promoting the discovery of novel ways to harvest energy from sun light, and designing algorithms to manage the generation and storage of renewable energy in the power grid. <br/><br/>Advancements in computational sustainability will lead to novel, low-cost, high-efficiency strategies for saving endangered species, helping indigenous peoples improve their way of life, and scaling renewables up to meet 21st century energy demand. CompSustNet is like the seed, the venture capital, to help the field of computational sustainability achieve what's possible."
498,1533795,XPS: EXPL: SDA: Scalable Concurrency Control Techniques for Distributed Systems,CCF,Exploiting Parallel&Scalabilty,9/1/15,8/14/15,Ananth Grama,IN,Purdue University,Standard Grant,Marilyn McClure,8/31/20,"$299,990.00 ",,ayg@cs.purdue.edu,Young Hall,West Lafayette,IN,479072114,7654941055,CSE,8283,,$0.00 ,"Virtually all distributed computing applications, from transactions on databases to updates on social media platforms, involve concurrent operations on data objects. For these applications, concurrency control mechanisms represent significant performance overheads. These applications typically exhibit strong and persistent patterns in data access. Motivated by the importance of the problem, this project investigates the use of dynamic data- and lock-access patterns in distributed computations to significantly improve the performance of concurrency control mechanisms for scalable systems, specifically, in conventional cloud environments and key-value stores such as BigTable, HBase, and Cassandra. In contrast to conventional techniques that collocate locks with corresponding data items, this project relies on a modular lock service that decouples lock locations from corresponding data objects, and maintains lock state of all data items in a small set of storage nodes. <br/><br/>This design choice motivates a number of questions for this research: (i) where and when should lock states be migrated into the lock service? (ii) when should lock state be repatriated to the data store? (iii) how should the lock service be scaled out? (iv) what are fault-tolerant, low-overhead, deadlock- and livelock-free protocols for these operations? and (v) how can long-lived data access patterns be leveraged in such systems? Building on preliminary results that demonstrate the feasibility and considerable promise of the approach, the project develops algorithms, protocols, analyses, and open-source software, along with comprehensive validation in the context of a diverse set of applications.<br/><br/>The project will result in a novel framework for concurrency control in scalable distributed systems. The concurrency control service has a number of desirable features: (i) modularity -- the service can be instantiated at runtime, with minimal change to underlying data storage organization and access mechanisms; (ii) extensibility ? the service adapts dynamically to load and service requirements; and (iii) high performance through the use of efficient algorithms exploiting data and lock access patterns. These features are achieved through a novel mix of algorithms for lock migration and collocation, statistical models for dynamic lock and data access, protocols for lock state management, associated proofs of correctness and fairness, fault tolerance, performance, and scalability. The concurrency control service is fully validated on private as well as public clouds on a mix of applications drawn from Online Transaction Processing and Machine Learning.<br/><br/>The project directly impacts an important class of cloud-based applications by providing a modular and extensible lock service.  The service relieves burden on the application programmer while providing high performance and elastic throughput. Beyond this, the project includes a number of educational initiatives aimed at undergraduate and graduate education, along with outreach efforts aimed at enhancing representation of minority groups. These include development of instructional material, curricula, organization of and presentations at workshops and summer schools, and recruitment initiatives aimed at students from under-represented groups."
499,1454547,CAREER: Mesoscale Modeling of Defect Structure Evolution in Metallic Materials,CMMI,"Mechanics of Materials and Str, Special Initiatives",2/1/15,8/23/19,Avinash Dongare,CT,University of Connecticut,Standard Grant,Siddiq Qidwai,1/31/21,"$599,999.00 ",,dongare@uconn.edu,438 Whitney Road Ext.,Storrs,CT,62691133,8604863622,ENG,"1630, 1642","013E, 022E, 024E, 027E, 1045, 9161",$0.00 ,"This Faculty Early Career Development (CAREER) Program project focusses on research in advanced computational mechanics for the virtual analysis of structural metallic materials for use in extreme environments. It supports research on advancing the understanding of the factors that control the evolution of defects and their structure, the micromechanisms for their evolution, and their collective influence on material performance. Understanding the links between the evolution of defect structures during operation and material performance and survivability is a key question in the mechanics of materials. The research will define a clear rationale for why a particular material results in improved toughness or improved strengths or both simultaneously. Such insight would support the development of materials for next generation automotive, aerospace, and defense applications. The virtual analysis contributes to the goals of the Materials Genome Initiative by supplementing physical experiments to reduce costs and time in materials deployment.  Educational initiatives through this award will focus on active involvement of undergraduate students and the establishment of a field of study specialization in computational materials science at the University of Connecticut. The integration of materials science, mechanical engineering, and computer science in this framework will help to stimulate an interest in the undergraduate students involved to pursue higher education in science and engineering. Outreach activities will introduce mechanics of materials into pre-college education through leadership roles in local chapters of professional societies and encourage active participation of underrepresented groups to promote diversity in science and engineering education.  <br/>  <br/>The objective of this research is to establish insight into the effects of microstructure and loading conditions on the micromechanisms responsible for the nucleation, accumulation, and interaction of defect structures (dislocations, twins, interfaces) as well as nucleation, growth, and coalescence of voids to form cracks (damage). The research employs a newly developed quasi-coarse-grained dynamics (QCGD) method that is able to retain the atomic scale physics of processes involved during deformation and failure but extends the time and length scale capabilities of molecular dynamics simulations. This approach bridges the gap between the atomistic and continuum simulations, and is located at the mesoscale. Machine learning algorithms will be used to map the evolution and distribution of defect structures to the macroscale stress-strain response and identify the distributions that trigger critical events such as damage initiation. This will allow direct connections between the microstructural evolution during deformation and the strength and toughness response for structural metallic materials. This virtual analysis framework capable of providing insights into the performance and survivability will lead to significant advancements in the current state-of-art for materials modeling and can be extended to other structural materials."
500,1454548,CAREER: Fundamentals of Low-complexity Relaxations for Nonconvex Optimization Problems with Conic Structure,CMMI,OPERATIONS RESEARCH,2/1/15,1/9/15,Fatma Kilinc-Karzan,PA,Carnegie-Mellon University,Standard Grant,Georgia-Ann Klutke,1/31/21,"$500,000.00 ",,fkilinc@andrew.cmu.edu,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,ENG,5514,"034E, 035E, 1045, 8024, 9102",$0.00 ,"This Faculty Early Career Development (CAREER) Program grant will pioneer novel tools for the analysis and design of efficient and scalable algorithms for solving large-scale nonconvex optimization problems with conic constraints. These problems are critical components of operational problems in many diverse fields facing uncertainty such as energy, finance, and telemedicine, and are also  frequently used in extracting useful information from high-dimensional data. While convex conic optimization problems are efficiently solvable, the presence of nonconvexities, such as yes/no decisions, present significant new challenges and the state-of-the-art algorithms do not scale well. This award supports foundational research to establish frameworks that overcome these challenges by exploiting valuable structural information in a unified manner. The main developments will address key trade-offs on relaxation quality and computational tractability. If successful, these developments will advance the fundamental tool set in optimization, thus improving the efficiency of operations in a broad range of activities in the aforementioned sectors, having a profound impact on US economy and society. Progress in this vein will provide valuable insights to researchers and practitioners in interdisciplinary domains such as machine learning and high dimensional statistics. The outcomes of this research will be incorporated into commonly used open-source platforms and integrated into the graduate curriculum. These efforts will also go hand-in-hand with synergistic activities to promote operations research among underrepresented groups as well as encourage creative mathematical problem solving skills in K-12 education. <br/><br/>This award aims to develop foundational theory to study the key properties of structured non-convex sets and design new efficient algorithms. The focus will be on development of new systemic techniques to generate classes of low-complexity relaxations (expressed in linear or conic form) that are effective and easy to incorporate into existing and/or novel algorithmic frameworks. This research will introduce non-traditional paradigms for incorporating more information into the convexification process from different types of cones, multiple conic structures simultaneously present, and specific sources of nonconvexities such as nonconvex quadratics. Degradation of relaxation quality due to partial information use and resulting computational complexity trade-offs will be rigorously quantified. Whenever possible, these developments will be supplemented with explicit results on convex hull characterizations and accompanied with efficient algorithms based on reduced-complexity optimization methods to further enhance the scalability of this approach. Research findings will be studied in a diverse set of models from a number of interdisciplinary fields"
501,1565699,"I-Corps Team: ClaimBuster: Automated, Live Fact-Checking",IIP,I-Corps,11/15/15,11/6/15,Chengkai Li,TX,University of Texas at Arlington,Standard Grant,Steven Konsek,4/30/17,"$50,000.00 ",,cli@uta.edu,"701 S Nedderman Dr, Box 19145",Arlington,TX,760190145,8172722105,ENG,8023,,$0.00 ,"New forms of technology and media have made it easier than ever to disseminate false information, which can have adverse consequences on businesses, the financial sector, science, and beyond.  Fact-checking is thus critically important to increase accountability, improve discourse, and minimize socioeconomic disruptions.  A key challenge is that, in light of the new forms of communications, human fact-checkers have difficulty keeping up with rapidly emerging false data and information.<br/> <br/>This I-Corps team aims to commercialize ClaimBuster, an automated, live fact-checking system that finds claims, matches them with claims that have already been checked by professionals, and, for new claims not yet checked, provides analysis of relevant data to help check their validity.  The team?s technology leverages novel research advances in databases, data and text mining, machine learning, etc.  By participating in the I-Corps Teams program, the team?s goal is to clearly identify target customers and their needs, which will enable the team to prepare a viable commercialization plan and further develop necessary components for deploying their technology."
502,1501418,Measuring the Ages of Stars in the Era of Big Data Time-Domain Astronomy,AST,NSF ASTRON & ASTROPHY PSTDC FE,9/1/15,7/24/17,James Davenport,WA,Davenport               James,Fellowship Award,Harshal Gupta,8/31/18,"$278,000.00 ",,,,Seattle,WA,981951580,,MPS,1609,1207,$0.00 ,"James Davenport is awarded an NSF Astronomy and Astrophysics Postdoctoral Fellowship to carry out a program of research and education at Western Washington University. A key mission in astronomy is to understand the ages and histories of stars within our galaxy, yet their ages remain one of the most difficult properties to measure. When stars are young they spin rapidly and exhibit strong magnetic activity, including frequent high energy flares that can potentially impact life on nearby planets. Stars lose angular momentum over time, slowing their rotation, which should decrease this magnetic activity and generate fewer flares. This change in flare rate can be used as an approximate clock and is crucial for understanding how stars and planets evolve over time. To calibrate this clock, Davenport will use the ultra-precise data from NASA's planet-hunting Kepler mission as a training sample, finding every flare from over 200,000 stars. This work requires analyzing large volumes of data using modern computational techniques. Davenport will use this Kepler data to train students at smaller institutions core principles of data science and visualization, which are critical for work in technical fields.<br/><br/>Using stellar magnetic activity as a clock has long been suggested as a possible means for determining stellar ages. Furthermore, planet habitability may be impacted by the evolution of the host star's magnetic activity over time. Interest in low-mass stars as exoplanet hosts has fueled a growing need for better constraints on stellar ages. With the advent of continuous monitoring from space-based missions like Kepler, we are finally able to calibrate this method. Davenport will develop the first measurement of this activity-age relationship by studying the flare rates of stars from the Kepler mission. Over 200,000 stars will be studied for flares using machine learning and time series statistical techniques. This endeavor will involve using modern data science techniques to mine large databases. The demand for such skills in the private and public sectors is growing rapidly, and Davenport will use large, dynamic, and open access astronomical datasets to teach data science methods. Gaining experience in data science for STEM majors can be challenging at smaller institutions, and data from the Kepler mission are ideal for teaching these skills. The course will also serve as a template for data science STEM training in other departments at small institutions."
503,1521529,Collaborative Research: Novel Computational and Statistical Approaches to Prediction and Estimation,DMS,CDS&E-MSS,8/1/15,8/10/17,Alexander Rakhlin,PA,University of Pennsylvania,Continuing Grant,Yong Zeng,8/31/18,"$150,000.00 ",,rakhlin@mit.edu,Research Services,Philadelphia,PA,191046205,2158987293,MPS,8069,"7433, 8084, 9263",$0.00 ,"Terabytes of data are collected by companies and individuals every day. These data possess no value unless one can efficiently process them and use them to make decisions. The scale and the streaming nature of data pose both computational and statistical challenges. The objective of this research project is to develop novel approaches to making online, real-time decisions when data are constantly evolving and highly structured. In particular, this project focuses on online prediction problems involving multiple users in dynamic networks. The project also aims to tackle the privacy issues arising in such multi-user scenarios.<br/><br/>In recent years, it was shown that a majority of online machine learning algorithms can be viewed as solutions to approximate dynamic programming (ADP) problems that incorporate one additional datum per step. Along with directly addressing the computational concerns, the ADP framework also provides guaranteed performance on prediction problems. This project is to use and extend the ADP framework to develop prediction algorithms that simultaneously address the issues of computation, robustness, non-stationarity, privacy, and multiplicity of users."
504,1541876,FEW: A Workshop to Identify Interdisciplinary Data Science Approaches and Challenges to Enhance Understanding of Interactions of Food Systems and Water Systems,IIS,Sustainable Energy Pathways,6/1/15,6/19/15,Shashi Shekhar,MN,University of Minnesota-Twin Cities,Standard Grant,Sylvia Spengler,5/31/17,"$50,000.00 ",David Mulla,shekhar@cs.umn.edu,200 OAK ST SE,Minneapolis,MN,554552070,6126245599,CSE,8026,"004Z, 7556",$0.00 ,"In coming decades, the world population is projected to grow significantly increasing the demand for food, water, energy, and other resources. Furthermore, these resource challenges may be amplified due to climate change, urbanization, and the interdependent and interconnected nature of food, energy, and other resources. Furthermore, these resource challenges may be amplified due to environmental changes, urbanization, and the interdependent and interconnected nature of food, energy and water (FEW) systems, which were traditionally analyzed and planned independently. Such piece-meal approaches (e.g., bio-fuels) to solving problems in one system (e.g., energy) have led to unanticipated problems (e.g., increase in food prices) in other systems. The goal of the nexus FEW security approach is to reduce such surprises by understanding, appreciating and visualizing the interconnections and interdependencies in the FEW system of systems at local, regional and global levels. However, the nexus approach for sustainable management of global resources faces significant challenges due to differences in data collection protocols, data representation standards, access to complete data and data analysis tools. In addition, the FEW system of systems provides major challenges and opportunities for novel data science research. Although data science analysis methods extensively applied to large and complicated systems, such as social networks, data science efforts in complex physical systems (e.g., system of FEW systems) have been far more meager. Given FEW systems' rich data-driven history, there is a tremendous opportunity to systematically integrate novel large-scale data analysis methods with the physical, experiential, process oriented, and even conceptual knowledge that the broad climate, water, and energy research communities have developed. In addition, data science methods need to account for dependence between models, variables, locations and seasons (of food, energy and water systems) to reduce the risk of yielding misleading results.<br/><br/>There is a tremendous need to significantly advance data science and realize the promise of the nexus approach to meet societal challenges in the face of population growth, urbanization and climate change. The proposed workshop will gather thought leaders from both data science and the relevant areas of system of food, water, and energy systems. This workshop will use both FEW nexus pull and data science technology push discussions to identify data science challenges in understanding, appreciating and visualizing the FEW systems. The first two successive sessions will explore these opposing directions. The second day will identify FEW inspired data science grand challenges in a synthesis session. Specifically, the goal of this workshop will be to create a vision of how data driven methods could make a significant contribution to understanding interactions between FEW systems and what research is needed to realize that vision. This proposal provides a detailed schedule of milestone and tasks including this team's resume for leading visioning workshops. The proposed workshop will facilitate and enable interdisciplinary partnerships between data scientists and FEW nexus researchers from academia, industry and federal agencies to develop innovative, interdisciplinary research approaches enhancing the understanding, appreciation, and visualization of the interactions between FEW systems. It has potential to formulate next generation data science research agenda towards better understanding, appreciation and visualization of the interactions and interdependencies among FEW systems. Workshop report will be included in reading lists of graduate courses on data science in Ph.D. to integrate the results in education. The report will also be used in professional graduate data science degrees for workforce training. A key goal will be to diversify participation across career stages, under-represented groups, geographies, and disciplines (e.g., machine learning, data mining, geo-spatial analytics, and nexus of food, water and energy systems)."
505,1540119,I-Corps: Health-Beacons: Flexible Wearable Health and Activity Tracker for Smart Living,IIP,I-Corps,5/15/15,5/15/15,Sajal Das,MO,Missouri University of Science and Technology,Standard Grant,Steven Konsek,10/31/16,"$50,000.00 ","Debraj De, Bonnie Bachman, Xian Huang",sdas@mst.edu,300 W 12th Street,Rolla,MO,654096506,5733414134,ENG,8023,9150,$0.00 ,"This I-Corps project aims at designing Health-Beacons, which are flexible wearable devices for health and activity tracking. Health-Beacons can provide valuable insights into a user's daily activity and health patterns, thus creating proactive awareness for improved wellness, health performance and quality of living. The benefiting community includes elderly people and their relatives, athletes, doctors and caregivers, among others. Commercial success of Health-Beacons technology will significantly reduce federal expenditures on Medicare, Medicaid and other health/wellness management programs for millions of active people as well as the elderly generation in the United States. The broader impacts of this project also include educational and outreach activities, multi-disciplinary student training, and promoting innovation and entrepreneurship culture among Missouri S&T students.<br/> <br/>The Health-Beacons system will integrate innovative bendable skin conformable electronics, multi-modal sensing, and machine learning-based sensor data analytics, in collaboration with wireless communications through Bluetooth beacons. The key novelty lies in flexible electronics, intelligent sensor data analytics techniques, and predictive modeling. Through contextual information visualization and interaction, Health-Beacons will be made user-friendly to allow interfacing with a multitude of technologies. The developed system will provide users with a quantified smart living dashboard based on health and activity tracking that can also be displayed on smartphones or smartwatches. Considering that wearable technology for health and wellness management is still in its infancy with early adopters, the Health-Beacons project will create a new body of knowledge in wearable computing, sensor data analytics, and smart healthcare."
506,1453397,CAREER: Differentiating mechanisms of ecological divergence in sympatric microbial populations using an integrated population genomics approach,DEB,"EVOLUTIONARY ECOLOGY, Dimensions of Biodiversity",5/1/15,7/13/18,Peter Bergholz,ND,North Dakota State University Fargo,Continuing Grant,Leslie J. Rissler,8/31/20,"$810,512.00 ",,peter.bergholz@ndsu.edu,Dept 4000 - PO Box 6050,FARGO,ND,581086050,7012318045,BIO,"7377, 7968","1045, 9150, 9169, EGCH",$0.00 ,"A centerpiece of evolutionary ecology research is to understand how new adaptations arise and spread in populations leading to speciation.  The adaptation of microbes is important to understand, because microbes are: a) the primary drivers of global biogeochemical cycles, b) the means of production for many applications in biotechnology, and c) the causes of most infectious diseases. Microbes are frequently moved around by the action of animals, plants, wind and water, and may be deposited in unfavorable habitats. As a consequence, microbes adapt to new environments through to a combination of fast mutation rates and acquisition of new genes directly from their environment. There is a pressing need to better understand these processes by which microbes generate and maintain adaptive genetic variation, because such information is needed to forecast the responses of microbial species to rapid changes in land management or climate. This project studies how adaptive genetic variation permits a common bacteria important in biotechnology and the food supply, Escherichia coli, to survive after deposition in unfavorable habitats. Data on this process can tell us about how adaptive variation in populations contributes to the formation of new species. <br/><br/>This project is designed to use the process of passive dispersal to diverse habitats as a natural experiment in order to differentiate between three models of ecological divergence in microbial populations. Studies of the landscape level structure in microbial populations will be complemented with genome-wide association studies. These experiments will link genomic polymorphisms to variation in phenotype, gene expression regulation, and source environment characteristics.  These analyses will be coupled with laboratory challenges to quantify the adaptive value of genomic polymorphisms. Machine learning analytical techniques will be used to determine which of three models of ecological divergence best explains the data. An education program will teach undergraduate students about the ecological processes contributing to microevolution and speciation in microbes. The program will combine an agent-based model and data from experiments to teach students to interpret genomic adaptations in light of ecological processes acting on microbes in natural environments."
507,1502242,SCH: EXP: Contingent Cueing for the Alleviation of Symptoms of Parkinson's Disease in Ambient Settings,IIS,"Smart and Connected Health, EPSCoR Co-Funding",9/1/15,4/17/20,Daniel Rucker,TN,University of Tennessee Knoxville,Standard Grant,Wendy Nilsen,8/31/21,"$551,196.00 ",Jindong Tan,drucker6@utk.edu,1331 CIR PARK DR,Knoxville,TN,379163801,8659743466,CSE,"8018, 9150","8018, 8061, 9150",$0.00 ,"Millions of Americans live with incurable chronic conditions such as stroke, traumatic brain injury, and Parkinson's disease (PD). These degenerative neurological conditions cause difficulty in the performance of daily life activities such as walking and self-care, limit independence, and lead to depressed quality of life. There are currently no quantitative tools that allow us to understand how these individuals are affected by their conditions on a daily basis, and how rehabilitation and medication affect these people in the long term. The advent of novel technologies such as smart phones and smart watches facilitates exciting new opportunities. These tools allow us to quantitatively monitor, and to positively affect the behaviors of such individuals. This award supports research that will determine how to measure the movements and behaviors of these individuals, and then tests a technique that uses real-time feedback and stimuli to minimize the disease symptoms. We evaluate our approach in individuals suffering from PD, a particularly debilitating disease. Research results will provide much richer information on how people respond to therapy and medication, and may prove useful in other populations of people aging with and into disability. The approach, which integrates engineering and medicine, will help broaden the participation of underrepresented groups in science, technology, engineering, and mathematics research.<br/><br/>Wearable sensing and telehealth are promising tools for revolutionizing healthcare outside of the clinical setting. However, the adaptation of these technologies to specific populations in remote settings remains a challenge. The current research approach focuses on individuals living with the chronic effects of Parkinson's Disease (PD). We propose a system that combines wearable sensors and smart phones to assist these individuals with a particularly debilitating syndrome, freezing of gait. This research will fill the knowledge gap of how to monitor and affect the symptoms of these individuals in ambient settings. We leverage electronic sensor design, machine learning, and signal processing approaches to: (1) determine user-specific models of movement from worn sensor data; (2) develop algorithms to predict onset of user symptoms; and (3) provide context-dependent external stimuli to alleviate symptoms. These methods may generalize to other individuals living with the chronic disability."
508,1453204,"CAREER: Many-body Ab initio Potentials and Quantum Dynamics Methods for ""First Principles"" Simulations in Solution: Hydration, Vibrational Spectroscopy, & Proton Transfer/Trans",CHE,"CAREER: FACULTY EARLY CAR DEV, Chem Thry, Mdls & Cmptnl Mthds",4/1/15,2/19/15,Francesco Paesani,CA,University of California-San Diego,Standard Grant,Evelyn Goldfield,3/31/20,"$625,000.00 ",,fpaesani@ucsd.edu,Office of Contract & Grant Admin,La Jolla,CA,920930621,8585344896,MPS,"1045, 6881","1045, 7433, 8084, 9216, 9263",$0.00 ,"Francesco Paesani of the University of California San Diego is supported by a CAREER award from the Chemical Theory, Models, and Computational Methods program in the Chemistry Division and the Division of Advanced Cyberinfrastructure to develop new theoretical and computational approaches for molecular-level computer simulations.   Such simulations have become a powerful tool in chemistry, often providing fundamental insights into complex phenomena which are otherwise difficult to obtain. However, achieving the necessary accuracy for realistic and predictive simulations remains challenging.  Paesani and his research group are meeting this challenge by combining a variety of approaches to generate very accurate models of many systems including ions in solution.  The new methodology enables computer simulations of aqueous systems with unprecedented accuracy, providing information on fundamental molecular processes from ion hydration in bulk and at interfaces to proton transfer and transport in solution. The new methodology will be available to the community through its implementation in the open and free OpenMM software toolkit for molecular simulations.  This ""reference implementation"" aims to provide the community with a completely open computational tool which may be used by other researchers interested in implementing it in their own simulation codes. In addition, this initial implementation is a starting point for future developments of unique software elements specifically designed for high-performance computing which will enable many-body simulations with unprecedented accuracy on both multicore CPU and GPU architectures.<br/><br/>In parallel with the proposed research activities,  Paesani has established an innovative education and outreach plan focusing on the development of an entry level course that introduces undergraduate students in their freshman and sophomore years to the use of computational methods in chemistry, as well as on mentoring activities specifically designed to promote study in the STEM disciplines among students from underprivileged and traditionally underrepresented groups through the development of a summer exchange program at UC-San Diego.<br/><br/>  <br/>Both the realism and the predicting power of a computer simulation strongly depend on the accuracy with which the molecular interactions and the overall system dynamics are described. Although ab initio methods can, in principle, enable the characterization of physicochemical processes without resorting to ad hoc simplifications, the associated computational cost effectively prevents the use of these methods to model realistic condensed-phase systems. Furthermore, a rigorous description of the actual molecular dynamics often requires a quantum-mechanical treatment of the nuclear motion, which further increases the computational cost associated with ab initio computer simulations.   The methods developed by Paesani and coworkers seeks to overcome these limitations by combining machine-learning many-body potential energy surfaces derived entirely from highly-correlated electronic structure data with novel quantum-dynamical approaches based on path-integral molecular dynamics and centroid molecular dynamics. The efficient integration of these components pushes the boundaries of current molecular dynamics techniques and provides new opportunities for realistic simulations of condensed-phase systems in direct connection with corresponding spectroscopic measurements. Although much broader in scope,  the initial application of the methodology is to modeling physicochemical processes in solution, with a specific focus on ion hydration, linear and nonlinear vibrational spectroscopy, and proton transfer/transport. The new methodology will be made available to the community through its implementation in the C++ ""reference platform"" of the open and free OpenMM software toolkit for molecular simulations. Specifically,  implementation will consist of an independent plug-in to provide the community with a completely open implementation of these many-body potentials. This plug-in will include a complete suite of unit tests that cover all energy and force components as well as the inner functions of our many-body potentials. A number of test cases will also be made available for comparing output energies and forces obtained with OpenMM with the reference values calculated with the PI's in house implementation. To facilitate the use of the new many-body potentials, the plug-in will also offer a Python wrapper that will simplify both setting up and running many-body molecular simulations to the point where all simulation parameters will be entirely defined in an XML file. This plug-in will thus provide other researchers with a comprehensive implementation of our many-body potentials for aqueous simulations, which can be used as a reference for the implementation in other software. In addition, this reference implementation will serve as a starting point for future developments of unique software elements for the OpenMM toolkit, specifically designed for high-performance computing on both multicore CPU and GPU architectures.  The development and application of the new simulation methodology will involve the training and education of undergraduate and graduate students as well as postdoctoral fellows, who will acquire a solid foundation in theoretical, physical, and computational chemistry. The interdisciplinary nature of the proposed project will provide an opportunity for students and postdocs to establish bridges and inter-connections between the fundamental laws of physical chemistry at a molecular level and the properties of condensed-phase systems. The possibility to work at the interface of different disciplines will prepare both students and postdocs for a wide range of scientific careers."
509,1545433,NRT-DESE:  Interdisciplinary Disease Ecology Across Scales: from Byte to Benchtop to Biosphere,DGE,NSF Research Traineeship (NRT),9/1/15,8/13/15,Vanessa Ezenwa,GA,University of Georgia Research Foundation Inc,Standard Grant,John Weishampel,8/31/21,"$2,997,528.00 ","Duncan Krause, John Drake",vezenwa@uga.edu,310 East Campus Rd,ATHENS,GA,306021589,7065425939,EHR,1997,"9179, SMET",$0.00 ,"NRT-DESE:  Interdisciplinary Disease Ecology Across Scales: from Byte to Benchtop to Biosphere<br/><br/>Infectious diseases are an ongoing threat to human health, animal health, and social stability. This National Science Foundation Research Traineeship (NRT) project prepares doctoral students at the University of Georgia with data-enabled science and engineering training to solve complex, interdisciplinary problems related to diseases and the environment. Effectively predicting where new infectious agents will go next, how fast they will spread, and how severely humans or other species may be impacted requires a basic knowledge of how hosts and pathogens interact at different scales of biological organization (e.g., cell, organism, population), as well as the ability to identify how processes taking place at one scale respond to or affect processes occurring at adjacent scales. This traineeship anticipates preparing thirty (30) doctoral students, including twenty (20) funded trainees, with mastery of both empirical and computational dimensions of contemporary infectious disease problems. <br/><br/>Research activities associated with the program focus on three topical areas critical to unraveling the complexity of host-pathogen interactions in nature: co-infection and pathogen interactions; environmental change and disease dynamics; and tipping points and early warning signals. Faculty from thirteen academic units with empirical expertise spanning single cells to whole ecosystems, and quantitative expertise in statistical modeling, mathematical modeling, stochastic processes, adaptive dynamics, machine learning, and geographic mapping will co-mentor student research. This traineeship will use interdisciplinary case study-based teaching, instruction in advanced techniques for computer modeling, and internships at non-academic and international organizations to provide students with the conceptual background, computational skills, and global perspective needed to tackle the most pressing infectious disease problems of our time. In addition, a range of professional development activities including a capstone experience, data clinics, internships, study abroad, and communication training will provide students with the skills to transfer basic knowledge to real world problems in both academic and non-academic settings. <br/><br/>The NSF Research Traineeship (NRT) Program is designed to encourage the development and implementation of bold, new, potentially transformative, and scalable models for STEM graduate education training.  The Traineeship Track is dedicated to effective training of STEM graduate students in high priority interdisciplinary research areas, through the comprehensive traineeship model that is innovative, evidence-based, and aligned with changing workforce and research needs."
510,1518382,BALSims:  a Spectral Synthesis Approach to Understanding Broad Absorption  Line Quasar Outflows,AST,EXTRAGALACTIC ASTRON & COSMOLO,9/1/15,6/7/17,Karen Leighly,OK,University of Oklahoma Norman Campus,Continuing Grant,Joseph E. Pesce,8/31/19,"$269,644.00 ",,leighly@ou.edu,201 Stephenson Parkway,NORMAN,OK,730199705,4053254757,MPS,1217,"1207, 7480, 9150",$0.00 ,"A longstanding problem in astrophysics is to understand how galaxies form and develop throughout their lifetimes.  Such understanding is necessary to uncover how our Universe evolved and to gain insight into the origin of our own Milky Way Galaxy.  One important aspect of understanding galaxy formation and evolution is to study supermassive black holes at the centers of galaxies and how these black holes interact with their host galaxies.  Observational evidence suggests that black holes and galaxies evolve together.  This evidence includes the tightness of the correlation between the black hole mass and galaxy bulge properties and the similar history of black hole growth and star formation.  This evidence is supported by theoretical arguments.  Black holes in their active phase emit more than enough energy to unbind the host galaxies' bulge, and in their non-active phase should be present in the centers of most present-day galaxies.  As a consequence, many models for galaxy formation include a unspecified process by which a fraction of the energy released in the region surrounding the black hole contributes to the galaxy formation environment.  This interaction is generically known as ""feedback,"" and the mechanism by which the black hole acts on the host galaxy remains a mystery.  This project uses a novel spectral synthesis and statistical approach to build the tools needed to constrain the key parameter characterizing one type of feedback.<br/><br/>In addition, the project activities will directly contribute to the training of young scientists, including undergraduate students, in techniques of spectral and statistical analysis.  The PI will develop and teach a course, directed toward advanced undergraduate and graduate students, on machine learning and statistical methods, some of which will be used in this project.  The PI will bring the results of this research to the citizens of Oklahoma through participation in the OU speakers program.<br/><br/>This research effort will constrain the key parameter characterizing quasar mode feedback: the ratio of the kinetic luminosity to bolometric luminosity, in the SDSS broad absorption line quasars as a whole.  In addition, the carefully-phased research program addresses related scientific questions at each check point.  These include the location and nature of feedback interactions, a search for and evaluation of absorption-line spectral diagnostics, and constraints of key outflow physical parameters, including column density, ionization parameter, and covering fraction.<br/><br/>This research project will yield insights into the physics of quasar outflows, and measurements of key parameters that will be valuable to a wide variety of researchers.  These include scientists who are interested in on-going interactions potentially occurring in infrared-loud quasars, data analysts who use spectral diagnostics to infer physical conditions from spectra of individual objects, and theorists who need observational constraints to develop their hydromagnetic wind models, as well as providing an assessment of the viability of quasar-mode feedback for galaxy evolution models."
511,1458557,Collaborative Research: ABI Innovation: Computational population-genetic analysis for detection of soft selective sweeps,DBI,ADVANCES IN BIO INFORMATICS,8/1/15,7/23/15,Vineet Bafna,CA,University of California-San Diego,Standard Grant,Peter McCartney,7/31/20,"$600,000.00 ",,vbafna@cs.ucsd.edu,Office of Contract & Grant Admin,La Jolla,CA,920930621,8585344896,BIO,1165,,$0.00 ,"The molecular process of adaptation-the rise in frequency of genetic variants that enable organisms to succeed in their environments-is a central process in evolutionary biology. Surmounting significant challenges such as the ability of infectious agents to evolve resistance to drugs and the ability of crop pests to defeat a diverse array of increasingly powerful insecticides requires an understanding of the nature of adaptation. Recent advances have demonstrated that adaptation often occurs via ""soft selective sweeps,"" in which an adaptive genetic variant originates multiple times or has become favored only after it has been present at a substantial frequency in the population. This project contributes to advancing knowledge of the fundamental evolutionary process of adaptation by developing new computational tools to detect and study the occurrence of adaptation by soft selective sweeps. Through the interactions of a multidisciplinary team spanning evolutionary biology and bioinformatics, the project integrates advances in evolutionary simulation with modern and efficient computational methods in order to produce progress on understanding adaptation, while simultaneously developing efficient computational tools applicable in the modern ""big-data"" era of inexpensive sequencing. In addition, its joint mentorship efforts from evolutionary and bioinformatics perspectives promote interdisciplinary training of graduate students and postdoctoral scientists. <br/><br/>The project has four objectives: (1) To design new tests for detecting selection in the case in which soft selective sweeps occur from standing genetic variation; (2) To identify haplotypes that carry a beneficial allele in genomic regions known to be experiencing positive selection; (3) To enhance new methods of analysis of natural selection to make them robust to confounding demographic scenarios; (4) To apply new selection methods in a series of data sets from multiple species, including humans, Drosophila, and Plasmodium malaria parasites. The project will use algorithmic techniques from combinatorial optimization and machine learning, and it will exploit ideas from population genetics and coalescent theory. It breaks ground on several fronts, providing a deeper understanding of the patterns in site-frequency spectra and haplotype data as a basis for selection signatures, and assisting in the design of subtyping studies for complex regions of the genome. As it becomes increasingly possible to sequence whole genomes of multiple individuals within a population, the intellectual challenge of designing tools for detecting selection to accommodate new phenomena such as soft sweeps coincides with the computational challenge of incorporating genomic data sets into selection studies. These challenges are addressed by the project, whose results will be available at http://proteomics.ucsd.edu/vbafna/research-2/nsf1458059/."
512,1454515,CAREER: Statistical Analysis of Massive Data Sets under Low-Complexity Constraints,DMS,"STATISTICS, Division Co-Funding: CAREER",6/1/15,8/24/18,Karim Lounici,GA,Georgia Tech Research Corporation,Continuing Grant,Gabor Szekely,8/31/18,"$203,293.00 ",,klounici6@math.gatech.edu,Office of Sponsored Programs,Atlanta,GA,303320420,4048944819,MPS,"1269, 8048",1045,$0.00 ,"The development of methods of statistical inference for massive data sets has become a focal point of research in statistics and machine learning in recent years with the dramatic increase of collected data in a wide range of applications such as meteorology, genomics, or finance. A common denominator of these new data sets is that they satisfy certain low-complexity structure conditions. In specific settings, this could mean, for instance, sparsity of a vector in high-dimensional regression or low-rank properties of a high-dimensional matrix. This research aims at providing new efficient solutions exploiting these particular structures in order to perform accurate inference. The future findings should be of interest to a broad audience in the data science community. The investigator also intends to integrate his contributions into educational activities such as course development and mentoring of undergraduate and graduate students.<br/><br/>The research objectives of this project are to identify new informative low-complexity structures in large scale data sets and propose new methods that are adaptive to these structures, computationally efficient, and statistically optimal in a variety of models, including in particular vector regression, trace regression, principal component analysis for standard and functional data. The new procedures will be based on penalized empirical risk minimization and exponential weights mixing that favor low-complexity solutions. The investigator plains to (a) determine fundamental characteristics of the problems that govern the performance of these procedures; (b) establish new oracle inequalities results in high dimension to assess the statistical performances of these procedures; (c) investigate the computational performance of these procedures under various conditions on the models."
513,1526118,SHF: Small: Collaborative Research:Text Retrieval in Software Engineering 2.0,CCF,Software & Hardware Foundation,9/1/15,7/20/15,Andrian Marcus,TX,University of Texas at Dallas,Standard Grant,Sol Greenspan,8/31/19,"$200,000.00 ",,amarcus@utdallas.edu,"800 W. Campbell Rd., AD15",Richardson,TX,750803021,9728832313,CSE,7798,"7923, 7944",$0.00 ,"Software systems contain large amounts of textual information captured in various software artifacts, such as, requirements documents, source code, user manuals, etc.  The productivity of software developers and the quality of the software they produce directly depends on their ability to retrieve and understand the textual information present in software.  Since humans cannot process and comprehend so much text, researchers proposed the use of text retrieval techniques to help software developers with many of their daily tasks.  In order to be useful, these techniques need to be properly configured, which requires calibrating many parameters.  As most software developers are not experts in text retrieval, they need help in determining the best text retrieval configuration in a given software engineering context.  The configuration problem is one of the main obstacles in the adoption of such techniques in the software industry, because many approaches proposed by researchers do not generalize well.  The outcomes of this project will transform the way software developers address many of their daily tasks, allowing them to easily adopt the use of text retrieval during software development.  The results of this research will also be used in software engineering courses to support students in their projects.  The new practices that the students will acquire will help them become better software engineers.  The proposed research also brings together work from different computing research communities: software engineering and information retrieval and it will bring new knowledge in both fields.  Existing approaches using text retrieval in software engineering will become more practical, rather than just promising, facilitating migration from the lab into industry and academia.<br/><br/>The outcome of this research will be: (1) a novel approach (called TRinSE2.0), which will achieve automatic, runtime query-based text retrieval configuration; and (2) improvements to important software engineering tasks, in practical settings, focusing on feature and bug location, impact analysis, traceability link recovery, and bug triage.  TRinSE2.0 will be evaluated on open source data, in the classroom, and in industrial settings.  The proposed work will transform the way text retrieval configuration is done in software engineering applications.  New, software-specific measures, as well as proven linguistic-based measures will be used to capture query properties in the context of software engineering tasks and data sets.  Machine learning algorithms will find the best configuration for a given query.  When writing a query to retrieve information from a software project, developers will get the best results, saving them time and effort, improving their productivity and the quality of their work.  The text retrieval configuration problem will no longer be heuristic-based, but it will become data-driven."
514,1505116,Collaborative Research: High-Throughput Quantification of Solid State Electrochemistry for Next Generation Energy Technologies,DMR,CERAMICS,7/1/15,7/10/18,Ichiro Takeuchi,MD,University of Maryland College Park,Continuing Grant,Lynnette Madsen,6/30/19,"$455,000.00 ",,takeuchi@umd.edu,3112 LEE BLDG 7809 Regents Drive,COLLEGE PARK,MD,207425141,3014056269,MPS,1774,"7237, 8396, 8399",$0.00 ,"NON-TECHNICAL DESCRIPTION:<br/>The goal of this research to advance the fundamental understanding of the behavior of oxide electrodes used in fuel cells, electrolysis cells, batteries, and other energy technologies. The approach combines high-throughput synthesis of libraries of material structures, with advanced high-throughput characterization and high-throughput data analysis. By making use of structures with well-defined geometric features, it is possible to directly interpret the electrochemical data. The insight afforded in turn enables deliberate engineering of structures to achieve exceptional performance. It also provides chemical guidance on how to create next generation materials. The performance enhancements can ultimately advance goals in sustainable energy. A broad cross-section  of students at all levels are incorporated into the research and training goals of this effort via internships for high school and undergraduate students, as well as doctoral research opportunities for graduate students. Outreach efforts include engaging local K-12 students in science and engineering.<br/><br/>TECHNICAL DESCRIPTION:<br/>This work aims to dramatically advance the understanding of electrochemical reaction pathways by making use of geometrically well-defined systems. Typical electrochemical structures incorporate random, high-surface area features to maximize overall performance and are not well-suited to extraction of fundamental behavior. In contrast, geometrically well-defined systems enable determination of properties such as length-specific triple-phase boundary activity, bulk chemical diffusion coefficient, area-specific surface activity, and much more. These are essential parameters for the deliberate engineering of high-performance structures. The painstaking nature of acquiring such data using individually prepared samples has, however, limited the study of geometrically well-defined electrochemical systems to a few important examples, despite growing recognition of its value. In this project, advanced fabrication tools are utilized to create libraries of electrode structures on electrolyte substrates and rapidly measure the entire contents of each library using an in-house constructed, unique scanning electrochemical probe system. Computational tools are developed to handle the massive quantities of data generated, including data mining and machine learning capabilities to create efficiencies in data acquisition and analysis. Libraries of geometrically graded microdot electrodes are complemented with selected compositionally-graded libraries, with the compositional space identified to further elucidate rate-limiting steps. Electrochemical studies are complemented with a broad suite of physical and chemical characterization methods to provide a comprehensive picture of material behavior as relevant to electrocatalysis. Generation of new insights into electrochemical reaction pathways is an essential step in the creation of next-generation electrochemical energy storage and conversion devices and as such has an important role in a sustainable energy future."
515,1522106,SCH: INT: Collaborative Research: S.E.P.S.I.S.: Sepsis Early Prediction Support Implementation System,IIS,Smart and Connected Health,10/1/15,5/8/17,Jeanne Huddleston,MN,Mayo Clinic Rochester,Standard Grant,Sylvia Spengler,9/30/19,"$368,539.00 ",,huddleston.jeanne@mayo.edu,200 First Street SW,Rochester,MN,559050001,5072844715,CSE,8018,"8018, 8062, 9102, 9251",$0.00 ,"Sepsis, infection plus systemic manifestations of infection, is the leading cause of in-hospital mortality. About 700,000 people die annually in US hospitals and 16% of them were diagnosed with sepsis (including a high prevalence of severe sepsis with major  complication). In addition to being deadly, sepsis is the most expensive condition associated with in-hospital stay, resulting in a 75% longer stay than any other condition.  The total burden of sepsis to the US healthcare system is estimated to be $20.3 billion, most of which is paid by Medicare and Medicaid. In fact, in June 2015 the Centers for Medicare & Medicaid Services (CMS) reported that sepsis accounted for over $7 billion in Medicare payments (second only to major joint replacement), a close to 10% increase from the previous year.  This pervasive drain on health care resources is due, in part, to difficulties in diagnosis and delayed treatment. For example, every one hour delay in treatment of severe sepsis/shock with antibiotics decreases a patient's survival probability by 10%. Many of these deaths could have been averted or postponed if a better system of care was in place. The goal of this research is to overcome these barriers by integrating electronic health records (EHR) and clinical expertise to provide an evidence-based framework to diagnose and accurately risk-stratify patients within the sepsis spectrum, and develop and validate intervention policies that inform sepsis treatment decisions.  The  project to bring together health care providers, researchers, educators, and students to add value to patient care by integrating machine learning, decision analytical models, human factors analysis, as well as system and process modeling to advance scientific knowledge, predict sepsis, and prevent sepsis-related health deterioration. In addition to the societal impact that clinical translation of these findings may bring, the project will provide engineering and computer science students and health services researchers with cross-disciplinary educational experience.<br/><br/>The proposed research will apply engineering and computer science methodologies to analyze patient level EHR across two large scale health care facilities, Mayo Clinic Rochester and Christiana Care Health System and to inform clinical decision making for sepsis. The multi-institutional, interdisciplinary collaboration will enable the development of health care solutions for sepsis by describing and accurately risk-stratifying hospitalized patients, and developing decision analytical models to personalize and inform diagnostic and treatment decisions considering patient outcomes and response implications. The Sepsis Early Prediction Support Implementation System (S.E.P.S.I.S.) project aims will be to: 1) Develop data-driven models to classify patients according to their clinical progression to diagnose sepsis and predict risk of deterioration, thus informing therapeutic actions. 2) Develop personalized intervention policies for patients within the sepsis spectrum.  3) Develop decision support systems (DSS) for personalized interventions focusing on resource implications and usability within a real hospital setting. The team will 1) identify important factors that uncover patient profiles based on Bayesian exponential family principal components analysis; 2) develop hidden Markov models (HMMs) and input-output HMMs to identify clusters of patients with similar progression patterns within the sepsis spectrum; 3) provide an analytical framework to support sepsis staging in clinical practice using bilevel optimization. They will 1) predict short- and long-term individual patient outcomes using multivariate statistical models and simulation; 2) develop semi-Markov decision process and partially observable semi-Markov decision process models to identify timing of therapeutic actions and diagnostic tests. Furthermore, the team will 1) predict demand for resources and develop and validate a hybrid mixed integer programming and queueing model to optimize system level allocations; 2) utilize human factors analysis and usability testing to assess the implementation of the DSS."
516,1527034,CSR: Small: ENACT: Environment-Aware Management of Mobile Systems,CNS,"Special Projects - CNS, CSR-Computer Systems Research",10/1/15,6/12/18,Tajana Rosing,CA,University of California-San Diego,Standard Grant,Marilyn McClure,9/30/19,"$466,000.00 ",,tajana@ucsd.edu,Office of Contract & Grant Admin,La Jolla,CA,920930621,8585344896,CSE,"1714, 7354","7923, 9102, 9251",$0.00 ,"Mobile systems are pervasive and have been adapted for diverse computing needs throughout the world. Smart mobile systems, with advanced computing and connectivity, promise superlative experience, yet battery constraints limit their use. Device temperatures are also a major concern as high temperatures reduce hardware reliability but also because human skin can tolerate only moderately warm phones. Mobile system usage is highly influenced by user context, hence can provide a crucial input for proactive, instead of reactive, device power, thermal and reliability management. Even with a small amount of contextual information such as user location and motion, battery lifetime can be improved by as much as a factor of five. Yet, despite the fact that there are already many sensors available in mobile systems, there have been no simple and flexible ways to incorporate context into mobile system resource management. To enable mobile systems to become more context-aware, this project is designing ENACT, an ENvironment Aware ConTrol framework that leverages overarching sensor data for system-wide context-aware management. <br/><br/>ENACT is a lightweight framework that enables a mobile system to tap into a vast array of sensors, and leverage comprehensive context about hardware, software, and user, to control system-wide actuators. The framework has two main components. A context recognition service leverages well-established statistical techniques to robustly derive semantic information from raw sensor data. Then, by observing system behavior with respect to context and actuators, it computes stochastic models. A context-aware control service, based on stochastic model predictive control, smartly sets the system's actuators to minimize the energy consumption while meeting reliability, temperature, and performance constraints. To efficiently meet these requirements, the control framework is supported by a hierarchical structure working at different time scales. ENACT can efficiently transforming a large set of raw sensor data into usable contextual information accessible system-wide, and leverage context to configure a set of actuators to improve the way mobile system resources are managed while providing excellent user experience. This allows for accurately aligning system goals to performance expectations. The project includes the implementation of novel context-aware techniques for energy, thermal and reliability management, which can smartly adapt to dynamic performance requirements without sacrificing energy margins. Students involved in this project gain valuable training in disciplines across machine learning, hierarchical control, optimization and resource management. The ENACT framework can be leveraged in academia and industry alike to implement context-awareness in systems, and to design resource management policies that can leverage context."
517,1522786,A Non-Convex Approach for Signal and Image Processing,DMS,"COMPUTATIONAL MATHEMATICS, GOALI-Grnt Opp Acad Lia wIndus",9/1/15,12/14/18,Yifei Lou,TX,University of Texas at Dallas,Standard Grant,Leland Jameson,8/31/19,"$208,382.00 ",,yifei.lou@utdallas.edu,"800 W. Campbell Rd., AD15",Richardson,TX,750803021,9728832313,MPS,"1271, 1504","019Z, 9263",$0.00 ,"As the digital revolution increases the amount of data generated by sensing methodology such as magnetic resonance imaging and radar, the need to process the data better, faster, and cheaper has been the focus of much research, most notably through work with compressive sensing (CS). However, CS is not without its problems, most of which have emerged as CS has moved from the theoretical to the practical. The theory was developed with convex problems, but many practical applications require the ability to process nonconvex problems that are not easy to solve as quickly as digital sensing systems require. This research project focuses on a particular nonconvex model along with associated numerical algorithms, which when completed will advance the field of nonconvex optimization. Theoretical investigations will be performed to establish conditions for guaranteed performance, which will help engineers and scientists devise experiments to acquire data and recover useful information in a more effective manner. The tools developed will have broad applicability due to the profound impacts of CS, specifically in the fields of medical imaging and geospatial information that are addressed in this project. Furthermore, the investigator will incorporate results of the research into undergraduate and graduate courses and will develop new interdisciplinary courses with focus on both the theory and application, including machine learning and medical imaging, which will serve as a springboard for student recruitment. <br/><br/>Compressive sensing (CS) can exactly recover a sparse signal (most elements being zero) from incoherent linear systems, in which any two measurements have as little correlation as possible. Sparsity and incoherence are two important assumptions in CS, but many practical problems are coherent, and conventional methods do not work well. To overcome the coherency barrier, the investigator and collaborators investigate a novel nonconvex model that has advantages over the state-of-the-art methods in CS. The goal of this project is to address key challenges regarding both computational and theoretical aspects of the algorithms, to establish new criteria for exact recovery, and to demonstrate its applicability in prototypical problems. As such, this research is organized with three objectives: (1) Developing efficient algorithms to solve the nonconvex minimization problem, using techniques in convex optimization and dynamical systems to design algorithms and analyze convergence; (2) Searching for conditions that can quantify the success of both convex and nonconvex methods, for example, coherence and minimum separation; (3) Conducting numerical experiments in two types of real problems, medical image reconstruction and hyperspectral image classification, to demonstrate the advantages of the method in terms of accuracy and efficiency. Overall, this project will advance theoretical understanding and algorithmic developments in computational mathematics and provide a new perspective to enable CS-based data recovery in a wide spectrum of applications."
518,1518776,SHF:Large:Collaborative Research: Inferring Software Specifications from Open Source Repositories by Leveraging Data and Collective Community Expertise,CCF,Software & Hardware Foundation,7/1/15,6/17/15,Robert Dyer,OH,Bowling Green State University,Standard Grant,Sol Greenspan,6/30/19,"$214,843.00 ",,rdyer@bgsu.edu,302 Hayes Hall,Bowling Green,OH,434030230,4193722481,CSE,7798,"7925, 7944",$0.00 ,"Today individuals, society, and the nation critically depend on software to manage critical infrastructures for power, banking and finance, air traffic control, telecommunication, transportation, national defense, and healthcare. Specifications are critical for communicating the intended behavior of software systems to software developers and users and to make it possible for automated tools to verify whether a given piece of software indeed behaves as intended. Safety critical applications have traditionally enjoyed the benefits of such specifications, but at a great cost.  Because producing useful, non-trivial specifications from scratch is too hard, time consuming, and requires expertise that is not broadly available, such specifications are largely unavailable. The lack of specifications for core libraries and widely used frameworks makes specifying applications that use them even more difficult. The absence of precise, comprehensible, and efficiently verifiable specifications is a major hurdle to developing software systems that are reliable, secure, and easy to maintain and reuse. <br/><br/>This project brings together an interdisciplinary team of researchers with complementary expertise in formal methods, software engineering, machine learning and big data analytics to develop automated or semi-automated methods for inferring the specifications from code. The resulting methods and tools combine analytics over large open source code repositories to augment and improve upon specifications by program analysis-based specification inference through synergistic advances across both these areas. <br/><br/>The broader impacts of the project include: transformative advances in specification inference and synthesis, with the potential to dramatically reduce, the cost of developing and maintaining high assurance software; enhanced interdisciplinary expertise at the intersection of formal methods software engineering, and big data analytics; Contributions to research-based training of a cadre of scientists and engineers with expertise in high assurance software."
519,1526929,SHF: Small: Collaborative Research:Text Retrieval in Software Engineering 2.0,CCF,Software & Hardware Foundation,9/1/15,7/20/15,Sonia Haiduc,FL,Florida State University,Standard Grant,Sol Greenspan,8/31/18,"$224,998.00 ",,shaiduc@cs.fsu.edu,"874 Traditions Way, 3rd Floor",TALLAHASSEE,FL,323064166,8506445260,CSE,7798,"7923, 7944",$0.00 ,"Software systems contain large amounts of textual information captured in various software artifacts, such as, requirements documents, source code, user manuals, etc.  The productivity of software developers and the quality of the software they produce directly depends on their ability to retrieve and understand the textual information present in software.  Since humans cannot process and comprehend so much text, researchers proposed the use of text retrieval techniques to help software developers with many of their daily tasks.  In order to be useful, these techniques need to be properly configured, which requires calibrating many parameters.  As most software developers are not experts in text retrieval, they need help in determining the best text retrieval configuration in a given software engineering context.  The configuration problem is one of the main obstacles in the adoption of such techniques in the software industry, because many approaches proposed by researchers do not generalize well.  The outcomes of this project will transform the way software developers address many of their daily tasks, allowing them to easily adopt the use of text retrieval during software development.  The results of this research will also be used in software engineering courses to support students in their projects.  The new practices that the students will acquire will help them become better software engineers.  The proposed research also brings together work from different computing research communities: software engineering and information retrieval and it will bring new knowledge in both fields.  Existing approaches using text retrieval in software engineering will become more practical, rather than just promising, facilitating migration from the lab into industry and academia.<br/><br/>The outcome of this research will be: (1) a novel approach (called TRinSE2.0), which will achieve automatic, runtime query-based text retrieval configuration; and (2) improvements to important software engineering tasks, in practical settings, focusing on feature and bug location, impact analysis, traceability link recovery, and bug triage.  TRinSE2.0 will be evaluated on open source data, in the classroom, and in industrial settings.  The proposed work will transform the way text retrieval configuration is done in software engineering applications.  New, software-specific measures, as well as proven linguistic-based measures will be used to capture query properties in the context of software engineering tasks and data sets.  Machine learning algorithms will find the best configuration for a given query.  When writing a query to retrieve information from a software project, developers will get the best results, saving them time and effort, improving their productivity and the quality of their work.  The text retrieval configuration problem will no longer be heuristic-based, but it will become data-driven."
520,1464421,CRII: NeTS: Data-Driven QoE for Mobile Videos,CNS,Networking Technology and Syst,7/1/15,7/12/16,Amit Pande,CA,University of California-Davis,Continuing Grant,Darleen Fisher,6/30/19,"$175,000.00 ",,pande@ucdavis.edu,OR/Sponsored Programs,Davis,CA,956186134,5307547700,CSE,7363,"7363, 8228",$0.00 ,"Mobile data traffic was around 18 Exabytes in 2013 and is expected to double every year. Of this, 67% is video traffic currently and this percentage is estimated to increase. This necessitates development of strategies to map subjective human scores into on-line deployable methods for Quality of user Experience (QoE) assessment, which can in turn control the underlying service architecture used by network and application providers.<br/><br/>The goal of this CISE Research Initiation Initiative project is to develop data-driven mobile video QoE models which can be used for real-time estimation of mobile-video quality in smart devices. The researchers envision collecting large volume of actual video-watch data using a customized application and processing the data to mine important patterns and trends. This work will build a data-driven video QoE model from the user's viewing experience to assess and improve the performance of mobile video applications in contrast to existing work, which has used distortion-specific metrics or full-reference approaches or have data-driven models to model user engagement. The research entails building mobile applications to measure video quality in mobile devices and collect subjective user-experience scores. The anticipated outcomes of this two-year project include answers to key research questions in video delivery for mobile devices: Can we define new objective models for QoE of mobile-multimedia to incorporate the loss in quality caused by freezing, distortions and other factors? Is it possible to develop simpler, more reliable and cost-effective methods for objective evaluation of video quality in battery constraint mobile devices? What is the impact of device aesthetics, network resources and user preferences, and can this impact be accurately modeled? What kinds of guarantees can be made in different application scenarios and what cannot? How can the new metrics be used to design better video delivery system with a focus to enhance user's quality of experience? <br/><br/>The researchers will first generate a mobile application to a generate pool of video data obtaining user's subjective video assessment as well as content/network/device/visual quality metadata while allowing a user to play videos from a project-generated dataset or content from popular websites such as Youtube or Netflix. This will be accompanied by development of metrics for quality assessment to capture delivery losses and their proportional impact on video quality. A machine learning model will be developed to model distortions as well as factors such as screen resolution, video resolution, freeze frequency and intensity which impact video quality perception and their proportional impact on video QoE. <br/><br/>The development of proposed quality-assessment tools can be used by content and network providers to identify the bottlenecks in ensuring video quality and rectify them with QoE-aware adaptive streaming, caching, transcoding and optimizations. The PI is also committed to including a strong educational plan that has both a conventional component using the project results to drive coursework and involvement of graduate and undergraduates students in research and in building a QoE application for smart phones to involve undergraduate and graduate students."
521,1446304,CPS: Synergy: Collaborative Research: Cyber-Physical Approaches to Advanced Manufacturing Security,CNS,"Special Projects - CNS, CPS-Cyber-Physical Systems, Cybersecurity Innovation, , , ",6/15/15,5/8/18,Christopher White,TN,Vanderbilt University,Cooperative Agreement,David Corman,5/31/19,"$262,862.00 ",,jules.white@vanderbilt.edu,Sponsored Programs Administratio,Nashville,TN,372350002,6153222631,CSE,"1714, 7918, 8027, O119, P351, Q235","7434, 7918, 8235, 8237, 9251",$0.00 ,"The evolution of manufacturing systems from loose collections of cyber and physical components into true cyber-physical systems has expanded the opportunities for cyber-attacks against manufacturing. To ensure the continued production of high-quality parts in this new environment requires the development of novel security tools that transcend both the cyber and physical worlds. Potential cyber-attacks can cause undetectable changes in a manufacturing system that can adversely affect the product's design intent, performance, quality, or perceived quality. The result of this could be financially devastating by delaying a product's launch, ruining equipment, increasing warranty costs, or losing customer trust. More importantly, these attacks pose a risk to human safety, as operators and consumers could be using faulty equipment/products. New methods for detecting and diagnosing cyber-physical attacks will be studied and evaluated through our established industrial partners. The expected results of this project will contribute significantly in further securing our nation's manufacturing infrastructure.<br/><br/>This project establishes a new vision for manufacturing cyber-security based upon modeling and understanding the correlation between cyber events that occur in a product/process development-cycle and the physical data generated during manufacturing. Specifically, the proposed research will take advantage of this correlation to characterize the relationships between cyber-attacks, process data, product quality observations, and side-channel impacts for the purpose of attack detection and diagnosis. These process characterizations will be coupled with new manufacturing specific cyber-attack taxonomies to provide a comprehensive understanding of attack surfaces for advanced manufacturing systems and their cyber-physical manifestations in manufacturing processes. This is a fundamental missing element in the manufacturing cyber-security body of knowledge. Finally, new forensic techniques, based on constraint optimization and machine learning, will be researched to differentiate process changes indicative of cyber-attacks from common variations in manufacturing due to inherent system variability."
522,1460620,REU Site: Computational Methods for Discovery Driven by Big Data,IIS,RSCH EXPER FOR UNDERGRAD SITES,5/1/15,3/17/15,Daniel Boley,MN,University of Minnesota-Twin Cities,Standard Grant,Wendy Nilsen,4/30/19,"$359,973.00 ",Amy Larson,boley@cs.umn.edu,200 OAK ST SE,Minneapolis,MN,554552070,6126245599,CSE,1139,9250,$0.00 ,"As participants in the University of Minnesota (UMN) REU Site program, students will engage in research that develops computational methods for scientific discovery across disciplines that are driven by big data.  In this 10-week summer program, in addition to immersion in research, students will receive technical training and professional development that encourages and prepares them for a sustained career in the sciences.  This includes Big Data Colloquia, Communicating Science workshops, career mentoring, and public dissemination of research findings. Towards an objective of increased participation and broader impacts, this program will bring together nationally recruited students and those from UMN and local institutions to establish a cohort with diverse academic and cultural backgrounds. A Big 10 University situated in a large urban environment, UMN has a strong research community that encourages transdisciplinary research within and outside the boundaries of the institution.<br/><br/>Closely mentored by a member of the Computer Science and Engineering (CS&E) faculty, each student will contribute to active research that addresses open questions in computational complexity, machine learning, parallel and distributed computing, mobile and cloud computing, or graphics and visualization. A UMN REU participant might use observation data to simulate crowd behavior, analyze genomic sequence data to better understand microbial communities, develop tools to analyze chemical-genetic interaction networks, improve spatial perception in a virtual environment, develop visualization techniques to better understand massive data sets, enhance parallel distributed processing through algorithm development or by harnessing the computational power of a network of mobile devices, or use graph-based approaches to better understand climate change. The diverse research of CS&E faculty represents collaboration across the University with faculty in genetics, chemistry, climate science, neuroscience, architecture, medicine, and biomedical engineering to propel all of these disciplines and computer science towards previously unattainable insights and discoveries.  The web site http://www-users.cs.umn.edu/~boley/big-data-reu/index.html has more information on this project."
523,1460870,REU Site: Team Research in Computational and Applied Mathematics (TRiCAM),DMS,WORKFORCE IN THE MATHEMAT SCI,6/1/15,2/18/15,Michael Brenner,MA,Harvard University,Standard Grant,James Matthew Douglass,5/31/19,"$499,775.00 ",Hanspeter Pfister,brenner@seas.harvard.edu,1033 MASSACHUSETTS AVE,Cambridge,MA,21385369,6174955501,MPS,7335,9250,$0.00 ,"The Team Research in Computational and Applied Mathematics (TRiCAM) REU program aims to give students an experience in real-world collaborative problem solving, challenging them to apply mathematics and computation to tackle team projects posed by Harvard faculty and industrial partners.  Projects will involve the application of computational and mathematical tools such as machine learning, data analysis, and numerical simulation to solve problems in fields such as geoscience, medicine, materials science, and the social sciences.  Topics will be chosen to appeal to a wide population of students who are at early stages of their academic development, and who have limited awareness of the vast range of potential career paths in applied mathematics and computational science.  The program's team-based approach to research will teach students an appreciation for scientific collaboration, prepare them for future employment, and provide opportunities to practice conflict resolution skills.  Ultimately the program will help develop a new generation of collaborative scientists and engineers who are excited about applying computation and mathematics to solve interdisciplinary, real-world problems.<br/> <br/>This REU project will support four teams of undergraduate students for ten weeks each summer as they gain both the mathematical, computational, and statistical skills necessary to tackle a research problem, and real-world experience of working in a team-based collaborative environment.  The summer will be divided into four phases: a two-week orientation where teams formulate a statement of work for summer in response to the problem posed by the faculty or industrial sponsor, two three-week work periods, the first resulting in a midterm report submitted to the sponsor, and the second focused on responding to feedback from the midterm report, and a one-week completion phase where teams prepare final reports and presentations to both the sponsors and the larger Harvard REU community. Students will be selected for teams based on their individual academic strengths and for their potential fit as a member of a team and for a particular project.  The program will focus on students for whom this will be an early, even perhaps a first, experience with research, by targeted recruiting at historically black colleges and non-research institutions."
524,1543863,"Proposal for Supporting US-Based Students to Attend the 2015 IEEE International Conference on Data Mining; Atlantic City, NJ; November 14-17, 2015.",IIS,Info Integration & Informatics,7/15/15,7/9/15,Yong Ge,NC,University of North Carolina at Charlotte,Standard Grant,Sylvia Spengler,6/30/16,"$24,000.00 ",,yongge@email.arizona.edu,9201 University City Boulevard,CHARLOTTE,NC,282230001,7046871888,CSE,7364,"7364, 7556",$0.00 ,"This grant provides travel support for U.S. based graduate student participants to attend the 2015 International Conference on Data Mining (ICDM 2015), which will be held in Atlantic City, NJ, USA on November 14-17, 2015. ICDM has established itself as the world's premier research conference in data mining. It provides an international forum for presentation of original research results, as well as exchange and dissemination of innovative, practical development experiences. The conference covers all aspects of data mining, including algorithms, software and systems, and applications, as well as related areas such as data management, machine learning and bioinformatics. The conference proceedings are published by IEEE. The conference seeks to continuously advance the state-of-the-art in data mining. With the growth of the Web, the Internet, and data intensive technologies such as Sensor Networks, and Bioinformatics, Data Mining is an extremely important area in Information Technology. Besides the technical program, the conference features workshops, tutorials, panels, data mining contest, and since this year, the Ph.D. forum.  A strong representation of U.S. researchers at the Conference is useful in maintaining U.S. competitiveness in this important area. The total number of ICDM participants in the past has been in excess of 500, with a majority of the participants from the U.S., then Europe and Asia. It is expected to provide scholarships to 16 U.S. based graduate student participants. This grant will partially support the travel costs for the U.S. based graduate student participants.  This project will promote students? exposure to research and enhance the integration of research and education. This project will greatly broaden the dissemination of research results to more graduate students. Particularly, the committee will consider candidates who are under-representative in the past meetings, and thus enhance the diversity in the field. The award results will be announced at the ICDM 2015 conference website (http://icdm2015.stonybrook.edu)."
525,1464288,CRII: CSR: Automatic Cross-Layer Memory Management to Achieve Power and Performance Goals,CNS,CSR-Computer Systems Research,8/1/15,3/3/15,Michael Jantz,TN,University of Tennessee Knoxville,Standard Grant,Marilyn McClure,7/31/18,"$150,296.00 ",,mrjantz@utk.edu,1331 CIR PARK DR,Knoxville,TN,379163801,8659743466,CSE,7354,"8228, 9150",$0.00 ,"This project focuses on developing a framework that will allow users to control and manage memory resources more effectively than conventional approaches, and has the potential to significantly improve both computing performance and energy efficiency in a wide range of computing domains. This project addresses the following technology gaps as it translates from research discovery toward commercial application: (1) It will develop the system tools and software necessary to effectively communicate and incorporate memory usage information across layers during memory management, (2) It will generate new knowledge in the areas of static analysis, profiling, and machine learning for the purposes of predicting and classifying memory usage behavior, (3) It will allow researchers to create and explore new collaborative memory management algorithms and techniques, and (4) It will investigate and evaluate the potential of various collaborative memory management schemes for achieving different power and performance goals using a variety of realistic workloads and usage scenarios. In addition, one graduate student will receive innovation and technology translation experiences through the development of the cross-layer memory management framework, and its release into the open source community.<br/><br/>Specifically, the project will exploring and assess the utility of cross-layer collaboration during memory management. It is difficult to control and optimize memory power and bandwidth because these effects depend upon activity across multiple layers of the vertical execution stack. However, current systems make no attempt to coordinate memory usage in the upper-level software with placement and management in the operating system and hardware. This project will develop a full system implementation of cross-layer memory management, where (a) the application-layer determines data object usage patterns, assigns objects to appropriate locations/pages in the virtual address space, and conveys corresponding page-level memory usage information to the OS, and (b) the OS-layer organizes its physical memory structures according to architectural information from the memory hardware, and incorporates the application guidance when deciding which physical page to use to back a particular virtual page."
526,1464104,CRII: CSR: Towards Understanding and Mitigating the Impact of Web Robot Traffic on Web Systems,CNS,"Special Projects - CNS, CSR-Computer Systems Research",5/1/15,11/5/15,Derek Doran,OH,Wright State University,Standard Grant,Marilyn McClure,4/30/19,"$174,319.00 ",,derek.doran@wright.edu,3640 Colonel Glenn Highway,Dayton,OH,454350001,9377752425,CSE,"1714, 7354","8228, 9178, 9251",$0.00 ,"This CSR-CRII project responds to the sudden rise of Web robot (a.k.a. Web crawler) traffic on Web systems around the world - from approximately 20% of all requests a decade ago to over 60% today. Because present Web systems' optimizations assume that the traffic serviced exhibit human-like patterns that robots do not, present robot activity on the Web may silently degrade performance, energy efficiency, and scalability of Web systems. As the Web continues to evolve towards a social platform where individuals upload extemporaneous thoughts and observations that only carry instantaneous value to organizations, and where the Internet of Things concept is expected to introduce millions of devices that collect data from the Web and submit requests to online services automatically, robot traffic will only rapidly increase in volume and intensity. For this reason, it is essential that we understand the impact of Web robot traffic on modern Web systems and devise technologies capable of mitigating their impact on system performance, energy efficiency, and scalability.<br/><br/>This effort will synthesize our present understanding of robot traffic with machine learning tools, statistical analysis, and data science methods not previously considered in the context of Web traffic analysis and user behavioral modeling. It will improve our ability to understand the impact of robot traffic on Web systems by: (i) devising automatic methods to classify robots by their functionality and by the demands they impose; and (ii) develop novel robot traffic generators, tailored to a specific profile of robot types that can test how a system reacts to robot traffic of varying intensity and functional type mixtures. The project will also explore a prototype robot-resilient caching system that could lead to immediate performance payoffs for existing Web systems. The project will result in preliminary analytical models, empirical results, and prototype analysis software leading to longer-term research endeavors. Recent data from Web systems that provide services across many Web domains are immediately available for the project.<br/><br/>The results of the project potentially may transform the way Web systems from single servers to large clouds are designed and optimized mitigating performance, energy efficiency, and the financial cost of servicing robots. Students to work on this project will be strategically recruited to broaden participation. Educational activities will provide students useful yet infrequently taught traffic analysis and Web systems security fostering stronger ties between knowledge engineering and cybersecurity student and research communities."
527,1513412,"Estimating Low Dimensional, High-Density Structure",DMS,STATISTICS,8/1/15,6/9/17,Isabella Verdinelli,PA,Carnegie-Mellon University,Continuing grant,Gabor Szekely,7/31/18,"$400,000.00 ","Larry Wasserman, Christopher Genovese",isabella@stat.cmu.edu,5000 Forbes Avenue,PITTSBURGH,PA,152133815,4122688746,MPS,1269,,$0.00 ,"Data in high dimensional spaces are now very common. This project will develop methods for analyzing these high dimensional data. Such data may contain hidden structures. For example, clusters (which are small regions with a large number of points) can be stretched out like a string forming a structure called a filament. Scientists in a variety of fields need to locate these objects. It is challenging since the data are often very noisy. This project will develop rigorously justified and computationally efficient methods for extracting such structures. The methods will be applied to a diverse set of problems in astrophysics, seismology, biology, and neuroscience. The project will advance knowledge in several fields including computational geometry, astronomy, machine learning, and statistics.<br/><br/>Finding hidden structure is useful for scientific discovery and dimension reduction. Much of the current theory on nonlinear dimension reduction assumes that the hidden structure is a smooth manifold and is very restrictive. The data might be concentrated near a low dimensional but very complicated set, such as a union of intersecting manifolds. Existing algorithms, such as the Subspace Constrained Mean Shift exhibit erratic behavior near intersections. This project will develop improved algorithms for these cases. At the same time, contemporary theory breaks down in these cases and this project will develop new theory to address the aforementioned problem. A complete method (which will be called singular clusters) will be developed for decomposing point clouds of varying dimensions into subsets."
528,1461886,BDD: Disaster Preparation and Response via Big Data Analysis and Robust Networking,CNS,Special Projects - CCF,4/1/15,3/30/15,Guoliang Xue,AZ,Arizona State University,Standard Grant,Ann Von Lehmen,3/31/19,"$300,000.00 ",Huan Liu,xue@asu.edu,ORSPA,TEMPE,AZ,852816011,4809655479,CSE,2878,"7363, 8230",$0.00 ,"Disasters are events with dire consequences, requiring multiple-agency responses and resources beyond the capability of a single community. Natural disasters, such as the 2011 Great East Japan Earthquake, can threaten the lives of many people and cause inordinate economic losses. Communication is critical to disaster preparation, response, and recovery, but may be damaged during the disaster. In this project, researchers from the US and Japan study novel approaches to disaster preparation, response and recovery using survivable communication networks and big data analysis of social media data. This collaborative effort involves expertise in disaster research, social media mining and big data analysis, network science, wireless communications, and machine learning, to examine resilient network architecture and algorithms, data collection and analysis before the disaster, and decision making and information dissemination during the disaster. The resilient network incorporates both wired and wireless communications to deal with multiple disaster-induced failures, aiming for efficient algorithms serving emergency applications. State-of-the-art data collection and analysis techniques will help build an important knowledge base in proactive preparation for disasters. Real time decision making and information dissemination during a disaster can assist disaster response and recovery effectively. <br/><br/>The proposed research aims to provide valuable guidance for disaster preparation, response, and recovery for both the US and Japan, and spearhead a new research direction in survivable communication network design and big data analysis. This project provides a conducive environment to further research collaboration of big data analysis and disaster relief between the US and Japan. Graduate students will be jointly trained in this international research project to actively collaborate in carrying out the proposed research tasks. Special efforts will be made to engage minority students and underrepresented groups."
529,1557589,QuBBD: Collaborative Research: Interactive Ensemble clustering for mixed data with application to mood disorders,DMS,,9/15/15,9/11/15,Rachael Hageman Blair,NY,SUNY at Buffalo,Standard Grant,Nandini Kannan,8/31/17,"$19,500.00 ",,hageman@buffalo.edu,520 Lee Entrance,Buffalo,NY,142282567,7166452634,MPS,O402,,$0.00 ,"The Big Data era has given rise to data of unprecedented size and complexity.  However, fully leveraging Big Data resources for knowledge and discovery is an open challenge due to the fact that conventional methods of data processing and analysis often fail or are inappropriate.  This project develops an innovative approach that utilizes Big Data to improve the classification of mood disorders for the purpose of improving diagnosis and outcomes for psychiatric patients.  Big Data issues are inherently more severe for mental disorders because of their elusive nature.  The psychiatric community has recognized the critical need for a more precise, evidence-based approach for the diagnosis and treatment of disease.  In fact, recent studies funded by the National Institute of Mental Health (NIMH) have found that psychiatric interventions were effective in less than 25% of patients presenting with an acute episode.  This low efficacy rate is especially problematic given the prevalence of mental disorders.  Mood disorders alone (e.g., depression) will be experienced by 1 in 5 adults in the United States at some point in their lives.  This project is motivated by the hypothesis that a more precise and personalized classification of mental health disease can be obtained through the development of novel clustering methods that identify clinically significant structures with these large population data sets.  However, such an approach must overcome a large number of methodological challenges introduced by the complexity of the problem and the nature of large-scale real-world electronic health data.  These challenges include, among others, complex and unknown structure, high dimensionality, heterogeneity, complex mixtures of variables, missing data, and sparsity.  <br/><br/>This award supports initiation of a collaborative research project, carried out by a team with interdisciplinary and complimentary skill sets, to develop methods for big data that address challenges inherent in the integration of biomedical data of this type.  Collective expertise of the team spans the areas of biomedical informatics, biostatistics, computer and information science, electrical and computer engineering, mathematics, and psychiatry.  A novel methodology is developed in a flexible and fully integrated framework that can be extended to other biomedical data and diseases.  Within this framework, clustering methods that capture different aspects of relatedness in the data are integrated in a rigorous way that not only accounts for model uncertainty, but also results in an interactive visualization that is accessible with strong model interpretability for the non-expert.  Specifically, the methodology will rely on novel modifications to bootstrap estimators of generalization error for the purpose of assembling a consensus over an ensemble of clusters inferred from topology-based and machine learning approaches.  The framework also supports iterative refinement of the consensus solution based on user input (via the visualization) to incorporate domain expertise.  The rigorous identification of sub-groups of individuals within heterogeneous populations will facilitate accurate and targeted diagnosis for mood disorders, and provide opportunity for personalized evidence-based interventions.   Applications focus on clustering individuals with mood disorders (bipolar disorder and major depression) from data collected in the Bipolar Disorder Research Network (BDRN).  Despite this focus, the methodology is generalizable to other diseases that face similar challenges for diagnosis and treatment.  In fact, this project supports the first steps of a long-term vision of generalizing the methods to more complex and less curated data, such as electronic health records, social media, and other sources.  This award is supported by the National Institutes of Health Big Data to Knowledge (BD2K) Initiative in partnership with the National Science Foundation Division of Mathematical Sciences."
530,1545587,AF: Small: New Perspectives on Special Methods for Graph Algorithms,CCF,ALGORITHMS,2/1/15,6/16/15,Lorenzo Orecchia,MA,Trustees of Boston University,Standard Grant,Tracy Kimbrel,8/31/16,"$90,696.00 ",,orecchia@uchicago.edu,881 COMMONWEALTH AVE,BOSTON,MA,22151300,6173534365,CSE,7926,"7923, 7926",$0.00 ,"Classical algorithms for many important graph primitives were designed at a time when the conventional notion of efficiency was polynomial running time.  However, many of today's applications involve graphs consisting of millions, or even billions, of nodes. On these massive inputs, practical algorithms must run in time that is as close to linear as possible in the size of the input.<br/><br/>The spectral approach to designing graph algorithms views the instance graph as a matrix and makes use of the algebraic properties of the corresponding linear operator. Recently, research in this area has led to the design of faster spectral algorithms for many essential graph problems, such as electrical flow, maximum flow and graph partitioning in undirected graphs.  The goal of this project is to develop a novel algorithmic approach by combining spectral methods and the idea of regularization from optimization.  Regularization is a mechanism for modifying a given optimization problem to make it more amenable to known algorithms without changing its salient characteristics. Surprisingly, many of the recent breakthroughs in the design of fast spectral algorithms can be viewed as applying different types of regularization. This research aims to exploit this interpretation to design algorithms that are faster, simpler to analyze and easier to implement. Another aim of this research is to integrate different perspectives on regularization from Machine Learning, Statistics and Convex Optimization, to create new bridges between these fields and the design of algorithms.<br/><br/>Due to the practical importance of the graph problems under consideration, the work will also focus on empirically evaluating the algorithms designed.  These evaluations will be disseminated to the relevant audiences to maximize the impact of the award. Moreover, because this project aims to develop new fundamental techniques in the design of algorithms, a particular effort will be devoted to incorporating material from this research into the PI's teaching activity and to preparing educational material for the public."
531,1507620,Collaborative Research: Smoothing Spline Semiparametric Density Models,DMS,STATISTICS,8/1/15,7/30/15,Yuedong Wang,CA,University of California-Santa Barbara,Standard Grant,Gabor Szekely,1/31/19,"$228,108.00 ",,yuedong@pstat.ucsb.edu,Office of Research,Santa Barbara,CA,931062050,8058934188,MPS,1269,,$0.00 ,"A probability density function of multiple variables describes the likelihood of different values the variables can jointly take, therefore, contains full information regarding the distribution of individual variables and their interactions. Given observed data of the random variables, density estimation is at the heart of Statistics and machine learning, where the classical problems such as regression, variable selection, clustering, and dimension reduction, can all be cast into a density estimation problem.  Advanced density estimation methods are therefore essential for the extraction of as much information as possible from the data. There has been lack of systematic research in flexible density estimation with high dimensional data or complex data such as clustered data. The overall goal of this project is to develop a smoothing spline based systematic framework that allows for flexible density model building for complex and high dimensional data. As such data arise from a wide range of applications, the results of this proposed research are useful for researchers from a wide range of fields. In particular, the proposed methods will be applied to analyze data in health and medicine, speech, environmental change, food and computer sciences, in collaboration with researchers in these areas. High-performance computing tools will be developed as a result of this research and made publicly available.<br/><br/>This project adopts a semi-parametric approach that combines advantages of parametric and nonparametric methods. Flexible and general semi-parametric density and conditional density models for independent and clustered data will be developed and studied. Regularization methods for adaptive density estimation, variable selection in high dimensional conditional density estimation and interaction selection in semi-parametric graphical models will be developed. Nonparametric components will be modeled using reproducing kernel Hilbert spaces which can deal with different density models on different domains with different penalties in a unified fashion. The semiparametric density models considered in this project contain most existing semiparametric density models as special cases as well as many new interesting models. Many methods in this project for adaptive estimation, model/variable selection, model diagnostics and inference are new. These novel methodologies constitute advances in density estimation."
532,1540283,Algorithmic Game Theory and Practice,CCF,SPECIAL PROJECTS - CCF,7/1/15,4/15/15,Richard Karp,CA,University of California-Berkeley,Standard Grant,Tracy Kimbrel,6/30/16,"$25,000.00 ",,karp@cs.berkeley.edu,Sponsored Projects Office,BERKELEY,CA,947101749,5106433891,CSE,2878,"7556, 7932",$0.00 ,"Many computer scientists have realized that, in order to productively model and study the Internet and its novel computational phenomena, they need models and insights from disciplines such as game theory and economic theory, while many economists have found that a computational point of view is essential in order to understand a world in which markets are networked and the default platforms of economic transactions are algorithmic. Algorithmic Game Theory (AGT) stands at the intersection of these two fields. It has had significant practical impact, in a broad range of applications including online, matching and assignment markets, internet advertising, information diffusion, airport security, etc. <br/><br/>This workshop will showcase the impact of  AGT on practice, and explore avenues for increasing the field's practical impact, including connections to machine learning, data science, and financial markets."
533,1452923,CAREER: Overcoming limitations to approximating combinatorial optimization problems,CCF,ALGORITHMIC FOUNDATIONS,5/1/15,5/7/18,Alexandra Kolla,IL,University of Illinois at Urbana-Champaign,Continuing grant,Rahul Shah,11/30/18,"$394,199.00 ",,alexkolla@gmail.com,1901 South First Street,Champaign,IL,618207406,2173332187,CSE,7796,"1045, 7926, 7927",$0.00 ,"This proposal seeks to develop a comprehensive understanding of the limitations of approximation algorithms for combinatorial optimization problems. Combinatorial optimization problems are of great importance to various areas such as operations research, machine learning, VLSI design, computational biology and statistical physics. The task of finding algorithms for combinatorial optimization problems arise in countless applications, from billion-dollar operations to everyday computing tasks. Many optimization problems are NP-hard and thus cannot be solved exactly in polynomial time unless P=NP. However, the majority of such problems are key elements to practical applications where often, if the optimal solution is hard to  find, producing an approximate solution su;ffices.<br/><br/>The research proposed will study the limitations of approximation algorithms posed by the Unique Games Conjecture and the limitations posed by the presence of unlikely worst-case instances. The PI aims to design a methodology to partially or fully overcome such limitations by addressing both of those factors. The proposal outlines a challenging plan focusing on research in a broad cross-section of spectral graph theory, convex optimization, multi-commodity flows, and harmonic analysis.   The contributions of the work described in this proposal will have great impact on the theory of approximability as well as real world problems for which efficient, exact algorithms will be provided for semi-random instances and will naturally result in collaborations between researchers across many different  fields such as mathematics, theory of computer science, industrial engineering, operations research and networking."
534,1537414,Gradient Sliding Schemes for Large-scale Optimization and Data Analysis,CMMI,OE Operations Engineering,10/1/15,7/26/15,Guanghui Lan,FL,University of Florida,Standard Grant,Donald Hearn,5/31/16,"$266,731.00 ",,george.lan@isye.gatech.edu,1 UNIVERSITY OF FLORIDA,GAINESVILLE,FL,326112002,3523923516,ENG,006Y,"072E, 073E, 077E",$0.00 ,"The rapid advances in technology for digital data collection have led to significant increases in the size and complexity of data sets, sometimes known as big data. Optimization models, when combined with novel statistical analysis, have been proven fruitful in analyzing these complex datasets. However, optimization problems arising from these applications often involve nonsmooth components that can significantly slow down the convergence of existing optimization algorithms. Moreover, the complex datasets are so big and often distributed over different storage locations that the usual assumption that an entire dataset can be completely traversed in each iteration of the algorithm is unrealistic. Gradient sliding schemes do not require this assumption and hence are ideally suited for optimization with big data.  The research aims at tackling these computational challenges through the design, analysis, and implementation of a novel class of optimization algorithms using gradient sliding schemes.  The effectiveness of these new optimization algorithms will be demonstrated by solving problems in image processing and machine learning.<br/><br/>The gradient sliding algorithms are first-order methods that use first-order information (gradients and function values) exclusively in addition to some auxiliary operations, such as projection over the feasible set. As opposed to existing first-order methods, gradient sliding methods can skip the computation of gradients from time to time, while still preserving the optimal convergence properties for solving different types of large-scale optimization problems. This research will also study a new class of conditional gradient sliding methods that require a linear optimization rather than a more involved projection over the feasible set in each iteration. These algorithms are expected to exhibit optimal rate of convergence in terms of both the number of gradient computations and the number of times for solving the linear optimization subproblem. Moreover, randomized variants of these gradient sliding algorithms which are amenable to parallel/distributed computing will also be studied.  When applied to data analysis, these algorithms can reduce, by orders of magnitude, the number of traverses through the datasets, the computational cost associated with the involved matrix-vector multiplications, as well as the communication costs for the distributed datasets."
535,1454377,CAREER: An Integrated Inferential Framework for Big Data Research and Education,DMS,"STATISTICS, Division Co-Funding: CAREER",7/1/15,7/13/17,Han Liu,NJ,Princeton University,Continuing grant,Nandini Kannan,9/30/18,"$192,630.00 ",,hanliu@northwestern.edu,Off. of Research & Proj. Admin.,Princeton,NJ,85442020,6092583090,MPS,"1269, 8048",1045,$0.00 ,"This project addresses several fundamental challenges in modern data analysis and aims to create a new research area named Big Data Inference. Currently available literature regarding Big Data research mainly focuses on developing new estimators for complex data. However, most of these estimators are still in lack of systematic inferential methods for uncertainty assessment. This project hopes to bridge this gap by developing new inferential theory for modern estimators unique to Big Data analysis.  The deliverables of this project include easy-to-use software packages, which directly help scientists to explore and analyze complex datasets. The principal investigator is also actively collaborating with many scientists to ensure the more direct impact of this project to the targeted scientific communities.<br/><br/>This project aims to develop novel inferential methods for assessing uncertainty (e.g., constructing confidence intervals or testing hypotheses) of modern statistical procedures unique to Big Data analysis. In particular, it develops innovative statistical inferential tools for a variety of machine learning methods which have not yet been equipped with inferential power. It also provides necessary inferential tools for the next generation of scientists to be competitive in modern data analysis."
536,1636452,Broadening Participation in Visualization (BPViz) Workshop,OAC,"INFORMATION TECHNOLOGY RESEARC, CYBERINFRASTRUCTURE",9/22/15,2/24/16,Vetria Byrd,IN,Purdue University,Standard Grant,Sushil Prasad,1/31/17,"$17,378.00 ",,vlbyrd@purdue.edu,Young Hall,West Lafayette,IN,479072114,7654941055,CSE,"1640, 7231","7361, 7556, 9150",$0.00 ,"The objective of this project is to broaden participation of women and underrepresented groups in visualization. The PI proposes to establish an annual two-day workshop and other activities designed to increase the participation of women and underrepresented groups in visualization. Visualization plays a significant role in the exploration and understanding of data across all disciplines with a universal goal: gaining insight into the complex relationships that exist within the data. The need to diversify a field with such far reaching influences by providing career mentoring advice, overviews of past accomplishments, and future research directions in visualization is important. The proposed workshops advances the NSF mission by informing, inspiring and encouraging diverse participants to engage in the multidisciplinary dynamics of visualization and scientific research.<br/><br/>The first supported workshop was held in 2014 at Clemson University in South Carolina. The second workshop will be held in 2016. About 20 participants at the undergraduate, graduate, post-graduate, and early career faculty level will be invited to participate in the workshop. The workshop will be organized as a series of presentations, panels with networking opportunities, and mentoring activities. To encourage collaborations, participants will present their work and explore the work of others during a poster session. Participants will be recruited via a number of different venues including, but not limited to: The Anita Borge Institute/Grace Hopper listserv, association listservs (CRA, CRA-W, CDC, AAPHDCS, Hispanics in Computing, and Women in Machine Learning), Broadening Participation Communities (A4RC, EL Alliance) and discipline-specific mailing lists (IEEE-Vis). Undergraduates will be recruited by on-campus organizations (Women in Science and Engineering and The Charles H. Houston Center). The inclusion of undergraduates will ensure the level of interest in visualization spans across academic and professional domains. Outcomes of the workshops will be assessed by surveying participants at the start and at the end of the workshop."
537,1522072,SCH: INT: Collaborative Research: S.E.P.S.I.S.: Sepsis Early Prediction Support Implementation System,IIS,Smart and Connected Health,10/1/15,12/14/17,Muge Capan,DE,Christiana Care Health Services Incorporated,Standard Grant,Sylvia J. Spengler,6/30/18,"$367,418.00 ","Ryan Arnold, Kristen Miller, Eric Jackson",mc3922@drexel.edu,200 Hygeia Drive,Newark,DE,197132049,3026236677,CSE,8018,"8018, 8062, 9102, 9150, 9251",$0.00 ,"Sepsis, infection plus systemic manifestations of infection, is the leading cause of in-hospital mortality. About 700,000 people die annually in US hospitals and 16% of them were diagnosed with sepsis (including a high prevalence of severe sepsis with major  complication). In addition to being deadly, sepsis is the most expensive condition associated with in-hospital stay, resulting in a 75% longer stay than any other condition.  The total burden of sepsis to the US healthcare system is estimated to be $20.3 billion, most of which is paid by Medicare and Medicaid. In fact, in June 2015 the Centers for Medicare & Medicaid Services (CMS) reported that sepsis accounted for over $7 billion in Medicare payments (second only to major joint replacement), a close to 10% increase from the previous year.  This pervasive drain on health care resources is due, in part, to difficulties in diagnosis and delayed treatment. For example, every one hour delay in treatment of severe sepsis/shock with antibiotics decreases a patient's survival probability by 10%. Many of these deaths could have been averted or postponed if a better system of care was in place. The goal of this research is to overcome these barriers by integrating electronic health records (EHR) and clinical expertise to provide an evidence-based framework to diagnose and accurately risk-stratify patients within the sepsis spectrum, and develop and validate intervention policies that inform sepsis treatment decisions.  The  project to bring together health care providers, researchers, educators, and students to add value to patient care by integrating machine learning, decision analytical models, human factors analysis, as well as system and process modeling to advance scientific knowledge, predict sepsis, and prevent sepsis-related health deterioration. In addition to the societal impact that clinical translation of these findings may bring, the project will provide engineering and computer science students and health services researchers with cross-disciplinary educational experience.<br/><br/>The proposed research will apply engineering and computer science methodologies to analyze patient level EHR across two large scale health care facilities, Mayo Clinic Rochester and Christiana Care Health System and to inform clinical decision making for sepsis. The multi-institutional, interdisciplinary collaboration will enable the development of health care solutions for sepsis by describing and accurately risk-stratifying hospitalized patients, and developing decision analytical models to personalize and inform diagnostic and treatment decisions considering patient outcomes and response implications. The Sepsis Early Prediction Support Implementation System (S.E.P.S.I.S.) project aims will be to: 1) Develop data-driven models to classify patients according to their clinical progression to diagnose sepsis and predict risk of deterioration, thus informing therapeutic actions. 2) Develop personalized intervention policies for patients within the sepsis spectrum.  3) Develop decision support systems (DSS) for personalized interventions focusing on resource implications and usability within a real hospital setting. The team will 1) identify important factors that uncover patient profiles based on Bayesian exponential family principal components analysis; 2) develop hidden Markov models (HMMs) and input-output HMMs to identify clusters of patients with similar progression patterns within the sepsis spectrum; 3) provide an analytical framework to support sepsis staging in clinical practice using bilevel optimization. They will 1) predict short- and long-term individual patient outcomes using multivariate statistical models and simulation; 2) develop semi-Markov decision process and partially observable semi-Markov decision process models to identify timing of therapeutic actions and diagnostic tests. Furthermore, the team will 1) predict demand for resources and develop and validate a hybrid mixed integer programming and queueing model to optimize system level allocations; 2) utilize human factors analysis and usability testing to assess the implementation of the DSS."
538,1525586,Population Analytics through a WiFi-based Edge Computing Platform,CNS,SPECIAL PROJECTS - CISE,6/15/15,6/11/15,Suman Banerjee,WI,University of Wisconsin-Madison,Standard Grant,Ralph Wachter,5/31/18,"$200,000.00 ",,suman@cs.wisc.edu,21 North Park Street,MADISON,WI,537151218,6082623822,CSE,1714,"7916, 7918",$0.00 ,"The focus of this project is on creating new techniques for understanding population analytics over a space of interest, e.g., a shopping mall, a busy street, or an entire city. Knowledge of population behavior important for many applications. For instance, knowledge of which are the busy corners of city sidewalk can provide city planners with input on where to invest city resources. Knowledge of where people congregate in a shopping mall allows officials to plan where to provide useful services, e.g., information kiosks, floor plans, and more. The process of gathering population analytics today is tedious -- some stores and shops use manual people counters to track how many persons are entering wireless technologies.<br/><br/>The technical contributions of this project are two-fold. First, it is attempting to reduce the complexity of determining location of people by reducing the number of infrastructure points needed. Second, automated approaches to population analytics are fraught with privacy concerns, and this project is examining techniques that mitigate such concerns.<br/><br/>Personnel involved in this project will be trained in significant technical skills across a broad set of domains including wireless technologies, privacy techniques, and machine learning.<br/><br/>To demonstrate the feasibility of this project, the PI team is deploying a version of the system in an urban downtown area of Madison, WI. The team is collaborating with a number of local partners -- the city of Madison, the University of Wisconsin Bookstore, 5NINES (a local Internet Service Provider), and a few local participants. Together they are entering this technology demonstration as part of the Global City Teams Challenge being hosted by NSF and NIST."
539,1464279,CRII: CPS: A Knowledge Representation and Information Fusion Framework for Decision Making in Complex Cyber-Physical Systems,CNS,CYBER-PHYSICAL SYSTEMS (CPS),5/15/15,8/17/17,Soumik Sarkar,IA,Iowa State University,Standard Grant,Ralph Wachter,4/30/18,"$208,406.00 ",,soumiks@iastate.edu,1138 Pearson,AMES,IA,500112207,5152945225,CSE,"033y, 7918","7918, 8228, 9150, 9178, 9251",$0.00 ,"This Data-driven Decision-making in Cyber-physical systems (CPS) project focuses on bringing tools from data science and systems science together to develop new tools for analyzing and making accurate decisions in complex cyber-physical systems (e.g., power-grid, transportation network, power plants and smart buildings) to make them safer, more efficient and highly secure. This project develops algorithms, implements software and demonstrates proof-of-concept using large integrated building system as a challenge application area. Potential advantages of the tools developed in this research over current methods will be higher degree of accuracy, increased automation and lower cost of implementation. <br/><br/>Majority of state-of-the-art methods use ad-hoc rules and physics-based models for such problems. However, they lack in accuracy and scalability due to the very complex nature of current and future large interconnected systems. The tools developed in this project will alleviate these issues significantly via intelligent use of large volume of data generated from the systems. The theoretical aspect of the research will make use of inherently multidisciplinary concepts from Nonlinear Dynamics, Information Theory, Machine Learning and Statistical Mechanics. The research project primarily supports interdisciplinary education and career development of graduate students as well as offers education and outreach programs to high school and undergraduate students in STEM disciplines.<br/><br/>The project engages the Center for Building Energy Research (CBER) at Iowa State to demonstrate success on a real platform. The center provides a unique opportunity to the researchers to test and validate the tools on the Interlock House test bed which is a high end field laboratory for energy efficiency research and data validation. This enhances the potential of transitioning the new technology toward commercial reality."
540,1539846,International Workshop on Dynamic Modeling of Health Behavior Change and Maintenance: Moving the Field Forward,IIS,Smart and Connected Health,8/1/15,5/27/15,Donna Spruijt-Metz,CA,University of Southern California,Standard Grant,Sylvia J. Spengler,9/30/16,"$25,000.00 ",Michael Pavel,dmetz@usc.edu,University Park,Los Angeles,CA,900890001,2137407762,CSE,8018,"7556, 8018",$0.00 ,"Poor health-related behaviors and habits are responsible for approximately 40% of preventable deaths and the majority of the chronic disease burden. However, our current understanding of health-related behavior is largely based on static snapshots of human behavior, rather than ongoing, dynamic feedback loops of behavior in response to ever-changing biological, social and personal environmental states. Rich streams of continuous data are becoming increasingly available through emerging technologies, including wearable and deployable sensors and mobile phones. This data, combined with sophisticated modeling techniques emanating primarily from engineering fields, can provide unprecedented opportunities to understand behavior in context and in real time. However, to take advantage of these opportunities, dedicated collaborations between behavioral/health scientists and data modelers, as well as across different schools of complex data modeling techniques, are required. The proposed workshop focuses on developing new concepts for modeling the temporally dense, contextually rich and personalized data increasingly afforded by emerging technologies. The major goals of this proposed workshop are to 1) amalgamate techniques from sub-disciplines across different computational modeling approaches, 2) facilitate development of a shared vocabulary/ontology to facilitate communication between modelers, behavioral and health scientists and 3) advance the rigor of dynamic behavioral theories to the next level towards causal and predictive dynamic models.<br/><br/>The challenge to 21st century data modeling and health behavior research is to move toward computational, dynamic modeling of behavior that can capture complex and rapid changes in behavioral state and related influencing factors. These new models will pave the way for Just-In-Time, Adaptive Interventions (JITAI) that provide feedback in context, in the moment, when people are most receptive and most likely to benefit. To accomplish this, the workshop proposed here will bring together experts in various types of modeling, for example (but not limited) to systems dynamics, social networks, agent-based modeling, machine learning, and Bayesian inference to work together with behavioral scientists and health care professionals to move this endeavor to the next level. The proposed workshop will follow the International Workshop on Methodologies for Developing and Evaluating Digital Health Interventions, which will be held in London UK, 10-11 September 2015, led by Dr. Susan Michie and Dr. Jeremy Wyatt, sponsored by the Medical Research Council (MRC), UK. The workshop proposed here brings the international leaders in health behavior and mobile interventions together with leaders in big data modeling. This workshop will facilitate an unprecedented collaboration between different streams of ""big data"" modelers and behaviorists to develop new paradigms for modeling temporally dense, contextualized behavioral data that can guide future JITIAs across health and wellness domains."
541,1557576,QuBBD: Collaborative Research: Interactive Ensemble clustering for mixed data with application to mood disorders,DMS,,9/15/15,9/11/15,Brian Chapman,UT,University of Utah,Standard Grant,Nandini Kannan,8/31/16,"$16,874.00 ",,brian.chapman@utah.edu,75 S 2000 E,SALT LAKE CITY,UT,841128930,8015816903,MPS,O402,9150,$0.00 ,"The Big Data era has given rise to data of unprecedented size and complexity.  However, fully leveraging Big Data resources for knowledge and discovery is an open challenge due to the fact that conventional methods of data processing and analysis often fail or are inappropriate.  This project develops an innovative approach that utilizes Big Data to improve the classification of mood disorders for the purpose of improving diagnosis and outcomes for psychiatric patients.  Big Data issues are inherently more severe for mental disorders because of their elusive nature.  The psychiatric community has recognized the critical need for a more precise, evidence-based approach for the diagnosis and treatment of disease.  In fact, recent studies funded by the National Institute of Mental Health (NIMH) have found that psychiatric interventions were effective in less than 25% of patients presenting with an acute episode.  This low efficacy rate is especially problematic given the prevalence of mental disorders.  Mood disorders alone (e.g., depression) will be experienced by 1 in 5 adults in the United States at some point in their lives.  This project is motivated by the hypothesis that a more precise and personalized classification of mental health disease can be obtained through the development of novel clustering methods that identify clinically significant structures with these large population data sets.  However, such an approach must overcome a large number of methodological challenges introduced by the complexity of the problem and the nature of large-scale real-world electronic health data.  These challenges include, among others, complex and unknown structure, high dimensionality, heterogeneity, complex mixtures of variables, missing data, and sparsity.  <br/><br/>This award supports initiation of a collaborative research project, carried out by a team with interdisciplinary and complimentary skill sets, to develop methods for big data that address challenges inherent in the integration of biomedical data of this type.  Collective expertise of the team spans the areas of biomedical informatics, biostatistics, computer and information science, electrical and computer engineering, mathematics, and psychiatry.  A novel methodology is developed in a flexible and fully integrated framework that can be extended to other biomedical data and diseases.  Within this framework, clustering methods that capture different aspects of relatedness in the data are integrated in a rigorous way that not only accounts for model uncertainty, but also results in an interactive visualization that is accessible with strong model interpretability for the non-expert.  Specifically, the methodology will rely on novel modifications to bootstrap estimators of generalization error for the purpose of assembling a consensus over an ensemble of clusters inferred from topology-based and machine learning approaches.  The framework also supports iterative refinement of the consensus solution based on user input (via the visualization) to incorporate domain expertise.  The rigorous identification of sub-groups of individuals within heterogeneous populations will facilitate accurate and targeted diagnosis for mood disorders, and provide opportunity for personalized evidence-based interventions.   Applications focus on clustering individuals with mood disorders (bipolar disorder and major depression) from data collected in the Bipolar Disorder Research Network (BDRN).  Despite this focus, the methodology is generalizable to other diseases that face similar challenges for diagnosis and treatment.  In fact, this project supports the first steps of a long-term vision of generalizing the methods to more complex and less curated data, such as electronic health records, social media, and other sources.  This award is supported by the National Institutes of Health Big Data to Knowledge (BD2K) Initiative in partnership with the National Science Foundation Division of Mathematical Sciences."
542,1448525,SBIR Phase I: A Novel Non-Invasive Intracranial Pressure Monitoring Method,IIP,SMALL BUSINESS PHASE I,1/1/15,12/2/14,Robert Hamilton,CA,Neural Analytics,Standard Grant,Jesus Soriano Molla,6/30/15,"$149,294.00 ",,robert@neuralanalytics.com,2440 S. Sepulveda Blvd,Los Angeles,CA,900641744,8183174999,ENG,5371,"124E, 5345, 5371, 8038, 8042",$0.00 ,"The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project will be to improve the treatment and decrease the high costs associated with treating patients who suffer severe traumatic brain injuries. This project aims to develop an accurate, affordable (<$100 per use) and non-invasive device to monitor a patient?s intracranial pressure following traumatic brain injury. Increased intracranial pressure can result in serious condition or death, if left untreated. However, the only available method to monitor intracranial pressure is expensive (~$10,000 per patient) and requires neurosurgery. The lack of a method to accurately screen patients to determine who needs surgery results in misdiagnoses and incorrect treatment in about 46% of patients among an estimated 50,000 patients in the US alone, and hundreds of thousands more globally. Successful commercialization of product is expected to result in savings in the range $250 million ever year to the US healthcare system.<br/><br/>The proposed project will test the feasibility of developing a non-invasive intracranial pressure (ICP) monitoring method for use outside of the neuro ICU. To develop an accurate, affordable, and non-invasive ICP monitoring device, the team will first write and validate a software framework that analyzes Cerebral Blood Flow Velocity (CBFV) waveforms. CBFV waveforms are acquired non-invasively by using transcranial Doppler (TCD) ultrasonography. In order to use CBFVs to predict ICP, two novel signal-processing methods will be developed. First, the high noise levels typical to TCD-acquired waveforms will be reduced within a machine-learning framework. Second, we will use a method to track morphological features that predict ICP from the CBFV waveform. Both these approaches to signal processing to analyze CBFV waveforms are entirely novel. This approach is expected to allow for accurate (>92% of area under the diagnostic ROC) non-invasive real time monitoring at an affordable price point that is within current reimbursement limits for TCD procedures."