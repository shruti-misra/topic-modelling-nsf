"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"1162606","RI: Medium: Advances and Applications in Submodularity for Machine Learning","IIS","Robust Intelligence","07/01/2012","06/30/2015","Jeffrey Bilmes","WA","University of Washington","Continuing grant","Weng-keen Wong","06/30/2018","$814,509.00","","bilmes@ee.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7495","7924","$0.00","Submodularity is an intuitive diminishing returns property, stating that adding an element to a smaller set helps more than adding it to a larger set. Submodularity allows one to efficiently find provably optimal or near-optimal solutions to discrete problems.  Submodular minimization has found use, e.g., in graphical model inference and clustering, whereas maximization has been applied, e.g., to variable/feature selection and active learning.  Submodularity, however, is still only beginning to show applicability in machine learning and its applications.  Moreover, work on submodular optimization in the combinatorics and operations research literature has been primarily unaware of unique problems arising in machine learning. Therefore, existing standard algorithms do not exploit certain structures or variants of the submodular problems arising in machine learning. Studying novel machine learning problems involving submodular objectives can thus lead to advances in the pure combinatorics literature.  We propose to pursue activities that bring together research in machine learning and combinatorial optimization to solve problems which neither of the communities can solve alone.<br/><br/>In particular, we propose to use insights from machine learning to enable scaling up typical submodular optimization problem sizes (by focusing on problem instances arising in learning). We also propose to further chart the territory that submodularity plays in machine learning. In this grant, we will introduce new submodular structures specifically related to submodularity. We will introduce submodular learning problems for machine learning. We will introduce new submodular optimization problems with constraints. And lastly, we will apply these submodular instances to real-world applications in computer vision, speech recognition, and natural language processing."
"1231124","EAAI-12: The Third Symposium on Educational Advances in AI","IIS","ROBUST INTELLIGENCE","03/01/2012","02/29/2012","Matthew Taylor","CA","Association for the Advancement of Artificial Intelligence","Standard Grant","James Donlon","02/28/2013","$17,000.00","","taylorm@eecs.wsu.edu","2275 E BAYSHORE RD STE 160","East Palo Alto","CA","943033224","6503283123","CSE","7495","7495, 7556","$0.00","This award supports participants to EAAI-12, the Third Symposium on Educational Advances in Artificial Intelligence. EAAI-12 will be collocated with the Twenty-sixth AAAI Conference on Artificial Intelligence (AAAI-12), to be held July 22-26, 2012, in Toronto. EAAI will be held on July 23 and 24. The goals of EAAI-12 are to expand the educational relevance of and benefits to researchers, educators, graduate students, and all others who may be interested within the field of Artificial Intelligence (AI). <br/><br/>EAAI-12 provides a venue for researchers and educators to discuss pedagogical issues and share resources related to teaching AI and using AI in education across a variety of curricular levels (K-12 through postgraduate training), with a natural emphasis on undergraduate and graduate teaching and learning. Materials to be presented, discussed, and shared by participants include: model AI assignments with innovative ready-to-adopt materials; syllabi, project ideas and pedagogical strategies related to teaching AI; multi-disciplinary curricula highlighting the use of AI in other contexts (e.g., computational biology, cognitive science, computational economics, philosophy); and resources for teaching specific subareas or topics within AI (e.g., machine learning, game playing, natural language processing, robotics, computer vision). There will be  a keynote lecture discussing recent experience with on-line AI classes at Stanford University. The EAAI symposium will seek to make available contributions from the symposium. Intellectual Merit and Broader Impacts of EAAI-12 include enhancements to the teaching, learning, and understanding of AI, the fostering of stronger research in AI, and increased participation of talented researchers and teachers in AI and more widely those in Computer Science and STEM disciplines."
"1237077","SHB: Type II (INT): Collaborative Research: Creating Learning Systems with Mobile Technology to Improve Coordination in Perioperative Services","IIS","SERVICE ENTERPRISE SYSTEMS, Info Integration & Informatics, Smart and Connected Health","10/01/2012","12/15/2017","Kevin Taaffe","SC","Clemson University","Standard Grant","Sylvia Spengler","06/30/2018","$821,066.00","Joel Greenstein, Lawrence Fredendall","taaffe@clemson.edu","230 Kappa Street","CLEMSON","SC","296345701","8646562424","CSE","1787, 7364, 8018","7364, 8018, 8023, 8062, 9150, 9251","$0.00","This project proposes to create a framework using a combination of mobile technology, learning systems, data analytics, education, and training to enhance cooperation and coordination of staff within and across perioperative services departments (POS).  Perioperative services comprise surgery preparation, operating rooms, post-anesthesia care, sterile processing and a variety of other services, such as radiology and endoscopy.  The specific objectives of this project are to: (1) enhance communication and coordination among POS staff to improve the quality of care by gathering and using important workflow milestones and introducing artificial intelligence techniques through the use of a smart-app, (2) analyze workflow data gathered with smart-apps using data analytics to provide intuitive displays of real-time information for frontline staff and a daily performance dashboard for managers, and (3) induce behavioral and cultural change in healthcare systems through training and education. While existing  information technology capabilities such as natural language processing, artificial intelligence, and speech recognition technology are promising developments in computing, their uses in health care are limited and thus need to be thoroughly investigated before they can be used in health care effectively. To accomplish these objectives, the research team will work closely with the partnering healthcare organizations, Greenville Hospital System (GHS), Palmetto Health (Palmetto), and the Medical University of South Carolina (MUSC), in developing the tools and models which will be pilot-tested at these organizations by their staff.  <br/><br/>The developed tools and models will be widely disseminated among health care providers in South Carolina. In addition, the smart-apps and agent-based simulation model will provide the team with a teaching and training tool that can be used in the classroom at Clemson University and the University of South Carolina (USC) to teach students across a variety of fields, such as business, engineering, science and healthcare students about information and workflow management techniques."
"1237080","SHB: Type II (INT): Collaborative Research: Creating Learning Systems with Mobile Technology to Improve Coordination in Perioperative Services","IIS","Info Integration & Informatics, Smart and Connected Health","10/01/2012","07/28/2014","Nathan Huynh","SC","University South Carolina Research Foundation","Standard Grant","Sylvia Spengler","09/30/2018","$585,916.00","Jose Vidal","huynhn@cec.sc.edu","1600 Hampton Street","COLUMBIA","SC","292080001","8037777093","CSE","7364, 8018","7364, 8018, 8062, 9150, 9251","$0.00","This project proposes to create a framework using a combination of mobile technology, learning systems, data analytics, education, and training to enhance cooperation and coordination of staff within and across perioperative services departments (POS).  Perioperative services comprise surgery preparation, operating rooms, post-anesthesia care, sterile processing and a variety of other services, such as radiology and endoscopy.  The specific objectives of this project are to: (1) enhance communication and coordination among POS staff to improve the quality of care by gathering and using important workflow milestones and introducing artificial intelligence techniques through the use of a smart-app, (2) analyze workflow data gathered with smart-apps using data analytics to provide intuitive displays of real-time information for frontline staff and a daily performance dashboard for managers, and (3) induce behavioral and cultural change in healthcare systems through training and education. While existing  information technology capabilities such as natural language processing, artificial intelligence, and speech recognition technology are promising developments in computing, their uses in health care are limited and thus need to be thoroughly investigated before they can be used in health care effectively. To accomplish these objectives, the research team will work closely with the partnering healthcare organizations, Greenville Hospital System (GHS), Palmetto Health (Palmetto), and the Medical University of South Carolina (MUSC), in developing the tools and models which will be pilot-tested at these organizations by their staff.  <br/><br/>The developed tools and models will be widely disseminated among health care providers in South Carolina. In addition, the smart-apps and agent-based simulation model will provide the team with a teaching and training tool that can be used in the classroom at Clemson University and the University of South Carolina (USC) to teach students across a variety of fields, such as business, engineering, science and healthcare students about information and workflow management techniques."
"1247414","EAGER: Toward Scalable Life-long Representation Learning","IIS","Robust Intelligence","08/01/2012","07/19/2012","Honglak Lee","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Todd Leen","07/31/2014","$115,000.00","","honglak@eecs.umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7495","7916","$0.00","Machine learning is a powerful tool for artificial intelligence and data mining problems. However, its success critically relies on a good feature representation of the data; therefore, the problem of feature construction poses a fundamental challenge. In recent years, representation learning has emerged as a promising method for learning useful feature representations from data. However, the current state-of-the-art methods are still limited in building intelligent agents that can learn and interact with complex environments and large amounts of sensory input. Specifically, the majority of the existing methods cannot scale well to large-scale data.<br/><br/>The goal of this project is to fill this gap by formulating a new framework that can effectively learn representations from complex environments and scale to large data. Specifically, we propose novel approaches for learning robust representations from large-scale data by (1) controlling the complexity of the feature representations and (2) adaptively modeling relevant patterns in the presence of significant amounts of irrelevant patterns or noise.<br/><br/>Key intellectual contributions of this project will be (1) a novel framework of representation learning that provides robust representations from large amounts of unlabeled data and relatively small amounts of labeled data, and (2) theoretical and algorithmic advances for inference, learning, and related optimization problems in representation learning for large-scale, complex sensory information processing.<br/><br/>This work will serve as a catalyst leading to applications, such as multimedia processing and search, medical image processing, speech recognition, and autonomous navigation. The results will be disseminated through publications and free software."
"1216554","Collaborative Research: Methods for Stochastic and Nonlinear Optimization","DMS","COMPUTATIONAL MATHEMATICS","08/01/2012","07/24/2012","Richard Byrd","CO","University of Colorado at Boulder","Standard Grant","rosemary renaut","07/31/2016","$120,000.00","","richard@cs.colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","MPS","1271","9263","$0.00","The projects described in this proposal are designed to advance the capabilities of optimization methods for a class of stochastic and deterministic optimization problems. The first project focuses on  problems where the objective function is given by an expectation or a loss function. We propose dynamic sample algorithms that attempt to bridge the gap between stochastic and batch  methods. Their essential characteristic is that they adapt the sample size during the progression of the optimization in a manner that leads to low computational effort and high accuracy in the solution, when so desired. The second project deals with the design of new active-set methods for solving constrained optimization and convex regularized L1 problems.  Our work builds on two algorithms recently proposed in the literature: the block active-set method (also called the primal-dual active-set method), and the orthant-wise method  for solving L1 regularized problems. Our new algorithms are provably convergent and applicable to a wider class of applications. The third project addresses the need to improve the robustness of nonlinear optimization methods in the presence of infeasibility. Our first goal is to design an interior point method endowed with infeasibility detection capabilities, and to show how its main mechanism can be extended to other interior point methods. The second goal is to develop a convergence theory that is applicable to both active set and interior point methods consisting of three components:  an optimization phase, a feasibility phase, and a mechanism for transitioning between the two phases.<br/><br/>The methods developed in this project are useful in big data analysis, which is playing a vital role in genomics, materials science, meteorology, climate modeling and  information science. In all these disciplines, vast amounts of data have become available in the last decade, with the rate of  generation  accelerating exponentially.  The challenge is to process this large amount of information to make inferences and predictions, thereby accelerating our basic understanding of physical and social systems. For example, the complex physics simulations employed in the design of advanced materials, meteorology and climate modeling, require the use of detailed information obtained over a large set of scenarios. The optimization and machine learning methods developed in this project can be integrated in support of such simulations, thereby obviating the need for  extremely complex models that are difficult to study and generalize. Our work has direct impact in genomics and other areas of biology. For example, we plan to investigate its use in metagenomics, specifically de novo assembly of next generation DNA sequencing data.  Sequences  can be tagged with markers, or found in reference data sets like transcriptomes.  A goal is to use this new information to enable faster and more accurate de novo assembly.  In computer science and information technology, our new algorithms will be useful in the development of a new generation of speech recognition and computer vision systems. Speech recognition, which will play an increasingly important role in many technological applications, can only advance by incorporating more data more intelligently, and the algorithms described in this proposal are designed precisely for that purpose."
"1216567","Collaborative Research: Methods for Stochastic and Nonlinear Optimization","DMS","COMPUTATIONAL MATHEMATICS, OPERATIONS RESEARCH","08/01/2012","07/24/2012","Jorge Nocedal","IL","Northwestern University","Standard Grant","Rosemary Renaut","07/31/2016","$300,000.00","","j-nocedal@northwestern.edu","750 N. Lake Shore Drive","Chicago","IL","606114579","3125037955","MPS","1271, 5514","073E, 9263","$0.00","The projects described in this proposal are designed to advance the capabilities of optimization methods for a class of stochastic and deterministic optimization problems. The first project focuses on  problems where the objective function is given by an expectation or a loss function. We propose dynamic sample algorithms that attempt to bridge the gap between stochastic and batch  methods. Their essential characteristic is that they adapt the sample size during the progression of the optimization in a manner that leads to low computational effort and high accuracy in the solution, when so desired. The second project deals with the design of new active-set methods for solving constrained optimization and convex regularized L1 problems.  Our work builds on two algorithms recently proposed in the literature: the block active-set method (also called the primal-dual active-set method), and the orthant-wise method  for solving L1 regularized problems. Our new algorithms are provably convergent and applicable to a wider class of applications. The third project addresses the need to improve the robustness of nonlinear optimization methods in the presence of infeasibility. Our first goal is to design an interior point method endowed with infeasibility detection capabilities, and to show how its main mechanism can be extended to other interior point methods. The second goal is to develop a convergence theory that is applicable to both active set and interior point methods consisting of three components:  an optimization phase, a feasibility phase, and a mechanism for transitioning between the two phases.<br/><br/>The methods developed in this project are useful in big data analysis, which is playing a vital role in genomics, materials science, meteorology, climate modeling and  information science. In all these disciplines, vast amounts of data have become available in the last decade, with the rate of  generation  accelerating exponentially.  The challenge is to process this large amount of information to make inferences and predictions, thereby accelerating our basic understanding of physical and social systems. For example, the complex physics simulations employed in the design of advanced materials, meteorology and climate modeling, require the use of detailed information obtained over a large set of scenarios. The optimization and machine learning methods developed in this project can be integrated in support of such simulations, thereby obviating the need for  extremely complex models that are difficult to study and generalize. Our work has direct impact in genomics and other areas of biology. For example, we plan to investigate its use in metagenomics, specifically de novo assembly of next generation DNA sequencing data.  Sequences  can be tagged with markers, or found in reference data sets like transcriptomes.  A goal is to use this new information to enable faster and more accurate de novo assembly.  In computer science and information technology, our new algorithms will be useful in the development of a new generation of speech recognition and computer vision systems. Speech recognition, which will play an increasingly important role in many technological applications, can only advance by incorporating more data more intelligently, and the algorithms described in this proposal are designed precisely for that purpose."
"1149803","CAREER: A New Neat Framework for Statistical Machine Learning","IIS","Robust Intelligence","03/01/2012","03/02/2015","Pradeep Ravikumar","TX","University of Texas at Austin","Continuing grant","Weng-keen Wong","02/28/2017","$458,368.00","","pradeepr@cs.cmu.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7495","1045","$0.00","The pendulum in Artificial Intelligence (AI) research has periodically swung from so called ""neat"" or mathematically rigorous approaches, and ""scruffy"" or more adhoc approaches. In recent years, real-world data across varied fields of science and engineering are increasingly complex, and involve a large number of variables, which has resulted in a surge of scruffier methods. This proposal develops a general ""neat"" framework for such modern settings by leveraging state of the art developments in two of the most popular subfields of machine learning methods: graphical models and high-dimensional statistical methods. These developments have in common that a complex model parameter is expressed as a superposition of simple components, which is then leveraged for tractable inference and learning.<br/><br/>Our unified framework results not only in a unified picture of these developments but also provides newer methods to work with such high-dimensional data. The research thus impacts problems across science and engineering wherever statistical machine learning approaches are being used (such as genomics, natural language processing and image analysis, to name a few). The work on a unified framework for statistical machine learning problems is highly coupled with a push for imparting training to students on what we call ""comptastical"" thinking. This combines both computational and statistical thinking required for addressing the problems of limited computation and limited data inherent in modern statistical AI application domains. The proposal also develops an infrastructure for component-based courses with relationally organized lecture module components."
"1217638","Workshop on Research Challenges and Opportunities in Knowledge Representation and Reasoning","IIS","Info Integration & Informatics","06/01/2012","06/04/2012","Natasha Noy","CA","Stanford University","Standard Grant","Sylvia Spengler","05/31/2013","$49,990.00","","natasha.noy@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","7364","7364, 7556","$0.00","Modern information systems in every area of science and engineering rely critically on some form of knowledge representation and inference. Established fields such as Artificial Intelligence and Software Engineering as well as emerging fields such as Semantic Web, Computational Biology, Software Agents, Social Computing, Bioinformatics, Discovery Informatics, Cyberlearning and many others both rely on and contribute to advances in knowledge representation. Formal representations of knowledge and the associated inference mechanisms provide the basis for encoding, sharing, analyzing, interpreting vast amounts of data from disparate sources as well as deciding and acting upon information in virtually every area of human endeavor. <br/><br/>This workshop aims to bring together scientists from all areas of knowledge-representation research and to discuss the new challenges and opportunities that this area faces in addressing the explosion of data and knowledge, increased reliance of scientists on computational data, its heterogeneity, and new modes of delivering, storing, and representing knowledge. This radical shift in the amount of data, in the way that scientists distribute, store, and aggregate this data, presents new challenges for knowledge representation (KR). KR researchers must consider the applicability  of their methods for representation and reasoning on data and knowledge at unprecedented complexity, quantity, and heterogeneity. The distributed and open nature of the data-intensive science presents additional challenges and opportunities in KR, with regard to  provenance, security, trustworthiness, and privacy. The increased adoption of Semantic Web technologies and the rapid increase in the amounts of structured knowledge that is becoming available on the Semantic Web presents additional challenges (e.g., need for coping with information with different degrees of reliability,  information that holds in different contexts, information that changes over time, information that can be conflicting, information that represents beliefs and opinions, etc.  The increased use of KR methods in Computer Vision, Robotics, Natural-Language Processing, Discovery Informatics, and others presents an opportunity for fruitful interdisciplinary collaborations that could dramatically alter the KR research landscape. As scientists, as well as laypersons  get  accustomed to social mechanisms for creating and sharing data and knowledge, it is incumbent upon the researchers in knowledge representation to develop KR mechanisms that support collaborative creation, sharing, and use of knowledge.  Against this background, the workshop brings together a diverse group of researchers and practitioners in KR and related areas to identify new KR research challenges and opportunities presented by  the recent developments in the world wide web, social networks and social media, collaborative and data-intensive e-science, among others.<br/><br/>Broader Impacts: The workshop will identify new areas of research for the Information and the Intelligent Systems at the intersection  Knowledge Representation and Inference, Information Integration and Informatics. Given the increasingly central role of formal representations of knowledge and associated tools for  inference in virtually every domain of human endeavor, the identification of KR the results of the workshop are likely to impact multiple disciplines. The workshop results, including a report summarizing new KR research challenges and opportunities, as well as publications by workshop participants will be broadly disseminated to the larger scientific community."
"1230418","SBIR Phase II: Serious Gaming Platform for Mastering the Physician-Patient Diagnostic Interview","IIP","SMALL BUSINESS PHASE II","09/01/2012","04/23/2014","David Baker","WV","IntelligentSimulations LLC","Standard Grant","Glenn H. Larsen","02/28/2015","$551,483.00","","vicbaker01@gmail.com","1022 Laurelwood Drive","Morgantown","WV","265088900","3042918977","ENG","5373","169E, 5373, 8031, 8032, 8042, 9150, 9178","$0.00","This Small Business Innovation Research (SBIR) Phase II project will complete development of the Artificial Intelligent (AI) Patient platform. This immersive simulation platform helps medical students hone their listening and rapport-building skills by interacting with ?virtual? patients. The student asks questions and the video clip response from the patient depends on the specific question asked, how that question is asked, and when it is asked in the interview. If the student asks the wrong question, or asks inappropriate questions, then the patient will respond in a way that will prevent a positive doctor-patient relationship from developing. Because it combines video clips and artificial intelligence and is web-based, we anticipate that this training platform will teach a medical school student (and other health professionals) how to really pay attention to their patients, and that it will do so more effectively than other training products already on the market. In Phase II, we will complete the platform build, incorporate additional artificial intelligence and speech recognition capabilities, create versions of the platform that will work on smart phones and tablets, and develop a set of six patient scenarios. This work will result in a platform that closely approximates a natural conversational experience.<br/><br/>The broader impact/commercial potential of this project will be to achieve a significant advancement over currently available computer-based training and other instructional tools. Medical schools are experiencing a transition from passive to active methods of teaching, as professors and students both have the desire to replace passive learning methods of PowerPoint slides, lectures, and instructional videos with more interactive methods of learning. The active experience provided by the AI Patient platform will help students learn to be more effective listeners and better health practitioners, which will result in better patient outcomes. The platform will also allow researchers to track and measure the effectiveness of using this kind of training tool. By working with our medical school teaming partners (West Virginia University School of Medicine and the West Virginia School of Osteopathic Medicine), we will develop a set of scenarios that we can license to other medical schools. Sales to the medical school community will support the development of more medical scenarios and enable us to expand into other sectors?such as law enforcement, HR training, and social work?where trust and rapport-building skills are critical."
"1208256","Collaborative Research:   AVATOL - Next Generation Phenomics for the Tree of Life","IOS","ASSEMBLING THE TREE OF LIFE","06/01/2012","06/17/2015","Emily Sessa","FL","University of Florida","Standard Grant","Irwin Forseth","05/31/2017","$440,135.00","","emilysessa@ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","BIO","7689","1355, 7689, 9169, 9178, 9251, EGCH, SMET","$0.00","Darwin's theory of evolution explained that millions of species are related, and dealt biologists and paleontologists the enormous challenge of discovering the branching pattern of the Tree of Life. Work on this great challenge is producing a map of species-relatedness through Earth history, and answering questions such as ""what is the closest relative of a species?"" and ""what species make important products?"" To do this scientists draw on all heritable features, including genotypes (e.g., DNA) and phenotypes (e.g., anatomy). Studying phenotypes, however, has remained complicated and slow, because it has not been revolutionized by computer science and engineering innovations.<br/><br/>     A team of biologists, computer scientists, and paleontologists will extend and adapt methods from computer vision, machine learning, crowd-sourcing and natural language processing to enable rapid and automated study of phenotypes for the Tree of Life on a vast scale. The three-year goal is to release large phenomic datasets built using new methods, and to provide the public and scientific community with tools for future work. Planned is the training of teachers and students (kindergarten - postdoctoral levels) and the engagement of ""citizen scien-tists."" Enormous phenomic datasets, many with images, will fill an important public interest in biodiversity and the fossil record."
"1208306","Collaborative Research:   AVATOL - Next Generation Phenomics for the Tree of Life","DEB","ASSEMBLING THE TREE OF LIFE","06/01/2012","05/21/2012","Nancy Simmons","NY","American Museum Natural History","Standard Grant","Simon Malcomber","05/31/2016","$64,843.00","Andrea Cirranello, Paul Velazco","simmons@amnh.org","Central Park West at 79th St","New York","NY","100240000","2127695975","BIO","7689","","$0.00","Darwin's theory of evolution explained that millions of species are related, and dealt biologists and paleontologists the enormous challenge of discovering the branching pattern of the Tree of Life. Work on this great challenge is producing a map of species-relatedness through Earth history, and answering questions such as ""what is the closest relative of a species?"" and ""what species make important products?"" To do this scientists draw on all heritable features, including genotypes (e.g., DNA) and phenotypes (e.g., anatomy). Studying phenotypes, however, has remained complicated and slow, because it has not been revolutionized by computer science and engineering innovations.<br/><br/>     A team of biologists, computer scientists, and paleontologists will extend and adapt methods from computer vision, machine learning, crowd-sourcing and natural language processing to enable rapid and automated study of phenotypes for the Tree of Life on a vast scale. The three-year goal is to release large phenomic datasets built using new methods, and to provide the public and scientific community with tools for future work. Planned is the training of teachers and students (kindergarten - postdoctoral levels) and the engagement of ""citizen scien-tists."" Enormous phenomic datasets, many with images, will fill an important public interest in biodiversity and the fossil record."
"1208310","Collaborative Research:   AVATOL - Next Generation Phenomics for the Tree of Life","DEB","ASSEMBLING THE TREE OF LIFE","06/01/2012","05/21/2012","Robert Thacker","AL","University of Alabama at Birmingham","Standard Grant","Simon Malcomber","12/31/2015","$79,110.00","","robert.thacker@stonybrook.edu","AB 1170","Birmingham","AL","352940001","2059345266","BIO","7689","","$0.00","Darwin's theory of evolution explained that millions of species are related, and dealt biologists and paleontologists the enormous challenge of discovering the branching pattern of the Tree of Life. Work on this great challenge is producing a map of species-relatedness through Earth history, and answering questions such as ""what is the closest relative of a species?"" and ""what species make important products?"" To do this scientists draw on all heritable features, including genotypes (e.g., DNA) and phenotypes (e.g., anatomy). Studying phenotypes, however, has remained complicated and slow, because it has not been revolutionized by computer science and engineering innovations.<br/><br/>     A team of biologists, computer scientists, and paleontologists will extend and adapt methods from computer vision, machine learning, crowd-sourcing and natural language processing to enable rapid and automated study of phenotypes for the Tree of Life on a vast scale. The three-year goal is to release large phenomic datasets built using new methods, and to provide the public and scientific community with tools for future work. Planned is the training of teachers and students (kindergarten - postdoctoral levels) and the engagement of ""citizen scien-tists."" Enormous phenomic datasets, many with images, will fill an important public interest in biodiversity and the fossil record."
"1208272","Collaborative Research:   AVATOL - Next Generation Phenomics for the Tree of Life","DEB","ASSEMBLING THE TREE OF LIFE","06/01/2012","09/04/2014","Thomas Dietterich","OR","Oregon State University","Standard Grant","Simon Malcomber","05/31/2017","$1,005,860.00","Sinisa Todorovic","tgd@cs.orst.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","BIO","7689","7689, 9169, EGCH","$0.00","Darwin's theory of evolution explained that millions of species are related, and dealt biologists and paleontologists the enormous challenge of discovering the branching pattern of the Tree of Life. Work on this great challenge is producing a map of species-relatedness through Earth history, and answering questions such as ""what is the closest relative of a species?"" and ""what species make important products?"" To do this scientists draw on all heritable features, including genotypes (e.g., DNA) and phenotypes (e.g., anatomy). Studying phenotypes, however, has remained complicated and slow, because it has not been revolutionized by computer science and engineering innovations.<br/><br/>     A team of biologists, computer scientists, and paleontologists will extend and adapt methods from computer vision, machine learning, crowd-sourcing and natural language processing to enable rapid and automated study of phenotypes for the Tree of Life on a vast scale. The three-year goal is to release large phenomic datasets built using new methods, and to provide the public and scientific community with tools for future work. Planned is the training of teachers and students (kindergarten - postdoctoral levels) and the engagement of ""citizen scien-tists."" Enormous phenomic datasets, many with images, will fill an important public interest in biodiversity and the fossil record."
"1208523","Collaborative Research:   AVATOL - Next Generation Phenomics for the Tree of Life","DEB","ASSEMBLING THE TREE OF LIFE","06/01/2012","05/21/2012","Marymegan Daly","OH","Ohio State University","Standard Grant","Simon Malcomber","08/31/2016","$85,859.00","","daly.66@osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","BIO","7689","","$0.00","Darwin's theory of evolution explained that millions of species are related, and dealt biologists and paleontologists the enormous challenge of discovering the branching pattern of the Tree of Life. Work on this great challenge is producing a map of species-relatedness through Earth history, and answering questions such as ""what is the closest relative of a species?"" and ""what species make important products?"" To do this scientists draw on all heritable features, including genotypes (e.g., DNA) and phenotypes (e.g., anatomy). Studying phenotypes, however, has remained complicated and slow, because it has not been revolutionized by computer science and engineering innovations.<br/><br/>     A team of biologists, computer scientists, and paleontologists will extend and adapt methods from computer vision, machine learning, crowd-sourcing and natural language processing to enable rapid and automated study of phenotypes for the Tree of Life on a vast scale. The three-year goal is to release large phenomic datasets built using new methods, and to provide the public and scientific community with tools for future work. Planned is the training of teachers and students (kindergarten - postdoctoral levels) and the engagement of ""citizen scien-tists."" Enormous phenomic datasets, many with images, will fill an important public interest in biodiversity and the fossil record."
"1208534","Collaborative Research:   AVATOL - Next Generation Phenomics for the Tree of Life","DEB","ASSEMBLING THE TREE OF LIFE","06/01/2012","05/21/2012","Carrine Blank","MT","University of Montana","Standard Grant","Simon Malcomber","05/31/2016","$144,385.00","","carrine.blank@umontana.edu","32 CAMPUS DRIVE","Missoula","MT","598120001","4062436670","BIO","7689","","$0.00","Darwin's theory of evolution explained that millions of species are related, and dealt biologists and paleontologists the enormous challenge of discovering the branching pattern of the Tree of Life. Work on this great challenge is producing a map of species-relatedness through Earth history, and answering questions such as ""what is the closest relative of a species?"" and ""what species make important products?"" To do this scientists draw on all heritable features, including genotypes (e.g., DNA) and phenotypes (e.g., anatomy). Studying phenotypes, however, has remained complicated and slow, because it has not been revolutionized by computer science and engineering innovations.<br/><br/>     A team of biologists, computer scientists, and paleontologists will extend and adapt methods from computer vision, machine learning, crowd-sourcing and natural language processing to enable rapid and automated study of phenotypes for the Tree of Life on a vast scale. The three-year goal is to release large phenomic datasets built using new methods, and to provide the public and scientific community with tools for future work. Planned is the training of teachers and students (kindergarten - postdoctoral levels) and the engagement of ""citizen scien-tists."" Enormous phenomic datasets, many with images, will fill an important public interest in biodiversity and the fossil record."
"1208567","Collaborative Research:   AVATOL - Next Generation Phenomics for the Tree of Life","DEB","ASSEMBLING THE TREE OF LIFE","06/01/2012","09/11/2014","Hong Cui","AZ","University of Arizona","Standard Grant","Simon Malcomber","05/31/2018","$460,343.00","","hongcui@email.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","BIO","7689","1355, 7689, 9169, 9178, 9251, EGCH, SMET","$0.00","Darwin's theory of evolution explained that millions of species are related, and dealt biologists and paleontologists the enormous challenge of discovering the branching pattern of the Tree of Life. Work on this great challenge is producing a map of species-relatedness through Earth history, and answering questions such as ""what is the closest relative of a species?"" and ""what species make important products?"" To do this scientists draw on all heritable features, including genotypes (e.g., DNA) and phenotypes (e.g., anatomy). Studying phenotypes, however, has remained complicated and slow, because it has not been revolutionized by computer science and engineering innovations.<br/><br/>     A team of biologists, computer scientists, and paleontologists will extend and adapt methods from computer vision, machine learning, crowd-sourcing and natural language processing to enable rapid and automated study of phenotypes for the Tree of Life on a vast scale. The three-year goal is to release large phenomic datasets built using new methods, and to provide the public and scientific community with tools for future work. Planned is the training of teachers and students (kindergarten - postdoctoral levels) and the engagement of ""citizen scien-tists."" Enormous phenomic datasets, many with images, will fill an important public interest in biodiversity and the fossil record."
"1208845","Collaborative Research:   AVATOL - Next Generation Phenomics for the Tree of Life","DEB","ASSEMBLING THE TREE OF LIFE","06/01/2012","05/21/2012","Dennis Stevenson","NY","New York Botanical Garden","Standard Grant","Simon Malcomber","05/31/2016","$90,739.00","Ramona Walls","dws@nybg.org","2900 Southern Blvd","Bronx","NY","104585126","7188178840","BIO","7689","9251","$0.00","Darwin's theory of evolution explained that millions of species are related, and dealt biologists and paleontologists the enormous challenge of discovering the branching pattern of the Tree of Life. Work on this great challenge is producing a map of species-relatedness through Earth history, and answering questions such as ""what is the closest relative of a species?"" and ""what species make important products?"" To do this scientists draw on all heritable features, including genotypes (e.g., DNA) and phenotypes (e.g., anatomy). Studying phenotypes, however, has remained complicated and slow, because it has not been revolutionized by computer science and engineering innovations.<br/><br/>     A team of biologists, computer scientists, and paleontologists will extend and adapt methods from computer vision, machine learning, crowd-sourcing and natural language processing to enable rapid and automated study of phenotypes for the Tree of Life on a vast scale. The three-year goal is to release large phenomic datasets built using new methods, and to provide the public and scientific community with tools for future work. Planned is the training of teachers and students (kindergarten - postdoctoral levels) and the engagement of ""citizen scien-tists."" Enormous phenomic datasets, many with images, will fill an important public interest in biodiversity and the fossil record."
"1208685","Collaborative Research:   AVATOL - Next Generation Phenomics for the Tree of Life","DEB","ASSEMBLING THE TREE OF LIFE","06/01/2012","08/25/2014","Lisa Moore","ME","University of Southern Maine","Standard Grant","Simon Malcomber","05/31/2018","$336,877.00","","lrmoore@maine.edu","96 Falmouth St","Portland","ME","041049300","2072288536","BIO","7689","7689, 9169, EGCH","$0.00","Darwin's theory of evolution explained that millions of species are related, and dealt biologists and paleontologists the enormous challenge of discovering the branching pattern of the Tree of Life. Work on this great challenge is producing a map of species-relatedness through Earth history, and answering questions such as ""what is the closest relative of a species?"" and ""what species make important products?"" To do this scientists draw on all heritable features, including genotypes (e.g., DNA) and phenotypes (e.g., anatomy). Studying phenotypes, however, has remained complicated and slow, because it has not been revolutionized by computer science and engineering innovations.<br/><br/>     A team of biologists, computer scientists, and paleontologists will extend and adapt methods from computer vision, machine learning, crowd-sourcing and natural language processing to enable rapid and automated study of phenotypes for the Tree of Life on a vast scale. The three-year goal is to release large phenomic datasets built using new methods, and to provide the public and scientific community with tools for future work. Planned is the training of teachers and students (kindergarten - postdoctoral levels) and the engagement of ""citizen scien-tists."" Enormous phenomic datasets, many with images, will fill an important public interest in biodiversity and the fossil record."
"1208619","Collaborative Research:   AVATOL - Next Generation Phenomics for the Tree of Life","DEB","ASSEMBLING THE TREE OF LIFE","06/01/2012","05/21/2012","Edward Theriot","TX","University of Texas at Austin","Standard Grant","Simon Malcomber","05/31/2016","$103,238.00","","etheriot@austin.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","BIO","7689","","$0.00","Darwin's theory of evolution explained that millions of species are related, and dealt biologists and paleontologists the enormous challenge of discovering the branching pattern of the Tree of Life. Work on this great challenge is producing a map of species-relatedness through Earth history, and answering questions such as ""what is the closest relative of a species?"" and ""what species make important products?"" To do this scientists draw on all heritable features, including genotypes (e.g., DNA) and phenotypes (e.g., anatomy). Studying phenotypes, however, has remained complicated and slow, because it has not been revolutionized by computer science and engineering innovations.<br/><br/>     A team of biologists, computer scientists, and paleontologists will extend and adapt methods from computer vision, machine learning, crowd-sourcing and natural language processing to enable rapid and automated study of phenotypes for the Tree of Life on a vast scale. The three-year goal is to release large phenomic datasets built using new methods, and to provide the public and scientific community with tools for future work. Planned is the training of teachers and students (kindergarten - postdoctoral levels) and the engagement of ""citizen scien-tists."" Enormous phenomic datasets, many with images, will fill an important public interest in biodiversity and the fossil record."
"1208270","Collaborative Research:   AVATOL - Next Generation Phenomics for the Tree of Life","DEB","GoLife, ASSEMBLING THE TREE OF LIFE","06/01/2012","03/01/2016","Maureen O'Leary","NY","SUNY at Stony Brook","Standard Grant","Simon Malcomber","05/31/2017","$978,113.00","","maureen.oleary@stonybrook.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","BIO","6133, 7689","6133, 7689, 9169, EGCH","$0.00","Darwin's theory of evolution explained that millions of species are related, and dealt biologists and paleontologists the enormous challenge of discovering the branching pattern of the Tree of Life. Work on this great challenge is producing a map of species-relatedness through Earth history, and answering questions such as ""what is the closest relative of a species?"" and ""what species make important products?"" To do this scientists draw on all heritable features, including genotypes (e.g., DNA) and phenotypes (e.g., anatomy). Studying phenotypes, however, has remained complicated and slow, because it has not been revolutionized by computer science and engineering innovations.<br/><br/>     A team of biologists, computer scientists, and paleontologists will extend and adapt methods from computer vision, machine learning, crowd-sourcing and natural language processing to enable rapid and automated study of phenotypes for the Tree of Life on a vast scale. The three-year goal is to release large phenomic datasets built using new methods, and to provide the public and scientific community with tools for future work. Planned is the training of teachers and students (kindergarten - postdoctoral levels) and the engagement of ""citizen scientists."" Enormous phenomic datasets, many with images, will fill an important public interest in biodiversity and the fossil record."
"1205589","CI-P:Collaborative Research:The Speech Recognition Virtual Kitchen","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","06/01/2012","05/16/2012","Florian Metze","PA","Carnegie-Mellon University","Standard Grant","Tatiana Korelsky","05/31/2013","$49,645.00","","fmetze@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7359","7359","$0.00","CI-P:Collaborative Research: The Speech Recognition Virtual Kitchen<br/><br/>This project provides a ""kitchen"" environment to promote community sharing of research techniques, foster innovative experimentation, and provide solid reference systems for automatically recognizing speech, as a tool for education, research, and evaluation.  The research infrastructure is built around virtual machines (VMs), which can be reconfigured and shared easily.  We liken the virtual machines to a ""kitchen"" because they provide the environmental infrastructure into which one can install ""appliances"" (e.g., speech recognition toolkits), ""recipes"" (scripts for creating state-of-the art systems for a toolkit), and ""ingredients"" (spoken language data), along with ""dishes"" (completed experiments with log-files for reference).<br/><br/>The planning project engages the community to tackle some of the issues that have previously hampered efforts in cross-community sharing, including distribution methods and intellectual property issues. The project also provides an example architecture, which serves as a focus point for community-wide discussion.<br/><br/>In terms of broader impacts, the project engages researchers and educators that typically do not participate in automatic speech recognition (ASR) research by providing travel scholarships to a workshop at INTERSPEECH2012.  In a wider scope, the infrastructure may be useable by other data-intensive fields (synthesis, dialog systems, NLP, computer vision, data mining). By providing a permanent, publicly available resource  for research,  education, and evaluation in ASR research, we can better train the next generation of undergraduates and graduates. The ""kitchen"" gives them easy access to a large number of state-of-the-art implementations, and facilitates deeper analysis of algorithms and better comparisons across systems."
"1205424","CI-P:Collaborative Research:The Speech Recognition Virtual Kitchen","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","06/01/2012","05/16/2012","Eric Fosler-Lussier","OH","Ohio State University","Standard Grant","Tatiana Korelsky","05/31/2013","$48,509.00","","fosler@cse.ohio-state.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","7359","7359","$0.00","CI-P:Collaborative Research: The Speech Recognition Virtual Kitchen <br/><br/>This project provides a ""kitchen"" environment to promote community sharing of research techniques, foster innovative experimentation, and provide solid reference systems for automatically recognizing speech, as a tool for education, research, and evaluation. The research infrastructure is built around virtual machines (VMs), which can be reconfigured and shared easily. We liken the virtual machines to a ""kitchen"" because they provide the environmental infrastructure into which one can install ""appliances"" (e.g., speech recognition toolkits), ""recipes"" (scripts for creating state-of-the art systems for a toolkit), and ""ingredients"" (spoken language data), along with ""dishes"" (completed experiments with log-files for reference). <br/><br/>The planning project engages the community to tackle some of the issues that have previously hampered efforts in cross-community sharing, including distribution methods and intellectual property issues. The project also provides an example architecture, which serves as a focus point for community-wide discussion. <br/><br/>In terms of broader impacts, the project engages researchers and educators that typically do not participate in automatic speech recognition (ASR) research by providing travel scholarships to a workshop at INTERSPEECH2012. In a wider scope, the infrastructure may be useable by other data-intensive fields (synthesis, dialog systems, NLP, computer vision, data mining). By providing a permanent, publicly available resource for research, education, and evaluation in ASR research, we can better train the next generation of undergraduates and graduates. The ""kitchen"" gives them easy access to a large number of state-of-the-art implementations, and facilitates deeper analysis of algorithms and better comparisons across systems."
"1161962","RI: Medium: Collaborative Research: Multilingual Gestural Models for Robust Language-Independent Speech Recognition","IIS","INFORMATION TECHNOLOGY RESEARC","10/01/2012","09/18/2012","Hosung Nam","CT","Haskins Laboratories, Inc.","Standard Grant","Tatiana D. Korelsky","09/30/2016","$203,791.00","","nam@haskins.yale.edu","300 George Street","New Haven","CT","065116610","2038656163","CSE","1640","1640, 7495, 7924","$0.00","Current state-of-the-art automatic speech recognition (ASR) systems typically model speech as a string of acoustically-defined phones and use contextualized phone units, such as tri-phones or quin-phones to model contextual influences due to coarticulation.  Such acoustic models may suffer from data sparsity and may fail to capture coarticulation appropriately because the span of a tri- or quin-phone's contextual influence is not flexible. In a small vocabulary context, however, research has shown that ASR systems which estimate articulatory gestures from the acoustics and incorporate these gestures in the ASR process can better model coarticulation and are more robust to noise.  The current project investigates the use of estimated articulatory gestures in large vocabulary automatic speech recognition.  Gestural representations of the speech signal are initially created from the acoustic waveform using the Task Dynamic model of speech production.  These data are then used to train automatic models for articulatory gesture recognition where the articulatory gestures serve as subword units in the gesture-based ASR system.  The main goal of the proposed work is to evaluate the performance of a large-vocabulary gesture-based ASR system using American English (AE).  The gesture-based system will be compared to a set of competitive state-of-the-art recognition systems in term of word and phone recognition accuracies, both under clean and noisy acoustic background conditions.<br/><br/>The broad impact of this research is threefold: (1) the creation of a large vocabulary American English (AE) speech database containing acoustic waveforms and their articulatory representations, (2) the introduction of novel machine learning techniques to model articulatory representations from acoustic waveforms, and (3) the development of a large vocabulary ASR system that uses articulatory representation as subword units.  The robust and accurate ASR system for AE resulting from the proposed project will deal effectively with speech variability, thereby significantly enhancing communication and collaboration between people and machines in AE, and with the promise to generalize the method to multiple languages.  The knowledge gained and the systems developed will contribute to the broad application of articulatory features in speech processing, and will have the potential to transform the fields of ASR, speech-mediated person-machine interaction, and automatic translation among languages.  The interdisciplinary collaboration will facilitate a cross-disciplinary learning environment for the participating faculty, researchers, graduate students and undergraduate students  Thus, this collaboration will result in the broader impact of enhanced training in speech modeling and algorithm development.  Finally, the proposed work will result in a set of databases and tools that will be disseminated to serve the research and education community at large."
"1162046","RI: Medium: Collaborative Research: Multilingual Gestural Models for Robust Language-Independent Speech Recognition","IIS","Information Technology Researc","10/01/2012","09/25/2012","Vikramjit Mitra","CA","SRI International","Standard Grant","Tatiana Korelsky","09/30/2015","$108,713.00","","vmitra@speech.sri.com","333 RAVENSWOOD AVE","Menlo Park","CA","940253493","7032478529","CSE","1640","1640, 7495, 7924","$0.00","Current state-of-the-art automatic speech recognition (ASR) systems typically model speech as a string of acoustically-defined phones and use contextualized phone units, such as tri-phones or quin-phones to model contextual influences due to coarticulation.  Such acoustic models may suffer from data sparsity and may fail to capture coarticulation appropriately because the span of a tri- or quin-phone's contextual influence is not flexible. In a small vocabulary context, however, research has shown that ASR systems which estimate articulatory gestures from the acoustics and incorporate these gestures in the ASR process can better model coarticulation and are more robust to noise.  The current project investigates the use of estimated articulatory gestures in large vocabulary automatic speech recognition.  Gestural representations of the speech signal are initially created from the acoustic waveform using the Task Dynamic model of speech production.  These data are then used to train automatic models for articulatory gesture recognition where the articulatory gestures serve as subword units in the gesture-based ASR system.  The main goal of the proposed work is to evaluate the performance of a large-vocabulary gesture-based ASR system using American English (AE).  The gesture-based system will be compared to a set of competitive state-of-the-art recognition systems in term of word and phone recognition accuracies, both under clean and noisy acoustic background conditions.<br/><br/>The broad impact of this research is threefold: (1) the creation of a large vocabulary American English (AE) speech database containing acoustic waveforms and their articulatory representations, (2) the introduction of novel machine learning techniques to model articulatory representations from acoustic waveforms, and (3) the development of a large vocabulary ASR system that uses articulatory representation as subword units.  The robust and accurate ASR system for AE resulting from the proposed project will deal effectively with speech variability, thereby significantly enhancing communication and collaboration between people and machines in AE, and with the promise to generalize the method to multiple languages.  The knowledge gained and the systems developed will contribute to the broad application of articulatory features in speech processing, and will have the potential to transform the fields of ASR, speech-mediated person-machine interaction, and automatic translation among languages.  The interdisciplinary collaboration will facilitate a cross-disciplinary learning environment for the participating faculty, researchers, graduate students and undergraduate students  Thus, this collaboration will result in the broader impact of enhanced training in speech modeling and algorithm development.  Finally, the proposed work will result in a set of databases and tools that will be disseminated to serve the research and education community at large."
"1162033","RI: Medium: Collaborative Research: Multilingual Gestural Models for Robust Language-Independent Speech Recognition","IIS","INFORMATION TECHNOLOGY RESEARC","10/01/2012","09/18/2012","Elliot Saltzman","MA","Trustees of Boston University","Standard Grant","Tatiana D. Korelsky","09/30/2017","$52,627.00","","esaltz@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","1640","1640, 7495, 7924","$0.00","Current state-of-the-art automatic speech recognition (ASR) systems typically model speech as a string of acoustically-defined phones and use contextualized phone units, such as tri-phones or quin-phones to model contextual influences due to coarticulation.  Such acoustic models may suffer from data sparsity and may fail to capture coarticulation appropriately because the span of a tri- or quin-phone's contextual influence is not flexible. In a small vocabulary context, however, research has shown that ASR systems which estimate articulatory gestures from the acoustics and incorporate these gestures in the ASR process can better model coarticulation and are more robust to noise.  The current project investigates the use of estimated articulatory gestures in large vocabulary automatic speech recognition.  Gestural representations of the speech signal are initially created from the acoustic waveform using the Task Dynamic model of speech production.  These data are then used to train automatic models for articulatory gesture recognition where the articulatory gestures serve as subword units in the gesture-based ASR system.  The main goal of the proposed work is to evaluate the performance of a large-vocabulary gesture-based ASR system using American English (AE).  The gesture-based system will be compared to a set of competitive state-of-the-art recognition systems in term of word and phone recognition accuracies, both under clean and noisy acoustic background conditions.<br/><br/>The broad impact of this research is threefold: (1) the creation of a large vocabulary American English (AE) speech database containing acoustic waveforms and their articulatory representations, (2) the introduction of novel machine learning techniques to model articulatory representations from acoustic waveforms, and (3) the development of a large vocabulary ASR system that uses articulatory representation as subword units.  The robust and accurate ASR system for AE resulting from the proposed project will deal effectively with speech variability, thereby significantly enhancing communication and collaboration between people and machines in AE, and with the promise to generalize the method to multiple languages.  The knowledge gained and the systems developed will contribute to the broad application of articulatory features in speech processing, and will have the potential to transform the fields of ASR, speech-mediated person-machine interaction, and automatic translation among languages.  The interdisciplinary collaboration will facilitate a cross-disciplinary learning environment for the participating faculty, researchers, graduate students and undergraduate students  Thus, this collaboration will result in the broader impact of enhanced training in speech modeling and algorithm development.  Finally, the proposed work will result in a set of databases and tools that will be disseminated to serve the research and education community at large."
"1162525","RI: Medium: Collaborative Research: Multilingual Gestural Models for Robust Language-Independent Speech Recognition","IIS","INFORMATION TECHNOLOGY RESEARC, ROBUST INTELLIGENCE","10/01/2012","09/18/2012","Carol Espy-Wilson","MD","University of Maryland College Park","Standard Grant","Tatiana D. Korelsky","09/30/2016","$234,868.00","","espy@eng.umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","1640, 7495","1640, 7495, 7924","$0.00","Current state-of-the-art automatic speech recognition (ASR) systems typically model speech as a string of acoustically-defined phones and use contextualized phone units, such as tri-phones or quin-phones to model contextual influences due to coarticulation.  Such acoustic models may suffer from data sparsity and may fail to capture coarticulation appropriately because the span of a tri- or quin-phone's contextual influence is not flexible. In a small vocabulary context, however, research has shown that ASR systems which estimate articulatory gestures from the acoustics and incorporate these gestures in the ASR process can better model coarticulation and are more robust to noise.  The current project investigates the use of estimated articulatory gestures in large vocabulary automatic speech recognition.  Gestural representations of the speech signal are initially created from the acoustic waveform using the Task Dynamic model of speech production.  These data are then used to train automatic models for articulatory gesture recognition where the articulatory gestures serve as subword units in the gesture-based ASR system.  The main goal of the proposed work is to evaluate the performance of a large-vocabulary gesture-based ASR system using American English (AE).  The gesture-based system will be compared to a set of competitive state-of-the-art recognition systems in term of word and phone recognition accuracies, both under clean and noisy acoustic background conditions.<br/><br/>The broad impact of this research is threefold: (1) the creation of a large vocabulary American English (AE) speech database containing acoustic waveforms and their articulatory representations, (2) the introduction of novel machine learning techniques to model articulatory representations from acoustic waveforms, and (3) the development of a large vocabulary ASR system that uses articulatory representation as subword units.  The robust and accurate ASR system for AE resulting from the proposed project will deal effectively with speech variability, thereby significantly enhancing communication and collaboration between people and machines in AE, and with the promise to generalize the method to multiple languages.  The knowledge gained and the systems developed will contribute to the broad application of articulatory features in speech processing, and will have the potential to transform the fields of ASR, speech-mediated person-machine interaction, and automatic translation among languages.  The interdisciplinary collaboration will facilitate a cross-disciplinary learning environment for the participating faculty, researchers, graduate students and undergraduate students  Thus, this collaboration will result in the broader impact of enhanced training in speech modeling and algorithm development.  Finally, the proposed work will result in a set of databases and tools that will be disseminated to serve the research and education community at large."
"1218863","RI: Small: Developing Large Scale Distributed Syntactic, Semantic and Lexical Language Models for Machine Translation and Speech Recognition","IIS","ROBUST INTELLIGENCE","08/01/2012","07/23/2013","Shaojun Wang","OH","Wright State University","Continuing grant","Tatiana D. Korelsky","07/31/2016","$460,000.00","Yunxin Zhao","swang.usa@gmail.com","3640 Colonel Glenn Highway","Dayton","OH","454350001","9377752425","CSE","7495","7923","$0.00","This project aims to build large scale distributed syntactic, semantic, and lexical language models that are trained by corpora with up to Web-scale data on a supercomputer in order to substantially improve the performance of machine translation and speech recognition systems.  It is conducted under the directed Markov random field paradigm to integrate both topics and syntax to form complex distributions for natural language, and uses hierarchical Pitman-Yor processes to model long-tail properties of natural language.  By exploiting this particular structure, the complex statistical estimation and inference algorithms are decomposed and performed in a distributed environment.  The language models are put into one-pass decoders of machine translation systems, and the lattice rescoring decoder into a speech recognition system.  In addition, a principled solution to a long-standing open problem, smoothing fractional counts due to latent variables in Kneser-Ney's sense, might be found. <br/><br/>This project fits into the NSF's strategic long term vision of a Cyber-infrastructure Framework for 21st Century Science and Engineering (CIF21).  The project integrates various kinds of known language models and provides a way to overcome the limitations of existing combination methods for language models and to deploy algorithmically interesting methodologies that are scalable to data sets available on the Web.  The project provides an environment for interdisciplinary education in information technology that bridges areas of language and speech processing, machine learning, and data-intensive science and engineering to benefit students at several levels."
"1142412","SBIR Phase I:  Contextual ASR to Support EHR Adoption","IIP","SBIR Phase I","01/01/2012","11/21/2011","Daniel Riskin","DE","VMT, Inc.","Standard Grant","Glenn H. Larsen","12/31/2012","$150,000.00","","grants@verantos.com","113 BARKSDALE PROFESSIONAL CTR","Newark","DE","197113258","6507777978","ENG","5371","5371, 8032, 8033, 8039","$0.00","This Small Business Innovative Research SBIR Phase I project will use statistical analysis of historical medical records to create families of language models for each section of the traditional medical note and switch lexicons in and out of the automatic speech recognition (ASR) in real time based on the contextual position within the narrative note.  Current speech recognition methods use a single, general-purpose medical lexicon to train a recognizer when identifying words.  Medical context-specific probabilities are ignored.  Because the DocTalk engine incorporates real time integrated ASR with natural language processing (NLP), there is an opportunity to utilize NLP contextual data to actually change the ongoing ASR process.  This innovative text structuring method will exploit the statistical variability of language used in each section of the medical record.  It is a unique opportunity to address delay in workflow; the largest barrier to a national electronic healthcare infrastructure, by using a cloud-based, open source leveraged solution.<br/><br/>The broader impact/commercial potential of this project includes the ability of physicians to increase usable information, avoid third party transcription errors, and mitigate workflow delays.  The majority of workflow delay in electronic medical records (EMR) is the need to perform manual operations to fill structured forms within the record, as opposed to simple unstructured narratives used in traditional written notes and transcriptions.  Successful completion of this innovative proposed program of NLP-enhanced context based ASR will provide the accuracy required to deploy an integrated, interactive, intuitive, low-cost data entry system for small practice primary care physicians, and help overcome the largest obstacle to a national electronic healthcare infrastructure."
"1201790","Super-Turing Computation and Brain-Like Intelligence","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","07/01/2012","04/30/2014","A. Steven Younger","MO","Missouri State University","Continuing grant","Radhakisan Baheti","06/30/2016","$449,999.00","Emmett Redd, Hava Siegelmann","SteveYounger@missouristate.edu","901 South National","Springfield","MO","658970027","4178365972","ENG","7607","1653","$0.00","The objective of this research is to advance significantly Artificial Intelligence by developing the first ever, brain-like Super-Turing system. <br/>The approach is to build a computational system that is much more powerful (Super) than digital computation.  Alan Turing, a WWII code breaker, developed a conceptual (Turing) machine that has been basic to the digital computation revolution of the last 70 years.  This project will develop a Super-Turing mathematical model and implement it on an Optical Analog Neural Network Computer.  The project will focus on understanding system abilities and differences between Super-Turing and Turing systems.  Natural and artificial time-series will be compared to those of Super-Turing models and modeled Super-Turing processes will be compared with those of the brain. <br/>Intellectual Merit <br/>Super-Turing Computation is a novel, transformative technology that will rewrite computational limits and lay a new foundation for Artificial Intelligence.<br/>Broader Impact<br/>Increasing the capabilities of machine intelligence will profoundly influence society; Intelligent Super-Turing systems are expected to be capable of improving quality of life, e.g. providing intelligent robotic assistants, improving individualized education, creating new industries and medical technologies producing great economic benefit.  Further, a deepened understanding of Super-Turing computation as it relates to living systems is likely to bring new insights into brain function - fundamentally impacting neuroscience and brain-related medical technologies. Graduate and undergraduates will be trained in the interdisciplinary areas of advanced mathematics, physics, and computer science."
"1209714","SoCS: Collaborative Research: A Human Computational Approach for Improving Data Quality in Citizen Science Projects","IIS","SOCIAL-COMPUTATIONAL SYSTEMS","08/01/2012","07/23/2012","Weng-Keen Wong","OR","Oregon State University","Standard Grant","William Bainbridge","07/31/2016","$174,635.00","","wong@eecs.oregonstate.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","CSE","7953","7953","$0.00","A unique interdisciplinary team of computer scientists, information scientists, ornithologists, project managers, and programmers will develop a novel network between machine learning methods and human observational capacity to explore the synergies between mechanical computation and human computation. This is called a Human/Computer Learning Network, and while the focus is to improve data quality in broad-scale citizen-science projects, the network has the potential for wide applicability in a variety of complex problem domains. The core of this network is an active learning feedback loop between machines and humans that dramatically improves the quality of both, and thereby continually improves the effectiveness of the network as a whole. The Human/Computer Learning Network will leverage the contributions of broad recruitment of human observers and process their contributed data with artificial intelligence algorithms leading to a total computational power far exceeding the sum of their individual parts. This work will use the highly successful eBird citizen-science project as a testbed to develop the Human/Computer Learning Network. eBird engages a global network of volunteers who submit tens of millions of bird observations annually to a central database.<br/>This research addresses three fundamental data quality challenges in citizen-science. These are: 1) reducing errors in identification or classification of objects; 2) identifying and quantifying the differences between individual observers; 3) reducing the spatial bias prevalent in many citizen-science projects. To address these challenges, the project will build on advances in artificial intelligence that now provide the opportunity to study systems through the generation of models that can account for enormous complexity. Preliminary work on observer classification will be extended by developing new multi-label machine learning classification algorithms that provide better ecological interpretations and more accurate predictions. In addition, the research will develop new active learning algorithms by constructing sampling paths that will optimize volunteer survey efforts to maximize overall spatial coverage, and incentivize participation via crowdsourcing techniques. Finally, it will study how participants can improve the quality of their observations based on the feedback and information provided by the artificial intelligence. <br/><br/>Broad-scale citizen-science projects can recruit extensive networks of volunteers, who act as intelligent and trainable sensors in the environment to gather observations. Artificial intelligence processes can dramatically improve the quality of the observational data that volunteers can provide by filtering inputs based on observers' expertise, a judgment that is based on aggregated historical data. By guiding the observers with immediate feedback on observation accuracy and customization of observation worksheets, the artificial intelligence processes contribute to advancing expertise of the observers, while simultaneously improving the quality of the training data on which the artificial intelligence processes make their decisions. The results of the project will have significant benefit for all citizen science and broader impact in an emerging world of ubiquitous computing in which human-machine partnerships will become increasingly common."
"1209589","SoCS: Collaborative Research: A Human Computational Approach for Improving Data Quality in Citizen Science Projects","IIS","SOCIAL-COMPUTATIONAL SYSTEMS","08/01/2012","11/21/2012","Steven Kelling","NY","Cornell University","Standard Grant","William Bainbridge","07/31/2015","$575,366.00","Carl Lagoze, Carla Gomes","stk2@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7953","7953","$0.00","A unique interdisciplinary team of computer scientists, information scientists, ornithologists, project managers, and programmers will develop a novel network between machine learning methods and human observational capacity to explore the synergies between mechanical computation and human computation. This is called a Human/Computer Learning Network, and while the focus is to improve data quality in broad-scale citizen-science projects, the network has the potential for wide applicability in a variety of complex problem domains. The core of this network is an active learning feedback loop between machines and humans that dramatically improves the quality of both, and thereby continually improves the effectiveness of the network as a whole. The Human/Computer Learning Network will leverage the contributions of broad recruitment of human observers and process their contributed data with artificial intelligence algorithms leading to a total computational power far exceeding the sum of their individual parts. This work will use the highly successful eBird citizen-science project as a testbed to develop the Human/Computer Learning Network. eBird engages a global network of volunteers who submit tens of millions of bird observations annually to a central database.<br/>This research addresses three fundamental data quality challenges in citizen-science. These are: 1) reducing errors in identification or classification of objects; 2) identifying and quantifying the differences between individual observers; 3) reducing the spatial bias prevalent in many citizen-science projects. To address these challenges, the project will build on advances in artificial intelligence that now provide the opportunity to study systems through the generation of models that can account for enormous complexity. Preliminary work on observer classification will be extended by developing new multi-label machine learning classification algorithms that provide better ecological interpretations and more accurate predictions. In addition, the research will develop new active learning algorithms by constructing sampling paths that will optimize volunteer survey efforts to maximize overall spatial coverage, and incentivize participation via crowdsourcing techniques. Finally, it will study how participants can improve the quality of their observations based on the feedback and information provided by the artificial intelligence. <br/><br/>Broad-scale citizen-science projects can recruit extensive networks of volunteers, who act as intelligent and trainable sensors in the environment to gather observations. Artificial intelligence processes can dramatically improve the quality of the observational data that volunteers can provide by filtering inputs based on observers' expertise, a judgment that is based on aggregated historical data. By guiding the observers with immediate feedback on observation accuracy and customization of observation worksheets, the artificial intelligence processes contribute to advancing expertise of the observers, while simultaneously improving the quality of the training data on which the artificial intelligence processes make their decisions. The results of the project will have significant benefit for all citizen science and broader impact in an emerging world of ubiquitous computing in which human-machine partnerships will become increasingly common."
"1219199","RI: Small: Efficient Learning Algorithms for Modeling Natural Data","IIS","ROBUST INTELLIGENCE","09/15/2012","08/31/2012","Michael DeWeese","CA","University of California-Berkeley","Standard Grant","Kenneth Whang","08/31/2016","$449,999.00","","deweese@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","7495","7923","$0.00","The ability to accurately model such complex phenomena as the natural scene statistics inherent in stacks of photographs or movies, or the collective behavior of hundreds of simultaneously recorded neurons in the cerebral cortex, would be transformative for our understanding of the natural world and of human thought.  The insights gained would not only enhance our understanding of the brain and the sensory stimuli it can process, but they would confer practical advantages as well -- leading to improvements in automated speech recognition and meaningful analysis of real-time video, for example.  The various data needed for these studies is coming online at a rapid pace, but these large and complex data sets defy traditional modeling and analysis techniques.  Unfortunately, the complexity and size of many recently acquired corpora in biology, physics, and engineering domains render them incapable of being fit by powerful mathematical models unless they are constrained by strong and unjustified assumptions about the data.  This, coupled with the general difficulty of developing general purpose machine learning algorithms has driven most contemporary scientists and engineers to focus on algorithms tailored to narrow problem spaces rather than tackling the more general machine learning problem.  Fortunately, some researchers have continued to push for general learning algorithms with capabilities more similar to human intelligence, but they have typically had to rely on ad hoc assumptions or uncontrolled approximations in order to make progress on this daunting problem.  This proposal is to further develop a recently introduced machine learning technique, called Minimum Probability Flow learning, so that it is capable of fitting exceedingly general parametric models to much larger data sets than has ever been possible before.  In addition, this proposal is to develop novel, complimentary methods for sampling efficiently from a model distribution once the parameters have been fit to data, so that the models can be understood and meaningfully compared with one another.  These techniques will be used to study the statistical structure of natural scenes by fitting a new and powerful mathematical model to a database consisting of a large number of photographs.  The program proposed here is highly interdisciplinary, drawing ideas and approaches from physics, engineering, computer science, and systems neuroscience."
"1257700","CAREER: Art and Vision: Scene Layout from Pictorial Cues","IIS","ROBUST INTELLIGENCE","04/01/2012","09/18/2012","Stella Yu","CA","International Computer Science Institute","Continuing grant","Jie Yang","12/31/2013","$151,027.00","","stellayu@berkeley.edu","1947 CENTER ST STE 600","Berkeley","CA","947044115","5106662900","CSE","7495","1045, 1187, 7495, 9218, HPCC","$0.00","<br/><br/>CAREER: Art and Vision: Scene Layout from Pictorial Cues<br/><br/>PI:  Stella (XingXing) Yu     <br/><br/>Institution: Boston College<br/>Artists are the masters of visual perception. Studying art and vision together can provide new solutions to fundamental problems in computer vision. We focus on inferring scene layout from a single image. This problem has been studied since the earliest days of Artificial Intelligence research, resulting in a host of so-called Shape-from-X methods, where X could be shading, perspective, etc.<br/>Unfortunately, each of these methods works under its own assumptions which often do not hold in real images. How these cues interact and integrate remains elusive. Painters constantly use a combination of four techniques: occlusion, perspective, shading, and form to effectively evoke a 3D percept from a 2D picture. Studying their techniques can lend insights into the computation of recovering scene layout from pixel values. The PI proposes to bring artists and vision scientists together to solve the computational problem of scene layout from pictorial cues. This project realizes it in three areas:<br/>education, experiments and computational modeling.<br/><br/>A new interdisciplinary course, Art and Visual Perception, has been developed at Boston College to give a comprehensive cross-examination of how art contributes to the understanding of vision, and how vision contributes to the generation and viewing of art. Students are actively engaged in both art practice and vision experiments.<br/>Learning art and vision together results in a deeper understanding than studying each discipline separately. Students' assignments also result in valuable datasets for vision research.<br/><br/>The computational approach to scene layout from pictorial cues in this project is to group pixels into spatially organized surfaces from a global integration of multiple pictorial cues in a spectral graph-theoretic framework. The goal is to turn artistic rendering knowledge on how these cues interact into a computational reality.<br/>The PI will study geometry (occlusion and perspective), appearance (brightness and color), and form using eye tracking and psychophysics experiments and computational models. These efforts are organized into two phases that progress from inferring the spatial layout from scenes made of planar surfaces (rooms and streets) to scenes made of curved surfaces (landscape and generic scenes).<br/><br/>Intellectual Merit<br/><br/>What is most remarkable about vision is its ability to perceive 3D spatial layout from a single 2D image. The proposed research replicates this ability in computation from a grouping perspective.<br/>Compared to statistical learning approaches, the grouping method is not only generic and thus scales well with the number of scenes, but can also produce a precise organization of surfaces in the scene.<br/>Compared to traditional Shape-from-X approaches, the grouping method examines each pictorial cue in conjunction with others. The integration of these multiple pictorial cues allows them for the first time to become applicable to real images. The PI has developed the essential grouping machinery in spectral graph theory for depth segregation. Compared to most existing formulations on this topic, it has unparalleled conceptual simplicity, computational efficiency, and guaranteed near-global optimality. The proposed research on brightness and color perception, in connection with Shape-from- Shading and surface organization, will help clarify the role of low- level and high-level mechanisms in the long-standing scientific debate between Hering and Helmholtz on color perception.<br/><br/>Broader Impact<br/><br/>This project bridges the gap between art and science not only in research but also in education by developing a new curriculum that traverses the areas of neuroscience, psychology, computer science, and visual arts, by involving students in art practice and scientific experiments, and by providing a forum for artists and scientists to exchange ideas on visual perception. These interdisciplinary efforts befit the liberal arts education tradition at Boston College. This project will not only benefit from the strong Fine Arts department on campus, but also cultivate computer science awareness and outreach to non-technical people, and promote the growth of the young Computer Science department at Boston College.<br/><br/>URL:  http://www.cs.bc.edu/~syu/artvis/<br/>"
"1205195","CI-ADDO-EN: Collaborative Research: 3D Dynamic Multimodal Spontaneous Emotion Corpus for Automated Facial Behavior and Emotion Analysis","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","09/01/2012","06/28/2012","Jeffrey Cohn","PA","University of Pittsburgh","Standard Grant","Ephraim Glinert","08/31/2016","$110,001.00","","jeffcohn@pitt.edu","300 Murdoch Building","Pittsburgh","PA","152603203","4126247400","CSE","7359","7359","$0.00","Emotion is the complex psycho-physiological experience of an individual's state of mind.  It affects every aspect of rational thinking, learning, decision making, and psychomotor ability.  Emotion modeling and recognition is playing an increasingly important role in many research areas, including human computer interaction, robotics, artificial intelligence, and advanced technologies for education and learning.  Current emotion-related research, however, is impeded by a lack of a large spontaneous emotion data corpus.  With few exceptions, emotion databases are limited in terms of size, sensor modalities, labeling, and elicitation methods.  Most rely on posed emotions, which may bear little resemblance to what occurs in the contexts wherein the emotions are really triggered.  In this project the PIs will address these limitations by developing a multimodal and multidimensional corpus of dynamic spontaneous emotion and facial expression data, with labels and feature derivatives, from approximately 200 subjects of different ethnicities and ages, using sensors of different modalities.  To these ends, they will acquire a 6-camera wide-range 3D dynamic imaging system to capture ultra high-resolution facial geometric data and video texture data, which will allow them to examine the fine structure change as well as the precise time course for spontaneous expressions.  Video data will be accompanied by other sensor modalities, including thermal, audio and physiological sensors.  An IR thermal camera will allow real time recording of facial temperature, while an audio sensor will record the voices of both subject and experimenter. The physiological sensor will measure skin conductivity and related physiological signals.  Tools and methods to facilitate and simplify use of the dataset will be provided.  The entire dataset, including metadata and associated software, will be stored in a public depository and made available for research in computer vision, affective computing, human computer interaction, and related fields.<br/><br/>Intellectual Merit <br/>This research will involve construction of a corpus of spontaneous multi-dimensional and multimodal emotion and facial expression data, which is significantly larger than any that currently exist.  To elicit natural and spontaneous emotions from subjects, the PIs will employ five approaches using physical experience, film clips, cold pressor, relived memories tasks, and interview formats.  The database will employ sensors of different modalities including high resolution 2D/3D video cameras, infrared thermal cameras, audio sensors, and physiological sensors.  The video data will be labeled according to a number of categories, including AU labeling and emotion labeling from self-report and perceptual judgments of nave observers.  Comprehensive emotion labeling will include dimensional approaches (e.g., valence, arousal), discrete emotions (e.g., joy, anger, smile controls), anatomic methods (e.g., FACS), and paralinguistic signaling (e.g., back-channeling).  Additional features will be derived from the raw data, including 2D/3D facial feature points, head pose, and audio parameters.<br/><br/>Broader Impact <br/>Project outcomes will immediately benefit researchers in computer vision and emotion modeling and recognition, because the database will allow them to train and validate their facial expression and emotion recognition algorithms.  The new corpus will facilitate the study of multimodal fusion from audio, video, geometric, thermal, and physical responses.  It will contribute to the development of a comprehensive understanding of mechanisms involving human behavior, and will allow enhancements to human computer interaction (e.g., through emotion-sensitive and socially intelligent interfaces), robotics, artificial intelligence, and cognitive science.  The work will likely also significantly impact research in diverse other fields such as psychology, biometrics, medicine/life science, law-enforcement, education, entrainment, and social science."
"1205664","CI-ADDO-EN: Collaborative Research: 3D Dynamic Multimodal Spontaneous Emotion Corpus for Automated Facial Behavior and Emotion Analysis","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","09/01/2012","06/28/2012","Lijun Yin","NY","SUNY at Binghamton","Standard Grant","Ephraim Glinert","08/31/2016","$306,800.00","","lijun@cs.binghamton.edu","4400 VESTAL PKWY E","BINGHAMTON","NY","139026000","6077776136","CSE","7359","7359","$0.00","Emotion is the complex psycho-physiological experience of an individual's state of mind.  It affects every aspect of rational thinking, learning, decision making, and psychomotor ability.  Emotion modeling and recognition is playing an increasingly important role in many research areas, including human computer interaction, robotics, artificial intelligence, and advanced technologies for education and learning.  Current emotion-related research, however, is impeded by a lack of a large spontaneous emotion data corpus.  With few exceptions, emotion databases are limited in terms of size, sensor modalities, labeling, and elicitation methods.  Most rely on posed emotions, which may bear little resemblance to what occurs in the contexts wherein the emotions are really triggered.  In this project the PIs will address these limitations by developing a multimodal and multidimensional corpus of dynamic spontaneous emotion and facial expression data, with labels and feature derivatives, from approximately 200 subjects of different ethnicities and ages, using sensors of different modalities.  To these ends, they will acquire a 6-camera wide-range 3D dynamic imaging system to capture ultra high-resolution facial geometric data and video texture data, which will allow them to examine the fine structure change as well as the precise time course for spontaneous expressions.  Video data will be accompanied by other sensor modalities, including thermal, audio and physiological sensors.  An IR thermal camera will allow real time recording of facial temperature, while an audio sensor will record the voices of both subject and experimenter. The physiological sensor will measure skin conductivity and related physiological signals.  Tools and methods to facilitate and simplify use of the dataset will be provided.  The entire dataset, including metadata and associated software, will be stored in a public depository and made available for research in computer vision, affective computing, human computer interaction, and related fields.<br/><br/>Intellectual Merit <br/>This research will involve construction of a corpus of spontaneous multi-dimensional and multimodal emotion and facial expression data, which is significantly larger than any that currently exist.  To elicit natural and spontaneous emotions from subjects, the PIs will employ five approaches using physical experience, film clips, cold pressor, relived memories tasks, and interview formats.  The database will employ sensors of different modalities including high resolution 2D/3D video cameras, infrared thermal cameras, audio sensors, and physiological sensors.  The video data will be labeled according to a number of categories, including AU labeling and emotion labeling from self-report and perceptual judgments of nave observers.  Comprehensive emotion labeling will include dimensional approaches (e.g., valence, arousal), discrete emotions (e.g., joy, anger, smile controls), anatomic methods (e.g., FACS), and paralinguistic signaling (e.g., back-channeling).  Additional features will be derived from the raw data, including 2D/3D facial feature points, head pose, and audio parameters.<br/><br/>Broader Impact <br/>Project outcomes will immediately benefit researchers in computer vision and emotion modeling and recognition, because the database will allow them to train and validate their facial expression and emotion recognition algorithms.  The new corpus will facilitate the study of multimodal fusion from audio, video, geometric, thermal, and physical responses.  It will contribute to the development of a comprehensive understanding of mechanisms involving human behavior, and will allow enhancements to human computer interaction (e.g., through emotion-sensitive and socially intelligent interfaces), robotics, artificial intelligence, and cognitive science.  The work will likely also significantly impact research in diverse other fields such as psychology, biometrics, medicine/life science, law-enforcement, education, entrainment, and social science."
"1218156","RI: Small: Collaborative Research: Contour-Assisted Visual Inference: Systems, Algorithms, and Applications","IIS","Info Integration & Informatics, Robust Intelligence","09/01/2012","03/26/2013","Haibin Ling","PA","Temple University","Standard Grant","Jie Yang","08/31/2016","$264,928.00","","hling@cs.stonybrook.edu","1801 N. Broad Street","Philadelphia","PA","191226003","2157077547","CSE","7364, 7495","7495, 7923, 9251","$0.00","Occlusion contour (OC) is well known to play important roles in many computer vision tasks. Unlike regular photographs, an OC image removes the effects of illumination, texture, and appearance while preserving important depth edges and silhouette. This project develops a comprehensive framework for acquiring, processing, and utilizing OCs in visual inference tasks. On the sensor front, the research team develops a new Occlusion Contour Camera or OC-Cam. The new OC-Cam extends the multi-flash camera by coupling an array of controllable infrared (IR) LEDs and a visible-IR camera pair.  On the algorithm and application fronts, the research team systematically develops OC-assisted visual inference algorithms. For recognition, the acquired OCs are used as a feature filter to improve category-level object recognition. For tracking, the PIs apply OCs to enhance target representation by filtering out the background and texture edges. Furthermore, the research team investigates the previously under-explored problems of OC-assisted image summarization and privacy protection.<br/><br/>This project can cast deep impact on broad areas of computer vision, artificial intelligence, criminal justices, and robotics, both in research and education. Due to the importance of OCs in human vision, the results can produce a testbed for the study of visual psychology. Furthermore, the OC-Cam is expected to serve as conceptual inspiration for constructing the next-generation surveillance systems. Finally, the captured OC datasets and relevant tools are made available to other researchers, to provide a platform for validating new OC-based computer vision algorithms."
"1218177","RI: Small: Collaborative Research: Contour-Assisted Visual Inference: Systems, Algorithms, and Applications","IIS","Robust Intelligence","09/01/2012","06/17/2013","Jingyi Yu","DE","University of Delaware","Continuing grant","Jie Yang","08/31/2015","$204,431.00","","yu@cis.udel.edu","210 Hullihen Hall","Newark","DE","197160099","3028312136","CSE","7495","7923, 9150","$0.00","Occlusion contour (OC) is well known to play important roles in many computer vision tasks. Unlike regular photographs, an OC image removes the effects of illumination, texture, and appearance while preserving important depth edges and silhouette. This project develops a comprehensive framework for acquiring, processing, and utilizing OCs in visual inference tasks. On the sensor front, the research team develops a new Occlusion Contour Camera or OC-Cam. The new OC-Cam extends the multi-flash camera by coupling an array of controllable infrared (IR) LEDs and a visible-IR camera pair.  On the algorithm and application fronts, the research team systematically develops OC-assisted visual inference algorithms. For recognition, the acquired OCs are used as a feature filter to improve category-level object recognition. For tracking, the PIs apply OCs to enhance target representation by filtering out the background and texture edges. Furthermore, the research team investigates the previously under-explored problems of OC-assisted image summarization and privacy protection.<br/><br/>This project can cast deep impact on broad areas of computer vision, artificial intelligence, criminal justices, and robotics, both in research and education. Due to the importance of OCs in human vision, the results can produce a testbed for the study of visual psychology. Furthermore, the OC-Cam is expected to serve as conceptual inspiration for constructing the next-generation surveillance systems. Finally, the captured OC datasets and relevant tools are made available to other researchers, to provide a platform for validating new OC-based computer vision algorithms."
"1227245","DIP: Using dynamic formative assessment models to enhance learning of the experimental process in biology","IIS","Cyberlearn & Future Learn Tech","10/01/2012","07/13/2015","Eli Meir","MT","SimBiotic Software","Standard Grant","kevin lee","09/30/2016","$1,333,395.00","Eric Klopfer, Joel Abraham, Zhushan Li","emeir@simbio.com","1280 S Third St. W","Missoula","MT","598010000","6173147701","CSE","8020","8045, 8842","$0.00","This project seeks to develop a dynamic formative assessment method for use with virtual labs. The research focuses on how to constrain a virtual lab experience to be amenable to automated feedback on relatively open-ended responses students are generating while still giving students an appropriate exploratory experience. The technology innovation question is how to do that with validity and reliability. The team is focusing on how to use available artificial intelligence technologies to make it possible to provide good feedback, both to learners working in these environments and to their teachers. PIs are adding dynamic formative assessment capabilities to  virtual lab experiences that are already extensively used in undergraduate and high-school biology classes.<br/><br/>This is an automated assessment project, focusing on assessing learner understanding and capabilities in situations where learners are exploring, having, and using ideas as they are learning STEM content and practices. There are several ways one could approach automated assessment for situations where learners are acting in a fairly unconstrained way -- design algorithms that can interpret and make inferences from free text, or find ways to design the environment in such a way that learners can explore, develop, and record as needed for deep learning but where they have a more constrained way of expressing themselves or limitations in what they can do that don't constrain the learning or engagement. This project seeks to find a sweet spot -- a happy medium where learners can explore, try things out, have ideas, refine ideas, and use ideas with significant freedom but just enough constraint for already-existing artificial intelligence algorithms to interpret what learners are doing, why they are doing it, and what they mean to express. Learning how to do this is essential to designing the learning environments of the future.<br/><br/>There is broad acknowledgement that more attention must be given in STEM fields to the teaching of higher-order thinking skills, including experimental design, data interpretation and evidence-based judgment. Timely formative assessment is a crucial component of such learning, but formative assessment is impossible for a teacher to do for a whole class of individuals at the time when it will have the most effect (when students are engaging in or have just finished engaging in such activities) and too labor intensive to be done regularly in large high school and introductory-level college classes. This project is therefore developing techniques for automatically providing immediate formative assessment as students are conducting simulation-based experiments and reasoning about their results. The investigation focuses on helping students learn to conduct experiments and interpret results within the discipline of biology; lessons learned will be applicable across STEM domains at the high-school and college levels."
"1239963","The 2012 Machine Learning Summer School at UC Santa Cruz","IIS","Info Integration & Informatics","07/01/2012","05/16/2012","Manfred Warmuth","CA","University of California-Santa Cruz","Standard Grant","Sylvia Spengler","06/30/2013","$30,000.00","","manfred@cse.ucsc.edu","1156 High Street","Santa Cruz","CA","950641077","8314595278","CSE","7364","7364, 7484, 7556","$0.00","The Machine Learning Summer Schools (http://mlss.cc) were established in 2002 with the aim of bringing together world class speakers from academia, the national labs, and industry to deliver tutorial-style lectures over a two week period. This project supports the two week long Machine Learning Summer School (MLSS) at UC Santa Cruz, CA during July 9-20 2012. <br/><br/>Intellectual Merits: Machine learning has many important applications in science and industry. Modern machine learning uses a mix of insights from different disciplines, most notably artificial intelligence, statistics and optimization - areas that traditionally have not had much overlap. The participants will take part in tutorials given by experts from several different areas of machine learning - an opportunity that many students do not have at their home institutions. There is substantial integration of research and education in this activity. Furthermore, the participants will be able to interact with the tutorial speakers and with other students. <br/><br/>Broader Impact: The Machine Learning Summer School is designed to enable participants with different backgrounds to gain in-depth knowledge of the current state of the art in machine learning. The participants will interact with leading machine learning experts from academia and industry. It contributes to the creation of a diverse cadre of machine learning researchers and practitioners by offering unique training opportunities for undergraduate and graduate students from under-represented groups through personalized mentoring and scholarships."
"1243409","ICML 2012 Workshop on Machine Learning for Clinical Data Analysis","IIS","Info Integration & Informatics","06/15/2012","06/07/2012","Milos Hauskrecht","PA","University of Pittsburgh","Standard Grant","Sylvia Spengler","05/31/2013","$18,000.00","","milos@cs.pitt.edu","300 Murdoch Building","Pittsburgh","PA","152603203","4126247400","CSE","7364","7364, 7556, 8018","$0.00","The wealth and variety of data generated in modern medical and health-care settings present tremendous research challenges as well as opportunities in artificial intelligence and machine learning. Extensive electronic medical records - with thousands of fields recording patient conditions, diagnostic tests, treatments, outcomes, as well as narrative about the patients and care delivery - provide an unprecedented source of information. Tapping into this data source can bring clues leading to improvements in a wide range of health-care applications, such as disease modeling and early detection, chronic disease management, and efficient design of clinical trials.<br/><br/>Intellectual Merit: The Workshop on Machine Learning for Clinical Data Analysis (http://sites.google.com/site/mlclinicaldata) will be held during the International Conference on Machine Learning (ICML), 2012, Edinburgh, Scotland on June 30-July 1, 2012. The workshop aims to bring together machine learning and informatics researchers interested in problems and applications in the clinical domain, with the goal of bridging the gap between the theory of machine learning and the needs of the health informatics applications. The award provides funds to cover the travel costs of invited speakers and graduate students. The Ph.D. student participants will be able to present their work, interact with their peers from other universities as well as hundreds of leading researchers in machine learning from around the world. In addition to attending the workshop, they will attend the technical sessions, plenary talks, and tutorials of their choice at the conference. The  invited speakers  will present talks covering state-of-the-art research as well as open machine learning research challenges in building predictive models from clinical data. The workshop aims to educate the machine learning research community regarding machine learning research opportunities and challenges in health care applications, especially in connection with recent electronic health record initiatives; identify new machine learning problems not previously addressed by the community; and help build a community of researchers who can advance machine learning informed by the challenges and opportunities presented by clinical data analytics. <br/><br/>Broader Impacts. Machine learning is playing an increasingly important role in many emerging data-rich sciences and application domains, such as bioinformatics, computational biology, health informatics, and security informatics. Participation in the workskshop and the ICML and COLT conferences will enrich the education and training of student researchers at early stages in their careers. The travel awards will help broaden the participation of women and members of underrepresented minority groups within the Machine Learning and Health Informatics research communities."
"1149882","CAREER: New Directions for Metric Learning","IIS","Robust Intelligence","03/01/2012","03/02/2015","Kilian Weinberger","MO","Washington University","Continuing grant","Todd Leen","10/31/2015","$478,280.00","","kilianweinberger@cornell.edu","CAMPUS BOX 1054","Saint Louis","MO","631304862","3147474134","CSE","7495","1045, 7495, 9251","$0.00","Quantifying similarity is a fundamental challenge in artificial intelligence and machine learning which - if performed perfectly - would reduce many tasks to a trivial nearest neighbor search. For example, determining whether an email were spam would be as simple as searching a labeled database of emails and assigning it the same label (spam or not) as the email considered most similar to it. But how can one measure the similarity of two email messages? Does the same measurement still apply when comparing medical images? How does our understanding of similarity depend on the problem specification? Metric learning optimizes distance functions specifically for a given task, taking into account both the learning problem and the data. Initial successes with linear metrics show great improvements on many ""k-nearest neighbors""-based learning tasks. <br/><br/>This project pursues four research directions that strengthen the theoretical understanding of metric learning within the research community, broaden its impact and significantly improve the current state-of-the-art: <br/><br/>1. Are there non-linear transformations that lead to equally elegant and efficient optimization problems as existing linear metrics? As data sets grow and become increasingly complex, linear metrics are no longer sufficient to capture similarity relations. By exploring the use of non-linear metrics, this research can substantially improve the impact of metric learning and the accuracy of similarity relations. <br/><br/>2. Can the impact of metric learning be extended to machine learning frameworks beyond nearest neighbors? Designing new metric learning algorithms that explicitly optimize distances for a broad variety of machine learning algorithms will significantly increase the number of applications and learning methods that can directly benefit from metric learning. <br/><br/>3. Can metrics be learned from weak supervision? Removing the dependency on labeled data will reduce the cost of metric learning and increase its applicability. <br/><br/>4. Can one develop a solid theoretical framework to explain preliminary empirical successes and to direct future research? This will strengthen the theoretical understanding of metric learning within the research community. <br/><br/>Successful resolution of the proposed problems will lead to novel learning methods which will be immediately applicable to ongoing high-impact medical research collaborations of the principal investigator. In conjunction with these research directions, the principal investigator will also pursue educational goals, including the co-development of a K-12 curriculum module estimated to impact 2,500 high-school students. Many topics in the proposed research plan have components ideal for introducing the research process to undergraduate and graduate students, and the principal investigator plans to use his research as a vehicle to instruct and inspire future computer scientists and next-generation researchers."
"1241434","UTEP Summer Program in Applied Intelligent Systems","IIS","IIS Special Projects","06/15/2012","06/01/2012","Olac Fuentes","TX","University of Texas at El Paso","Standard Grant","William Bainbridge","05/31/2016","$317,286.00","","ofuentes@utep.edu","ADMIN BLDG RM 209","El Paso","TX","799680001","9157475680","CSE","7484","7484","$0.00","The Computer Science Department at the University of Texas at El Paso (UTEP), in collaboration with the departments of Biological Sciences, Mathematics, Electrical Engineering, and Psychology, offers an interdisciplinary REU summer program in applied intelligent systems. The program hosts eight students for a period of ten weeks during the summer. It emphasizes interdisciplinary research, where techniques developed in the area of artificial intelligence are applied to real-world problems in science and engineering. This will allow the students to gain a broader overview of science and engineering as a potential career than would be possible in more narrowly-defined research areas. <br/><br/>Each student will work on an individual interdisciplinary research project under the co-supervision of a faculty member from the computer science department and a faculty member from one of the collaborating departments. The unifying research theme will be the use of intelligent systems techniques, including machine learning, data mining, optimization, and image analysis, to solve relevant data analysis problems in science and engineering fields. Students will be able to choose from a large list of projects that includes automated analysis of astronomical images, seismic tomography using machine learning, and identifying foreign accents in speech, among many others.<br/><br/>To assist the development of each student's oral communication skills and broaden their views of science and engineering beyond their project, each student will informally present the progress of their research project to all other REU participants, including mentors, the REU Program Director, and to their fellow REU students at the REU weekly meeting. In addition to the weekly meetings, there is a seminar series with the main goals of improving students' research skills and increasing their understanding of potential career paths in computing-related fields. The program will conclude with a one-day symposium to highlight the achievements of the REU students."
"1216467","RI: Small: Reinforcement Learning by Mirror Descent","IIS","Robust Intelligence","08/01/2012","07/16/2012","Sridhar Mahadevan","MA","University of Massachusetts Amherst","Standard Grant","Weng-keen Wong","07/31/2016","$449,991.00","","mahadeva@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","7495","7923","$0.00","A fundamental challenge in machine learning is the design of computational agents that, rather than being explicitly programmed, autonomously learn complex tasks in stochastic real-world environments.  Past approaches, such as reinforcement learning algorithms for solving Markov decision processes, scale poorly to large state spaces. The proposed research addresses this curse of dimensionality by investigating a novel framework combining reinforcement learning and online convex optimization, in particular mirror descent and related algorithms.  Mirror descent scales significantly better than classical first-order gradient descent in high-dimensional state spaces, by using a distance-generating function specific to a particular state space geometry.<br/><br/>The proposed framework enables several significant algorithmic advances in the design of autonomous machine learning agents: a new class of first-order mirror-descent based methods for learning sparse solutions to Markov decision processes will be developed that scale significantly significantly better than previous second-order methods; novel hierarchical methods for solving semi-Markov decision processes will be investigated; and finally, applications to a variety of high-dimensional Markov decision processes will be explored.<br/><br/>The anticipated outcomes of the proposed work include foundational advances in designing autonomous agents that learn to solve sequential decision-making problems, which will impact a large number of target applications from manufacturing to robotics and scheduling. The educational goal includes the development of a graduate-level course in online convex optimization for sequential decision-making, as well as interdisciplinary tutorials to enhance the cross-fertilization of ideas from applied mathematics and optimization to machine learning and artificial intelligence."
"1157121","Neural mechanisms underlying categorical perception in the processing of faces","BCS","Cognitive Neuroscience, EPSCoR Co-Funding","09/01/2012","08/28/2012","Ming Meng","NH","Dartmouth College","Standard Grant","Alumit Ishai","08/31/2016","$400,000.00","","Ming.Meng@Dartmouth.EDU","OFFICE OF SPONSORED PROJECTS","HANOVER","NH","037551421","6036463007","SBE","1699, 9150","1699, 9150","$0.00","Face perception is crucial for normal social interactions, and human observers excel at rapidly and reliably categorizing visual patterns as faces or non-faces. Even subtle impairment in this ability can have devastating consequences, as evidenced by autism and other developmental disorders. Although the neural mechanisms underlying face perception have been a major focus of primate electrophysiology and brain-imaging research, the computational mechanisms underlying how the brain processes faces are still far from clear. With support from the National Science Foundation, Dr. Ming Meng of Dartmouth College is addressing this question by synthesizing techniques drawn from psychophysics, human brain imaging, statistical data mining, and computer vision. First of all, this project is measuring brain activation in response to image sets that have been compiled by using computer vision. These images vary in their image-level facial similarity, ranging from non-faces to genuine faces. Computer vision systems may falsely categorize many of the face-like non-faces as faces. By contrast, categorical perception enables human observers to make unambiguous perceptual judgments on whether these images are faces or non-faces. Second, to understand the face processing neural network, it is more important to understand causal relationships among several brain regions than average response activation of each separate brain region. This project is applying state-of-the-art data mining techniques to provide the basis for dynamic causal modeling of the direct and indirect relationships among various factors, such as, low-level visual features, face semblance and the face/non-face categorization, linking neural processing stages from primary visual analysis to perceptual decision. Finally, to determine bottom-up versus top-down modulation on categorical face perception, sophisticated psychophysical paradigms are being used to investigate potential interactions between visual awareness and the neural correlates of categorical facial analysis. Through this approach, stimulus-driven and feed-forward models of face processing that are assumed to be independent of visual awareness can be compared to models that involve cognitive feedback modulations and visual awareness. <br/><br/>The results of this project are expected to lead to a better understanding of how the human brain processes visual information in the context of face perception, a domain that provides one of the most compelling examples of sensory organization. Understanding the process of how the human visual system analyzes faces may ultimately lead to the successful design of artificial face recognition systems. Moreover, continuous image-level facial similarity analysis versus binary categorical judgment in face perception underlies the crucial social ability to rapidly recognize a face. Teasing the two analyses apart may help characterize the neural pathology in the developmental disorders that involve face perception deficits. Based on the results, a cognitive therapy can be strategically designed to pinpoint such deficits, and to therefore help children and adults with these disorders."
"1216045","RI: Small: Efficient Bayesian Learning from Stochastic Gradients","IIS","Robust Intelligence","09/01/2012","08/30/2012","Max Welling","CA","University of California-Irvine","Standard Grant","Jie Yang","08/31/2016","$500,000.00","Babak Shahbaba","welling@ics.uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","CSE","7495","7923","$0.00","The total volume of data was estimated to be 0.8 Zettabytes in 2009 (1 Zettabyte = 1 trillion gigabytes) and predicted to grow to a staggering 35 Zettabytes in 2020, doubling every two years. Therefore, one of the primary challenges for machine learning is to develop statistically principled methods that will scale up to very large datasets. Moreover, we would like to (efficiently) learn highly complex models without the worry of overfitting and with confidence levels on our predictions. While Bayesian methods satisfy these latter desiderata, the current state-of-the-art inference procedures based on Markov Chain Monte Carlo (MCMC) posterior sampling do not meet the ""big-data"" challenge.<br/><br/>We propose a new family of MCMC procedures that typically requires only a few hundred data-cases per update. These ""stochastic gradient MCMC samplers"" inherit the efficiencies of stochastic approximation methods, but will asymptotically sample from the correct posterior distribution. This endows this family of methods with an ""anytime"" property, namely that one can sample cheaply from a rough approximation of the posterior but can obtain more accurate samples in exchange for more computation.<br/><br/>We believe this new class of methods will for the first time unlock the full strength of Bayesian methods for very large datasets. Due to their highly practical nature, the techniques developed under this grant are likely to gain widespread acceptance across a broad spectrum of academic disciplines as well as in industry. To expedite the transfer process we will publish open source software on our webpages and collaborate with a company (ID Analytics) to work on realistic, large scale inference problems. Two students at the University of California, Irvine (UCI) will be employed on this grant who will collaborate with a number of students and postdocs in the UK (University of Oxford and University of Bristol). UCI and UK students will also be exchanged for a few weeks a year to cross-fertilize research and to gain international experience. Research results from this grant will be integrated into artificial intelligence and machine learning courses at UCI through class projects."
"1203216","Conference on Statistical Learning and Data Mining","DMS","STATISTICS","01/01/2012","12/05/2011","Ji Zhu","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Gabor Szekely","12/31/2012","$25,000.00","","jizhu@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","MPS","1269","7556","$0.00","An international conference on ""Statistical Learning and Data Mining"" will be held June 4-7, 2012 on the Ann Arbor campus of the University of Michigan. The objective is to bring together researchers in statistical learning and data mining from academia, industry, and government in a relaxed and stimulating atmosphere focused on the development of statistical learning theory, methods and applications. The conference will feature three plenary talks by internationally prominent researchers whose work are cutting-edge in the field of statistical learning and data mining. Eighteen invited breakout sessions, each with three talks, will cover additional topics with great interest to the field. These include Computational Advertisement, Function Estimation, High-dimensional Methods, Structured Learning, Graphical Models, Learning Theory, Model Selection, Covariance Estimation, Network Analysis, Computational Biology, Signal and Image Processing and Data Mining Applications. There will also be seven contributed paper sessions and two contributed poster sessions where junior investigators and graduate students are expected to participate.<br/><br/>Statistical learning is a relatively new discipline, evolving from machine learning methods of artificial intelligence and multivariate statistics. The general goals of statistical learning are discovery, classification and prediction, often in very high, effectively infinite, dimensional contexts. The advent of powerful computers with accompanying massive data sets has brought the discipline to the forefront of statistical theory and practice. The major goal of the proposed conference is to present some of the most important recent advances in the field and to discuss future research directions. A major part of the conference focuses on bringing statistical research leaders together with students, postdoctoral fellows, and young academics in a stimulating environment. The funding from the NSF will mainly support graduate students and junior researchers in American universities to attend the conference and present either a talk or a poster. The conference is expected to accelerate interactions and collaborations among researchers in the important area of statistical learning and data mining, and thereby lead to the development of new and more effective methods of modeling and inference."
"1205354","CI-P:Collaborative Research: Visual entailment data set and challenge for the language and vision communities","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","07/01/2012","06/20/2012","Tamara Berg","NY","SUNY at Stony Brook","Standard Grant","Jie Yang","06/30/2014","$57,190.00","","tlberg@cs.unc.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","7359","7359","$0.00","Vision and language provide fundamental means to interpret, learn, and communicate about the world around us. A primary goal of computer vision and natural language processing research is therefore to automatically uncover and analyze the information that images and video, or text and speech, convey about the world.  Both communities are concerned with tasks that require increasingly deeper understanding, including the ability to reason with and draw inferences from this information.  Since vision and language are complementary modalities, there is now also an increasing amount of work at the interface of both fields. However, progress in multimodal analysis requires a tighter collaboration between the two communities, since each currently relies on its own set of techniques, datasets and evaluation criteria. <br/><br/>This community planning grant explores the need for, feasibility, and usefulness of a ""visual entailment"" corpus and associated visual entailment recognition task. In natural language, entailment recognition is the problem of determining whether a particular statement can be inferred from a text document. This project explores a novel related problem - visual entailment - where the goal is to determine whether a statement in natural language can be inferred from an image or video.  The outcomes of the project include a novel dataset and prototype research challenge, as well as increased collaboration between the vision and language communities."
"1205627","CI-P:Collaborative Research: Visual entailment data set and challenge for the Language and Vision Community","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","07/01/2012","06/20/2012","Julia Hockenmaier","IL","University of Illinois at Urbana-Champaign","Standard Grant","Jie Yang","10/31/2014","$41,000.00","","juliahmr@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7359","7359","$0.00","Vision and language provide fundamental means to interpret, learn, and communicate about the world around us. A primary goal of computer vision and natural language processing research is therefore to automatically uncover and analyze the information that images and video, or text and speech, convey about the world.  Both communities are concerned with tasks that require increasingly deeper understanding, including the ability to reason with and draw inferences from this information.  Since vision and language are complementary modalities, there is now also an increasing amount of work at the interface of both fields. However, progress in multimodal analysis requires a tighter collaboration between the two communities, since each currently relies on its own set of techniques, datasets and evaluation criteria. <br/><br/>This community planning grant explores the need for, feasibility, and usefulness of a ""visual entailment"" corpus and associated visual entailment recognition task. In natural language, entailment recognition is the problem of determining whether a particular statement can be inferred from a text document. This project explores a novel related problem - visual entailment - where the goal is to determine whether a statement in natural language can be inferred from an image or video.  The outcomes of the project include a novel dataset and prototype research challenge, as well as increased collaboration between the vision and language communities."
"1247834","Collaborative Research: Design, Analysis and Implementation of Social Interactions in Cognitive Radio Networks","CNS","Special Projects - CNS","10/01/2012","09/12/2012","Husheng Li","TN","University of Tennessee Knoxville","Standard Grant","wenjing lou","09/30/2015","$140,000.00","Chien-fei Chen","husheng@eecs.utk.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","CSE","1714","7976, 9150","$0.00","Cognitive radio is an efficient approach to access frequency spectrum. The performance of cognitive radio networks is substantially determined by the information on spectrum occupancies. Experiments have demonstrated the temporal and spatial correlations of spectrum availability, which is of key importance in the design and analysis of cognitive radio networks. Motivated by the observation, this research studies the social interaction mechanism for secondary users to fully exploit the correlations. A recommendation mechanism is useful for secondary users to share correlated information about the spectrum. Collaborative filtering can enhance the capability of better learning the spectrum situations. For better understand and assist the design, analysis is conducted for the social interaction mechanism, based on powerful tools in social networks, such as master equation, mean filed dynamics and epidemic propagation. Continuum model, such as partial differential equations for diffusions, is also employed as the limit case of discrete cognitive radio networks. Beacon based and packet based mechanisms are used for the recommendation protocols. A 100-node hardware cognitive radio network testbed is built to demonstrate the proposed mechanisms, algorithms and protocols. The research involves aspects of wireless communications, networking, artificial intelligence and physics; thus the inter-disciplinary essence of the research also lends itself to cross-disciplinary education. New courses are devised, which involve the topics of cognitive radio networks, machine learning and image processing. This project also attracts traditionally underrepresented groups, as well as outreach high school students."
"1247778","Collaborative Research: Design, Analysis and Implementation of Social Interactions in Cognitive Radio Networks","CNS","Special Projects - CNS","10/01/2012","09/12/2012","Robert Qiu","TN","Tennessee Technological University","Standard Grant","Monisha Ghosh","09/30/2017","$110,000.00","","rqiu@tntech.edu","Dixie Avenue","Cookeville","TN","385050001","9313723374","CSE","1714","7976, 9150","$0.00","Cognitive radio is an efficient approach to access frequency spectrum. The performance of cognitive radio networks is substantially determined by the information on spectrum occupancies. Experiments have demonstrated the temporal and spatial correlations of spectrum availability, which is of key importance in the design and analysis of cognitive radio networks. Motivated by the observation, this research studies the social interaction mechanism for secondary users to fully exploit the correlations. A recommendation mechanism is useful for secondary users to share correlated information about the spectrum. Collaborative filtering can enhance the capability of better learning the spectrum situations. For better understand and assist the design, analysis is conducted for the social interaction mechanism, based on powerful tools in social networks, such as master equation, mean filed dynamics and epidemic propagation. Continuum model, such as partial differential equations for diffusions, is also employed as the limit case of discrete cognitive radio networks. Beacon based and packet based mechanisms are used for the recommendation protocols. A 100-node hardware cognitive radio network testbed is built to demonstrate the proposed mechanisms, algorithms and protocols. The research involves aspects of wireless communications, networking, artificial intelligence and physics; thus the inter-disciplinary essence of the research also lends itself to cross-disciplinary education. New courses are devised, which involve the topics of cognitive radio networks, machine learning and image processing. This project also attracts traditionally underrepresented groups, as well as outreach high school students."
"1208051","CRCNS: Collaborative Research: Neural Correlates of Hierarchical Reinforcement Learning","IIS","CRCNS","10/01/2012","07/16/2013","Andrew Barto","MA","University of Massachusetts Amherst","Continuing grant","Aude Oliva","09/30/2015","$43,337.00","","barto@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","7327","7327","$0.00","Research on human behavior has long emphasized its hierarchical structure: Simple actions group together into subtask sequences, and these in turn cohere to bring about higher-level goals. This hierarchical structure is critical to humans' unique ability to tackle complex, large-scale tasks, since it allows such tasks to be decomposed or broken down into more manageable parts. While some progress has been made toward understanding the origins and mechanisms of hierarchical behavior, key questions remain: How are task-subtask-action hierarchies initially assembled through learning? How does learning operate within such hierarchies, allowing adaptive hierarchical behavior to take shape? How do the relevant learning and action-selection processes play out in neural hardware? <br/><br/>To pursue these questions, the present proposal will leverage ideas emerging from the computational framework of Hierarchical Reinforcement Learning (HRL). HRL builds on a highly successful machine-learning paradigm known as reinforcement learning (RL), extending it to include task-subtask-action hierarchies. Recent neuroscience and behavioral research has suggested that standard RL mechanisms may be directly relevant to reward-based learning in humans and animals. The present proposal hypothesizes that the mechanisms introduced in computational HRL may be similarly relevant, providing insight into the cognitive and neural underpinnings of hierarchical behavior. <br/><br/>The project brings together two computational cognitive neuroscientists and a computer scientist with expertise in machine learning. The proposed research, which includes both computational modeling and human functional neuroimaging and behavioral studies, pursues a set of hypotheses drawn directly from HRL research. A first set of hypotheses relates to the question of how complex tasks are decomposed into manageable subtasks. Here, fMRI and computational work will leverage the idea, drawn from HRL research, that useful decompositions ""carve"" tasks at points identifiable through graph-theoretic measures of centrality. A second set of hypotheses relates to the question of how learning occurs within hierarchies. Here, fMRI and modeling work will pursue the idea that hierarchical learning may be driven by reward prediction errors akin to those arising within the HRL framework. The project as a whole aims to construct a biologically constrained neural network model, translating computational HRL into an account of how the brain supports hierarchically structured behavior."
"1207833","CRCNS: Collaborative Research: Neural Correlates of Hierarchical Reinforcement Learning","IIS","CRCNS-Computation Neuroscience","10/01/2012","08/07/2013","Matthew Botvinick","NJ","Princeton University","Continuing Grant","aude oliva","09/30/2015","$564,649.00","Yael Niv","matthewb@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","7327","7327","$0.00","Research on human behavior has long emphasized its hierarchical structure:  Simple actions group together into subtask sequences, and these in turn cohere to bring about higher-level goals.  This hierarchical structure is critical to humans' unique ability to tackle complex, large-scale tasks, since it allows such tasks to be decomposed or broken down into more manageable parts. While some progress has been made toward understanding the origins and mechanisms of hierarchical behavior, key questions remain: How are task-subtask-action hierarchies initially assembled through learning?  How does learning operate within such hierarchies, allowing adaptive hierarchical behavior to take shape?  How do the relevant learning and action-selection processes play out in neural hardware?  <br/><br/>To pursue these questions, the present proposal will leverage ideas emerging from the computational framework of Hierarchical Reinforcement Learning (HRL). HRL builds on a highly successful machine-learning paradigm known as reinforcement learning (RL), extending it to include task-subtask-action hierarchies. Recent neuroscience and behavioral research has suggested that standard RL mechanisms may be directly relevant to reward-based learning in humans and animals. The present proposal hypothesizes that the mechanisms introduced in computational HRL may be similarly relevant, providing insight into the cognitive and neural underpinnings of hierarchical behavior. <br/><br/>The project brings together two computational cognitive neuroscientists and a computer scientist with expertise in machine learning.  The proposed research, which includes both computational modeling and human functional neuroimaging and behavioral studies, pursues a set of hypotheses drawn directly from HRL research.  A first set of hypotheses relates to the question of how complex tasks are decomposed into manageable subtasks. Here, fMRI and computational work will leverage the idea, drawn from HRL research, that useful decompositions ""carve"" tasks at points identifiable through graph-theoretic measures of centrality.  A second set of hypotheses relates to the question of how learning occurs within hierarchies.  Here, fMRI and modeling work will pursue the idea that hierarchical learning may be driven by reward prediction errors akin to those arising within the HRL framework.  The project as a whole aims to construct a biologically constrained neural network model, translating computational HRL into an account of how the brain supports hierarchically structured behavior."
"1240450","USA-Sino Summer School in Vision, Learning, Pattern Recognition, VLPR 2012","IIS","Robust Intelligence","06/01/2012","05/23/2012","Yanxi Liu","PA","Pennsylvania State Univ University Park","Standard Grant","Jie Yang","05/31/2013","$50,000.00","","yanxi@cse.psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","7495","7495, 7556","$0.00","This project supports US faculty members and graduate students to attend USA - Sino Summer School in Vision, Learning, Pattern Recognition (VLPR2012). The VLPR summer school has become a mutually beneficial annual event for computer vision researchers and students from both US and China. The impact and reputation of VLPR summer school sequence grows rapidly and VLPR 2012 continues this positive trend by focusing on Computer Vision in Biomedical Image Applications: from Micro to Macro. This topic complements the methodology-centered VLPRs in the past with an application-oriented and human-centered focus on life forms in drastically different scales: from human and animals (macro) to cells (micro). VLPR 2012 aims to illustrate a cross-sectional state of the art computer vision and machine learning methodologies, to broaden the horizon for innovations and creativities, and to address some fundamental issues of high throughput computing on multi-modality, high-dimensional, and high volume image data (Big Data). <br/><br/>The theme of this summer school reflects the growing international trend of interdisciplinary research in biomedical image and smart health, and facilitates collaborative efforts between science and technology, computer science and medicine/biology, and US and China. In addition, this summer school offers a great opportunity for intellectual and cultural exchanges between a group of world-class computer vision researchers and motivated students from the two countries to converse and mingle with each other in an exciting, culturally rich environment during a week-long period."
"1214590","SBIR Phase I: Hybrid Human-Machine Vision System for Convenient and Accurate Image Annotation","IIP","SMALL BUSINESS PHASE I","07/01/2012","09/26/2012","Boris Babenko","CA","Anchovi Labs, Inc","Standard Grant","Glenn H. Larsen","12/31/2012","$0.00","","bbabenko@gmail.com","2420 Sand Hill Road, Suite 300","Menlo Park","CA","940250000","9494636033","ENG","5371","5371, 8033, 8040","$0.00","This Small Business Innovation Research Phase I project aims to investigate the viability of a system for image annotation via a combination of machine vision and crowd-sourcing. Annotating a large body of images quickly, accurately and inexpensively would be a valuable capability in scientific, medical and other commercial applications. Examples include: medical diagnosis (e.g. trace the vasculature in a slice of cancerous tissue), neuroscience (e.g. count and locate all cells marked with a given antibody), and urban planning (e.g. count all houses that have private pools in a neighborhood). Machine vision is making progress towards delivering such capabilities. However, accuracy is not yet sufficient for many applications. In recent years a complementary solution has become available: crowdsourcing, that is, dynamically recruiting thousands of people to carry out an assigned task from their home computer. Our research (at Caltech and UC San Diego) suggests that it is possible to combine the complementary strengths of human annotators and machines into a hybrid system that is flexible, accurate, fast and inexpensive. We will build a prototype to test this hypothesis.<br/><br/>The broader impact/commercial potential of this project affects a wide array of domains and applications. As imaging becomes more available and storage inexpensive, the amount of image data will continue to increase. This is true for the scientific research market (e.g. the advent of high-throughput microscopes and automated telescopes), the geospatial information systems market (e.g. the advent of commercially operated satellites with on-board imaging systems), and the consumer market (e.g. the advent of smart phones with cameras). The proposed venture will fill the need to scale annotation and analysis of this data, and keeping this process as inexpensive and fast as possible. By combining computer vision and machine learning automation with humans, both experts and non-expert annotators, our system will be quickly configurable and trainable by the users to address virtually any image analysis challenge. Easy, accurate and inexpensive image analysis will revolutionize many areas of scientific research and commerce."
"1218492","SHF Small: Hierarchical Unsupervised Inference Using Robust Neuromorphic Computation","CCF","Software & Hardware Foundation","10/01/2012","09/14/2012","Jeremiah Holleman","TN","University of Tennessee Knoxville","Standard Grant","Sankar Basu","09/30/2016","$250,000.00","Itamar Arel","jhollem3@uncc.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","CSE","7798","7923, 7945, 9150","$0.00","The human brain contains roughly 100 billion neurons. Each operates at approximately 150 Hz, far slower than modern digital processors, suggesting that the brain's computational strength stems from its massively parallel architecture rather than sheer processing speed. Deep machine learning (DML) has recently emerged as a promising framework for mimicking the information representation capabilities of the brain. Inspired by discoveries in neurobiology, hidden layers of deep learning systems encode hierarchically distributed representations of complex inputs. However, the fundamental mismatch between a highly parallel architecture and the serial structure of conventional processors limits the scalability of software-based DML systems. By fully leveraging the computational power of individual transistors, analog neuromorphic circuits achieve much greater density and energy efficiency than digital technology. The goal of this research is to use neuromorphic analog computational elements to enable scalable deep learning systems.<br/><br/>The proposed research offers the potential to revolutionize the design and utilization of large-scale deep machine learning systems with applications to real-world, complex, high-dimensional pattern recognition problems. In particular, the study of floating-gate circuitry for realizing such systems is expected to have broad impact on the field of neuromorphic engineering. The long-range impact of compact, power-efficient learning systems will be profound, with benefits to society ranging from micro-scale health-monitoring sensors capable of learning from their own observations to large-scale autonomous processing of multimedia data. The interdisciplinary nature of the project will provide an opportunity for graduate and undergraduate students to work at the intersection of different specializations and provide a unique platform on which to study the emergent properties of complex biologically-inspired systems."
"1201170","Deontic Modeling and Natural Language Processing for Automated Environmental and Green Compliance Checking","CMMI","CIVIL INFRASTRUCTURE SYSTEMS","06/01/2012","02/25/2012","Nora El-Gohary","IL","University of Illinois at Urbana-Champaign","Standard Grant","yanfeng ouyang","05/31/2016","$320,000.00","","gohary@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","ENG","1631","029E, 036E, 039E, 9102","$0.00","This grant provides funding for the development of a computational approach for checking the compliance of textual construction plans (such as storm-water pollution prevention plans) with environmental regulations and contractual requirements. This is intended to automatically detect non-compliance instances and provide a relatively rich analysis of the non-compliance such as which regulation was violated, the reason for the violation, the possible consequences of the violation, etc. To achieve such deep levels of text processing and automated reasoning, three algorithms will be developed and combined into one computational platform: 1) a machine learning-based algorithm for classifying text, 2) a hybrid syntactic-semantic (that utilizes grammatical and meaning-descriptive features of the text) algorithm for information extraction, and 3) a logic-based algorithm for compliance reasoning. The automated analysis will be facilitated by a knowledge model of normative reasoning (deontic model) in the construction domain. The algorithms will be implemented in a proof of concept prototype software. Real life-based test case scenarios will be formulated - in collaboration with construction contractors - and used to test and validate the algorithms developed.<br/><br/>If successful, the results of this research will lead to improvements in the area of automated regulatory and contractual compliance checking in construction. The long-term goal of this research is to provide construction industry professionals and regulators with an approach that can help reduce compliance assessment errors, as well as reduce the cost and time associated with compliance checking. The proposed work will also contribute to the computational tools and methodologies available for both deep natural language understanding/processing and semantic reasoning. This project will also involve educational and outreach activities to promote teaching and learning, engage undergraduate and graduate students, and reach out to female high-school students. This includes the development of a project website, course modules, and hands-on outreach material about environmentally-conscious construction."
"1212798","RI: Large: Collaborative Research: Reconstructive recognition: Uniting statistical scene understanding and physics-based visual reasoning","IIS","Robust Intelligence","10/01/2012","09/12/2012","Trevor Darrell","CA","University of California-Berkeley","Standard Grant","Jie Yang","09/30/2018","$818,181.00","Jitendra Malik","trevor@eecs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","7495","7925","$0.00","This project is creating a novel paradigm for computer vision, termed ""reconstructive recognition"", that incorporates the strongest elements of previous machine learning-based recognition efforts and the strongest elements of previous reconstruction efforts based on radiometric reasoning. The goal is to provide a new foundation for machine perception, and the potential for a transformative advance in applications of computer vision. The project seeks novel physics-based methods for recognition as well as novel learning-based methods for interpreting pixel values in terms of the physics of a scene. The agenda is structured around four aims: Aim I develops generalized reconstructive processes that unify the recovery of shape, materials, motion and illumination. Aim II focuses on supervised visual learning methods that exploit such reconstructive image representations. Aim III pursues unsupervised discovery of reconstructive representations that converge to be similar to the engineered models of Aim I. Finally, Aim IV introduces well-defined challenge problems that focus the field and serve as measurable proxies for progress in computer vision applications that have high potential impact on society. <br/><br/>There is a significant broader impact to this project, not least being the improvement in computer vision pedagogy that ensues from a reunification of the currently divergent recognition and reconstruction views of the field. More broadly, this project pursues critical steps toward a future where machines can see, a future that will bring changes to robotics, human-computer interfaces, security, and autonomous navigation, to name a few."
"1212948","RI: Large: Collaborative Research: Reconstructive recognition: Uniting statistical scene understanding and physics-based visual reasoning","IIS","Robust Intelligence","10/01/2012","01/08/2014","Marshall Tappen","FL","The University of Central Florida Board of Trustees","Standard Grant","Jie Yang","09/30/2018","$545,454.00","Hassan Foroosh","mtappen@gmail.com","4000 CNTRL FLORIDA BLVD","Orlando","FL","328168005","4078230387","CSE","7495","7925","$0.00","This project is creating a novel paradigm for computer vision, termed ""reconstructive recognition"", that incorporates the strongest elements of previous machine learning-based recognition efforts and the strongest elements of previous reconstruction efforts based on radiometric reasoning. The goal is to provide a new foundation for machine perception, and the potential for a transformative advance in applications of computer vision. The project seeks novel physics-based methods for recognition as well as novel learning-based methods for interpreting pixel values in terms of the physics of a scene. The agenda is structured around four aims: Aim I develops generalized reconstructive processes that unify the recovery of shape, materials, motion and illumination. Aim II focuses on supervised visual learning methods that exploit such reconstructive image representations. Aim III pursues unsupervised discovery of reconstructive representations that converge to be similar to the engineered models of Aim I. Finally, Aim IV introduces well-defined challenge problems that focus the field and serve as measurable proxies for progress in computer vision applications that have high potential impact on society. <br/><br/>There is a significant broader impact to this project, not least being the improvement in computer vision pedagogy that ensues from a reunification of the currently divergent recognition and reconstruction views of the field. More broadly, this project pursues critical steps toward a future where machines can see, a future that will bring changes to robotics, human-computer interfaces, security, and autonomous navigation, to name a few."
"1212928","RI: Large: Collaborative Research: Reconstructive recognition: Uniting statistical scene understanding and physics-based visual reasoning","IIS","Robust Intelligence","10/01/2012","05/05/2017","Todd Zickler","MA","Harvard University","Standard Grant","Jie Yang","09/30/2018","$1,090,910.00","Kate Saenko","zickler@eecs.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","7495","7925","$0.00","This project is creating a novel paradigm for computer vision, termed ""reconstructive recognition"", that incorporates the strongest elements of previous machine learning-based recognition efforts and the strongest elements of previous reconstruction efforts based on radiometric reasoning. The goal is to provide a new foundation for machine perception, and the potential for a transformative advance in applications of computer vision. The project seeks novel physics-based methods for recognition as well as novel learning-based methods for interpreting pixel values in terms of the physics of a scene. The agenda is structured around four aims: Aim I develops generalized reconstructive processes that unify the recovery of shape, materials, motion and illumination. Aim II focuses on supervised visual learning methods that exploit such reconstructive image representations. Aim III pursues unsupervised discovery of reconstructive representations that converge to be similar to the engineered models of Aim I. Finally, Aim IV introduces well-defined challenge problems that focus the field and serve as measurable proxies for progress in computer vision applications that have high potential impact on society. <br/><br/>There is a significant broader impact to this project, not least being the improvement in computer vision pedagogy that ensues from a reunification of the currently divergent recognition and reconstruction views of the field. More broadly, this project pursues critical steps toward a future where machines can see, a future that will bring changes to robotics, human-computer interfaces, security, and autonomous navigation, to name a few."
"1212849","RI: Large: Collaborative Research: Reconstructive recognition: Uniting statistical scene understanding and physics-based visual reasoning","IIS","ROBUST INTELLIGENCE","10/01/2012","09/12/2012","William Freeman","MA","Massachusetts Institute of Technology","Standard Grant","Jie Yang","09/30/2017","$545,454.00","","wtf@ai.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7495","7925","$0.00","This project is creating a novel paradigm for computer vision, termed ""reconstructive recognition"", that incorporates the strongest elements of previous machine learning-based recognition efforts and the strongest elements of previous reconstruction efforts based on radiometric reasoning. The goal is to provide a new foundation for machine perception, and the potential for a transformative advance in applications of computer vision. The project seeks novel physics-based methods for recognition as well as novel learning-based methods for interpreting pixel values in terms of the physics of a scene. The agenda is structured around four aims: Aim I develops generalized reconstructive processes that unify the recovery of shape, materials, motion and illumination. Aim II focuses on supervised visual learning methods that exploit such reconstructive image representations. Aim III pursues unsupervised discovery of reconstructive representations that converge to be similar to the engineered models of Aim I. Finally, Aim IV introduces well-defined challenge problems that focus the field and serve as measurable proxies for progress in computer vision applications that have high potential impact on society. <br/><br/>There is a significant broader impact to this project, not least being the improvement in computer vision pedagogy that ensues from a reunification of the currently divergent recognition and reconstruction views of the field. More broadly, this project pursues critical steps toward a future where machines can see, a future that will bring changes to robotics, human-computer interfaces, security, and autonomous navigation, to name a few."
"1207592","US-German Collaboration: Unravel CNS regeneration - From Fact Extraction to Experiment Design","IIS","Information Technology Researc, Info Integration & Informatics, Robust Intelligence","12/01/2012","09/06/2012","Lawrence Hunter","CO","University of Colorado at Denver","Standard Grant","Kenneth Whang","11/30/2017","$487,862.00","","Larry.Hunter@ucdenver.edu","MS F428, AMC Bldg 500","Aurora","CO","800452570","3037240090","CSE","1640, 7364, 7495","7327","$0.00","This research is motivated by the problem of spinal cord injuries, with the goal of producing data that can lead to pharmacological or other interventions to make recovery possible.  It is hypothesized that major changes after a spinal cord injury occur on the protein level, and that a model of protein changes and interactions between proteins will make it possible to elucidate previously unnoticed relationships between the participating proteins and protein pathways.  To do this, a computational neuroscience approach is a necessary addition to laboratory experiments.  This analysis requires a considerable amount of background knowledge; to acquire this knowledge, natural language processing (text mining) is necessary; specifically, new techniques for natural language processing need to be developed in the neuroscience domain.<br/><br/>The research plan involves a series of proteomics experiments to be carried out on rats with induced spinal cord injuries and the interpretation of these results using a computational systems biology approach.  The quality of knowledge-based computational analysis depends critically on the breadth of formally represented knowledge in the program.  A major challenge is that much of the requisite knowledge is ""buried"" in scientific publications, rather than being available in computable form in databases.  Therefore, novel text mining techniques will be developed, tailored for the neuroscience domain, to extract such knowledge from scientific publications and convert it into a computable form.  A generic computational analytical tool, known as Hanalyzer, is already available but needs to be adapted to the specific problem; the challenge is to develop the natural language processing technology.  A neuroscience-specific aspect of this challenge is that there is a high diversity in the surface forms of words that are used to refer to spinal cord regeneration, making machine-learning-based approaches susceptible to data sparsity and rule-based approaches vulnerable to an intractable number of keywords that must be accounted for.  A distributional approach will be used to approach this challenge, with novel techniques that make use of semantic role labeling, recognition of ontological concepts, and dependency parsing to learn the surface forms that correspond to abstract or implicit concepts like spinal cord regeneration.<br/><br/>This project is a collaboration involving investigators in Denver, Colorado and in Duesseldorf, Germany.  A companion project is being funded by the German Ministry of Education and Research (BMBF)."
"1216839","I-Corps:  Combining Machine Vision and Crowdsourcing for Convenient and Accurate Image Annotation","IIP","I-Corps","03/01/2012","02/15/2012","Pietro Perona","CA","California Institute of Technology","Standard Grant","Errol Arkilic","08/31/2012","$50,000.00","","perona@caltech.edu","1200 E California Blvd","PASADENA","CA","911250600","6263956219","ENG","8023","","$0.00","Annotating a large body of images quickly, accurately and inexpensively would be a valuable capability in scientific, medical and other commercial applications.  Machine vision is making progress in these arenas.  However, accuracy is not yet sufficient for many applications.  In recent years, a complementary solution has become available: crowdsourcing, that is, dynamically recruiting thousands of people to carry out an assigned task from their computer.  The team's research suggests that it is possible to combine the complementary strengths of human annotators and machines into a hybrid system that is flexible, accurate, fast and inexpensive.  To demonstrate effectiveness and potential commercial opportunity, the team will develop a prototype and a business model around this approach.<br/><br/>As imaging becomes more available and storage inexpensive, the amount of image data will continue to increase.  This is true for the scientific, research, geospatial information systems and consumer markets.  The proposed effort will address the need to scale annotation and analysis of this data while keeping the process as inexpensive and fast as possible with today's computational power.  By combining computer vision and machine learning automations with humans (both experts and non-expert annotators), the system promises to be quickly configurable and trainable across virtually any image analysis challenge."
"1143463","SBIR Phase I: Use of Machine Learning Techniques for Robust Crop and Weed Detection in Agricultural Fields","IIP","SMALL BUSINESS PHASE I","01/01/2012","11/18/2011","Lee Redden","CA","Blue River Technology Inc","Standard Grant","Muralidharan S. Nair","06/30/2012","$150,000.00","","leeredden@gmail.com","575 N Pastoria Ave","Sunnyvale","CA","940852916","4087187457","ENG","5371","5371, 6840, 8035, 9139, HPCC","$0.00","This Small Business Innovation Research (SBIR) Phase I project seeks to understand the fundamental visual cues and characteristics of plants found in agricultural facilities for the purpose of rapid automated identification of plant species. The human eye, coupled with the brain?s processing power , can readily distinguish between different plant species. This capability was one of the basic needs for humans to become an agrarian society (farming requires weeding), which helped start enormous social advancement. Similarly, to bring automated systems to the next generation of capability, computer vision must interact with the natural world with greater fidelity. Today?s computer vision has ability to detect a ?splotch? of vegetation versus no vegetation. This project will advance computer vision by developing the equipment and software algorithms necessary to automatically distinguish plant types. The project team will build a computer vision algorithm based on a field customized support vector machine (SVM) that can automatically and reliably identify a known crop versus a foreign plant (i.e. weed) for use in a larger system for automated weeding. By creating the ability for computers to distinguish between plant types, we will enable food to be grown with reduced amounts of chemical herbicides.<br/><br/>The broader impact/ commercial potential of this project is to increase the competitiveness of vegetable farms, particularly organic ones, while improving human health and the environment. Today, organic farms represent 5% of the U.S. agricultural economy and are growing at a pace to double organic acreage every 4 years. A key feature of organic farming is the lack of herbicides. Consequently, organic farms are normally weeded by hand. Weed control represents approximately 50% of operating costs for organic farms, compared to less than 10% for conventional ones. With an estimated $700M spent annually on weeding organic farms, there is a substantial commercial opportunity to create a system that can weed farms automatically. This project will develop a system that uses a computer system towed behind a tractor to automatically detect and eliminate weeds at early plant stages. The system can be developed and deployed at less than 1/5 the life-cycle costs of hand weeding. The technology is also applicable to conventional crop thinning where it can significantly reduce the amount of herbicides used. Additionally this technology has a profound health and sustainability benefits by eliminating human exposure to chemical herbicides through food and avoids herbicides leaching into the soil."
"1250350","EAGER: Nonparametric Machine Learning on Sets, Functions, and Distributions","IIS","Info Integration & Informatics","09/01/2012","08/18/2012","Barnabas Poczos","PA","Carnegie-Mellon University","Standard Grant","Sylvia Spengler","08/31/2014","$200,000.00","Artur Dubrawski","bapoczos@andrew.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7364","7364, 7916","$0.00","Most  machine learning algorithms operate on fixed dimensional feature vector representations. In many applications, however, the natural representation of the data consists of more complex objects, for example functions, distributions, and sets, rather than finite-dimensional vectors. This project aims to develop a new family of machine learning algorithms that can operate directly on these complex objects. The key innovation is efficient estimation of certain information theoretic quantities for learning predictive models from complex data. The research is organized around three specific aims: (a) Development and analysis of nonparametric estimators for certain important functionals of densities, such as entropy, mutual information, conditional mutual information, and divergence; and study of the theoretical properties of these estimators including consistency, convergence rates of the bias and variance, and asymptotic normality. (b) Use of the preceding estimators to design new learning algorithms for clustering, classification, regression, and anomaly detection that work directly on sets, functions, and distributions without any additional, hand-made feature extraction, histogram creation, or density estimation steps that could lead to loss of information. (c) Study of the theoretical properties of these new machine learning algorithms (computation time, sample complexity, generalization error) and empirical evaluation of the algirithms them to a variety of important real-world problems, including nuclear detection astronomical data analysis, and computer vision in collaboration with researchers at Lawrence Livermore, University of Washington and Johns Hopkins University, and Carnegie Mellon University respectively.<br/><br/>Broader Impact. The project, if successful, could substantially advance the current state-of-the-art in building predictive models from complex data. The results of research, including publications and open source software, will be freely disseminated to the larger scientific community. The project provides enhanced research-based training opportunities for graduate and undergraduate students at Carnegie Mellon University as well as the collaborating institutions."
"1161645","CGV: Medium: Collaborative Research: Understanding Translucency: Physics, Perception, and Computation","IIS","GRAPHICS & VISUALIZATION","09/01/2012","07/24/2015","Kavita Bala","NY","Cornell University","Continuing grant","Ephraim P. Glinert","08/31/2016","$398,263.00","","kb@cs.cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7453","7453, 7924","$0.00","People care greatly about the appearance of translucent materials such as food, skin, soap, and marble, and they are able to distinguish subtle differences in these materials based on their appearance.  The translucent appearance of these materials is caused by internal volumetric scattering, which is challenging to simulate, especially because humans are so sensitive to their subtleties.  Since in the natural world scattering materials are the norm, not the exception, it makes sense that the human visual system is so well engineered to analyze them.  However, very little is known about how this analysis is achieved because the perception of volumetric translucency is almost unstudied.<br/><br/>This collaborative project, involving faculty from three universities with complementary expertise in computer graphics, human vision, machine learning, and computer vision, addresses the fundamental unsolved problem of understanding translucency for graphics.  The PIs will develop a perceptually-motivated pipeline for translucency, contributing new scattering representations, perceptual dimensions, and computational algorithms to computer graphics.  The scattering representations, based on a polydispersion model, will provide analytic expressions for wavelength-dependent bulk scattering properties of translucent media; this will significantly expand the range of materials that can be simulated with high visual fidelity.  Finding perceptual knobs that relate physical scattering parameters with visual appearance will be achieved by coupling large-scale computation (using cloud computing) with controlled perceptual studies.  Novel acquisition approaches that employ hyperspectral imaging will be created, as will editing and rendering applications that use the new perceptual representations of translucency.  Low-dimensional models to represent scattering media will be developed and used to enable efficient and accurate acquisition and rendering.  A suite of test materials and scenes will be developed to evaluate the fidelity of rendered images based on the developed theory and computational applications.<br/><br/>Broader Impacts:   Currently, the simulation of translucency presents challenges in terms of both computation and visual fidelity.  This restricts the ability of practical algorithms to predictively simulate translucent materials, thus fundamentally limiting the use of graphics in real applications. By building the computational tools to characterize, study, and use knowledge of translucency perception, this research will fundamentally change the graphics pipeline for translucent materials. and will potentially revolutionizing industrial design, interior design, skin care and cosmetics, and entertainment.<br/><br/>The project includes an education program that is tightly coupled to the research program.  The PIs have already been meeting twice a week for more than six months, and their graduate students already share data, code, and equipment.  During the activity, the students will make week-long and month-long visits to each other's laboratories to collaborate, and in this way the project will produce a generation of researchers who are ""T-shaped"" in the sense of being both deep in their respective fields and able to work effectively across these synergistic disciplines.  The PIs also plan to organize a workshop that will brins together researchers in vision science, computer graphics, and computer vision, so that the important ties between these fields are strengthened even further."
"1161564","CGV: Medium: Collaborative Research: Understanding Translucency: Physics, Perception, and Computation","IIS","GRAPHICS & VISUALIZATION","09/01/2012","08/29/2012","Todd Zickler","MA","Harvard University","Standard Grant","Ephraim P. Glinert","08/31/2016","$389,660.00","","zickler@eecs.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","7453","7453, 7924","$0.00","People care greatly about the appearance of translucent materials such as food, skin, soap, and marble, and they are able to distinguish subtle differences in these materials based on their appearance.  The translucent appearance of these materials is caused by internal volumetric scattering, which is challenging to simulate, especially because humans are so sensitive to their subtleties.  Since in the natural world scattering materials are the norm, not the exception, it makes sense that the human visual system is so well engineered to analyze them.  However, very little is known about how this analysis is achieved because the perception of volumetric translucency is almost unstudied.<br/><br/>This collaborative project, involving faculty from three universities with complementary expertise in computer graphics, human vision, machine learning, and computer vision, addresses the fundamental unsolved problem of understanding translucency for graphics.  The PIs will develop a perceptually-motivated pipeline for translucency, contributing new scattering representations, perceptual dimensions, and computational algorithms to computer graphics.  The scattering representations, based on a polydispersion model, will provide analytic expressions for wavelength-dependent bulk scattering properties of translucent media; this will significantly expand the range of materials that can be simulated with high visual fidelity.  Finding perceptual knobs that relate physical scattering parameters with visual appearance will be achieved by coupling large-scale computation (using cloud computing) with controlled perceptual studies.  Novel acquisition approaches that employ hyperspectral imaging will be created, as will editing and rendering applications that use the new perceptual representations of translucency.  Low-dimensional models to represent scattering media will be developed and used to enable efficient and accurate acquisition and rendering.  A suite of test materials and scenes will be developed to evaluate the fidelity of rendered images based on the developed theory and computational applications.<br/><br/>Broader Impacts:   Currently, the simulation of translucency presents challenges in terms of both computation and visual fidelity.  This restricts the ability of practical algorithms to predictively simulate translucent materials, thus fundamentally limiting the use of graphics in real applications. By building the computational tools to characterize, study, and use knowledge of translucency perception, this research will fundamentally change the graphics pipeline for translucent materials. and will potentially revolutionizing industrial design, interior design, skin care and cosmetics, and entertainment.<br/><br/>The project includes an education program that is tightly coupled to the research program.  The PIs have already been meeting twice a week for more than six months, and their graduate students already share data, code, and equipment.  During the activity, the students will make week-long and month-long visits to each other's laboratories to collaborate, and in this way the project will produce a generation of researchers who are ""T-shaped"" in the sense of being both deep in their respective fields and able to work effectively across these synergistic disciplines.  The PIs also plan to organize a workshop that will brins together researchers in vision science, computer graphics, and computer vision, so that the important ties between these fields are strengthened even further."
"1161731","CGV: Medium: Collaborative Research: Understanding Translucency: Physics, Perception, and Computation","IIS","GRAPHICS & VISUALIZATION","09/01/2012","08/29/2012","Edward Adelson","MA","Massachusetts Institute of Technology","Standard Grant","Ephraim Glinert","08/31/2018","$395,000.00","","adelson@csail.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7453","7453, 7924","$0.00","People care greatly about the appearance of translucent materials such as food, skin, soap, and marble, and they are able to distinguish subtle differences in these materials based on their appearance.  The translucent appearance of these materials is caused by internal volumetric scattering, which is challenging to simulate, especially because humans are so sensitive to their subtleties.  Since in the natural world scattering materials are the norm, not the exception, it makes sense that the human visual system is so well engineered to analyze them.  However, very little is known about how this analysis is achieved because the perception of volumetric translucency is almost unstudied.<br/><br/>This collaborative project, involving faculty from three universities with complementary expertise in computer graphics, human vision, machine learning, and computer vision, addresses the fundamental unsolved problem of understanding translucency for graphics.  The PIs will develop a perceptually-motivated pipeline for translucency, contributing new scattering representations, perceptual dimensions, and computational algorithms to computer graphics.  The scattering representations, based on a polydispersion model, will provide analytic expressions for wavelength-dependent bulk scattering properties of translucent media; this will significantly expand the range of materials that can be simulated with high visual fidelity.  Finding perceptual knobs that relate physical scattering parameters with visual appearance will be achieved by coupling large-scale computation (using cloud computing) with controlled perceptual studies.  Novel acquisition approaches that employ hyperspectral imaging will be created, as will editing and rendering applications that use the new perceptual representations of translucency.  Low-dimensional models to represent scattering media will be developed and used to enable efficient and accurate acquisition and rendering.  A suite of test materials and scenes will be developed to evaluate the fidelity of rendered images based on the developed theory and computational applications.<br/><br/>Broader Impacts:   Currently, the simulation of translucency presents challenges in terms of both computation and visual fidelity.  This restricts the ability of practical algorithms to predictively simulate translucent materials, thus fundamentally limiting the use of graphics in real applications. By building the computational tools to characterize, study, and use knowledge of translucency perception, this research will fundamentally change the graphics pipeline for translucent materials. and will potentially revolutionizing industrial design, interior design, skin care and cosmetics, and entertainment.<br/><br/>The project includes an education program that is tightly coupled to the research program.  The PIs have already been meeting twice a week for more than six months, and their graduate students already share data, code, and equipment.  During the activity, the students will make week-long and month-long visits to each other's laboratories to collaborate, and in this way the project will produce a generation of researchers who are ""T-shaped"" in the sense of being both deep in their respective fields and able to work effectively across these synergistic disciplines.  The PIs also plan to organize a workshop that will brins together researchers in vision science, computer graphics, and computer vision, so that the important ties between these fields are strengthened even further."
"1218570","HCC: Small: Non-Visual Skimming: Improving the Usability of Web Access for Blind People","IIS","HCC-Human-Centered Computing","10/01/2012","09/11/2012","Yevgen Borodin","NY","SUNY at Stony Brook","Standard Grant","Ephraim Glinert","09/30/2016","$500,000.00","I. Ramakrishnan","borodin@charmtechlabs.com","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","7367","7367, 7923","$0.00","In our information driven Web-based society, we are all gradually falling victims to information overload.  But while sighted people can develop ways to quickly skim Web content in order to get the gist of the information and find what they need, blind users are stymied because they must rely on screen reader software with limited functionality to narrate content using computer-generated speech through a serial audio interface, which does not allow these users to find out what content is important before they listen to it.  So, they either listen to all content or listen to the first part of each sentence or paragraph before skipping to the next.  The PI's goal in this project is to address this problem by developing novel interfaces and algorithmic techniques for non-visual skimming that will empower people with visual impairments to access information on the Web significantly faster than is currently possible with state-of-the-art screen readers.<br/><br/>When skimming, sighted people quickly look through content and pick out keywords and relevant phrases.  The PI's approach is to emulate this process and enable a computer-assisted skimming experience for screen-reader users.  To this end, he will iteratively and concurrently pursue two research directions: designing interfaces for non-visual skimming, and developing algorithms to enable these interfaces.  Through a process of participatory design interfaces will be derived for skimming with standard shortcut-driven screen-readers, touch-based devices, and simulated haptic surfaces, while algorithms for generating summaries will be created to support skimming of various types of Web content at different levels of granularity and speed.  Controlled and in-situ real-world experiments will be conducted to evaluate the utility of the resulting interfaces and algorithms.  Project outcomes will include an open-source skimming tool and reusable datasets of Web pages with annotations and summaries.<br/><br/>Broader Impacts:  Technology to facilitate non-visual skimming will transform information access for visually impaired users while also contributing to the fields of natural language processing, machine learning, and Web information retrieval.  Additionally, this research will help us better understand how a combination of touch and haptic interfaces can improve website navigation and skimming.  Although not explored in this research, project outcomes will likely prove useful to people with other disabilities such as cognitive and motor impairments, and they may ultimately also be helpful to sighted people."
"1218749","III: Small: Collaborative Research: Efficient, Nonparametric and Local-Minimum-Free Latent Variable Models: With Application to Large-Scale Computer Vision and Genomics","IIS","Info Integration & Informatics","10/01/2012","08/07/2013","Le Song","GA","Georgia Tech Research Corporation","Continuing grant","Sylvia Spengler","09/30/2017","$299,979.00","Alexander Gray","lsong@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7364","7923","$0.00","Many modern applications ranging from computer vision  to biology require modeling and inferring high-dimensional continuous variables based on distributions with multimodality, skewness, and rich latent structures.  Most existing models in this regime rely heavily on parametric assumptions where the components of the model are typically assumed to be discrete or multivariate Gaussian, or the relations between variables are linear, which may be very different from the actual data generating processes. Furthermore, existing algorithms for discovering the latent dependency structures and learning the latent parameters largely are restricted to local search heuristics such as expectation maximization. Conclusions inferred under these restricted assumptions and suboptimal solutions can be misleading, if the underlying assumptions are violated or if the suboptimal solutions differ greatly from the globally optimal ones. This project aims to develop a novel framework which can (i) discover and take advantage of latent structures in the data, while (ii) allowing parts to handle near-arbitrary distributions, and (iii) allowing the models to scale to modern massive datasets in a local-minimum-free fashion. <br/><br/>The key innovation in the project is a novel nonparametric latent variable modeling framework based on kernel embedding of distributions. The basic idea is to map distributions into infinite dimensional feature spaces using kernels, such that subsequent comparisons and manipulations of distributions can be achieved via feature space operations, such as inner products, distances, projections, linear transformations and spectral analysis.  Conceptually, the  framework represents components from latent variable models, such as marginal distributions over a single variable, joint distributions over variable pairs, triplets and more variables, as infinite dimensional vectors, matrices, tensors and high-order tensors respectively. Probabilistic relations between these components, i.e., conditional distributions, Sum Rule, Product Rule etc. become linear transformations and relations between these feature space components.<br/><br/>The framework supports modeling  data with diverse statistical features without the need for making restrictive assumptions about the type of distributions and relations. It supports the application of a large pool of linear and multi-linear algebraic (tensor) tools for addressing challenging graphical model problems in the presence of latent variables, including structure discovery, inference, parameter learning and latent feature extraction.  The framework applies not only to general continuous variables, but also to variables that  take values on strings, graphs, groups, compact manifolds, and other domains on which kernels may be defined.<br/><br/>Besides advancing the state of the art in machine learning,the new non-parametric methods resulting from the project find applications in image data and understanding and gene expression data analysis. It also contributes to research-based training of graduate and undergraduate students at Georgia Tech and CMU."
"1218282","III: Small: Collaborative Research: Efficient, Nonparametric and Local-Minimum-Free Latent Variable Models: With Application to Large-Scale Computer Vision and Genomics","IIS","Info Integration & Informatics","10/01/2012","07/22/2013","Eric Xing","PA","Carnegie-Mellon University","Continuing grant","Sylvia Spengler","09/30/2016","$200,000.00","","epxing@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7364","7364, 7923","$0.00","Many modern applications ranging from computer vision  to biology require modeling and inferring high-dimensional continuous variables based on distributions with multimodality, skewness, and rich latent structures.  Most existing models in this regime rely heavily on parametric assumptions where the components of the model are typically assumed to be discrete or multivariate Gaussian, or the relations between variables are linear, which may be very different from the actual data generating processes. Furthermore, existing algorithms for discovering the latent dependency structures and learning the latent parameters largely are restricted to local search heuristics such as expectation maximization. Conclusions inferred under these restricted assumptions and suboptimal solutions can be misleading, if the underlying assumptions are violated or if the suboptimal solutions differ greatly from the globally optimal ones. This project aims to develop a novel framework which can (i) discover and take advantage of latent structures in the data, while (ii) allowing parts to handle near-arbitrary distributions, and (iii) allowing the models to scale to modern massive datasets in a local-minimum-free fashion. <br/><br/>The key innovation in the project is a novel nonparametric latent variable modeling framework based on kernel embedding of distributions. The basic idea is to map distributions into infinite dimensional feature spaces using kernels, such that subsequent comparisons and manipulations of distributions can be achieved via feature space operations, such as inner products, distances, projections, linear transformations and spectral analysis.  Conceptually, the  framework represents components from latent variable models, such as marginal distributions over a single variable, joint distributions over variable pairs, triplets and more variables, as infinite dimensional vectors, matrices, tensors and high-order tensors respectively. Probabilistic relations between these components, i.e., conditional distributions, Sum Rule, Product Rule etc. become linear transformations and relations between these feature space components.<br/><br/>The framework supports modeling  data with diverse statistical features without the need for making restrictive assumptions about the type of distributions and relations. It supports the application of a large pool of linear and multi-linear algebraic (tensor) tools for addressing challenging graphical model problems in the presence of latent variables, including structure discovery, inference, parameter learning and latent feature extraction.  The framework applies not only to general continuous variables, but also to variables that  take values on strings, graphs, groups, compact manifolds, and other domains on which kernels may be defined.<br/><br/>Besides advancing the state of the art in machine learning,the new non-parametric methods resulting from the project find applications in image data and understanding and gene expression data analysis. It also contributes to research-based training of graduate and undergraduate students at Georgia Tech and CMU."
"1137396","Automated Analysis of the Reproductive Response of Two Panamanian Forests to ENSO Climate Variation - A Machine Learning Pilot Study","EF","MacroSysBIO & NEON-Enabled Sci","03/01/2012","06/10/2013","Surangi Punyasena","IL","University of Illinois at Urbana-Champaign","Standard Grant","timothy kratz","09/30/2015","$236,907.00","David Tcheng","punyasena@life.illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","BIO","7959","7959, 9251","$0.00","Contained within the fossil pollen and spore record is one of the most comprehensive histories of vegetation and its response to long term environmental change. However, scientists have been unable to efficiently use this record because pollen is still studied much in the same low technology way it was a century ago: by a highly trained expert using a transmitted light microscope. The proposed research intends to transform the study of pollen and spores into a high throughput and precise science by developing an automated system capable of identifying and classifying extremely diverse pollen and spore samples. This system will establish standards approaches for automated classification and be based on recent advances in machine learning and computer vision. The system will be developed and tested on 900 samples of pollen material collected over seventeen years from pollen traps placed in two tropical forests in Panama. Automation will allow, for the first time, a detailed study of the seasonal production of pollen in response to multiple El Nino-Southern Oscillation, or ENSO, events.<br/><br/>The results of the proposed research will increase the quantity and quality of tropical pollen data and promote needed research on the large scale dynamics of plant communities that is recorded in fossil pollen records. This research represents a fundamental transformation in the way scientists think about and approach the analysis of pollen data. The machine learning software that will be developed will be publically available, with the hope that other researchers will then adopt the methods and standards and establish objective measures for consistent and reliable pollen identifications. Educational goals of this project include undergraduate research training for underrepresented and first generation college students."
"1252987","EAGER: Memory-based learning of effective actions","IIS","Robust Intelligence","09/01/2012","09/05/2012","Benjamin Kuipers","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Hector Munoz-Avila","08/31/2014","$80,000.00","","kuipers@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7495","7495, 7916","$0.00","This project addresses the foundational question in Robust Intelligence of how an autonomous agent can learn use low-level sub-symbolic (pixel-level) sensorimotor experiences with its environment to learn higher level effective concepts, ranging from learning to use a hand to manipulate objects on a tabletop, to learning to balance and walk, to learning to move through a complex environment without collisions with walls or pedestrians. This project will develop computational models of how this learning process could take place and will implement and test these computational models on an actual robot. Understanding such autonomous concept learning has the potential to impact a range of disciplines including Cognitive Science, Psychology, AI in general, and robotics, computer vision, and machine learning in particular. Understanding how concepts come into being and evolve in the specific domain of robot navigation also has the potential to contribute to advances in systems that help persons with physical and learning disabilities.<br/><br/>The project draws on insights from two different approaches from the PI's lab that have complementary strengths: (1) QLAP (Qualitative Learner of Action and Perception), and (2) MPEPC system (Model Predictive Equilibrium Point Control). The QLAP system exploits a qualitative abstraction of continuous sensor input in order to learn causal contingencies, DBN (Dynamic Belief Network) and MDP models of the causal world, and to build a hierarchy of action models. It uses perception with laser rangefinders and correlation peaks between changes to the motor vector and events in the sense vector -- so-called contingencies -- to discern motor signals that produce resulting perceptual events that may be more than random variation. Reliable episodes can be remembered as cases and used in learning. The MPEPC system factors the continuous navigation problem for a mobile robot into a local unconstrained control and a global optimization process that balances constraints such as progress and collision avoidance. Both methods have a local phase (learning contingencies and local control laws), and a global phase (learning a hierarchy of actions and finding extended routes that balance constraints). These two approaches will be augmented by learning methods from Case-Based Reasoning (CBR) that use features of the presenting case to retrieve related cases from case memory. Two levels of case representation will be employed. The lowest level case representation is a simple feature vector: in the case of local motion control, it specifies the target pose location in the egocentric frame of reference, along with the parameters of the motion control law that attempts to reach it, and the quality of the resulting trajectory. Retrieval will be done using Nearest Neighbor, combining information from the retrieved cases by Locally Weighted Regression or Locally Weighted Projection Regression. At the higher level of action learning, a case is to be described by identifying the critical environmental constraints that determine the global structure of the action."
"1230556","I/UCRC:  Collaborative Research:  Detecting cancer using advanced computer vision techniques","CNS","IUCRC-Indust-Univ Coop Res Ctr","08/15/2012","06/11/2013","Anneliese Andrews","CO","University of Denver","Standard Grant","Thyagarajan Nandagopal","07/31/2016","$107,977.00","Mohammad Mahoor","andrews@cs.du.edu","2199 S. University Blvd.","Denver","CO","802104711","3038712000","CSE","5761","1049, 116E, 7609, 8039, 9102, 9178, 9251, SMET","$0.00","The proposed research proposes to develop novel image processing and machine learning algorithms for the detection and segmentation of cancerous regions from high-resolution images of tissue slides, distinguishing them from healthy/benign regions. The proposed research plans to advance (i) A reliable framework for use in the accurate segmentation of cancerous regions in tissue slides; (ii) New algorithms for texture analysis; (iii) Innovative representations that can increase the system throughput; (iv) Effective machine learning techniques and transparent user interfaces to assist in the reduction of the time that a pathologist needs to examine each slide; and (v) Extensive testing to a variety of cancers including prostate and breast cancer.<br/><br/>The proposed work has the potential to yield algorithms to facilitate the design of systems that can increase the likelihood of cancer detection. The work is supported by the Industry Advisory Board as well as individual industry members of the center and has the potential to extend the center?s portfolio. The PIs plan to introduce the research content into summer robotic camps for middle schoolers from underrepresented groups, create a website along with a wiki, visit local groups and K-12 schools, and include the research products into the curricula of the participating institutions.<br/>"
"1241661","Collaborative Project: Enriching Security Curricula and Enhancing Awareness of Security in Computer Science and Beyond","DUE","CYBERCORPS: SCHLAR FOR SER","09/15/2012","09/15/2012","Ping Chen","TX","University of Houston - Downtown","Standard Grant","Victor P. Piotrowski","04/30/2014","$166,981.00","Shengli Yuan","ping.chen@umb.edu","One Main Street","Houston","TX","770021001","7132218005","EHR","1668","7254, 9178, SMET","$0.00","This project is building our nation's capacity to develop a workforce well trained in computer security.  Faculty from the University of Houston, University of Houston-Downtown and Texas Southern University, are developing interactive courseware and conducting workshops on the latest machine learning, data mining, natural language processing and statistics techniques as applied to computer security.  Courseware on security-critical domains such as wireless sensor networks, and on key societal privacy and security concerns is also being developed. Workshops are being organized to cross-fertilize security curricula with related fields and topics and to develop a broad-based consensus on courseware contents.  In addition to interactive courseware for security courses, materials are being developed to infuse security topics and increase awareness of information assurance in the broader computer science curriculum and, beyond computer science, in other STEM disciplines.  A hypertextbook integrating the courseware is being implemented.<br/><br/>Through faculty workshops, a community of information assurance education and research scholars is being created.  This community will build bridges between information assurance professionals and professionals in related fields. The workshops are training diverse groups of faculty from different types of institutions.  <br/><br/>Courseware and other results are available in the National Science Digital Library as well as at a project-specific web site which also links to exemplary courseware developed elsewhere.  Courseware and workshops are being rigorously evaluated."
"1241626","Collaborative Project: Enriching Security Curricula and Enhancing Awareness of Security in Computer Science and Beyond","DGE","CYBERCORPS: SCHLAR FOR SER","09/15/2012","09/15/2012","Wei Li","TX","Texas Southern University","Standard Grant","Victor Piotrowski","08/31/2016","$173,131.00","Lila Ghemri, M. Farrukh Khan","Liw@tsu.edu","3100 Cleburne Street","Houston","TX","770044501","7133137457","EHR","1668","7254, 9178, SMET","$0.00","This project is building our nation's capacity to develop a workforce well trained in computer security.  Faculty from the University of Houston, University of Houston-Downtown and Texas Southern University, are developing interactive courseware and conducting workshops on the latest machine learning, data mining, natural language processing and statistics techniques as applied to computer security.  Courseware on security-critical domains such as wireless sensor networks, and on key societal privacy and security concerns is also being developed. Workshops are being organized to cross-fertilize security curricula with related fields and topics and to develop a broad-based consensus on courseware contents.  In addition to interactive courseware for security courses, materials are being developed to infuse security topics and increase awareness of information assurance in the broader computer science curriculum and, beyond computer science, in other STEM disciplines.  A hypertextbook integrating the courseware is being implemented.<br/><br/>Through faculty workshops, a community of information assurance education and research scholars is being created.  This community builds bridges between information assurance professionals and professionals in related fields. The workshops are training diverse groups of faculty from different types of institutions.  <br/><br/>Courseware and other results are available in the National Science Digital Library as well as at a project-specific web site which also links to exemplary courseware developed elsewhere.  Courseware and workshops are being rigorously evaluated."
"1241772","Collaborative Project: Enriching Security Curricula and Enhancing Awareness of Security in Computer Science and Beyond","DGE","CYBERCORPS: SCHLAR FOR SER","09/15/2012","07/03/2017","Rakesh Verma","TX","University of Houston","Standard Grant","Victor Piotrowski","08/31/2018","$435,395.00","Shou-Hsuan Huang, Ernst Leiss, Carlos Ordonez","rverma@uh.edu","4800 Calhoun Boulevard","Houston","TX","772042015","7137435773","EHR","1668","7254, 9178, SMET","$0.00","This project is building our nation's capacity to develop a workforce well trained in computer security.  Faculty from the University of Houston, University of Houston-Downtown and Texas Southern University, are developing interactive courseware and conducting workshops on the latest machine learning, data mining, natural language processing and statistics techniques as applied to computer security.  Courseware on security-critical domains such as wireless sensor networks, and on key societal privacy and security concerns is also being developed. Workshops are being organized to cross-fertilize security curricula with related fields and topics and to develop a broad-based consensus on courseware contents.  In addition to interactive courseware for security courses, materials are being developed to infuse security topics and increase awareness of information assurance in the broader computer science curriculum and, beyond computer science, in other STEM disciplines.  A hypertextbook integrating the courseware is being implemented.<br/><br/>Through faculty workshops, a community of information assurance education and research scholars is being created.  This community will build bridges between information assurance professionals and professionals in related fields. The workshops are training diverse groups of faculty from different types of institutions.  <br/><br/>Courseware and other results are available in the National Science Digital Library as well as at a project-specific web site which also links to exemplary courseware developed elsewhere.  Courseware and workshops are being rigorously evaluated."
"1208245","NRI-Small: Expert-Apprentice Collaboration","IIS","Info Integration & Informatics, Robust Intelligence, NRI-National Robotics Initiati","10/01/2012","09/19/2012","Carlo Tomasi","NC","Duke University","Standard Grant","Sylvia Spengler","09/30/2017","$746,924.00","Ronald Parr","tomasi@cs.duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","7364, 7495, 8013","7923, 8086","$0.00","Recent advances in robot platforms have outpaced our ability to effectively program robots to accomplish useful tasks, often in complex environments that they share with humans.   In order for flexible, general purpose robots to become widespread e.g., in teaching skills to children, assisting the elderly, there must be a way of interacting with them beyond programming. Teaching by demonstration offers a potentially powerful and practical approach to realizing the promise of large scale personal robotics in a wide range of applications. In teaching by demonstration, the expert (human), demonstrates the task on different hardware than what the apprentice or student (robot) uses. <br/><br/>The project aims to develop visual feature-based methods that allow robots to teach humans and learn from them by unifying apprenticeship learning, learning by demonstration (or by imitation) and teaching humans, taking into account the differences between experts and apprentices. The resulting system will be evaluated on a PR2 robot (mostly on grasping and manipulation tasks). The scientific advances resulting from the project in learning from demonstration and imitation learning, both general techniques with broad applicability, will greatly simplify the programming of robots which would make it easier for non-expert users to perform this important task which currently requires considerable expertise in robotics as well as computer science. <br/><br/>Broader impacts of the research include development of new robotics curricula, enhanced opportunities for research-based interdisciplinary training at the intersection of computer vision, machine learning, and robotics,  outreach  activities (including participation in a public school robotics instruction program). All of the results of the research, including publications, open-source software and datasets, will be made freely available to the larger scientific and academic community."
"1216528","RI: Small: Unsupervised Object Class Discovery via Bottom-up Multiple Class Learning","IIS","Robust Intelligence","09/01/2012","07/17/2013","Zhuowen Tu","CA","University of California-Los Angeles","Continuing Grant","Jie Yang","10/31/2013","$346,260.00","","zhuowen.tu@gmail.com","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","7495","7923","$0.00","This project develops an integrated framework to perform simultaneous object discovery and detector training in an unsupervised setting. It takes advantages of large amount (millions or even billions) of well-organized internet images to automatically learn rich image representations for a wide range of objects. The main activities in this project include the following. (1) The central component of this project is a formulation to turn unsupervised data into weakly-supervised ""noisy input"" through which commonalities are explored for rich object representation using a new learning method. (2) A large dictionary of mid-level image representations will be learned on a large scale number of images retrieved using thousands of object words through the internet search engine. (3) A new flexible object representation is developed to deal with articulated/non-rigid objects.<br/><br/>The project advances computer vision and machine learning fields by developing an unsupervised paradigm to explore a large scale of internet images. The learned mid-level and high-level representations from images retrieved using thousands of words can significantly enhance the object representation power and benefit researchers in the object recognition field. The formulations, algorithms, and methods resulted from this project are also helpful to researchers in other fields such as medical imaging and data mining. The project dissemination plan includes the source code and learned mid-level and high-level representations."
"1228082","CAREER: Similarity-based Representation of Large-scale Image Collections","IIS","Info Integration & Informatics, Robust Intelligence","03/01/2012","05/24/2013","Svetlana Lazebnik","IL","University of Illinois at Urbana-Champaign","Continuing grant","Maria Zemankova","07/31/2015","$372,050.00","","slazebni@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7364, 7495","1045, 1187, 7364, 9102, 9216, HPCC","$0.00","This proposal is to develop a general representation framework that uses similarity to capture relationships in large scale image collections. The representation is not restricted to any specific distance function, feature, or learning model. It includes new methods to combine multiple kernels based on different cues, learn low-rank kernels, and improve indexing efficiency. In addition, new methods for nearest neighbor search and semi-supervised learning are proposed. It  has relevance to machine learning and computer vision research agendas.  Two major research problems addressed are: (1) defining and computing similarities between images' in vast, expanding, repositories, and representing those similarities in an efficient manner so the right pairs can be retrieved on demand; and (2) developing a system that can learn and predict similarities with 'sparse supervisory information and constantly evolving data.' The approach is notable in its embrace of the scale of web archives and its use of verbal and visual means of analysis.<br/><br/>"
"1230817","I/UCRC:  Collaborative Research: Detecting Cancer Using Advanced Computer Vision Techniques","IIP","IUCRC-Indust-Univ Coop Res Ctr","08/15/2012","07/22/2014","Nikolaos Papanikolopoulos","MN","University of Minnesota-Twin Cities","Standard Grant","Thyagarajan Nandagopal","07/31/2015","$131,998.00","Arindam Banerjee, Vassilios Morellas","npapas@cs.umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","ENG","5761","1049, 116E, 7609, 8039, 9178, 9251, SMET","$0.00","The proposed research proposes to develop novel image processing and machine learning algorithms for the detection and segmentation of cancerous regions from high-resolution images of tissue slides, distinguishing them from healthy/benign regions. The proposed research plans to advance (i) A reliable framework for use in the accurate segmentation of cancerous regions in tissue slides; (ii) New algorithms for texture analysis; (iii) Innovative representations that can increase the system throughput; (iv) Effective machine learning techniques and transparent user interfaces to assist in the reduction of the time that a pathologist needs to examine each slide; and (v) Extensive testing to a variety of cancers including prostate and breast cancer.<br/><br/>The proposed work has the potential to yield algorithms to facilitate the design of systems that can increase the likelihood of cancer detection. The work is supported by the Industry Advisory Board as well as individual industry members of the center and has the potential to extend the center?s portfolio. The PIs plan to introduce the research content into summer robotic camps for middle schoolers from underrepresented groups, create a website along with a wiki, visit local groups and K-12 schools, and include the research products into the curricula of the participating institutions.<br/>"
"1217433","RI: Small: Hard Clustering via Bayesian Nonparametrics","IIS","Robust Intelligence","07/01/2012","06/25/2015","Brian Kulis","OH","Ohio State University","Continuing grant","Weng-keen Wong","06/30/2016","$439,689.00","","bkulis@bu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","CSE","7495","7923","$0.00","Modern machine learning algorithms often encounter a trade off between scalability and modeling power.  For the problem of data clustering, Bayesian approaches enjoy numerous modeling advantages over classical methods, but hard clustering methods such as k-means are often preferred in practice due to their simplicity and scalability.<br/><br/>This project explores bridging the gap between classical hard clustering methods and clustering models based on Bayesian nonparametrics.  The first step is an asymptotic result connecting the Dirichlet process Gaussian mixture model with a k-means-like algorithm that does not fix the number of clusters in advance.  Using this key result, the PI and his team will explore four related research directions which collectively demonstrate the utility of this asymptotic approach: (1) extensions of the analysis to hierarchical Bayesian models, leading to scalable hard clustering methods over multiple data sets; (2) connections to spectral methods and graph clustering, leading to novel and flexible graph clustering methods; (3) extensions beyond the Gaussian setting, leading to new approaches to topic modeling and other discrete-data clustering problems; and (4) extensive experiments in both the computer vision and text domains.<br/><br/>Given that k-means is truly a workhorse of machine learning, these four directions have the potential to impact a wide array of large-scale applications including computer vision, bioinformatics, social network analysis, and many other domains.  Furthermore, the research will benefit the broader community through released software and integration into coursework at Ohio State University."
"1211071","SoCS: Collaborative Research: Focusing Attention to Improve the Performance of Citizen Science Systems: Beautiful Images and Perceptive Observers","IIS","HCC-Human-Centered Computing, SOCIAL-COMPUTATIONAL SYSTEMS","09/01/2012","05/09/2014","Carsten Oesterlund","NY","Syracuse University","Standard Grant","Ephraim Glinert","08/31/2017","$337,099.00","","costerlu@syr.edu","OFFICE OF SPONSORED PROGRAMS","SYRACUSE","NY","132441200","3154432807","CSE","7367, 7953","7367, 7953, 9251","$0.00","The goal of this project is to develop a next-generation socio-computational citizen science platform that combines the efforts of human classifiers with those of computational systems to maximize the efficiency with which human attention can be used. Dealing with the flood of digital data that confronts researchers is the fundamental challenge of twenty-first century research.  New techniques, tools and strategies for dealing with massive data sets, whether they consist of vast numbers of base-pair DNA sequences or terabytes of data from all-sky astronomical surveys, present an opportunity to establish a new paradigm of scientific discovery, but the task is not easy. In many areas of research, the relentless growth of data sets has led to the adoption of increasingly automated and unsupervised methods of classification. In many cases, this has led to degradation in classification quality, with machine learning and computer vision unable to replicate the successes of human pattern recognition. The growth of citizen science on the web has provided a temporary solution to this problem, demonstrating that it is possible to recruit hundreds of thousands of volunteers to make an authentic contribution to results, boosting human analysis through the collective wisdom of a crowd of classifiers. However, human classifiers alone will not be able to cope with expected flood of data from future scientific instruments. <br/><br/>This research will be carried out by a partnership between computer and social scientists, addressing research problems both in automated data analysis and social science through systems implementation, alongside field research and experiments with project participants.  The intellectual merit of this project lies in its contribution to advancing knowledge and understanding in multiple domains of science. First, the work will contribute to developing new methods of computational data analysis, initially with analysis of astronomical images, and later extending to additional fields. Second, the project includes social science research to test and apply theories of human motivation and learning in an online context, which can then be applied to a broad range of social-computational problems. By mixing human and computational elements, the planned system has the potential to transform the application of citizen science and its approach to data analysis. <br/><br/>This project will advance science while promoting teaching, training and learning. One of the most significant broader impacts for its citizen science activities is enabling a community of hundreds of thousands of volunteers to participate in research, a powerful and rapidly developing form of informal science education.   By choosing the relatively generic topic of image classification, beginning with astronomy but not limited to that field of science, the techniques developed under this grant will be of significant value to future investigations in similar research areas, thus enhancing the infrastructure for research and education."
"1211094","SoCS: Collaborative Research: Focusing Attention to Improve the Performance of Citizen Science Systems: Beautiful Images and Perceptive Observers","IIS","SOCIAL-COMPUTATIONAL SYSTEMS","09/01/2012","08/25/2015","Arfon Smith","IL","Adler Planetarium","Standard Grant","Ephraim P. Glinert","08/31/2017","$395,902.00","","arfon@adlerplanetarium.org","1300 S. Lake Shore Drive","Chicago","IL","606052403","3123220325","CSE","7953","7953","$0.00","The goal of this project is to develop a next-generation socio-computational citizen science platform that combines the efforts of human classifiers with those of computational systems to maximize the efficiency with which human attention can be used. Dealing with the flood of digital data that confronts researchers is the fundamental challenge of twenty-first century research.  New techniques, tools and strategies for dealing with massive data sets, whether they consist of vast numbers of base-pair DNA sequences or terabytes of data from all-sky astronomical surveys, present an opportunity to establish a new paradigm of scientific discovery, but the task is not easy. In many areas of research, the relentless growth of data sets has led to the adoption of increasingly automated and unsupervised methods of classification. In many cases, this has led to degradation in classification quality, with machine learning and computer vision unable to replicate the successes of human pattern recognition. The growth of citizen science on the web has provided a temporary solution to this problem, demonstrating that it is possible to recruit hundreds of thousands of volunteers to make an authentic contribution to results, boosting human analysis through the collective wisdom of a crowd of classifiers. However, human classifiers alone will not be able to cope with expected flood of data from future scientific instruments. <br/><br/>This research will be carried out by a partnership between computer and social scientists, addressing research problems both in automated data analysis and social science through systems implementation, alongside field research and experiments with project participants.  The intellectual merit of this project lies in its contribution to advancing knowledge and understanding in multiple domains of science. First, the work will contribute to developing new methods of computational data analysis, initially with analysis of astronomical images, and later extending to additional fields. Second, the project includes social science research to test and apply theories of human motivation and learning in an online context, which can then be applied to a broad range of social-computational problems. By mixing human and computational elements, the planned system has the potential to transform the application of citizen science and its approach to data analysis. <br/><br/>This project will advance science while promoting teaching, training and learning. One of the most significant broader impacts for its citizen science activities is enabling a community of hundreds of thousands of volunteers to participate in research, a powerful and rapidly developing form of informal science education.   By choosing the relatively generic topic of image classification, beginning with astronomy but not limited to that field of science, the techniques developed under this grant will be of significant value to future investigations in similar research areas, thus enhancing the infrastructure for research and education."
"1149787","CAREER: Multimodal and Multialgorithm Facial Activity Understanding by Audiovisual Information Fusion","IIS","ROBUST INTELLIGENCE, EPSCoR Co-Funding","03/01/2012","02/27/2012","Yan Tong","SC","University South Carolina Research Foundation","Standard Grant","Jie Yang","09/30/2018","$443,803.00","","tongy@cec.sc.edu","1600 Hampton Street","COLUMBIA","SC","292080001","8037777093","CSE","7495, 9150","1045, 9150","$0.00","This project develops a unified multimodal and multialgorithm fusion framework to recognize facial action units, which describe complex and rich facial behaviors. The information from voice is incorporated with visual observations to effectively improve facial activity understanding since voice and facial activity are intrinsically correlated. The developed framework systematically captures the inherent interactions between the visual and audio channels in a global context of human perception of facial behavior. Advanced machine learning techniques are developed to integrate these relationships together with uncertainties associated with various visual and audio measurements in the fusion framework to achieve a robust and accurate understanding of facial activity. It is these coordinated and consistent interactions that produce a meaningful facial display.<br/><br/>The research work from this project fosters computer vision and machine learning technologies with applications across a wide range of fields varying from psychiatry to human-computer interaction. The new audiovisual emotional database constructed in this research facilitates benchmark evaluations and promotes new research directions, especially, in human behavior analysis. An integration of research and education promotes cutting-edge training on human-computer interactions to K-12, undergraduate, and graduate students, especially encourages the participation of women in engineering and computing."
"1160585","Planning Grant:  I/UCRC for Identification Technology Research","IIP","IUCRC-Indust-Univ Coop Res Ctr","02/01/2012","01/31/2012","Venugopal Govindaraju","NY","SUNY at Buffalo","Standard Grant","Rathindra DasGupta","01/31/2013","$12,977.00","","govind@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","ENG","5761","5761, 8039","$0.00","I/UCRC for Identification Technology Research <br/>1160585 SUNY at Buffalo; Venugopal Govindaraju <br/><br/>The University at Buffalo, SUNY (UB) proposes to join the Center for Identification Technology Research (CITeR) as a university site. CITeR, currently comprised of Clarkson University (lead), West Virginia University and the University of Arizona, was established about a decade ago to be the forum for bringing industry and academic interests to converge in the biometrics arena.. <br/><br/>The UB site proposes to advance the science of biometric technologies for both civilian and homeland security applications by integrating pattern recognition and machine learning algorithms with sensors technology. The UB team plans to fuse brain scans with facial expressions and recognition to attempt to record emotion. UB's interest is in encrypting biometric templates while maintaining efficiency of indexing and search. The multidisciplinary faculty at the UB site will design chemical and photo sensors to propose new biometric modalities with integrated liveness detection. <br/><br/>The UB site will further CITeR's objective to serve as a comprehensive academic center serving the growing identification technology research, undergraduate and graduate education, and outreach needs of the public and private sectors. The UB site brings an experienced team in the areas of behavioral science, computer vision, visualization, pattern recognition, machine learning, and pervasive computing. Under the auspices of BEAM (Buffalo Area Engineering Awareness for Minorities), the proposed site plans to attract the participation of local high school and undergraduate students. The UB site also intends to increase the number of under-represented minorities obtaining graduate degrees in science, technology, engineering and mathematics. CITeR meetings are expected to serve as an excellent opportunity for students to interact with potential employers to understand challenges in the field."
"1232676","Collaborative Research: Emotional Sophistication - Studies of Facial Expressions in Decision Making","BCS","DECISION RISK & MANAGEMENT SCI, PERCEPTION, ACTION & COGNITION, ROBUST INTELLIGENCE","09/01/2012","07/13/2016","Marian Bartlett","CA","University of California-San Diego","Standard Grant","Betty H. Tuller","08/31/2016","$332,962.00","","mbartlett@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","SBE","1321, 7252, 7495","6867, 7298","$0.00","Social and economic decisions cannot be fully explained by ""rational"" attempts to maximize monetary gain, even in very simple game-theoretic scenarios. Complex emotional processes such as anger, guilt or generosity act as hidden forces that lead to observable actions. Such ""non-rational"" motivations can drive our own decisions and they affect our beliefs about what motivates others' decisions as well. The goal of this project is to use automatic measurements of dynamic facial expressions, in combination with other measurements such as functional MRI (fMRI) and eye-tracking, to investigate the role of non-rational motivations in social decision making. The core of the approach is to use state-of-the-art computer vision techniques to extract facial actions from video in real-time while participants interact with a computer or with each other, in some cases viewing live video of each others' faces. The investigators will use powerful statistical machine learning techniques to make inferences about  the participants' internal emotional states during the interactions.  The goal is to use the inferences concerning emotional state (a) to predict participants' behavior; (b) to explain why a decision is made in terms of the hidden forces driving it; and (c) to build autonomous agents that can use this information to drive their interactions with humans. <br/><br/>This multidisciplinary project contributes to several fields such as psychology, neuroscience, and economics. First, it develops new methodologies to study decision processes. Second, it uses these methods to test hypotheses about social decision-making and to bridge the gap between observable actions and the internal states that generated them. Third, the investigators intend to make available a dataset and toolset that should be an extremely useful for other investigators analyzing facial expression in multiple contexts. Additionally, automatic and on-line decoding of internal motivational states lays the groundwork for ""affectively-aware"" interactive computers, or artificial systems that can make inferences about the emotions and intentions of their users. Through the development of these systems, this project will make a significant contribution to the growing field of human-machine interaction.<br/><br/>[Supported by Perception, Action and Cognition, Decision, Risk and Management Sciences, and Robust Intelligence]"
"1232639","Collaborative Research: Emotional Sophistication - Studies of Facial Expressions in Decision Making","BCS","Decision, Risk & Mgmt Sci, Perception, Action & Cognition","09/01/2012","08/27/2012","Clayton Morrison","AZ","University of Arizona","Standard Grant","Betty Tuller","08/31/2016","$166,977.00","Alan Sanfey","claytonm@email.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","SBE","1321, 7252","6867, 7298","$0.00","Social and economic decisions cannot be fully explained by ""rational"" attempts to maximize monetary gain, even in very simple game-theoretic scenarios. Complex emotional processes such as anger, guilt or generosity act as hidden forces that lead to observable actions. Such ""non-rational"" motivations can drive our own decisions and they affect our beliefs about what motivates others' decisions as well. The goal of this project is to use automatic measurements of dynamic facial expressions, in combination with other measurements such as functional MRI (fMRI) and eye-tracking, to investigate the role of non-rational motivations in social decision making. The core of the approach is to use state-of-the-art computer vision techniques to extract facial actions from video in real-time while participants interact with a computer or with each other, in some cases viewing live video of each others' faces. The investigators will use powerful statistical machine learning techniques to make inferences about  the participants' internal emotional states during the interactions.  The goal is to use the inferences concerning emotional state (a) to predict participants' behavior; (b) to explain why a decision is made in terms of the hidden forces driving it; and (c) to build autonomous agents that can use this information to drive their interactions with humans. <br/><br/>This multidisciplinary project contributes to several fields such as psychology, neuroscience, and economics. First, it develops new methodologies to study decision processes. Second, it uses these methods to test hypotheses about social decision-making and to bridge the gap between observable actions and the internal states that generated them. Third, the investigators intend to make available a  dataset and toolset that should be an extremely useful for other investigators analyzing facial expression in multiple contexts. Additionally, automatic and on-line decoding of internal motivational states lays the groundwork for ""affectively-aware"" interactive computers, or artificial systems that can make inferences about the emotions and intentions of their users. Through the development of these systems, this project will make a significant contribution to the growing field of human-machine interaction.<br/><br/>[Supported by Perception, Action and Cognition, Decision, Risk and Management Sciences, and Robust Intelligence]"
"1149048","CAREER: Fast algorithms via a spectral theory for graphs with a prescribed cut structure","CCF","Algorithmic Foundations","07/01/2012","07/12/2016","Ioannis Koutis","PR","University of Puerto Rico-Rio Piedras","Continuing Grant","Balasubramanian Kalyanasundaram","01/31/2019","$500,000.00","","ikoutis@njit.edu","18 Ave. Universidad, Ste.1801","San Juan","PR","009252512","7877634949","CSE","7796","1045, 7933, 9150","$0.00","Critical applications involving very large data sets require algorithms that run fast and provide strong performance guarantees. Among the numerous examples are the analysis of medical scans and the acquisition -via imaging- of connectivity in neural systems, an important task in current computational neuroscience. These problems are very often approached by first modeling the data as networks -also called graphs- and then applying graph-specific algorithms to solve them. Among many possibilities, algorithms that rely on certain algebraic representations of graphs have become very appealing due to recent theoretical progress that renders them very time-efficient.  However, efficiency appears to come at the cost of an occasionally inferior quality in the generated solutions. Via the proposed extensions of the theory studying these algebraic representations, the project will design new algorithms with strong guarantees and wide applicability.<br/><br/>Spectral graph theory studies the connections between algebraic and combinatorial properties of graphs. It is well known that these connections can be far from tight. For example, two given graphs may have approximately the same cuts, but significantly different eigenvalues and eigenvectors. As a result, spectral algorithms for cut problems on graphs, albeit very fast, do not provide good approximation guarantees. This project will extend aspects of spectral graph theory to a spectral theory for cut structures, defined as sets of graphs with approximately prescribed cuts. The central question of the new theory is:  What kind of spectral properties can be realized by graphs within a given cut structure?<br/><br/>A goal of the project is to show that any cut structure contains graphs whose eigenvectors provide tight information about its cuts.  The project will also study algorithms for the efficient computation of these special graphs, by essentially modifying the spectrum of an input graph without significantly altering its cuts. Then, the combination of spectral modification and classical spectral algorithms will yield fast algorithms with enhanced approximation guarantees.  The project will draw from connections of spectral graph theory with graph decompositions discovered in the context of oblivious routing algorithms. In turn, it is expected that the project will have an impact on routing problems too. In later stages the project will study the theoretical limits of spectral modification. It will also examine the descriptive quality of the developed theory in the performance of algorithms and other phenomena on interesting classes of graphs, such as social or biological networks.<br/><br/>The project will freely disseminate prototype implementations of the new algorithms and will apply them to computer vision and machine learning problems in industry and academia. Applications will be pursued via selected interdisciplinary collaborations. The balance between theoretical and applied work will serve a broader educational effort at both the undergraduate and graduate level, which will also include the introduction of new courses. A significant part of the research will be carried out at the University of Puerto Rico, and so the project is expected to have a significant impact in the education of underrepresented minorities."
"1161123","Collaborative Research: Automatic Behavior Monitoring for In-depth Analysis of Construction Fatalities and Injuries","CMMI","CIS-Civil Infrastructure Syst","05/01/2012","08/20/2012","SangHyun Lee","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","David Mendonca","04/30/2016","$243,672.00","Silvio Savarese","shdpm@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","ENG","1631","029E, 036E, 039E, 116E, 9178, 9251","$0.00","The objective of this research is to explore computer vision-based monitoring methods, enabling the automatic and constant observation of construction workers for in-depth safety analysis. Taking into account the fact that the number of fatalities in construction remains the highest among all industries, and that approximately 80 to 90 percent of accidents are strongly associated with workers' unsafe behavior and acts, the automatic capture and systematic understanding of unsafe behavior has great potential to contribute to the reduction and prevention of injuries and fatalities in construction. Specifically, using video and image sequences, the proposed system estimates the 2D location of a human skeleton, computes the 3D location of body joints, and identifies the worker?s unsafe motions using machine learning techniques. To investigate the feasibility and potential of the proposed methods for behavior monitoring, several representative motions in traumatic (e.g., falls) and ergonomic (e.g., overexertion and repetitive motions) injuries are tested as a case study. <br/><br/>If successful, the findings of this research could lead to the prevention of injuries and fatalities in the construction industry by providing an in-depth understanding of human behavior and actions in terms of safety. Further, the motion analysis techniques developed in this project can be applied to diverse industries (e.g., manufacturing and shipbuilding) where labor is an important resource, providing a means to automatically collect data on human behavior and thus enabling its effective understanding. In addition, the education plan (e.g., the course that deals with understanding human behavior in safety, and the workshop planned for industry professionals and students) will provide effective education of future managers and engineers, both of whom will improve safety in the US workplace. Further, female and underrepresented students will be recruited and integrated into the planned research and education activities (e.g., safety seminars and workshops and interdisciplinary research participation opportunities)."
"1200120","Collaborative Research: Automatic Behavior Monitoring for In-depth Analysis of Construction Fatalities and Injuries","CMMI","CIVIL INFRASTRUCTURE SYSTEMS","05/01/2012","03/04/2012","Feniosky Pena-Mora","NY","Columbia University","Standard Grant","Elise D. Miller-Hooks","04/30/2015","$68,328.00","","feniosky@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","ENG","1631","029E, 036E, 039E, 9102","$0.00","The objective of this research is to explore computer vision-based monitoring methods, enabling the automatic and constant observation of construction workers for in-depth safety analysis. Taking into account the fact that the number of fatalities in construction remains the highest among all industries, and that approximately 80 to 90 percent of accidents are strongly associated with workers' unsafe behavior and acts, the automatic capture and systematic understanding of unsafe behavior has great potential to contribute to the reduction and prevention of injuries and fatalities in construction. Specifically, using video and image sequences, the proposed system estimates the 2D location of a human skeleton, computes the 3D location of body joints, and identifies the worker?s unsafe motions using machine learning techniques. To investigate the feasibility and potential of the proposed methods for behavior monitoring, several representative motions in traumatic (e.g., falls) and ergonomic (e.g., overexertion and repetitive motions) injuries are tested as a case study. <br/><br/>If successful, the findings of this research could lead to the prevention of injuries and fatalities in the construction industry by providing an in-depth understanding of human behavior and actions in terms of safety. Further, the motion analysis techniques developed in this project can be applied to diverse industries (e.g., manufacturing and shipbuilding) where labor is an important resource, providing a means to automatically collect data on human behavior and thus enabling its effective understanding. In addition, the education plan (e.g., the course that deals with understanding human behavior in safety, and the workshop planned for industry professionals and students) will provide effective education of future managers and engineers, both of whom will improve safety in the US workplace. Further, female and underrepresented students will be recruited and integrated into the planned research and education activities (e.g., safety seminars and workshops and interdisciplinary research participation opportunities)."
"1247368","EAGER: A Research Infrastructure for Analyzing Speech-based Interfaces","IIS","Robust Intelligence","08/01/2012","09/14/2012","Florian Metze","PA","Carnegie-Mellon University","Standard Grant","Tatiana Korelsky","01/31/2014","$100,000.00","Matthew Kam","fmetze@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7495","7916","$0.00","The research community's understanding of the speech-to-text problem has reached a point at which most challenges can in principle be met, given a baseline system, enough data from the target domain, and an expert, who knows how to develop or adapt a recognizer for the target context-of-use. Unfortunately, this approach does not scale: despite the growing interest in speech-user interfaces, there are a limited number of experts equipped to analyze and develop an accurate speech recognizer.<br/><br/>This Early Grant for Exploratory Research explores the possibility of formalizing a speech recognition expert's implicit knowledge of the required analysis and development steps in a rule-based knowledge base, which can help a speech recognition non-expert develop a speech recognizer as part of an application, such as a dialog system in a rare dialect. Speech recognition experts adapt and improve recognizers by listening to data, aggregating error reports, and then adjusting parameters, retraining models, or applying adaptation techniques, based on their assessment of the mismatched context of use. This project extracts intuition from contextual interviews with such experts, develops a proof-of-concept expert system to predict the gains a system would see from specific adaptation techniques, and explores the factors which will make this approach feasible.<br/><br/>This project creates ways to make development of speech-enabled applications more accessible to a broader class of researchers, students, and practitioners, particularly from the user interface area. It will make joint development of user interface and speech recognition feasible, without requiring large teams with varied skill-sets."
"1149225","CAREER: New Models, Representations, and Dimensionality Reduction Techniques for Structured Data Sets","CCF","Comm & Information Foundations","05/01/2012","06/01/2016","Michael Wakin","CO","Colorado School of Mines","Continuing grant","Phillip Regalia","04/30/2018","$400,000.00","","mwakin@mines.edu","1500 Illinois","Golden","CO","804011887","3032733000","CSE","7797","1045, 7936","$0.00","ABSTRACT:<br/><br/>A significant byproduct of the modern Information Age has been an explosion in the sheer quantity of data demanded from sensing systems. This project focuses on developing effective new frameworks for data acquisition, processing, and understanding that will help meet the technological challenges posed by this ever growing demand for information. Possible areas of impact include, but are not limited to: social choice theory, recommendation engines, sensor networks, computer vision, LIDAR, machine learning, medical imaging, drug discovery, and neuroscience. Integrated with the research in this project are the educational goals to inspire and educate students by creating and disseminating new curricular and K-12 outreach materials that focus both on the challenges of high-dimensional data processing and on the principles behind the dimensionality reduction techniques for alleviating them.<br/><br/>The research in this project draws from the concepts of sparsity and geometry in pursuing theoretically sound, integrated models and representations for broad classes of natural data. Of particular interest are (i) pairwise comparison matrices, which arise in a number of applications including recommendation engines, economic exchanges, elections, and psychology but are inadequately captured by low-rank models, and (ii) point clouds, which arise in signal and image databases and sensor networks but for which current models fail to properly capture intra- and inter-signal structures. In order to help mitigate the challenges in collecting and storing high-dimensional data sets (including those above), this project is developing principled techniques for recovering matrix-structured data sets from partial information that exploit far richer models than conventional low-rank recovery techniques."
"1247809","EAGER: Collaborative Research: Towards Modeling Human Speech Confusions in Noise","IIS","Robust Intelligence","08/01/2012","08/08/2012","Abeer Alwan","CA","University of California-Los Angeles","Standard Grant","Tatiana Korelsky","07/31/2015","$100,000.00","Jody Kreiman","alwan@ee.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","7495","7495, 7916","$0.00","This EArly-concept Grant for Exploratory Research (EAGER) supports an exploratory study to evaluate model components for prediction of human speech recognition in the presence of noise. Such a model has the potential to predict confusions between fine phonetic distinctions in different levels of background noise and at different speaking rates. The study takes advantage of modern physiological results that indicate that the primary auditory cortex performs spectro-temporal filtering; that is, that there are cells that are sensitive to particular spectro-temporal modulations at each auditory frequency. In this project, perceptual experiments in the presence of both stationary and non-stationary additive noise and at different signal-to-noise ratios for a database of CVC syllables recorded at 2 different speaking rates yield confusion statistics. These statistics are then compared to those resulting from an auditory model enhanced by elements incorporating these spectro-temporal filters. <br/><br/>Successful results from this study will suggest enhancements to current hearing models and ultimately, after a broader study for which this EAGER is a pilot, advance the understanding of human speech perception. Background noise presents a challenging problem for a variety of speech and hearing devices including hearing aids and automatic speech recognition (ASR) systems. Since normal-hearing human listeners are extremely adept at perceiving speech in noise, this improved understanding of human models could lead to better artificial systems for speech processing. The databases and tools developed for this study will be disseminated to the research community."
"1244687","BCC-SBE: Seeing Speech: Building a Community","BCS","Data Infrastructure","09/15/2012","01/15/2013","Diana Archangeli","AZ","University of Arizona","Standard Grant","John Yellen","08/31/2016","$255,272.00","","dba@email.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","SBE","8068","7433, 9179, 9251","$0.00","Human language integrates several complex systems. Sound is the most accessible of these, with measurable acoustic and articulatory properties, yet it is poorly understood because it involves multiple interacting modalities: lips, velum, tongue, and larynx all conspire to give each sound its unique properties. Understanding how sound works in language is important to basic linguistic and cognitive sciences as well as to applied research such as speech sciences and automatic speech recognition.<br/><br/>Technological advances make collecting articulatory data relatively trivial, while technology for annotation and analysis lags behind.  In response, we are developing (i) a software suite for simultaneous extraction and automatic analysis of articulatory speech data (UltraPraat), integrated seamlessly with the premier software for acoustic analysis of speech (Praat); and (ii), a database coupling articulatory and acoustic speech data (UltraSpeech) to support development and evaluation of theories of acoustic and articulatory phonetics, based on an existing database such as TIMIT.<br/><br/>The success of such a venture depends on both the quality of execution and on how well it is received by the community. The goal of this project is to determine the key properties for both software and database that, if developed, would be most beneficial to the community of users.  This project will create a prototype of the analytic software, develop the community of potential users, and refine software and database specifications to best meet the community's needs. The outcomes will be a working prototype, a community of researchers who understand its benefits, and a set of specifications for the infrastructure.<br/><br/>This project brings together language researchers with a wide range of interests in human articulation. The discussions will ensure that the infrastructure developed to fill the current gap in articulatory data and analysis software will provide as much benefit as possible to all the related fields in technology and the sciences; they may also give rise to new research synergies. The prototype will be made available publicly, so others may begin using the results of this project for research, even before the full model is complete. UltraPraat and UltraSpeech have the potential for transforming research in all domains of oral tract articulation, such as the diversity of human language sounds, language acquisition and endangered languages, speech deficits, foreign language pronunciation, oral language for the profoundly deaf, speech recognition and synthesis software, and how musicians shape sounds while playing wind instruments."
"1218159","RI: Small: Collaborative Research: 'Houston, We Have a Solution': Novel Speech Processing Advancements for Analysis of Large Asynchronous Multi-Channel Audio Corpora","IIS","ROBUST INTELLIGENCE","09/01/2012","08/09/2012","Douglas Oard","MD","University of Maryland College Park","Standard Grant","Tatiana Korelsky","08/31/2016","$84,102.00","","oard@glue.umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","7495","7923","$0.00","This project is focused on developing new speech processing techniques which will transform access to large asynchronous multi-channel and diverse collections of multimedia materials. In particular, the algorithms developed are being employed to create a novel multi-source and multi-scale event reconstruction system that brings together the massive archives of the Apollo lunar missions, to create experiential interaction with historical materials. Specific research advancements are focused on state of the art acoustic environment analysis, speech recognition including keyword spotting, speaker identification under adverse conditions, multimodal content alignment, and automated linking for events and entities from spoken content. Specifically, the research is developing: (i) new techniques for noise- and channel-robust acoustic processing, exploiting missing-features concepts with novel feature extraction and compensation techniques, (ii) a new articulatory framework for speech recognition for robustness to variations in speech production, (iii) environmental ""sniffing"" techniques to automatically characterize acoustic environments to improve robustness, and (iv) automatic detection of novel task-specific audio-events. Since the data is asynchronous, unique speech analytics techniques are being formulated to address the large number of ""local loop"" intercom circuits in the NASA Mission Control Center, audio recorded onboard the two Apollo spacecrafts during specific mission events, and space-to-ground radio circuits. The specific speech, language, and knowledge extraction advancements will be integrated into a new automated evaluation model that reflects specific challenges encountered in the event reconstruction task. This platform will be deployed and evaluated by actual users from the Science and Engineering Education Center (SEEC) of the University of Texas at Dallas. <br/><br/>Integration of robust speech processing algorithms with event reconstruction systems will have a direct and immediate impact on education, society, and government organizations. Working with NASA's Apollo mission data allows for the development of speech technology for challenging audio that contains severe communication channel artifacts, cross-talk/static/tones, and low signal-to-noise ratios. The software being developed in this project will be made available to any non-profit organization for use in audio/video search (download with training modules). Students working on senior design teams will also develop a Contact Science station to be deployed in Dallas, TX and overseen by the University of Texas in Dallas Science and Engineering Education Center to illustrate and assess student use of the advancements.  As a lasting legacy for this project, this project team includes eminent historians of human space flight, who will explore opportunities to deploy this event reconstruction system in a museum setting where it can support both scholarship and public engagement, and we will make the system itself available on an open-source basis to support other researchers."
"1248047","EAGER: Collaborative Research: Towards Modeling Human Speech Confusions in Noise","IIS","Robust Intelligence","08/01/2012","08/08/2012","Nelson Morgan","CA","International Computer Science Institute","Standard Grant","Tatiana Korelsky","07/31/2015","$100,000.00","","morgan@icsi.berkeley.edu","1947 CENTER ST STE 600","Berkeley","CA","947044115","5106662900","CSE","7495","7495, 7916","$0.00","This EArly-concept Grant for Exploratory Research (EAGER) supports an exploratory study to evaluate model components for prediction of human speech recognition in the presence of noise. Such a model has the potential to predict confusions between fine phonetic distinctions in different levels of background noise and at different speaking rates. The study takes advantage of modern physiological results that indicate that the primary auditory cortex performs spectro-temporal filtering; that is, that there are cells that are sensitive to particular spectro-temporal modulations at each auditory frequency. In this project, perceptual experiments in the presence of both stationary and non-stationary additive noise and at different signal-to-noise ratios for a database of CVC syllables recorded at 2 different speaking rates yield confusion statistics. These statistics are then compared to those resulting from an auditory model enhanced by elements incorporating these spectro-temporal filters. <br/><br/>Successful results from this study will suggest enhancements to current hearing models and ultimately, after a broader study for which this EAGER is a pilot, advance the understanding of human speech perception. Background noise presents a challenging problem for a variety of speech and hearing devices including hearing aids and automatic speech recognition (ASR) systems. Since normal-hearing human listeners are extremely adept at perceiving speech in noise, this improved understanding of human models could lead to better artificial systems for speech processing. The databases and tools developed for this study will be disseminated to the research community."
"1219130","RI: Small: Collaborative Research: 'Houston We Have A Solution': Novel Speech Processing Advancements for Analysis of Large Asynchronous Multi-Channel Audio Corpora","IIS","ROBUST INTELLIGENCE","09/01/2012","01/27/2016","John H. L. Hansen","TX","University of Texas at Dallas","Standard Grant","Tatiana D. Korelsky","12/31/2016","$393,309.00","Abhijeet Sangwan","John.Hansen@utdallas.edu","800 W. Campbell Rd., AD15","Richardson","TX","750803021","9728832313","CSE","7495","7495, 7923, 9251","$0.00","This project is focused on developing new speech processing techniques which will transform access to large asynchronous multi-channel and diverse collections of multimedia materials. In particular, the algorithms developed are being employed to create a novel multi-source and multi-scale event reconstruction system that brings together the massive archives of the Apollo lunar missions, to create experiential interaction with historical materials. Specific research advancements are focused on state of the art acoustic environment analysis, speech recognition including keyword spotting, speaker identification under adverse conditions, multimodal content alignment, and automated linking for events and entities from spoken content. Specifically, the research is developing: (i) new techniques for noise- and channel-robust acoustic processing, exploiting missing-features concepts with novel feature extraction and compensation techniques, (ii) a new articulatory framework for speech recognition for robustness to variations in speech production, (iii) environmental ""sniffing"" techniques to automatically characterize acoustic environments to improve robustness, and (iv) automatic detection of novel task-specific audio-events. Since the data is asynchronous, unique speech analytics techniques are being formulated to address the large number of ""local loop"" intercom circuits in the NASA Mission Control Center, audio recorded onboard the two Apollo spacecrafts during specific mission events, and space-to-ground radio circuits. The specific speech, language, and knowledge extraction advancements will be integrated into a new automated evaluation model that reflects specific challenges encountered in the event reconstruction task. This platform will be deployed and evaluated by actual users from the Science and Engineering Education Center (SEEC) of the University of Texas at Dallas. <br/><br/>Integration of robust speech processing algorithms with event reconstruction systems will have a direct and immediate impact on education, society, and government organizations. Working with NASA's Apollo mission data allows for the development of speech technology for challenging audio that contains severe communication channel artifacts, cross-talk/static/tones, and low signal-to-noise ratios. The software being developed in this project will be made available to any non-profit organization for use in audio/video search (download with training modules). Students working on senior design teams will also develop a Contact Science station to be deployed in Dallas, TX and overseen by the University of Texas in Dallas Science and Engineering Education Center to illustrate and assess student use of the advancements.  As a lasting legacy for this project, this project team includes eminent historians of human space flight, who will explore opportunities to deploy this event reconstruction system in a museum setting where it can support both scholarship and public engagement, and we will make the system itself available on an open-source basis to support other researchers."
"1218056","HCC: Small: Collaborative Research: Real-Time Captioning by Groups of Non-Experts for Deaf and Hard of Hearing Students","IIS","HCC-Human-Centered Computing","08/01/2012","05/18/2016","Raja Kushalnagar","NY","Rochester Institute of Tech","Standard Grant","Ephraim Glinert","07/31/2016","$129,554.00","","raja.kushalnagar@gallaudet.edu","1 LOMB MEMORIAL DR","ROCHESTER","NY","146235603","5854757987","CSE","7367","7367, 7923, 9251","$0.00","Many deaf and hard of hearing students use real-time captioning to participate in education.  Generally, real-time captions are provided by skilled professional captionists (stenographers) who use specialized keyboards or software to keep up with natural speaking rates of up to 225 words per minute.   But professional captionists are expensive and must be arranged in advance in blocks of at least an hour.  Automatic speech recognition (ASR) is improving, but still experiences high error rates in real classrooms.  In this collaborative effort involving the University of Rochester and Rochester Institute of Technology, the PIs will address these issues by blending human- and machine-powered captioning to produce captions on demand, in real time, for low cost.  The PIs' approach is for multiple non-experts and ASR to collectively caption speech in under 5 seconds, with the help of interfaces which encourage quick, incomplete captioning of live audio.  Because non-experts cannot keep up with natural speaking rates, new algorithms will merge incomplete captions in real time. (While the sequence alignment problem can be solved exactly with dynamic programming, existing approaches are too slow, are not robust to input error, and do not incorporate natural language semantics.)  Systematically varying audio saliency will encourage complete coverage of speech.  Non-expert captions will train ASR engines in real time, so that ASR may improve during a lecture. (Traditional approaches for ASR training assume that training occurs offline.)  The quikCaption mobile application will embody these ideas and will be iteratively designed with deaf and hard of hearing students at the National Technical Institute of the Deaf (NTID) via design sessions, lab studies and in-class deployments.  Non-expert captionists can be drawn from broad sources: volunteers willing to donate their time, classmates with relevant domain knowledge, or always-available paid workers.  They may be local (in the classroom) or remote.  Captionists may have experience from prior quikCaption sessions, or novice crowd workers recruited on demand from existing marketplaces (e.g., Mechanical Turk).  A flexible worker pool will allow real-time captions to be available on demand at low cost and for only as long as needed.<br/><br/>Broader Impacts:  This research will dramatically improve education for deaf and hard of hearing students by enabling access to serendipitous opportunities, such as conversations after class or last-minute guest lectures for which no interpreter or captionist was arranged.  Real-time captioning will also be useful in other settings such as school programs, artistic performances, and political events.  Older hard of hearing adults usually prefer captioning, and represent a sizable and growing population; hearing people may benefit because captioning is a first step in automatic translation of aural speech.  The algorithms developed as part of this project for real-time merging of incomplete natural language will likely be adaptable for other applications such as collaborative translation or communication over noisy mediums."
"1218209","HCC: Small: Collaborative Research: Real-Time Captioning by Groups of Non-Experts for Deaf and Hard of Hearing Students","IIS","HCC-Human-Centered Computing","08/01/2012","07/16/2013","Jeffrey Bigham","NY","University of Rochester","Continuing Grant","Ephraim Glinert","09/30/2014","$419,243.00","Daniel Gildea","jbigham@cmu.edu","518 HYLAN, RC BOX 270140","Rochester","NY","146270140","5852754031","CSE","7367","7367, 7923","$0.00","Many deaf and hard of hearing students use real-time captioning to participate in education.  Generally, real-time captions are provided by skilled professional captionists (stenographers) who use specialized keyboards or software to keep up with natural speaking rates of up to 225 words per minute.   But professional captionists are expensive and must be arranged in advance in blocks of at least an hour.  Automatic speech recognition (ASR) is improving, but still experiences high error rates in real classrooms.  In this collaborative effort involving the University of Rochester and Rochester Institute of Technology, the PIs will address these issues by blending human- and machine-powered captioning to produce captions on demand, in real time, for low cost.  The PIs' approach is for multiple non-experts and ASR to collectively caption speech in under 5 seconds, with the help of interfaces which encourage quick, incomplete captioning of live audio.  Because non-experts cannot keep up with natural speaking rates, new algorithms will merge incomplete captions in real time. (While the sequence alignment problem can be solved exactly with dynamic programming, existing approaches are too slow, are not robust to input error, and do not incorporate natural language semantics.)  Systematically varying audio saliency will encourage complete coverage of speech.  Non-expert captions will train ASR engines in real time, so that ASR may improve during a lecture. (Traditional approaches for ASR training assume that training occurs offline.)  The quikCaption mobile application will embody these ideas and will be iteratively designed with deaf and hard of hearing students at the National Technical Institute of the Deaf (NTID) via design sessions, lab studies and in-class deployments.  Non-expert captionists can be drawn from broad sources: volunteers willing to donate their time, classmates with relevant domain knowledge, or always-available paid workers.  They may be local (in the classroom) or remote.  Captionists may have experience from prior quikCaption sessions, or novice crowd workers recruited on demand from existing marketplaces (e.g., Mechanical Turk).  A flexible worker pool will allow real-time captions to be available on demand at low cost and for only as long as needed.<br/><br/>Broader Impacts:  This research will dramatically improve education for deaf and hard of hearing students by enabling access to serendipitous opportunities, such as conversations after class or last-minute guest lectures for which no interpreter or captionist was arranged.  Real-time captioning will also be useful in other settings such as school programs, artistic performances, and political events.  Older hard of hearing adults usually prefer captioning, and represent a sizable and growing population; hearing people may benefit because captioning is a first step in automatic translation of aural speech.  The algorithms developed as part of this project for real-time merging of incomplete natural language will likely be adaptable for other applications such as collaborative translation or communication over noisy mediums."
"1150028","CAREER: Communicative Efficiency and Adaptiveness in the Ideal Speaker","IIS","Perception, Action & Cognition, Robust Intelligence","01/01/2012","12/22/2016","T. Florian Jaeger","NY","University of Rochester","Continuing grant","D.  Langendoen","12/31/2017","$567,843.00","","fjaeger@bcs.rochester.edu","518 HYLAN, RC BOX 270140","Rochester","NY","146270140","5852754031","CSE","7252, 7495","1045, 7495, 9251","$0.00","Human communication is typically robust even at high speeds. This suggests that both speakers and listeners efficiently deal with the uncertainty and noise inherent to perception, production, and the environment. This CAREER award investigates how the human brain accomplishes this. A mathematical model of efficient communication based on probability and information theory (the Ideal Speaker model) is tested against data from conversational speech. Specifically, the project investigates how the pronunciation of words in spontaneous speech depends on words' expected confusability in context, the cognitive load the speaker is under and the situational incentive for robust communication. The Ideal Speaker model also predicts that efficient communication with a particular interlocutor requires adaptation to that interlocutor, a prediction that the project tests in behavioral paradigms against task-oriented speech production. <br/><br/>The project contributes to our understanding of how humans produce language, why language has the properties it has, and to what extent the neural systems underlying language production can adjust to different communicative task demands. These insights can contribute to the development of better automatic speech recognition systems (this project is limited to the evaluation of such systems). In addition, novel paradigms to gather large amounts of language data are developed that will dramatically cut research costs. Finally, training in the emerging field of computational psycholinguistics is provided to a broad international audience via summer schools and workshops. This will contribute to a new generation of multidisciplinary scientists working across traditional boundaries between computer science, linguistics, and cognitive psychology."
"1161505","SHF: Medium: Collaborative Research: Ultra-Responsive Architectures for Mobile Plattorm","CCF","Software & Hardware Foundation","05/01/2012","08/29/2013","Thomas Wenisch","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","tao li","04/30/2017","$560,000.00","Marios Papaefthymiou, Kevin Pipe","twenisch@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7798","7924, 7941","$0.00","Conventional microprocessors are designed primarily for sustained<br/>performance; they can operate at near-peak performance essentially<br/>indefinitely until their energy source is exhausted. In battery and<br/>cooling constrained environments such as mobile devices, sustained power<br/>(and, consequently, peak performance) must be limited to at most a few<br/>watts so that the device can dissipate heat using only passive<br/>convection. However, many interactive mobile applications (such as<br/>handwriting/speech recognition or image-based search) instead call for<br/>bursts of intense computation in response to user input, creating the<br/>need for a new ultra-responsive operating regime: rather than limit peak<br/>power assuming sustained operation, systems should instead exploit heat<br/>storage to enable brief computation bursts that greatly exceed<br/>sustainable thermal limits without overheating.  This project<br/>investigates an approach called ""computational sprinting"", the central<br/>essence of which is to compute at unsustainable rates but only briefly<br/>so that temperatures do not reach unsafe levels.<br/><br/>The goal of this project is to address the architectural, thermal,<br/>electrical, and software barriers to ultra-responsiveness via<br/>computational sprinting. In particular, the project explores: (1)<br/>architectures and memory systems that sprint via parallelism (activating<br/>tens of reserve functional units/cores) and voltage boosting<br/>(overdriving cores for single-thread performance) while facilitating<br/>fast burst onsets; (2) thermal designs that improve thermal response<br/>behavior to enable longer and more intense sprints; (3) mobile-optimized <br/>electrical designs that provide stable supply voltages<br/>despite current surges of an order-of-magnitude or more; and <br/>(4) software mechanisms that explicitly manage limited thermal budgets <br/>and anticipate and stage data needed during the sprint.  The project<br/>includes the fabrication of an experimental testbed to approximate the<br/>computational, thermal and electrical capabilities of a future many-core<br/>mobile device and to provide practical experience with sprinting.<br/>Project impacts include (1) developing and advancing techniques for<br/>improving the responsiveness of mobile devices and (2) integrating the<br/>discoveries into a new cross-departmental course on computer system<br/>design."
"1161681","SHF: Medium: Collaborative Research: Ultra-Responsive Architectures for Mobile Platforms","CCF","SOFTWARE & HARDWARE FOUNDATION","05/01/2012","09/02/2015","Thomas Wenisch","PA","University of Pennsylvania","Continuing grant","tao li","02/29/2016","$296,250.00","","twenisch@umich.edu","Research Services","Philadelphia","PA","191046205","2158987293","CSE","7798","7924, 7941","$0.00","Conventional microprocessors are designed primarily for sustained<br/>performance; they can operate at near-peak performance essentially<br/>indefinitely until their energy source is exhausted. In battery and<br/>cooling constrained environments such as mobile devices, sustained power<br/>(and, consequently, peak performance) must be limited to at most a few<br/>watts so that the device can dissipate heat using only passive<br/>convection. However, many interactive mobile applications (such as<br/>handwriting/speech recognition or image-based search) instead call for<br/>bursts of intense computation in response to user input, creating the<br/>need for a new ultra-responsive operating regime: rather than limit peak<br/>power assuming sustained operation, systems should instead exploit heat<br/>storage to enable brief computation bursts that greatly exceed<br/>sustainable thermal limits without overheating.  This project<br/>investigates an approach called ""computational sprinting"", the central<br/>essence of which is to compute at unsustainable rates but only briefly<br/>so that temperatures do not reach unsafe levels.<br/><br/>The goal of this project is to address the architectural, thermal,<br/>electrical, and software barriers to ultra-responsiveness via<br/>computational sprinting. In particular, the project explores: (1)<br/>architectures and memory systems that sprint via parallelism (activating<br/>tens of reserve functional units/cores) and voltage boosting<br/>(overdriving cores for single-thread performance) while facilitating<br/>fast burst onsets; (2) thermal designs that improve thermal response<br/>behavior to enable longer and more intense sprints; (3) mobile-optimized <br/>electrical designs that provide stable supply voltages<br/>despite current surges of an order-of-magnitude or more; and <br/>(4) software mechanisms that explicitly manage limited thermal budgets <br/>and anticipate and stage data needed during the sprint.  The project<br/>includes the fabrication of an experimental testbed to approximate the<br/>computational, thermal and electrical capabilities of a future many-core<br/>mobile device and to provide practical experience with sprinting.<br/>Project impacts include (1) developing and advancing techniques for<br/>improving the responsiveness of mobile devices and (2) integrating the<br/>discoveries into a new cross-departmental course on computer system<br/>design."
"1215302","ICES: Small: Artificial Human Agents for Virtual Economies","CCF","Economics, Inter Com Sci Econ Soc S (ICE)","08/15/2012","08/09/2012","Yixin Chen","MO","Washington University","Standard Grant","Tracy Kimbrel","07/31/2015","$199,823.00","David Levine","chen@cse.wustl.edu","CAMPUS BOX 1054","Saint Louis","MO","631304862","3147474134","CSE","1320, 8052","7923, 7932","$0.00","The goal of the project is to be able to replace human agents with<br/>artificial agents in studying two-player games. This project, if<br/>successful, will greatly enhance the ability of economists to test<br/>economic theories by partially replacing laboratory experiments with<br/>simulations. Over the last decades laboratory studies have proven<br/>invaluable both for the validation (and invalidation) of economic<br/>theories, and for the practical purpose of testing mechanisms (such as<br/>auctions) in the laboratory prior to practical implementation. The<br/>ability to use simulations with artificial agents in place of<br/>laboratory experiments with live human beings will both reduce the<br/>cost of validation and testing, and make it possible to explore<br/>quickly a much wider range of theories and policy alternatives. It<br/>will also enhance our understanding of human behavior and enrich our<br/>knowledge of the connection between human and artificial intelligence.<br/><br/>Agent-based modeling is an emerging and attractive approach to<br/>validating economic theories. Existing research has focused on simple<br/>and naive agents. This project proposes as the next step to develop<br/>artificial human (economic) agents capable of mimicking the behavior<br/>of human laboratory subjects in the context of two player simultaneous<br/>move games. Substantial and detailed data is available on human play<br/>under these conditions. Existing algorithms fall only slightly short<br/>of the ability to mimic human play but do not yet implement fully<br/>autonomous agents. Based on hidden Markov models, the PIs propose to<br/>develop and investigate a framework of belief learning that is broad<br/>enough to encompass many existing learning algorithms including<br/>reinforcement learning, fictitious play, and smooth fictitious play.<br/>Moreover, based on this framework, the PIs propose to derive more<br/>sophisticated learning methods to fully develop artificial agents. This<br/>research will pursue two important directions. First, the PIs will<br/>introduce the initial calibration of priors based on available<br/>information and a cognitive hierarchy model. Second, the PIs will allow for<br/>the reconsideration of the existing model when ""surprises"" occur. The<br/>project will lead to the next stage of research in both economics and<br/>computer science in broadening the class of artificial agents to<br/>attack broader and more economically important tasks. It will also<br/>enrich the research on graphical model learning for artificial<br/>intelligence."
"1212384","Workshop: Support for Student Participation in the Intelligent User Interface 2012 Conference","IIS","HCC-Human-Centered Computing","02/15/2012","02/09/2012","Sharon Oviatt","WA","Incaa Designs","Standard Grant","Ephraim Glinert","07/31/2012","$19,598.00","","oviatt@incaadesigns.org","11140 Wing Point Drive N.E.","Bainbridge Island","WA","981102976","2068423153","CSE","7367","7367, 7556","$0.00","This is funding to provide financial support for 10 graduate students from universities in the United States (or who are U.S. citizens) to attend the 2012 International Conference on Intelligent User Interfaces (IUI 2012), to be held in Lisbon, Portugal, on February 15-17, 2012, as participants in the doctoral session, presenters in the main conference, and attendees at the conference for general training purposes.  Sponsored by ACM, the annual IUI conferences represent the growing interest in next-generation intelligent and interactive user interfaces; they are the premier forum where researchers from academia and industry, who work at the intersection of Human-Computer Interaction (HCI) and Artificial Intelligence (AI), come together to exchange complementary insights and to present and discuss outstanding research and applications whose goal is to make the computerized world a more amenable place.  Unlike traditional AI the focus is not so much on making the computer smart all by itself, but rather on making the interaction between computers and people smarter.  Unlike traditional HCI, there is a focus on solutions that involve large amounts of knowledge and emerging technologies such as natural language understanding, brain computer interfaces, and gesture recognition.  To this end, IUI encourages contributions not only from computer science but also from related fields such as psychology, cognitive science, computer graphics, the arts, etc.  IUI 2012 will be the 15th conference in the series; topics of interest this year include: Intelligent interactive interfaces, systems, and devices; Ubiquitous interfaces; Smart environments and tools; Human-centered interfaces; Mobile interfaces; Multimodal interfaces; Pen-based interfaces; Spoken and natural language interfaces; Conversational interfaces; Affective and social interfaces; Tangible interfaces; Collaborative multi-user interfaces; Adaptive interfaces; Sensor-based interfaces; User modeling and interaction with novel interfaces and devices; Interfaces for personalization and recommender systems; Interfaces for plan-based systems; Interfaces that incorporate knowledge- or agent-based approaches; Help interfaces for complex tasks; Example- and demonstration-based interfaces; Interfaces for intelligent generation and presentation of information; Intelligent authoring systems; Synthesis of multimodal virtual characters and social robots; Interfaces for games and entertainment; for learning-based interactions and for health informatics; Empirical studies and evaluations of IUI interfaces; New approaches to designing Intelligent User Interfaces. More information about the conference is available at http://iuiconf.org/. <br/><br/>This year IUI is organizing a student Doctoral Consortium (workshop) for the first time to expand student attendance and training in this area (see: http://iuiconf.org/doctoral_consortium.html). The Doctoral Consortium will be held as a day-long event on February 14, 2012, immediately preceding the conference, and is designed to give students exposure to their research community, including an opportunity to present their work to and receive constructive feedback from their peers as well as senior scientist mentors in the field.  The students' work will also be featured during the main conference in the poster session, where they will gain additional experience explaining their work to others in the field.  It is essential that our current generation of students receive good quality training in intelligent interactive interfaces, since such interfaces will be key components of future interactive systems in a variety of domains; therefore, the doctoral consortium is designed to encourage Ph.D. students to start building a professional support network of peers and mentors.  The organizing committee has undertaken to proactively recruit student participants from schools that have not traditionally been well represented in the IUI community.  Priority will be given to funding students representing different institutions (no more than 2 from any given institution); women, minority students, the disabled, and veterans all will be encouraged to participate.<br/><br/>Broader Impacts:  This funding will enable attendance at this conference by students who might otherwise be unable to do so for financial reasons.  It will enhance the educational experience of funded participants, by bringing them into contact with leading researchers in the field and by exposing them to the lively discussion during the course of the conference that often leads to opportunities for career advancement.  The quality of the conference itself will be enhanced as well, thanks to a broadening of the base of institutions represented and increased diversity of participants.  The rich exchange of ideas at IUI has previously proven to be a valuable source of ideas for future research, as well as leading to collaborative efforts; this funding will extend the opportunities for collaboration and provide intellectual stimulus to programs that have previously sent few or no representatives to this conference."
"1152678","SBIR Phase II:  Commercial Development of An Intelligent Modular Robot Platform for Research and Education","IIP","SBIR Phase II","04/01/2012","12/11/2014","Graham Ryland","CA","Barobo, Inc.","Standard Grant","Glenn H. Larsen","12/31/2014","$796,678.00","","gryland@barobo.com","813 Harbor Blvd, Suite 335","West Sacramento","CA","956912201","9167158840","ENG","5373","115E, 116E, 1591, 165E, 169E, 5373, 7218, 8031, 8036, 9102, 9179, 9231, 9251","$0.00","This Small Business Innovation Research (SBIR) Phase II project will study the feasibility for commercialization of an intelligent reconfigurable modular robot system called iMobot, which was originally developed at the University of California, Davis. Robotics has grown beyond automation to encompass systems that are self-reliant, reconfigurable, mobile, intelligent, and aware of their environment. iMobot has four degrees of freedom capable of full mobility and assembly into clusters. Because of its flexibility, modularity, and reconfigurability iMobot is an ideal platform for many research and teaching programs at colleges and universities. iMobot allows researchers to study artificial intelligence, swarm technology, robot collaboration, mobile networking, sensor fusion, gait simulation, and programming for re-configurability. Each module has am open architecture, with a processor capable of embedded Linux. Users can customize software and accessories for their specific needs.  Proposed product feasibility research includes adaptable connectivity between modules, intelligent plug-and-play sensors, a robust and lightweight chassis, along with re-configurability. In this proposed Phase II project, a professional design team will develop necessary technology related to assembling into clusters including mechanical design, electrical interface, sensors, algorithms, control and control software and customer interface.<br/><br/>The broader impact/commercial potential of this project is great. This proposed project will be one of the first attempts to scale up an intelligent reconfigurable modular robot for commercial deployment. The initial market for iMobot will be for university research and teaching. With a standardized hardware base using an open architecture users will be able to more widely share their work with each other, and create a valuable open educational resource. The future release of different iMobot versions will be for life-saving rescue and search operations in the first responder system, and for K-12 education. Robotics is an interdisciplinary field. The unique full mobility and reconfigurability of iMobot are very appealing. Modules can be used alone or in collaboration with others, making it a flexible and scalable educational tool. Because of the homogeneous nature of modular robotics, the cost of manufacturing is reduced through production of a large volume of similar parts. By introducing students to interesting robotic projects with affordable hardware platforms, which involve a variety of math, physics, information technology, and engineering principles, we can excite their imagination and give them confidence to pursue STEM careers, especially for underrepresented and economically disadvantaged groups."
"1238095","CAP: Support for Young Researchers to attend the International Intelligent Tutoring Systems Conference 2012","IIS","Cyberlearn & Future Learn Tech","06/15/2012","06/07/2012","Beverly Woolf","MA","University of Massachusetts Amherst","Standard Grant","Lee Zia","11/30/2013","$20,000.00","","bev@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","8020","7556, 8045, 8055","$0.00","The International Conference on Intelligent Tutoring Systems (ITS) provides a forum for interchange of ideas around the applications of computer science to education and human learning. Presentations at the conference focus on developments and rigorous research around the design and use of interactive and adaptive learning technologies for learners of all ages, for subject matters that span the school curriculum, and for professional applications in industry, the military, and medicine. The conferences promotes cross-fertilization of information and ideas from several cyberlearning related fields: artificial intelligence, cognitive science, education, learning sciences, human-computer interaction, educational technology, psychology, and STEM disciplines.<br/><br/>This project will support travel for advanced graduate students from US universities to attend the 11th International Conference on Intelligent Tutoring Systems (ITS), to be held in Chania, Crete, in Greece, from June 15 to 18, 2012 (http://its2012.teicrete.gr).  Those advanced graduate students will participate in the Young Research Track at the conference. That track is designed to provide young researchers with mentoring beyond what they get at their home institutions that will help them transition from graduate school to a fruitful research career. Young Researcher activities include structured poster sessions in which students present their work and one-on-one mentoring throughout the conference from a senior member of the ITS community who shares research interests with a young researcher and who comes from a different university and has a different approach than the young researcher experiences in his/her home institution. It is expected that conversations between peers and between mentors and mentees will continue throughout each young researcher's career. <br/><br/>This activity supports the mission of NSF to train more advanced professionals in Science, Technology, Engineering, and Mathematics. This conference is unique in its synthesis and cross-fertilization across three STEM capacities: building cutting-edge learning technologies, investigating pedagogical methods that are theoretically grounded in the cognitive, social, and learning sciences, and rigorously testing the learning environments for their effectiveness at promoting learning (in STEM disciplines and other disciplines) among K-12, college, and workplace populations."
"1230187","SBIR Phase II:  Artificial Intelligence Software to Tutor Literary Braille to the Blind and Visually Impaired","IIP","SMALL BUSINESS PHASE II","10/01/2012","04/02/2014","Benny Johnson","PA","Quantum Simulations Incorporated","Standard Grant","Glenn H. Larsen","09/30/2014","$382,028.00","","johnson@quantumsimulations.com","5275 SARDIS RD","MURRYSVILLE","PA","156689536","7247338603","ENG","5373","5373, 8031, 8032, 8042","$0.00","This Small Business Innovation Research (SBIR) Phase II project focuses on developing the first artificial intelligence (AI) educational software to tutor literary Braille to blind/visually impaired students. Braille is the primary medium for written communication for persons who are blind and there has been a dramatic decline in Braille literacy, negatively impacting academic performance, ability to navigate the everyday world and employment opportunities. The ability to bring proven effective AI technology to the table, which is unprecedented in this area of special education, will make a meaningful difference in providing equitable education opportunities to all students, as this project speaks directly to issues of basic literacy. The proposed intervention is an Internet-based adaptive learning system that provides expert instruction on demand during general and special education at school and at home. The software is supplemental to existing curricula, uses standard accessibility technology and integrates directly with existing lessons. In addition to improving learning outcomes for students, this project also includes support for mainstream teachers and teachers of the visually impaired (TVIs). To ensure the product is effective in real-world settings, ongoing formative evaluations with teachers/TVIs will be conducted and student outcomes will be measured during a year two field study.<br/><br/>The broader impact/commercial potential of this project will be the first-ever Braille education software based on AI, delivered on-demand through the Internet. The anticipated impact is that students achieve literacy and are able to perform at a higher level (e.g. academics, daily living, employment) resulting in improved quality of life and increased societal contributions. To have an impact, the product must be affordable, effective for a heterogeneous population in diverse learning environments, easy to use and easily accessed at convenient times and locations in informal and formal educational settings. In SBIR research supported by NIH, Quantum has successfully created the first-ever AI-based educational software that is accessible to the blind (in chemistry and mathematics). Furthermore, Quantum has patented and commercialized unique AI technologies in chemistry and accounting using a business-to-business licensing model that provides educational companies with first-to-market and strong sustainable advantages. This model engages the entire spectrum of educational vendors, offering breakthrough technology that permits increased market share for customers and rapid dissemination to end users. For this project, Quantum will partner with organizations with established channels, who distribute the software as an online service, such as the American Printing House for the Blind, a partner on this project."
"1231683","AAAI/SIGART 2012 Doctoral Consortium","IIS","ROBUST INTELLIGENCE","03/15/2012","03/12/2012","Elizabeth Sklar","CA","Association for the Advancement of Artificial Intelligence","Standard Grant","James Donlon","02/28/2013","$17,631.00","","elizabeth.sklar@kcl.ac.uk","2275 E BAYSHORE RD STE 160","East Palo Alto","CA","943033224","6503283123","CSE","7495","7495, 7556","$0.00","This award supports participation of U.S.-based doctoral students in the 16th SIGART/AAAI Doctoral Consortium to be held July 22-23, 2012 in Toronto, Ontario, Canada in conjunction with the Twenty-Sixth Conference on Artificial Intelligence (AAAI-12), to be held July 22-26. This award provides travel stipends for 12 student participants as well as mentoring lunches, a group dinner, and a poster session. The participants will be selected on the basis of a packet of submitted materials that include a summary of thesis research; a curriculum vitae; and a letter of recommendation from the Ph.D. advisor.<br/><br/>The Doctoral Consortium will extend over two days and will include participant presentations, panel discussions, feedback to participants from assigned mentors, informal discussions (over lunch and during breaks), a group dinner for students and mentors, and a poster session. Each participant will give a 20-minute presentation that will be followed by 20 minutes of discussion led by an assigned mentor to provide feedback on the research and the presentation itself. To help the participants make the transition from being graduate students to launching a successful research program, there will be two panel discussions on relevant career issues: a panel of researchers who have completed their Ph.D.s within the last 5 years will focus on how to finish a dissertation and conduct a job search; and a panel of senior researchers from academia and industry will focus on establishing research funding and presenting oneself during the job search process. In addition, there will be a third panel that will further address issues on research, careers, and funding. All of these activities are in furtherance of the overarching goal of supporting the emerging next generation of researchers. The activities planned for the Doctoral Consortium have significant Intellectual Merit in that they strengthen the scientific quality of the participants' doctoral dissertation projects and provide valuable exposure to additional perspectives on their work at a critical time in their research endeavors and professional development. The Doctoral Consortium has potential for significant Broader Impact by furthering the education of the next generation of scientists, broadening participation in Robust Intelligence research, for instance, by including those from at institutions without strong programs in this area, and by fostering energetic scholarship and collaboration throughout all sectors of the community."
"1216977","EXP: Building a Learning Analytics System to Improve Student Learning and Promote Adaptive Teaching Across Multiple Domains","IIS","TUES-Type 2 Project, REAL, Cyberlearn & Future Learn Tech","09/01/2012","09/05/2012","Marsha Lovett","PA","Carnegie-Mellon University","Standard Grant","Maria Zemankova","08/31/2016","$496,315.00","Christopher Genovese","lovett@cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7511, 7625, 8020","8045, 8841, 9178, SMET","$0.00","This PI team aims to use artificial intelligence to exploit data collected from intelligent tutoring systems to provide feedback both to students and to teachers effectively and at the right times. The team is using a new analytic approach, which introduces hierarchical modeling to learning analytics, to investigate how to better understand students' learning states. Algorithms make valid interpretable and actionable inferences from student-learning data, drawing on cognitive theories and statistics to make it work. As in tutoring systems, analysis is at the level of component skills rather than looking at end performance on a task as a whole. Research is around construction of the algorithms for deducing student learning and student learning states and around learning ways of signaling both to learners and to their teachers what concepts and skills learners understand and are capable of and which they are having trouble with. A learning dashboard will allow teachers to visualize the learning needs of a whole class and adapt activities to student needs. Feedback aimed at learners themselves will help learners recognize activities they need to engage in next to better their skills or understanding. Evaluation will include the degree to which learners development of metacognitive skills when such tools are available. <br/><br/><br/>The proposed work will contribute towards the next generation of intelligent tutoring systems as well as contribute to the data analytics needed to make use of large-scale educational data repositories. Because the Learning Dashboard will be independent of any particular domain, and because metacognition and self-assessment are foregrounded, the Learning Dashboard and what is learned about designing an effective learning dashboard should be applicable across disciplines and classes. The proposal brings together what is known about learning, metacognition, and intelligent tutoring systems to address timely learning analytics issues."
"1229739","Conference: Meeting: Molluscan Neuroscience in the Genomic Era: from Gastropods to Cephalopods, in Jupiter, FL on May 16 - 19, 2012","IOS","Activation","05/15/2012","05/14/2012","David Glanzman","CA","University of California-Los Angeles","Standard Grant","David Coppola","04/30/2013","$15,845.00","","dglanzman@physci.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","BIO","7713","1096, 1228, 9179","$0.00","Historically, many of the most important advances in cellular neurobiology have been made studying mollusks, such as squid and snails, as model systems because mollusks have exceptionally large nerve cells. The giant nerve cells of squid enabled the analysis of the mechanism of the action potentials found in nerve cells and the mechanisms by which neurotransmitters, such as serotonin, are released at synapses. Snails, with their very large nerve cells and simple neural circuitry, have enabled major advances in our<br/>understanding of molecular mechanisms of learning and memory. Many key cellular and molecular discoveries in mollusks have proved generalizable to the brains of mammals, because these mechanisms have been conserved throughout evolution.<br/><br/>This meeting will bring together neurobiologists who work on two important molluscan groups, the simpler gastropods (e.g. snails) and the more complex cephalopods (e.g.squid and octopus) to freely discuss their latest findings. Octopus and squid represent advanced forms of intelligence that evolved entirely independently from vertebrates.  These distinct forms of intelligence have the potential for providing influential models for developing artificial intelligence and computer-controlled precise motion of artificial appendages.<br/><br/>The meeting will generate novel collaborations among attendees, particularly because this is a unique venue for discussions between leaders in gastropod and cephalopod neuroscience. Cellular, molecular and genomic advances in gastropods have the potential for greatly benefiting research in more complex cephalopods. Abstracts of talks will be available on an open website, along with new methodologies presented. This will accelerate the pace of research, leading to new discoveries that can provide insights into the basic functioning of the brain. Graduate students and postdoctoral fellows will present their research to senior scientists in the field, and interact with them at mentoring sessions, which will benefit their development as researchers."
"1239062","EAGER: RI: Collecting and Filtering Online Information in Science","IIS","ROBUST INTELLIGENCE","05/01/2012","06/26/2012","Bruce Buchanan","CA","Association for the Advancement of Artificial Intelligence","Standard Grant","James Donlon","04/30/2013","$21,982.00","","buchanan@cs.pitt.edu","2275 E BAYSHORE RD STE 160","East Palo Alto","CA","943033224","6503283123","CSE","7495","7495, 7916","$0.00","This proposal focuses on developing methods to automate the time-consuming process of selecting good overview and review articles from on-line new sources. The goal is to select articles understandable by persons outside the field and use them on the AITopics information portal maintained by the AAAI. The AITopics website is a widely used authoritative source about Artificial Intelligence that is used by a wide variety of people seeking information about AI: novices, students, the press, etc.  AITopics is intended to provide explanatory, accessible material about multiple subtopics that make up the field of AI. This project will develop new methods for this finding/filtering such articles and build on methods already used by AITopics. The resulting software will be open-source and is intended in the long run to be applicable to other scientific discipline beyond AI, particularly by their professional societies.<br/><br/>The project will pursue two thrusts to generalize the current NewsFinder program, which was developed as a part of AITopics and currently finds many types of news stories about AI. The first thrust of this project is to enable it to find overview and review articles automatically. The second thrust is to enable the program to learn from data collected from readers and administrators."
"1241561","Symposium on Combinatorial Search - 2012","IIS","ROBUST INTELLIGENCE","06/15/2012","06/07/2012","Richard Korf","CA","University of California-Los Angeles","Standard Grant","Hector Munoz-Avila","05/31/2014","$17,000.00","","korf@cs.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","7495","7495, 7556","$0.00","This award supports participation of US-based graduate student researchers in the 2012 Symposium on Combinatorial Search (SoCS 2012) to be held July 19-21, 2012 in Niagara Falls, Canada, just prior to the Twenty-Sixth Conference on Artificial Intelligence (AAAI-12) to be held July 22-26 in Toronto. The funds will help defray the travel, lodging, and registration costs for 13 graduate students as well as two invited speakers. SoCS 2012 will bring together researchers in all aspects of heuristic search and combinatorial optimization and its use in a broad range of areas in AI including core topics of Robust Intelligence like robotics, planning, constraint programming, and in other areas of interest across IIS like bioinformatics. One of the oldest subareas of research in AI, search continues to grow. Its advances include real-time search, parallel search, search using external memory, methods for using inconsistent heuristics, etc. Today it is central to solving problems in areas as diverse as optimal alignment of DNA sequences and real-time navigation. A special focus of SoCS 2012 is path-planning, which is important in robotics and computational biology. SoCS 2012 will include a new grid-based path planning competition."
"1214817","SBIR Phase I: A Novel Human Robot Interaction System Using Affective Theory-of-Mind Computing to Improve User-Responses and Efficacy of Automated Tutoring","IIP","SMALL BUSINESS PHASE I","07/01/2012","10/23/2012","Kino Coursey","TX","Hanson Robokind And Intelligent Bots LLC","Standard Grant","Muralidharan S. Nair","06/30/2013","$150,000.00","","kino.coursey@gmail.com","10515 Lennox Lane","Dallas","TX","752295415","2148084591","ENG","5371","5371, 6840, 8034, 9139, HPCC","$0.00","This Small Business Innovation Research (SBIR) Phase I project advances artificial intelligence (AI) and robotics by developing the foundation for emotionally responsive systems. Building on Theory of Mind research in psychology and robotics, this project focuses on enhancing a robot's capacity to integrate two pieces of information in order to determine a third piece. These inferences can be concrete or as abstract as conjectures about another agent's - including a human user's -beliefs, desires, and intentions. The project creates a framework for the robot's AI system to maintain several competing models of the world (particles) and select and modify those models that have a higher likelihood of matching the real world (filtering). This method of ""Particle Filtering"" is an effective strategy or navigating complex environments. The core research focuses on adapting and assessing particle filtering methods within abstract contexts including knowledge, emotional, and goal states. If successful, as the robot gains additional information through sensors and language processing, it will adapt its models of the users' emotional state and adjust its interactions accordingly. Essentially, this integrative framework will enable robots to understand human desires and frustrations to find creative solutions in response to those needs. <br/><br/>The broader impact/commercial potential of this project is the evolution of an Artificial Intelligence interface that emulates an empathic human-to-human experience. Because the system will represent and track multiple dimensions about a human user, the robot can recognize when a user is confused or frustrated and respond accordingly. This intuitive, naturalistic AI platform has substantial research, education, and therapeutic applications and commercialization channels. With its open source software infrastructure and the relative low-cost of the platform, the initial commercial targets are as a platform for robust Science, Technology, Engineering, and Mathematics (STEM) education and as therapeutic technology for children and adults on the autism spectrum."
"1219142","RI: Small: Semantics-Based, Weakly-Supervised Coreference Resolution","IIS","Robust Intelligence","08/01/2012","06/15/2017","Vincent Ng","TX","University of Texas at Dallas","Continuing grant","Tatiana Korelsky","07/31/2018","$347,410.00","","vince@hlt.utdallas.edu","800 W. Campbell Rd., AD15","Richardson","TX","750803021","9728832313","CSE","7495","7923","$0.00","This research seeks to push the frontiers of coreference resolution research via achieving two objectives. First, it addresses the Winograd Schema Challenge by examining a class of difficult-to-resolve anaphors whose resolution requires commonsense knowledge involving the roles played by the participants in an event and their causal relationships. It adopts a deep text-understanding approach to this problem. Specifically, it ventures into unexplored areas of coreference research, including the use of script-like knowledge and sentiment analysis, as well as an examination of the role of discourse connectives. Second, it enables the acquisition of coreference resolvers for a substantially larger number of natural languages and domains than is currently possible. One of the major obstacles to the deployment of coreference technologies across a large number of languages and domains is the high cost associated with coreference-annotating data in a language and domain. It investigates two cost-effective approaches to data annotation, one involving translation-based projection and the other bootstrapping.<br/><br/>As coreference is an enabling technology for many traditional and emerging text-processing applications, the project has the potential to improve these applications. Through the construction of a multi-lingual, multi-domain coreference resolver and the availability of annotated data produced in the course of this investigation, the project may stimulate research in under-studied languages and domains by a broader community of researchers. Equally importantly, the use of commonsense knowledge in the resolver mimics the human coreference resolution process, bringing artificial intelligence researchers one step closer to building an intelligent agent that can truly understand natural language."
"1315129","EAGER:  Self-Assembly of Complex Systems","CCF","BIO COMPUTING","08/10/2012","12/18/2012","Russell Deaton","TN","University of Memphis","Standard Grant","Mitra Basu","07/31/2014","$142,989.00","","rjdeaton@memphis.edu","Administration 315","Memphis","TN","381523370","9016783251","CSE","7946","7916","$0.00","Abstract for EAGER: Self-Assembly of Complex Systems <br/>Intellectual Merit:  <br/>Self-assembly is a model for how individual components arrange themselves through local interactions to form organized structures. It originated as a model for construction of nanotechnology. The theory of complex systems describes many phenomena in nature, from human intelligence and evolving communities of organisms to human social networks, economies, and cultures. In these systems, complexity emerges in ways that is not immediately obvious from an understanding of the component parts and the relationships between them. In this project, self-assembly will be investigated as a mechanism for creation of complex systems. Self-assembly can be shown to be equivalent to other models of complex systems. In addition, it can be programmed like a computer. A goal of this project is to develop efficient ways to program self-assembly to produce interesting complex systems, which have practical applications. As a beginning to this, we have developed a mapping of self-assembly onto graphs that enables us to use an efficient algorithm to determine the system that is constructed. Thus, self-assembly should be able to generate complex systems and to provide efficient and realistic simulation of those types of systems. In the project, the self-assembly algorithms will be applied to automatic content generation for games, in which the self-assembly automatically creates situations and non-player characters with which players of the game interact. The conjecture is that this will provide more dynamic and realistic game environments, and moreover, will be an interesting test-bed for investigation of the relationship between self-assembly and complex systems.<br/>Broader Impacts:  <br/>This research integrates ideas from chemistry, physics, biology, and computer science to relate self-assembly to complex systems, and to produce potentially transformative tools that will not only improve understanding of complex systems, but also form the basis for innovative complex systems in a variety of application domains. These include nanotechnology, artificial intelligence, art, literature, and computer games. There are many natural phenomena (i.e. human intelligence, living systems) for which traditional symbolic models of computation are only able to capture a part of their essential capabilities and characteristics.  Human language is an example. This research conceivably could result in software that is able to produce target systems that capture some of the capability, adaptability, and complexity that is observed in nature.  If successful, the project could result in a new paradigm for realistic and complex behavior through computer programs, and would potentially impact not only nanotechnology, but also applications that require automatic generation of realistic content. Moreover, our models of self-assembly can generate this content in tractable ways. In addition, under the direction of the investigator, graduate and undergraduate students will work together in a team on this project, and will be educated in the unique multidisciplinary approach that has been proposed."
"1217073","RUI: Uncertainty reduction through better nonlinear particle filters","DMS","COMPUTATIONAL MATHEMATICS","08/15/2012","07/01/2014","Haiyan Cheng","OR","Willamette University","Continuing grant","rosemary renaut","07/31/2016","$132,169.00","","hcheng@willamette.edu","900 State Street","Salem","OR","973013930","5033706049","MPS","1271","9178, 9229, 9251, 9263","$0.00","The objective of this project is to create and implement new methods for improving the performance of filtering, or data assimilation techniques, applied to non-linear, non-Gaussian systems. Many problems in computational statistics, artificial intelligence, and geophysics involve such systems, and utilize specialized Monte Carlo sampling methods, called particle filters, for data analysis and forecasting, and for better understanding of the underlying phenomena. The principal investigator studies variational data assimilation methods to demonstrate that targeted ensemble generation using those methods delivers more effective filter performance, and that dynamic adaptation of the size of the particle ensemble improves the computational efficiency of the filter.<br/><br/>Statistical computations are an essential tool for the solution of problems in such diverse areas as artificial intelligence, industrial and consumer electronics, robotics, weather systems, climate change, ocean ecosystems, and land surface processes.  The proposed research improves statistical computations required for the analysis, estimation, and forecasting of information that arrives over time, and contributes to a better combination of theoretical models with observational data.  In addition, the project involves the training of undergraduate students in applied scientific research.   <br/>"
"1240710","Support for student participation in a 2012 AAAI Fall Series Symposium","IIS","ROBUST INTELLIGENCE","09/01/2012","08/27/2012","Victor Raskin","IN","Purdue University","Standard Grant","Tatiana D. Korelsky","08/31/2014","$12,000.00","Julia Rayz","vraskin@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7495","7495, 7556","$0.00","The American Association for Artificial Intelligence (AAAI) Fall Series Symposium (FSS) is the first international gathering in North America in the field of computational aspects of affective narrative, bringing together participants from a long list of contributing disciplines. One of the aims of the Symposium is to attract students to a new and exciting multidisciplinary area, where it is still easier to attract the experts' attention and mentoring. The goal of this grant is to subsidize travel, registration fees, and housing expenses of students selected to participate in the Symposium which will be held, along with several other AAAI Fall Symposia, on November 2-4, 2012, in Arlington, VA. <br/><br/>The Symposium calls for long and short papers both from leaders in the field of computational aspects of affective narrative and pertinent areas and from graduate students as well as poster presentations from the undergraduate students interested in the subject. Papers from undergraduates, attracted through Research Experience for Undergraduates (REU) and Senior Research Opportunity Program (SROP) networks as well as solo graduate contributions will be carefully mentored to the level of poster or short paper eligibility. The multi-format program of the Symposium will also accommodate special student sessions, especially for promising but not fully developed ideas. <br/><br/>The AAAI FSS Symposium on computational aspects of affective narrative provides a valuable opportunity for the next generation of multidisciplinary researchers in a variety of pertinent disciplines to enter the computational affective narrative research community.  It is expected that the Symposium will attract underrepresented populations as well as provide benefit to future development of computational systems from better understanding of affective narrative as the inherently human phenomenon."
"1219114","RI: Small: A New Approach to Influence Diagram Evaluation","IIS","Robust Intelligence","09/01/2012","07/24/2012","Eric Hansen","MS","Mississippi State University","Standard Grant","Weng-keen Wong","08/31/2016","$445,000.00","","hansen@cse.msstate.edu","PO Box 6156","MISSISSIPPI STATE","MS","397629662","6623257404","CSE","7495","7923, 9150","$0.00","A central goal of research in artificial intelligence, operations research, and related fields is the development of algorithmic approaches to decision making under uncertainty.  This project considers influence diagrams, a widely-used graphical model for representing and solving problems of sequential decision-making under imperfect information.  Influence diagrams were originally developed to provide a more compact representation of a decision problem than is provided by a decision tree, which suffers from an exponential explosion in the number of its branches as a function of the number of variables in the model. Although influence diagrams provide a compact problem representation, standard algorithms for solving influence diagrams do not represent the solution to a decision problem in a similarly compact form. This project addresses this limitation by introducing a more compact graphical representation of decision strategies that improves the scalability of algorithms for solving influence diagrams, makes it easier for a human user to understand the recommended decision strategy, and allows a principled approach to approximation. In addition, the project develops a new approach to solving influence diagrams based on branch-and-bound search, including an incremental approach to probabilistic inference.<br/><br/>The algorithms and software tools developed from this project will improve the scalability and utility of influence diagrams as an approach to automated decision making under uncertainty.  These contributions will have a broad impact in the many disciplines in which influence diagrams are applied, including medical decision analysis and support, therapy plan selection, user modeling, information retrieval, climate change analysis, and many others."
"1154889","Doctoral Dissertation Research: On Negotiating the Role of Computers in Developing Mathematical Proofs","SES","SCIENCE, TECH & SOCIETY","03/15/2012","02/22/2012","Peter Galison","MA","Harvard University","Standard Grant","Frederick M Kronz","02/28/2014","$9,941.00","Stephanie Dick","galison@fas.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","SBE","7603","1353","$0.00","Introduction<br/>Digital computing changed mathematics at every level down to the core of mathematical practice, the search for proofs. This award supports doctoral dissertation research that traces how computers have been used in the development of proofs and how their use changed mathematical knowledge and practice in the United States. The project is motivated historically, anthropologically, and philosophically to explore the following questions. How are mathematical ideas related to the technologies with which they are discovered and explored? How is the cognitive work of mathematics altered by the introduction of computing? How is human intelligence understood through attempts to build reasoning machines? The period of interest begins in the mid-1950s with early Artificial Intelligence, and it concludes with the contemporary work of Fields medalist Vladimir Voevodksy. <br/><br/>Intellectual Merit<br/>This project engages the ongoing interest of Science Studies in knowledge making, especially at the interface between humans and machines. It will explore mathematical knowledge and its material dimensions, which has until now been largely unexplored. Using archival materials, interviews, and technical documents including operations manuals and source code, the doctoral candidate will explore transformations in the institutional setting, material constitution, practices, and cognitive landscape of mathematical research that manifested around the digital computer. The intellectual merit of this project lies in its synthesizing and intervening in theoretical scholarship from science studies, history of mathematics, history of technology, and history of science (often pursued separately). <br/><br/>Potential Broader Impact<br/>The significance of computer proof assistance in mathematical research is intensifying. This project can assist current researchers and funding agencies in the assessment and pursuit of new projects in light of past research and historical context. Further, an analysis of the multifarious past and present strategies for using computing in mathematics can suggest new possibilities for using computer assistance in other disciplines that may otherwise be unaware of them."
"1208143","Advanced Study Institute on Global Healthcare Grand Challenges and Opportunities, July 15 - 30, 2012, Antalya, Turkey","CBET","Engineering of Biomed Systems","05/01/2012","12/20/2011","Metin Akay","TX","University of Houston","Standard Grant","Athanassios Sambanis","04/30/2015","$50,000.00","","makay58@gmail.com","4800 Calhoun Boulevard","Houston","TX","772042015","7137435773","ENG","5345","004E, 017E, 137E, 138E, 7237, 7479","$0.00","Abstract<br/>1208143, Akay<br/><br/>The National Academy of Engineering announced the 14 Grand Challenges for Engineering at the annual meeting of the American Association for the Advancement of Science. The committee also praised biomedical and biological engineering as the research field to fulfill the promise of personalized medicine. Included among these 14 challenges were Reverse-Engineering the Brain, Engineer better Medicines and Advance Health Informatics. An important way of exploiting such information would be through the development of methods that allow doctors to forecast the benefits and side effects of potential treatments or cures. ""Reverse-Engineering"" the Brain, is an emerging discipline that helps us to understand how the brain works and treats several diseases. It furthermore helps us to develop computerized artificial intelligence. Advanced computer intelligence, in turn, should enable automated diagnosis and prescriptions for treatment. Computerized catalogs of health information will enhance the medical system's ability to track the spread of disease and analyze the comparative effectiveness of different approaches to prevention and therapy. Finally, engineering new medicines will help us fight the growing danger of attacks from novel disease-causing agents. For instance, certain deadly bacteria have repeatedly evolved new properties, conferring resistance against even the most powerful antibiotics. New viruses arise with the power to kill and spread more rapidly than disease-prevention systems are designed to counteract.<br/><br/>Intellectual Merits: The main objective of the Advanced Summer Institute on Global Healthcare-- Challenges and Opportunities is to highlight and discuss these emerging grand challenges, mainly focused on the latest advances in the areas of science, engineering, technology and medicine. The institute provides a unique environment to discuss the emerging research areas, challenges and opportunities which lead to very fruitful discussions.<br/><br/>Broader Impacts: It exposes the attendees with biology and medicine backgrounds to the latest developments in these emerging enabling technologies. It is also helpful to those with engineering and science background who are interested in doing research in bionanoscience and nanomedicine, neuroscience and engineering since the advanced institute provides exceptional insights into the fundamental challenges in biology and medicine."
"1155592","Doctoral Dissertation Research:Modeling temporal coordination in speech production using an artificial central pattern generator neural network","BCS","LINGUISTICS","08/01/2012","07/23/2012","Jennifer Cole","IL","University of Illinois at Urbana-Champaign","Standard Grant","William J. Badecker","01/31/2014","$5,298.00","Erin Rusaw","jennifer.cole1@northwestern.edu","1901 South First Street","Champaign","IL","618207406","2173332187","SBE","1311","1311, 9179","$0.00","This project investigates how timing is coordinated in speech production and how this coordination can be computationally modeled using artificial neural networks. Research has shown that speech rhythm reflects a multi-tiered, hierarchical organization of speech units (syllables, accentual units, phrases) that are essentially cyclical in nature. One type of neural network which shares these properties, the central pattern generator (CPG), has been hypothesized to underlie speech timing. This project presents two speech production experiments designed to investigate temporal coordination within and among three levels of speech units in French and English, languages with distinct rhythm types, and to discover which timing properties they share and which are language-specific. A three-level CPG-type artificial neural network is presented which is used to model the results of both experiments in order to test the ability of such a model to simulate the timing behavior of two rhythmically distinct languages. Experiment 1 focuses on the coordination between phrases, accentual units, and syllables through a comparison of the durational effects of lengthening due to phrasal stress and phrase-final syllable lengthening in both languages. Experiment 2 compares the overall temporal coordination of spoken phrases and their constituent parts in the two languages by measuring the changes in articulatory timing of phrases which are repeated multiple times. The main goals of the proposed studies and modeling work are to investigate temporal coordination between the hierarchically structured levels of speech units, specifically how that coordination varies between languages, and to test the ability of a biologically inspired CPG-type neural network model to model this coordination and the speech patterns that result from it.<br/><br/>This work contributes to an understanding of the coordination and timing of rhythm units in speech and how that coordination may be generated by the brain.  In its broader impact, it will enable the integration of theories of speech production with other motor behavior in biological and cognitive models, and help to forge links between research on speech, linguistic prosody, and more broadly, neural models of animal behavior. This work will also contribute to the construction of a unified model of rhythm and prosody which is sufficient to explain both the underlying universalities and the variations of human language. The project will also enrich the training of the graduate student co-PI."
"1258305","EAGER:  Toward Automatic Generation of Challenge-Driven Interactive Narratives","IIS","HCC-Human-Centered Computing","12/01/2012","11/07/2012","Michael Mateas","CA","University of California-Santa Cruz","Standard Grant","William Bainbridge","11/30/2014","$151,044.00","Noah Wardrip-Fruin, Arnav Jhala","michaelm@soe.ucsc.edu","1156 High Street","Santa Cruz","CA","950641077","8314595278","CSE","7367","7367, 7916","$0.00","This project will conduct the first exploration of a computational system that co-generates intertwined narrative and gameplay progressions. The system will utilize a novel representation called ludo-narrative units (LNUs), representing combinations of narrative, gameplay challenges, and choices. Utilizing audience experience models taking into account phenomena such as player agency, narrative comprehension, and gameplay learning objectives, the system will reason about progressions of narrative choices, narrative exposition, and gameplay challenges, and can dynamically reshape content to match designated goals and audience context. To accomplish this, the project will build an artificial intelligence director to construct interactive scenarios for audiences during their engagement with a game-based narrative.<br/><br/>The dynamic construction of scenarios from a pool of authored LNUs would already comprise more flexibility than currently appears in state-of-the-art scenario driven games and other popular interactive narratives, since LNUs can be selected in multiple orders in response to audience activity. But the envisioned system will provide further flexibility and customization through modification of LNU content using story generation technology. Specifically, it will use a story-planning system based on imaginative recall to dynamically provide this content. Further, global narrative and gameplay structure will be provided by using a constraint-solving algorithm to create full experiences satisfying the various system goals, written using answer set programming techniques. This enables the crucial flexibility that allows the system to reason over an arbitrary number of domains without locking it into pre-planned interfaces and infrastructure.<br/><br/>A high-level impact of this research will be contributing to a deeper understanding of these scenarios, as it defines them with the level of rigor required for generation. At a more technical level, the research will produce the first system that generates mixed interactive narrative and gameplay guided by shared experience goals. One of the high-level motivations for pursuing this work is that it has potentially profound impact for education, allowing next-generation educational software to benefit from the motivating power of scenarios and to be generated specifically for the learning profile, goals, and progress of particular learners. It is also potentially the case that experiences generated in this manner, and based on these models, could more effectively reach audiences that have grown up with games as a primary media form."
"1249349","2012 Waterman Award","CCF","Information Technology Researc","09/01/2012","08/29/2012","Scott Aaronson","MA","Massachusetts Institute of Technology","Standard Grant","Dmitri Maslov","08/31/2017","$1,000,000.00","","aaronson@csail.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","1640","0000, 041P","$0.00","The National Science Foundation (NSF) is pleaserd to announce the selection of MIT's Scott Aaronson, an Associate Professor of Electrical Engineering and Computer Science, to receive its 2012 Alan T. Waterman Award.  Dr. Aaronson works with the Computer Science and Artificial Intelligence Laboratory, MIT's largest interdepartmental lab. <br/><br/>The Waterman Award is the National Science Foundation's (NSF) highest honor. The annual award recognizes outstanding researchers under the age of 35 in any field of science or engineering NSF supports. This is the first year that two awardees have been selected. <br/><br/>In addition to a medal, each of this year's awardees will receive a $1 million grant---twice the amount of last year's award---over a five-year period for further advanced study in his field.<br/><br/>Prof. Aaronson, a theoretical computational scientist, pursues research interests that focus on the limitations of quantum computers and computational complexity theory more generally.  His research addresses a variety of topics, including the information content of quantum states, the physical resources needed for quantum computers to surpass classical computers, and the barriers to solving computer science's vexing P versus NP question, that is, whether every problem whose solution can be quickly verified by a computer can also be quickly solved by a computer."
"1157105","REU-RET Site: Willamette Valley Mathematics Research Consortium for Undergraduates and Teachers","DMS","WORKFORCE IN THE MATHEMAT SCI","03/01/2012","02/24/2012","Inga Johnson","OR","Willamette University","Standard Grant","Jennifer Slimowitz Pearl","02/28/2015","$513,069.00","Colin Starr","ijohnson@willamette.edu","900 State Street","Salem","OR","973013930","5033706049","MPS","7335","1359, 9250","$0.00","The Willamette Valley Mathematics Research Consortium for Undergraduates and Teachers is a summer REU-RET program at Willamette University, Linfield College, Lewis & Clark College, and the University of Portland.   Our program consists of four research teams, one per partner institution, each with four students, one teacher, and two faculty mentors for an eight-week REU and seven-week RET. Each team will focus on one of four challenging research projects from a faculty mentors? area of expertise, such as matroid theory, graph theory, combinatorial game theory, stochastic modeling, artificial intelligence, tiling theory, complex algebraic geometry, computational biology, knot theory or digital sensor networks. Consortium meetings will bring together the four research groups regularly to speak about their projects, learn about the progress of their peers, listen to presentations by invited speakers from academia and industry, and network with other students and faculty.  Recruitment for student applicants is nationwide and includes targeted recruitment from underrepresented groups in consultation with the Pacific Northwest Louis Stokes Alliance for Minority Participation. Teacher applicants are recruited regionally in Oregon and southwest Washington.<br/><br/>The primary goal of the Willamette Valley Mathematics Research Consortium for Undergraduates and Teachers is to immerse undergraduates and teachers in a challenging, transformative research experience that will reveal the nature of mathematics research.  Throughout the summer undergraduates and teachers will develop new content knowledge, research skills, and an increased understanding of the process of mathematical discovery and research.  Within their research teams and at Consortium meetings, participants will improve their ability to communicate current mathematical research in an effective and engaging manner.  All participants will develop a greater awareness of career opportunities in STEM fields.  As in previous years, REU research results will be disseminated through student co-authored publications, through public presentations, and online. Teachers will create materials to be used in the classroom and disseminated at professional meetings; the REU-RET provides partial funding for students and teachers to present at conferences. Through these program activities our broader goals are to increase the number of students who pursue advanced study and careers in STEM fields, increase the number of teachers with an understanding of mathematical discovery and research, and increase the effective communication of mathematical research."
"1145949","Diffusional Landscapes for the Study of Neural Differentiation","IOS","Organization, EPSCoR Co-Funding","08/01/2012","07/18/2012","Scott Collins","ME","University of Maine","Standard Grant","Sridhar Raghavachari","07/31/2016","$310,000.00","Rosemary Smith, Gregory Cox","scott.collins@maine.edu","5717 Corbett Hall","ORONO","ME","044695717","2075811484","BIO","7712, 9150","1096, 9150, 9178, 9179","$0.00","During early development of the central nervous system, the global landscape and concentration of chemical species within the spinal cord serve to direct neuron differentiation and development.  The chemical constituency and concentrations of myriad chemical stimulants within the spinal cord signals both the type of neural cells that will develop as well as provide a ""trail of breadcrumbs"" to direct the precise synaptic connections to other neurons.  This research presents a microsystem that generates specific concentration profiles of multiply relevant chemical species to study the development and differentiation of neurons.  Using micro and nanofabrication, a microsystem will be designed, fabricated, characterized, and used to generate chemical profiles of known chemicals that affect spinal cord development.  An anthology of four mouse spinal cord chemical mediators will be studied:  sonic hedgehog (Shh), bone morphogenic protein (BMP), retinoic acid (RA), and noggin, and the spatial and temporal histological distributions of resulting neural cells will be mapped to the imposed chemical mediator concentrations. The advantage of this approach is that cells can be studied in biologically relevant environments without the introduction of the myriad unknown and uncontrolled parameters found in vivo.  <br/><br/>This project will support the education and laboratory research training of both graduate and undergraduate students at the University of Maine in a collaborative, multi-disciplinary research project involving biological, micro/nano fabrication, and engineering methods in instrumentation and techniques. The project will also be highlighted in UMaine's ""Consider Engineering"" summer program for high school students and in several undergraduate and graduate level courses to introduce science & engineering initiates to interdisciplinary research. The microsystem will be extended within these programs/courses to encompass applications involving: cognition models, artificial intelligence, biocomputing, and neural networks."
"1242938","LIVE DEMONSTRATION EVENT at the14TH GEC in BOSTON","CNS","Networking Technology and Syst","06/01/2012","06/15/2012","Karen Sollins","MA","Massachusetts Institute of Technology","Standard Grant","Joseph Lyles","05/31/2013","$45,722.00","","sollins@csail.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7363","","$0.00","The Global Environment for Network Innovations 'GENI' is a suite of research infrastructure supported by the NSF that is rapidly emerging in prototype form across the United States. GENI aims to transform experimental research in networking and distributed systems, as well as emerging research into very large socio-technical systems, by providing a suite of infrastructure for 'at scale' experiments in future internets. <br/><br/>The GENI Project Office organizes three major GENI Engineering conferences (GEC) per year, in which the entire GENI community meets to review current status, and to decide on subsequent steps in GENI's evolution. These GECs include community-based working groups leading GENI's design and planning, and demonstrating progress with live experiments. <br/><br/>MIT is hosting the fourteenth edition of the GENI Engineering conference. This project supports organizing the demo session to be held on the MIT Campus. About 400 leading researchers and Ph.D. students from diverse US institutions will gather at MIT to showcase their ideas and results. The campus venue has an existing 10GB link which will provide access to the National Lambda Rail (NLR) network in addition to MIT's more standard Internet services.  Each demo will be provided with 1G wired connection to the GENI infrastructure.    Additionally, because MIT and its Computer Science and Artificial Intelligence Laboratory (CSAIL) provide wireless access to all guests, wireless will be available for demonstrations and participants.  <br/><br/>Broader Impact: The GEC Demo sessions provide graduate students with both an opportunity to demonstrate and explain their work to the GENI community prior to formal publication. It helps new graduate students understand what is being done with GENI and encourages cross-university cooperation by providing a method for students and faculty to discover who amongst their peers at other institutions might be valuable resources. It also supports outreach to new community members, including the emerging US Ignite community."
"1152898","Analysis of causal cognition in rats","BCS","PERCEPTION, ACTION & COGNITION","06/01/2012","07/30/2013","Aaron Blaisdell","CA","University of California-Los Angeles","Continuing grant","Betty H. Tuller","05/31/2015","$499,996.00","","blaisdell@psych.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","SBE","7252","1045, 7252","$0.00","Humans learn about their world through both personal experience and social communication but complete information is rarely available. In fact, we are remarkably adept at drawing conclusions from only partial information (for example, a doctor diagnosing a disease or a detective solving a crime) and imagining alternate scenarios (an especially useful enterprise for scientists). Like humans, many animals are able to learn about the world through personal experience, as Pavlov showed in dogs and Skinner showed in rats and pigeons. In previous NSF-funded research, the principal investigator showed that rats have the capacity to go beyond direct experience and training and make causal inferences, that is, inferences about a cause-effect relationship they had never previously observed. Causal inferences were much like those of a scientist or a human child in that they originated from the rat's interactions with the world. Rats also act as if they expect a cause to be present even if it is not directly observable. Thus, rats exhibit at least some of the hallmarks of human reasoning. The current research project builds on these results by exploring other ways in which animals are able to reason about their world. Can a rat, like a doctor or detective, determine a cause-effect relationship from only partial and incomplete information? If so, what are the psychological mechanisms that support these causal inferences? To what extent can a rat reason about hidden events in order to solve ambiguous problems? <br/><br/>Answers to these questions can determine aspects of cognition that are uniquely human and aspects that humans share with other animals. Such knowledge may impact the development of artificial intelligence systems designed to make adaptive decisions and inferences based on limited prior knowledge.  The principal investigator also serves as faculty mentor in two programs for high-school students in the greater Los Angeles area (drawn from diverse backgrounds) and will involve the students in this research."
"1156847","REU Site: The Rice University Summer Institute of Statistics (RUSIS)","DMS","WORKFORCE IN THE MATHEMAT SCI","04/15/2012","05/01/2013","Javier Rojo","TX","William Marsh Rice University","Continuing grant","Jennifer Slimowitz Pearl","04/30/2014","$137,686.00","","javier.rojo@oregonstate.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","7335","9250","$0.00","This award provides continued support for a successful 10-week summer REU site within the Statistics Department at Rice University for the study of Statistics and its applications. As the number of domestic graduate students in the Mathematical Sciences continues to decline, there is a critical need to develop human resources to continue supporting the United States' advantage in the world of science and technology. The Rice University Summer Institute of Statistics (RUSIS), now in its 10th year and fourth funding cycle, has been successful in encouraging students to pursue graduate degrees in Mathematics and Statistics. Roughly 85% of the students who have attended RUSIS and have graduated, are now doctoral students in Ph.D. programs around the country, and roughly 61% of them are members of underrepresented populations in Mathematics. RUSIS has accomplished this through intensive courses, close supervision of research projects, and visits to various research institutes and agencies in Houston. <br/><br/><br/>The program will train and mentor 12 (7 NSF- and 5 NSA-supported) selected underrepresented minority students and students with no easy access to a research experience at their institution, including community college students, through intensive core courses in probability, stochastic processes, and statistical inference, with special emphasis on areas of current interest; e.g. multiple comparisons, extreme value theory, multivariate survival analysis, risk-reliability-sustainability of complex infrastructure systems, artificial intelligence, statistical learning, statistical genetics, and general biostatistics. The RUSIS engages the students in research projects under close collaboration with faculty mentors, and with the objective of producing joint publications, when the summer work merits it. Students present their results at national meetings and they are mentored in the preparation and presentation of their talks. In addition, students meet with an advisory committee composed of top scientists and present their work to them. The RUSIS also teaches short courses on the use of Unix platforms, LaTeX, and software to be utilized for research purposes such as Mathematica, Splus and/or R, and Matlab. In addition, the program organizes student and faculty visits to scientific facilities (e.g., Biomathematics and Biostatistics at MD Anderson Cancer Center, NASA). Through informal meetings, the program discusses a variety of topics ranging from applying to Graduate School to career experiences by outstanding scientists, and discussion of cutting-edge topics in Statistics. The program evaluates and monitors the progress of students for seven years (expected time for them to finish graduate school) after their participation, and an annual evaluation of the program by the participating students and an external advisory committee is an integral and valuable part of the program. The investment is starting to produce concrete results. The first RUSIS alumnus received his Ph.D. during the summer of 2011 and is now an assistant professor of statistical sciences in a Ph.D. program. Several more are due to obtain their PhDs in the next few years."
"1261052","I-Corps:  Innovation Accelerator: A New Web-based Portal Software Tool to Find Disruptive Innovative Solutions","IIP","I-Corps","10/01/2012","09/20/2012","Sundar Krishnamurty","MA","University of Massachusetts Amherst","Standard Grant","Rathindra DasGupta","08/31/2013","$50,000.00","","skrishna@ecs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","ENG","8023","","$0.00","Researchers are developing a human-machine synergism in which the machine complements human weaknesses to being innovative while the human returns the favor for the machine. This method called Innovation Assistance (IA) represents a fundamental new way of thinking about innovation and replaces the minimally successful Artificial Intelligence techniques from the 1980's that were unable to get machines to be innovative by themselves. This is accomplished by carefully understanding the fundamental axis upon which all innovation turns: every innovative solution is based upon at least one overlooked (i.e., obscure) feature of the problem. Humans and machines have different reasons for overlooking obscure features. Each partner in the human-machine interaction will help counter the other?s weaknesses. Researchers have thus far devised nearly two dozen innovation techniques that counteract the many cognitive reasons why humans overlook obscure features. <br/><br/>This technology has applications for STEM education, lawyers, the military and engineers. The problem-solving model used in this method has the potential to be used to alter innovative education in STEM fields. Techniques developed through IA will be able to more efficiently search databases for similar solutions to an entered problem. There is potential for this technology to be used in a military setting for training forces to be more innovative problem solvers in the field. Engineering applications of this technology could assist in moving projects more rapidly through research and development phases. This technology addresses a growing need to improve the innovation capabilities of individuals and organizations."
"1252557","Planning Future Directions in SE & AI","CCF","Software & Hardware Foundation","09/01/2012","08/27/2012","Timothy Menzies","WV","West Virginia University Research Corporation","Standard Grant","Sol Greenspan","08/31/2013","$14,700.00","","timm@ieee.org","P.O. Box 6845","Morgantown","WV","265066845","3042933998","CSE","7798","7944, 9150","$0.00","The grant funds travel to a meeting of Software Engineering (SE) and Artificial Intelligence (AI) researchers to promote synergies between the two fields.  Advances in several AI areas, such as data mining, natural language understanding and constraint solving, have become ready for application in many domains, including SE. SE as an area for AI -- automating the activities of programmers -- is still a long-term vision for AI. SE goals, such as component-based, evolvable software containing valid world models and correct code, are important to AI as they are to other fields. The workshop will explore synergies between the fields and serve as a planning meeting for future interactions."
"1144591","IGERT: Soft Material Robotics","DGE","IGERT FULL PROPOSALS, NSF Research Traineeship (NRT), IGERT Chemistry, IGERT Physics, IGERT Materials Research","07/01/2012","07/20/2016","Barry Trimmer","MA","Tufts University","Continuing grant","Laura Regassa","12/31/2018","$2,709,035.00","David Kaplan","barry.trimmer@tufts.edu","136 Harrison Ave","Boston","MA","021111817","6176273696","EHR","1335, 1997, 8062, 8063, 8064","1335, 9179, SMET","$0.00","IGERT: Soft Material Robotics.<br/><br/>This Integrative Graduate Education and Research Traineeship (IGERT) award creates an interdisciplinary graduate program to  develop advances in the field of soft robotics. These machines, inspired by animals, will be capable of complex tasks that are difficult to achieve with conventional robots, suitable for close interactions with humans, and able to work in environmentally sensitive locations. The research will cross traditional disciplinary boundaries, employing novel biomaterials, exploiting cellular processes and tissue engineering methods, and using control strategies derived from evolutionary principles ? approaches that are comparatively rare in conventional robotics.<br/><br/>Broader Impacts: The development of this new technology provides an exciting opportunity to train inventive and entrepreneurial future science and engineering leaders. IGERT trainees will train in multiple disciplines, including materials science, neuromechanics, mechanical control systems, computer science, artificial intelligence and product design. Novel collaborative training approaches include the formation of mentorship teams and an innovative problem solving-based model of education. IGERT trainees will exploit bioengineering approaches to machine design, fabricate and control new robotic devices to address a wide range of current medical, social and environmental challenges. For example, soft robots could be developed for internal medical diagnosis and delivery of therapeutics, or for search and rescue operations and bioremediation. The international partners, leaders in the emerging field of soft robots, will help trainees form collaborations and affiliations across the world. These students will bridge emerging areas of biology and engineering to create revolutionary new technologies including robots for human assistance and environmentally-friendly biodegradable robots. <br/><br/>IGERT is an NSF-wide program intended to meet the challenges of educating U.S. Ph.D. scientists and engineers with the interdisciplinary background, deep knowledge in a chosen discipline, and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to establish new models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries, and to engage students in understanding the processes by which research is translated to innovations for societal benefit.<br/><br/>"
"1225666","Memory encoding in spatially structured networks: dynamics, discrete geometry & topology","DMS","MATHEMATICAL BIOLOGY","09/15/2012","09/11/2012","Carina Curto","NE","University of Nebraska-Lincoln","Standard Grant","Mary Ann Horn","04/30/2015","$164,999.00","","ccurto@psu.edu","151 Prem S. Paul Research Center","Lincoln","NE","685031435","4024723171","MPS","7334","9150","$0.00","Hippocampal networks are believed to be a major center of associative learning due to the central role of the hippocampus in learning and memory, as well as the relatively high levels of recurrent connectivity and synaptic plasticity.  The lack of topographic structure in hippocampus has made it a natural inspiration for associative memory models, such as the Hopfield model, for encoding memories in unstructured recurrent networks. At the same time, studies in rodents have uncovered the critical role of hippocampus in spatial navigation and, more recently, time-tracking. In contrast to associative memory encoding, these functions have been successfully modeled using spatially structured networks. How can these viewpoints be reconciled? The central goal of this research is to develop a mathematical theory of memory encoding in spatially structured networks, and to study the neural codes that arise from such networks.  Specifically, the research will develop mathematical theory to answer the following questions: (1) How can overlapping memory patterns be encoded precisely as attractors of an unstructured neural network, without introducing unwanted ""spurious states""? (2) How can memories be encoded in a spatially structured network, such as a bump attractor network, while maintaining functions that depend on the network's spatial organization?  (3) Aside from error correction, what are the advantages of redundancy in a neural code, such as the hippocampal place field code, that is characterized by heavily overlapping receptive fields? This last question will also be explored via the analysis of cortical and hippocampal data sets provided by collaborating labs.<br/><br/>The hippocampus is often thought of as a ""Swiss knife"" in the brain.  Decades of experimental work have uncovered its essential role in learning and memory, as well as in spatial navigation.  From a theoretical standpoint, it is puzzling how the same neural network can achieve such disparate functions.  In particular, mathematical models of memory encoding are fundamentally quite different from models of spatial navigation.  This work will integrate these two major types of neural network models, with the goal of understanding how the hippocampus can support multiple important functions.  At its core, the research will advance the mathematical theory behind our understanding of network-level computation in the brain."
"1144611","EAGER: Microfabricated non-linear fractal architectures for propagation and differentiation of human neural progenitors","CBET","Cellular & Biochem Engineering","05/15/2012","05/09/2012","Vamsi Yadavalli","VA","Virginia Commonwealth University","Standard Grant","Friedrich Srienc","04/30/2015","$110,558.00","Raj Rao","vyadavalli@vcu.edu","P.O. Box 980568","RICHMOND","VA","232980568","8048286772","ENG","1491","020E, 138E, 7916","$0.00","1144611/ Yadavalli<br/><br/>This EAGER award funded by the Biotechnology, Biochemical and Biomass Engineering Program in the Chemical, Bioengineering, Environmental and Transport Division of NSF seeks to develop novel bio-inspired microfabricated networks as platforms for stem cell growth and differentiation.  Challenges in the development of successful stem cell therapies involve engineering and control of stem cell cues to regulate the balance between differentiation and self-renewal. Equally critical is the void that exists in the knowledge base on the cues that guide self-renewal and differentiation during early human development. The study of neural network formation involves the ability to control the spatial positioning and connectivity of neuronal cells and progenitors. However, this has been particularly challenging on account of complexity of architecture and function. A first step in designing such networks is to provide topological and signaling cues for the growing cells to form functional connections. In the context of these engineering considerations, the researchers play to mimick early neuronal development via the establishment of a biomimetic framework along which neural progenitors can organize themselves into oriented constructs as nerves. They hypothesize that microfabricated non-linear fractal architectures will promote a relaxed self-supportive niche that will promote cellular fate related to propagation of human neural progenitors (hNPs) and directed differentiation towards neurons. <br/><br/>The intellectual merit of this proposal comes from going beyond existing research paradigms that focus on simplistic geometric designs for the spatial organization of cells on substrates. By mimicking a biological network that allows for spreading of the cells instead of confining them in a groove or a well, a nonlinear configuration can promote a relaxed, self-supportive stem cell niche. By the tailoring of non-homogeneous adhesion sites via the geometry and the compliance and roughness of the substrate, the PIs believe they will enable a versatile microenvironment that promotes neural progenitor propagation and neuronal differentiation. <br/><br/>The broader research impact stems from the integration of biomimetics, microfabrication and stem cell biology which will make key transformative contributions in applied tissue engineering as well as understanding fundamental neuronal development. The proposed research will lead to synthesizing novel, biomimetic, cell culture platforms for the precise control of cell growth and spacing and yield new insights into the mechanisms determining the organizational features and signaling cues in neural architectures and will contribute to tissue engineering for nerve repair.. The broader educational impact is the enormous potential for communicating research findings to a wider audience and for students and the general public to get excited about the latest interdisciplinary approaches in nanotechnology and stem cell research. This will be achieved via integration of research material into course content taught by the PIs - ENGR 591-Concepts in Nanobiotechnology and CLSE 561 Stem Cell Engineering. The PIs will also be engaged in summer courses and other outreach programs at VCU and will develop presentation and laboratory modules that will benefit high school students, undergraduate students and the general public."
"1247365","Workshop on Cognitive Science: the Computational Paradigm","BCS","Perception, Action & Cognition, HCC-Human-Centered Computing, Robust Intelligence","10/01/2012","09/17/2012","Peter Erdi","MI","Kalamazoo College","Standard Grant","Anne Cleary","09/30/2014","$20,000.00","","perdi@kzoo.edu","1200 Academy Street","Kalamazoo","MI","490063291","2693377162","SBE","7252, 7367, 7495","7252, 7367, 7495, 7556","$0.00","This award will provide support for a satellite workshop to the International Joint Conference on Neural Networks (IJCNN), to be held in Dallas, TX on August 4-9, 2013.  The goal of the workshop is to explore subfields in cognitive science that hold the most promise for increasing our understanding of neural networks and computational intelligence.<br/><br/>The IJCNN explores the theoretical and computational understanding of the brain in order to develop new and more effective forms of machine intelligence. Cognitive science is the interdisciplinary, scientific study of the mind and mental processes. The workshop is intended to foster  more effective integration between the two communities. The  workshop will provide a venue in which neural network researchers and students can learn more about the state of the art in cognitive science and its interface with computational intelligence. The broader impacts of the workshop include fostering new collaborations between neural network researchers and those working in other areas of cognitive science. In addition, it provides for reduced registration for women and other scientists underrepresented in the field."
"1146607","Spatial Organization Of A Neural Network For Serial-Order Behavior","IOS","Activation","03/15/2012","03/19/2013","James Johnson","FL","Florida State University","Continuing grant","Sridhar Raghavachari","02/28/2015","$350,000.00","Richard Bertram, Wei Wu, Richard Hyson","johnson@psy.fsu.edu","874 Traditions Way, 3rd Floor","TALLAHASSEE","FL","323064166","8506445260","BIO","7713","1096, 9179","$0.00","How does the brain create orderly sequences of behavior?  The problem of serial order in behavior permeates human language, the ability to play a musical instrument, or any number of other activities where human or animal intent can only be effectively communicated if several behavioral gestures are chained together in a specific sequence.  How the brain accomplishes this task is currently unknown.  One approach is to use an animal model (a songbird, the zebra finch) in which the brain region responsible for the serial ordering of song syllables has been identified (called ""HVC"", the acronym is the proper name).  This project will investigate the hypothesis that the serial ordering of song syllables is mapped across several spatially-arranged chains of HVC neurons.  This hypothesis is based on preliminary data indicating that HVC neural activity is largely confined to a single spatial axis during singing.  Experiments will delineate the spatial organization of connections and electrophysiological properties of HVC neurons.  Computational models based on the properties of HVC neurons will then be used to discover network configurations that produce orderly, sequential patterns of neural activity.  Models will be validated with circuit-breaking experiments in behaving birds.  Results will provide a first look at a network architecture used by an animal brain to create order and sequence in behavior, which in turn will provide a computational platform to understand how the process of learning new behavioral sequences utilizes or shapes such architectures.  The research plan coordinates the activity of a faculty research team from three different academic departments (Psychology, Mathematics and Statistics), providing graduate and undergraduate students with access to the expertise of faculty researchers outside their home departments. Computational software tools as well as data from this project will be made available to the public at http://www.math.fsu.edu/~bertram/software/birdsong/ and at http://www.songbirdscience.com."
"1227879","Collaborative Research: A Neurodynamic Programming Approach for the Modeling, Analysis, and Control of Nanoscale Neuromorphic Systems","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","09/15/2012","07/26/2017","Pinaki Mazumder","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Radhakisan Baheti","08/31/2018","$239,053.00","","mazum@eecs.umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","ENG","7607","102E, 1653","$0.00","The objective of this research is to develop new neurodynamic programming (NDP) learning algorithm for controlling neuron-level activity (spiking) and synaptic-level plasticity in CMOS/memristor devices, such that the subsequent system-level response achieves desired sensorimotor behavioral goals.  The approach is to uses a radically new training paradigm that induces functional plasticity by controlling the neural activity of selected input neurons via programming voltages, rather than by directly manipulating the synaptic weights, as do virtually all existing training algorithms.<br/><br/>Intellectual merit<br/>This research aims to develop a model of the closures required to translate synaptic-level plasticity into functional-level plasticity that results into high-level behavioral goals and problem solving abilities.  The same critical challenge has been identified in neuroscience research aimed at reverse engineering the brain, and in the regulation of deep-brain stimulation (DBS).  Due to this knowledge gap, even when a measure of adequate or desired behavior is available, it may not be easily utilized to stimulate a neural network at the cell level in order to produce the appropriate macroscopic behavior.<br/><br/>Broader impact<br/>The learning model developed in this research will be used toward the development of nanoscale neuromorphic systems that mimic neuro-biological architectures in the nervous system.  Thanks to their abilities to recreate the synaptic plasticity, device density, scalability, and fault-tolerance of biological neuronal networks, these neuromorphic systems can enable a wide range of technological advancements, such as intelligent robots with highly-sophisticated sensorimotor skills, and neuroprosthetic devices capable of adapting to changing conditions and environments."
"1147944","SI2-SSI: The Language Application Grid: A Framework for Rapid Adaptation and Reuse","OAC","Linguistics, Methodology, Measuremt & Stats, Special Projects - CNS, International Research Collab, IIS Special Projects, Software & Hardware Foundation, Software Institutes, ","08/01/2012","05/31/2018","Nancy Ide","NY","Vassar College","Standard Grant","Bogdan Mihaila","07/31/2019","$994,057.00","James Pustejovsky, Eric Nyberg, Christopher Cieri","ide@vassar.edu","124 Raymond Avenue","Poughkeepsie","NY","126040657","8454377092","CSE","1311, 1333, 1714, 7298, 7484, 7798, 8004, O422","1311, 1333, 5983, 7433, 7944, 8004, 8009, 9251","$0.00","The need for robust language processing capabilities across academic disciplines, education, and industry is without question of vital importance to national security, infrastructure development, and the competitiveness of American business. However, at this time a robust, interoperable software infrastructure to support natural language processing (NLP) research and development does not exist. To fill this gap, this project establishes an large international collaborative effort involving key international players to develop an open, web-based infrastructure through which massive and distributed language resources can be easily accessed, in whole or in part, and within which tailored language services can be efficiently composed, disseminated and consumed by researchers, developers, and students. <br/><br/>The goal of this project is to build a comprehensive network of web services and resources within the NLP community. This requires four specific activities: (1) Design, develop and promote a service-oriented architecture for NLP development that defines atomic and composite web services, along with support for service discovery, testing and reuse; (2) Construct a Language Application Grid (LAPPS Grid) based on Service Grid Software developed in Japan; (3) Provide an open advancement (OA) framework for component- and application-based evaluation that enables rapid identification of frequent error categories within modules, thus contributing to more effective investment of resources; (4) Actively promote adoption, use, and community involvement with the LAPPS Grid. <br/><br/>By providing access to cloud-based services and support for locally-run services, the LAPPS Grid will lead to the development of a massive global network of language data and processing capabilities that can be used by scientists and engineers from diverse disciplines, providing components that require no expertise in language processing to use. Research in sociology, psychology, economics, education, linguistics, digital media, and the humanities will be impacted by the ability to easily manipulate and process diverse language data sources in multiple languages."
"1147912","SI2-SSI: The Language Application Grid: A Framework for Rapid Adaptation and Reuse","OAC","Special Projects - CNS, Special Projects - CCF, International Research Collab, CSR-Computer Systems Research, IIS Special Projects, Robust Intelligence, Software & Hardware Foundation, Software Institutes, ","08/01/2012","08/24/2018","James Pustejovsky","MA","Brandeis University","Standard Grant","Bogdan Mihaila","07/31/2019","$1,764,929.00","Marc Verhagen, Eric Nyberg, Christopher Cieri","pustejovsky@gmail.com","415 SOUTH ST MAILSTOP 116","WALTHAM","MA","024532728","7817362121","CSE","1714, 2878, 7298, 7354, 7484, 7495, 7798, 8004, O422","1714, 2878, 5983, 7433, 7484, 7944, 8004, 8009","$0.00","The need for robust language processing capabilities across academic disciplines, education, and industry is without question of vital importance to national security, infrastructure development, and the competitiveness of American business. However, at this time a robust, interoperable software infrastructure to support natural language processing (NLP) research and development does not exist. To fill this gap, this project establishes an large international collaborative effort involving key international players to develop an open, web-based infrastructure through which massive and distributed language resources can be easily accessed, in whole or in part, and within which tailored language services can be efficiently composed, disseminated and consumed by researchers, developers, and students. <br/><br/>The goal of this project is to build a comprehensive network of web services and resources within the NLP community. This requires four specific activities: (1) Design, develop and promote a service-oriented architecture for NLP development that defines atomic and composite web services, along with support for service discovery, testing and reuse; (2) Construct a Language Application Grid (LAPPS Grid) based on Service Grid Software developed in Japan; (3) Provide an open advancement (OA) framework for component- and application-based evaluation that enables rapid identification of frequent error categories within modules, thus contributing to more effective investment of resources; (4) Actively promote adoption, use, and community involvement with the LAPPS Grid. <br/><br/>By providing access to cloud-based services and support for locally-run services, the LAPPS Grid will lead to the development of a massive global network of language data and processing capabilities that can be used by scientists and engineers from diverse disciplines, providing components that require no expertise in language processing to use. Research in sociology, psychology, economics, education, linguistics, digital media, and the humanities will be impacted by the ability to easily manipulate and process diverse language data sources in multiple languages."
"1227877","Collaborative Research: A Neurodynamic Programming Approach for the Modeling, Analysis, and Control of Nanoscale Neuromorphic Systems","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","09/15/2012","08/25/2014","Silvia Ferrari","NC","Duke University","Continuing grant","Radhakisan Baheti","08/31/2015","$240,000.00","","ferrari@cornell.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","ENG","7607","102E, 1653, 9102","$0.00","The objective of this research is to develop new neurodynamic programming (NDP) learning algorithm for controlling neuron-level activity (spiking) and synaptic-level plasticity in CMOS/memristor devices, such that the subsequent system-level response achieves desired sensorimotor behavioral goals.  The approach is to uses a radically new training paradigm that induces functional plasticity by controlling the neural activity of selected input neurons via programming voltages, rather than by directly manipulating the synaptic weights, as do virtually all existing training algorithms.<br/><br/>Intellectual merit<br/>This research aims to develop a model of the closures required to translate synaptic-level plasticity into functional-level plasticity that results into high-level behavioral goals and problem solving abilities.  The same critical challenge has been identified in neuroscience research aimed at reverse engineering the brain, and in the regulation of deep-brain stimulation (DBS).  Due to this knowledge gap, even when a measure of adequate or desired behavior is available, it may not be easily utilized to stimulate a neural network at the cell level in order to produce the appropriate macroscopic behavior.<br/><br/>Broader impact<br/>The learning model developed in this research will be used toward the development of nanoscale neuromorphic systems that mimic neuro-biological architectures in the nervous system.  Thanks to their abilities to recreate the synaptic plasticity, device density, scalability, and fault-tolerance of biological neuronal networks, these neuromorphic systems can enable a wide range of technological advancements, such as intelligent robots with highly-sophisticated sensorimotor skills, and neuroprosthetic devices capable of adapting to changing conditions and environments."
"1232070","Collaborative Research: Computational Intelligence Methods for Dynamic Stochastic Optimization of Smart Grid Operation with High Penetration of Renewable Energy","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","09/01/2012","07/23/2017","Ganesh Venayagamoorthy","SC","Clemson University","Standard Grant","Radhakisan Baheti","09/30/2018","$196,263.00","","gkumar@ieee.org","230 Kappa Street","CLEMSON","SC","296345701","8646562424","ENG","7607","155E, 1653, 9102, 9150, 9251","$0.00","The objective of this research is to develop advanced computational intelligence methods to monitor, optimize and control large or so-called wide areas of a power network that will include solar farms (SFs) and wind farms (WFs), and controllable network transformers (CNTs), in order to ensure optimum usage of all these resources both during slow changing semi-steady state conditions, as well as during transient conditions. The research will be carried out in off-line simulations and then implemented on a real-time simulator.<br/><br/>Intellectual merit<br/>The behavior of renewable energy sources is uncertain and variable, and it is difficult for static optimization methods to optimize uncertain non-stationary distributed energy resources in a smart grid for maximum utilization. A novel ACD controller is proposed for the development of a real- time dynamic stochastic optimization smart grid engine. Advanced intelligent methods such as the biologically inspired artificial neural network and smart devices (CNTs) provide better identification and control capabilities for implementation of optimal power flows.<br/><br/>Broader Impacts<br/>Economically operated reliable and secure power systems that can accommodate high penetration of renewable energy are of national interest.  Being able to route power through underutilized lines will have major economic and environmental benefits due to avoiding the need for new lines. This project will have several dissemination channels, including software, websites, new contents added to existing courses, special sessions and tutorials at conferences, and journal publications."
"1232031","Collaborative Research: Computational Intelligence Methods For Dynamic Stochastic Optimization Of Smart Grid Operation With High Penetration Of Renewable Energy","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","09/01/2012","07/18/2012","Ronald Harley","GA","Georgia Tech Research Corporation","Standard Grant","Radhakisan Baheti","08/31/2016","$179,999.00","","rharley@ee.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","ENG","7607","155E, 1653, 9150","$0.00","The objective of this research is to develop advanced computational intelligence methods to monitor, optimize and control large or so-called wide areas of a power network that will include solar farms (SFs) and wind farms (WFs), and controllable network transformers (CNTs), in order to ensure optimum usage of all these resources both during slow changing semi-steady state conditions, as well as during transient conditions. The research will be carried out in off-line simulations and then implemented on a real-time simulator.<br/><br/>Intellectual merit<br/>The behavior of renewable energy sources is uncertain and variable, and it is difficult for static optimization methods to optimize uncertain non-stationary distributed energy resources in a smart grid for maximum utilization. A novel ACD controller is proposed for the development of a real- time dynamic stochastic optimization smart grid engine. Advanced intelligent methods such as the biologically inspired artificial neural network and smart devices (CNTs) provide better identification and control capabilities for implementation of optimal power flows.<br/><br/>Broader Impacts<br/>Economically operated reliable and secure power systems that can accommodate high penetration of renewable energy are of national interest.  Being able to route power through underutilized lines will have major economic and environmental benefits due to avoiding the need for new lines. This project will have several dissemination channels, including software, websites, new contents added to existing courses, special sessions and tutorials at conferences, and journal publications."
"1209172","Digging into Human Rights Violations:  Anaphora Resolution and Emergent Witnesses","SMA","DiD Challenge","01/01/2012","08/06/2012","Benjamin Miller","GA","Georgia State University Research Foundation, Inc.","Standard Grant","Joan Maling","12/31/2014","$174,999.00","","miller@gsu.edu","58 Edgewood Avenue","Atlanta","GA","303032921","4044133570","SBE","8677","","$0.00","Digging into Human Rights Violations will develop methods and tools for discovering and visualizing the stories of hidden victims and unidentified perpetrators in collections of human rights violations reports and witness statements. This research in natural language processing is necessary because, a) collections documenting human rights violations frequently number many tens of thousands to tens of millions of documents, a scale that conceals victim, violation, and violator; b) many victim?s stories are only present as fragments in the reports and accounts of observers and survivors; and c) current tools excel at searching, not reading. <br/><br/>This project's computational reader, developed using collections compiled for various Truth and Reconciliation and Historical Clarification Committees, will consist of an engine for reconstructing stories from fragments scattered across a collection, and an interface for navigating those compiled stories and the documents from which they come. Although developed for use by investigators and prosecutors of human rights violations, this novel tool will expand the possibilities of text mining and will be generally useful for scholars, researchers, and practitioners with a need to track entities, events, and patterns across large document collections. The key development is to further work on anaphora resolution (where anaphora is the existence of an expression referring to another -- usually a pronoun to its antecedent). In some cases identifying anaphora relatively straightforward, but in many cases it presents a significant challenge to traditional natural language processing methods. This project will involve a combination of natural language processing and qualitative language analysis.<br/><br/>This project's broader impacts lie in its clear potential to inform scholars and government officials working on issues of human rights and in truth and reconciliation contexts. Further, the project includes significant student training, including training in interdisciplinary research methods.<br/><br/>This grant was made as part of the Digging Into Data Challenge, an international competition designed to foster research collaboration across countries and to encourage innovative approaches to analyzing large data sets in the social sciences and humanities. The US based researchers will collaborate with scholars in Canada to achieve the goals of this project."
"1147581","A Linguistic Taxonomy of English Web Registers","BCS","LINGUISTICS","09/15/2012","09/19/2012","Douglas Biber","AZ","Northern Arizona University","Standard Grant","Joan Maling","02/28/2017","$331,806.00","Mark Davies","douglas.biber@nau.edu","ARD Building #56, Suite 240","Flagstaff","AZ","860110001","9285230886","SBE","1311","1311, 7434, SMET","$0.00","For both general users and linguists, the Internet provides a massive amount of information and linguistic data, readily accessible to anyone with a computer, that has value for research in linguistics, computational linguistics and the social sciences. However, the nature of the different types of language used on the web remains unclear. To better understand the language of the internet, Drs. Biber and Davies will develop a comprehensive linguistic taxonomy of web registers. This taxonomy will be applied to a large, representative corpus of internet texts, which will be made freely available as an online resource for a range of research purposes. <br/><br/>The initial framework for classifying texts into register categories will be developed through hand-coding (using a rubric based on situational characteristics) and computational linguistic analysis of a sample of web sites indexed by Netscape's Open Directory Project. The resulting taxonomy will then be applied automatically to a second corpus (circa 100,000 texts) and integrated into a searchable online interface. Assuming that each web text contains an average of 1,000 words, this online searchable corpus will contain approximately 100 million words.<br/><br/>The linguistic descriptions resulting from this project, and the searchable online corpus, will provide the basis for more principled uses of the web as a data source, including using the web as a corpus to test hypotheses about language variation and change; using the web to identify probabilistic patterns, incorporated into tools for lexicographic research or natural language processing applications; studying ways to extract essential information on specialized topics from web documents;  and identifying examples of 'authentic' language illustrating the use of words or grammatical constructions.  The present project will provide detailed linguistic descriptions of the nature of the source documents, as well as a valuable online resource to facilitate future investigations of web-based texts."
"1208522","NRI-Small: A Biologically Plausible Architecture for Robotic Vision","IIS","National Robotics Initiative","09/01/2012","08/27/2012","Nuno Vasconcelos","CA","University of California-San Diego","Standard Grant","Reid Simmons","08/31/2018","$1,150,000.00","","nuno@ece.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","8013","1653, 7923, 8086","$0.00","The objective of this research is to develop a generic robotic vision architecture that is both biologically plausible and jointly optimal, in a decision theoretic sense, for attention, object tracking, object recognition, and action recognition, in both static and dynamic environments. The research is motivated by the observation that all these problems are solved by biological vision with very homogeneous neural computations. The approach is to exploit a mapping of accepted computational models of visual cortex into the elementary computations of statistical learning and inference in order to derive unified algorithms for all tasks. <br/><br/>Intellectual merit: the proposed unification of vision tasks is novel and of paramount importance for robotics, since it is computationally infeasible for a robot to implement a large set of disjoint vision algorithms. It will also exploit task synergies, producing algorithms that leverage the solution of one task to improve performance on another. This will likely enable overall better performance of vision systems. Finally, the project will produce novel insights on the structure of the visual world, and how it can be leveraged by robotic vision, by introducing new models for natural image statistics. <br/><br/>Broader impacts: The research has applicability in manufacturing, intelligent systems, health care, homeland security, etc.  The expected theoretical insights are likely to be of wide application in statistics (models of feature dependence), neuroscience (models of neural computation), and computer vision (synergistic models). Educationally, the project provides an exciting opportunity for the involvement of undergraduates in research."
"1220521","NAACL-HLT 2012 Student Workshop","IIS","ROBUST INTELLIGENCE","02/15/2012","02/08/2012","Ani Nenkova","PA","University of Pennsylvania","Standard Grant","Tatiana Korelsky","01/31/2013","$15,000.00","","nenkova@seas.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","CSE","7495","7495, 7556","$0.00","The NAACL HLT conference is the major international conference in North America in the field of natural language processing. The goal of this grant is subsidize travel, conference, and housing expenses of students selected to participate in the NAACL HLT Student Research Workshop which will be held during the conference June 3-8 in Montreal, Canada. The workshop aims to attract papers both from authors who are in their early stages of academic work (possibly undergraduate and masters students) as well as from students who are approaching graduate and would like to present their thesis work. The goal is to help attract students from the first group to pursue further academic work, and to help students from the second group in their job search and career planning.<br/><br/>Papers from the student workshop are presented as posters during the main poster session of the conference. Senior researchers are assigned as mentors to each student and  provide individual feedback. A general session on on how to review papers and what to expect from reviewers is held during a lunch slot. The workshop is organized and run by students.<br/><br/>The Student Research Workshop provides a valuable opportunity for the next generation of natural language processing researchers to enter the computational linguistics community. It allows the best students in the field to take their first important step toward becoming professional computational linguists by receiving critical feedback on their work from external experts, and by making contacts with other students and senior researchers in their field. The students who are involved in running and selecting papers for the workshop also gain valuable opportunities for professional growth and interaction with the researchers on the organizing committee of the main conference. The workshop contributes to the maintenance and development of a skilled and diverse computational linguistics and natural language processing research community."
"1212280","ACL 2012 Student Research Workshop","IIS","ROBUST INTELLIGENCE","02/15/2012","01/20/2015","Yang Liu","TX","University of Texas at Dallas","Standard Grant","Tatiana D. Korelsky","01/31/2016","$19,500.00","","yangl@hlt.utdallas.edu","800 W. Campbell Rd., AD15","Richardson","TX","750803021","9728832313","CSE","7495","7495, 7556","$0.00","The Association for Computational Linguistics (ACL) is the primary international organization in the field of natural language processing and computational linguistics. The ACL's annual conference is the major international conference in this field. This project is to subsidize travel, conference and housing expenses of students selected to participate in the ACL Student Research Workshop, which will take place during the main ACL conference on July 8-14, 2012 in Jeju Island, Korea. The Student Research Workshop accepts papers in two categories: thesis/research proposal and general research papers. The thesis/research proposal can have only one author who must be a student.  The research papers can have multiple authors, with the first author being a student. The workshop is organized and run by students.<br/><br/>The Student Research Workshop provides a valuable opportunity for the next generation of natural language processing researchers to enter the research community. It allows the students in the field to take an important step towards becoming professional computational linguists by receiving critical feedback on their work from experts outside of their dissertation committee, and by making contacts with other students and senior researchers in their field. The students who are involved in running and reviewing for the student workshop also gain valuable opportunities for professional growth and interaction with the researchers on the organizing committee of the main conference. The ACL Student Research Workshop contributes to the maintenance and development of a skilled and diverse computational linguistics and natural language processing community."
"1304404","Empowering the visually impaired by understanding links between tactility and properties of surfaces","CBET","Disability & Rehab Engineering","08/15/2012","11/02/2012","Christian Schwartz","IA","Iowa State University","Standard Grant","alexander leonessa","12/31/2014","$147,864.00","","cris1@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","ENG","5342","010E","$0.00","In blind or visually impaired (BVI) individuals, a primary means of delivering information is through tactile channels such as Braille and raised-line illustrations. However, these systems present complications in translating non-tactile information to these forms, and also result in cumbersome and space-consuming materials for instruction and communication of technical and artistic ideas. This is a considerable hindrance to the ability of BVI persons to fully participate in engineering and the sciences, which rely heavily on the presentation and manipulation of graphical information during instruction and peer discussion. A revolutionary improvement in technology is required to address this issue. Currently, there is a lack of fundamental science to address how surfaces can be manipulated to convey tactile information. Thus, there is a critical need to better understand the causative relationships between surface properties (mechanical, textural, thermal, chemical) and the resulting tactile attributes of surfaces (texture, softness, abrasiveness, etc.), as well as a need to train engineers to objectively consider tactility and other sensory attributes during design. Addressing these needs will have a transformative effect not only on the materials science-based understanding of human interaction with surfaces, but also on the paradigms of education and professional occupation of the visually impaired. The objectives of this proposed work are<br/>to test the hypotheses that: 1. Surface tactility can be described quantitatively by a collection of objective tactile descriptors and associated scores, 2. Objective causal relationships exist that relate tactile attributes to engineering-based surface properties, 3. Tactility can be used with a problem-based learning approach to educate engineers to address sensory<br/>design goals, as well as attract BVI students to study engineering and science in college.<br/>This work will employ a thorough experimental approach. Quantitative Descriptive Analysis (QDA) will be used with human evaluators to meticulously identify and quantify the individual tactile descriptors of both textile and solid polymer surfaces. Surfaces will also be analyzed by a number of methods including tribological testing, dynamic mechanical analysis, and surface topography measurement. Statistical and neural network techniques will be employed to identify and investigate relationships between the tactile descriptors and the surface property values. The research tasks will be closely integrated with the educational activities of this work through a number of innovative mechanisms.<br/>Intellectual Merit. This work is novel and transformative because it will be the first broad engineering based approach to understanding how to directly control and optimize the tactile feel of a variety of surfaces and thus produce insights into efficient means of conveying tactile information. This knowledge will also foster new fields of research that will bridge the gap between engineering and neuroscience. Control of tactility will revolutionize paradigms in such fields as haptic displays and textiles, but more importantly will open new doors into the possibilities for educational tools for BVI students as well as technologies to facilitate greater participation of these persons in engineering and science. There is a unique synergy with the biotribology and polymers background of the PI and the resources of his institution that maximize the probability of successful completion of this work.<br/>Broader Impacts. These results will have far-reaching impact on the fundamental understanding of the materials science based origins of tactility. The work will also serve as an instructional platform to expose students to design experiences that incorporate sensory assessment of engineered products. A problem based pedagogical approach, termed iSENSE, will be incorporated into graduate and capstone design courses. A goal of iSENSE is to enable engineering students to transcend discipline paradigms in order to address real-world design challenges that involve sensory assessment. iSENSE also targets the recruitment of middle- and high school BVI students to engineering and science, by their participation in PI-led enrichment courses that will incorporate tactility into engineering design inspired activities. Undergraduate engineering students will work on design projects to develop instructional technology to help facilitate the learning of the BVI students during these enrichment courses."
"1227329","Developing and Testing New Geospatial Approaches in Paleoanthropology","BCS","GEOGRAPHY AND SPATIAL SCIENCES, Biological Anthropology","09/01/2012","04/03/2013","Robert Anemone","MI","Western Michigan University","Standard Grant","Carolyn Ehardt","02/28/2014","$180,459.00","Charles Emerson","robert.anemone@uncg.edu","1903 West Michigan Avenue","Kalamazoo","MI","490085200","2693878298","SBE","1352, 1392","1352, 1392","$0.00","Paleontologists search for fossils today in very nearly the same ways that our predecessors have since the beginnings of the discipline in the nineteenth century. They study geological and topographic maps in order to locate places where fossils of a certain age may be found, and then they walk long distances with their eyes scouring the ground for hints of eroding fossils.  As a result, many important paleontological sites are literally stumbled upon, and chance and luck continue to play a large role in the success or failure of many paleontological expeditions.<br/><br/>This interdisciplinary research utilizes state-of-the-art imaging methods and analytical techniques from remote sensing and the spatial sciences to develop and test new predictive models for determining where paleontologists should concentrate their efforts in the field in order to maximize their effectiveness at finding productive fossil-bearing localities.  The investigators will use Landsat imagery as well as high-resolution, commercially available satellite imagery to determine the spectral characteristics of known productive localities in the Eocene deposits of Wyoming's Great Divide Basin.  A number of analytical approaches will be tested for their ability to identify the spectral signatures of productive localities, including artificial neural network analysis and geographic object-based image analysis.  The research team will spend two summer field seasons searching those areas on the ground that are predicted to have a high potential to be fossil-bearing, in order to statistically evaluate the success of the predictive models.<br/><br/>The international research team includes specialists from vertebrate paleontology, paleoanthropology, geology and geography, and their aim is to stimulate the application of new approaches from the geographic and spatial sciences to the sciences of paleoanthropology and paleontology. Broader impacts include the training of undergraduate and graduate students in this cross-disciplinary approach to field-based anthropological science, and development of web-based tools for education and outreach."
"1147499","Testing and improving methods for efficient annotation through the construction of a large parsed corpus","BCS","Linguistics, Robust Intelligence","07/15/2012","08/03/2015","Anthony Kroch","PA","University of Pennsylvania","Standard Grant","William Badecker","06/30/2016","$360,225.00","Seth Kulick","kroch@ling.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","SBE","1311, 7495","1311, 7434, 7495, 9179, 9251, SMET","$0.00","Electronic corpora annotated with linguistic information play a crucial role in natural language processing (NLP) and in linguistic research. Treebanks (corpora annotated with syntactic information) are especially important since they mark the grammatical structure necessary for understanding sentence and discourse meaning. For NLP, treebanks provide testbeds for developing language understanding systems. For linguistic research, they provide the basis for precise and replicable studies of the patterns of use of syntactic forms. Unfortunately, accurate annotation is difficult. Automatic parsers have relatively high error rates and the correction of these errors by human annotators is both slow and itself error-prone. Based on recent advances that Dr. Kroch and his collaborators have made in the creation and quality control of three large treebanks for different languages, Dr. Kroch proposes a major effort to improve corpus construction through the creation of a two-million-word English treebank.  Along with this useful and substantial result, the project will develop and test hypotheses on speeding up treebank construction. The work will be guided by two complementary strategies. The first aims to reduce the parser's error rate by enhancing the part-of-speech (POS) tagged input to the parser while the second aims to make the correction of residual errors more efficient by shifting some of the burden from human to automatic error detection and correction. Speeding up the construction of accurate, consistent treebanks will improve the size and quality of training data for parsers, leading to improved performance in real-world NLP applications that rely on parsing. The availability of larger treebanks and of better methods for constructing them will also improve linguistic research. Moreover, as treebanks grow in size, they will become more useful in literary and historical studies, where the rhetorical structure of texts will become investigable in a more precise way than is currently possible.<br/><br/>In addition to the intellectual merit of the proposed research and the impact it can be expected to have on text-based research that relies on automated processing techniques, the project will provide valuable training opportunities for graduate and undergraduate students.  In contributing to improvements in automated techniques for language processing, this project may also benefit the analytic needs in industry and government security."
"1216875","RI: Small: Learning Meaning and Grammar from Interaction, Context, and the World","IIS","Robust Intelligence","08/01/2012","07/19/2012","Daniel Jurafsky","CA","Stanford University","Standard Grant","Tatiana Korelsky","07/31/2014","$150,000.00","","jurafsky@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","7495","7923","$0.00","Natural language processing  tasks like question answering or machine translation require sophisticated parsers: systems that extract grammatical dependency relations between words.  But traditional supervised methods of training parsers rely on very expensive hand-labeled datasets, and generalize poorly to new words, grammar, languages, or genres of text.  This project is pursuing three directions to significantly augment current unsupervised models of grammar induction.  First is a new mathematical model of dependency parsing that draws on linguistic intuitions of constituency.  Second is an architecture that jointly learns grammar and parts-of-speech, eliminating the need for supervised part-of-speech tags and hand-labeled datasets, and making grammar induction possible on a vast number of languages and genres. Third are ways to exploit new sources of data for unsupervised learning, including anchor text in web data, vastly expanding the scope of the problem from the small clean annotated treebanks commonly used in current work.<br/><br/>Language understanding by machine is a crucial tool for our nation: machine translation makes international web sites broadly accessible, sentiment analysis helps newspapers make politics more  transparent, question answering systems help people disseminate knowledge, and information extraction helps corporations and people draw insights from vast databases of documents.  By improving the fundammental parsing technology that underlies each of these tasks, and making it possible to parse new languages and genres that have not been parsable before, this project has the power to vastly increase both the power and scope of these key applications."
"1058937","Sequential learning from a scale-invariant representation of remembered time","BCS","PERCEPTION, ACTION & COGNITION","01/15/2012","12/28/2011","Marc Howard","MA","Trustees of Boston University","Standard Grant","Catherine Arrington","12/31/2015","$366,567.00","","mahoward@syr.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","SBE","7252","","$0.00","It can be argued that the primary adaptive function of memory is not to remember the past, but to predict the future. Knowledge of past experiences combined with an understanding of the present state gives organisms the ability to anticipate future rewards or avoid impending danger. The goal of the proposed research is to develop a mathematical theory describing how people use past history and a representation of the present to predict the future. The theory will be built on a mathematical model of how the history leading up to the present moment can be efficiently compressed into a representation that could be maintained by the brain. The investigators pursue the development of the theory using three techniques. First, in order to see if human learners behave in a way consistent with the hypothesis, the investigators will conduct a series of behavioral experiments using undergraduate students as research subjects. The experiments present the subjects with a series of symbols chosen according to a hidden sequence and ask them to predict the symbols that will follow at various stages. Second, the investigators will conduct computer simulations to train the equations on a large body of naturally-occuring language. Language has a rich temporal structure defined by the way words, and combinations of words, follow one another. Third, the investigators will work to extend the mathematics of the hypothesis to enable it to describe a wide range of phenomena in learning and memory. The large scale goal is to reorient several subfields of cognitive psychology---episodic memory, semantic memory, conditioning, and interval timing---around an understanding of how temporal history is represented and utilized by the human brain.<br/><br/>If successful, the proposed research could have far-reaching practical impacts. It could provide insight into how children and adults learn, leading to better instructional tools. If successful, it would also represent a large step forward in completely automated natural language processing. The rise of electronic communication has led to vast quantities of text---more than could ever be read by even a small army of human readers. Algorithms that can extract knowledge from large quantities of text currently find use in applications as far-ranging as essay grading and other educational applications to intelligence uses. The investigators anticipate that the equations will be much better at extracting knowledge from natural text than several widely-used algorithms. Finally, there may be useful technologies that exploit the ability to predict the future from the past and the present with the level of efficiency that humans can."
"1265301","EAGER: Constructing, Indexing, and Searching Super-Enriched Document Representations in the Cloud","IIS","Info Integration & Informatics, ","09/01/2012","11/29/2012","Eduard Hovy","PA","Carnegie-Mellon University","Standard Grant","Sylvia Spengler","08/31/2014","$236,111.00","","hovy@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7364, K155","170E, 7364, 7916","$0.00","There are billions of new digital documents created around the world every day. Examples include emails, blog posts, legal documents, and news articles. To enable effective information management, many of these documents are processed by information retrieval systems, such as desktop search tools or Web search engines. Most existing technologies represent documents digitally. To a computer, these representations are nothing more than a sequence of bits, completely devoid of any explicit meaning. Since most modern search engines utilize such basic representations, they often fail to properly account for the meaning of the words found in the documents, thereby diminishing the quality of their results. Despite the importance of this fundamental problem, there have been surprisingly few attempts to build, and subsequently search, document representations that encode the deeply rich meaning of text, especially for data sets that contain millions or billions of text documents.<br/><br/>This research investigates how to automatically construct, index, and search next-generation super-enriched document representations. The approach relies on the careful integration of traditional text representations with natural language processing-based sources (e.g., named entities, synonyms, and paraphrases), rich knowledge sources (e.g., Wikipedia and Freebase), contextual sources, and other value-added sources of content. Constructing such representations for large document collections requires computationally intensive batch processing to mine, aggregate, and join data across disparate sources. To overcome these challenges, a scalable, massively distributed cloud computing solution is adopted. The resulting enriched document representations can be effectively applied to a wide variety of information retrieval, natural language processing, and data mining tasks."
"1262860","SHB: Large: Collaborative Research: Companionbots for Proactive Therapeutic Dialog on Depression","IIS","Information Technology Researc, Smart and Connected Health","07/01/2012","11/20/2012","Rodney Nielsen","TX","University of North Texas","Standard Grant","Sylvia Spengler","08/31/2016","$1,118,752.00","","rodney.nielsen@unt.edu","1155 Union Circle #305250","Denton","TX","762035017","9405653940","CSE","1640, 8018","1640, 7925, 8018","$0.00","This collaborative research investigates a new class of dialog-based, home robotic healthcare assistants to facilitate a new level of in-home, real-time care to elderly and depressed patients, providing lower total costs and higher quality of life. An emotive, physical avatar, called a companionbot, which possesses the ability to engage humans in a way that is unobtrusive and suspends disbelief will be built in this project. The companionbot will be an integration of human language technology, vision, other sensory processing and emotive robotic technology to proactively recognize and dialog with isolated and elderly patients suffering from depression. The companionbot will utilize proactive or companionable dialog based on the context with users suffering from depression. This will require the first multimodal integration of a user model, environment model, and temporal processing with spoken dialog understanding and generation to produce dynamic dialog and emotive interaction, beyond the traditional scripted dialog and emotion. Object recognition, facial expression recognition, and human activity recognition will augment natural language processing to provide current and historical context important to dynamic dialog. <br/><br/>A team of skilled researchers, assembled from the University of Colorado Boulder, University of Denver, CU Anschutz Medical Campus, and Boulder Language Technologies, will work together to achieve the project goals. The investigators will use the companionbots as a tool to run clinical trials to monitor and dialog with their partners to detect signs of physical and emotional deterioration. The companionbots can then notify remote caregivers, as necessary, provide warnings, reminders, life coaching and therapeutic dialog, extending independence and quality of life, and even saving lives. The other benefits of such a system include continuous, annotated data to improve doctor-patient interaction and analysis, real-time monitoring of mental state for remote healthcare providers and, ultimately, real-time intervention as part of a comprehensive treatment strategy.<br/><br/>In addition, this research will promote both STEM practice and research education at the graduate and the undergraduate levels of the affiliated institutions. The companionbots are ideal for teaching the next generation of engineers and scientists in critical emerging technologies, as they permit either a deep focus on specific topics or an interdisciplinary perspective while providing a simple high-level interface to manage everything else. Furthermore, the project will develop related educational material to support others and will provide public outreach to K-12 classes in the area."
"1224173","The Power of Policy Ideas: Tracing Language in Policymaking","SES","POLITICAL SCIENCE","09/01/2012","06/29/2017","John Wilkerson","WA","University of Washington","Standard Grant","Brian D. Humes","08/31/2018","$236,422.00","","jwilker@uw.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","SBE","1371","0000, OTHR","$0.00","The purpose of the proposed project is to initiate the systematic study of policy ideas. Bills often contain a multitude of policy provisions and evolve during their consideration. This project addresses the extent to which successful bills incorporate policy ideas originally proposed by lawmakers other than the bill's sponsor. Is policy influence substantially more widespread when assessed in terms of the progress of policy ideas rather than bills? What personal and institutional factors promote or inhibit the incorporation of policy ideas? How do external focusing events or internal schedules such as reauthorizations shape opportunities to advance policy ideas?<br/><br/>The project's intellectual merit is associated with its comprehensive data collection and analysis, interdisciplinary component, and attention to critical policy questions. To study these questions, the project uses a ""big data"" approach to examine the substance of the approximately 80,000 House and Senate bills introduced during the 1993-2011 time period. Each bill is divided, or parsed, by section (approximately 800,000 sections), before assessing the similarity of each section to each other section using algorithms developed to study ""text reuse"" in computer science. An underlying assumption is that similar sections found in different bills constitute the same policy idea. This allows the researcher to follow the progress of an idea as distinct from the progress of a particular bill. Validation will be a central focus of the project. The project will engage computer scientists to test alternative approaches to assessing section similarity to better understand their implications for Type I and Type II errors. It is expected that a study of policy ideas will lead to important insights into three topics integrally related to mainstream legislative research: policy influence, legislative agenda setting, and comparative committee processes.<br/><br/>The broader impacts of the project are directly related to the data it will produce. The corpus will be a valued resource for future researchers and students in many fields. By partitioning complex and often large laws and bills into a researchable format, scholars will be able to systematically peer ""under the hoods"" of bills and therefore gain valuable new perspectives on the policymaking process. For example, the dataset will provide a valuable resource for scholars interested in legislative networks such as policy subsystems, policy representation, logrolling, lobbying influence, and many other topics. In addition, computer and information scientists will value the corpus as a test bed for natural language processing research. Finally, any citizen or reporter will be able to utilize our searchable database to trace the substance and progress of policy ideas proposed by a given lawmaker. Such a search will yield a more accurate assessment of priorities and policy influence."
"1209666","NSF East Asia and Pacific Summer Institute for FY 2012 in Japan","OISE","EAPSI","06/01/2012","05/21/2012","Jason Narad","NY","Narad Jason R","Fellowship","Anne L. Emig","05/31/2013","$5,000.00","","","","Fulton","NY","130693311","","O/D","7316","5921, 5978, 7316","$0.00","This action funds Jason Narad of the University of Massachusetts Amherst to conduct a research project entitled ""Improving relation extraction through marginalization of hidden syntactic structure,"" during the summer of 2012 at the Nara Institute of Science and Technology in Ikoma, Nara, Japan. The host scientist is Professor Yuji Matsumoto.<br/><br/>The Intellectual Merit of the research project is to provide a general framework for performing syntax-dependent natural language processing (NLP) tasks in resource-poor languages. This research focuswa on relation extraction, a subfield of NLP, which aims to uncover and categorize the relationships between entities (people, places, organizations) from unstructured text. The most successful approaches to this problem rely on syntactic annotation which is unavailable for many languages and costly to produce. The research uses an alternative approach which alleviates this requirement by instead learning a syntax-like scaffolding as a set of latent variables in a graphical model. <br/><br/>Broader Impacts of an EAPSI fellowship include providing the Fellow a first-hand research experience outside the U.S.; an introduction to the science, science policy, and scientific infrastructure of the respective location; and an orientation to the society, culture and language. These activities meet the NSF goal to educate for international collaborations early in the career of its scientists, engineers, and educators, thus ensuring a globally aware U.S. scientific workforce."
"1205540","CI-P: Collaborative Research: LexLink: Aligning WordNet, FrameNet, PropBank and VerbNet","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","06/01/2012","06/01/2012","Collin Baker","CA","International Computer Science Institute","Standard Grant","Tatiana Korelsky","05/31/2013","$30,000.00","","collinb@icsi.berkeley.edu","1947 CENTER ST STE 600","Berkeley","CA","947044115","5106662900","CSE","7359","7359","$0.00","This Computing Research Infrastructure planning grant addresses two challenges for automatic systems performing deep semantic processing: identifying the context-appropriate sense of polysemous words and interpreting the meanings and interrelations of verbs and nouns in event-denoting phrases. Preliminary steps are taken for aligning and linking four existing widely used lexical resources (WordNet, FrameNet, PropBank and VerbNet) with different but complementary contents and coverage. Methods for completing current cross-resource links and full transitive closure are explored and tested. The resulting infrastructure (LexLink) is designed to make the resources fully interoperable, capitalizing on their particular strengths with respect to word sense disambiguation and Semantic Role labeling.<br/><br/>Four activities are carried out in the context of planning LexLink. First, a workshop is held where key representatives of the Natural Language Processing and computational semantics communities articulate needs and requirements for the planned resource and offer advice on algorithms, annotation techniques and evaluation. Second, a subsection of cross-resource links for word senses and Semantic Role labels (Agent, Instrument, etc.) resulting from the automatic transitive closure is evaluated, yielding estimates for the error rate and leading to fine-tuning of algorithms. Third, current best performing mapping algorithms for  word senses and Semantic Role labels are evaluated against a human-annotated Gold Standard. Fourth, new Gold Standard data are created for additional training and testing and to refine existing algorithms. As a whole, the work provides a solid foundation for a resource with significant beneficial impact on a range of natural language applications, including machine translation, text summarization and sentiment analysis affecting areas such as health care, marketing, and education."
"1205484","CI-P:  Collaborative Research: LexLink:  Aligning WordNet, FrameNet, PropBank and VerbNet","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","06/01/2012","06/01/2012","Martha Palmer","CO","University of Colorado at Boulder","Standard Grant","Tatiana Korelsky","05/31/2013","$25,000.00","","mpalmer@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","7359","7359","$0.00","This Computing Research Infrastructure planning grant addresses two challenges for automatic systems performing deep semantic processing: identifying the context-appropriate sense of polysemous words and interpreting the meanings and interrelations of verbs and nouns in event-denoting phrases. Preliminary steps are taken for aligning and linking four existing widely used lexical resources (WordNet, FrameNet, PropBank and VerbNet) with different but complementary contents and coverage. Methods for completing current cross-resource links and full transitive closure are explored and tested. The resulting infrastructure (LexLink) is designed to make the resources fully interoperable, capitalizing on their particular strengths with respect to word sense disambiguation and Semantic Role labeling.<br/><br/>Four activities are carried out in the context of planning LexLink. First, a workshop is held where key representatives of the Natural Language Processing and computational semantics communities articulate needs and requirements for the planned resource and offer advice on algorithms, annotation techniques and evaluation. Second, a subsection of cross-resource links for word senses and Semantic Role labels (Agent, Instrument, etc.) resulting from the automatic transitive closure is evaluated, yielding estimates for the error rate and leading to fine-tuning of algorithms. Third, current best performing mapping algorithms for  word senses and Semantic Role labels are evaluated against a human-annotated Gold Standard. Fourth, new Gold Standard data are created for additional training and testing and to refine existing algorithms. As a whole, the work provides a solid foundation for a resource with significant beneficial impact on a range of natural language applications, including machine translation, text summarization and sentiment analysis affecting areas such as health care, marketing, and education."
"1205473","CI-P:  Collaborative Research: LexLink:  Aligning WordNet, FrameNet, PropBank and VerbNet","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","06/01/2012","06/01/2012","Christiane Fellbaum","NJ","Princeton University","Standard Grant","Tatiana Korelsky","05/31/2013","$45,000.00","","fellbaum@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","7359","7359","$0.00","This Computing Research Infrastructure planning grant addresses two challenges for automatic systems performing deep semantic processing: identifying the context-appropriate sense of polysemous words and interpreting the meanings and interrelations of verbs and nouns in event-denoting phrases. Preliminary steps are taken for aligning and linking four existing widely used lexical resources (WordNet, FrameNet, PropBank and VerbNet) with different but complementary contents and coverage. Methods for completing current cross-resource links and full transitive closure are explored and tested. The resulting infrastructure (LexLink) is designed to make the resources fully interoperable, capitalizing on their particular strengths with respect to word sense disambiguation and Semantic Role labeling.<br/><br/>Four activities are carried out in the context of planning LexLink. First, a workshop is held where key representatives of the Natural Language Processing and computational semantics communities articulate needs and requirements for the planned resource and offer advice on algorithms, annotation techniques and evaluation. Second, a subsection of cross-resource links for word senses and Semantic Role labels (Agent, Instrument, etc.) resulting from the automatic transitive closure is evaluated, yielding estimates for the error rate and leading to fine-tuning of algorithms. Third, current best performing mapping algorithms for  word senses and Semantic Role labels are evaluated against a human-annotated Gold Standard. Fourth, new Gold Standard data are created for additional training and testing and to refine existing algorithms. As a whole, the work provides a solid foundation for a resource with significant beneficial impact on a range of natural language applications, including machine translation, text summarization and sentiment analysis affecting areas such as health care, marketing, and education."
"1156822","REU Site: Exploring Human Centered and Socially Relevant Interactive Technologies in Computer Vision, Visualization, Pervasive Computing, Serious Games, and Social Networks","IIS","RSCH EXPER FOR UNDERGRAD SITES, RES EXP FOR TEACHERS(RET)-SITE","03/01/2012","06/04/2014","Jamie Payton","NC","University of North Carolina at Charlotte","Standard Grant","William Bainbridge","02/28/2017","$343,658.00","Tiffany Barnes, Richard Souvenir","payton@uncc.edu","9201 University City Boulevard","CHARLOTTE","NC","282230001","7046871888","CSE","1139, 1359","9250","$0.00","The objective of the REU Site at UNC Charlotte is to implement effective practices to engage a diverse<br/>group of undergraduate students in computing research. Undergraduate students will actively pursue<br/>research that applies human-computer interaction (HCI) concepts to advance the state of the art in a<br/>particular field, including pervasive computing, computer vision, data visualization, social networks, and<br/>serious games. Each research project has a socially relevant theme, providing students with the<br/>opportunity to apply and extend their computing knowledge to address problems that have a real-world<br/>impact. The site emphasizes participation of promising students from underrepresented groups in<br/>computing and from institutions that have limited research programs. To help students become rapidly<br/>immersed in the culture of research and to prepare them for meaningful research experiences, the site<br/>cultivates an authentic community of practice that values the contributions of REU students as<br/>researchers, and provides REU participants with explicit, just-in-time training in research skills at a<br/>weekly workshop.<br/><br/>The intellectual merit of this project lies in the REU students? contributions to the discovery and<br/>evaluation of innovative, interactive computing solutions to address problems of interest in society;<br/>specific projects may address open problems in the fields of computer vision, pervasive computing, data<br/>visualization, social networks, and serious games. The broader impact of this project lies in the<br/>opportunity to introduce talented students from underrepresented populations to computing research and<br/>in the potential to increase the enrollment of domestic students in doctoral computing programs."
"1257024","EAGER: Solving Markov Random Fields with Mutual Exclusion Constraints","IIS","ROBUST INTELLIGENCE","09/15/2012","09/11/2012","Longin Jan Latecki","PA","Temple University","Standard Grant","Jie Yang","08/31/2013","$71,624.00","","latecki@temple.edu","1801 N. Broad Street","Philadelphia","PA","191226003","2157077547","CSE","7495","7495, 7916","$0.00","This project explores a new way to increase the expressive power of Markov Random Fields (MRF) while at the same time improving the computing efficiency and the quality of solutions. The research focuses on utilizing quadratic mutual exclusion constraints (QMCs) expressed in quadratic equality form. Current approaches to increasing the expressive power of MRFs face a very challenging problem of higher computing time. In contrast, this approach is able to restrict the solution search space with QMCs, which in turn not only leads to significantly better solutions but also to reduced computing time.<br/><br/>Many problems in computer vision, including but not limited to image and video segmentation, stereo, and image restoration, object detection and recognition, tracking, and activity recognition, are formulated as optimization problems involving inference of the maximum a posteriori (MAP) solution of a Markov Random Field (MRF). QMCs are more general than mutex constraints expressed in a linear equality form. Hence QMCs offer increased expressive power to more accurately model many computer vision problems. This property is particularly important when unary and binary MRF potentials are unreliable and uninformative, which is the rule rather than an exception in real applications. Hence, this project can increase the ability of computer vision systems to broaden their application scope, ranging from image retrieval to computer vision systems on mobile robots."
"1152578","A History of the Impact of Euro-American Linguistic Technologies on Chinese Information Infrastructure","SES","STS-Sci, Tech & Society","09/01/2012","06/09/2014","Thomas Mullaney","CA","Stanford University","Continuing grant","Frederick Kronz","08/31/2016","$197,942.00","","tsmullaney@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","SBE","7603","1353","$0.00","Introduction<br/>During the height of European imperialism in the nineteenth century, China was caught up in a new development in global information technology that was not of its own design. It was predicated on a set of linguistic technologies and symbolic systems (such as the telegraph and its attendant encoding systems) that depended upon something the Chinese language does not possess, an alphabetic script. What ensued was over a century of critical experimentation with the technological limits and potentials of Chinese script. Involving a vast, transnational cast of characters, a culture of innovation emerged around the Chinese Problem, and culminated in the development of a new character-based information infrastructure that would come to govern an immense Chinese character-based information environment, including indexes, lists, catalogs, dictionaries, Braille, telegraph codes, stenograph codes, typesetting machines, typewriters, computers, text messaging, and so forth. Based upon archival and oral historical work conducted in 55 archives, special collections, museums, private collections, and sites in more than ten countries, this project charts the global history of this novel Chinese information infrastructure, employing its most important and illustrative domain, the Chinese typewriter, as a lens through which to understand this broader history.<br/><br/>Intellectual Merit <br/>This project is in direct conversation with scholarship in at least ten distinct fields beyond the history of science and technology. As a study of an unexamined aspect of modern East Asian history, this project is of critical interest to specialists in Chinese, Japanese, and Korean history. As a history involving a wide array of major corporations including IBM, the RAND Corporation, Mergenthaler, Olivetti, and Wang Laboratories (among many others), the project builds on scholarship in Business History, Economics, and Political Science. Closely connected to the history of the Chinese language itself, this project also draws upon and contributes to Linguistics and Cultural Studies. At the same time, the fascinating and unexamined history of the Chinese Typewriter Girl is likely to be of interest to researchers in Women's Studies, both inside and outside of China.<br/><br/>Potential Broader Impacts <br/>Beyond academia, this project will be of substantial interest to a broad audience within professional and public circles. It will provide easy entry to practitioners and theorists in design, human-computer interaction, natural language processing, and other branches of computer science into the still poorly understood world of Chinese information technology. Within the public media, this project will foster a deeper, empirical understanding of Chinese information technology as well; given China's emergence as a global IT powerhouse, acquiring that understanding has become more critical than ever. Results of the research will be disseminated through a wide variety of media, including single-author articles, collaborative articles, an interactive research website, public lectures, and a book monograph. This project will also continue to inform the development of new undergraduate and graduate coursework pertaining to the history of technology, particularly in non-Western contexts."
"1156990","REU Site: Research Experience for Undergraduates in Computer Vision","IIS","RSCH EXPER FOR UNDERGRAD SITES","05/01/2012","02/28/2012","Mubarak Shah","FL","The University of Central Florida Board of Trustees","Standard Grant","William Bainbridge","04/30/2015","$359,770.00","Niels da Vitoria Lobo","shah@eecs.ucf.edu","4000 CNTRL FLORIDA BLVD","Orlando","FL","328168005","4078230387","CSE","1139","9250","$0.00","This project represents a continuation of a Research Experience for Undergraduates site in Computer Vision which has operated successfully at the University of Central Florida for the past twenty four years. Approximately two hundred and forty undergraduate students from forty-five institutions all over the country have participated in this program over the years. The research focus area is Computer Vision. The current proposal is to have 10 participants per year, for three years. Each year, students will participate in a 12-week duration full-time Summer program. From lessons learned in prior years, the proposed model includes round-the-clock mentoring by a team that includes a professor, and a post-doctoral fellow or a graduate student; a streamlined short course that lets participants start their research projects sooner; daily meetings with mentors to plan activities throughout the day; training in MatLab for quick turnaround of research ideas. Participants take the short course, match themselves to a project topic that they most desire, and spend sufficient time in focused research. They then can opt for follow-through over the year by working with the professors to write a technical report on their project, to prepare for the GREs and to apply to graduate programs. In past years, a substantial fraction of our REU participants have been able to prepare a paper for submission to a refereed conference, have the paper accepted and then attend the conference to present the paper."
"1211277","SoCS: Collaborative Research: Data-Driven, Computational Models for Discovery and Analysis of Framing","IIS","SOCIAL-COMPUTATIONAL SYSTEMS","10/01/2012","09/13/2012","Noah Smith","PA","Carnegie-Mellon University","Standard Grant","William Bainbridge","09/30/2015","$232,240.00","","nasmith@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7953","7953","$0.00","This project studies framing, a central concept in political communication that refers to portraying an issue from one perspective with corresponding de-emphasis of competing perspectives.  Framing is known to significantly influence public attitudes toward policy issues and policy outcomes.  As social media allow greater citizen engagement in political discourse, scientific study of the political world requires reliable analysis of how issues are framed, not only by traditional media and elites but by citizens participating in public discourse. Yet conventional content analysis for frame discovery and classification is complex and labor-intensive. Additionally, existing methods are ill-equipped to capture those many instances when one frame evolves into another frame over time. <br/><br/>This project therefore develops new computational modeling methods, grounded in data-driven computational linguistics, aimed at improving the scientific understanding of how issues are framed by political elites, the media, and the public.  This collaboration between political scientists and computer scientists has four goals: (a) developing novel methods for semi-automated frame discovery, whereby computational models guided by political scientists? expert knowledge speed up and augment their analytical process; (b) developing novel algorithms based on natural language processing for automatic frame analysis, producing measurably accurate results comparable with reliable human coders; (c) establishing the validity of these processes on well-understood cases; and (d) applying these methods to several current policy issues, using data across years and across traditional and social media streams. The resulting evolutionary framing data will help unpack the mechanisms of framing and help predict trends in public opinion and policy."
"1149853","CAREER: Representing, Understanding, and Enhancing Scenes at the Internet-Scale","IIS","GRAPHICS & VISUALIZATION","01/15/2012","02/10/2016","James Hays","RI","Brown University","Continuing grant","Ephraim Glinert","06/30/2016","$486,222.00","","hays@gatech.edu","BOX 1929","Providence","RI","029129002","4018632777","CSE","7453","1045, 7453, 9150","$0.00","CAREER: Understanding, Representing, and Enhancing Scenes at the Internet-scale<br/><br/>Photography has an enormous impact on society -- it is our primary visual history and a medium for storytelling, entertainment, and art. But our visual world is extraordinarily complex which makes it difficult for computer vision to understand photos and for computer graphics to synthesize visual content. However, the emergence of Internet-scale photo collections in recent years enables new research directions. We use scene-based representations to leverage Internet-scale data. Scenes (places or environments) are the context in which all other visual phenomena exist and it seems possible to brute-force the space of scenes -- with millions of scenes, we find qualitatively similar scenes and create massively data-driven algorithms with capabilities that are complementary to typical bottom-up graphics and vision pipelines. The underlying principle of this study is that joint investigations of scene representations and large image databases will advance the state-of-the-art in graphics and vision. <br/><br/>First, we are investigating detail synthesis tasks which alleviate camera shake, motion blur, defocus, atmospheric scattering, or low resolution. Scene representations are robust enough to find matching scenes in Internet-scale photo collections even in the presence of dramatic blurring. These matching scenes provide a context-specific statistical model which can be used to insert convincing texture and object detail. Second, we are studying attribute-based representations of scenes. We use crowdsourcing to discover attributes and build large databases for the community. Attributes are a powerful intermediate representation for the next generation of big data imaging research which can have broad societal impact through applications such as robotics, security, assistance to vision-impaired, and vehicle safety. The investigators also are developing a new introductory course for Brown students to explore big data computing across scientific disciplines and are creating an online community for visual computing education to benefit students interested in photography and programming."
"1211201","SoCS: Collaborative Research: Data-Driven, Computational Models for Discovery and Analysis of Framing","IIS","SOCIAL-COMPUTATIONAL SYSTEMS","10/01/2012","09/13/2012","Justin Gross","NC","University of North Carolina at Chapel Hill","Standard Grant","William Bainbridge","08/31/2015","$169,540.00","","jhgross@umass.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","7953","7953","$0.00","This project studies framing, a central concept in political communication that refers to portraying an issue from one perspective with corresponding de-emphasis of competing perspectives.  Framing is known to significantly influence public attitudes toward policy issues and policy outcomes.  As social media allow greater citizen engagement in political discourse, scientific study of the political world requires reliable analysis of how issues are framed, not only by traditional media and elites but by citizens participating in public discourse. Yet conventional content analysis for frame discovery and classification is complex and labor-intensive. Additionally, existing methods are ill-equipped to capture those many instances when one frame evolves into another frame over time. <br/><br/>This project therefore develops new computational modeling methods, grounded in data-driven computational linguistics, aimed at improving the scientific understanding of how issues are framed by political elites, the media, and the public.  This collaboration between political scientists and computer scientists has four goals: (a) developing novel methods for semi-automated frame discovery, whereby computational models guided by political scientists? expert knowledge speed up and augment their analytical process; (b) developing novel algorithms based on natural language processing for automatic frame analysis, producing measurably accurate results comparable with reliable human coders; (c) establishing the validity of these processes on well-understood cases; and (d) applying these methods to several current policy issues, using data across years and across traditional and social media streams. The resulting evolutionary framing data will help unpack the mechanisms of framing and help predict trends in public opinion and policy."
"1211266","SoCS: Collaborative Research: Data-Driven, Computational Models for Discovery and Analysis of Framing","IIS","HCC-Human-Centered Computing, SOCIAL-COMPUTATIONAL SYSTEMS","10/01/2012","08/06/2013","Amber Boydstun","CA","University of California-Davis","Standard Grant","William Bainbridge","09/30/2016","$199,782.00","","aboydstun@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","CSE","7367, 7953","7367, 7953, 9251","$0.00","This project studies framing, a central concept in political communication that refers to portraying an issue from one perspective with corresponding de-emphasis of competing perspectives.  Framing is known to significantly influence public attitudes toward policy issues and policy outcomes.  As social media allow greater citizen engagement in political discourse, scientific study of the political world requires reliable analysis of how issues are framed, not only by traditional media and elites but by citizens participating in public discourse. Yet conventional content analysis for frame discovery and classification is complex and labor-intensive. Additionally, existing methods are ill-equipped to capture those many instances when one frame evolves into another frame over time. <br/><br/>This project therefore develops new computational modeling methods, grounded in data-driven computational linguistics, aimed at improving the scientific understanding of how issues are framed by political elites, the media, and the public.  This collaboration between political scientists and computer scientists has four goals: (a) developing novel methods for semi-automated frame discovery, whereby computational models guided by political scientists? expert knowledge speed up and augment their analytical process; (b) developing novel algorithms based on natural language processing for automatic frame analysis, producing measurably accurate results comparable with reliable human coders; (c) establishing the validity of these processes on well-understood cases; and (d) applying these methods to several current policy issues, using data across years and across traditional and social media streams. The resulting evolutionary framing data will help unpack the mechanisms of framing and help predict trends in public opinion and policy."
"1217043","CIF: Small: Message Passing Networks","CCF","Comm & Information Foundations","07/01/2012","06/21/2012","Devavrat Shah","MA","Massachusetts Institute of Technology","Standard Grant","Phillip Regalia","06/30/2014","$110,000.00","","devavrat@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7797","7923, 7935","$0.00","The goal of this project is to develop foundations for message-passing algorithms, in the realm of probabilistic graphical models. The project will focus on three classes of problems in graphical models: (a) learning graphical models from observed data, (b) computing the mode of a graphical model, and (c) computing marginals of variables in a graphical model. The project will identify the strengths and limitations of the popular belief propagation algorithm for all three problems. In addition, the project will develop new class of efficient message-passing algorithms with provable performance guarantees by means of exploiting the geometry of the graphical model.<br/><br/>Probabilistic graphical models have become a standard way to represent uncertainty succinctly in a wide variety of applications: communication and signal processing, computation, vision and image processing, bioinformatics, natural language processing, and more. The problems faced in these applications are largely <br/><br/>The research outcome of this project will be folded in the course titled ""Algorithms for Inference (6.438)"" taught by the PI. The course is a popular entry level graduate course which also disseminates material online through MIT Open CourseWare and going forward, it may explore possibility of wider dissemination through MITx/EdX."
"1211153","SoCS: Collaborative Research: Data Driven, Computational Models for Discovery and Analysis of Framing","IIS","SOCIAL-COMPUTATIONAL SYSTEMS","10/01/2012","09/13/2012","Philip Resnik","MD","University of Maryland College Park","Standard Grant","William Bainbridge","09/30/2015","$164,413.00","","resnik@umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","7953","7953","$0.00","This project studies framing, a central concept in political communication that refers to portraying an issue from one perspective with corresponding de-emphasis of competing perspectives.  Framing is known to significantly influence public attitudes toward policy issues and policy outcomes.  As social media allow greater citizen engagement in political discourse, scientific study of the political world requires reliable analysis of how issues are framed, not only by traditional media and elites but by citizens participating in public discourse. Yet conventional content analysis for frame discovery and classification is complex and labor-intensive. Additionally, existing methods are ill-equipped to capture those many instances when one frame evolves into another frame over time. <br/><br/>This project therefore develops new computational modeling methods, grounded in data-driven computational linguistics, aimed at improving the scientific understanding of how issues are framed by political elites, the media, and the public.  This collaboration between political scientists and computer scientists has four goals: (a) developing novel methods for semi-automated frame discovery, whereby computational models guided by political scientists? expert knowledge speed up and augment their analytical process; (b) developing novel algorithms based on natural language processing for automatic frame analysis, producing measurably accurate results comparable with reliable human coders; (c) establishing the validity of these processes on well-understood cases; and (d) applying these methods to several current policy issues, using data across years and across traditional and social media streams. The resulting evolutionary framing data will help unpack the mechanisms of framing and help predict trends in public opinion and policy."
"1248076","INSPIRE: Symmetry Group-based Regularity Perception in Human and Computer Vision","IIS","INFORMATION TECHNOLOGY RESEARC, PERCEPTION, ACTION & COGNITION, ROBUST INTELLIGENCE, INSPIRE","10/01/2012","09/09/2012","Yanxi Liu","PA","Pennsylvania State Univ University Park","Standard Grant","Jie Yang","09/30/2016","$800,000.00","Anthony Norcia, Rick Gilmore","yanxi@cse.psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","1640, 7252, 7495, 8078","8653","$0.00","This INSPIRE award is partially funded by the Robust Intelligence (RI) Program in the Division of Information and Intelligent Systems in the Directorate for Computer and Information Science and Engineering, and the Perception, Action and Cognition (PAC) Program in the Division of Behavioral and Cognitive Sciences in the Directorate for Social, Behavioral and Economic Sciences.<br/><br/>This research integrates theoretical, experimental and algorithmic thrusts to construct a novel conceptual framework for predicting and understanding the full range of regularity perception, both in humans, by measuring human brain activation and behavior, and in machines, through a computational framework for adaptive symmetry detection in computer vision. The ability to detect patterns in natural scenes serves critical biological needs while posing substantial computational difficulties for machine intelligence. Research on human and computer perception of pattern regularity has primarily focused on bilateral symmetry, despite a wide variety of regular patterns beyond reflection. A unique feature of the proposed project is to use symmetry group theory as an organizing principle for the study of both human and computer perception of patterns. Symmetry group theory, instantiated by its subgroup hierarchy, provides a formal and exhaustive categorization of all regular patterns. <br/><br/>The project sits at an interdisciplinary nexus between computer science, psychology, neuroscience, and mathematics. The outcomes of this research could potentially transform the theory of human pattern perception and make a quantum leap in robust automatic detection of real world regularities. Because patterns are ubiquitous, this research impacts all information processing systems challenged by large digital datasets that are hard to explore manually. Its impact is strengthened further by a systemic outreach to the respective research communities through interdisciplinary workshops, publications, data sharing, classroom lectures, postdoc and student training. Applications include anomaly detection in medicine and surveillance data; mobile robot localization in man-made environments; and generic pattern indexing and retrieval."
"1230793","Current problems in lightness theory","BCS","PERCEPTION, ACTION & COGNITION","09/15/2012","03/13/2015","Alan Gilchrist","NJ","Rutgers University Newark","Standard Grant","Catherine Arrington","08/31/2016","$362,081.00","","alan@psychology.rutgers.edu","Blumenthal Hall, Suite 206","Newark","NJ","071021896","9739720283","SBE","7252","9251, 7252","$0.00","The two leading theories of how the white/gray/black shade of a surface is computed in your brain will be tested against each other in a series of 10 experiments. Human observers will be presented with three-dimensional displays carefully constructed so that, under controlled viewing conditions, the two theories make different predictions regarding the gray shade that will be perceived for a test surface embedded within the display. All of the displays will incorporate a serious challenge for vision, namely, multiple areas of light and shadow. These displays will vary in terms of (a) whether the change of illumination is projected onto a surface or occurs at a corner (or bend) in the surface, (b) the number of different gray shades on each surface, and (c) the range of gray shades on a given surface. The observers will be asked to match critical parts of each display for gray level and level of illumination. The strengths and weaknesses of each of the theories will be evaluated based on the observer matches. Ideally these results will suggest how the respective strengths of the two theories might be integrated.<br/><br/>How humans see the white, gray, and black shade of objects has not yet been explained scientifically. The basic problem is that the light that an object reflects to the eye does not reveal the shade of the object because it depends so heavily on the intensity of illumination on the object. Thus any intensity of light can be reflected from any shade of gray. The problem can only be solved by analyzing the surrounding context. Advances in our understanding of the nature of this analysis are absolutely necessary for guiding brain research in this area. The work will impact computer vision as well. At the moment, no robot can simply look at an object and report its shade of gray. Work with human vision provides the best hope for making better computer programs. Ironically, the proposed work will be conducted, not using computer images, as is almost universally done in human vision research, but with actual three-dimensional displays that are far more realistic."
"1156893","REU Site: Research Experiences in Computer Science for Students at Undergraduate Institutions","CCF","RSCH EXPER FOR UNDERGRAD SITES","03/15/2012","03/22/2012","Scott Thede","IN","DePauw University","Standard Grant","Anindya Banerjee","02/28/2015","$285,630.00","Brian Howard","sthede@depauw.edu","313 South Locust Street","Greencastle","IN","461350037","7656584091","CSE","1139","9250","$0.00","This funding renews a long-running and highly-successful CISE Research<br/>Experiences for Undergraduates (REU) site at DePauw University.  The<br/>site exposes students to a range of topics in Computer Science, including<br/>pen-based computing, computer-supported cooperative work, assistive<br/>technology, natural language processing, functional programming<br/>languages, wireless sensor networks, programming pedagogy, parallel<br/>programming, and virtual reality. Students will work in small teams<br/>with a mentor and will experience conducting research as part of a team,<br/>disseminating research results, and participating in a community of<br/>scholars.<br/><br/>This REU site will allow 24 students (over three years) to experience<br/>research in a variety of areas in Computer Science.  The site focuses<br/>on recruiting students from undergraduate-only institutions, who do not<br/>have as ready access to research opportunities as students at research<br/>universities.  The site?s primary objective is to encourage talented<br/>students enrolled at undergraduate institutions to pursue graduate<br/>studies and research careers in Computer Science."
"1147089","ABI Development: Ecosynth: An Advanced Open-Source 3D Toolkit for Forest Ecology","DBI","ADVANCES IN BIO INFORMATICS","03/01/2012","12/12/2017","Erle Ellis","MD","University of Maryland Baltimore County","Standard Grant","Jennifer Weller","09/30/2018","$879,800.00","Marc Olano","ece@umbc.edu","1000 Hilltop Circle","Baltimore","MD","212500002","4104553140","BIO","1165","1165, 9178, 9179","$0.00","The University of Maryland Baltimore County is awarded a grant to develop Ecosynth, an open-source 3D toolkit for scanning woodland ecosystems based on recent innovations in computer-vision technologies coupled with an online community system for browsing, sharing, visualizing, tagging and analyzing 3D scans of terrestrial ecosystems.  Three dimensional (3D) scanning technologies are enabling fundamental advances in scientific understanding of ecosystem structure and function in woodlands (forests and shrublands) while providing new tools for rapid assessment of biodiversity and carbon storage across landscapes.  Existing 3D scanning technologies are based on precision laser scanning systems, which remain prohibitively expensive and technically challenging to deploy, limiting their application to a relatively small number of field sites, investigators and time periods.  This project aims to transform the practice of field ecology in woodlands by enabling the routine and frequent acquisition, use, and sharing of 3D scanning data by both ecologists and citizen scientists.  This will enable remote sensing and advanced 3D analytics to become a ""user-driven"" rather than ""expert-driven"" technology.  Ecosynth 3D scanning technology applies Structure from Motion algorithms to images acquired in the field using ordinary consumer-grade digital cameras, including camera-equipped cell phones, deployed in computer-optimized patterns on the ground or from the air by hobbyist remote controlled aircraft.  The Ecosynth toolkit developed by this project will be offered as an open-source development resource on the community website (http://ecotope.org/projects/ecosynth/), and to potential users as a set of tools enabling the following capabilities: 1) computer assisted optimal image acquisition for 3D scanning (ground and aerial), 2) rapid generation of high-spatial resolution georeferenced 3D scans (multispectral point clouds), 3) an online browser-based system for sharing and visualizing 3D scans of terrestrial ecosystems that will enable expert and other users to identify, tag and make other spatially explicit markups of ecological objects in a 3D virtual environment.  Development and extension of these capabilities will bring the cutting edge of ecological observations and informatics into the hands of those with the most field experience, potentially enabling the ""crowd-sourcing"" of landscape ecology.<br/><br/>Woodlands are currently targeted for carbon and biodiversity conservation by REDD+ and other international agreements and are therefore key areas for expanded scientific research and monitoring- Ecosynth tools will aid in these efforts.  Enabling regular inexpensive use of 3D imaging tools by field ecologists and citizen scientists will expand forest observation capabilities to a much wider pool of observers, thereby assisting in global forest conservation efforts and even urban forestry.  The project is built upon engaging students in interdisciplinary teams across multiple stages in their careers, from high school students (field testing), undergraduates (Ecology, Geography, Math, Computer Science, Electrical Engineering, Mechanical Engineering, and Information Systems), graduate students (Geography & Environmental Systems, Computer Science & Engineering), and postdoctoral researchers.  Work will also engage the public, guided by an advisory panel including both disciplinary experts (ecology, computer science) and conservation organizations with citizen science programs.  Ultimately the Ecosynth project aims to spark the development of an open source 3D ecology community dedicated to developing, sharing and investigating the ecology of woodlands based on the results of 3D scanning using computer vision."
"1161282","RI: Medium: Collaborative Research: Graph Cut Algorithms for Domain-specific Higher Order Priors","IIS","Robust Intelligence","06/01/2012","04/19/2014","Pedro Felzenszwalb","RI","Brown University","Continuing grant","Jie Yang","05/31/2016","$330,000.00","","pff@brown.edu","BOX 1929","Providence","RI","029129002","4018632777","CSE","7495","7495, 7924, 9150","$0.00","Optimization is a powerful paradigm for expressing and solving a variety of imaging problems. Modern optimization methods have had considerable success on problems that involve interactions between pairs of pixels. This has lead to important advances, but many imaging problems clearly require explicit modeling of higher-order interactions. This project is addressing this challenge through a close collaboration between researchers with expertise in graph algorithms and computer vision. The project is focused on two core applications: MRI image reconstruction and boundary detection in natural images. Besides their innate interest, these applications are closely related to other important imaging problems such as fMRI distortion correction, super-resolution, angiography and road detection. <br/><br/>Optimization problems with high-order interactions are inherently difficult from a computational point of view. The computational complexity can be reduced for problems with specific properties. By identifying common properties in many important imaging problems it is possible to design powerful optimization methods that are broadly applicable. The project is bringing together researchers in computer vision and algorithms. The collaboration is leading to new algorithms that are of broad interest to the computer vision and imaging communities. These algorithms have the potential to transform the way that several important classes of problems are solved. All of the algorithms being developed are being carefully evaluated, with their implementations made widely available on a web repository. Dissemination of the ideas is facilitated by workshops and mini-courses being organized at Brown, Cornell and Rutgers."
"1161860","RI: Medium: Collaborative Research: Graph Cut Algorithms for Domain-specific Higher Order Priors","IIS","Robust Intelligence","06/01/2012","05/26/2016","Ramin Zabih","NY","Cornell University","Continuing grant","Jie Yang","05/31/2017","$431,512.00","","rdz@cs.cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7495","7495, 7924, 9251","$0.00","Optimization is a powerful paradigm for expressing and solving a variety of imaging problems.  Modern optimization methods have had considerable success on problems that involve interactions between pairs of pixels.  This has lead to important advances, but many imaging problems clearly require explicit modeling of higher-order interactions.  This project is addressing this challenge through a close collaboration between researchers with expertise in graph algorithms and computer vision. The project is focused on two core applications: MRI image reconstruction and boundary detection in natural images.  Besides their innate interest, these applications are closely related to other important imaging problems such as fMRI distortion correction, super-resolution, angiography and road detection.<br/><br/>Optimization problems with high-order interactions are inherently difficult from a computational point of view.  The computational complexity can be reduced for problems with specific properties.  By identifying common properties in many important imaging problems it is possible to design powerful optimization methods that are broadly applicable. The project is bringing together researchers in computer vision and algorithms.  The collaboration is leading to new algorithms that are of broad interest to the computer vision and imaging communities.  These algorithms have the potential to transform the way that several important classes of problems are solved.  All of the algorithms being developed are being carefully evaluated, with their implementations made widely available on a web repository.  Dissemination of the ideas is facilitated by workshops and mini-courses being organized at Brown, Cornell and Rutgers."
"1161476","RI: Medium: Collaborative Research: Graph Cut Algorithms for Domain-specific Higher Order Priors","IIS","ROBUST INTELLIGENCE","06/01/2012","09/26/2013","Endre Boros","NJ","Rutgers University New Brunswick","Continuing grant","Jie Yang","05/31/2016","$354,980.00","","Endre.Boros@rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7495","7495, 7924","$0.00","Optimization is a powerful paradigm for expressing and solving a variety of imaging problems. Modern optimization methods have had considerable success on problems that involve interactions between pairs of pixels. This has lead to important advances, but many imaging problems clearly require explicit modeling of higher-order interactions. This project is addressing this challenge through a close collaboration between researchers with expertise in graph algorithms and computer vision. The project is focused on two core applications: MRI image reconstruction and boundary detection in natural images. Besides their innate interest, these applications are closely related to other important imaging problems such as fMRI distortion correction, super-resolution, angiography and road detection. <br/><br/>Optimization problems with high-order interactions are inherently difficult from a computational point of view. The computational complexity can be reduced for problems with specific properties. By identifying common properties in many important imaging problems it is possible to design powerful optimization methods that are broadly applicable. The project is bringing together researchers in computer vision and algorithms. The collaboration is leading to new algorithms that are of broad interest to the computer vision and imaging communities. These algorithms have the potential to transform the way that several important classes of problems are solved. All of the algorithms being developed are being carefully evaluated, with their implementations made widely available on a web repository. Dissemination of the ideas is facilitated by workshops and mini-courses being organized at Brown, Cornell and Rutgers."
"1162581","RI: Medium: Quantifying and utilizing confidence in machine learning","IIS","Information Technology Researc, Robust Intelligence","09/01/2012","08/31/2012","Yoav Freund","CA","University of California-San Diego","Standard Grant","Weng-keen Wong","08/31/2017","$1,000,000.00","Sanjoy Dasgupta, Kamalika Chaudhuri","yfreund@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","1640, 7495","7924","$0.00","This project defines meaningful notions of confidence in prediction, designs procedures for computing such notions, and applies these procedures to core machine learning tasks such as active learning, crowd-sourced learning, and tracking. In many applications it is helpful to have classifiers that output, together with each prediction, a rating of the confidence that the prediction is in fact correct. Existing literature either provides various ad-hoc ways for computing such ratings which typically lack a rigorous mathematical footing, or provides mathematically consistent methods (in the Bayesian framework) for computing confidence ratings under very strong assumptions that are unlikely to hold in practice. The research team investigates methods of computing measures of confidence that are mathematically rigorous while making minimal assumptions on the way data is generated, and use these measures to further develop solutions to core machine learning tasks.<br/><br/>Defining and computing mathematically sound measures of confidence lies at the heart of machine learning, pattern recognition and uncertainty in AI. Confidence-rated prediction, active learning, and tracking are fundamental tasks of machine learning and statistics that arise repeatedly in large-scale problems; this project will develop rigorous solutions to these problems. The algorithms developed in this work are tested and used in the Automatic Cameraman project, an interactive, audio-visual installation in the UCSD Computer Science department. The interactive Automatic Cameraman system are used an educational tool to be extended in many different directions, by teams of students at a variety of skill levels."
"1218729","RI: Small: Distributed Combinatorial Optimization for Crowd-Scene Analysis","IIS","Robust Intelligence, Comm & Information Foundations","09/01/2012","07/17/2017","Robert Collins","PA","Pennsylvania State Univ University Park","Standard Grant","Jie Yang","08/31/2018","$450,000.00","","rcollins@cse.psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","7495, 7797","7323, 7923, 7936","$0.00","This project develops efficient computer vision algorithms based on distributed message passing for solving crowd-scene analysis tasks such as detection and tracking of closely spaced individuals. These and other vision tasks can be formulated as discrete combinatorial optimization problems, e.g., binary linear or quadratic programs, and studying their underlying mathematical structure can yield insights that allow larger and more challenging problem instances to be addressed.  Recent theoretical work proving the correctness of message passing for some classes of binary linear programming problems is being leveraged to develop practical vision algorithms for crowd scene analysis and extended to develop algorithms for finding good approximate solutions to harder problems. The project team is also exploring approximate inference methods based on randomization and on decomposition of large-scale problems into collections of interrelated subtasks that can be solved more efficiently and in parallel.  <br/><br/>Automated vision systems can continuously monitor crowded public spaces to provide real-time situational awareness of crowd density and to detect early signs of dangerous behavior or deviations from normal traffic flow. The ability to track individuals through a crowd and to detect the interactions of groups of people has applications in the areas of homeland security, law enforcement, and defense. Results from this project will be disseminated through collaboration with other scholars, publication of peer-reviewed articles, presentations at professional meetings, introduction of course modules into the graduate and undergraduate computer science curriculum, and through public release of source code."
"1218756","RI: Small: GraphLab 2: An Abstraction and System for Large-Scale Parallel Machine Learning on Natural Graphs","IIS","CI REUSE, Robust Intelligence","08/01/2012","08/01/2012","Carlos Guestrin","PA","Carnegie-Mellon University","Standard Grant","Kenneth Whang","10/31/2012","$450,000.00","","guestrin@cs.washington.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","6892, 7495","6892, 7433, 7923","$0.00","With the growth of the Web and improvements in data collection technology in Science, datasets have been rapidly increasing in size and complexity, necessitating comparable scaling of machine learning algorithms. However, designing and implementing efficient parallel machine learning algorithms is challenging and time consuming. To address this challenge, we recently released GraphLab, a framework providing an expressive and efficient high-level abstraction satisfying the needs of a broad range of machine learning algorithms.  The performance of our system has attracted significant attention, receiving thousands of downloads from many universities and companies.<br/><br/>Currently, GraphLab only addresses batch processing in multicore settings.  In this project, we are developing GraphLab 2: addressing the much more challenging online and distributed settings, tackling: 1) Cloud-based distributed machine learning.  2) Natural graphs, with very high-degree vertices that are not amenable to graph partitioning methods.  3) Online tasks, where data and queries are streaming over time.  4) Off-core computation, since huge problems may not fit into memory, even across the cloud.<br/><br/>One of the key contributions of the project is the continual dissemination and transfer of our technology.  Our open-source software releases will continue to enable large-scale machine learning applications in science and engineering.<br/><br/>Our ambitious broader impact goals, beyond theory and systems, include the development of a new curriculum focused on preparing students for the industrial and scientific needs in this field.  Our proposed courses include ""Machine Learning on the Web"" and ""Cloud Computing for Big Machine Learning and Data Mining."""
"1258741","RI: Small: GraphLab 2: An Abstraction and System for Large-Scale Parallel Machine Learning on Natural Graphs","IIS","CI REUSE, ROBUST INTELLIGENCE","08/01/2012","09/07/2012","Carlos Guestrin","WA","University of Washington","Standard Grant","Kenneth C. Whang","07/31/2017","$450,000.00","","guestrin@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","6892, 7495","6892, 7433, 7923","$0.00","With the growth of the Web and improvements in data collection technology in Science, datasets have been rapidly increasing in size and complexity, necessitating comparable scaling of machine learning algorithms. However, designing and implementing efficient parallel machine learning algorithms is challenging and time consuming. To address this challenge, we recently released GraphLab, a framework providing an expressive and efficient high-level abstraction satisfying the needs of a broad range of machine learning algorithms.  The performance of our system has attracted significant attention, receiving thousands of downloads from many universities and companies.<br/><br/>Currently, GraphLab only addresses batch processing in multicore settings.  In this project, we are developing GraphLab 2: addressing the much more challenging online and distributed settings, tackling: 1) Cloud-based distributed machine learning.  2) Natural graphs, with very high-degree vertices that are not amenable to graph partitioning methods.  3) Online tasks, where data and queries are streaming over time.  4) Off-core computation, since huge problems may not fit into memory, even across the cloud.<br/><br/>One of the key contributions of the project is the continual dissemination and transfer of our technology.  Our open-source software releases will continue to enable large-scale machine learning applications in science and engineering.<br/><br/>Our ambitious broader impact goals, beyond theory and systems, include the development of a new curriculum focused on preparing students for the industrial and scientific needs in this field.  Our proposed courses include ""Machine Learning on the Web"" and ""Cloud Computing for Big Machine Learning and Data Mining."""
"1205426","II-NEW:  Infrastructure for Research and Education in Machine Perception and Sensory Motor Learning for Home Health Care","CNS","CCRI-CISE Cmnty Rsrch Infrstrc, EPSCoR Co-Funding","08/01/2012","08/03/2012","Gary Holness","DE","Delaware State University","Standard Grant","Jie Yang","07/31/2015","$298,226.00","","gholness@desu.edu","1200 N. Dupont Highway","Dover","DE","199012277","3028576001","CSE","7359, 9150","7359, 9150","$0.00","This project provides infrastructure to support research concerning the investigation, design, and integration of reusable sensory-motor services that proactively monitor patient health status and provide flexible tools for intervention. The shortage of skilled allied healthcare professionals and their finite capacity prevents the continuous monitoring of patient vital statistics.  Leveraging an aging population?s preference to remain at home while managing chronic illness, this project enables research into the use of machine perception and sensory motor learning for recognition and intervention of degrading health status in a home care environment.<br/><br/>The investigators are building a new infrastructure consisting of compute nodes, robotic platforms, sensing systems, and novel service framework for research and education in Machine Perception and Sensory Motor Learning for healthcare applications at Delaware State University.  By factoring perceptual systems and making them accessible services on the network, a robot becomes a virtual assemblage that recruits arbitrary resources at run-time.   Such service compositions provide a richer suite of perceptual and robotic services because it draws from the power-set of available sensory-motor apparatus.   Service compositions include methods, such as ensembles in machine learning, that construct high fidelity models for pattern learning by combining a number of simpler component models.  <br/><br/>The infrastructure built from this project enables research in many areas, such as perceptual computing, robotics, and machine learning. The broader impact of the project includes more proactive tools for detection and intervention in healthcare and training of undergraduate and graduate students (including underrepresented students in STEM) in interdisciplinary research."
"1212370","29th International Conference on Machine Learning (ICML 2012)","IIS","Info Integration & Informatics, Robust Intelligence","04/01/2012","12/16/2011","Vishwanathan Swaminathan","IN","Purdue University","Standard Grant","Todd Leen","03/31/2014","$30,000.00","","vishy@ucsc.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7364, 7495","7556","$0.00","This award provides support to students and other young researchers for travel and accommodation to the International Conference on Machine Learning (ICML) 2012 to be held at the University of Edinburgh, Scotland, June 26-July 1, 2012.  The International Conference on Machine Learning (ICML) is held annually and is considered to be the premier conference in Machine Learning.  The students funded via this grant will be selected by a peer review process based on their financial need and alignment of research areas. The student selection process will also take into consideration goals of broadening participation, both to traditionally underrepresented groups and to students whose institutions may have limited machine learning resources. The students will get an opportunity to network with experts in the area and gain valuable insights into the cutting edge of Machine Learning research. These student scholarships are very important for encouraging student participation in this premier conference and for shaping the future of the field as a whole."
"1215109","SBIR Phase I: Direct 3D Manipulation for Computer Aided Design","IIP","SMALL BUSINESS PHASE I","07/01/2012","02/20/2013","Robert Wang","CA","3Gear Systems","Standard Grant","Muralidharan S. Nair","06/30/2013","$179,682.00","","rywang@threegear.com","1110 Polynesia Dr #314","Foster City","CA","944041745","4128605468","ENG","5371","5371, 6840, 8033, 9139, HPCC","$0.00","This Small Business Innovation Research (SBIR) Phase I project aims to demonstrate the feasibility of a 3D gestural user input system for computer aided design (CAD) that enables engineering design firms to more rapidly explore and produce new designs. This technology uses commodity depth-sensing cameras and applies computer vision algorithms to deliver markerless finger-precise hand-tracking, allowing users to simply reach out with their hands to directly manipulate virtual 3D objects. In order to confirm feasibility, the objectives of the project include data gathering, investigation of computer vision algorithms, investigation of 3D input usability issues, and a thorough technical and usability evaluation. More specifically, a data set of 3D scans of hand shapes and hand poses will be collected. Next, a detailed investigation will be performed of multi-view registration and database-driven hand pose estimation algorithms for rapid calibration and tracking of a user's hands. The success of the proposed technology will crucially depend on its delivery of a superior user experience, and the usability issues regarding 3D input will be carefully analyzed. Finally, a detailed technical and usability evaluation will be performed, and the commercial viability will be based on the results.<br/><br/>The broader impact/commercial potential of this project is in the area of engineering design. In today's market place, the competitive capacity of an engineering firm is determined by its ability to rapidly design new products and evaluate new engineering solutions. Most engineering design firms use computer aided design software to accelerate this process. However, the productivity of CAD engineers is still bottlenecked by the user interface and the user input devices available to them. This is because many CAD tasks involve 3D manipulation, which is difficult and tedious to perform with a 2D mouse. A typical CAD user spends four to eight hours a day in CAD software. Over a million CAD users fit this profile. The proposed technology makes these users more productive by enabling them to directly manipulate virtual CAD objects with their bare hands in 3D, at their desks---a technical feat that has never before been achieved. The result is faster product and engineering design in a variety of industries including manufacturing, construction and architecture which spent $6B on CAD software in 2011. This research will advance scientific and technological understanding by creating a fundamentally new way to interact with computers for productivity applications."
"1161876","RI: Medium: Integrating Humans and Computers for Image and Video Understanding","IIS","ROBUST INTELLIGENCE","05/01/2012","09/27/2013","Tamara Berg","NY","SUNY at Stony Brook","Continuing grant","Jie Yang","07/31/2014","$764,288.00","Dimitrios Samaras, Gregory Zelinsky","tlberg@cs.unc.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","7495","7495, 7924","$0.00","In this project, the research team explores several research challenges to exploit the relationship between images, video, and the people viewing this visual imagery.  Areas of exploration include: 1) behavioral experiments to better understand the relationship between human viewers and imagery, 2) development of human-computer collaborative systems for image and video understanding that utilize automatic computer vision algorithms in conjunction with active and passive cues from human viewers, and 3) implementing retrieval and collection organization applications using our collaborative models.<br/><br/>Billions of images and millions of videos are now available online via the infrastructure of amazingly successful companies from Google to Microsoft to Facebook. This wealth of visual data is creating considerable opportunities for communication and community, and tightening the social fabric of our world.  In parallel to this explosion in online imagery, there is also an increasing proliferation of cameras viewing the user, from the ever present webcams peering out at us from our laptops, to cell phone cameras carried in our pockets wherever we go. This record of a user's viewing behavior, particularly of their eye, body movements, or descriptions, can provide enormous insight into how people interact with images or video, and can inform construction of more effective visual applications such as image or video retrieval.  In addition, understanding what people recognize, attend to, or describe about an image or video is a necessary step toward high level goals of human centric image understanding that will have research benefits to many diverse fields, including computer vision and behavioral science."
"1140191","Collaborative Research: Transforming the Understanding, Assessment and Prediction of Teamwork Effectiveness in Software Engineering Education using Machine Learning","DUE","S-STEM-Schlr Sci Tech Eng&Math, TUES-Type 1 Project","06/01/2012","05/22/2012","Shihong Huang","FL","Florida Atlantic University","Standard Grant","Paul Tymann","05/31/2015","$33,650.00","","shihong@fau.edu","777 GLADES RD","BOCA RATON","FL","334316424","5612970777","EHR","1536, 7513","9178, SMET","$0.00","In response to demands of the software engineering industry, this project is developing powerful machine learning-based methods to understand, assess and predict student learning of software engineering teamwork across globally-distributed teams. The project includes the following activities.  Objective and quantitative teamwork data are being collected on student activities in ongoing, jointly-taught undergraduate computer science classes at three geographically-distant institutions (San Francisco State, Florida Atlantic, and Fulda Universities). Novel machine learning tools are being applied to these data to discover models, rules and metrics that define student success in acquiring teamwork skills and that facilitate early intervention for teams at risk by identifying predictors of teamwork outcomes. With input from external evaluators, the methods and tools developed in this project are being refined and disseminated to educators for early adoption.   <br/><br/>The project advances the field of software engineering with new tools for assessing student software engineering teamwork skills. This project is the first to apply novel machine learning techniques to assess and predict student learning of these skills. In the era of global collaboration, the project is also assessing the impact of developing teamwork skills within globally-distributed teams. The participating US universities serve highly-diverse student populations; thus the project is preparing large numbers of students underrepresented in STEM fields to enter the software engineering profession and to successfully meet the challenges of communicating effectively in globally-distributed teams. As a result, the project is contributing both to diversifying and to maintaining the competitiveness of the US software engineering workforce.<br/><br/>A training workshop for software engineering educators from across the California State University system and from a San Francisco community college is part of the project."
"1140172","Collaborative Research: Transforming the Understanding, Assessment and Prediction of Teamwork Effectiveness in Software Engineering Education using Machine Learning","DUE","S-STEM-Schlr Sci Tech Eng&Math, TUES-Type 1 Project","06/01/2012","05/22/2012","Dragutin Petkovic","CA","San Francisco State University","Standard Grant","Paul Tymann","05/31/2016","$166,046.00","","petkovic@sfsu.edu","1600 Holloway Ave","San Francisco","CA","941321722","4153387090","EHR","1536, 7513","9178, SMET","$0.00","In response to demands of the software engineering industry, this project is developing powerful machine learning-based methods to understand, assess and predict student learning of software engineering teamwork across globally-distributed teams. The project includes the following activities.  Objective and quantitative teamwork data are being collected on student activities in ongoing, jointly-taught undergraduate computer science classes at three geographically-distant institutions (San Francisco State, Florida Atlantic, and Fulda Universities). Novel machine learning tools are being applied to these data to discover models, rules and metrics that define student success in acquiring teamwork skills and that facilitate early intervention for teams at risk by identifying predictors of teamwork outcomes. With input from external evaluators, the methods and tools developed in this project are being refined and disseminated to educators for early adoption.   <br/><br/>The project advances the field of software engineering with new tools for assessing student software engineering teamwork skills. This project is the first to apply novel machine learning techniques to assess and predict student learning of these skills. In the era of global collaboration, the project is also assessing the impact of developing teamwork skills within globally-distributed teams. The participating US universities serve highly-diverse student populations; thus the project is preparing large numbers of students underrepresented in STEM fields to enter the software engineering profession and to successfully meet the challenges of communicating effectively in globally-distributed teams. As a result, the project is contributing both to diversifying and to maintaining the competitiveness of the US software engineering workforce.<br/><br/>A training workshop for software engineering educators from across the California State University system and from a San Francisco community college is part of the project."
"1149789","CAREER: Machine Learning Methods and Statistical Analysis Tools for Single Network Domains","IIS","Info Integration & Informatics","01/01/2012","06/10/2016","Jennifer Neville","IN","Purdue University","Continuing grant","Sylvia Spengler","12/31/2017","$496,640.00","","neville@cs.purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7364","1045","$0.00","CAREER: Machine Learning Methods and Statistical Analysis Tools for Single Network Domains<br/><br/>Machine learning researchers focus on two distinct learning scenarios for structured network data (i.e., where there are statistical dependencies among the attributes of linked nodes). In the first scenario, the domain consists of a population of structured examples (e.g., chemical compounds) and we can reason about learning algorithms asymptotically, as the number of structured examples increases. In the second scenario, the domain consists of a single, potentially infinite-sized network (e.g., the World Wide Web). In these ""single network"" domains, an increase in data corresponds to acquiring a larger portion of the underlying network. Even when there are a set of network samples available for learning and prediction, they correspond to subnetworks drawn from the same underlying network and thus may be dependent. <br/><br/>Although estimation and inference methods from the field of statistical relational learning have been successfully applied in single-network domains, the algorithms were initially developed for populations of networks, and thus the theoretical foundation for learning and inference in single networks is scant. This work focuses on the development of robust statistical methods for single network domains -- since many large network datasets about complex systems rarely have more than a few subnetworks available for model estimation and evaluation. Specifically, the aims of the project include (1) strengthening the theoretical foundation for learning in single network domains, (2) creating accurate methods for determining the significance of discovered patterns and features, (3) formulating novel model selection and evaluation methods, and (4) developing improved approaches for network learning and prediction based on the unique characteristics of single network domains.<br/><br/>The research will enhance our understanding of the mechanisms that influence the performance of network analysis methods and drive the development novel methods for complex network domains. Expanding the applicability of machine learning techniques for single network domains could have a transformational impact across a broad range of areas (e.g., psychology, communications, education, political science) where current methods limit research to the investigation of processes in dyad or small group settings. Also, the project results will serve as an example application of computer science in the broader network science context, which will attract and retain students that might not otherwise be engaged by conventional CS topics. For more details see:<br/>http://www.cs.purdue.edu/homes/neville/research-nsf-career.html"
"1245018","Computer Vision and Astronomy: A New Technique for Exoplanet Discovery","AST","STELLAR ASTRONOMY & ASTROPHYSC","09/01/2012","08/27/2012","Rebecca Oppenheimer","NY","American Museum Natural History","Standard Grant","Maria Womack","08/31/2014","$165,366.00","","roppenheimer@amnh.org","Central Park West at 79th St","New York","NY","100240000","2127695975","MPS","1215","1207, 7916","$0.00","This award will support the development of innovative image analysis algorithms for the direct detection of faint exoplanets in the presence of scattered starlight from the host star.  The research team will try a new interdisciplinary approach, using ideas from computer vision experiments, with the goal of reducing systematic errors resulting from incomplete modeling of background and scattered light.  <br/><br/>The work is expected to result in the creation of a flexible pipeline for the analysis of exoplanets and other objects.  Codes from this project to the public will be released under open-source licenses."
"1227530","DIP: BioSourcing: A Crowdsourcing Approach to Increasing Public Understanding in Computational Biosciences","IIS","Cyberlearn & Future Learn Tech","09/01/2012","05/17/2017","Kurt Squire","WI","University of Wisconsin-Madison","Standard Grant","Amy Baylor","08/31/2017","$1,349,989.00","Michael Ferris, Enid Montague, Bilge Mutlu, Benjamin Shapiro","ksquire@uci.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","8020","8045, 8842","$0.00","This project explores the hypothesis that compelling learning games based on contemporary science that offer opportunities to contribute to scientific inquiry will lead to increased interest in science, increased career choice of science, increased conceptual understanding of science content, and better scientific literacy around what scientists do. The idea is to capitalize on crowd sourcing both to shed light on the answers to open scientific questions and to engage the public in authentic and needed scientific inquiry in meaningful ways. The PIs will extend four games that are already designed and built or that are under construction and develop a platform for supporting a broad range of participatory science games that offer the public opportunities to contribute to scientific inquiry. The chosen games all encourage sustained and deep participation, include apprenticeship opportunities and opportunities for practicing authentic science, promote reflection in and on action, and are designed to be emotionally compelling. Games come from four game genres: role-playing, strategy, action, and puzzle, as different people are drawn to different types of experiences. All are in the areas of bioscience and biotechnology, and each addresses some open question in bioscience or biotechnology that participants might shed light on. The broad range of games serves several purposes -- offering a substantial enough range of experiences that a broad range of participants can be expected to join in, offering enough diversity to know that the infrastructure tying the games together has all of the functionality required to support a broad range of such games, and offering enough diversity to answer targeted research questions. Research focuses on identifying the challenges in creating a broad and diverse public gaming community that interacts with more formal and established scientific and educational cultures, how learning occurs in such an environment and how to promote sustained engagement and deep learning, identifying core features and mechanisms of games that promote sustained engagement and science learning, and understanding the design features in the particular games being studied that contribute to sustained engagement and learning.<br/><br/>There is an increasing awareness among scientists that many contemporary science problems require (or could benefit tremendously from) an actively engaged public. Communicating the challenges and opportunities of science, and mobilizing the public to participate in and support scientific inquiry, requires shared understandings about the values, methods, and epistemologies of science (e.g., observation, data collection and analysis, reasoning from evidence, skepticism). This project focuses on design of learning opportunities that are both engaging and informative with respect to scientific literacy. The public is invited to participate in a variety of science-related ""games,"" experiences with scientific inquiry that are engaging and exciting and that can contribute to scientific findings. Participants engage as scientists, carrying out the practices of scientists and reasoning about evidence to draw conclusions, in the process experiencing the thrills and frustrations involved in scientific discovery and inquiry. Investigators observe the participants in these games to draw out principles for designing additional learning experiences that can engage the public in science and promote scientific literacy and learning at the same time. What is learned in this analysis will also be applicable to designing engaging science experiences for use in schools."
"1205695","II-New: Infrastructure to Support Integrated Research and Education in Socially Intelligent Computing at Missouri S&T","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","06/01/2012","05/31/2012","Sriram Chellappan","MO","Missouri University of Science and Technology","Standard Grant","Ephraim Glinert","05/31/2016","$281,680.00","Zhaozheng Yin","sriramc@usf.edu","300 W 12th Street","Rolla","MO","654096506","5733414134","CSE","7359","7359","$0.00","The PI's efforts to conduct original and practical research in socially intelligent computing - an emerging and important paradigm centered on integrating people and computers to create new forms of collaboration, communication, and intelligence previously unachievable by humans or computers alone - have been hindered, in scope, scale and quality, by the lack of a dedicated and realistic infrastructure.  This proposal requests funds to set up such an infrastructure at the PI's institution, which will support integrated research and education.  The infrastructure requested includes high-end computational and storage servers, desktop machines, laptops, smart phones, sensors, cameras, software and accessories for collecting, processing and extracting knowledge from large scale data arising from the daily interactions of society with the Internet and with mobile phones.  The overall goal is to utilize the knowledge gained from social computing data to create a spectrum of practical services and applications benefitting society.  In particular, three research projects that emphasize the close integrations of society with technology are identified in the proposal:  a) Detection of depressive disorders in college settings by mining Internet usage data; b) Human ""fingerprinting"" by mining Internet and smart phone usage; and c) Tracking humans in the social world by fusing heterogeneous sensor data.<br/><br/>Intellectual Merit<br/>The planned research activities are well described and will likely significantly advance the state of the art in socially intelligent computing.  The PI has pioneered the mining of real Internet data to detect depressive behavior in college students.  His prior research has identified critical Internet usage features that show strong statistical differences between students with and without depressive symptoms.   He next plans to design, using computational intelligence techniques, classifiers which can proactively detect depressive behavior in college students with high accuracy while being transparent and preserving privacy.  He is also exploring the feasibility of mining Internet usage patterns to fingerprint humans, with applications to Internet forensics and mitigation of insider attacks.  Similar techniques will be applied to mine sensor data from smart phones in order to fingerprint mobility patterns and to lay the foundation for a variety of pervasive services.  While conventional tracking algorithms leverage either a network of cameras or physical sensory data or electronic signals, the PI plans to pursue an integrated approach that fuses multiple orthogonal data source and which incorporates novel feature extraction and pattern recognition techniques for human tracking in both outdoor and indoor environments.<br/><br/>Broader Impact<br/>This project has applications in diverse areas including mental health screening, insider attack and fraud prevention, phone and vehicle theft detection, participatory sensing etc.  Research outcomes will be shared periodically with diverse stakeholders in psychology, law enforcement, forensics, business, etc. The courses taught by the PI and his team in networking, security and computer vision will be enhanced with content deriving from this project, and the infrastructure will help students learn by practical experience.  Research findings, learning materials and team experiences will be disseminated periodically to a wide audience (including educators and students in HBCUs and K-12) via conferences and the Web."
"1331009","CAREER: Application and Theory of Geometric Shape Handling","CCF","NUMERIC, SYMBOLIC & GEO COMPUT","09/01/2012","02/25/2013","Carola Wenk","LA","Tulane University","Continuing grant","Dmitry Maslov","12/31/2013","$63,316.00","","cwenk@tulane.edu","6823 ST CHARLES AVENUE","NEW ORLEANS","LA","701185698","5048654000","CSE","2865","1045, 1187, 7929, 9218, HPCC","$0.00","The objective of this research is to help increase speed, quality, and productivity of shape handling in practice. Geometric shapes are at the core of a wide range of cutting-edge technological sectors including computer vision, computer aided design (CAD), robotics, bioinformatics, computational biology, medical imaging, geographical information systems (GIS), and drug design, in which a multitude of tasks for manipulating and handling geometric shapes have to be performed efficiently and reliably.<br/><br/>This research is based on two research tracks: (i) shape handling applications and (ii) shape handling theory. The investigator will continue to foster interdisciplinary communication, contribute more theoretical soundness to applied problems, and pursue a stronger theoretical foundation which can be the basis for a wider range of applications. Specifically, this research involves several applied projects including, but not limited to, computational proteomics, computational neuroscience, and spatiotemporal traffic databases. Semi-automatic algorithms will be combined with theoretical expertise in order to pave the road for high-throughput processing in areas with very noisy data. Theoretical projects include matching and distance measure problems for curves and surfaces, multi-curve matching, initiation of a general study of geodesic distance measures in which distances are measured using shortest paths on a surface, and shape simplification. Lower bounds will be investigated in order to gain better insight into the structure of the problems, and application-friendly algorithms such as output-sensitive algorithms and approximation algorithms will be devised in order to better cope with outliers and noisy inputs.<br/><br/><br/>"
"1208519","NRI-Small: Context-Driven Haptic Inquiry of Objects Based on Task Requirements for Artificial Grasp and Manipulation","CBET","National Robotics Initiative","08/15/2012","08/13/2012","Veronica Santos","AZ","Arizona State University","Standard Grant","alexander leonessa","11/30/2014","$651,499.00","","vjsantos@ucla.edu","ORSPA","TEMPE","AZ","852816011","4809655479","ENG","8013","7923, 8086","$0.00","PI: Santos, Veronica<br/>Proposal Number: 1208519<br/><br/>Intellectual Merit: Human-like dexterous manipulation is featured prominently as a grand challenge in the 2009 Roadmap for U.S. Robotics' report. Human dexterity relies heavily on tactile sensation and is influenced by proprioceptive and visual feedback. The proposed work aims to advance artificial manipulators by integrating a new class of multimodal tactile sensors with anthropomorphic artificial hands and developing generalizable routines for context-driven haptic inquiry of objects based on task requirements for artificial grasp and manipulation. A primary goal is the development of capabilities for a robot hand to efficiently learn about objects in its unstructured environment through touch, specifically for cases where computer vision would fail to provide critical information about the physical hand-object interactions. While computer vision provides preliminary information about an object and its environment, vision alone cannot provide all essential information necessary for successful physical hand-object interactions. This is especially true when digits are occluded by the grasped object, and when the hand-object interaction is completely out of view. Inspiration for the haptic inquiry framework will be drawn from a suite of human haptic exploration procedures. In contrast to haptic exploration, haptic inquiry will require that the order and time spent on each exploratory procedure depend on task goals. The order and type of questions to be asked haptically will be context-dependent and designed to yield high-level, task-directed information at a low cost of inquiry. The weight given to each mode of tactile sensing (force, vibration, temperature) will also be tuned according to the context of the task.<br/>This proposal aims to strengthen the robustness of co-robot systems by developing a framework for context-driven, task-directed haptic inquiry that integrates multi-digit tactile and proprioception data in a task-appropriate manner. The framework will be developed and deployed on an anthropomorphic robot hand outfitted with a new class of commercially-available multimodal tactile sensors. The work is transformative because it will enable co-robot systems to remain functional even in the absence of visual feedback, which is typically the primary form of feedback for robotic systems. The long-term research objective of this proposal is to reduce the cognitive burden on the user of an artificial manipulator. <br/><br/>Broader Impacts: The proposed translational research could enhance the functional capabilities of co-robot systems in which humans use artificial manipulators to work in unstructured, unsafe, or limited access environments (prosthetic, rehabilitative, assistive, space, underwater, military, rescue, surgery).  The proposed work could benefit the human user of a co-robot system by empowering the robot with the ability to control low-level perception-action loops autonomously without burdening the human. The ROS operating system may be used to simulate and control an anthropomorphic robot hand outfitted with commercially-available tactile sensors using commercially-available actuators. Custom source code (C, MATLAB, ROS) and an open source haptic library for a commercially-available tactile sensor (suitable for data mining) will be made publicly available for the benefit and advancement of the robotics community."
"1223699","TWC: Small: Protecting Privacy of Biometric Data throughout Computation","CNS","SPECIAL PROJECTS - CISE, Secure &Trustworthy Cyberspace","12/01/2012","05/06/2013","Marina Blanton","IN","University of Notre Dame","Standard Grant","Deborah Shands","11/30/2016","$358,207.00","","mblanton@buffalo.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","CSE","1714, 8060","7434, 7923, 9102, 9178, 9251","$0.00","Progress in computer vision makes biometric authentication and recognition become more reliable and readily available than before, with a potential for ubiquitous use of such data well beyond traditional authentication. The need to protect highly sensitive biometrics is apparent, which, unlike other types of data used for authentication purposes, cannot be revoked and replaced with a new value. This means that any breach in security of biometric data has far reaching consequences to the individual than loss of other sensitive data.<br/><br/>Previously, only limited mechanisms for protecting privacy of biometry were available. To address the paucity of adequate protection mechanisms, this project develops novel techniques for enhancing privacy protection of biometric data when it is being processed and used in environments which are not fully trusted or secured in a broad set of contexts and applications that go well beyond comparing two biometric codes. The project's scope covers (i) secure interactive computation on biometric data, (ii) secure outsourcing of biometric computation to untrusted servers, and (iii) efficient verification of the correctness of the result of outsourced computation. A distinct benefit of this work is that it combines cryptographic design with practical evaluation on real biometric data, as well as includes integration of developed tools into a biometric computing grid environment. The outcomes of this research are expected to enable and promote safer practices in handing sensitive biometric data, including utilizing untrusted computing power, and a wider degree of collaboration between entities who are otherwise not permitted to share their data."
"1235958","Beyond Boredom: Modeling and Promoting Engagement during Complex Learning","DRL","REAL","01/27/2012","06/10/2013","Sidney D'Mello","IN","University of Notre Dame","Standard Grant","Gregg Solomon","08/31/2016","$1,084,752.00","","sidney.dmello@gmail.com","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","EHR","7625","9150, 9177, 9251, SMET","$0.00","The core research question of this proposal is how interactions between learners (i.e., individual differences), instructional materials (i.e., the text), and learning activities (i.e., the task) modulate engagement during learning of critical thinking skills and scientific reasoning. The proposed research will address this goal by: (a) systematically investigating the mechanisms that facilitate or hinder engagement, and (b) leveraging these insights towards the development of interventions that promote persistent and productive engagement trajectories during deep learning.   The work will be conducted at the University of Memphis.  The research subjects will be undergraduate students. <br/><br/>The research design includes four experiments in which learning gains and self-reported engagement, physiological arousal, eye gaze patterns, and facial features will be tracked while learners study instructional texts. Comprehending these texts for mastery requires active engagement as learners generate inferences, understand causality, identify problems, discriminate the quality of experimental designs, and ask diagnostic questions. Analyses will be conducted using nonlinear time series analysis techniques, such as recurrence quantification analysis. The project evaluation will include an expert advisory committee that will be used to critically review the investigators' findings and interpretations.  In addition, the investigators will develop and validate a web-based computer program that dynamically tailors both the instructional text and the learning activity to the needs and learning styles of individual learners to enhance engagement. The proposed research will balance the theoretical goal of theory building and model testing via systematic experimentation with the practical goal of developing innovative advanced learning technologies that aspire to promote engagement and learning of difficult subject matter.<br/><br/>This research is important in the STEM education field's ongoing efforts to increase engagement and the productivity of learning of STEM subject matter.  If the research is successful, the derivative knowledge and tools will be significant contributions and could be applied widely in other intelligent tutoring systems, other instructional technologies, and in our understanding of student learning in general.  This research is potentially transformative in two ways.  First, it will provide a detailed understanding in real time of engagement at the micro, relational level of student, task, and materials. Second, the creation of an intelligent tutoring system based on these findings holds the possibility of being able to 'correct' low levels of student engagement on a moment-to-moment basis and therefore boost learning productivity while increasing student satisfaction and engagement with the experience.  Dissemination will include the public availability of the technological tools as well as contributions to the scholarly literature."
"1219016","CGV: Small: RUI: Analyzing subspace structure for group level image understanding","IIS","GRAPHICS & VISUALIZATION","07/15/2012","06/06/2017","Lopamudra Mukherjee","WI","University of Wisconsin-Whitewater","Standard Grant","Jie Yang","06/30/2018","$231,313.00","","mukherjl@uww.edu","800 West Main Street","Whitewater","WI","531901705","2624725289","CSE","7453","7453, 7923, 9229","$0.00","The massive growth in the number of images being generated today has fostered a new direction in computer vision research. Images today are almost never generated independently, rather manifest as collections. Therefore, instead of thinking of images as individual entities, we now need to consider images within a group that may have significant correlational structure. Yet, formalizations of several fundamental image analysis tasks such as image segmentation, for the most part, still consider one image at a time. This project makes the case for exploiting this shared structure for understanding content in images. It develops an efficient framework which permits the segmentation of images at a group level and therefore is applicable to various scenarios in which visual data typically presents itself. The primary objective is to design a comprehensive system that addresses all aspects of this problem -- from preconditioning the input using training data, to providing powerful segmentation models that take the group structure into account, to building dictionaries which can then be used to effectively segment a set of related images. <br/><br/>This project provides a vehicle for engaging undergraduates to become immersed in research during the semesters and full-time in summers. Students are exposed to and participate in state-of-the-art research in computer vision which furthers their understanding of these topics and stimulate their intellectual curiosity. This can significantly increase the possibility of these students choosing to pursue higher studies in computer science."
"1149393","CAREER: Sensing the World with the Distributed Camera","IIS","GRAPHICS & VISUALIZATION, ROBUST INTELLIGENCE","03/01/2012","02/29/2012","Noah Snavely","NY","Cornell University","Standard Grant","Jie Yang","09/30/2018","$498,956.00","","snavely@cs.cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7453, 7495","1045, 7453","$0.00","We live in a world of ubiquitous imagery, where the number of images at our fingertips is growing at a seemingly exponential rate.  These images come from a wide variety of sources, including Internet mapping sites, webcams, surveillance and reconnaissance cameras, and millions of photographers around the world uploading billions and billions of images to photo-sharing websites.  Taken together, these sources of photos can be thought of as constituting a distributed camera capturing the entire world at unprecedented scale, and continually documenting its cities, mountains, buildings, people, and events.<br/><br/>This research is creating the basic computational tools for ""calibrating"" this distributed camera, through use of a world-wide database of 3D models built from Internet photo collections using computer vision techniques.  The focus is on creating faster, more robust algorithms for 3D reconstruction from unstructured photo collections, as well as techniques for world-scale pose estimation--computing precisely where in the world a photo was taken from image data alone.  These tools are yielding large, world-wide databases of calibrated imagery that can help answer questions in science (e.g., finding all available photos of Central Park so as to track flowering times of different plants) and engineering (e.g., finding all of the photos ever taken of a particular bridge to help figure out why it collapsed), and have impact other areas including security, consumer photography, and multimedia search.  This research is closely integrated with education and outreach, and includes plans for a summer workshop for high-school students to engage with 3D vision technologies."
"1149299","CAREER: Deep sparse dictionary context models and their application to image parsing and neuron tracking for connectomics","IIS","ROBUST INTELLIGENCE, EPSCoR Co-Funding","09/01/2012","08/30/2012","Tolga Tasdizen","UT","University of Utah","Standard Grant","Kenneth C. Whang","08/31/2017","$409,406.00","","tolga@sci.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","7495, 9150","1045, 9150","$0.00","The research objective of this proposal is to create novel computational algorithms and image processing tools that will make it possible for biologists to reconstruct large-scale neural circuits from electron microscopy volumes. Electron microscopy is a key technology in reconstruction of neural circuits at the level of individual neurons and synapses, also known as connectomics. While an important motivation of connectomics is providing anatomical ground truth for neural circuit models, the ability to decipher neural wiring maps at the individual cell level is also important in studies of many neurodegenerative diseases. State-of-the-art image analysis solutions are still far from the accuracy and robustness of human vision and biologists are still limited to studying small neural circuits using mostly manual analysis. The proposed computational models will provide biologists a tool for segmenting individual neurons and detecting other structures such as synapses in very large electron microscopy volumes, and proof reading these automatically produced results in a time efficient manner.<br/><br/>Reconstruction of a neural circuit from an electron microscopy volume involves pixel-by-pixel annotation of these images into classes such as cell membrane, mitochondria and synaptic vesicles and the segmentation of individual neurons in three dimensions. This task demands extremely high accuracy. Even with 99% pixel accuracy, an acceptable accuracy for many other applications, it is virtually certain that almost every neuron in a volume will be incorrectly segmented due to their global, tree-like structure and correspondingly large surface area. Therefore, lack of reliable automated solutions is a critical bottleneck in the field of connectomics. In this project, a novel hierarchical model will be created by combining the representation power of sparse dictionaries and their ease of learning with an inference and proof reading capability. Human experts will contribute to the process by providing ground truth for supervised learning and proof reading of automatically produced results. The combination of deep sparse dictionaries with inference using connection weights from conditional probabilities can provide a very fast way to learn hierarchical models. Several variants of the model will be studied for understanding the relative importance of feature representation, inference, symmetric connections, deep and lateral connections. The model will be applied to general object classification and image parsing problems in computer vision as well as connectomics datasets. Success will be evaluated on real datasets annotated by experts."
"1205521","II-EN: GridIron","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","06/01/2012","06/27/2012","Christopher Bailey-Kellogg","NH","Dartmouth College","Standard Grant","Wendy Nilsen","05/31/2017","$474,888.00","Thomas Cormen, Andrew Campbell, Hany Farid","cbk@cs.dartmouth.edu","OFFICE OF SPONSORED PROJECTS","HANOVER","NH","037551421","6036463007","CSE","7359","7359, 9150","$0.00","GridIron is a computing grid resource for the Dartmouth computer science department, enabling cutting-edge research and training in a wide range of projects involving analysis of large quantities of data and search over large, complex spaces.  In computational structural biology, researchers are developing and applying methods to search protein conformation spaces, to systematically decompose the protein structure universe, and to model and design protein-protein interactions for specificity.  In computer vision, researchers are developing and applying methods to authenticate digital images and to perform large-scale image search.  Other significant areas of investigation include large-scale smartphone sensing, latency mitigation, and malware detection.  The grid is also an asset for Digital Arts projects and research, including information visualization for large data sets and real-time rendering for motion capture.  Finally, the grid supports a number of education and outreach activities, including non-major, undergraduate, and graduate courses and a summer camp for high school students.  It provides invaluable practical experience in the use of large-scale computation in solving difficult challenges in the analysis of massive data sets."
"1219001","SHF: Small: Synthesis-Driven Methods for Reuse, Integration, and Programming of Specialized Accelerators in Systems-on-Chip","CCF","Software & Hardware Foundation","07/01/2012","05/08/2014","Luca Carloni","NY","Columbia University","Continuing Grant","Sankar Basu","06/30/2017","$450,000.00","","luca@cs.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","7798","7923, 7945","$0.00","A System-on-Chip (SoC) is the emerging computing platform that lies at the core of a variety of systems from computer servers in data centers to embedded systems and mobile devices. As semiconductor technology progresses, in order to deliver higher performance under tighter power constraints, SoCs increasingly combine various programmable cores, which provide precious flexibility through software, with many specialized accelerators, hardware modules optimized to execute only specific functions without dissipating too much power. The result is a heterogeneous system that is energy efficient but also very difficult to design. Indeed, the growth of SoC complexity has been outpacing progress in the computer-aided design (CAD) tools which are available to computer engineers. This design-productivity gap is a gloomy trend for the entire semiconductor industry.   The PI will address this challenge by establishing Supervised Design-Space Exploration as the foundation for a new component-based design environment in which hardware-accelerator developers, software programmers and system architects can interact effectively while they each pursue their specific goals. In particular, the PI will develop CAD methodologies and tools that:  (1) at the component-level, assist developers and programmers in the cost/performance modeling and optimization of accelerators to enable architectural exploration and to increase their reusability across many potential SoC designs; and (2) at the system-level, assist architects in the automatic integration of accelerators and other heterogeneous components to obtain an optimal implementation of a given SoC.<br/><br/>This proposal will allow the PI to train graduate and undergraduate students in the design and programming of innovative SoC platforms for a variety of application domains from computer vision to security and networking. A core part of the proposal is the development of a new capstone course that is aimed at breaking the historical boundaries between software programming and hardware design which are still present across many electrical engineering and computer science curricula."
"1217557","AF: Small: Relaxation Techniques in Symbolic-Numeric Computation","CCF","Algorithmic Foundations","09/01/2012","08/18/2012","Agnes Szanto","NC","North Carolina State University","Standard Grant","jack snoeyink","08/31/2016","$300,000.00","","aszanto@ncsu.edu","2701 Sullivan DR STE 240","Raleigh","NC","276950001","9195152444","CSE","7796","7923, 7933","$0.00","In many physical and engineering applications one needs to solve ``ill-posed'' or ``ill-conditioned'' computational problems, i.e. problems such that a small perturbation of the input substantially changes  the output.  The objective of the project is to tackle the following ill-posed or ill-conditioned problems:<br/><br/>1. Solution of consistent overdetermined systems of polynomial equations, i.e.  systems with more equations than unknowns. The coefficients of the input polynomials may be given only with limited accuracy due to measurement or rounding errors, thus the actual input system may be inconsistent. Such ill-posed problems arise for example in geometric modeling,  robotics, or computer vision.  <br/><br/>2. Decomposition of symmetric tensors which have low rank, or equivalently, decomposition of homogeneous polynomials into minimal sums of  powers of linear forms.  Again, small perturbations of the entries of the tensor will increase the rank to the ``generic rank'', so the structure of the decomposition changes, thus the problem is ill-posed. Low rank tensors have been utilized in numerous application areas where two-dimensional matrix representation of data was not sufficient for obtaining satisfying data analysis, including image and signal processing, algebraic complexity theory, higher order statistics, etc. <br/><br/>3. Solution of polynomial systems of equations which have root multiplicities. Small perturbations of the coefficients will create clusters of roots, completely changing the root structure, so these systems are ill-posed. Furthermore, roots of systems near ones with multiple roots are very sensitive to coefficient perturbations, thus they are ill-conditioned. These systems pose significant difficulties for global numerical solvers, and the distance from such degenerate systems  is closely related to the computational complexity of such solvers. <br/><br/>Although distant in appearance, these problems will be tackled using very similar techniques:  convert  them into well-conditioned optimization problems based on the underlying  geometry that made these problems ill-posed, analyze output sensitivity under perturbations of the input, and apply relaxation techniques to improve efficiency. This project builds on the PI's and her collaborators' previous results,  further advancing the understanding of how these different symbolic-numeric techniques relate to each other, and finding a unified platform which enhance their efficiency and robustness."
"1205260","II-New: Acquisition of a Light Detection and Ranging (LiDAR) Scanner System","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","05/01/2012","06/13/2012","Jyh-Ming Lien","VA","George Mason University","Standard Grant","Marilyn McClure","04/30/2017","$200,775.00","David Wong, Jim Chen, Jana Kosecka, Fernando Camelli","lienjyhming@gmail.com","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","CSE","7359","7359","$0.00","This award provides funding for a new Computing Research Infrastructure (CRI) project focused on acquisition of a LiDAR scanner, a GPS unit, and an omnidirectional video camera to collect indoor and outdoor 3D positional data and imagery at George Mason University (GMU). Several research and educational opportunities will be enabled by the requested infrastructure. The intellectual merit of this project revolves around enabling research on several CISE research fields at GMU, including three-dimensional (3D) mapping for robotic navigation and manipulation, semantic parsing, motion planning and visibility-based tracking under significant environmental uncertainties. The equipment also provides data sources for studying and evaluating important geometric algorithms under significant sensor uncertainties including data compression, scene segmentation, and object recognition. <br/><br/>The broader impacts of this project, beyond enabling research in robotic navigation and manipulation, include support for research in other disciplines, and education. The new infrastructure will also enable multidisciplinary research teams at GMU to conduct research on important problems in homeland security, home and factory automation, disaster recovery, and environmental and infrastructure monitoring; and lead to enhanced research collaboration with other institutions. Research results, collected and processed geometric data, and software developed using this instrumentation will be made available to the wider research community. With strong institution support from GMU, the acquisition has sustainable management and maintenance plans to support STEM programs for underserved communities at GMU, activities in undergraduate and graduate courses and independent studies in robotics, computer vision, geometric computing, computer game design and GIS, and will help spawn student learning and interest in these cutting-edge technology areas."
"1218310","HCC: Small: Automatic Simplification Methods for Tactile Graphics","IIS","HCC-Human-Centered Computing","09/01/2012","05/10/2016","Dianne Pawluk","VA","Virginia Commonwealth University","Continuing Grant","Ephraim Glinert","08/31/2017","$436,161.00","Carolyn Graham","dtpawluk@vcu.edu","P.O. Box 980568","RICHMOND","VA","232980568","8048286772","CSE","7367","7367, 7923, 9251","$0.00","Unfortunately for individuals who are blind and visually impaired, although access to text has greatly improved with computers, the computer has also greatly facilitated the ease of producing, storing, transmitting and using graphical representations to convey information.  This has created a serious obstacle for the millions of people in the United States who are visually impaired or blind, because graphics are now ubiquitous and used in many instances as the sole information presentation method.  Although presenting graphical information in terms of tactile diagrams is one alternative that has been proven useful, creating these diagrams remains a complex art that usually requires participation of a trained human.  This is because to be effective tactile diagrams must be simplified; otherwise, they may be impossible to understand.  Automating this process would have advantages in terms of cost, in terms of providing diagrams in a timely fashion (or even providing them at all), and in terms of independence for the intended users.  In this project, which takes a first step toward this goal, the PI and her team will focus on the automatic conversion to tactile diagrams of two kinds of graphics: photographs and line drawings.  To this end, a multi-step process is envisaged.  First, techniques to segment images without a lot of fragmentation will be examined, modified and/or developed based on the current computer vision literature.  Next, techniques will be developed for automating the simplification process, based on the manual steps as outlined by the Braille Authority of North America.  Two main aspects will be considered: simplifying lines/curves in the diagram, and reducing clutter (defined as a situation where components of the graphic are too close together or so similar that they are hard to distinguish tactually).  Then, automatic, metric rectification techniques will be explored, modified and/or developed to remove perspective from an image and replace it with a standard canonical view (because removing perspective seems to improve the user's ability to understand a tactile diagram).  Finally, user studies will be conducted to evaluate the effectiveness of the tactile graphics created by the new automatic methods in comparison to those created manually by expert tactile diagram makers.<br/><br/>Broader Impacts:  The ultimate goal of this line of research is to provide people who are blind or visually impaired with immediate and automatic access to any desired information, and thereby to remove the current barriers due to the graphical nature of computers and visual digital media.   Project outcomes will be useful not only to adult members of the target communities; they will also make it possible to provide access to children's picture books to promote development in blind and visually impaired youngsters.  The project will train at least one graduate and several undergraduate students in understanding the needs of the target communities and developing assistive technology for them.  Results will be disseminated through the usual scientific channels and also in presentations to relevant stakeholders including individuals who are visually impaired, their teachers and other rehabilitation professionals.  Results from this work will also be presented in workshops to K-12 students at the Math, Science, Innovation Center of Richmond to foster interest in STEM fields."
"1347706","CAREER: Machine Learning Approaches for Genome-wide Biological Network Inference","IIS","Info Integration & Informatics","11/15/2012","08/15/2013","Xue-Wen Chen","MI","Wayne State University","Continuing grant","Sylvia Spengler","04/30/2014","$85,226.00","","xwchen@wayne.edu","5057 Woodward","Detroit","MI","482023622","3135772424","CSE","7364","1045, 1187, 7364, 9150, 9216, HPCC","$0.00","NSF-0644366<br/>Chen, Xue-Wen<br/><br/>The objectives of this research program are (1) to develop and apply novel computational<br/>approaches for uncovering genome-wide networks of interactions between genes and proteins, and (2) to conduct related educational activities in a newly established bioinformatics program in the Department of Electrical Engineering and Computer Science at the University of Kansas. Specifically, built upon reconstructing biological networks of moderate size, the new research will computationally uncover genome-wide biological networks and map interactions of genes and proteins across a variety of organisms. The research directions include: Simultaneously integrating multiple biological knowledge into dynamic Bayesian networks for learning networks of gene interactions; learning networks of protein interactions from heterogeneous data; learning integrated networks of gene and protein interactions; learning genome-wide networks of gene and protein interactions; and cross-species network learning. It will advance the state of the art by developing machine learning methods for effectively integrating multiple prior knowledge from different sources of data, including  learning for highly heterogeneous data  and large-scale network. The research will also produce new methods and user-friendly software that can be applied by molecular biologists to gain insight into diverse biological problems, such as how biological processes are regulated on a genome scale and how individual bio-molecules interact with one another in the cell.<br/><br/>Learning with prior knowledge and highly heterogeneous data sources are fundamental to computational biology, information theory, machine learning, data mining, and other areas. Thus, the proposed research will benefit a variety of application domains including research in biology and medicine. The biological discovery derived from this project will also contribute to a variety of fields that include agriculture development, rational drug design, and health care. The research program will foster and facilitate collaborations between biologists and the PI. The educational components are closely tied to the research activities, which include (1) developing and improving bioinformatics courses that are closely related to the research outlined here and integrating them into the core bioinformatics curriculum, and (2) providing special training opportunities in the interdisciplinary area of bioinformatics for a wide-range of students, from high school through graduate school, including groups typically underrepresented in the field of science and technology.<br/><br/>"
"1118943","Connecting Lava Rheology and Flow Dynamics Using Novel Field and Modeling Techniques","EAR","PETROLOGY AND GEOCHEMISTRY","01/01/2012","12/08/2011","Einat Lev","NY","Columbia University","Standard Grant","Sonia Esperanca","12/31/2013","$150,000.00","Marc Spiegelman","einatlev@ldeo.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","GEO","1573","","$0.00","Lava flows are abundant throughout the solar system, and are the most common fashion in which erupted magmas are emplaced. Lava flows hold key information about fundamental processes of planetary evolution, but at the same time present a great risk to the communities residing near some active volcanoes. Despite their clear importance in shaping the planet and affecting society, there are many open questions regarding the properties and behavior of lava flows. This project aims to combine a novel observational technique for measuring lava deformation in the field with a comprehensive flow modeling program in order to develop a better understanding of lava physical properties and the behavior and dynamics of active flows. Gaining more accurate descriptions of the mechanical properties of lavas in their natural environment and of the processes controlling flow emplacement will help address fundamental scientific questions, such as the way oceanic crust is formed or how the faces of volcanically-active moons and planets are shaped. The proposed work is applicable to lava flows in a wide range of environments.  <br/><br/>The researchers will employ a new experimental, observational and analytical methodology designed to measure lava velocity in active channelized flows in great detail and to infer a rheology model from it. They will capture, in-situ, the entire surface velocity and temperature fields of the flowing lava using both visible and infrared high-resolution cameras. They make observations on both natural lava flows in active volcanoes (e.g., in Hawai'i or Italy) and man-made lava flows at the Lava Project experimental facility in Syracuse University (http://lavaproject.syr.edu). The theoretical aspects of this work will employ modern computer-vision techniques to extract the velocity field from the captured imagery. Data obtained in the experiments and in the field will be used to narrow down the most appropriate rheological model and parameters that are needed to describe flowing lava. This will be done by systematically examining numerical forward-models of channelized flow with varying rheologies and geometries. This work will be the first time that lava rheology and deformation are studied at such detail and close range. In parallel to the observational effort, it is planned to advance the computational tools used to model lava flows, in order to allow models that account for complex rheologies and flow structures. For example, they will strive to develop a modeling tool that will include the field-based rheological model and will support self- channelization, an important capability currently not available to the community. They will make their modeling tool general and flexible, to accommodate a wide set of eruption environments, including terrestrial, submarine and volcanic terrains on other planets."
"1314484","CPS:Medium:Quantitative Visual Sensing of Dynamic Behaviors for Home-based Progressive Rehabilitation","CNS","INFORMATION TECHNOLOGY RESEARC, CYBER-PHYSICAL SYSTEMS (CPS)","08/29/2012","04/25/2014","Yun Fu","MA","Northeastern University","Standard Grant","Sylvia J. Spengler","11/30/2016","$1,027,398.00","","y.fu@neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","1640, 7918","1640, 7752, 7918, 7924, 9178, 9251","$0.00","The objective of this research is to develop a comprehensive theoretical and experimental cyber-physical framework to enable intelligent human-environment interaction capabilities by a synergistic combination of computer vision and robotics. Specifically, the approach is applied to examine individualized remote rehabilitation with an intelligent, articulated, and adjustable lower limb orthotic brace to manage Knee Osteoarthritis, where a visual-sensing/dynamical-systems perspective is adopted to: (1) track and record patient/device interactions with internet-enabled commercial-off-the-shelf computer-vision-devices; (2) abstract the interactions into parametric and composable low-dimensional manifold representations; (3) link to quantitative biomechanical assessment of the individual patients; (4)  facilitate development of  individualized user models and exercise regimen; and (5) aid the progressive parametric refinement of exercises and adjustment of bracing devices. This research and its results will enable us to understand underlying human neuro-musculo-skeletal and locomotion principles by merging notions of quantitative data acquisition, and lower-order modeling coupled with individualized feedback. Beyond efficient representation, the quantitative visual models offer the potential to capture fundamental underlying physical, physiological, and behavioral mechanisms grounded on biomechanical assessments, and thereby afford insights into the generative hypotheses of human actions.<br/><br/>Knee osteoarthritis is an important public health issue, because of high costs associated with treatments. The ability to leverage a quantitative paradigm, both in terms of diagnosis and prescription, to improve mobility and reduce pain in patients would be a significant benefit. Moreover, the home-based rehabilitation setting offers not only immense flexibility, but also access to a significantly greater portion of the patient population. The project is also integrated with extensive educational and outreach activities to serve a variety of communities."
"1237992","STEM Achievement in Baltimore Elementary Schools (SABES)","DRL","STEM + Computing (STEM+C) Part, MSP-TARGETED AWARDS","09/15/2012","08/11/2015","Michael Falk","MD","Johns Hopkins University","Continuing Grant","Kathleen Bergin","08/31/2018","$7,414,585.00","Ekaterina Denisova, Stephen Plank, Yolanda Abel, Carolyn Parker","mfalk@jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","EHR","005Y, 1792","1792, SMET","$0.00","STEM Achievement in Baltimore Elementary Schools (SABES), a Partnership between core partners Johns Hopkins University (JHU) and Baltimore City Public Schools (BCPS), works within neighborhoods to build a Community Enterprise for STEM Learning.  Supporting Partners include the Greater Homewood Community Corporation, Park Heights Renaissance, Southeast Community Development Corporation, Greektown Community Development Corporation, Education Based Latino Outreach, Child First Authority, Baltimore National Aquarium, and the Maryland Science Center.  SABES is a unique approach to STEM education that builds expertise and excitement for STEM learning within target communities by engaging BCPS teachers and students in grades 3-5, caregivers, community-based organizations, afterschool program providers, faculty and students from JHU, members of Baltimore's high-tech businesses, and local museums.  SABES not only directly engages approximately forty STEM teachers and more than 1,620 grade 3-5 students at nine elementary schools in three high-minority, low-income, Baltimore neighborhoods, but it extends far into the wider community, drawing on the expertise of higher education faculty at JHU, museums and STEM business leaders. In the process, SABES prepares afterschool STEM project facilitators in each neighborhood, creating a community-wide culture of STEM learning relevant to the lives of its participants in ways that engage caregivers to sustain student curiosity outside of the classroom.  <br/><br/>A fundamental premise that undergirds SABES' work is that the integration of science into the learner's world, as opposed to bringing students into the world of scientists, has great potential to enable deep learning, self-efficacy and student agency.  Grounded in this perspective, SABES establishes Mutually Beneficial Partnerships (MBPs) in three low-income, majority-minority communities and employs three main strategies to obtain its goals of broad participation in science, increased student achievement in STEM, and increased teacher proficiency.  These strategies are:  (1) sustained/collaborative professional development including professional learning communities, (2) scaffolds that bridge school learning with applications of STEM in the community including biannual community STEM Recognition Events featuring students' STEM projects, and (3) STEM visiting experts from JHU and high-tech industries as well as field trips linked to careers in action.<br/><br/>The SABES research agenda pursues the following questions:  1. If the impact evaluation shows that some intended outcomes are affected in desired directions by SABES, but others not, what theory-building or holistic understandings of educational improvement efforts and mechanisms can emerge from these findings? 2. What aspects of the proposed intervention are most effective for creating a sustainable STEM community where previously there exists little expertise or organized activity outside the school? 3. How does effectiveness vary between neighborhoods or schools that differ in student composition (e.g., race/ethnicity, and English language proficiency), neighborhood resources and infrastructure, and other aspects of school organization (e.g., other high-priority initiatives in a school that might compete with SABES for staff attention, stability of principal or teacher incumbency)?  The research design employs the application of a multi-level, ecological perspective which will result in important findings related to developing science literacy in a community, engagement of formal and informal settings and structures as assets for developing teaching and learning in science, and examining the impacts on achievement, particularly related to closing the achievement gap among students of different ethnicities, language proficiencies and income levels."
"1160046","Senior Design Program on Assistive Technology to Aid Visually Impaired People","CBET","Disability & Rehab Engineering","09/01/2012","08/17/2012","Jizhong Xiao","NY","CUNY City College","Standard Grant","Michele Grimm","08/31/2017","$124,998.00","Zhigang Zhu","jxiao@ccny.cuny.edu","Convent Ave at 138th St","New York","NY","100319101","2126505418","ENG","5342","010E","$0.00","PI: Xiao, Jixhong<br/>Proposal Number: 1160046<br/><br/>The objective of this project is to create a two-semester joint senior design program on assistive technology for CCNY undergraduate seniors in electrical engineering (EE), computer science (CS), and computer engineering (CpE), providing them with major engineering design opportunities to experience real-world entrepreneurial challenges. The proposal seeks funding for 6 senior design projects per year for a period of 5 years. The proposed senior design program builds on our existing capstone design course structure, but with a new concentration on assistive technology for visually impaired people. It will also fully utilize the opportunities provided by the CCNY Entrepreneurship program. In the first semester of the year-long senior design program, the course instructor will offer general lectures on project management which will introduce Total Design methodology, market analysis, IP issues, entrepreneurship, etc. In parallel, the PIs will offer technical lectures to introduce state-of-the-art of assistive technology and empower students with necessary technical skills to perform design. Under the guidance of the PIs, the students will work in teams in a multidisciplinary environment to analyze the real needs of blind/visually impaired people, propose design concept and custom design assistive devices throughout the year to directly help the blind users. In addition, the student teams are encouraged to attend the seminars offered by CCNY entrepreneurship program and to compete for the Kaylie Prize for Entrepreneurship. The winning teams will receive additional financial support and on-campus housing to work over summers to turn their ideas into successful business start-ups.<br/><br/>Intellectual Merits: Leveraging Prof. Xiao's expertise in robotic navigation and Prof. Zhu's expertise in computer vision and scene understanding, the two PIs have been working on the development of human centric assistive navigation systems for the blind people to achieve independent travel in unfamiliar environments. The senior design program creates a great opportunity for students to work on the latest technology and custom design assistive devices and software tools to aid visually impaired people using multimodal sensing techniques and simultaneous localization and mapping (SLAM) methods. The pieces of work produced in the senior design projects will directly help the blind people, and when put together, will constitute a more sophisticated assistive navigation system which will outperform the existing travel aids in the market. The senior design program provides an ideal platform to involve students from multiple disciplines (EE, CS, CpE) in transformative research to contribute their knowledge and skills learned from their 4-year undergraduate training in circuit design, computer programming and human-computer interaction. This cross-disciplinary design experience on assistive devices will prepare the students to be competent in their engineering careers. The students will be guided by the PIs to exercise Total Design practice in every stages of the design effort: user needs and market analysis, product design specification, design ideas, detail design, prototyping, usability testing, and business planning.<br/><br/>Broader Impact: The research results and assistive device prototypes produced in the senior design projects have great potential for commercialization, which will directly benefit blind/visually impaired people and assist them to access unfamiliar environments. The team will work closely with the New York State Commission for the Blind and Visually Handicapped (CBVH) next door to City College on usability testing. The senior design program on assistive technology inspires innovation and supports the training of the future work force from our traditionally under-represented students at CCNY (1/3 Black, 1/3 Hispanic, many women, as well as disabled students) to meet the critical national need for smart healthcare that promoting the wellbeing for all. The results produced in the senior projects will be disseminated through technical publications, outreach to the visual-aid communities (CCVIP, CBVH, etc.), and design studio wiki pages."
"1218823","NETS: Small: Machine Learning Based Algorithms for Quasi-Static Ad Hoc Wireless Networks","CNS","Networking Technology and Syst","09/01/2012","07/20/2012","Rohit Negi","PA","Carnegie-Mellon University","Standard Grant","Wenjing Lou","08/31/2016","$300,000.00","","negi@ece.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7363","7923","$0.00","Quasi-static ad hoc wireless networks are models for city-wide mesh networks, machine-to-machine networks deployed for control, and certain sensor networks, which are of growing importance in applications such as smart electricity grids, transportation grid control and infrastructure monitoring. The goal of this project is to design distributed online network protocols that learn the characteristics of such networks and self-optimize to achieve more predictable performance, such as delay guarantees. This is especially important for networks such as smart grid, where end-to-end delay must be predictable. A factor graph is used to model the probability distribution over the allowed network control actions. A novel information theoretic formulation is being investigated, which measures the information sharing required to coordinate network actions, between devices and across the network stack within each device. The solution to this problem will then be obtained by using statistical sampling techniques from machine learning, providing the online algorithm for network control. This algorithm will be distributed, because it is obtained by minimizing the information sharing required. Thus, the project will provide a formal mathematical basis for the probabilistic design of distributed protocols for quasi-static ad hoc networks, utilizing the information theoretic concepts of information, entropy, and side-information to quantify the value of distributed actions. <br/><br/>Results from this project will be published and presented in major professional conferences and journals, and will be available to the wider public. The project will support the training of doctoral students in the important field of wireless networks. The theoretical framework obtained will be discussed in courses on statistical engineering methods and information theory. The MAC protocol concepts will be used to augment the PI?s under-graduate experimental set-up based on software radios, so that students of communication theory can obtain hands-on experience with system design."
"1217676","RI: Small: Collaborative Research: Ontology based Perceptual Organization of Audio-Video Events using Pattern Theory","IIS","Robust Intelligence","09/01/2012","05/13/2013","Sudeep Sarkar","FL","University of South Florida","Standard Grant","Jie Yang","08/31/2016","$257,843.00","","sarkar@usf.edu","4019 E. Fowler Avenue","Tampa","FL","336172008","8139742897","CSE","7495","7495, 7923, 9251","$0.00","It is natural that events of interest in observed scenes manifest themselves across multiple sensing modalities - vision, hearing, smell, etc. The remarkable perceptions of audio-video signals by natural systems, such as humans, also points to superiority of inferences drawn across modalities. It, therefore, seems natural to enhance performance of automated systems by using joint, cross-modal statistical inferences. However, the detection, organization, and understanding of cues and events in real-world scenarios are difficult tasks. This project seeks to develop a pattern-theoretic framework for achieving these goals. The main research items are: (1) development of mathematical quantities to represent audio and visual events and their spatiotemporal relations, (2) use domain-specific ontologies to impose semantic structure and to incorporate prior knowledge, and (3) derive algorithms for Bayesian inferences using efficient adaptations of Markov Chain Monte Carlo sampling. <br/><br/>The use of pattern theory allows bridging of gaps between raw signals and high-level, domain-dependent semantics, and helps discovers large groups of audio-visual events likely to represent the same underlying event. This effort combines ideas from perceptual organization in computer vision, computational analysis of auditory signals, pattern theory, and prior developments in ontological structures. The methods developed here are applicable to many scenarios that deploy audio and video sensors, including problems of audio annotations of videos, speaker tracking in teleconferencing, and separation of multiple objects in remote surveillance. Broader impact activities involve the development of teaching modules, innovation and entrepreneurial training of the students, and communication of the findings to the community."
"1242042","CVPR 2012 Conference Doctoral Consortium","IIS","Robust Intelligence","06/15/2012","06/07/2012","James Hays","RI","Brown University","Standard Grant","Jie Yang","05/31/2013","$15,050.00","","hays@gatech.edu","BOX 1929","Providence","RI","029129002","4018632777","CSE","7495","7495, 7556, 9150","$0.00","This travel grant supports senior PhD students who are nearing graduation to take part in a doctoral consortium at the 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). The doctoral consortium highlights the work of these up and coming researchers, and pairs each student with a senior member of the computer vision community who serves as their mentor. The mentorship process provides each student with valuable feedback on their research, as well as meaningful career advice as the students move on to the next phase of their professional development. The doctoral consortium event aims to have representation from a diverse group of participants (in terms of gender, ethnic background, academic institution and geographic location). The travel grant ensures participation from a broad range of institutions across the country and gives visibility to a diverse population of students."
"1217515","RI: Small: Collaborative Research: Ontology based Perceptual Organization of Audio-Video Events using Pattern Theory","IIS","ROBUST INTELLIGENCE","09/01/2012","05/13/2013","Anuj Srivastava","FL","Florida State University","Standard Grant","Jie Yang","08/31/2016","$255,648.00","","anuj@stat.fsu.edu","874 Traditions Way, 3rd Floor","TALLAHASSEE","FL","323064166","8506445260","CSE","7495","7495, 7923, 9251","$0.00","It is natural that events of interest in observed scenes manifest themselves across multiple sensing modalities - vision, hearing, smell, etc. The remarkable perceptions of audio-video signals by natural systems, such as humans, also points to superiority of inferences drawn across modalities. It, therefore, seems natural to enhance performance of automated systems by using joint, cross-modal statistical inferences. However, the detection, organization, and understanding of cues and events in real-world scenarios are difficult tasks. This project seeks to develop a pattern-theoretic framework for achieving these goals. The main research items are: (1) development of mathematical quantities to represent audio and visual events and their spatiotemporal relations, (2) use domain-specific ontologies to impose semantic structure and to incorporate prior knowledge, and (3) derive algorithms for Bayesian inferences using efficient adaptations of Markov Chain Monte Carlo sampling. <br/><br/>The use of pattern theory allows bridging of gaps between raw signals and high-level, domain-dependent semantics, and helps discovers large groups of audio-visual events likely to represent the same underlying event. This effort combines ideas from perceptual organization in computer vision, computational analysis of auditory signals, pattern theory, and prior developments in ontological structures. The methods developed here are applicable to many scenarios that deploy audio and video sensors, including problems of audio annotations of videos, speaker tracking in teleconferencing, and separation of multiple objects in remote surveillance. Broader impact activities involve the development of teaching modules, innovation and entrepreneurial training of the students, and communication of the findings to the community."
"1162095","RI: Medium: Automated Calibration of Ultrasound for Image-Guided Surgical Procedures","IIS","Robust Intelligence","07/01/2012","11/30/2015","Gregory Chirikjian","MD","Johns Hopkins University","Continuing Grant","Reid Simmons","06/30/2017","$1,024,444.00","Emad Boctor Mikhail","gregc@jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","CSE","7495","7924","$0.00","This project, developing advanced mathematical and computational methods for the online calibration of ultrasound probes that takes into account a probabilistic version of the well-known AX = XB sensor-calibration problem that has been overlooked in the robotics and computer vision literature, will advance current capabilities in computer-integrated surgical interventions, leading to lower radiation exposure to patients and better outcomes for minimally-invasive surgery.<br/><br/>Ultrasound has many benefits for minimally-invasive surgical procedures, including cost, ease-of-use, and patient/doctor radiation exposure. But ultrasound images are fuzzy and require extensive training for proper use during surgical procedures. As a result, outcomes are heavily dependent on an individual surgeon's skill with the device. <br/><br/>Broader Impacts: Beyond the potential benefits to surgical procedures, the Laboratory for Computational Sensing and Robotics (LCSR) at JHU has an established summer program for visiting undergraduate students that will facilitate involvement of undergraduates in the proposed research. In addition, the PI continues to mentor high school students from Baltimore Polytechnic High School through research experiences both during the academic year and the summer. The hands-on and visual nature of ultrasound image acquisition together with the mathematical problems of registration and calibration make this an ideal project to introduce students to the importance of mathematics."
"1161350","GOALI/Collaborative Research:  Modeling, Monitoring, and Analysis of Spatial Point Patterns for Manufacturing Quality Control","CMMI","GOALI-Grnt Opp Acad Lia wIndus, MANFG ENTERPRISE SYSTEMS","07/01/2012","04/12/2012","Yong Chen","IA","University of Iowa","Standard Grant","Diwakar Gupta","06/30/2015","$187,469.00","","yong-chen@uiowa.edu","2 GILMORE HALL","IOWA CITY","IA","522421320","3193352123","ENG","1504, 1786","071E, 1504, 1786, 9147, 9150, MANU","$0.00","The research objective of this Grant Opportunity for Academic Liaison with Industry (GOALI) collaborative research award is to establish a series of quality control methodologies on modeling, monitoring, and diagnosis of spatial point patterns. A spatial point pattern is a set of locations randomly distributed within a designated region. It is a natural way to model many critical quality characteristics in various manufacturing processes, such as the surface defects on steel bars, slabs and semiconductor wafer, and the distribution of reinforce particles in composite materials. The research approach is in three focus areas: (i) Modeling and monitoring of replicated spatial point patterns by integrating the deterministic point pattern alignment methods and spatial statistics techniques; (ii) Identification of the impacts of covariates on replicated point patterns based on a nonparametric functional regression model; and (iii) Geometric and three-dimension point pattern detection by bringing the Hough Transform method, an interesting computer vision method, into the quality control area.<br/><br/>If successful, the results of this research will provide a novel set of quality control tools to various relevant industries such as steel rolling, semiconductor manufacturing, and composite fabrication. These tools will take advantage of the increasingly available spatial point data and help to significantly improve the process productivity and quality. The close collaboration with OG Technologies in this Grant Opportunities for Academic Liaison with Industry project will lead to a realistic testbed and quick dissemination of research results among practitioners, as well as initiation of technology transfer. The interdisciplinary nature of this project can provide students the unique opportunity to obtain training in spatial statistics, manufacturing quality control, and computer vision."
"1161077","GOALI/Collaborative Research:  Modeling, Monitoring, and Analysis of Spatial Point Patterns for Manufacturing Quality Control","CMMI","GRANT OPP FOR ACAD LIA W/INDUS, MANFG ENTERPRISE SYSTEMS","07/01/2012","04/12/2012","Shiyu Zhou","WI","University of Wisconsin-Madison","Standard Grant","diwakar gupta","06/30/2015","$202,387.00","Tzyy-Shuh Chang","szhou@engr.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","ENG","1504, 1786","071E, 1504, 1786, 9147, MANU","$0.00","The research objective of this Grant Opportunity for Academic Liaison with Industry (GOALI) collaborative research award is to establish a series of quality control methodologies on modeling, monitoring, and diagnosis of spatial point patterns. A spatial point pattern is a set of locations randomly distributed within a designated region. It is a natural way to model many critical quality characteristics in various manufacturing processes, such as the surface defects on steel bars, slabs and semiconductor wafer, and the distribution of reinforce particles in composite materials. The research approach is in three focus areas: (i) Modeling and monitoring of replicated spatial point patterns by integrating the deterministic point pattern alignment methods and spatial statistics techniques; (ii) Identification of the impacts of covariates on replicated point patterns based on a nonparametric functional regression model; and (iii) Geometric and three-dimension point pattern detection by bringing the Hough Transform method, an interesting computer vision method, into the quality control area.<br/><br/>If successful, the results of this research will provide a novel set of quality control tools to various relevant industries such as steel rolling, semiconductor manufacturing, and composite fabrication. These tools will take advantage of the increasingly available spatial point data and help to significantly improve the process productivity and quality. The close collaboration with OG Technologies in this Grant Opportunities for Academic Liaison with Industry project will lead to a realistic testbed and quick dissemination of research results among practitioners, as well as initiation of technology transfer. The interdisciplinary nature of this project can provide students the unique opportunity to obtain training in spatial statistics, manufacturing quality control, and computer vision."
"1227495","NRI-Large: Collaborative Research: Purposeful Prediction: Co-robot Interaction via Understanding Intent and Goals","IIS","NRI-National Robotics Initiati","10/01/2012","03/01/2017","James Bagnell","PA","Carnegie-Mellon University","Continuing grant","Ephraim Glinert","09/30/2017","$2,175,468.00","Martial Hebert, Anind Dey","dbagnell@ri.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","8013","7367, 7925, 8086, 9251","$0.00","In order for robots to collaborate with humans, they need to be able to accurately forecast human intent and action.  People act with purpose: that is, they make sequences of decisions to achieve long-term objectives. For instance, in driving from home to a store, people carefully plan a sequence of roads that will get them there efficiently. In predicting a person's next decision, algorithms must be developed that reflect these purposeful actions. <br/><br/>Currently, robots are unable to anticipate human needs and goals, and this represents a fundamental barrier to their large-scale deployment in the home and workplace. The aim of this project is to develop a new science of purposeful prediction that can be applied to human-robot interaction across a wide variety of domains.  The work draws on recent techniques based on Inverse Optimal Control and Inverse Equilibria Theory that enable statistically sound reasoning about observed deliberate behavior. These new methods provide the foundations of a theoretical framework that integrates traditional decision making techniques like optimal control, search and planning with probabilistic methods that reason about uncertainty and hidden information, particularly about goals, utility and intent. <br/><br/>Intellectual merit:  The project will provide a general framework that allows robots to anticipate and adapt to the activities of their human co-workers based on perceptual cues. The investigators will develop the theory, a computational toolbox, and, in collaboration with industrial partners, prototype deployments of these new methods for the prediction of peoples' behavior in a diverse set of robotics domains from computer vision to motor control. The project is transformative in that it combines a novel theoretical/algorithmic framework with extensive support in terms of volume of data and validation infrastructure in the context of many applications.        <br/><br/>Broader impacts: A revolution in personal robotics in both the home and workplace depends on the ability to forecast human activities and intents; small- and medium- scale manufacturing will make a leap forward through agile robotic systems intelligent enough to understand and assist their co-workers in flexible assembly tasks; and robust models of pedestrian and vehicular traffic flow will enable more effective driver warning systems and safer autonomous mobile robots. Purposeful prediction technology is an important step towards enabling such understanding of actions and intents in these arenas. The research work will involve the training and mentoring of undergraduate, masters and doctoral students as well as post-doctoral fellows in this emerging multi-disciplinary research area at the intersection of computer and cognitive sciences and robotics."
"1159695","Collaborative Research: Using a Fully Autonomous Brain-Body Interface to Study the Cortical Dynamics of Learning","CBET","Engineering of Biomed Systems","07/15/2012","07/18/2012","Emilio Bizzi","MA","Massachusetts Institute of Technology","Standard Grant","Athanassios Sambanis","06/30/2015","$215,000.00","","ebizzi@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","ENG","5345","004E, 137E","$0.00","1159652/1159695<br/>Brown/Bizzi<br/><br/>The brain controls movements of the body by means of neural signals transmitted through the spinal cord.  Researchers in the field of Motor Neural Prosthetics attempt to tap into these neural signals and use them to control artificial actuators, such as a robotic arm or computer cursor, or native limbs that have been paralyzed. The long-term goal of such research is to help restore function to a variety of patient populations for whom the normal spinal pathways of movement control have been disrupted due to neurological disease, brain and or spinal cord injury. At present, however, the movements generated by neuroprosthetic devices lack the smoothness, and fluidity of natural movement.  Furthermore, although Brain-Machine Interfaces would appear to provide a unique opportunity for studying brain function, the technology of neural prosthetics has not yet made major contributions to an improved understanding of how the circuits in the brain that control movement work. <br/><br/>This grant proposes the construction of a novel Brain-Body interface in animals for which elbow function is reversibly paralyzed. By connecting the output of recording electrodes placed in the brain to stimulating electrodes placed near the paralyzed elbow muscles, a pathway is created to re-establish control of the lost motor function.  Unlike most research in the field, this research requires that all of the learning takes place within the brain and none of the learning takes place through a ""Decoder"" - that is, a computer-based machine-learning algorithm that attempts to read the mind of the user and extract the desired signals to control the movement. Thus, our Brain-Body interface is fully autonomous and requires no outside intervention. Initially, performance may be inadequate. Just as an infant requires considerable time to establish control over motor pathways, so too will a subject require time to learn to control this entirely new pathway. However, given adequate learning time, this architecture is likely to lead to superior performance, because all of the control resides within the brain and because the brain has a remarkable adapt and learn.  <br/><br/>There are two key intellectual merits to the proposed research.  The first is to use this novel preparation to the study natural systems-level mechanisms of learning present in the brain, so that how our brains self-organize during development and how our brains adapt to injury or disease are better understood. The second is to transform the fields of Brain-Machine and Brain-Body Interfacing by exploring the level of control attainable when the native brain does all of the learning rather than computer algorithms. The research has additional broader significance.  In particular, through this work it is hoped that a better-performing Brain-Body interface can be developed to help restore movement function in those suffering from neurological disorders or brain injury. Moreover, this work further cultivates the methodology of Direct Brain Control (a particular form of biofeedback) for the design of rehabilitative devices. In so doing, it enhances the existing research infrastructure by bringing together clinical anesthesiologists, neurophysiologists, and engineers to forge a cross-disciplinary solution to this important problem."
"1249316","EAGER: Preliminary Study of Hashing Algorithms for Large-Scale Learning","IIS","Info Integration & Informatics","09/01/2012","08/18/2012","Ping Li","NY","Cornell University","Standard Grant","Sylvia Spengler","08/31/2014","$100,000.00","","pingli@stat.rutgers.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7364","7364, 7916","$0.00","Many emerging applications of data mining call for techniques that can deal with data instances with millions, if not billions of dimensions. Hence, there is a need for effective approaches to dealing with extremely high dimensional data sets. <br/><br/>This project focuses on a class of novel theoretically well-founded hashing algorithms that allow high dimensional data to be encoded in a form that can be efficiently processed by standard machine learning algorithms. Specifically, it explores: One-permutation hashing, to dramatically reduce the computational and energy cost of hashing; Sparsity-preserving hashing, to take advantage of  data sparsity for efficient data storage and improved generalization; Application of the new hashing techniques with standard algorithms for learning  ""linear""  separators in high dimensional spaces. The  success of this EAGER project could lay the foundations of a longer-term research agenda by the PI and other investigators focused on developing effective methods for building predictive models from extremely high dimensional data using ""standard"" machine learning algorithms. <br/><br/>Broader Impacts: Effective approaches to building predictive models from  extremely high dimensional data can impact many areas of science that rely on machine learning as the primary methodology for knowledge acquisition from data. The PI's education and outreach efforts aim to  broaden the participation of women and underrepresented groups. The publications, software, and datasets resulting from the project will be freely disseminated to the larger scientific community."
"1257163","EAGER: Smart Space-Time Sampling for Recovering and Recognizing Dynamic Scenes","IIS","Robust Intelligence","09/15/2012","07/30/2013","Jinwei Gu","NY","Rochester Institute of Tech","Standard Grant","Jie Yang","08/31/2014","$91,512.00","","jwgu@cis.rit.edu","1 LOMB MEMORIAL DR","ROCHESTER","NY","146235603","5854757987","CSE","7495","7495, 7916","$0.00","Traditional, dynamic scenes are captured as video frames sampled at regular space-time grids. For many computer vision tasks, however, this uniform sampling may be either inefficient (e.g., low light, high-speed motion) or unnecessary (e.g., motion/change/event detection). This project explores non-uniform, adaptive sampling schemes that exploit the underlying structures of space-time volumes (e.g., sparsity, temporal coherence, statistical priors). These sampling schemes are implemented with novel programmable pixel-wised coded exposure and aperture in cameras. The captured information-rich coded projections of space-time volumes are used for video reconstruction or directly as features for motion/event detection. In addition to higher efficiency in imaging and higher signal-to-noise ratio in reconstructed results, the method also provides benefits in data security and privacy protection for video surveillance because decoding the captured images requires the knowledge of coded patterns and dictionaries. <br/><br/>This research has many applications in surveillance, machine vision inspection, and high-speed imaging. The developed technology is being tested in transportation imaging for traffic monitoring and accident detection. A database of high-speed videos of traffic scenes and events is being captured and plan to be released online when it is finished. In addition to videos, the technical approach can also be applicable to other high-dimensional signals such as light fields or light transport matrices."
"1218880","RI: Small: Active Learning with Rich Query Types on Networks and Trees","IIS","Robust Intelligence","09/01/2012","07/31/2012","Mark Craven","WI","University of Wisconsin-Madison","Standard Grant","Rebecca Hwa","08/31/2016","$449,917.00","","craven@biostat.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","7495","7923","$0.00","Supervised machine learning is a critical component of software systems in a wide variety of applications.  Although models induced via supervised learning algorithms often provide state-of-the-art accuracy, they are not applied as widely as they could be because they require labeled training instances, which are often expensive to acquire.  One promising approach to addressing this limitation is to employ active learning algorithms.  These methods are able to make queries in which they choose which instances are labeled and added to the training set.  The goal of this project is to develop a new class of algorithms for active learning that broadens the applicability of this approach to more complex, realistic settings.  Specifically, we will develop methods that (i) address complex learning tasks such as biological network reconstruction and event extraction from natural language, (ii) assemble batches of queries when it is cost effective to do so, (iii) are able to employ a variety of query types, and (iv) reason about the costs incurred for various queries.<br/><br/>Machine learning represents an important methodology for inferring models that can make useful predictions in scientific, educational, health-care, business and consumer applications.  The methods to be developed in this project will provide substantial benefits to machine-learning applications in such problem domains by reducing the cost required to obtain enough data to learn accurate models. Moreover, because this project is connected to specific collaborations with biologists, it is likely to have a direct impact on the ability of scientists to design, conduct and interpret experiments investigating networks of complex relationships such as host-virus interactions.  The project will also play a role in training undergraduate and graduate students in interdisciplinary research, and in recruiting undergraduate students from under-represented minority groups into scientific careers."
"1218712","III: Small: Pattern Learning in a Minimax Framework","IIS","Info Integration & Informatics","09/01/2012","08/10/2012","Qiang (Shawn) Cheng","IL","Southern Illinois University at Carbondale","Standard Grant","Sylvia Spengler","08/31/2017","$254,661.00","","qiang.cheng@uky.edu","Ofc. of Sponsored Projects Admin","Carbondale","IL","629014308","6184534540","CSE","7364","7364, 7923","$0.00","Machine learning currently offers one of the most cost-effective approaches to building predictive models from data. However, practical applications of machine learning have to cope with sparsity, noise, and uncertainty of data present. Against this background, this project aims to: 1) Introduce a minimax framework for pattern learning to unify various regularization models, and characterize a variety of data uncertainty (e.g. incomplete data, local nonrigid displacements, lighting variations) and the corresponding regularizations regarding their intrinsic properties (e.g. sparsity, locality, robustness); 2) Establish new feature subset selection methods and quantify group effects as well as confidence levels/intervals of selecting/discarding features; 3) Construct new methods of sparse grouping representation for resilience to labeling errors by exploiting effective regularizations and their properties. The project aims to explicitly model various classes of data uncertainty (distortions) within a minimax framework, to optimize pattern learning process based on the worst distortion(s) in a given class, and to exploit regularization properties (e.g. sparsity, robustness).<br/><br/>Anticipated results of the project include: (1) New models and methods for accounting for various classes of distortions and for finding optimal solutions under the worst distortion(s) over a given class; (2) New methods for selecting features with confidence analysis and for learning predictive models resilient to labeling errors; (3) Rigorous evaluation of the resulting methods on real-world data sets.<br/><br/>The new machine learning algorithms resulting from this research find applications in many areas that rely on predictive modeling from large data sets(e.g. medical analysis, earthquake modeling). All of the software tools developed in this project will be made available to the scientific community, educators and students. The project offers enhanced research-based training opportunities for graduate and undergraduate students, as well as outreach to K-12 students, and efforts aimed at broadening the participation of under-represented groups in Computer Science research and education."
"1159652","Collaborative Research: Using a Fully Autonomous Brain-Body Interface to Study the Cortical Dynamics of Learning","CBET","Engineering of Biomed Systems","07/15/2012","07/18/2012","Emery Brown","MA","Massachusetts General Hospital","Standard Grant","Athanassios Sambanis","06/30/2015","$215,000.00","Wasim Malik, jonathan winograd","ebrown@partners.org","Research Management","Somerville","MA","021451446","8572821670","ENG","5345","004E, 137E","$0.00","1159652/1159695<br/>Brown/Bizzi<br/><br/>The brain controls movements of the body by means of neural signals transmitted through the spinal cord.  Researchers in the field of Motor Neural Prosthetics attempt to tap into these neural signals and use them to control artificial actuators, such as a robotic arm or computer cursor, or native limbs that have been paralyzed. The long-term goal of such research is to help restore function to a variety of patient populations for whom the normal spinal pathways of movement control have been disrupted due to neurological disease, brain and or spinal cord injury. At present, however, the movements generated by neuroprosthetic devices lack the smoothness, and fluidity of natural movement.  Furthermore, although Brain-Machine Interfaces would appear to provide a unique opportunity for studying brain function, the technology of neural prosthetics has not yet made major contributions to an improved understanding of how the circuits in the brain that control movement work. <br/><br/>This grant proposes the construction of a novel Brain-Body interface in animals for which elbow function is reversibly paralyzed. By connecting the output of recording electrodes placed in the brain to stimulating electrodes placed near the paralyzed elbow muscles, a pathway is created to re-establish control of the lost motor function.  Unlike most research in the field, this research requires that all of the learning takes place within the brain and none of the learning takes place through a ""Decoder"" - that is, a computer-based machine-learning algorithm that attempts to read the mind of the user and extract the desired signals to control the movement. Thus, our Brain-Body interface is fully autonomous and requires no outside intervention. Initially, performance may be inadequate. Just as an infant requires considerable time to establish control over motor pathways, so too will a subject require time to learn to control this entirely new pathway. However, given adequate learning time, this architecture is likely to lead to superior performance, because all of the control resides within the brain and because the brain has a remarkable adapt and learn.  <br/><br/>There are two key intellectual merits to the proposed research.  The first is to use this novel preparation to the study natural systems-level mechanisms of learning present in the brain, so that how our brains self-organize during development and how our brains adapt to injury or disease are better understood. The second is to transform the fields of Brain-Machine and Brain-Body Interfacing by exploring the level of control attainable when the native brain does all of the learning rather than computer algorithms. The research has additional broader significance.  In particular, through this work it is hoped that a better-performing Brain-Body interface can be developed to help restore movement function in those suffering from neurological disorders or brain injury. Moreover, this work further cultivates the methodology of Direct Brain Control (a particular form of biofeedback) for the design of rehabilitative devices. In so doing, it enhances the existing research infrastructure by bringing together clinical anesthesiologists, neurophysiologists, and engineers to forge a cross-disciplinary solution to this important problem."
"1150000","CAREER: Efficient Structural Analysis of Multivariate Fields for Scalable Visualization","OAC","CAREER: FACULTY EARLY CAR DEV, GRAPHICS & VISUALIZATION","07/01/2012","02/29/2012","Xavier Tricoche","IN","Purdue University","Standard Grant","Sushil Prasad","09/30/2017","$513,789.00","","xmt@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","1045, 7453","1045","$0.00","Visualization plays a crucial role in research and industry by offering users a graphical interface to their data that affords them an intuitive basis for interpretation, assessment and decision making. Yet, the rapidly growing size, dimensionality, and multi-scale complexity of the produced scientific data create a pervasive analysis challenge that is not properly addressed by existing visualization technology. In particular, the investigation of inherently multivariate and multifield physical problems is typically reduced to the visualization of a single scalar or vector field, thereby neglecting a wealth of important information.<br/><br/>The PI will address the limitations of the current state of the art by pioneering a comprehensive approach for the efficient visual analysis of large-scale datasets. Central to the proposed approach is a novel definition of geometric saliency that will permit the automatic identification of remarkable manifolds in scientific data. To that end, the PI will unify and subsume a variety of concepts from mathematics and computer vision to create a principled and versatile model for the structural analysis and visual representation of multifield problems. Innovative data structures and sparse sampling strategies leveraging parallel architectures will be devised to enable the efficient processing of large and multivariate datasets at scale. Lastly, these computational foundations will power a user-centric visual analysis framework that the PI will assess in the context of multidisciplinary collaborations spanning fluid dynamics, materials engineering, high-energy physics, and cardiovascular research.<br/><br/>This research effort will benefit the scientific community by contributing a rigorous and scalable framework for the effective analysis and visualization of computational or measured datasets across a broad range of scientific problems. The PI will distribute the created software artifacts through an open source web portal and integrate them in leading visualization tools to facilitate their dissemination. The PI will organize tutorials and workshops at premier conferences to raise the awareness of the user community about the developed technology and he will provide benchmark datasets and sample results  to promote a collaborative effort in the visualization community. Beyond research, the PI will create new courses at both the undergraduate and graduate levels to expose students to the critical importance of data analysis in science and engineering and to the role of advanced visualization in this context. Finally, these education activities will naturally complement an outreach effort toward underrepresented minorities and local K-12 programs."
"1227504","NRI-Large: Collaborative Research: Purposeful Prediction: Co-robot Interaction via Understanding Intent and Goals","IIS","National Robotics Initiative","10/01/2012","08/05/2014","Joshua Tenenbaum","MA","Massachusetts Institute of Technology","Continuing grant","Ephraim Glinert","09/30/2017","$520,000.00","","jbt@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","8013","7925, 8086","$0.00","In order for robots to collaborate with humans, they need to be able to accurately forecast human intent and action.  People act with purpose: that is, they make sequences of decisions to achieve long-term objectives. For instance, in driving from home to a store, people carefully plan a sequence of roads that will get them there efficiently. In predicting a person's next decision, algorithms must be developed that reflect these purposeful actions. <br/><br/>Currently, robots are unable to anticipate human needs and goals, and this represents a fundamental barrier to their large-scale deployment in the home and workplace. The aim of this project is to develop a new science of purposeful prediction that can be applied to human-robot interaction across a wide variety of domains.  The work draws on recent techniques based on Inverse Optimal Control and Inverse Equilibria Theory that enable statistically sound reasoning about observed deliberate behavior. These new methods provide the foundations of a theoretical framework that integrates traditional decision making techniques like optimal control, search and planning with probabilistic methods that reason about uncertainty and hidden information, particularly about goals, utility and intent. <br/><br/>Intellectual merit:  The project will provide a general framework that allows robots to anticipate and adapt to the activities of their human co-workers based on perceptual cues. The investigators will develop the theory, a computational toolbox, and, in collaboration with industrial partners, prototype deployments of these new methods for the prediction of peoples' behavior in a diverse set of robotics domains from computer vision to motor control. The project is transformative in that it combines a novel theoretical/algorithmic framework with extensive support in terms of volume of data and validation infrastructure in the context of many applications.        <br/><br/>Broader impacts: A revolution in personal robotics in both the home and workplace depends on the ability to forecast human activities and intents; small- and medium- scale manufacturing will make a leap forward through agile robotic systems intelligent enough to understand and assist their co-workers in flexible assembly tasks; and robust models of pedestrian and vehicular traffic flow will enable more effective driver warning systems and safer autonomous mobile robots. Purposeful prediction technology is an important step towards enabling such understanding of actions and intents in these arenas. The research work will involve the training and mentoring of undergraduate, masters and doctoral students as well as post-doctoral fellows in this emerging multi-disciplinary research area at the intersection of computer and cognitive sciences and robotics."
"1246332","CC-NIE Integration: Advancing Science Through Next Generation SDN Networks","OAC","INFORMATION TECHNOLOGY RESEARC, Campus Cyberinfrastrc (CC-NIE)","10/01/2012","09/12/2012","Vincent Kellen","KY","University of Kentucky Research Foundation","Standard Grant","Kevin Thompson","09/30/2015","$997,437.00","James Griffioen","vkellen@ucsd.edu","109 Kinkead Hall","Lexington","KY","405260001","8592579420","CSE","1640, 8080","1640, 9150","$0.00","This University of Kentucky CC-NIE Integration project is focused on the ever-growing demands for improved cyberinfrastructure to support<br/>data-intensive scientific research.  This project, a partnership led by UK Information Technology using technology and services from UK<br/>Computer Science, the Laboratory for Advanced Networking, and the UK Center for Computational Sciences, provides software-defined network (SDN) infrastructure and control to UK researchers and affiliates.  The project not only upgrades network components, it also provides a programmable network infrastructure tailored to the needs of researchers. Separation of UK research traffic from administrative and academic traffic enables research traffic to avoid the institutional policy constraints currently placed on all traffic.<br/><br/>The resulting SDN network will have a lasting impact on research projects spanning a wide range of areas including astrophysics, bio-medical, computer vision, visualization, and networking research. The most obvious improvement will be enhanced capacity for data-intensive research applications, with transmission speeds of up to 10 Gbps at the research access layer, 10 Gbps from the distribution layer to the UK research core, and 10+ Gbps from the research core to Internet2 and the regional networks (KyRON and KPEN). In addition, system administrators will be able to apply fine-grained control and prioritization of traffic across the campus backbone. It will also enable end-to-end user-defined provisioning of network access and capacity so that each research project can obtain precisely the performance it requires of the network.  Finally, being integrated with the GENI network will enable researchers to access additional resources all across the GENI network."
"1227234","NRI-Large: Collaborative Research: Purposeful Prediction: Co-robot Interaction via Understanding Intent and Goals","IIS","National Robotics Initiative","10/01/2012","07/08/2014","Dieter Fox","WA","University of Washington","Continuing grant","Ephraim P. Glinert","09/30/2017","$533,332.00","","fox@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","8013","7925, 8086","$0.00","In order for robots to collaborate with humans, they need to be able to accurately forecast human intent and action.  People act with purpose: that is, they make sequences of decisions to achieve long-term objectives. For instance, in driving from home to a store, people carefully plan a sequence of roads that will get them there efficiently. In predicting a person's next decision, algorithms must be developed that reflect these purposeful actions. <br/><br/>Currently, robots are unable to anticipate human needs and goals, and this represents a fundamental barrier to their large-scale deployment in the home and workplace. The aim of this project is to develop a new science of purposeful prediction that can be applied to human-robot interaction across a wide variety of domains.  The work draws on recent techniques based on Inverse Optimal Control and Inverse Equilibria Theory that enable statistically sound reasoning about observed deliberate behavior. These new methods provide the foundations of a theoretical framework that integrates traditional decision making techniques like optimal control, search and planning with probabilistic methods that reason about uncertainty and hidden information, particularly about goals, utility and intent. <br/><br/>Intellectual merit:  The project will provide a general framework that allows robots to anticipate and adapt to the activities of their human co-workers based on perceptual cues. The investigators will develop the theory, a computational toolbox, and, in collaboration with industrial partners, prototype deployments of these new methods for the prediction of peoples' behavior in a diverse set of robotics domains from computer vision to motor control. The project is transformative in that it combines a novel theoretical/algorithmic framework with extensive support in terms of volume of data and validation infrastructure in the context of many applications.        <br/><br/>Broader impacts: A revolution in personal robotics in both the home and workplace depends on the ability to forecast human activities and intents; small- and medium- scale manufacturing will make a leap forward through agile robotic systems intelligent enough to understand and assist their co-workers in flexible assembly tasks; and robust models of pedestrian and vehicular traffic flow will enable more effective driver warning systems and safer autonomous mobile robots. Purposeful prediction technology is an important step towards enabling such understanding of actions and intents in these arenas. The research work will involve the training and mentoring of undergraduate, masters and doctoral students as well as post-doctoral fellows in this emerging multi-disciplinary research area at the intersection of computer and cognitive sciences and robotics."
"1207651","Extending the use of machine learning algorithms to Sufficient Dimension Reduction","DMS","STATISTICS","09/15/2012","11/14/2013","Andreas Artemiou","MI","Michigan Technological University","Continuing grant","Gabor Szekely","08/31/2013","$19,385.00","","aartemio@mtu.edu","1400 Townsend Drive","Houghton","MI","499311295","9064871885","MPS","1269","","$0.00","The investigator develops new methodologies for sufficient dimension reduction both for linear and nonlinear dimension reduction problems.  A method developed recently by the investigator and his collaborators utilizes classic two-class Support Vector Machine algorithms to develop a new class of algorithms for linear and nonlinear sufficient dimension reduction under a unified framework.  In this work the investigator extends this methodology in several directions. First, different extensions of the classic Support Vector Machine algorithms are used to improve the performance and the asymptotic properties of the original method. Second, the method is extended to algorithms that allow for multi-class classification.  Third, the investigator develops new method-specific and method-free variable selection methodologies for sufficient dimension techniques based on ideas in the machine learning literature.  Finally, new algorithms for the order determination of the dimension reduction space based on the new methodology are developed.<br/><br/>Recent advancements in computer science have increased computer power and, subsequently, the capability of storing large datasets efficiently.  Thus, to analyze large datasets effectively in many sciences, like Biology, Meteorology, Genetics and Economics, new techniques are needed to reduce the dimensionality of the datasets.  This work creates new algorithms to reduce the dimensionality of large datasets effectively, for both linear or nonlinear relationships between variables.  These techniques transform a high-dimensional regression or classification problem to a lower-dimensional one, which helps to identify hidden relationships among variables.  The methodology being developed will be an efficient tool for scientists working with large datasets, and it will open new research frontiers to statisticians to develop new ideas in the area of dimension reduction."
"1151805","Body Movement and Action Perception","BCS","Cross-Directorate  Activities, Perception, Action & Cognition, Other Global Learning & Trng","07/01/2012","08/29/2019","Ayse Saygin","CA","University of California-San Diego","Continuing Grant","Betty Tuller","06/30/2020","$587,350.00","","apsaygin@gmail.com","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","SBE","1397, 7252, 7731","1045, 5946, 7252, CL10","$0.00","Understanding the movements of others is critical for a wide range of functions such as detecting prey and predators, inferring the goals and intentions of others, and engendering emotional responses, such as feeling compassion and empathy toward others.  The investigators will use a range of complementary methods to understand how the human brain detects and interprets the body language of others.  For example, body movements can be characterized by very minimalist visual representations, consisting of a dozen points of light attached to the joints on the body seen in darkness (a method used in many motion capture systems).  In addition, the investigators will use high-resolution videos to link the work to the field of social cognition by determining how processing is affected when the movements are produced by an artificial agent like a robot, rather than by a human.  <br/><br/>Understanding the perceptual and neural basis for body movement processing is essential to an account of how humans negotiate objects and events in the world and can inform fields as wide-ranging as cognitive science, neuroscience, robotics, brain-computer interfaces, social cognition, technology design, visual arts, and computer vision.  In addition, social artificial agents such as humanoid robots and virtual animated characters are becoming increasingly common in a range of domains such as education, defense, healthcare and entertainment.  This research can help guide the design of these interactive technologies.  The investigators also aim to further science and education through recruitment and retention of young people, especially minorities underrepresented in science.  Since early engagement is critical for this goal, the focus will be on the development of hands-on research and computation skills for high school and undergraduate students."
"1153572","SBIR Phase I: Mobility Monitor: An autonomous intelligent system developed to quantitatively determine mobility.","IIP","SMALL BUSINESS PHASE I","01/01/2012","12/06/2011","Ralph Reinhold","IL","Kaliber Imaging, Incorporated","Standard Grant","Juan E. Figueroa","09/30/2012","$149,882.00","","ralph@kaliberimaging.com","744 Littleton Trail","Elgin","IL","601207009","8474939354","ENG","5371","5371","$0.00","This Small Business Innovation Research (SBIR) Phase I project is developing an integrated computer vision system to objectively measures a person's gait, range of motion and balance and other mobility static and dynamic factors, targeted at fall risk in the elderly. This compact system is cost-effective and easy to use. It tracks people's mobility, identifies problems to correct losses, and provides feedback to motivate the patient to follow their prescribed treatment. The initial focus is the rapidly growing older adult population, who are living longer through advances in medicine, and yet, there are gaps in modern healthcare technologies that prevent elderly people from living independent lives. The project will result in an autonomous intelligent system developed to assess the elderly and others with potential limitations in mobility, to provide comparisons with norms, and to archive test outcomes to allow the subject to see their progress or regress and allow for clinical intervention. The product uses state-of-the-art developments in hardware and software, including existing motion analysis, aerospace technology, mobile telephones and the computer game industry, resulting in a system equipped to follow the motion of a person at a constant scale and quantitatively determine that motion. <br/><br/>The broader impact/commercial potential of this project is in the analysis, rehabilitation and monitoring of mobility issues. The project succinctly responds to priority areas of robotics technology development in the following ways: (1) leveraging improvements in core technologies and algorithms to innovatively yet cost efficiently develop a highly intelligent system capable of making objective, quantitative, ""real time"" measurements of mobility to replace current subjective testing or time-consuming clinical gait analysis; and (2) using this technology to support and enhance independent living and improve health service delivery to elders and the disabled allowing for more effective treatment protocol. The above broad aims are proven feasible through focused Phase I objectives: (1) connecting the tracking data to articulated human skeletal movement and (2) evaluating key, high-risk components and algorithms using a test article and computer simulation. Phase I will clear barriers to development of an advanced prototype in Phase II--resulting in additional refinements, testing in clinical trials and partnering with a manufacturer in transition to commercialization. The growth plan includes home care applications with the capacity to telemonitor and report to practitioners."
"1218228","SHF: Small: Automatic Software Architecture Recovery: A Machine Learning Approach","CCF","Software & Hardware Foundation","09/01/2012","08/29/2012","Cristina Lopes","CA","University of California-Irvine","Standard Grant","Sol Greenspan","08/31/2016","$500,000.00","","lopes@ics.uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","CSE","7798","7923, 7944","$0.00","The widespread practice of open source development is changing the IT industry in significant ways. Open source, these days, is a strategy that companies consider as part of their product's marketability. In Science and Engineering, open source has an established track record, and having the source code available to everyone these days is as important as having the data supporting scientific claims available, since Science and Engineering rely more and more on software for substantiating claims. Unfortunately, undocumented source code is as difficult to understand as raw, undocumented data; having it available without being able to understand it is not of much benefit. Open source projects, in particular, are notorious for their lack of documentation, since the developers often don't have the resources to produce artifacts beyond the code, so ""the code is the documentation."" This is a pervasive problem that impacts Science the most, as it increasingly relies on software that is produced under slim budgets without margin for documentation efforts.<br/><br/>This project seeks to automatically recover high-level knowledge from software artifacts in order to make software components understandable in the absence of documentation. Recovering high-level knowledge from software artifacts has been a long-sought goal of software engineering research. The achievements so far have been limited. The approach taken here is to use machine learning techniques. This approach may finally start to produce usable solutions to this elusive problem. In pursuing the goal, this project unveils important knowledge and tools related to open source projects. First, it unveils knowledge about which and what kind of relations among source code artifacts correlate with the architecture recovery process. Second, it will produce a catalog of unsupervised learning algorithms tailored for software component identification. This will be publicly available for others to use and study. Third, it will produce a benchmark of software architectures of projects from various domains. Fourth, it will produce a catalog describing the artifacts and the learning technique which best recovered their architecture. Finally, it will produce reusable implementations of (i) several component identification algorithms; and (ii) structural, behavioral, and domain feature extraction. This project combines all this knowledge and tools in a plugin for Eclipse that supports automatic recovery of software architecture."
"1141033","Earlier and broader access to machine learning","DUE","S-STEM-Schlr Sci Tech Eng&Math, TUES-Type 1 Project","08/15/2012","08/08/2012","Lillian Cassel","PA","Villanova University","Standard Grant","Victor Piotrowski","07/31/2017","$199,874.00","Thomas Way, Paula Matuszek","cassel@acm.org","800 Lancaster Avenue","Villanova","PA","190851676","6105194220","EHR","1536, 7513","9178, SMET","$0.00","The project goal is to develop a collection of modules to be used in various combinations, to introduce undergraduate students to the concepts and techniques of Machine Learning.<br/><br/>The project brings the concepts and techniques of Machine Learning to undergraduates, and explores the feasibility of making modular presentations of this material in formats for use in various combinations to meet the needs in a variety of subject areas. The project motivates the study by working with real scenarios from the subject domains and by using real data sets provided by an advisory board.<br/><br/>This project increases the pool of graduates available for research and development in areas where large quantities of data must be analyzed. The PIs attract students from underrepresented groups, both as testers of the materials and as students in the initial course deployments. The resulting modules are freely available for adaptation and reuse."
"1240252","EAGER: Density functionals from Machine Learning","CHE","CONDENSED MATTER & MAT THEORY, Theory, Models, Comput. Method, CDS&E-MSS","05/01/2012","05/31/2013","Kieron Burke","CA","University of California-Irvine","Continuing grant","Evelyn Goldfield","04/30/2015","$300,000.00","","kieron@uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","MPS","1765, 6881, 8069","7433, 7569, 7721, 7916, 7969, 9216, 9263, HPCC","$0.00","Kieron Burke of the University of California, Irvine, is supported by an Eager award from the Chemical Theory, Models and Computational Methods program in the Chemistry Division.  The award is cofunded by the Condensed Matter and Materials Theory program in the Division of Materials Research and the Computational and Data Enabled Science Program in the Division of Mathematical Science.  Density functional theory (DFT) is at the heart of modern electronic structure calculations, which play an ever increasing role in chemical and material design. Present-day approximations are created by an unholy alliance of inspiration and pragmatism. Progress in their improvement is slow and unsystematic.  Burke and his colleagues are applying Machine Learning (ML) to the   problem of approximating density functionals.  ML provides a totally new way to approximate functionals that takes maximum advantage of DFT formalism.  They have demonstrated that chemical accuracy on self-consistent densities can be reached for a simple model case, and a measure of the reliability of the approximation can be given. The goal of the EAGER grant is to develop these methods in application to DFT, overcoming any challenges, and reaching the real world of electronic structure calculations as quickly as possible, by approaching the problem in a well-defined series of small steps.<br/><br/><br/>This project creates an entirely new subfield of theory/computation, in which machine learning (ML), a branch of computer science, is applied to electronic structure problems, a branch of theoretical physics and chemistry that allows prediction of new molecules and materials by solving the equations of quantum mechanics. New algorithms, at the cutting edge of ML research, are being developed in order to apply ML to find density functionals, needed for solving electronic structure. This is a true synergy of physical and computer sciences. Success of this proposal would revolutionize materials design, allowing millions of atoms to be treated instead of hundreds by present methods. This would truly transform predictive capability in a broad range of scientific and technological problems, from biomolecular liquid simulations to crack propagation in materials."
"1157714","Conference on ""Symmetries of Differential Equations: Frames, Invariants and Applications""","DMS","APPLIED MATHEMATICS","05/01/2012","02/08/2012","Willard Miller","MN","University of Minnesota-Twin Cities","Standard Grant","Annalisa Calini","04/30/2013","$44,700.00","","miller@math.umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","MPS","1266","7556","$0.00","This grant provides travel support for graduate students, postdocs, and invited lecturers to participate in the Conference on ""Symmetries of Differential Equations: Frames, Invariants and Applications"" to be held at the School of Mathematics, University of Minnesota, May 17-19, 2012. The general conference theme is inspired by the lifetime research achievements of Professor Peter Olver, but focus will be on current advances, promising directions, and novel applications. The main topics addressed are symmetries of differential equations and variational problems, with special emphasis on moving frames, Cartan theory of differential forms and invariant theory. Also Hamiltonian systems and integrable systems including solitons will be treated, as well as applications of symmetry based methods to image processing, fluid mechanics and elasticity. The organizing committee is: Niky Kamran (Mathematics, McGill University), Gloria Mari Beffa (Mathematics, University of Wisconsin), Guillermo R. Sapiro (Electrical and Computer Engineering, University of Minnesota), and Willard Miller, Jr, (Mathematics, University of Minnesota). Detailed information is available on the website http://math.umn.edu/conferences/olver/ and results of the meeting, including abstracts, slides and a panel discussion, will also be posted there. In conjunction with the conference the on line journal SIGMA will publish a special issue entitled ``Symmetries of Differential Equations: Frames, Invariants and Applications'' with the conference organizers as guest editors, http://www.emis.de/journals/SIGMA/SDE2012.html.<br/><br/>The general theme of the conference is the exploitation of symmetries of mathematical and physical systems to deduce properties of these systems. More specifically, most physical theories are expressed in terms of differential equations and the mathematical symmetries of these equations can be exploited to reach important conclusions about the systems that obey the equations. The meeting comes at a propitious time - an explosion of new results and new methods in symmetry analysis, with applications ranging over geometry, differential equations, computer vision, numerical analysis, mechanics, and physics, make this an ideal time to assemble the leading experts, and promising younger researchers to assess the rapid advances in the area, reach a consensus on what are the most important problems, and evaluate the most promising directions for further advance. We expect the conference to be a major event in the history of this research area."
"1146747","ABI Innovation:  Interactive Learning Tools For Individual Identification in Large Biological Image Databases","DBI","ADVANCES IN BIO INFORMATICS","06/15/2012","06/11/2012","Srinivas Ravela","MA","Massachusetts Institute of Technology","Standard Grant","Jennifer Weller","05/31/2016","$432,477.00","","ravela@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","BIO","1165","9178, 9179","$0.00","The goal of this research is to develop scalable methods for individual identification in large biological databases with application to multiple species in ongoing conservation efforts around the world. This work couples crowd-sourcing with computational learning. In an identification system that learns from user inputs, multiple inputs can accelerate the quality and rate of identification, which in turn dramatically reduces subsequent work-cycles any human performs. The hypothesis is that Computational learning will improve the quality of human-machine coupled system's solutions to help solve large and diverse identification problems. Several interactive learning algorithms are proposed in all stages of indexing and search in Biological Image Databases. The algorithms are based on non-parametric Bayesian inference and deliver incremental online learning methods. To test the interactive learning hypothesis, the MIT Sloop system will be extended to include the proposed interactive learning tools. Sloop is a robust toolkit for vision tools that has been tested on multiple species and supports an operational deployment. As part of this research a distributed Sloop system indexing multiple species will be developed.<br/> <br/>It is difficult to accurately estimate the effectiveness of conservation efforts for many rare and endangered species without an ability to quantify the spatial scales and other statistics of animal migration and movement. Tagging, an established method, is of limited effectiveness because it is often invasive and cannot be conducted in large numbers. This research looks at whether a large number of animals in multiple species can be identified individually using photographs stored in a database. The ""Animal biometrics"" proposed here will examine how citizen scientists can crowd-source relevance judgments and other inputs, how judgments can improve the computer's identification performance, and how improved performance reduces the citizen or expert scientist's workload. The mechanics of the symbiotic human-computer interaction mediated by machine learning, and the methods by which patterns are indexed and searched also have impacts in other fields. For example, some of the tools developed here have been applied in Geosciences and Weather Prediction."
"1217301","EXP: Deepening Conceptual Understanding with Hands-on, Augmented-Reality Experimentation","IIS","Cyberlearn & Future Learn Tech","09/01/2012","08/31/2012","David Johnson","UT","University of Utah","Standard Grant","Amy Baylor","08/31/2017","$524,051.00","Kirsten Butcher","dejohnso@cs.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","8020","8045, 8841, 9150","$0.00","This project team, led by a learning scientist and a computer graphics and augmented reality expert are working together to design a new kind of science laboratory environment that uses augmented reality (AR) to make invisible scientific phenomena visible and continuously reinforce the conceptual principles at work during hands-on scientific experimentation. The envisioned technology visually captures the what students see as they are physically carrying out experiments, interactively processes the imagery to make key aspects available for analysis and exploration, and, in real time, automatically augments the imagery with visualizations that overlay visualizations of scientific phenomena on the real-world phenomena students are experiencing, allowing students to experience real-world phenomena and the conceptual models that describe those phenomena simultaneously. The base technology is a camera-equipped tablet. Initial content is on force, work, and motion. The technological innovation includes, first, recognizing the components in a scene, even when the lighting is bad and people and other objects occlude some of it, and second, calculating the positions of the objects so that the virtual imagery can be transformed to match the real-world positions. Foundations for the innovation can be found in what is known about the affordances of integrating multiple representations in reasoning and problem solving and the challenges and difficulties in doing that. The aim is to overcome those challenges so that the affordances of multiple representations, each of which can communicate different things, can be taken advantage of by learners. Scientists do this regularly, and a visceral understanding of how this is done by scientists is expected to help students better understand what scientists do and what counts as evidence. Research addresses how students' thinking processes and domain understanding are influenced by the addition of augmented reality visualizations to their hands-on scientific investigations. <br/><br/>There is broad agreement that the current state of educational preparedness of students in STEM areas is inadequate and that improvement in these areas is critical both for American technological leadership and for an informed, policy-making citizenry. This research works to improve science education with engaging tools that tackle one of the most difficult science educational issues: deep conceptual understanding applied to real-world situations. This project team applies an elegant technological approach to an helping learners connect the abstract science concepts they are learning about to the real world they live in. Computer vision and augmented reality will be integrated and refined in a way that addresses the needs of learners. The team will make the programs and curriculum freely and broadly available via Web-based distribution and access."
"1139158","Making Sense at Scale with Algorithms, Machines, and People","IIS","Information Technology Researc, Expeditions in Computing","04/01/2012","06/27/2018","Michael Franklin","CA","University of California-Berkeley","Continuing Grant","Sylvia Spengler","03/31/2020","$10,000,000.00","Scott Shenker, Alexandre Bayen, Michael Jordan, Michael Franklin, Ion Stoica","franklin@cs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","1640, 7723","7723","$0.00","Making Sense at Scale with Algorithms, Machines, and People<br/>University of California, Berkeley<br/><br/>The world is increasingly awash in data.   As more and more human activities move on line, and as a growing array of connected devices become integral part of daily life, the amount and diversity of data being generated continues to explode.   According to one estimate, more than a Zettabyte (one billion terabytes) of new information was created in 2010 alone, with the rate of new information increasing by roughly 60% annually.  This data takes many forms: free-form tweets, text messages, blogs and documents; structured streams produced by computers, sensors and scientific instruments; and media such as images and video.<br/>Buried in this flood of data are the keys to solving huge societal problems, for improving productivity and efficiency, for creating new economic opportunities, and for unlocking new discoveries in medicine, science and the humanities.   However, raw data alone is not sufficient; we can only make sense of our world by turning this data into knowledge and insight.  This challenge, known as the Big Data problem, cannot be solved by the straightforward application of current data analytics technology due to the sheer volume and diversity of information.  Rather, to solve it requires throwing away old preconceptions about data management and breaking down many of the traditional boundaries in and around Computer Science and related disciplines. <br/><br/>The Algorithms, Machines, and People (AMP) expedition at the University of California, Berkeley is addressing this challenge head-on.    AMP is a collaboration of researchers with a wide range of data-related expertise, committed to working together to create a new data analytics paradigm.  AMP will produce fundamental innovations in and a deep integration of three very different types of computational resources:<br/> 1. Algorithms: new machine-learning and analysis methods that can operate at large scale and can give flexible tradeoffs between timeliness, accuracy, and cost.<br/> 2. Machines: systems infrastructure that allows programmers to easily harness the power of scalable cloud and cluster computing for making sense of data.<br/> 3. People: crowdsourcing human activity and intelligence to create hybrid human/computer solutions to problems not solvable by today's automated data analysis technologies alone.<br/><br/>AMP research will be guided and evaluated through close collaboration with domain experts in key societal applications including: cancer genomics and personalized medicine, large-scale sensing for traffic prediction and environmental monitoring, urban planning, and network security.    Advances pioneered by the project will be made widely available through the development of the Berkeley Data Analysis System (BDAS), an open source software platform that seamlessly blends Algorithm, Machine and People resources to solve big data problems.<br/><br/>For more information visit http://amplab.cs.berkeley.edu"
"1249756","EAGER: Learning Upsampling Operators for Animation of Cloth and Fluids","IIS","GRAPHICS & VISUALIZATION","08/15/2012","08/08/2012","Adam Bargteil","UT","University of Utah","Standard Grant","Lawrence Rosenblum","07/31/2013","$99,999.00","","adamb@umbc.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","7453","7453, 7916, 9150","$0.00","The PI's goal in this exploratory research is to tackle the fundamental obstacle preventing high quality, interactive animation of natural phenomena, namely the enormous number of degrees of freedom involved.  Because hand animating a typical mesh is a tedious and time consuming task, computer graphics has turned to physics and simulation to animate most natural phenomena.  In this context, the promise of simulation is generality; an infinite space of material properties and initial conditions can be explored.  This generality is also simulation's greatest limitation; the space of possible animations is vast, while the space of desirable animations is a great deal smaller.  The PI's approach is to use simulation's strength (its ability to create rich animation data under a variety of conditions) to combat its greatest limitations (high dimensionality and computational expense).<br/>  <br/>To this end, he will develop machine learning tools for finding new and more expressive low-dimensional representations, which do not describe all possible animations but rather succinctly describe the space of desirable animations.  Previous attempts to apply machine learning to the animation of natural phenomena have shown promise, but also significant limitations.  These approaches have suffered from over-fitting, have sacrificed locality, and have not allowed artistic control over the space of possible animations.  Furthermore, these approaches have been too data-driven, failing to allow for the input of valuable human knowledge and intuition or mathematical and physical models.  Until these limitations are addressed, the promise of high-quality interactive computer animation of natural phenomena will remain out of reach.  For concreteness the PI will focus on cloth and fluids as test bed domains (initially assuming an algorithmic paradigm of coarse simulation enhanced by data-driven upsampling operators), for which he will explore questions of sparseness, expanded feature sets, combining operators, and artistic control.  <br/><br/>Broader Impacts:  Simulation is a powerful technique whose usefulness is not limited to computer animation.  So while the test bed domains fall within the realm of traditional computer graphics, project outcomes will allow for high-quality, interactive computer animation of natural phenomena across all of science and engineering, with particular applicability to film, video games, virtual reality, medical training, etc.  Moreover, the unique context of computer animation will necessarily require new machine learning algorithms that will feed back into that community as well.  The PI plans to develop and release the majority of his source code under free BSD licenses."
"1237941","Travel Support for Students to Attend a Symposium on the Influences of Behavior-Based Robotics on the Field","IIS","HCC-Human-Centered Computing","04/15/2012","04/05/2012","Holly Yanco","MA","University of Massachusetts Lowell","Standard Grant","Ephraim Glinert","03/31/2013","$11,250.00","","holly@cs.uml.edu","Office of Research Admin.","Lowell","MA","018543692","9789344170","CSE","7367","7367, 7556","$0.00","This is funding for approximately 50 students to attend a full day symposium on the impact of behavior-based robotics on over 25 years of robotics research and development, which will be held on Friday, March 9, 2012, on the MIT campus, immediately following the ACM/IEEE International Conference on Human-Robot Interaction which takes place this year in Boston.  The symposium's primary focus in on possible future research trajectories; to this end, the symposium will examine the impact of the approach on both academic research and commercial development.  For example, the relationship between the subsumption-based robots of the late 1980s and the Roombas being sold today is clear: the Roomba very much resembles the insect-like robots Genghis and Attila in its programming and behaviors.  Are there such connections to all robot systems currently under development?  The symposium will examine several questions, including the successes of behavior based robots, the adaptations made to this paradigm in order to develop robots for more complicated tasks, and the likely trends in robotics research and development over the next decade.<br/><br/>The symposium is structured into four main sections.  The first section will address the early years of behavior-based robotics and the subsumption architecture, exploring the initial controversy and early work in this domain.  The second section will look at the global impact of the work, discussing related research in labs around the world and how behavior-based robotics have been used and expanded in many different robotics domains.  The third will look at the connections between behavior-based robotics and other research areas such as multi-agent learning and computer vision.  The final section will explore the legacies of the work, both in terms of research and commercial robot development, and the future prospects for behavior-based robotics.<br/><br/>Broader Impacts:  Student attendees at the symposium will have an opportunity to meet and network with a community of top robotics researchers, opening up channels for collaboration and mentoring.  Faculty advisors will be able to introduce their students in-person to the field's luminaries (as most students will have their faculty advisor present); these introductions, combined with the relatively small size of the gathering, promise to provide a more personal, more memorable, and, thus, more influential experience for the students.  This event will provide students with many opportunities for making connections with faculty, with people working in robotics companies, and with other students.  These networking opportunities may lead to future collaborations and research."
"1219261","HCC: Small: Telecollaboration in Physical Spaces","IIS","HCC-Human-Centered Computing","09/01/2012","08/30/2012","Matthew Turk","CA","University of California-Santa Barbara","Standard Grant","William Bainbridge","08/31/2016","$499,970.00","Tobias Hollerer","mturk@cs.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","CSE","7367","7367, 7923","$0.00","The goal of this project is to develop and evaluate novel methods for telecollaboration - remote collaboration that effectively integrates video and voice communication, computer vision based tracking, and augmented reality display in order to enable participants to more fully leverage the local physical environment in their remote interactions. In this telecollaboration paradigm, remote and local users will interact with the physical environment using models derived from live imagery from a camera that the local user holds or wears, rather than merely passively viewing the video stream. A key aspect of the approach is that it does not require preparation of the environment or model information, so it can be viewed as an ""anywhere, anytime"" mobile communication technology.<br/>    <br/>Intellectual merit: The proposed research builds on promising preliminary work on telecollaboration showing the effectiveness of world-stabilized ""telepointers"" - markers controlled by the remote user that stick to the real-world referents in a dynamic environment. The project will advance the state of the art in remote collaboration by providing new capabilities to integrate real and virtual, verbal and spatial, local and remote. The proposed developments are needed in order to make the physical space a more fundamental part of telecollaboration, and significant user studies will be conducted to acquire a better understanding of the opportunities and limitations of physically-grounded remote interaction. <br/>    <br/>Broader impacts: Better, more compelling tools for telecollaboration can have a tremendous impact in terms of productivity and environmental consequences, as improved remote interaction reduces the need for physical collocation and thus travel. The educational impacts of the project include the training and mentoring of graduate students and a new seminar course. The research team will disseminate research results and collected data widely and use the developed technologies to provide outreach opportunities for select groups to participate in lab open houses."
"1218931","RI: Small: Non-parametric Approximate Dynamic Programming for Continuous Domains","IIS","Robust Intelligence","08/01/2012","07/20/2017","Ronald Parr","NC","Duke University","Standard Grant","Weng-keen Wong","07/31/2018","$458,000.00","","parr@cs.duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","7495","7495, 7923, 9251","$0.00","This project concerns a machine learning technique known as reinforcement learning, which is related to, but distinct from, the notion of reinforcement learning used in psychology.  The common element is that both views study changes in behavior that result from experience.  In the machine learning case, the behaviors are often decision making in dynamic environments, such as controlling a robot, a factory, inventory levels for a warehouse or even drug dosage levels.   Current theoretical development in this area guarantees that optimal decisions can be made by reinforcement learning algorithms, but only under restrictive assumptions that are difficult to ensure in practice.  Efforts to apply reinforcement learning to significant practical problems have enjoyed some success, but such efforts often forgo theoretical guarantees and rely upon tedious parameter adjustments by experts (human trial and error) to achieve success.<br/><br/>This research seeks to reduce the amount of human trial and error needed to make reinforcement learning successful, thereby making it a more accessible tool to a wider range of people.  Specifically, it will focus on algorithms for domains described by continuous variables, seeking to provide stronger theoretical guarantees for such domains as well as an approach that balances the anticipated benefit of trying new things with the benefit of sticking to what is already known about a problem (exploration vs. exploitation).  A practical benefit of success in this area would be improved techniques that make it easier for people to deploy algorithms that learn and improve performance in a variety of practical tasks like those mentioned above: robot or factory control, inventory management, or drug delivery.<br/><br/>This project plans to use a model helicopter as a challenge domain, but it is not about helicopter control per se.   Rather, it seeks to develop general techniques that can apply to many problems, including helicopters, and will use model helicopters as an inexpensive and fun way to motivate students.  The project aims to develop a model helicopter simulator (to reduce the cost and risk of trying everything on an actual helicopter) and plans to make this simulator available to the research community, providing a fun and challenging benchmark problem."
"1207771","Mining structured tensor data","DMS","STATISTICS","07/01/2012","04/01/2014","Xiaotong Shen","MN","University of Minnesota-Twin Cities","Continuing grant","Gabor J. Szekely","06/30/2016","$200,165.00","","xshen@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","MPS","1269","","$0.00","Structured modeling situations, with multivariate data involving tensors, are treated using constrained likelihood approaches, as an efficient means to exploit lower-dimensional structure for high-order analysis.  In gene network analysis, for example, a constrained approach helps reveal gene-gene relations in a context of multiple graphical models. Special attention is devoted to the appropriate choice of the constraints for adaptation to a variety of structures. The general theme of the proposed project is the development of statistical methods of practical utility, both in prediction and estimation. In particular, the proposed project develops methods for (a) multiple graphical models for structure extraction, and for (b) high-order analysis of tensor data. The proposed research is primarily motivated by challenging problems that arise in gene network analysis and collaborative filtering, where one central issue is how to leverage and utilize lower-dimensional structure to battle high statistical uncertainty in a discovery process.  New techniques are proposed and investigated, both computationally and statistically, which target biomedical and engineering problems. In (a) and (b), our effort will be on classification and regression, and on structure adaptation through tensor decomposition and factorization, with most effort focused towards condition specific extraction of lower-dimensional structure.<br/><br/>Modern scientific and engineering investigation, as in biomedical research and computer vision, now produces enormous data that aim to simultaneously explore relations among hundreds and thousands interacting units.  This project proposes methods for treating the new scientific environment. The project develops technology that is directly applicable to applied research, particularly in automatic machine processing and data mining, biomedical research, advertisement, and economics. Plans for technology transfer are described, in addition to an educational program that will train students in statistical learning and data mining. Educational activities include developing a course, and attracting undergraduate students to research."
"1218709","RI: Small: Structured Sparse Conditional Random Fields Models for Joint Categorization and Segmentation of Objects.","IIS","ROBUST INTELLIGENCE","09/01/2012","08/31/2012","Rene Vidal","MD","Johns Hopkins University","Standard Grant","Jie Yang","08/31/2016","$449,794.00","","rvidal@cis.jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","CSE","7495","7923","$0.00","Object categorization and segmentation are arguably among the most important and challenging problems in computer vision. While these two problems are clearly related, most of the existing literature treats these tasks separately. This project bridges this gap by developing algorithms for joint categorization and segmentation of objects, which simultaneously use category-level information (top-down) and pixel-level information (bottom-up).  This project develops a novel graph-theoretic paradigm that combines principles from conditional random fields and sparse representation theory. In this framework, each semantic region is represented in terms of an over-complete dictionary of objects, object parts, subparts and superpixels. To simultaneously estimate both the segmentation and a sparse representation for each region, the research team defines an energy function for the random field, which includes new higher order potentials obtained as the output of a classifier applied to the sparse representation of a segmented region. The research team also explores methods based on structured-sparse dictionary learning and latent support vector machines for learning the dictionaries and the classifier parameters. Furthermore, the research team investigates efficient discrete optimization techniques for minimizing the new energies resulting from the combination of structured-sparse models with different classifiers.<br/><br/>Applications of this research include image search, autonomous navigation (localization and identification of the road, street signs, pedestrians and vehicles), medical diagnostic tools (detection, localization and classification of lesions and tumors in medical images), surveillance (localization of suspicious people, weapons and vehicles) and robotics (identifying the boundaries and extent of objects to be interacted with). The project involves students of different levels."
"1217797","RI: Small: Uncertainty-driven Dynamic 3D Reconstruction","IIS","ROBUST INTELLIGENCE","08/01/2012","07/16/2012","Philippos Mordohai","NJ","Stevens Institute of Technology","Standard Grant","Jie Yang","07/31/2016","$368,003.00","","Philippos.Mordohai@stevens.edu","CASTLE POINT ON HUDSON","HOBOKEN","NJ","070305991","2012168762","CSE","7495","7923","$0.00","This project develops technologies of dynamic 3D reconstruction with applications to broader areas, such as free-viewpoint video, markerless motion capture, special effects for 3D and conventional films, and augmented reality.  While image and video-based 3D reconstruction of static scenes is well-understood and is among the most active research areas in computer vision, the current 3D reconstruction methods are not be able to reconstruct  dynamic scenes containing non-rigidly moving people, animals or objects well.  Furthermore, these methods are unable to self-assess their output. This research effort casts dense multi-view 3D reconstruction as an estimation problem with explicit uncertainty modeling distinguishing between geometric and correspondence uncertainty which are due to very different causes. Other innovations include the combined use of viewpoint-based and world-based processing with explicit and implicit representations and uncertainty-driven regularization.<br/><br/>The outcomes of this project improve 3D reconstruction quality and reduce cost for the above applications which have broader impact on different research areas. Ongoing outreach efforts focus on improving Science, Technology, Engineering and Mathematics (STEM) education in several ways: by teaching high school students during the summer, by mentoring them as interns and by training graduate students working with high school STEM teachers. The project also includes a plan of creating the first publicly available dataset of multiple-view dynamic scenes with ground truth depth."
"1208598","NRI-Small: The Intelligent Workcell - Enabling Robots and People to Work Together Safely in Manufacturing Environments","CMMI","NRI-National Robotics Initiati","10/01/2012","05/18/2017","Daniel Huber","PA","Carnegie-Mellon University","Standard Grant","Irina Dolinskaya","09/30/2017","$690,000.00","Paul Rybski, Daniel Huber, Deva Ramanan, Kris Kitani","dhuber@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","ENG","8013","7923, 8086","$0.00","The research objective of this award is to investigate methods to enable people and industrial robots to work safely within the same workspace.  Current robotic manufacturing practice requires the physical separation of people and robots, which ensures safety, but is inefficient in terms of time and resources, and limits the tasks suitable for robotic manufacturing.  This research will develop an ""Intelligent Workcell,"" which augments the traditional robotic workcell with perception systems that observe workers within the workspace.  Methods to explicitly track workers and estimate their body pose will enable dynamically adaptive safety zones surrounding the robot, thereby preventing the robot from injuring workers.  Algorithms will be developed to recognize the activities that workers are performing.  These algorithms will learn a task-independent vocabulary of fundamental action components, which will form the building blocks for a hierarchical activity recognition framework.  Finally, mechanisms for providing feedback to workers about the robot's intended actions will be studied.<br/><br/>This research is expected to provide new capabilities in robotic workcell safety and monitoring, allowing people and industrial robots to work safely and effectively in the same environment.  Such capabilities would improve the efficiency of existing robotic workcells, since the robot would not be required to stop whenever a person enters the workspace (as is current practice).  Furthermore, new manufacturing processes that involve robots and people working together on a single task would be enabled.  Students at the graduate and undergraduate level will benefit from using the prototype Intelligent Workcell in project courses, and grade-school students will participate in short courses and workshops designed to ignite interest in STEM activities related to industrial robotics and computer vision."
"1209828","NSF East Asia and Pacific Summer Institute for FY 2012 in China","OISE","EAPSI","06/01/2012","05/22/2012","Adrian Johnson","FL","Johnson Adrian","Fellowship","Anne L. Emig","05/31/2013","$5,836.00","","","","Tampa","FL","336209951","","O/D","7316","5978, 7316, 9200","$0.00","This action funds Adrian S. Johnson of University of South Florida to conduct a research project, entitled ""Learning anatomy in Chinese society via a natural spatial augmented game environment (SAGE) projected onto the self,"" during the summer of 2012 at the Institute of HCI and Media Integration at Tsinghua University in Beijing, China. The host scientist is Dr. Linmi Tao.<br/><br/>The Intellectual Merit of the research project is related to using computer vision and augmented reality to create a more natural learning modality and user experience. <br/><br/>Broader Impacts of an EAPSI fellowship include providing the Fellow a first-hand research experience outside the U.S.; an introduction to the science, science policy, and scientific infrastructure of the respective location; and an orientation to the society, culture and language. These activities meet the NSF goal to educate for international collaborations early in the career of its scientists, engineers, and educators, thus ensuring a globally aware U.S. scientific workforce."
"1224088","Pathways:   Large Alternate Reality Games for Education -- Assessing Performance and Play","DRL","AISL","09/01/2012","09/11/2012","Yu-Han Chang","CA","University of Southern California","Standard Grant","Arlene de Strulle","08/31/2015","$250,000.00","Jihie Kim, Rajiv Maheswaran","ychang@isi.edu","University Park","Los Angeles","CA","900890001","2137407762","EHR","7259","9177, SMET","$0.00","The University of Southern California (USC) will build on prior work to test a robust model for assessing player content engagement and social interactions within an augmented reality game (ARG). In partnership with No Mimes Media, USC will use machine learning algorithms to make automated player inferences to customize game play. The content focus of the game will span a range of STEM disciplines, with a special emphasis on earth science content and scientific investigation & experimentation reasoning. High school youth from underserved communities in Los Angeles will be recruited to participate in the endeavor. <br/><br/>This pathways project will use various ""rabbit hole"" techniques to attract freshmen and sophomore students from partner charter schools to the online game. The rabbit hole strategies may include cryptic posters, inquisitive signs, & SQR codes strategically placed in plain and open view of the target group. The game will be fully accessible to the target group online. During the ARG experience, youth players will encounter STEM concepts and scientific problems. Antagonistic characters will promulgate scientific misconceptions and nonscientific reasoning and challenge players to employ their scientific knowledge and skills to level-up, gain badges, and move through the game. As game play persists, machine learning algorithms will gather data on the players learning competencies and social interactions within the game. These data will be aggregated and analyzed to assess learning and interactions within the ARG environment.  Additional analyses will be conducted by the mixed methods approach the external evaluation group, CRESST, will employ for the project formative and summative evaluations.<br/><br/>Approximately 300 youth, within the target grade levels, are expected to participate in the gaming experience. However, given that access to the game and assessment tools will expand beyond the target group, the potential reach of the project could be much greater. Further, the stated aim of the project is not only to produce a scalable model for broad implementation but it also endeavors to provide puppetmasters with research and assessment tools to create more individualized experiences and improved learning outcomes for players within ARG environments."
"1217485","III: Small: Collaborative Research: RUI: Learning to Model Sequences","IIS","Info Integration & Informatics","10/01/2012","04/29/2016","Douglas Turnbull","NY","Ithaca College","Continuing grant","Maria Zemankova","09/30/2016","$234,000.00","","dturnbull@ithaca.edu","953 Danby Road","Ithaca","NY","148507000","6072741206","CSE","7364","7364, 7923, 9229, 9251","$0.00","This collaborative research project involves faculty and students from Cornell University (IIS-1217686) and Ithaca College (IIS-1217485) in an interdisciplinary project. The ability to learn predictive models of sequences is a component in several application problems, ranging from language models for machine translation to the recommendation of material to read in order to master a subject. The goal of this project is to develop new machine learning algorithms that can learn sequence models for items that are difficult to describe by attributes. In particular, the project develops models that automatically embed items in a latent feature space based on training sequences, that can integrate partial and noisy side information, and that have the ability to model long-range dependencies.<br/><br/>The resulting predictive models have a potential to be employed in science and education and can support the economic shift towards online business applications. The project focuses on the recommendation of music playlists as the main testbed. A deployed online music recommendation system not only provides the framework for testing and evaluation, this application domain helps to attract a broad spectrum of students from collaborating institutions, Cornell University and Ithaca College (an undergraduate institution), enabling the integration of undergraduates in the research. Project results, including open source software and annotated data set, are disseminated via the project Web site (http://www.cs.cornell.edu/People/tj/playlists/)."
"1217686","III: Small: Collaborative Research: Learning to Model Sequences","IIS","Info Integration & Informatics","10/01/2012","07/22/2013","Thorsten Joachims","NY","Cornell University","Continuing grant","Maria Zemankova","09/30/2016","$314,000.00","","tj@cs.cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","7364","7923","$0.00","This collaborative research project involves faculty and students from Cornell University (IIS-1217686) and Ithaca College (IIS-1217485) in an interdisciplinary project. The ability to learn predictive models of sequences is a component in several application problems, ranging from language models for machine translation to the recommendation of material to read in order to master a subject. The goal of this project is to develop new machine learning algorithms that can learn sequence models for items that are difficult to describe by attributes. In particular, the project develops models that automatically embed items in a latent feature space based on training sequences, that can integrate partial and noisy side information, and that have the ability to model long-range dependencies.<br/><br/>The resulting predictive models have a potential to be employed in science and education and can support the economic shift towards online business applications. The project focuses on the recommendation of music playlists as the main testbed. A deployed online music recommendation system not only provides the framework for testing and evaluation, this application domain helps to attract a broad spectrum of students from collaborating institutions, Cornell University and Ithaca College (an undergraduate institution), enabling the integration of undergraduates in the research. Project results, including open source software and annotated data set, are disseminated via the project Web site (http://www.cs.cornell.edu/People/tj/playlists/)."
"1161280","quantizing Schur functors","DMS","Combinatorics","07/01/2012","05/29/2012","Jonah Blasiak","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Tomek Bartoszynski","05/31/2014","$128,541.00","","jblasiak@gmail.com","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","MPS","7970","9251","$0.00","Geometric complexity theory is an approach to P versus NP and related problems in complexity theory using algebraic geometry and  representation theory.  A fundamental problem in representation theory, believed to be important for this approach, is the Kronecker problem, which asks for a positive combinatorial formula for decomposing the tensor product of two irreducible representations of the symmetric group into irreducibles.  The theory of quantum groups and crystal bases, which has been actively developed over the last several decades, is a powerful tool for connecting combinatorics and representation theory.  In the last decade, there have been several attempts, by Berenstein, Zwicknagl, Mulmuley, Sohoni, and others, to use this tool to study the Kronecker and related plethysm problems.  Recently, the investigator and his collaborators Mulmuley and Sohoni have obtained the beginnings of a theory of crystal bases for the Kronecker problem.  The main goal of this project is to push this approach further.  More broadly, this project aims to further explore the potentially deep connections between complexity theory and positivity in algebraic combinatorics as has been initiated by geometric complexity theory.<br/><br/>Objects arising in algebra are typically complicated, mysterious, and have many symmetries.  Algebraic combinatorics is the study of counting and organizing such objects.  This project will explore some surprising connections between this area and complexity theory (the study of algorithms and their limitations).  The tensor decomposition problem is an important and difficult problem that shows up in many fields, including algebraic combinatorics, complexity theory, and statistics, and has applications in medicine, computer vision, chemistry, and fast matrix multiplication.  Essentially, it is the problem of recovering individual signals from a mixture of signals.  This project offers potential new insights into this problem by applying powerful tools from algebraic combinatorics."
"1200374","Hybrid 4-Dimensional Augmented Reality Environments for Ubiquitous Markerless Context-Aware AEC/FM Applications","CMMI","CIS-Civil Infrastructure Syst","08/15/2012","07/14/2012","Mani Golparvar-Fard","VA","Virginia Polytechnic Institute and State University","Standard Grant","Dennis Wenger","10/31/2013","$299,961.00","Christopher White","mgolpar@illinois.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","ENG","1631","029E, 036E, 039E","$0.00","The objective of this project is to test whether a framework proposed by the PIs can in near real-time read, write, and receive feedback from a model which fuses pictures from mobile devices and Building Information Models for the purpose of providing ubiquitous and marker-less contextual awareness for Architecture/ Engineering/ Construction and  Facility Management (AEC/FM) applications. According to the framework, field personnel can use mobile devices to take pictures that include specific project elements (e.g., column), touch or click on the elements in the image, and be presented with (or be able to add) a detailed list of information, such as architectural/structural plan related to the physical elements. The mobile device can use onboard GPS and other sensors to perform a rough calculation of the device's field-of-view and location. Initial image processing is done on the mobile device to extract and send feature points/descriptors, field-of-view, and location to the Hybrid 4-dimensional Augmented Reality (HD4AR) server. Based on a new computer vision method, the server uses this information from the phone to derive the mobile device's position at a resolution that is an order of magnitude more accurate than with current approaches based solely on GPS. The server uses the derived high-precision camera position to determine what cyber-information is in view of the device's camera. The extracted information, along with pixel coordinates of where each cyber-information item should appear in the photo, is returned to the mobile device and visualized in augmented reality format. <br/><br/>If successful, the results of this research will provide the first feasible platform for context aware applications which does not require reliable and high accurate GPS/sensor-based location and orientation tracking and works based on existing image collections. It further assists field personnel through visualization of queried plan and actual site information in form of augmented reality, and supports interactions among project personnel and field information. By providing immediate access to information, the proposed framework automatically provides inexpensive, global and frequent reports from the field activities, and in turn can reduce downtime, rework, waste, and ultimately cost overrun. This project also involves educational and outreach activities to promote teaching and learning, engage undergraduate and graduate students, and reach out to underrepresented groups, K-12 students, and industry professionals. These activities include development of two course modules of ""visual sensing for civil infrastructure engineering and management"" and ""mobile cyber-physical systems,"" as well as creating new software tools and hands-on outreach materials for context aware AEC/FM applications, which will be widely distributed among research and professional communities."
"1209387","Geometry and Topology in the Presence of Lower Curvature Bounds","DMS","GEOMETRIC ANALYSIS","09/15/2012","09/11/2012","Karsten Grove","IN","University of Notre Dame","Standard Grant","Christopher W. Stark","08/31/2016","$320,273.00","","kgrove2@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","MPS","1265","","$0.00","Abstract<br/><br/>Award: DMS 1209387, Principal Investigator: Karsten Grove<br/><br/>The principal investigator plans to continue his work on global problems in differential geometry and related areas. Special emphasis will be devoted to the pursuit of global Riemannian geometry via additional structures. This includes but is not limited to structures arising from the presence of symmetries, and to structures arising from taking external or internal limits. Our efforts concerning investigations of relations between curvature, symmetry and topology is guided by the so-called symmetry program initiated by the investigator and set forth by the aim: ""Classify or describe the structure of manifolds with positive or nonnegative curvature and large isometry groups"". This area has experienced significant advances in various directions during the past two decades with contributions from many people, resulting in a number of classification type theorems, the discovery of new structures and numerous examples in non-negative curvature and one in the illusive case of positive curvature. Most recently rigidity phenomena providing a link to Tits geometry of buildings has emerged. Here topology of buildings is introduced via the Hausdorff topology of so-called chamber systems. The main focus of the project will be to further develop this connection to buildings, but will also include investigations of structures arising from the collapse of manifolds under a lower curvature bound, and recent connections between comparison geometry and a new applied area concerned with the ""processing of manifold-valued data"". The latter also deals with averages via non-linear ""center of mass"" and taking internal limits.<br/><br/>The project deals with a vast and flexible extension of the classical rigid and maximally symmetric euclidean, spherical and hyperbolic geometries, as well as of the theory of surfaces. Finding and exhibiting relations between geometry and topology is at the heart of the subject. Here geometry refers to those properties of a space that are invariant under distance preserving transformations (called symmetries here), whereas topology refers to the more flexible properties of a space that are invariant under transformations such as stretching, bending and deforming. Curvature governs the local behavior of geodesics, i.e., of the ""straight lines"" in the space. By comparison, the angle sum of a geodesic triangle in a positively curved space is bigger than 180 degrees, which is the angle sum of a triangle in the flat Euclidean plane. This general type of geometry plays a vital role in much of mathematics, physics and more recently in applications to signal processing and more. Most of the proposed activity falls under the umbrella of the ""symmetry program"" designed by the investigator. The main specific goals within the next few years are on the one hand to find additional new examples of positively and nonnegatively curved manifolds, and at the opposite extreme to show that the so-called symmetric spaces indeed are rigid objects in a natural sense. The latter is based on a recently discovered link to a very different area, namely Tits geometry of buildings, an area that has had profound and diverse applications within mathematics. Other important goals include developing new directions in geometry motivated by the emerging field of processing of manifold-valued data to, e.g., computer vision, medical imagining, sensor networks, and statistical analysis of shapes."
"1161852","Temporal Stimulus Segmentation with Spiking Neurons","IIS","INFORMATION TECHNOLOGY RESEARC","10/01/2012","09/06/2012","Giancarlo La Camera","NY","SUNY at Stony Brook","Standard Grant","Kenneth C. Whang","09/30/2016","$316,236.00","","giancarlo.lacamera@stonybrook.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","1640","7495, 7924","$0.00","Human beings have an unmatched ability to learn to abstract information from the environment, use this information to predict the consequences of their actions, and develop appropriate behavioral strategies. State-of-the-art artificial devices are far from exhibiting such autonomous learning abilities. This project focuses on a subset of the skills required for autonomous learning, specifically, the ability to identify the relevant cues and stimuli from the environment and establish how to act in their presence. In many situations, such as speech processing, this problem amounts to extracting temporally extended segments embedded in a continuous sensory stream of irrelevant stimuli and noise, a problem of temporal stimulus segmentation. Traditional algorithms for stimulus segmentation that learn based just on the unsegmented input stream are unlikely to succeed in this task, especially if the relevant segments do not exhibit features that are detectable by some standard pre-processing strategy. As a consequence, existing models typically endow a learning agent with prior knowledge of what is relevant (the so-called ""states"" of the agent) and focus on the problem of relating each state with the outcome they predict. Such models, of widespread use in computational neuroscience and machine learning, are unable to form or modify their own relevant states, preventing the development of truly autonomous learning and decision-making devices.<br/><br/>This project aims to develop a biologically relevant solution to the problem of temporal stimulus segmentation, with a focus on foundational theory and principles. In this theory, the states are represented by spatio-temporal patterns of spike trains and are processed by a network of spiking neurons capable of online, spike-based learning. The network learns to segment the input stream by taking appropriate actions at the right time, using a spike-based synaptic plasticity rule that approximates gradient ascent on the average reward, and makes use of locally available information about the network's decision. Importantly, the relevant segments are constructed so as to be behaviorally meaningful but not statistically different from irrelevant stimuli and noise, and therefore their boundaries are not detectable by standard pre-processing techniques.<br/><br/>Segmentation performance will be quantified as a function of the network's properties (such as number of neurons, connectivity and architecture) in the framework of different neurobiologically-inspired stimulus-coding schemes, and contrasted to more traditional approaches such as artificial neural networks and hidden Markov models. The research will be further enhanced by developing a hard challenge application related to natural stimuli, with the long-term goal of demonstrating the power and usefulness of the new approach compared to more traditional ones. This will also demonstrate that the network can be applied widely, across modalities (e.g., vision as well as speech), and in real-life scenarios."
"1149917","CAREER: A Multiagent Teacher/Student Framework for Sequential Decision Making Tasks","IIS","ROBUST INTELLIGENCE","09/01/2012","08/01/2012","Matthew Taylor","PA","Lafayette College","Standard Grant","James Donlon","09/30/2013","$402,065.00","","taylorm@eecs.wsu.edu","High Street","Easton","PA","180421768","6103305029","CSE","7495","1045","$0.00","Physical (robotic) agents and virtual (software) agents are becoming increasingly common in industry, education, and domestic environments. Although recent research advances have enabled agents to learn how to complete tasks without human intervention, little is known about how best to have humans teach agents or agents teach other agents or even how agents might teach humans. Considering the full matrix of agent/human learning, in which either an agent or a human can play the role of teacher or student, would increase the potential benefits of leveraging human and agent expertise and knowledge. <br/><br/>This project aims to study agent/human learning in the context of sequential decision-making problems, a class of central importance for real-world agent systems. This project aims to develop a novel teacher/student framework that integrates autonomous learning with teaching by another agent or a human. The project plans to develop and evaluate a set of core algorithms to allow: (1) agents to teach agents, thus enabling robust knowledge sharing among agents; (2) humans to teach agents, thus allowing humans to share or transfer common sense or domain-specific knowledge with agents; and (3) agents to teach humans, thus helping humans better understand how to perform or recast sequential decision-making tasks already understood or performed by autonomous agents. In all cases, the goal is to develop methods that significantly improve learning performance relative to learning without guidance from a teacher. Issues to be explored include mismatch between teacher/student abilities, learning from multiple teachers, and shared knowledge representation between teacher/student. The PI plans to focus on several scenarios, each with different sets of assumptions about the knowledge or skill of the student or teacher and the kind of interaction possible between them (e.g., whether the teacher can tell the student what action to take). The techniques developed in the project will be evaluated in a variety of tests domains and will involve simulations as well as actual robots.<br/><br/>The teacher/student framework will enable agents to teach other agents and humans, as well as integrate autonomous learning with agent and human teaching. Understanding how to best teach agents is of key importance in developing deployable agent systems. The platform- and domain-independent approach incorporates ideas from multiagent systems, machine learning, human-computer interaction, and human-robot interaction communities, and has the potential to impact each of these areas. This work takes a step towards transitioning agents from specialized systems usable only by experts into useful tools and teammates for people without programming expertise. <br/><br/>This project has a strong educational component. The PI teaches at an undergraduate college and undergraduate students will play a crucial role throughout the project. Furthermore, the research produced by this project will be incorporated into five of the PI's courses, providing exciting new material to attract and retain computer science majors. The PI will also continue outreach to secondary school students as well as to underrepresented groups via Lafayette College's S-STEM and Higher Achievement programs."
"1248052","INSPIRE: Studying and Promoting Quantitative and Spatial Reasoning with Complex Visual Data Across School, Museum, and Web-Media Contexts","DUE","Geography and Spatial Sciences, Information Technology Researc, TUES-Type 2 Project, Discovery Research K-12, Cyberlearn & Future Learn Tech, INSPIRE","09/15/2012","09/15/2012","Leilah Lyons","IL","University of Illinois at Chicago","Standard Grant","Myles Boylan","08/31/2016","$799,151.00","Joshua Radinsky, Andrew Beveridge","llyons@uic.edu","809 S. Marshfield Avenue","Chicago","IL","606124305","3129962862","EHR","1352, 1640, 7511, 7645, 8020, 8078","1352, 1640, 7645, 8045, 8653, 9178","$0.00","This INSPIRE award is partially funded by the Transforming Undergraduate Education in STEM Program in the Division of Undergraduate Education and the Discovery Research K12 Program in the Division of Research on Learning in the Directorate for Education and Human Resources; the Geography and Spatial Sciences Program in the Division of Behavioral and Cognitive Sciences in the Directorate for Social and Behavioral Sciences; and the Cyberlearning program in the Division of Information and Intelligent Systems in the Directorate for Computer & Information Science & Engineering.  <br/><br/>This is a research program to promote and study effective strategies and habits of mind for understanding complex geospatial data using interactive visualization tools, and to generate and test design strategies for such tools in three contexts: an online data access website, interactive museum exhibit, and social science classrooms. All three research sites will utilize geographic information system (GIS) visualization tools to give learners access to geospatially-referenced historical U. S. Census data for examining changing populations across space and time. These complex data tools are increasingly used in different disciplines and in multiple aspects of everyday life.  However, how learners interact with them is still poorly understood. The three research sites will strategically employ multiple, coordinated research methods, including design-experimentation, machine-learning analyses, multimodal interaction analyses, and grounded-theory generation of hypotheses. The outcomes of these empirical studies, coordinated across three contexts, will be used for iterative generation and evaluation of design strategies to promote effective reasoning with complex data for learners in each of these learning environments.  This project also will build a coherent, interdisciplinary understanding of representational fluency for complex geospatial data across learning contexts. <br/><br/>Intellectual Merit:  The merit of this research program derives from its fundamental interdisciplinarity, combining research methods, concepts, and design expertise from Sociology, Computer Science, the Learning Sciences, and GIS design and large-scale implementation, in the service of addressing pressing questions about how people learn with social-scientific data. Because the proposed research does not fit neatly within any single academic discipline, it is an ideal match for the goals of the INSPIRE program. The conceptual and methodological diversity of this project contributes to its potentially transformative nature, as a model for cross-discipline collaboration in transforming status-quo approaches to studying data visualization and the design of learning environments.  The collaborating investigators, as active members of multiple research communities and regular contributors to the work of educational practitioners, are uniquely situated to effectively translate empirical research findings into accessible designs and professional development opportunities across multiple communities.<br/><br/>Broader Impacts: The proposed project has the potential to have substantial impact due to the exceptional scale of dissemination currently realized by the Social Explorer project and related installations at the New York Science Museum, as well as the diversity of audiences for design-based research conducted in these three contexts (museum, classroom and web-based learning environments). In addition, the core of the research is the archive of historical U. S. Census data, which is central to research practices across social science disciplines, as well as popular news media, and is also highly relevant to people's understandings of our society."
"1219258","RI: Small: Integrating Learning and Search for Structured Prediction","IIS","Robust Intelligence","08/01/2012","05/22/2013","Prasad Tadepalli","OR","Oregon State University","Standard Grant","Jie Yang","07/31/2017","$463,437.00","Alan Fern","tadepall@eecs.orst.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","CSE","7495","7495, 7923, 9251","$0.00","The field of machine learning is extremely successful in solving classification problems where the inputs are fixed size feature vectors and the outputs are a small fixed number of classes. However, many applications such as natural language understanding and visual scene interpretation involve inputs and outputs of variable size that have rich internal structure. This project will study new approaches for such structured prediction problems. For example, the inputs may be natural language documents or visual scenes and the outputs may be formal representations of their semantic content, such as entities inferred or observed and the relationships between them. Most approaches to structured prediction learn a cost function to score potential structured outputs. Finding the correct output for the given structured input then consists of inferring the least cost output. Unfortunately, the computational cost of this inference is prohibitive for expressive cost functions; this forces the use of either simpler cost functions or approximate inference. In either case, prediction accuracy can suffer.<br/><br/>The current project aims to address this issue by integrating learning and search in a new framework that allows for the development of novel algorithms for structured prediction. In particular, this project will address three topics: (1) A generic framework will be developed for cost function learning by imitating the decisions of an optimal time-bounded search procedure on the training data. This will allow for a wide range of state-of-the-art search algorithms to be leveraged for structured prediction. (2) A theory and framework will be developed to learn to speed up the search for a global optimum by compressing the search into a shorter time-frame. This will allow for learning to address not only accuracy but also the computational efficiency of the predictor. (3) Both the cost function learning and speedup learning will be instantiated in multiple search algorithms and evaluated in different applications.<br/><br/>The project seeks to make contributions to a variety of applications of broad impact including natural language understanding, tracking objects in video, and personalized scheduling. The frameworks, algorithms and testbeds for learning to search and structured prediction will be integrated into the Weka tool-box so that they can be easily combined with different supervised learning algorithms and used in further research. The results and benchmark domains will be publicly distributed through the project's web pages. A special topics graduate course will be taught on the topic of this proposal at Oregon State University."
"1132419","EAGER: Social Context and Functions of Testosterone Pulses in wild California mice","IOS","Animal Behavior","01/01/2012","08/16/2011","Matina Kalcounis-Ruppell","NC","University of North Carolina Greensboro","Standard Grant","Bruce Cushing","12/31/2013","$74,460.00","Catherine Marler","mckalcou@uncg.edu","1111 Spring Garden Street","Greensboro","NC","274125013","3363345878","BIO","7659","1228, 7916, 9178","$0.00","In nature, animals need to respond rapidly to changes in their environment.  This is especially true for males of species in which both males and females help to raise young because males have to both defend territories and care for young. One mechanism that might explain this quick change is the rapid release of testosterone (T).  When T is released, animals show a preference for the place in which they were located at the time of T release. We will test whether, in nature, the context (defending a territory at a home range boundary or being a caring parent at a nest) of the T release has an influence on behavior.  We predict that when T release occurs near the territory boundary, the animal will interact with intruders, likely in conjunction with ultrasound vocalizations.  On the other hand, we predict that when T release occurs near the nest, the animal will focus its attention on its mate and offspring.  We will test our predictions by injecting T into monogamous California mouse males at their territory boundary and at their nest and look for behavioral responses using remote sensing methods that include automated radio-telemetry, acoustic recording, and thermal-imaging.  Our results are expected to provide evidence that T is an important hormone for regulating relatively rapid tradeoffs among behaviors involving competition and those involving the mate and offspring.  This project is an important collaboration between an R1 and small research campus in which all participants will be uniquely trained in innovative approaches that address cutting edge yet fundamental issues in animal behavior. All aspects of this project will reach the broader public through our community outreach program entitled Bats and Mice In Your Backyard and our widely accessed www-based streaming content and field blog. We will broaden participation in this project through recruitment efforts for the field assistant at a local California State University campus near our field site. We also have exercises in place to formally assess our broader impacts."
"1232623","Design of Efficient Saddle Point Algorithms for Large-scale/Complex Geometry Convex Optimization","CMMI","COMPUTATIONAL MATHEMATICS, OPERATIONS RESEARCH","09/01/2012","07/18/2012","Arkadi Nemirovski","GA","Georgia Tech Research Corporation","Standard Grant","george hazelrigg","12/31/2015","$450,000.00","Alexander Shapiro","nemirovs@isye.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","ENG","1271, 5514","072E, 073E, 077E, 9263","$0.00","This award provides funding for theoretical and software development of novel algorithms for processing large-scale optimization models arising in Signal Processing,<br/>Medical Image Reconstruction, Machine Learning, and high-dimensional Statistical inference, where huge and steadily growing amounts of underlying data result in the<br/>necessity to process optimization problems with hundreds of thousands of variables and constraints. In addition, some of applications, such as low rank matrix<br/>approximations, lead to problems with difficult geometry, which amplifies significantly the challenges caused by sheer problem sizes. These challenges will be<br/>met via developing algorithms with cheap iterations utilizing state-of-the-art approaches, primarily bilinear saddle point reformulation of the problem of interest<br/>combined with duality-based handling difficult geometry and accelerating algorithms via various types of randomization. Theoretical and algorithmic developments<br/>will be adjusted to several generic applications (sparsity- and low-rank oriented Signal Processing and Machine Learning, extensions of total variation-based Image<br/>processing, and some others) and will be aimed at developing optimization techniques with good theoretical performance guarantees and visible practical potential;<br/>the latter will be validated by extensive numerical experimentation with both simulated and real life problems.<br/><br/>If successful, the research will advance theory and practice of optimization by enriching its abilities to process large-scale/complex geometry problems and thus will<br/>contribute significantly to the computational toolboxes in Signal Processing, Image Reconstruction, Machine Learning, and some other subject domains. As a byproduct,<br/>the research will contribute to recent tendency of bridging the corresponding research communities, with clear mutual benefits. In addition, the results of<br/>the research could form the base of new Ph.D.-level optimization courses."
"1237174","SHB: Type II (INT): DELPHI: Data E-platform Leveraged for Patient Empowerment and Population Health Improvement","IIS","Smart and Connected Health","10/01/2012","09/19/2012","Kevin Patrick","CA","University of California-San Diego","Standard Grant","Sylvia J. Spengler","09/30/2017","$2,000,000.00","Theodore Chan, Sanjoy Dasgupta, William Griswold, Yannis Papakonstantinou","kpatrick@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","8018","8018, 8062","$0.00","In response to a healthcare crisis of epidemic proportions, thousands of software developers have been innovating new personal healthcare applications and technologies that leverage advances in medical and computing technology.  Despite the endless streams of personal data that these tools process -- weight, activity, diet, heart rate, etc. -- they are relatively data poor.  Left out of these applications is a comprehensive set of users' clinical electronic medical records, genomic data, comparative data with relevant subpopulations, and data on environmental influences important to health and quality of life.<br/><br/>There are numerous barriers to incorporating such data in applications, the dominant factors being the tremendous volume and heterogeneity of such data, much of it streaming in real-time and spread across disparate stakeholder platforms.  A related problem is drawing inferences from these data. With the advances in databases and machine learning proposed, we envision a new era of health and healthcare where patients, providers and consumers are empowered by data access and applicability that we characterize as personalized population health.  In particular, we anticipate a new category of healthcare applications that infer one's health status - and help execute interventions - in the perspective of one's entire life history and context.<br/><br/>This project is conducting fundamental and applied research in support of a platform, called DELPHI, that enables integrated access and analysis of all data relevant to health, and consequently promotes more rapid development of empowering, data-driven health apps and tools by a broad community of health-related software developers. The platform supports an integrated ""whole health information model"" of the individual that provides developers a single point of access that both (a) hides distribution and data heterogeneities, and (b) facilitates drawing inferences from these ""noisy"" data. The platform enables novel forms of analyses based on contextual and statistical metadata.  Scalability is achieved through theoretically proven and newly proposed database and machine learning techniques.  Our research is driven by three disparate case studies and field trials: a clinician-facing type-1 diabetes intervention, a patient and consumer-facing hypertension application, and a regional population health asthma and respiratory disease scenario.<br/><br/>Intellectual Merit <br/><br/>DELPHI is yielding fundamental advances in databases and machine learning that enable a wide community of programmers - from full-time professional to relative novices - to program on top of a ""live"", streaming population-scale medical dataset.  Additionally, these techniques are being evaluated in at least three realistic field trials, yielding new insights on both the nature of computing on medical ""big data"" and the techniques we have proposed to make it tractable.<br/><br/>Broader Impact<br/><br/>This will be demonstrated through a personal well-being and population health applications ecosystem, with three immediate beneficiaries: 1) The San Diego Beacon Community, a model for health information exchanges currently under development nationally. 2) Governmental and non-profit agencies who serve as an example of public/private partnerships to promote community-wide health. 3) Private industry, in this case Qualcomm Life's/2net platform where we demonstrate how to utilize existing services in novel ways to handle health data. Finally, this project will serve as a training ground in personalized population health for graduate students, post docs and medical residents."
"1248077","INSPIRE:   Computer Learning of Dynamical Systems to Investigate Cognitive and Motivational Effects of Social Media Use on Political Participation","SES","Social Psychology, Political Science, Information Technology Researc, HCC-Human-Centered Computing, SOCIAL-COMPUTATIONAL SYSTEMS, INSPIRE","09/15/2012","09/22/2015","John Jost","NY","New York University","Standard Grant","Brian Humes","08/31/2017","$1,199,319.00","Jonathan Nagler, Joshua Tucker, Richard Bonneau","jj54@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","SBE","1332, 1371, 1640, 7367, 7953, 8078","0000, 8653, 9179, OTHR","$0.00","This INSPIRE award is partially funded by Human-Centered Computing Program and by Social-Computational Systems Program both in the Division of Information and Intelligent Systems in the Directorate for Computer & Information Science & Engineering, and by the Social Psychology Program in the Division of Behavioral and Cognitive Sciences and the Political Science Program in the Division of Social and Economic Sciences in the Directorate for Social, Behavioral and Economic Sciences.<br/><br/>With regards to intellectual merit, the goal of this project is to forge an interdisciplinary collaboration that examines the impact of social media on political behavior. First, from social psychology and political science, fundamental hypotheses will be developed about how, why and when social media affects citizens' cognition and motivation with respect to political participation. Second, these questions will be expressed as testable hypotheses derived from behavioral models. And third, drawing from biology and computer science, the project adapts sophisticated computational methods of approximate inference and machine learning (adapting methods developed for the analysis of Systems Biology data) to evaluate the behavioral models using extremely large social media and social network datasets.   <br/><br/>The scientific opportunities afforded by the use of social media are readily apparent when we consider the richness and precision of data on participation in elections, protests, riots, and other spontaneous political events. This project will construct a comprehensive data set of incoming and outgoing social media messages messages using systematically structure formats that are ideally suited to machine learning methods, and this information will be integrated with information on social network connectivity and a vast array of metadata on individuals and their social contacts. By developing new methods to harvest and combine these data sources effectively, it will be possible to transform the scientific study of social and political attitudes and behavior. Every time individuals use social media, they leave behind a digital footprint of what was communicated, when it was communicated, and, to whom it was communicated. Typically, such precise estimates of these variables are available only to laboratory investigators working in artificial settings. No previous study has successfully used fine-grained social influence data such as these to predict consequential behavioral outcomes, such as attendance at a given protest or rally. The structure of the data means that we will have panel data on respondents, many of potentially long duration.  In addition, the investigators will conduct a panel survey, which is essential for drawing causal inferences about the cognitive and motivational processes whereby social media use facilitates political participation.<br/><br/>With regards to broader impacts, this research will enhance interdisciplinary training for graduate and undergraduate students. These include students in psychology, political science, computer science, and biology and also includes students from groups that are underrepresented in these sciences. In addition, opportunities will be provided for high school students to become involved in the research process. The research program will foster broad dissemination of scientific understanding by leveraging past experience of the principal investigators with disseminating large code-bases, data-bases, and data-sets to share work with other scientists (pre-publication). Finally, the researchers are committed to making their research available to the general public and have extensive experience doing so."
"1152342","Beyond harmonic transition state theory for accelerating molecular dynamics","CHE","OFFICE OF MULTIDISCIPLINARY AC, Theory, Models, Comput. Method, CI REUSE","08/15/2012","08/15/2012","Graeme Henkelman","TX","University of Texas at Austin","Standard Grant","Evelyn M. Goldfield","07/31/2016","$491,829.00","","henkelman@utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","MPS","1253, 6881, 6892","1401, 5560, 7237, 7433, 7722, 9216, 9263, HPCC","$0.00","Graeme Henkelman from the University of Texas at Austin is supported by the Chemical Theory, Models and Computational Methods (CTMC) Program and the Office of Cyber Infrastructure (OCI) in developing algorithms for accelerating molecular dynamics simulations. For rare event systems, such as diffusion in solids and reactions at surfaces, transition state theory (TST) allows for a separation of time scales between that of molecular vibrations and the reactive events of interest. When reactive mechanisms are not known, an adaptive version of the kinetic Monte Carlo (KMC) method is used to find saddle points and rates are evaluated with harmonic TST.  To reduce the computational cost, a database is employed to store kinetic events and to make them available for later use in separate calculations with similar chemistry. When the reaction mechanisms for a class of system are known, the computational cost is low because all events are drawn from the database. To further accelerate the simulations, groups of states connected by fast rates are escaped using an analytic solution to the master equation. Storing the local connectivity of these states is essential for efficiency.  Accuracy will be improved with a set of methods ranging from harmonic TST to the (classically) exact full TST plus dynamical corrections. The challenge of TST is finding a dividing surface that separates reactants from any product state. A support vector machine is being developed to provide an analytic classification function based upon learned data.  Sampled points around the decision surface will be used to train the machine so that it can provide an accurate transition state without prior knowledge of reaction mechanisms.<br/><br/>This work is directed at alleviating the ultralong computational times needed for simulation of realistic chemical or material processes, e.g., catalysis, protein folding, molecular diffusion and so on.  The characteristic times for basic molecular motion are many, many times faster than the time scales we measure in the laboratory, and so acceleration algorithms must be developed.  EON2 is being developed as a distributed open-source program developed by the PI and collaborators to calculate long-timescale molecular dynamics in systems, for example, undergoing catalys and clustering on metal substrates.  It will use advanced database techniques and machine learning, and will be usable in conjunction with other software through customized interfaces.  A discussion forum is hosted by the PI for anyone with questions about the algorithms, software or science.  The project has been opened up so that computers on campus, supercomputers with idle time, and anyone from the public can contribute computational resources to accelerate dynamics simulations at the atomic scale. This aspect of the outreach program gives the public direct access to the research being done as part of this project."
"1201986","Adaptive and Intelligent Forecast Engine Platform for Full-Spectrum Prediction of Solar Power Output","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","05/01/2012","04/14/2012","Carlos Coimbra","CA","University of California-San Diego","Standard Grant","Radhakisan Baheti","04/30/2015","$384,452.00","","ccoimbra@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","ENG","7607","155E, 1653","$0.00","The objective of this research is to develop the concept of Smart Solar Farms. The approach is to integrate advanced solar forecasting methods into parallel computer platforms with the objective of quantifying the variability and availability of solar energy for power production, thus enabling solar integration with the power grid. <br/><br/>Intellectual Merit<br/>This project integrates a very high number of inputs, varying from radiometric and meteorological time series to high-resolution remote sensing and ground-to-sky images, into a single adaptive and intelligent learning platform capable of local processing of high-fidelity forecasts. The proposed research is transformative in that novel, highly optimized natively parallel platforms will generate, in-situ, real-time forecasts for all time horizons of interest by a combination of machine learning software/hardware integration with intelligent energy management systems.<br/><br/>Broader Impacts<br/>The reliable and economical integration of solar energy resources into the power grid is one of the most critical global technological challenges of our time. This project develops cost-effective tools that are essential to the integration of solar energy into a variety of possible smart grid solutions that aims at energy security and power quality improvements. We will also educate a new diverse generation of green energy experts, who master critical machine learning techniques specifically designed for variable resource power production and control. The concept of Smart Solar Farms will be demonstrated, and the test-bed will continue to provide high quality information, research opportunities and educational experiences for the community-at-large for many years beyond the duration of the project."
"1149018","CAREER: Limits of Communication","CCF","ALGORITHMIC FOUNDATIONS","06/01/2012","03/04/2016","Alexander Sherstov","CA","University of California-Los Angeles","Continuing grant","Dmitry Maslov","05/31/2018","$499,995.00","","sherstov@cs.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","7796","1045, 7927, 7928","$0.00","Consider a function whose arguments are distributed among several parties, making it impossible for any one party to compute it in isolation.  Communication complexity theory studies how many bits of communication are needed to evaluate the function.  Pioneered by Andrew Yao over thirty years ago, communication complexity has become a central research area in theoretical computer science.  First of all, studying communication as a limited resource has a strong practical motivation.  Moreover, open problems in many other computational models reduce to questions about communication.  To date, communication complexity theory has impacted almost every subject in theoretical computer science, from classical models such as Turing machines and circuits to more recent topics such as data structures, learning theory, and quantum computing.<br/><br/>This award takes aim at studying three longstanding open questions in communication complexity theory: (i) the limits of multiparty communication; (ii) the limits of communication with alternating existential and universal quantifiers; (iii) the conjectured equivalence of the combinatorial and matrix-theoretic views of communication.  A resolution of these questions would have major consequences in theoretical computer science beyond communication, including lower bounds for ACC circuits and neural networks, efficient learning of DNF formulas, and an equivalence of quantum and classical communication. <br/><br/>Progress on the proposed problems will exploit insights from, and contribute new ideas to, other disciplines such as machine learning and matrix theory.  This award provides an ample source of research problems at various levels of difficulty and will be used in advising students and teaching new graduate and undergraduate courses.  As an integral part of the award, the PI will promote theory research in the Los Angeles area and take an active part in scientific dissemination."
"1246073","AF: EAGER: Bayesian Factor Modeling of Context-Specific Gene Regulation","CCF","Algorithmic Foundations","08/01/2012","07/12/2012","Yufei Huang","TX","University of Texas at San Antonio","Standard Grant","Mitra Basu","07/31/2015","$296,859.00","Yidong Chen","yhuang@utsa.edu","One UTSA Circle","San Antonio","TX","782491644","2104584340","CSE","7796","7916, 7931","$0.00","Response of cells to their changing environment is governed by intricate regulations of gene expression by regulating molecules in cells including, most importantly, transcription factors (TFs) and microRNAs (miRNAs). Understanding how TF and miRNA regulations define cellular states such as cell survival, cell proliferation, and cell death, and eventually phenotypes including various diseases is a major challenge facing computational systems biologists. <br/><br/>Intellectual Merit<br/> This EAGER project will develop and validate a novel computational model called Semi-parametric Bayesian FActor Regulatory Model (SB-FARM) for TF and miRNA regulation of gene expression. The advantages of SB-FARM over other existing models are that it integrates existing knowledge about gene regulations in the model and enables the discovery of specific regulations by TFs and miRNAs under unmeasured conditions or contexts. The investigators will examine the modeling details as well as algorithms for reconstructing the model from a set of gene expression data.  They will apply SB-FARM to study the context-specific regulations in E-coli and human cancer. <br/><br/>The long term goal of the investigators is to develop signal processing and statistical learning methods for the system-level understanding of gene regulatory networks underlying different biological processes and apply them to better understand the genomic basis for diversity in organisms and development of diseases. The SB-FARM has a general structure that permits integration of additional aspects of gene regulation. The success of the SB-FARM will have long lasting impact on gene regulation research and is expected to also significantly advance statistical signal processing and Bayesian learning.<br/><br/>Broader Impact <br/>This research is highly interdisciplinary, cross-cutting science and engineering. It will provide an environment for advanced interdisciplinary learning and education in the area of genomics signal processing and computational biology. The PIs will also actively involve graduate and undergraduate students in research activities. Particularly, they will utilize the minority institution status of UTSA and UTHSCSA to recruit and involve minority students in this research. The developed computational methods will be implemented into publically available software to aid computational biology researchers to investigate context specific gene regulations. The computational methods and tools will enhance the signal processing and machine learning research and ultimately lead to development of new theory and methods."
"1217793","AF: Small: Fundamental High-Dimensional Algorithms based on Convex Geometry and Spectral Methods","CCF","ALGORITHMIC FOUNDATIONS","09/01/2012","07/19/2012","Santosh Vempala","GA","Georgia Tech Research Corporation","Standard Grant","Tracy J. Kimbrel","08/31/2016","$420,000.00","","vempala@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7796","7923, 7926, 7929","$0.00","This project seeks to discover new algorithms and develop algorithmic tools to address fundamental open problems in the theory of algorithms under the following focus topics: (1) Rounding. The power of affine transformations in the design of algorithms and in their analysis, including for solving LP's in strongly polynomial time and approximately sandwiching convex bodies. (2) Learning. Algorithms for learning polyhedra, learning subspace juntas and identifying planted cliques in random graphs. (3) Isoperimetric inequalities. Extensions of Cheeger's method to higher eigenvalues and multi-partitions, and the KLS hyperplane conjecture. (4) Lattices and convex geometry. Optimization and sampling problems over lattices, including: (a) the complexity of integer programming (determining whether a convex body intersects a given lattice), (b) the complexity of cutting plane methods, and (c) conditions under which lattice points in a convex body be sampled efficiently.<br/><br/>The problems explored are of a basic nature, and originate from many areas, including optimization (both discrete and continuous), sampling, machine learning and data mining. With the increasing availability of high-dimensional data in important application areas, efficient tools to handle such data are a necessity. This award addresses some of the most basic questions arising from this need. <br/><br/>The PI, an active member of the Algorithms and Randomness Center (ARC), served as its founding director and continues in-depth collaborations with scientists from various fields to identify problems and ideas that could play a fundamental role in understanding the complexity of computation. The project will contribute to graduate courses with online notes, textbooks and up-to-date survey articles."
"1217559","CGV: Small: Making Sense out of Large Graphs - Bridging HCI with Data Mining","IIS","Info Integration & Informatics, GRAPHICS & VISUALIZATION","09/15/2012","07/31/2014","Christos Faloutsos","PA","Carnegie-Mellon University","Continuing Grant","Maria Zemankova","08/31/2016","$528,578.00","Aniket Kittur, Duen Horng Chau","christos@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7364, 7453","7364, 7453, 7923, 9251","$0.00","The goal of this research project is to help people make sense of large graphs, ranging from social networks to network traffic. The approach consists of combining two complementary fields that have historically had little interaction -- data mining and human-computer interaction -- to develop interactive algorithms and interfaces that help users gain insights from graphs with hundreds of thousands of nodes and edges.  The goal of the project is to develop mixed-initative machine learning, visualization, and interaction techniques in which computers do what they are best at (sifting through huge volumes of data and spotting outliers) while humans do what they are best at (recognizing patterns, testing hypotheses, and inducing schemas). This research addresses two classes of tasks: first, attention routing -- using machine learning to direct an analyst's attention to interesting nodes or subgraphs that do not conform to normal behavior. Second, sensemaking -- helping analysts build in-depth representations and mental models of a specific areas or aspects of a graph. Evaluation of the tools will involve both controlled laboratory studies as well as long-term field deployments.<br/><br/>As large graphs appear in many settings -- national security, intrusion detection, business intelligence (recommendation systems, fraud detection), biology (gene regulation), and academia (scientific literature) -- the potential benefits of new tools for making sense of graphs is far reaching. Project results, including open-source software and annotated data sets, will be disseminated via the project web site (http://kittur.org/large_graphs.html) and incorporated into educational activities."
"1208186","NRI-Small: Collaborative Research: Assistive Robotics for Grasping and Manipulation using Novel Brain Computer Interfaces","IIS","NRI-National Robotics Initiati","10/01/2012","09/06/2012","Sanjay Joshi","CA","University of California-Davis","Standard Grant","Irina Dolinskaya","09/30/2018","$430,000.00","","maejoshi@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","CSE","8013","7923, 8086","$0.00","This is a collaborative proposal (with UC Davis) which is aimed at making concrete some of the major goals of Assistive Robotics. A team of experts has been brought together from the fields of signal processing and control, robotic grasping, and rehabilitative medicine to create a field-deployable assistive robotic system that will allow severely disabled patients to control a robot arm/hand system to perform complex grasping and manipulation tasks using novel Brain Muscle Computer Interfaces (BMCI). Further, the intent of this effort is not just technology-driven, but is also driven by clear and necessary clinical needs, and will be evaluated on how well it meets these clinical requirements.  Validation will be performed at the Department of Regenerative and Rehabilitation Medicine at Columbia University on a diverse set of disabled users who will provide important feedback on the technology being developed, and this feedback will be used to iterate on the system design and implementation.<br/><br/>Intellectual Merit: The intellectual merit of this proposal includes:<br/>o Novel research in Human Machine Interfaces that has the potential to be transformative in eliciting rich, multi-degree-of-freedom signal content from simple and non-invasive surface electromyographic (sEMG) sensors.<br/>o Development of smart adaptive software that employs machine learning algorithms that can continually monitor user performance, and then automatically calibrate and tune system parameters based on system performance.<br/>o Data driven methods for real-time grasp planning algorithms that can be used with both known and unknown objects.<br/>o Methods for finding pose-robust grasps that are tolerant of errors in sensing.<br/>o Evaluation of an underactuated hand as a grasping device for certain application tasks.<br/>o Integration of 3D vision with real-time grasp planning.<br/>o Scientific evaluation at the clinical level of the impact of these new technologies on the disabled population.<br/><br/>Broader Impacts: The broader impacts of this proposal include:<br/>o Development of a complete system to aid the severely disabled population with tetraplegia.<br/>o Extensions of this technlogy to others lacking motor control function including multiple sclerosis, stroke, amyotrophic lateral sclerosis (ALS or Lou Gehrig disease), cerebral palsy, and muscular dystrophy.<br/>o New technology that can extend the reach and impact of the field of Assistive Robotics.<br/>o Major extensions to the open-source GraspIt! software system that will allow many other researchers to leverage the results of this project.<br/>o Educational thrusts that will bring together engineering students, clinicians and the disabled population to extend the reach and scope of Assistive Robotics.<br/>o New directions in Human Machine Interfaces that can extend beyond the disabled population and into a variety of other applications."
"1208153","NRI-Small: Collaborative Research: Assistive Robotics for Grasping and Manipulation using Novel Brain Computer Interfaces","IIS","Disability & Rehab Engineering, NRI-National Robotics Initiati","10/01/2012","06/20/2016","Peter Allen","NY","Columbia University","Standard Grant","Irina Dolinskaya","09/30/2018","$800,498.00","Joel Stein","allen@cs.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","5342, 8013","7923, 8086, 9251","$0.00","This is a collaborative proposal (with UC Davis) which is aimed at making concrete some of the major goals of Assistive Robotics. A team of experts has been brought together from the fields of signal processing and control, robotic grasping, and rehabilitative medicine to create a field-deployable assistive robotic system that will allow severely disabled patients to control a robot arm/hand system to perform complex grasping and manipulation tasks using novel Brain Muscle Computer Interfaces (BMCI). Further, the intent of this effort is not just technology-driven, but is also driven by clear and necessary clinical needs, and will be evaluated on how well it meets these clinical requirements.  Validation will be performed at the Department of Regenerative and Rehabilitation Medicine at Columbia University on a diverse set of disabled users who will provide important feedback on the technology being developed, and this feedback will be used to iterate on the system design and implementation.<br/><br/>Intellectual Merit: The intellectual merit of this proposal includes:<br/>o Novel research in Human Machine Interfaces that has the potential to be transformative in eliciting rich, multi-degree-of-freedom signal content from simple and non-invasive surface electromyographic (sEMG) sensors.<br/>o Development of smart adaptive software that employs machine learning algorithms that can continually monitor user performance, and then automatically calibrate and tune system parameters based on system performance.<br/>o Data driven methods for real-time grasp planning algorithms that can be used with both known and unknown objects.<br/>o Methods for finding pose-robust grasps that are tolerant of errors in sensing.<br/>o Evaluation of an underactuated hand as a grasping device for certain application tasks.<br/>o Integration of 3D vision with real-time grasp planning.<br/>o Scientific evaluation at the clinical level of the impact of these new technologies on the disabled population.<br/><br/>Broader Impacts: The broader impacts of this proposal include:<br/>o Development of a complete system to aid the severely disabled population with tetraplegia.<br/>o Extensions of this technlogy to others lacking motor control function including multiple sclerosis, stroke, amyotrophic lateral sclerosis (ALS or Lou Gehrig disease), cerebral palsy, and muscular dystrophy.<br/>o New technology that can extend the reach and impact of the field of Assistive Robotics.<br/>o Major extensions to the open-source GraspIt! software system that will allow many other researchers to leverage the results of this project.<br/>o Educational thrusts that will bring together engineering students, clinicians and the disabled population to extend the reach and scope of Assistive Robotics.<br/>o New directions in Human Machine Interfaces that can extend beyond the disabled population and into a variety of other applications."
"1221984","IPGA: Characterization, Modeling, Prediction, and Visualization of the Plant Transcriptome","IOS","Plant Genome Research Resource","01/01/2012","12/18/2012","Volker Brendel","IN","Indiana University","Standard Grant","Diane Okamuro","06/30/2015","$1,384,920.00","Shailesh Lal, Karin Dorman","vbrendel@indiana.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","BIO","7577","1228, 1329, 9109, 9150, 9178, 9179, BIOT","$0.00","PI: Volker P. Brendel (Indiana University) <br/><br/>CoPIs: Karin Dorman (Iowa State University), Shannon Schlueter (University of North Carolina - Charlotte) and Shailesh Lal (Oakland University) <br/><br/>Senior Personnel: Jon Duvick and Yasser El-Manzalawy (Iowa State University) <br/><br/>The premise of this project is that the scale of sequence and other data accumulation in plant genomics necessitates the development of novel, highly automated, scalable, comprehensive, and accurate approaches to genome annotation. The depth of transcript data accumulating for many plant species under numerous experimental conditions provide unprecedented evidence for the evaluation of all aspects of transcription, including precise mapping of transcription start sites as well as dominant and alternative splice sites. This project engages a team of experts in a wide range of fields, including genomics, molecular biology, bioinformatics, statistics, machine learning, high performance computing, and software engineering to jointly work toward a solution for accurately predicting the expressed protein-coding gene transcriptome from plant genome sequences. Successful completion of the project will result in the deployment of (1) software that implements the novel prediction algorithms, (2) visualization and data access portals, and (3) a cyberinfrastructure environment implementation of the developed tools for distributed computing, sharing of protocols, and analysis provenance recording. In the long run, the project seeks to explore the extent to which genomic biology can transition from a largely descriptive to a highly predictive science driven by quantitative measurements, with algorithms and computation as the domain-adapted language. <br/><br/>The project will generate standardized, accurate protein-coding gene structure annotation for 25 plant genomes from a wide range of the phylogenetic spectrum. Initial emphasis will be on improved annotation of recently sequenced genomes, which will benefit the entire community of researchers working on these important crops. The anticipated algorithms for transcriptome prediction will be essential to the analysis of the thousands of complete plant genome sequences likely to become available within the next few years. Through the development of reliable gold standard annotations and the dissemination of training and test sets for algorithmic development, a larger community of computational data analysts, in particular from the machine learning community, will be engaged. All software developed and data generated in this research is freely available through project Web sites, in particular www.plantgdb.org. The project's plan for integration of research and education will train a new generation of scientists to work on genomics data with the broad range of interdisciplinary approaches represented by the project team."
"1161480","AF: Medium: Collaborative Research: Uncertainty Aware Geometric Computing","CCF","Algorithmic Foundations","09/01/2012","06/26/2014","Leonidas Guibas","CA","Stanford University","Continuing grant","Tracy Kimbrel","08/31/2016","$300,000.00","","guibas@cs.stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","7796","7924, 7929","$0.00","Most scientific and engineering disciplines today have enormous opportunities for creation of knowledge from massive quantities of data available to them. But the lack of appropriate algorithms and analysis tools for processing, organizing, and querying this data deluge makes this task extremely challenging. A large portion of the data being acquired today has a geometric character, and even non-geometric data are often best analyzed by embedding them in a multi-dimensional feature space and exploiting the geometry of that space. This data is invariably full of noise, inaccuracies, outliers, is often incomplete and approximate, yet most of the existing geometric algorithms are unable to cope with any data uncertainty in relating their output to their input. <br/><br/>The project aims to fill this void by investigating uncertainty-aware geometric computing, with an express goal of designing algorithmic techniques and foundations that will help extract ``knowledge'' from large quantities of geometric data in the presence of various non-idealities and uncertainties. It focuses on a number of fundamental geometric problems, all dealing with uncertain data. A unified set of models will be developed for modeling uncertainty that can deal with multiple uncertainty types, and attention will be paid to handling noise/outliers in heterogeneous and dynamic data. Algorithms will be investigated for understanding how input uncertainty carries over to output uncertainty (e.g. by associating a confidence level or likelihood with each output, or computing certain statistics of the output) and how the input uncertainty impacts the quality of the output (e.g. by defining and computing the stability of the output in terms of the input uncertainty). Since exact solutions are likely to be computationally infeasible, the emphasis will be on simple, efficient approximation techniques (e.g. computing a compact, approximate distribution of geometric/topological structures such as Delaunay triangulations and their subcomplexes of uncertain data). <br/><br/>A key ingredient of the award is to address a variety of computational issues that arise in the presence of uncertainty using a few key problems, and to develop a core set of techniques that illuminate algorithmic design under uncertainty not only on these key problems but that can also be transferred to other geometric problems, as needed. This research touches upon many topics in theoretical computer science and applied mathematics including discrete and computational geometry, discrete and continuous optimization, estimation theory, and machine learning. This study will strengthen connections of computational geometry with a variety of disciplines, including machine learning, probabilistic databases, statistics, and GIS. Since so many problems require geometric data analysis, the project has the potential of enhancing the capability of various government, commercial, and civic units to make informed decisions that impact the society at large."
"1161495","AF: Medium: Collaborative Research: Uncertainty Aware Geometric Computing","CCF","ALGORITHMIC FOUNDATIONS","09/01/2012","06/25/2014","Subhash Suri","CA","University of California-Santa Barbara","Continuing grant","Tracy J. Kimbrel","08/31/2016","$300,000.00","","suri@cs.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","CSE","7796","7924, 7929","$0.00","Most scientific and engineering disciplines today have enormous opportunities for creation of knowledge from massive quantities of data available to them. But the lack of appropriate algorithms and analysis tools for processing, organizing, and querying this data deluge makes this task extremely challenging. A large portion of the data being acquired today has a geometric character, and even non-geometric data are often best analyzed by embedding them in a multi-dimensional feature space and exploiting the geometry of that space. This data is invariably full of noise, inaccuracies, outliers, is often incomplete and approximate, yet most of the existing geometric algorithms are unable to cope with any data uncertainty in relating their output to their input. <br/><br/>The project aims to fill this void by investigating uncertainty-aware geometric computing, with an express goal of designing algorithmic techniques and foundations that will help extract ``knowledge'' from large quantities of geometric data in the presence of various non-idealities and uncertainties. It focuses on a number of fundamental geometric problems, all dealing with uncertain data. A unified set of models will be developed for modeling uncertainty that can deal with multiple uncertainty types, and attention will be paid to handling noise/outliers in heterogeneous and dynamic data. Algorithms will be investigated for understanding how input uncertainty carries over to output uncertainty (e.g. by associating a confidence level or likelihood with each output, or computing certain statistics of the output) and how the input uncertainty impacts the quality of the output (e.g. by defining and computing the stability of the output in terms of the input uncertainty). Since exact solutions are likely to be computationally infeasible, the emphasis will be on simple, efficient approximation techniques (e.g. computing a compact, approximate distribution of geometric/topological structures such as Delaunay triangulations and their subcomplexes of uncertain data). <br/><br/>A key ingredient of the award is to address a variety of computational issues that arise in the presence of uncertainty using a few key problems, and to develop a core set of techniques that illuminate algorithmic design under uncertainty not only on these key problems but that can also be transferred to other geometric problems, as needed. This research touches upon many topics in theoretical computer science and applied mathematics including discrete and computational geometry, discrete and continuous optimization, estimation theory, and machine learning. This study will strengthen connections of computational geometry with a variety of disciplines, including machine learning, probabilistic databases, statistics, and GIS. Since so many problems require geometric data analysis, the project has the potential of enhancing the capability of various government, commercial, and civic units to make informed decisions that impact the society at large."
"1208315","Rare and Weak Signals in Big Data: How to Find Them and How to Use Them","DMS","STATISTICS","08/01/2012","08/09/2014","Jiashun Jin","PA","Carnegie-Mellon University","Standard Grant","Gabor Szekely","07/31/2017","$119,999.00","","jiashun@stat.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","MPS","1269","","$0.00","A research effort is proposed to create new tools for high dimensional data analysis, focusing on the very challenging regime where signals are both rare and weak. In particular,  the proposer proposes to:   (a).  Develop graphlet screening as a new tool for high dimensional variable selection, introduce a new theoretic framework for assessing the optimality of variable selection, and show that graphlet screening achieves the optimal rate of convergence in terms of Hamming distance of the selection errors.  (b). Develop a new method of spectral clustering by using the recent idea of Higher Criticism thresholding, and investigates the fundamental limits for several problems related to low-rank matrix recovery, including high dimensional clustering, sparse Principle Component Analysis, and a testing problem related to the underlying large-size covariance matrix. (c) Extend and apply the proposed methods and theory to the analysis of Big data  generated in  various scientific fields,  including genomics and machine learning. <br/><br/>We are often said that we are entering the era of 'Big Data', where massive datasets consisting of millions of observations are mined for associations and patterns. What is never said about this pervasive trend is that, unfortunately, the signal we are looking for is usually very rare and weak and is hard to find, and it is easy to be fooled. The project introduces new ideas, new tools, and novel theory that are appropriate for rare and weak signals in Big Data, and apply the theory and methods to various scientific fields, including genomics and machine learning."
"1149662","CAREER: Quantifying diffusion and dynamics on healthcare, innovation and communication networks","IIS","Info Integration & Informatics, CLB-Career","07/01/2012","07/13/2016","Edoardo Airoldi","MA","Harvard University","Continuing Grant","Sylvia Spengler","08/31/2019","$495,208.00","","airoldi@temple.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","7364, 9103","1045, 7364, CL10","$0.00","Many modern data collections, gathered for the purpose of providing insights into matters of national interest such as medical and technological innovation, typically measure quickly evolving interactions, in addition to traditional unit-level measurements, in the context of a network. This project develops an integrated research and educational program to enable scientific and quantitative analyses of interactions and other combinatorial measurements as they change over time. Technical problems being addressed include, but are not limited to: an efficient representation that facilitates quantitative analyses of large-scale networks; models of how information and behavior evolve over time as a consequence of the network context they are embedded in; and fast algorithms to perform estimation of critical parameters in these models. These methods will be demonstrated on case studies exploring: the diffusion of medical innovations among physicians and its impact on health; technological innovation dynamics in the United States and the role of non-compete agreements; the estimation of point-to-point communications on a network, from aggregate traffic that is passively monitored.<br/><br/>The presence of interactions and other combinatorial measurements as a source of observed variation in the data creates new statistical and inferential challenges. For instance, generalized linear model theory needs to be extended to responses on a network. The analysis of processes on a network often induces constraints that make the inferential problems ill posed, since they involve a large number of unknown quantities to describe few observations. Estimation may require sampling from, and integrating over, extremely constrained parameter spaces. Importantly, interactions do not necessarily encode statistical dependence. In this sense, dealing with observed interactions requires original thinking; the data settings they entail are not amenable to analysis with classical methods, in which interactions are inferred as a means to encode dependence among unit-level observations. This project tackles technical challenges with a statistical and machine learning approach. Anticipated technical results include, but are not limited to: (1) a new wavelet decomposition of multivariate and dynamic networks; (2) statistical models of diffusion of information on a given network, and models of inhomogeneous network dynamics in continuous time; (3) scalable estimation algorithms for these models; and (4) theoretical foundations of inference with big data. This research will be evaluated qualitatively and quantitatively, at Harvard and in collaboration with industrial partners.<br/><br/>The proposed research is integrated with an interdisciplinary educational program, which will attract undergraduates to research at the intersection of statistics and computer science, in the context of problems of national importance. It will provide opportunities to actively encourage students from underrepresented groups to pursue careers in statistics and computer science. Key elements of the educational program include the development of a statistical machine learning curriculum; lectures on YouTube available to everyone; tutorials at national and international conferences and workshops; and a monograph. Outreach activities include open-source software and webtools for the community at-large, and a collaborative effort with industrial partners to leverage the new computational tools and algorithms for benefiting their pools of users worldwide. Additional details regarding the project can be found at: http://www.fas.harvard.edu/~airoldi/career.html."
"1216758","III: Small: Advancing the Scientific Understanding of Bullying Through the Lens of Social Media","IIS","Info Integration & Informatics","09/01/2012","07/17/2017","Xiaojin Zhu","WI","University of Wisconsin-Madison","Standard Grant","Sylvia Spengler","08/31/2018","$499,866.00","Amy Bellmore","jerryzhu@cs.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","7364","7923","$0.00","Bullying has been recognized as a serious national health issue. Traditional approaches to the scientific study of bullying are hindered by data acquisition. For example, the standard approach has been to conduct personal surveys in schools. Due to its relatively small sample size and low temporal resolution, neither the true frequency of bullying over the population nor the evolution of bullying roles can be satisfactorily studied.  The traditional approaches are also very labor intensive. <br/><br/>Social media has developed to the point where it contains enough signal about bullying.  This project develops novel machine learning models that automatically monitor and analyze publicly available social media data to understand bullying.  These machine learning models reconstruct hidden bullying episodes from a sequence of social media posts.  They automatically determine who participated in which bullying episode as what role.  In addition, this project conducts human studies on bullying in school and in social media in parallel, by collecting self-report surveys by school-aged children and their social media posts simultaneously.  Such studies correlate the traditional psychological approach and social media data on bullying. Taken together, the project will provide significant new scientific data toward understanding, intervention, and helping policy-making regarding bullying."
"1217153","New Active-Set Methods for Optimization and Complementarity Problems","DMS","COMPUTATIONAL MATHEMATICS","08/01/2012","07/01/2014","Daniel Robinson","MD","Johns Hopkins University","Continuing grant","Junping Wang","07/31/2015","$210,000.00","","dpr219@lehigh.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","MPS","1271","9263","$0.00","The principal investigator and his student consider the design, analysis, implementation, and validation of a new class of active-set algorithms for solving large-scale optimization and complementarity problems.  In the first part of the project they introduce a new class of rapidly adapting active-set methods in the context of solving large-scale convex quadratic problems. These algorithms benefit if a good initial guess is available and allow for rapid updates to the active-set, which are essential for handling large-scale problems.  Consequently, this algorithm not only solves stand-alone quadratic programs, but may allow other methods, e.g., sequential quadratic programming methods, to handle problems that are larger than is currently possible.  In the second part of the proposal, they introduce the concept of an acceleration phase, which improves upon existing subspace phases.  Subspace phases have been used with great success to enhance basic algorithms for solving various complementarity, variational inequality, quadratic, and nonlinear problems, as well as applications in machine learning and compressed sensing.  This success, however, has masked a prevailing weakness: all iterates generated must remain in the subspace.  An acceleration phase has the added flexibility of selectively enlarging the subspace, when deemed necessary.  In the final part of the project, they consider a new fast and robust active-set algorithm for solving complementarity problems.  Efficiency and reliability are acquired by (i) utilizing a special property of the Newton-like direction that holds in this setting to formulate an improved search procedure; and (ii) suggesting a simple framework that allows for acceleration phases to be incorporated naturally.<br/><br/>The new active-set framework, convergence results, and freely available software will have a large sphere of influence by aiding in solving challenging problems arising in the design of large complex systems.  In particular, they will serve as useful tools for the future design and development of new highly-efficient algorithms for solving large-scale real-world problems related to energy transmission, trajectory optimization, optimal control, contact problems in computational mechanics, regularized machine learning problems, and various equilibrium problems associated with traffic flow, optimal design, and the pricing of energy.  For example, their work will help answer questions such as ""How can we plan for energy transmission infrastructure that accounts for uncertainties such as the location and type of future energy generation, technology, policy, and economic development?""  The improved optimization tools provided by the principal investigator and his student will aid regulators and regional transmission organizations to develop more robust investment plans that may save the consumers millions of dollars every year."
"1150318","CAREER: High-Dimensional Variable Selection in Nonlinear Models and Classification with Correlated Data","DMS","STATISTICS","08/01/2012","06/13/2016","Yingying Fan","CA","University of Southern California","Continuing grant","Gabor J. Szekely","07/31/2017","$400,000.00","","fanyingy@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","MPS","1269","1045","$0.00","Estimation and prediction with large-scale data sets commonly arise in statistics and related fields and pose great challenges. To address these challenges, four interrelated research topics are proposed for investigation. First, the investigator proposes robust variable selection methods for heavy-tailed data in the ultra-high dimensional setting of dimensionality increasing exponentially with the sample size. To address the heavy-tailedness, regularization methods with robust losses and general penalty functions in various model settings are investigated. The risk properties of these methods are studied and the optimality of penalty function and loss function is characterized. Robust independence screening methods are also proposed and studied. Second, variable selection in high-dimensional functional regression models with functional predictors and/or functional response is investigated. Model fitting procedures are proposed and sampling properties of the proposed methods are thoroughly investigated. Third, the investigator studies the regularization parameter selection in penalized empirical risk minimization in both settings of correctly specified and misspecified models in ultra-high dimensions. The appropriate tradeoff between the model fitting and model complexity is characterized. This study also answers the question on whether conventional model selection criteria such as AIC and BIC continue to work in ultra-high dimensions.  Fourth, high-dimensional classification with correlated features is extensively studied under the unified framework of thresholding classification rules, and the optimal choice of threshold that minimizes the classification error is identified. The investigator studies Gaussian classification and generalizes the methods and results to the case of correlated discrete features.<br/><br/>Thanks to the advent of modern technologies such as the handwritten digital recognition and single-nucleotide polymorphism (SNP) genotyping experiments, massive data sets with a large number of variables are becoming more and more common in various scientific fields such as computational biology, economics, finance, machine learning, and climatology. How to effectively analyze these data sets poses great challenges in both methodology and computation that are not present in smaller scale studies. A major goal of this proposal is to propose new or extended methodologies and investigate their sampling properties in depth and width for high-dimensional model building and model evaluation in various settings of regression and classification problems. The PI has broad research interests in many fields outside statistics such as computational biology, finance, econometrics, and machine learning. The proposed methods will be tested on real data sets and extended to these different areas. In addition, the PI plans to develop software packages to implement the proposed methods, and make them publicly available. The proposed work will benefit a broad range of scientists and researchers in various fields. The PI also plans to integrate education activities with the proposed research, such as involving minority students, undergraduate students, and graduate students in the proposed projects and incorporating cutting-edge high-dimensional statistical methods into new courses."
"1161359","AF:Medium:Collaborative Research: Uncertainty Aware Geometric Computing","CCF","ALGORITHMIC FOUNDATIONS","09/01/2012","04/10/2015","Pankaj Agarwal","NC","Duke University","Continuing grant","Tracy J. Kimbrel","08/31/2016","$315,999.00","","pankaj@cs.duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","7796","7924, 7929, 9251","$0.00","Most scientific and engineering disciplines today have enormous opportunities for creation of knowledge from massive quantities of data available to them.  But the lack of appropriate algorithms and analysis tools for processing, organizing, and querying this data deluge makes this task extremely challenging.  A large portion of the data being acquired today has a geometric character, and even non-geometric data are often best analyzed by embedding them in a multi-dimensional feature space and exploiting the geometry of that space.  This data is invariably full of noise, inaccuracies, outliers, is often incomplete and approximate, yet most of the existing geometric algorithms are unable to cope with any data uncertainty in relating their output to their input.<br/><br/>The project aims to fill this void by investigating uncertainty-aware geometric computing, with an express goal of designing algorithmic techniques and foundations that will help extract ``knowledge'' from large quantities of geometric data in the presence of various non-idealities and uncertainties.  It focuses on a number of fundamental geometric problems, all dealing with uncertain data.  A unified set of models will be developed for modeling uncertainty that can deal with multiple uncertainty types, and attention will be paid to handling noise/outliers in heterogeneous and dynamic data.  Algorithms will be investigated for understanding how input uncertainty carries over to output uncertainty (e.g. by associating a confidence level or likelihood with each output, or computing certain statistics of the output) and how the input uncertainty impacts the quality of the output (e.g. by defining and computing the stability of the output in terms of the input uncertainty).  Since exact solutions are likely to be computationally infeasible, the emphasis will be on simple, efficient approximation techniques (e.g. computing a compact, approximate distribution of geometric/topological structures such as Delaunay triangulations and their subcomplexes of uncertain data).<br/><br/>A key ingredient of the award is to address a variety of computational issues that arise in the presence of uncertainty using a few key problems, and to develop a core set of techniques that illuminate algorithmic design under uncertainty not only on these key problems but that can also be transferred to other geometric problems, as needed.  This research touches upon many topics in theoretical computer science and applied mathematics including discrete and computational geometry, discrete and continuous optimization, estimation theory, and machine learning.  This study will strengthen connections of computational geometry with a variety of disciplines, including machine learning, probabilistic databases, statistics, and GIS.  Since so many problems require geometric data analysis, the project has the potential of enhancing the capability of various government, commercial, and civic units to make informed decisions that impact the society at large."
"1225204","Scalable Bayesian Inference in Large Medical Databases","OAC","CI Fellowships","09/01/2012","07/20/2012","Finale Doshi-Velez","MA","Doshi                   Finale         P","Fellowship","Sushil Prasad","08/31/2015","$240,000.00","","","","Cambridge","MA","021394309","","CSE","7696","9150","$0.00","This project will develop new efficient algorithms for large-scale nonparametric Bayesian inference in the context of medical prediction problems. The computational techniques will be applied to discover intermediary pathways, known as pathophenotypes, that can be used to predict cardiovascular disease expression and events based on electronic health record and genomic data. While many articles have discussed the promise of electronic health records to improve drug surveillance and efficacy, the advanced statistical methodologies needed to realize this promise requires innovation in efficient computational techniques specially designed for large datasets and modern computer architectures including distributed computing and dealing with streaming data. The contributions of the proposed work will result both in contributions to computer science and statistics as well as in direct impacts for safer, higher-quality healthcare. The work on scalable Bayesian inference will be of general interest to wide breadth of statistics, operations research, economics, and machine learning communities working on efficient inference techniques.<br/><br/>Each of the technical research objectives is inspired by problems in analyzing large medical data, and thus the proposed work will have direct implications for improved healthcare through more timely predictions of adverse drug events and patient outcomes (well-aligned with NSF?s mission to improve national health, prosperity and welfare). The fellowship will allow the researcher to build on her established teaching record through co-instructing a machine learning course at Harvard, supervising 1-2 masters? students on projects related to the proposed research, and high school outreach through the MIT Educational Studies Program."
"1217761","RI: Small: Agent-Assisted Trading in Real-World Auctions","IIS","Robust Intelligence","09/01/2012","02/01/2018","Amy Greenwald","RI","Brown University","Standard Grant","James Donlon","08/31/2018","$465,990.00","","amy@cs.brown.edu","BOX 1929","Providence","RI","029129002","4018632777","CSE","7495","7495, 7923, 9150, 9251","$0.00","In many large economic markets, goods are sold through auctions. Examples include eBay, wireless spectrum auctions, the New York Stock Exchange, and the Dutch flower auctions. This project focuses on one particular real-world auction domain: the Dutch flower auctions (DFA). The project will perform an in-depth analysis of this domain both from the point of view of the bidders and from the point of view of the auctioneers.<br/><br/>This project involves several component tasks. (1) Creation of a configurable auction server, comprised of basic auction building blocks, like simultaneous and sequential auctions, and sealed-bid, ascending, and descending auctions. (2) Construction of an interactive DFA simulation using the configurable auction server that is capable of handling decisions made by both human bidders and autonomous agents; this simulation will model DFA bidder preferences, which will be learned automatically using inverse reinforcement learning techniques from historical data and manually by surveying and interviewing DFA experts. (3) Creation of autonomous DFA bidding agents and comparison of their performance to that of expert human DFA bidders using the interactive simulation. (4) Further simulation of these agents to empirically find game-theoretic equilibria under various settings of the auction parameters so that those parameters can be optimized. The project will also develop a prototype of a mixed-initiative system that can be used to assist DFA bidders.<br/><br/>Potential broader impacts of this research include increased knowledge and understanding of how to design and implement artificially intelligent agents that can effectively assist humans with their decision-making efforts, particularly in information-rich and time-critical environments. Although this project focuses on DFA-type auctions, methodologies developed here will transfer to other domains that use auction mechanisms, such as automated bidding for resource allocation in smart grids. Detailed focus on the DFA provides a concrete starting point for the study of various auction-related topics, for instance, how to build mixed-initiative decision support tools to improve the quality of bidding practices, rigorous comparison of various auction mechanisms and parameter choices, and advancing bidding agent design from a heuristic approach to one that is theoretically grounded.<br/><br/>The project research team includes both graduate and undergraduate students whose participation in this research will strengthen their understanding of the various fields that are critical to the development of decision support systems (e.g., autonomous agents, preference elicitation and representation, machine learning, software engineering) and provide them experience collaborating across disciplines, and working on a real-world application. In addition, there are plans to develop introductory lessons on the general topic of autonomous bidding agents for use in the Artemis project, a five week summer program in which female rising ninth graders are exposed to the breadth of applications of computer science, and are introduced to a variety of technologies underlying computing."
"1207036","Numerical Simulations of Quantum Computers and Disordered Systems","DMR","CONDENSED MATTER & MAT THEORY","09/15/2012","05/27/2014","Allan Peter Young","CA","University of California-Santa Cruz","Continuing grant","Daryl W. Hess","08/31/2016","$330,000.00","","peter@physics.ucsc.edu","1156 High Street","Santa Cruz","CA","950641077","8314595278","MPS","1765","7203, 7569, 9161, AMPP","$0.00","TECHNICAL SUMMARY<br/><br/>This award supports theoretical research and education to study two fields where computer simulations can give important information: quantum computers and spin glasses.<br/><br/>It is well known that there are certain specialized problems which can be solved much more efficiently on a quantum computer than on a classical computer. The question addressed in this project is whether an eventual quantum computer could, in addition, solve a broad range of optimization problems more efficiently than a classical computer using the Quantum Adiabatic Algorithm.  Results already obtained by the PI for several optimization problems indicate that the simplest application of this algorithm does not give an improvement over classical algorithms for large problem sizes. The PI will therefore use the inherent flexibility of the quantum adiabatic algorithm to see if modifications to it give a significant improvement, and also apply the algorithm to other types of problem. In particular, the PI will investigate if quantum algorithms will be helpful in the field of machine learning.<br/><br/>The PI will also investigate several questions in the field of spin glasses, which are systems with disorder and frustration.  The study of spin glasses is important far beyond the rather narrow field of dilute magnetic alloys where they were first studied, because ideas developed for them have applicability to a wide range of complex systems, such as combinatorial optimization problems in computer science, protein folding in biology, and structural glasses (e.g. window glass).  Spin glasses are a convenient system in which to study this class of problems since they can be probed in fine detail in experiments by applying a magnetic field, and can be represented theoretically by simplified models which are amenable to computer simulation. Understanding spin glasses will help our understanding of these other problems as well. In particular, the PI will apply optimization methods developed using spin glass ideas to solve other optimization problems.<br/><br/>This award will support the education of students in developing state-of-the-art algorithms for large-scale numerical simulations.  The research will enable them to pursue scientific careers in many related fields and become part of a scientifically sophisticated workforce.  Recent students of the PI have gone on to apply the ideas learned under his supervision in both academia and industry. The PI will also continue to teach techniques used in his research as part of courses on computational physics, which are offered to both undergraduates and graduate students.<br/><br/>NON-TECHNICAL SUMMARY<br/><br/>This award supports theoretical research and education to study two fields where computer simulations can give important information: quantum computers and spin glasses <br/><br/>Information in a computer is stored as ""bits"" which take values 1 or 0. It has been proposed that certain problems could be solved more efficiently on a quantum computer in which the bits are replaced by ""qubits"" which follow the laws of quantum mechanics and can be simultaneously in states 1 and 0, which is called a superposition. So far, it has proved very difficult to build a useful quantum computer because a small amount of external noise destroys superposition. However, it is still of interest to study what problems could be solved efficiently on a quantum computer if and when a quantum computer can be built. The PI will study whether a particular broad class of problems, known as optimization problems, can be solved more efficiently on a quantum computer than on a classical computer. Since we do not have a quantum computer, the PI will emulate the behavior of a quantum computer by doing numerical simulations on a classical computer.<br/><br/>The PI will also study a class of systems called ""spin glasses"" which exhibit glassy behavior at low temperatures, that is, they do not come to equilibrium but are always evolving with time. Ideas developed for spin glasses have applicability to a wide range of complex systems, such as some optimization problems in computer science, protein folding in biology, and structural glasses (e.g. window glass).  Spin glasses are a convenient system in which to study this class of problems since they can be probed in fine detail in experiments, and can be represented theoretically by simplified models which are amenable to computer simulations. Understanding spin glasses will help our understanding of these other problems as well.<br/><br/>This award will support the education of students in developing state-of-the-art algorithms for large-scale numerical simulations.  The research will enable them to pursue scientific careers in many related fields and become part of a scientifically sophisticated workforce.  Recent students of the PI have gone on to apply the ideas learned under his supervision in both academia and industry. The PI will also continue to teach techniques used in his research as part of courses on computational physics, which are offered to both undergraduates and graduate students."
"1148085","Collaborative Research SI2 SSE: Pipeline Framework for Ensemble Runs on Clouds","OAC","Software Institutes","05/01/2012","05/07/2012","Craig Mattocks","FL","University of Miami Rosenstiel School of Marine&Atmospheric Sci","Standard Grant","Daniel Katz","04/30/2014","$199,725.00","Brian Soden","cmattock@rsmas.miami.edu","4600 RICKENBACKER CSWY","KEY BISCAYNE","FL","331491031","3054214089","CSE","8004","8004, 8005","$0.00","Cloud computing is an attractive computational resource for e-Science because of the ease with which cores can be accessed on demand, and because the virtual machine implementation that underlies cloud computing reduces the cost of porting a numeric or analysis code to a new platform. It is difficult to use cloud computing resources for large-scale, high throughput ensemble jobs however. Additionally, the computationally oriented researcher is increasingly encouraged to make data sets available to the broader community. For the latter to be achieved, using capture tools during experimentation to harvest metadata and provenance reduces the manual burden of marking up results. Better automatic capture of metadata and provenance is the only means by which sharing of scientific data can scale to meet the burgeoning explosion of data.<br/><br/>This project develops a pipeline framework for running ensemble simulations on the cloud; the framework has two key components: ensemble deployment and metadata harvest. Regarding the former, on commercial cloud platforms typically a much smaller number of jobs than desired can be started at any one time. An ensemble run will need to be pipelined to a cloud resource, that is, executed in well-controlled batches over a period of time. We will use platform features of Azure, and employ machine learning techniques to continuously refine the pipeline submission strategy and workflow strategies for ensemble parameter specification, pipelined deployment, and metadata capture. Regarding the latter key component, we expect to reduce the burden of sharing scientific datasets resulting from the use of cloud resources through automatic metadata and provenance capture and representation that aligns the metadata with emerging best practices in data sharing and discovery. Ensemble simulations result in complex data sets, whose reuse could be increased by expressive, granule and collection level metadata, including the lineage of the resulting products, to contribute towards trust.<br/><br/>In this project we focus on a compelling and timely application from climate research: One of the more immediate and dangerous impacts of climate change could be a change in the strength of storms that form over the oceans. In addition, as sea level rises due to global warming and melting of the polar ice caps, coastal communities will become increasingly vulnerable to storm surge. There have already been indications that even modest changes in ocean surface temperature can have a disproportionate effect on hurricane strength and the damage inflicted by these storms. In an effort to understand these impacts, modelers turn to predictions generated by hydrodynamic coastal ocean models such as the Sea, Lake and Overland Surges from Hurricanes (SLOSH) model. The proposed research advances the knowledge and understanding of probabilistic storm surge products by enhancements to the SLOSH model itself and through mechanisms that take advantage of commercial cloud resources. This knowledge is expected to have application in research, the classroom, and in operational settings.<br/><br/>The broader significance of the project is several-fold. Cloud computing is an important economic driver but it remains difficult for use in computationally driven scientific research. This project lowers the barriers to conducting e-Science research that utilizes cloud resources, specifically Azure. It will contribute tools to help researchers share, preserve, and publicize the scientific data sets that result from their research. Because we focus on and improve an application that predicts storm surge in response to sea level changes and severe storms, our work contributes to societal responses and adaptations to climate change, including planning and building the sustainable, hazard-resilient coastal communities of the future."
"1243868","I-Corps:  Zero-UI: Translation of Purdue Gesture-based Creative Interaction Technologies into the Real World","IIP","I-Corps","07/01/2012","06/19/2012","Karthik Ramani","IN","Purdue University","Standard Grant","Errol Arkilic","12/31/2012","$50,000.00","","ramani@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","ENG","8023","","$0.00","During the last 30 years, use of computers to initially make 2D drawings and sketches, and then to design 3D shapes, have lead to deep research and plethora of geometric and graphic design tools. An analysis of past work reveals that applications are limited by (1) use of 2D input devices such as mouse, keyboard, touch screen, etc. (2) and the limitations of corresponding procedural programs and user interfaces that map user input to computational procedures, and (3) inability to sense, feel and interact with these virtual data and shapes. The current state of unnatural user interactions severely limits the creative expression and manipulation of shapes. It urges us to develop new forms of human machine interaction to unleash the creativity of the mind's eye. The team's vision is to enable a new class of natural user interaction driven shape creation and manipulation paradigms. The team's approach will completely eliminate traditional user interfaces since users will be able to directly interact with 3D content using hand and body. This vision is based on a recent set of our discoveries, enabled by prior NSF support on natural sketch-based interfaces and shape analysis that resulted in the foundation for commercialization.<br/><br/>The science of natural user interfaces and interactions are at a very early stage of development. Highly-digitized user interfaces using windows, icons, mouse and pointer have resulted in overlooking the role of the user in creative interaction with virtual shapes. The team's work contributes and creates new ways in which users will interact with geometric data spaces. Areas of scientific inquiry we will contribute to include human-computer interaction, machine learning, gesture-based interaction, and ubiquitous computing. Gesture based natural user interfaces (NUI) enabled by 3D sensing cameras are the next frontier in computing, media and entertainment and other verticals. This work can result in new capabilities in edutainment and game design, training technologies, haptic interfaces for disabled, interactive advertising, personal manufacturing and many other applications not yet imagined."
"1148359","Collaborative Research SI2 SSE: Pipeline Framework for Ensemble Runs on Clouds","OAC","Software Institutes","05/01/2012","05/07/2012","Beth Plale","IN","Indiana University","Standard Grant","Daniel Katz","04/30/2014","$292,863.00","","plale@cs.indiana.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","CSE","8004","8004, 8005","$0.00","Cloud computing is an attractive computational resource for e-Science because of the ease with which cores can be accessed on demand, and because the virtual machine implementation that underlies cloud computing reduces the cost of porting a numeric or analysis code to a new platform. It is difficult to use cloud computing resources for large-scale, high throughput ensemble jobs however. Additionally, the computationally oriented researcher is increasingly encouraged to make data sets available to the broader community. For the latter to be achieved, using capture tools during experimentation to harvest metadata and provenance reduces the manual burden of marking up results. Better automatic capture of metadata and provenance is the only means by which sharing of scientific data can scale to meet the burgeoning explosion of data.<br/><br/>This project develops a pipeline framework for running ensemble simulations on the cloud; the framework has two key components: ensemble deployment and metadata harvest. Regarding the former, on commercial cloud platforms typically a much smaller number of jobs than desired can be started at any one time. An ensemble run will need to be pipelined to a cloud resource, that is, executed in well-controlled batches over a period of time. We will use platform features of Azure, and employ machine learning techniques to continuously refine the pipeline submission strategy and workflow strategies for ensemble parameter specification, pipelined deployment, and metadata capture. Regarding the latter key component, we expect to reduce the burden of sharing scientific datasets resulting from the use of cloud resources through automatic metadata and provenance capture and representation that aligns the metadata with emerging best practices in data sharing and discovery. Ensemble simulations result in complex data sets, whose reuse could be increased by expressive, granule and collection level metadata, including the lineage of the resulting products, to contribute towards trust.<br/><br/>In this project we focus on a compelling and timely application from climate research: One of the more immediate and dangerous impacts of climate change could be a change in the strength of storms that form over the oceans. In addition, as sea level rises due to global warming and melting of the polar ice caps, coastal communities will become increasingly vulnerable to storm surge. There have already been indications that even modest changes in ocean surface temperature can have a disproportionate effect on hurricane strength and the damage inflicted by these storms. In an effort to understand these impacts, modelers turn to predictions generated by hydrodynamic coastal ocean models such as the Sea, Lake and Overland Surges from Hurricanes (SLOSH) model. The proposed research advances the knowledge and understanding of probabilistic storm surge products by enhancements to the SLOSH model itself and through mechanisms that take advantage of commercial cloud resources. This knowledge is expected to have application in research, the classroom, and in operational settings.<br/><br/>The broader significance of the project is several-fold. Cloud computing is an important economic driver but it remains difficult for use in computationally driven scientific research. This project lowers the barriers to conducting e-Science research that utilizes cloud resources, specifically Azure. It will contribute tools to help researchers share, preserve, and publicize the scientific data sets that result from their research. Because we focus on and improve an application that predicts storm surge in response to sea level changes and severe storms, our work contributes to societal responses and adaptations to climate change, including planning and building the sustainable, hazard-resilient coastal communities of the future."
"1139161","Collaborative Research: Socially Assistive Robots","IIS","Information Technology Researc, HCC-Human-Centered Computing, Robust Intelligence, Expeditions in Computing","04/01/2012","03/15/2019","Pamela Hinds","CA","Stanford University","Continuing Grant","Ephraim Glinert","03/31/2020","$1,340,840.00","Daniel Schwartz","phinds@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","1640, 7367, 7495, 7723","1640, 7367, 7723, 9251","$0.00","Socially Assistive Robots<br/>Lead PI/Institution: Brian Scassellati, Yale University<br/>This Expedition will develop the fundamental computational techniques that will enable the design, implementation, and evaluation of robots that encourage social, emotional, and cognitive growth in children, including those with social or cognitive deficits.  The need for this technology is driven by critical societal problems that require sustained, personalized support that supplements the efforts of educators, parents, and clinicians.   For example, clinicians and families struggle to provide individualized educational services to children with social and cognitive deficits, whose numbers have quadrupled in the US in the last decade alone.  In many schools, educators struggle to provide language instruction for children raised in homes where a language other than English is spoken (over 20%), the fastest-growing segment of the school-age population.  This Expedition aims to support the individual needs of these children with socially assistive robots that help to guide the children toward long-term behavioral goals, that are customized to the particular needs of each child, and that develop and change as the child does.  <br/>To achieve this vision, this Expedition will advance the state-of-the-art in socially assistive human-robot interaction from short-term interactions in structured environments to long-term interactions that are adaptive, engaging, and effective. This progress will require transformative computing research in three broad and naturally interrelated research areas. First, the Expedition will develop computational models of the dynamics of social interaction, so that robots can automatically detect, analyze, and influence agency, intention, and other social interaction primitives in dynamic environments. Second, the Expedition will develop machine learning algorithms that adapt and personalize interactions to individual physical, social, and cognitive differences, enabling robots to teach and shape behavior in ways that are tailored to the needs, preferences, and capabilities of each individual. Third, the Expedition will develop systems that guide children toward specific learning goals over periods of weeks and months, allowing for truly long-term guidance and support. Research in these three areas will be integrated into socially assistive robots that are deployed in schools and homes for durations of up to one year.  <br/>This Expedition has the potential to substantially impact the effectiveness of education and healthcare for children, and the technological tools developed will serve as the basis for enhancing the lives of children and other groups that require specialized support and intervention. The proposed computing research is tied to a comprehensive student training program, bringing a compelling, engaging, and grounded STEM experience to K-12 students through in-school and after-school activities. It also establishes an annual training summit to provide undergraduates with the multi-disciplinary background to engage in this promising research area in graduate school. Finally, by establishing a brand name for socially assistive robotics, this effort will create a central authority for the distribution of high-quality, peer-reviewed information, providing a coherent focal point for enhancing outreach and education.<br/>For more information visit www.yale.edu/SAR"
"1139148","Collaborative Research: Socially Assistive Robots","IIS","Information Technology Researc, HCC-Human-Centered Computing, Robust Intelligence, Expeditions in Computing","04/01/2012","02/26/2020","Maja Mataric","CA","University of Southern California","Continuing Grant","Ephraim Glinert","03/31/2021","$2,583,000.00","Gisele Ragusa, Fei Sha, Donna Spruijt-Metz","mataric@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","1640, 7367, 7495, 7723","1640, 7367, 7723, 9251","$0.00","Socially Assistive Robots<br/>Lead PI/Institution: Brian Scassellati, Yale University<br/>This Expedition will develop the fundamental computational techniques that will enable the design, implementation, and evaluation of robots that encourage social, emotional, and cognitive growth in children, including those with social or cognitive deficits.  The need for this technology is driven by critical societal problems that require sustained, personalized support that supplements the efforts of educators, parents, and clinicians.   For example, clinicians and families struggle to provide individualized educational services to children with social and cognitive deficits, whose numbers have quadrupled in the US in the last decade alone.  In many schools, educators struggle to provide language instruction for children raised in homes where a language other than English is spoken (over 20%), the fastest-growing segment of the school-age population.  This Expedition aims to support the individual needs of these children with socially assistive robots that help to guide the children toward long-term behavioral goals, that are customized to the particular needs of each child, and that develop and change as the child does.  <br/>To achieve this vision, this Expedition will advance the state-of-the-art in socially assistive human-robot interaction from short-term interactions in structured environments to long-term interactions that are adaptive, engaging, and effective. This progress will require transformative computing research in three broad and naturally interrelated research areas. First, the Expedition will develop computational models of the dynamics of social interaction, so that robots can automatically detect, analyze, and influence agency, intention, and other social interaction primitives in dynamic environments. Second, the Expedition will develop machine learning algorithms that adapt and personalize interactions to individual physical, social, and cognitive differences, enabling robots to teach and shape behavior in ways that are tailored to the needs, preferences, and capabilities of each individual. Third, the Expedition will develop systems that guide children toward specific learning goals over periods of weeks and months, allowing for truly long-term guidance and support. Research in these three areas will be integrated into socially assistive robots that are deployed in schools and homes for durations of up to one year.  <br/>This Expedition has the potential to substantially impact the effectiveness of education and healthcare for children, and the technological tools developed will serve as the basis for enhancing the lives of children and other groups that require specialized support and intervention. The proposed computing research is tied to a comprehensive student training program, bringing a compelling, engaging, and grounded STEM experience to K-12 students through in-school and after-school activities. It also establishes an annual training summit to provide undergraduates with the multi-disciplinary background to engage in this promising research area in graduate school. Finally, by establishing a brand name for socially assistive robotics, this effort will create a central authority for the distribution of high-quality, peer-reviewed information, providing a coherent focal point for enhancing outreach and education.<br/>For more information visit www.yale.edu/SAR"
"1149344","CAREER: Addressing Fundamental Challenges for Wireless Coverage Service in the TV White Space","CNS","Special Projects - CNS","09/01/2012","06/25/2012","Zhenghao Zhang","FL","Florida State University","Standard Grant","Monisha Ghosh","08/31/2018","$450,883.00","","zzhang@cs.fsu.edu","874 Traditions Way, 3rd Floor","TALLAHASSEE","FL","323064166","8506445260","CSE","1714","1045","$0.00","This project addresses fundamental issues in the White-Cover network which provides data coverage services in the densely populated areas with the TV White Space spectrum. It is motivated by the fact that a White-Cover cell will likely host hundreds of users while supporting highly dynamic data traffic, a scenario beyond the capabilities of existing network protocols. In addition, the White-Cover devices will likely have to coexist in the same channel with heterogeneous devices due to the lack of free TV channels in the densely populated areas. These challenges are addressed by a combination of novel physical layer techniques and advanced machine learning techniques, and the solutions are enhanced by further optimizations. This project studies: 1) efficient physical layer schemes for node state detection with which the transmissions in the cell can be scheduled very efficiently, 2) advanced protocols that adapt to the heterogamous interferences conditions with the reinforcement learning technique, 3) optimized protocol for power consumption and integrated partial packet recovery, and 4) network-wide performance analysis and planning. <br/><br/>The White-Cover network designed in this project provides a better alternative to the current technologies for the TV White Space. Users may enjoy data coverage at much less cost than from the cellular phone networks and new business may be enabled. The expected results of this project include the protocols and algorithms optimized for the unique features of White-Cover and tested with experiments. The results will be disseminated in conferences and journals; the source code will be freely available online."
"1229597","MRI:  Acquisition of High Performance Compute Cluster for Multivariate Real-time and Whole-brain Correlation Analysis of fMRI Data","BCS","MAJOR RESEARCH INSTRUMENTATION","08/15/2012","08/20/2012","Jonathan Cohen","NJ","Princeton University","Standard Grant","John Yellen","07/31/2015","$527,978.00","Kai Li, Kenneth Norman, Nicholas Turk-Browne, Ray Lee","jdc@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","SBE","1189","1189","$0.00","This Major Research Instrumentation award permits Dr. Jonathan Cohen and four co-investigators to purchase a high-performance computing instrumentation (3,584 cores; 2TB/core; 100TB flash storage) to be used by faculty, postdocs, graduate students and undergraduates within the Princeton Neuroscience Institute (PNI).  The instrumentation will allow the analysis of human brain imaging data at a speed and scale not previously possible.<br/><br/>The collaborating researchers are cognitive neuroscientists and computer scientists at Princeton with complementary expertise in human brain imaging and large scale computing. Two primary research objectives are proposed, building on recent progress in applying multivariate pattern analysis (MVPA) methods from machine learning to detect neural signals that correspond to internal mental states, such as perceptions, memories and intentions that are otherwise not accessible to direct observation.  To date, use of MVPA has been restricted to the ""offline"" analyses"" after data have been fully collected.  However, a growing and powerful use of brain imaging is to give participants feedback about their brain states in real time, allowing them to use this information to better control brain function (e.g., providing feedback about pain areas as a way of learning to control chronic pain).  Such real-time feedback methods could be greatly enhanced by adding MVPA.  However, this has been computationally intractable until now.  Objective 1 addresses this challenge, by inserting a high performance computing system into the brain scanning pipeline.  This will be tested in an experiment that uses MVPA to detect patterns of brain activity associated with sustained attention, allowing us to provide real-time brain-based feedback to improve attentional abilities (with potential educational and health benefits).<br/><br/>Objective 2 focuses on another major advance in brain imaging, in which correlations between areas of activity are analyzed, rather than areas of activity in isolation of one another.  Such correlations - often referred to as ""functional connectivity"" - are likely to reveal more about how the brain actually functions, by providing critical information about the interactions between areas.  At present, virtually all approaches to functional connectivity focus on the correlations among a limited set of brain areas of interest. However, a more powerful approach would be to examine the correlation of every area with all others.  This requires computing the whole-brain correlation matrix. The analysis of such high dimensional data would be further enhanced by applying MVPA to patterns of correlation.  However, doing this further increases computational demands. Applying this approach to a routine brain imaging dataset, using currently available instrumentation, would take 880 years to complete.  The work under Objective 2 addresses this challenge, by coupling massively parallel computing with sophisticated software optimizations. Doing so can bring previously intractable problems into the range of practicality.  These  methods will be tested in an experiment that seeks to identify neural representations of intentions, and their influence on brain mechanisms responsible for executing these intentions."
"1219150","SHF: Small: Collaborative Research: Lighthouse: Resource-Aware Advisor for High-Performance Linear Algebra","CCF","SOFTWARE & HARDWARE FOUNDATION","09/01/2012","09/11/2013","Boyana Norris","IL","University of Chicago","Standard Grant","Tao Li","08/31/2015","$250,000.00","","norris@cs.uoregon.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","CSE","7798","7923, 7942","$0.00","This research will study ways to ease the production and<br/>use of high-performance matrix algebra software. Matrix algebra<br/>calculations constitute the most time-consuming part of simulations<br/>in diverse fields, and lowering the runtimes of those computations<br/>can have a significant impact on overall application performance. The<br/>process of converting matrix algebra from algorithm to high-quality<br/>implementation is, however, a complex one. At each step, the code<br/>developer is confronted with a myriad of possibilities, many requiring<br/>expertise in numerical computation, mathematical software, compilers,<br/>and computer architecture. In response to these difficulties, the PIs<br/>have developed a prototype taxonomy implementation entitled Lighthouse,<br/>which is a guide to the linear system solver routines from the software<br/>package LAPACK. It is the first framework that combines a matrix algebra<br/>software ontology with code generation and tuning capabilities. Its<br/>interface is designed for users across a spectrum of disciplines, career<br/>levels, and programming experience.<br/><br/>The PIs will dramatically extend the<br/>Lighthouse framework in a number of new directions. First, they will<br/>construct a general taxonomy of software that can be used to build<br/>highly-optimized mathematical applications. The taxonomy will initially<br/>provide an organized ontology of software components for high-performance<br/>matrix algebra and later other numerical software from a variety of<br/>problem domains. It will serve as a guide to practitioners seeking<br/>to learn what is available for their mathematical programming tasks,<br/>how to use it, and how the various parts fit together. Second, the PIs<br/>will apply a combination of source code analysis and machine learning<br/>techniques to fully automate the generation of parameterized models that,<br/>given representative inputs and a simple architecture description, can be<br/>evaluated to identify methods from various libraries that best reflect<br/>the user's resource and performance requirements. This automation is<br/>critical for ensuring that the taxonomy is comprehensive enough to be<br/>useful and that it accurately reflects the features and performance<br/>of the latest versions of numerical libraries. Finally, the PIs will<br/>advance the state-of-the-art in tuning tools by improving some of the<br/>tools included in the taxonomy, broadening their ranges of functionality<br/>in terms of problem domains and languages. This project will produce<br/>the following impacts: greater performance by applications, enabling<br/>both more discovery with available computing resources and greater<br/>productivity of application programmers; greater understanding of the<br/>interaction between architecture and algorithms; and an educational tool<br/>for future computational scientists."
"1139078","Collaborative Research:  Socially Assistive Robots","IIS","Robust Intelligence, Expeditions in Computing","04/01/2012","09/18/2015","Brian Scassellati","CT","Yale University","Continuing Grant","Ephraim Glinert","03/31/2018","$4,025,000.00","Fred Volkmar, John Morrell, Frederick Shic, Aaron Dollar, Rhea Paul","brian.scassellati@yale.edu","Office of Sponsored Projects","New Haven","CT","065208327","2037854689","CSE","7495, 7723","7723","$0.00","Socially Assistive Robots<br/>Lead PI/Institution: Brian Scassellati, Yale University<br/>This Expedition will develop the fundamental computational techniques that will enable the design, implementation, and evaluation of robots that encourage social, emotional, and cognitive growth in children, including those with social or cognitive deficits.  The need for this technology is driven by critical societal problems that require sustained, personalized support that supplements the efforts of educators, parents, and clinicians.   For example, clinicians and families struggle to provide individualized educational services to children with social and cognitive deficits, whose numbers have quadrupled in the US in the last decade alone.  In many schools, educators struggle to provide language instruction for children raised in homes where a language other than English is spoken (over 20%), the fastest-growing segment of the school-age population.  This Expedition aims to support the individual needs of these children with socially assistive robots that help to guide the children toward long-term behavioral goals, that are customized to the particular needs of each child, and that develop and change as the child does.  <br/>To achieve this vision, this Expedition will advance the state-of-the-art in socially assistive human-robot interaction from short-term interactions in structured environments to long-term interactions that are adaptive, engaging, and effective. This progress will require transformative computing research in three broad and naturally interrelated research areas. First, the Expedition will develop computational models of the dynamics of social interaction, so that robots can automatically detect, analyze, and influence agency, intention, and other social interaction primitives in dynamic environments. Second, the Expedition will develop machine learning algorithms that adapt and personalize interactions to individual physical, social, and cognitive differences, enabling robots to teach and shape behavior in ways that are tailored to the needs, preferences, and capabilities of each individual. Third, the Expedition will develop systems that guide children toward specific learning goals over periods of weeks and months, allowing for truly long-term guidance and support. Research in these three areas will be integrated into socially assistive robots that are deployed in schools and homes for durations of up to one year.  <br/>This Expedition has the potential to substantially impact the effectiveness of education and healthcare for children, and the technological tools developed will serve as the basis for enhancing the lives of children and other groups that require specialized support and intervention. The proposed computing research is tied to a comprehensive student training program, bringing a compelling, engaging, and grounded STEM experience to K-12 students through in-school and after-school activities. It also establishes an annual training summit to provide undergraduates with the multi-disciplinary background to engage in this promising research area in graduate school. Finally, by establishing a brand name for socially assistive robotics, this effort will create a central authority for the distribution of high-quality, peer-reviewed information, providing a coherent focal point for enhancing outreach and education.<br/>For more information visit www.yale.edu/SAR"
"1138986","Collaborative Research: Socially Assistive Robots","IIS","INFORMATION TECHNOLOGY RESEARC, EXPERIMENTAL EXPEDITIONS","04/01/2012","09/18/2015","Cynthia Breazeal","MA","Massachusetts Institute of Technology","Continuing grant","Ephraim P. Glinert","03/31/2018","$2,075,000.00","Mitchel Resnick","cynthiab@media.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","1640, 7723","1640, 7723","$0.00","Socially Assistive Robots<br/>Lead PI/Institution: Brian Scassellati, Yale University<br/>This Expedition will develop the fundamental computational techniques that will enable the design, implementation, and evaluation of robots that encourage social, emotional, and cognitive growth in children, including those with social or cognitive deficits.  The need for this technology is driven by critical societal problems that require sustained, personalized support that supplements the efforts of educators, parents, and clinicians.   For example, clinicians and families struggle to provide individualized educational services to children with social and cognitive deficits, whose numbers have quadrupled in the US in the last decade alone.  In many schools, educators struggle to provide language instruction for children raised in homes where a language other than English is spoken (over 20%), the fastest-growing segment of the school-age population.  This Expedition aims to support the individual needs of these children with socially assistive robots that help to guide the children toward long-term behavioral goals, that are customized to the particular needs of each child, and that develop and change as the child does.  <br/>To achieve this vision, this Expedition will advance the state-of-the-art in socially assistive human-robot interaction from short-term interactions in structured environments to long-term interactions that are adaptive, engaging, and effective. This progress will require transformative computing research in three broad and naturally interrelated research areas. First, the Expedition will develop computational models of the dynamics of social interaction, so that robots can automatically detect, analyze, and influence agency, intention, and other social interaction primitives in dynamic environments. Second, the Expedition will develop machine learning algorithms that adapt and personalize interactions to individual physical, social, and cognitive differences, enabling robots to teach and shape behavior in ways that are tailored to the needs, preferences, and capabilities of each individual. Third, the Expedition will develop systems that guide children toward specific learning goals over periods of weeks and months, allowing for truly long-term guidance and support. Research in these three areas will be integrated into socially assistive robots that are deployed in schools and homes for durations of up to one year.  <br/>This Expedition has the potential to substantially impact the effectiveness of education and healthcare for children, and the technological tools developed will serve as the basis for enhancing the lives of children and other groups that require specialized support and intervention. The proposed computing research is tied to a comprehensive student training program, bringing a compelling, engaging, and grounded STEM experience to K-12 students through in-school and after-school activities. It also establishes an annual training summit to provide undergraduates with the multi-disciplinary background to engage in this promising research area in graduate school. Finally, by establishing a brand name for socially assistive robotics, this effort will create a central authority for the distribution of high-quality, peer-reviewed information, providing a coherent focal point for enhancing outreach and education.<br/>For more information visit www.yale.edu/SAR"
"1162270","CSR: Medium: Collaborative Research: Architecting Performance Sensitive Applications for the Cloud","CNS","Computer Systems Research (CSR","08/15/2012","07/28/2015","T. S. Eugene Ng","TX","William Marsh Rice University","Continuing grant","M. Mimi McClure","07/31/2016","$400,000.00","","eugeneng@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","CSE","7354","7354, 7924","$0.00","Cloud computing offers IT organizations the ability to create<br/>geo-distributed, and highly scalable applications while providing<br/>attractive cost-saving advantages. Yet, architecting, configuring, and<br/>adapting cloud applications (latency-sensitive web applications and<br/>bulk data processing applications) to meet their stringent performance<br/>requirements is a challenge given the rich set of configuration<br/>options, shared multi-tenant nature of cloud platforms, and dynamics<br/>resulting from activities such as planned maintenance.<br/><br/>This project is developing novel methodologies, algorithms, and<br/>systems that can enable application architects to (1) judiciously<br/>architect applications across multiple cloud data-centers while<br/>considering application performance requirements, cost saving<br/>objectives, and cloud pricing schemes guided by performance and cost<br/>models of cloud components; (2) automatically learn effective<br/>application configurations and configuration-to-performance prediction<br/>models through statistical machine learning techniques; and (3) create<br/>applications that can adapt to ongoing dynamics in cloud environments<br/>through transaction reassignment over shorter time-scales, and<br/>application migration over longer time-scales.<br/><br/>The impact of this research is multi-fold: (1) Enable IT organizations<br/>to significantly reduce costs by optimally moving their operations to<br/>the cloud; (2) create benchmarks based on operationally deployed<br/>applications and collecting workload traces which will be made<br/>available to the research community; (3) make developed algorithms and<br/>systems widely available as open source software; (4) inform the<br/>design of a nation-wide health-care cloud in Thailand; (4) introduce<br/>cloud computing related topics in the undergraduate and graduate<br/>curriculum; and (6) train multiple Ph.D., M.S., and undergraduate<br/>students, with explicit effort to recruit and train students from<br/>under-represented minority groups."
"1162333","CSR: Medium: Collaborative Research: Architecting Performance Sensitive Applications for the Cloud","CNS","SPECIAL PROJECTS - CISE, Computer Systems Research (CSR","08/15/2012","09/12/2014","Sanjay Rao","IN","Purdue University","Continuing grant","M. Mimi McClure","07/31/2017","$400,000.00","Mohit Tawarmalani","sanjay@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","1714, 7354","7354, 7924","$0.00","Cloud computing offers IT organizations the ability to create<br/>geo-distributed, and highly scalable applications while providing<br/>attractive cost-saving advantages. Yet, architecting, configuring, and<br/>adapting cloud applications (latency-sensitive web applications and<br/>bulk data processing applications) to meet their stringent performance<br/>requirements is a challenge given the rich set of configuration<br/>options, shared multi-tenant nature of cloud platforms, and dynamics<br/>resulting from activities such as planned maintenance.<br/><br/>This project is developing novel methodologies, algorithms, and<br/>systems that can enable application architects to (1) judiciously<br/>architect applications across multiple cloud data-centers while<br/>considering application performance requirements, cost saving<br/>objectives, and cloud pricing schemes guided by performance and cost<br/>models of cloud components; (2) automatically learn effective<br/>application configurations and configuration-to-performance prediction<br/>models through statistical machine learning techniques; and (3) create<br/>applications that can adapt to ongoing dynamics in cloud environments<br/>through transaction reassignment over shorter time-scales, and<br/>application migration over longer time-scales.<br/><br/>The impact of this research is multi-fold: (1) Enable IT organizations<br/>to significantly reduce costs by optimally moving their operations to<br/>the cloud; (2) create benchmarks based on operationally deployed<br/>applications and collecting workload traces which will be made<br/>available to the research community; (3) make developed algorithms and<br/>systems widely available as open source software; (4) inform the<br/>design of a nation-wide health-care cloud in Thailand; (4) introduce<br/>cloud computing related topics in the undergraduate and graduate<br/>curriculum; and (6) train multiple Ph.D., M.S., and undergraduate<br/>students, with explicit effort to recruit and train students from<br/>under-represented minority groups."
"1240178","EAGER: Construction of Inter-Igbo","IIS","Robust Intelligence","07/01/2012","07/30/2012","Eugene Charniak","RI","Brown University","Standard Grant","Tatiana Korelsky","06/30/2014","$91,000.00","","ec@cs.brown.edu","BOX 1929","Providence","RI","029129002","4018632777","CSE","7495","7495, 7916, 9150, 9251","$0.00","Igbo is a language spoken by twenty million people, mostly in southern Nigeria.  This EAGER project makes use of a corpus of spoken Igbo, which will cover all of the dialects of the language.  The corpus is to be used for explorations in which statistical machine learning (ML) programs are created to learn an ""inter-Igbo'' consisting cognate sets (Igbo words pronounced differently in different locations but having the same meaning) that enable the corpus to be treated as if it were spoken as a single language, even though the dialects are, at extreme ends of the Igbo homeland, mutually unintelligible.  Another aspect of our work is the extension of the existing corpus to fill in gaps in dialect coverage where there currently are recordings from locations that have no near geographical neighbors.  The need for this stems from the fact that the closer a dialect's neighbors the more similar they are, and the easier for programs to locate words which differ systematically.<br/><br/>Achieving goals of this exploratory project is of considerable interest for computational linguistics.  As opposed to language change over time, there is little computational work on language change over geography, and finding the appropriate ML models for the latter aspect of language variation is a considerable challenge."
"1161997","III: Medium: Hardware/Software Accelerated Data Mining for Real-Time Monitoring of Streaming Pediatric ICU Data","IIS","Info Integration & Informatics, IIS Special Projects","07/01/2012","05/30/2014","Eamonn Keogh","CA","University of California-Riverside","Continuing grant","Sylvia Spengler","06/30/2017","$1,199,822.00","Walid Najjar, Vassilis Tsotras","eamonn@cs.ucr.edu","Research & Economic Development","RIVERSIDE","CA","925210217","9518275535","CSE","7364, 7484","7364, 7924","$0.00","On any given day in America, there are at least one thousand children fighting for their lives in Pediatric Intensive Care Units (PICUs). In the PICU the patient's condition is carefully monitored with automatic sensors. Most of this data is shown in a five-minute ""sliding window"" display, so a doctor summoned to a patient's bedside always has her most recent history to consider. However what happens to the data that ""falls off"" this sliding window? In most PICUs, a tiny fraction of it is coarsely aggregated and recorded, but surprisingly, most of this data is simply discarded.  Even if most or all the data is recorded, its sheer volume simply overwhelms researchers and analysts; very few tools exist to help them make sense of and learn from this data. This currently discarded data is a potential goldmine of actionable knowledge that could improve outcomes (decreased mortality/morbidity, reduce pain, etc.), and reduce costs (implicit in reduced length of stay).  However, the very nature of this data - multivariate, heterogeneous, high dimensional, temporal, noisy, biased, and high frequency - poses significant challenges for traditional analytical techniques from statistics and data mining.<br/><br/>In this project, an interdisciplinary team of investigators is developing: (a) xcalable machine learning algorithms for mining archives of annotated PICU data to find regularities and patterns that can be used to aid in diagnostics and prediction of outcomes; and (b) techniques for monitoring ICU telemetry in real time to detect whether the patterns and rules discovered in the offline step have occurred and can be used to guide interventions (actions by the doctor).<br/><br/>The project brings together experts in data mining (Keogh, Tsotras), high performance computing (Najjar), and medicine (Wetzel) to investigate holistic solutions to the above problems. The project contributes to research-based advanced training of graduate and undergraduate students at the University of California Riverside. The findings, datasets, software, and teaching materials created by this project will be archived in perpetuity at www.cs.ucr.edu/~eamonn/UCRPICU/"
"1218734","SHF: Small: Collaborative Research: Adaptive Automatic Parallelization","CCF","Special Projects - CCF","10/01/2012","09/04/2012","John Cavazos","DE","University of Delaware","Standard Grant","Anindya Banerjee","09/30/2015","$249,998.00","","cavazos@cis.udel.edu","210 Hullihen Hall","Newark","DE","197160099","3028312136","CSE","2878","7329, 7433, 7923, 9150","$0.00","To effectively exploit the power of multi-core processors, programs must be structured as a collection of independent tasks, where separate tasks execute on independent cores.  The complexity of modern software makes it difficult for programmers to express their algorithms within this model, both due to the amount of program analysis needed to identify regions of code that can run in parallel, and the likelihood that different regions of code will be best suited by distinct, and possibly incompatible, models of parallel computing.  In particular, some codes are best parallelized through speculative techniques, while others favor regular analysis, such as that provided by the polyhedral approach.<br/><br/>The proposed research addresses fundamental issues in the creation of parallel programs through a novel combination of automatic and profile-driven techniques.  The heart of the research is a robust system based on machine learning, through which a compilation tool can analyze a program, assess the suitability of a variety of parallelization techniques to that program, and then apply the most promising techniques automatically.  At run-time, the program will also employ learning to adapt its behavior according to inputs and environment.  Furthermore, the programmer will be given a profile-driven feedback mechanism, in order to guide the tool to refine its parallelization of the program, and guide the program's self-tuning behavior.  In conjunction with the creation of this system, new algorithms and tools for speculative parallelization and large-scale program analysis will be invented. Prototypes and source code will be distributed as open-source software."
"1218530","SHF:Small:Collaborative Research:Adaptive Automatic Parallelization","CCF","Special Projects - CCF","10/01/2012","09/04/2012","Michael Spear","PA","Lehigh University","Standard Grant","Anindya Banerjee","09/30/2015","$247,793.00","","spear@cse.lehigh.edu","Alumni Building 27","Bethlehem","PA","180153005","6107583021","CSE","2878","7329, 7433, 7923","$0.00","To effectively exploit the power of multi-core processors, programs must be structured as a collection of independent tasks, where separate tasks execute on independent cores.  The complexity of modern software makes it difficult for programmers to express their algorithms within this model, both due to the amount of program analysis needed to identify regions of code that can run in parallel, and the likelihood that different regions of code will be best suited by distinct, and possibly incompatible, models of parallel computing.  In particular, some codes are best parallelized through speculative techniques, while others favor regular analysis, such as that provided by the polyhedral approach.<br/><br/>The proposed research addresses fundamental issues in the creation of parallel programs through a novel combination of automatic and profile-driven techniques.  The heart of the research is a robust system based on machine learning, through which a compilation tool can analyze a program, assess the suitability of a variety of parallelization techniques to that program, and then apply the most promising techniques automatically.  At run-time, the program will also employ learning to adapt its behavior according to inputs and environment.  Furthermore, the programmer will be given a profile-driven feedback mechanism, in order to guide the tool to refine its parallelization of the program, and guide the program's self-tuning behavior.  In conjunction with the creation of this system, new algorithms and tools for speculative parallelization and large-scale program analysis will be invented. Prototypes and source code will be distributed as open-source software."
"1219089","SHF: Small: Collaborative Research: Lighthouse: Resource-Aware Advisor for High-Performance Linear Algebra","CCF","SOFTWARE & HARDWARE FOUNDATION","09/01/2012","04/03/2014","Elizabeth Jessup","CO","University of Colorado at Boulder","Standard Grant","Nina Amla","08/31/2016","$263,000.00","","jessup@cs.colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","7798","7923, 7942, 9178, 9251","$0.00","This research will study ways to ease the production and<br/>use of high-performance matrix algebra software. Matrix algebra<br/>calculations constitute the most time-consuming part of simulations<br/>in diverse fields, and lowering the runtimes of those computations<br/>can have a significant impact on overall application performance. The<br/>process of converting matrix algebra from algorithm to high-quality<br/>implementation is, however, a complex one. At each step, the code<br/>developer is confronted with a myriad of possibilities, many requiring<br/>expertise in numerical computation, mathematical software, compilers,<br/>and computer architecture. In response to these difficulties, the PIs<br/>have developed a prototype taxonomy implementation entitled Lighthouse,<br/>which is a guide to the linear system solver routines from the software<br/>package LAPACK. It is the first framework that combines a matrix algebra<br/>software ontology with code generation and tuning capabilities. Its<br/>interface is designed for users across a spectrum of disciplines, career<br/>levels, and programming experience.<br/><br/>The PIs will dramatically extend the Lighthouse framework in a number of new directions. <br/>First, they will construct a general taxonomy of software that can be used to build<br/>highly-optimized mathematical applications. The taxonomy will initially<br/>provide an organized ontology of software components for high-performance<br/>matrix algebra and later other numerical software from a variety of<br/>problem domains. It will serve as a guide to practitioners seeking<br/>to learn what is available for their mathematical programming tasks,<br/>how to use it, and how the various parts fit together. Second, the PIs<br/>will apply a combination of source code analysis and machine learning<br/>techniques to fully automate the generation of parameterized models that,<br/>given representative inputs and a simple architecture description, can be<br/>evaluated to identify methods from various libraries that best reflect<br/>the user's resource and performance requirements. This automation is<br/>critical for ensuring that the taxonomy is comprehensive enough to be<br/>useful and that it accurately reflects the features and performance<br/>of the latest versions of numerical libraries. Finally, the PIs will<br/>advance the state-of-the-art in tuning tools by improving some of the<br/>tools included in the taxonomy, broadening their ranges of functionality<br/>in terms of problem domains and languages. This project will produce<br/>the following impacts: greater performance by applications, enabling<br/>both more discovery with available computing resources and greater<br/>productivity of application programmers; greater understanding of the<br/>interaction between architecture and algorithms; and an educational tool<br/>for future computational scientists."
"1208687","NRI-Small: Improved safety and reliability of robotic systems by faults/anomalies detection from uninterpreted signals of computation graphs","IIS","National Robotics Initiative","10/01/2012","09/30/2013","Andrea Censi","CA","California Institute of Technology","Standard Grant","Ralph Wachter","12/31/2013","$860,000.00","Richard Murray, Andrea Censi","censi@MIT.EDU","1200 E California Blvd","PASADENA","CA","911250600","6263956219","CSE","8013","7923, 8086","$0.00","One of the main challenges to designing robots that can operate around humans is to create systems that can guarantee safety and effectiveness, while being robust to the nuisances of unstructured environments, from hardware faults to software issues, erroneous calibration, and less predictable anomalies, such as tampering and sabotage. However, the fact that the streams of observations and commands possess coherence properties suggests that many of these disturbances could be detected and automatically mitigated with general methods that imply very low design efforts.  Currently, robotic systems are developed as a set of components realizing a directed ""computation graph"". This project focuses on theoretical methods, applicable designs, and reference implementation of a faults/anomalies detection mechanism for low-level  robotic sensorimotor signals. The system, without any prior information about the robot configuration, should learn a model of the robot and the environment by passive observations of the signals exposed in the computation graph, and, based on this model, instantiate faults/anomalies detection components in an augmented computation graph.<br/><br/>The project engages undergraduate and graduate students in advanced robotics design and development.  It is expected the research results will have a significant impact on future robotic systems and machine learning."
"1303350","CAREER: The Dynamics of Collective Intelligence","IIS","Robust Intelligence","11/01/2012","07/19/2013","Sanmay Das","VA","Virginia Polytechnic Institute and State University","Continuing Grant","James Donlon","01/31/2014","$181,344.00","","sanmay@wustl.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","CSE","7495","1045, 1187","$0.00","This project studies the design of information systems like wikis and information markets. Research in social science has established that often there is a ""wisdom of the crowd"" -- i.e., collectives can display more intelligence than the individuals they are composed of. When such collective information systems work, they serve as superb aggregators and disseminators of information. However, fundamental computational challenges remain in understanding how to design them optimally.<br/><br/>This research is advancing along several lines, including<br/><br/>(1) general theories of how information is aggregated in different social media, developed and validated using real data gathered from existing databases and generated from user experiments; <br/><br/>(2) algorithms for facilitation of user interactions so that the medium in question can deliver the promised results (for example, market-making algorithms for liquidity provision in information markets);<br/><br/>(3) theoretical and practical characterization of the possibilities for rogue users to manipulate collective wisdom systems;<br/><br/>(4) algorithms for detecting malicious users, and mechanisms that thwart miscreants. <br/><br/>The research is naturally interdisciplinary in nature, drawing from machine learning and probabilistic reasoning, data mining and social networks, as well as finance and economics. It contributes to our understanding of complex social phenomena like the growth of information in wikis and blogs, as well as to the development of intelligent reasoning algorithms for agents in complex, uncertain multi-agent environments like markets.<br/><br/>The design of agents that participate in markets and social systems improves the quality of online markets and improves information flow in virtual spaces. Further, insights gained from modeling market structures and social spaces can tell us how to design them better. For example, understanding the impact of different levels of central control on wiki articles or open source software projects yields guidelines for how much central control is optimal in different settings.<br/><br/>In a world where computation and social systems are increasingly intertwined, the PI's research and education program exposes students to multidisciplinary ideas through the introduction of a new class on collective intelligence, social networks and e-commerce, and the development and extensive use of the very objects of study -- information markets and wikis -- in classroom and lab settings. The PI is also developing an experimental project for putting freely accessible course wikis online, similar to online course materials at other universities, but open to editing by the community."
"1160915","Practical Algorithms for Applied Submodular Optimization","CMMI","OPERATIONS RESEARCH","05/01/2012","02/29/2012","Jonathan Lee","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Donald Hearn","04/30/2016","$260,000.00","","jonxlee@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","ENG","5514","072E, 073E, 077E","$0.00","This award provides funding for the development and analysis of practical exact and approximation algorithms for general submodular optimization problems, by generalizing and extending mathematical-optimization and approximation methods that have been successful in specific application areas. Important application areas include redesigning environmental monitoring networks and Boolean quadratic optimization. Thus far, general techniques for submodular optimization have been developed with the aim of providing provably good worst-case behavior. On the other hand, practical algorithms, not encumbered by theoretical requirements, have been developed and implemented for several special cases. This project is aimed at extending such success to the general case.<br/><br/>Algorithms will be instantiated and distributed as open-source software thus providing practical, general-purpose tools for attacking this ubiquitous problem class. With particular special cases defined through a black box (i.e., a function-evaluation subroutine), such software will have broad applicability across a variety of application areas, such as economics, machine learning, biodiversity conservation and statics. Moreover, the project is within the broader area of nonlinear discrete optimization, and it is natural to expect to have a broader influence within this important current topic."
"1250171","CDS&E: Data Management and Visualization in Petascale Turbulent Combustion Simulation","CBET","CDS&E","09/15/2012","08/24/2015","Peyman Givi","PA","University of Pittsburgh","Standard Grant","Ron Joslin","08/31/2017","$500,000.00","Georgeta-Elisab Marai, Server Yilmaz, Panos Chrysanthis, Alexandros Labrinidis","peg10@pitt.edu","300 Murdoch Building","Pittsburgh","PA","152603203","4126247400","ENG","8084","064E, 7433","$0.00","CBET-1250171<br/>PI: Peyman Givi, Univ. of Pittsburgh<br/><br/>This award provides funding for developments of (1) a robust computational method for petascale simulation<br/>of turbulent combustion, (2) a scalable data management system for such simulations and (3) a systematic<br/>visual analysis of the generated data. The computational method will be based on the filtered density function<br/>(FDF) methodology for large eddy simulation (LES) of complex turbulent flames. The simulation will<br/>be based on a new algorithm, termed ?irregularly portioned Lagrangian Monte Carlo-Finite Difference?<br/>(IPLMCFD) which facilitate FDF simulations on massively parallel, up to petascale platforms. Data management<br/>will be provided by addressing research challenges in management of annotations, management<br/>of workflows and data archiving. Effective data visualization and analysis will be conducted through a<br/>machine learning ?feature-extraction? approach. This work crosses the disciplines of engineering and computer<br/>science and expands the state-of-the-art in high fidelity predictions of turbulent reacting flows. At the<br/>conclusion of the work, an open source LES code will be provided for use by the public.<br/><br/>If successful, the results of this research will have a significant impact in combustion, both in gas-turbine<br/>industry and in government. It is firmly believed that LES will constitute the primary means of predictions<br/>for future design and manufacturing of combustion systems. Having it coupled with robust and versatile data<br/>management and visualization capabilities will be useful for both basic and applied research purposes. Some<br/>of the other broader impacts are through involvement of undergraduate students in research and attracting<br/>them to graduate school, K-12 outreach, involvement of high school students in research, and recruitment<br/>of students from minority and under-represented groups."
"1242525","I-Corps:  ExpressionBlast","IIP","I-Corps","07/01/2012","07/02/2012","Ziv Bar-Joseph","PA","Carnegie-Mellon University","Standard Grant","Rathindra DasGupta","12/31/2013","$50,000.00","","zivbj@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","ENG","8023","","$0.00","New experimental methods for collecting high throughput data are revolutionizing biology and clinical studies and are now routinely used for pre-clinical drug discovery. Several large public and proprietary databases collect these types of data, however this data is largely unstructured and is difficult to utilize.  The proposed effort is developed as a search engine for genomic data aimed at pharmaceutical companies, biotechnology companies, academic institutes, and medical centers allowing them to utilize large volumes of condition-specific data from public repositories, and also integrate it with proprietary, in-house data. The framework automatically downloads, parses, and annotates data from different repositories and is complemented with an easy-to-use web interface. The team created a large collection of automatically-annotated data and the platform offers search capabilities within and across species as well as additional advanced analysis options. These can provide new validation for current experimental results as well as new research directions. In particular, drug discovery is usually conducted on lower mammals before it is applied to human, hence the ability to easily perform cross species comparisons can reduce drug development cost and time. <br/><br/>The proposed technology addresses the classic problem of dealing with heterogeneity in unstructured data and integrating massive amounts of data from different sources into a seamless framework. The unique challenge here is a function of the domain (biological and pre-clinical data). The team's goal is to create a system that manages heterogeneity in more than a single aspect and provides vertical integration that allows the data to be searchable and comparable on many levels. This integration is made possible through the use of computational text mining and machine learning methods that are able to derive high quality information from the free text in order to automatically categorize and annotate the large volumes of data. This work also provides a holistic approach for the incorporation of new analysis tools for genomic data, offering standard services and benchmarks that can significantly shorten development time and increase usage. The ability to easily query large volumes of genomic data can facilitate basic research of cell processes by academic researchers and the discovery of new drugs or repurposing of old drugs by pharmaceutical companies. In addition, large medical centers are starting to collect genomics and genetics data for individual patients aiming to provide personalized medicine tailored specifically to each individual. The ability to compare results of an individual patient to a large collection of patients and their clinical records is a key to finding better suited treatments for that individual leading to reduced hospitalization time and fewer complications. Lastly, the software and methods created here are intended to be reusable for any science moving from individual lab practices to a shared, global collaboratory system.  If successfully deployed, this technology has the potential to make a significant impact across a wide span of the health care industry."
"1313810","Collaborative Research: Biological Shape Spaces, Transforming Shape into Knowledge","DBI","ADVANCES IN BIO INFORMATICS, Cross-BIO Activities","09/01/2012","06/24/2013","Cindy Grimm","OR","Oregon State University","Standard Grant","Anne Maglia","08/31/2014","$135,850.00","","grimmc@onid.orst.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","BIO","1165, 7275","1719, 1729, 7969, 9183, 9184, 9251","$0.00","Collaborative Research: Biological Shape Spaces, Transforming Shape into Knowledge<br/><br/>This project will develop a framework to represent, analyze and interpret shapes extracted from images, supporting a wide range of biological investigations. The primary objectives are: (1) to develop a mathematical framework and computational tools for the quantification and analysis of shapes; (2) to integrate these computational models with machine learning and statistical inference methods to enable new discoveries, transforming imaging data into biological knowledge; (3) to deliver novel quantitative methodologies for shape analysis that start from a biological premise, rather than a purely geometric one. The aim is thus not only to quantitatively describe shape, but to develop methods for linking morphological variation to its underlying biological causes.  To ensure that the project focuses on methods that are most promising to biology with significant breadth of application, model and tool development will be guided and supported by a set of diverse case studies, ranging from the sub-cellular to organismal scales. <br/><br/>Shape represents a complex and rich source of biological information that is fundamentally linked to underlying mechanisms and function. However, shape is still often examined on a qualitative basis in many disciplines in biology, an approach that is time consuming and prone to human subjectivity. While ad hoc quantitative methods do exist, they are often inaccessible to non-experts and do not easily generalize to a wide variety of problems. The inability of biologists to systematically link shape to genetics, development, environment, function and evolution often precludes advances in biological research spanning diverse spatial and temporal scales, from the movement of molecules within a cell to adaptive changes in organismal morphology. The primary goal of this project is to develop a new suite of widely applicable quantitative methods and tools into the study of biological shape to address the significant need for consistent and repeatable analysis of shape data."
"1216011","ICES: Small: Collaborative Research: Data-driven mechanisms in healthcare","CCF","Inter Com Sci Econ Soc S (ICE)","09/01/2012","08/22/2012","Mohsen Bayati","CA","Stanford University","Standard Grant","Tracy Kimbrel","08/31/2015","$199,997.00","","bayati@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","8052","7923","$0.00","The main objective of the project is to develop and study new incentive and reimbursement mechanisms within the context of the delivery of medical care. The project takes a game-theoretic perspective, with the main players being the government, health plans, healthcare providers, and patients. Some of the design goals of the mechanisms are to motivate the parties involved to improve healthcare quality while controlling costs. Among others, the study will try to extend the algorithmic game theory framework to address the unique challenges that occur in the context of healthcare. A key goal of the study is understanding the extent to which utilizing electronically available medical records may enable new mechanisms in the context of healthcare. In particular, the project studies potential advantages of high quality risk-adjustment schemes that are empowered by the application of machine learning to medical data.<br/><br/>In recent years, much of the focus in the US public policy debate has been given to problems surrounding providing and sustaining healthcare services. One of the key problems the US healthcare system faces is that of aligning incentives among the various stakeholders - the government, health plans, care providers, and patients - to ensure favorable outcomes and efficient resource utilization. The study will use tools from algorithms and game theory to develop new alignment strategies. In particular, the study will look at ways in which the collection and data-mining of electronic health records may be used to provide better incentives for the stakeholders, and lead to better overall care at lower cost."
"1212372","AF: Large: Collaborative Research: Exploiting Duality between Algorithms and Complexity","CCF","Algorithmic Foundations","07/01/2012","07/01/2015","Ryan Williams","CA","Stanford University","Continuing grant","Tracy Kimbrel","06/30/2016","$450,000.00","","rrw@mit.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","7796","7925, 7926, 7927","$0.00","Meta-algorithms are algorithms that take other algorithms as input. <br/>Meta-algorithms are important in a variety of applications, from <br/>minimizing circuits in VLSI to verifying hardware and software to <br/>machine learning. Lower bound proofs show that computational problems <br/>are difficult in the sense of requiring a prohibitive <br/>amount of time, memory, or other resource to solve. <br/>This is particularly important in the context of cryptography, <br/>where it is vital to ensure that no feasible adversary can break <br/>a code. Surprisingly, recent research by the PIs and others <br/>shows that designing meta-algorithms is, in a formal sense, <br/>equivalent to proving lower bounds. In other words, one can prove a <br/>negative (the non-existence of a small circuit to solve a problem) by <br/>a positive (devising a new meta-algorithm). This was the key to a <br/>breakthrough by PI Williams, proving lower bounds on constant <br/>depth circuits with modular arithmetic gates. <br/><br/>The proposed research will utilize this connection both to <br/>design new meta-algorithms and to prove new lower bounds. <br/>A primary focus will be on meta-algorithms for <br/>deciding if a given algorithm is 'trivial' or not, such as algorithms <br/>for the Boolean satisfiability problem. The proposed research will devise new <br/>algorithms that improve over exhaustive search for many variants <br/>of satisfiability. On the other hand, it will also explore <br/>complexity-theoretic limitations on how much improvement is <br/>possible, using reductions and lower bounds for restricted <br/>models. Satisfiability will provide a starting point for a more <br/>general understanding of the exact complexities of other NP-complete <br/>problems such as the traveling salesman problem and k-colorability. <br/>The proposal addresses both worst-case performance and the use <br/>of fast algorithms as heuristics for solving this problem. <br/><br/>This exploration will be mainly mathematical. However, when <br/>new algorithms and heuristics are developed, they will be <br/>implemented and the resulting software made widely available. <br/>This research will be incorporated in courses taught by <br/>the PI's, at both graduate and undergraduate levels. <br/>Both graduate and undergraduate students will perform research <br/>as part of the project."
"1262126","REU Site: Bug Wars: A Collaborative Software Testing Research Experience for Undergraduates","CCF","RSCH EXPER FOR UNDERGRAD SITES","09/01/2012","05/15/2014","Renee Bryce","TX","University of North Texas","Standard Grant","Anindya Banerjee","02/29/2016","$300,699.00","","Renee.Bryce@unt.edu","1155 Union Circle #305250","Denton","TX","762035017","9405653940","CSE","1139","9150, 9250","$0.00","This funding establishes a new CISE Research Experiences for Undergraduates<br/>(REU) site at Utah State University.  The project, called<br/>Bug Wars, exposes students to research on software testing and AI<br/>planning through both competition and collaboration.  This project will<br/>create new knowledge about user-session-based testing, model-based<br/>testing with AI planning, and the combination of these two techniques<br/>as applied to web applications.   A novel feature of this REU is that<br/>it encourages both competition and collaboration. The students initially<br/>split into two teams that strive to find the most faults in web application<br/>systems under test. One team collects, reduces, and prioritizes<br/>user-session-based test suites. A second team uses machine learning to<br/>build models of the software and AI planning to generate test suites.<br/>Students compete to show the merits of their approach on the same systems<br/>by considering the sizes and fault detection effectiveness of their<br/>test suites.  The students then critically discuss their work and propose<br/>combining the different approaches to further improve effectiveness.  <br/><br/>The broader impacts of this research are that twenty-four students over a<br/>three-year period have the opportunity to participate in a supportive<br/>environment that encourages them to pursue graduate studies in Computer<br/>Science. The students are better prepared for graduate school as they gain<br/>basic research skills, including the formulation of research questions,<br/>design of experiments, critical evaluation, and written and oral<br/>communication."
"1217281","III: Small: Topical Positioning System (TPS) for Informed Reading of Web Pages","IIS","Info Integration & Informatics","10/01/2012","09/06/2012","James Allan","MA","University of Massachusetts Amherst","Standard Grant","Maria Zemankova","09/30/2017","$499,752.00","","allan@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","7364","7923","$0.00","This work addresses the challenge of increasing the critical literacy of people looking for information on the Web, including information regarding healthcare, policy, or any other broadly discussed topic. The proposed research on Topical Positioning System ""TPS"" drives the vision of developing a browser tool that shows a person whether the web page in front of them discusses a provocative topic, whether the material is presented in a heavily biased way, whether it represents an outlier (fringe) idea, and how its discussion of issues relates to the broader context and to information presented in ""familiar"" sources. This research applies and extends text analysis and comparison techniques to this problem. It uses statistical language modeling, topic modeling, machine learning, and link analysis techniques to represent Web pages and clusters of Web pages. It requires both off-line pre-processing to organize web-scale collections and on-line, query-time fine-tuning of the organization for presentation in a TPS browser add-on. <br/><br/>This research will be the foundation of class projects as well as graduate student research, exposing a large number of students to issues of web-based search and critical evaluation of information. More importantly, however, this work has the potential to impact many people, helping them make more informed choices in response to what they read on the Web and elsewhere. The results of this project will be published in peer-reviewed venues and listed at the project Web site (http://ciir.cs.umass.edu/research/tps).  A freely available TPS browser add-on will be available at this Web site."
"1217408","CSR: SMALL: Automatically Detecting, Diagnosing and Resolving Abnormal Battery Drain Issues on Smartphone Systems","CNS","CSR-Computer Systems Research","09/01/2012","08/25/2012","Yuanyuan Zhou","CA","University of California-San Diego","Standard Grant","Marilyn McClure","08/31/2015","$350,000.00","Lawrence Saul","yyzhou@cs.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7354","7923, 9102","$0.00","This project presents a software system to have the smartphone itself deal with the abnormal battery drain (ABD) issues that are caused by 'battery bugs' in smartphone applications or system software as well as by battery-related configuration errors and environmental changes. The proposed system architecture contains four subsystems, namely information collection, data analysis, diagnosis, and resolution, to self-detect, self-diagnose, and self-recover with little user intervention if possible when ABD events happen. Various technical methods, including machine learning and statistical approaches, will be investigated to achieve the design goal."
"1247581","BIGDATA: Mid-Scale: DA: Analytical Approaches to Massive Data Computation with Applications to Genomics","IIS","Information Technology Researc, Big Data Science &Engineering, ","10/01/2012","09/19/2012","Eli Upfal","RI","Brown University","Standard Grant","Sylvia Spengler","09/30/2018","$1,566,685.00","Benjamin Raphael, Fabio Vandin","Eliezer_Upfal@Brown.edu","BOX 1929","Providence","RI","029129002","4018632777","CSE","1640, 8083, L218","170E, 7433, 7924, 8083, 9150","$0.00","The goal of this project is to design and test mathematically well-founded algorithmic and statistical techniques for analyzing large scale, heterogeneous and noisy data. The proposed research is transformative in its emphasis on rigorous analytical evaluation of algorithms' performance and statistical measures of output uncertainty, in contrast to the primarily heuristic approaches currently used in data mining and machine learning.  Any progress in that direction will have significant contribution to the reliability and scientific impact of massive data analysis. This project is motivated by the challenges in analyzing molecular biology data. Molecular biology provides an excellent source of data for testing advanced data analysis techniques: specifically, DNA/RNA sequence data repositories are growing at a super-exponential rate. The data is typically large and noisy, and in some cases includes both genotype and phenotype features that permits experimental validation of the analysis.  However, the methods and techniques developed in this project will be broadly applicable to other scientific communities that process massive multi-variant data sets. <br/><br/>The major technical goals of the project include: (1) Design efficient algorithms that provide guarantees on the output when the data comes from independent random samples from an unknown distribution. (2) Develop techniques for estimating the minimum number of samples required to test hypothesis of varying complexity in large datasets, building on techniques in computational statistics. (3) Design algorithms to analyze data on graphs that represent interactions between samples or features in the dataset.  These data may be static (e.g. mutations on interacting genes represented by a protein interaction network) or dynamic (e.g. information dissemination on a social network).<br/><br/>This project will advocate a responsible approach to data analysis, based on well-founded mathematical and statistical concepts. The capacity building activities of the project include:  (1) Creation and dissemination of algorithms and software that implement rigorous computational and statistical approaches to big data analysis.  (2) Educational initiatives at the graduate and undergraduate level to build a bigger workforce of data scientists with the appropriate foundational skills both to apply analytical tools to existing datasets and to develop new approaches to future datasets.  The proposed work will be tested on extensive cancer genome data, contributing to health IT, one of the National Priority Domain Areas."
"1161196","AF: Medium: Collaborative Research: Sparse Approximation: Theory and Extensions","CCF","Algorithmic Foundations","07/01/2012","05/02/2012","Atri Rudra","NY","SUNY at Buffalo","Standard Grant","Rahul Shah","06/30/2016","$305,467.00","Hung Ngo","atri@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","7796","7924, 7926, 7933","$0.00","In the past ten years the theoretical computer science, applied math and electrical engineering communities have extensively studied variants of the problem of ``solving"" an under-determined linear system.  One common mathematical feature that allows us to solve these problems is sparsity; roughly speaking, as long as the unknown vector does not contain too many non-zero components (or has a few dominating components), we can ``solve'' the under-determined system for the unknown vector.  These problems are referred to as sparse approximation problems and have applications in diverse areas such as signal and image processing, biology, imaging, tomography, machine learning and others.<br/>The proposed research project aims to develop a comprehensive, rigorous theory of sparse approximation, broadly defined.  The research proposal entails two complementary research directions: <br/>(1) a robust and more complete view of the combinatorial, algorithmic, and complexity-theoretic foundations of sparse approximations (including its generalization to functional sparse approximation where we want to ``solve"" for some function of the unknown vector instead of the vector itself),<br/>(2) coupled with either its interactions or direct applications in other areas of theoretical computer science, from complexity theory to coding theory, and of electrical engineering, from signal processing to analog-to-digital converters.<br/>A general theory of sparse approximation that concentrates both on the optimal tradeoffs between competing parameters and the computational feasibility of attaining such tradeoffs will not only help explore the theoretical limits and possibilities of sparse approximations, but also feed algorithmic techniques and theoretical benchmarks back to its application areas.  Sparse approximation already has been shown to have impact in a variety of fields, including imaging and signal processing, Internet traffic analysis, and design of experiments in biology and drug design."
"1302755","GV: Small: Collaborative Research: Supporting Knowledge Discovery through a Scientific Visualization Language","IIS","GRAPHICS & VISUALIZATION, EPSCoR Co-Funding","08/31/2012","05/07/2013","Jian Chen","MD","University of Maryland Baltimore County","Standard Grant","Maria Zemankova","10/31/2015","$173,593.00","","chen.8028@osu.edu","1000 Hilltop Circle","Baltimore","MD","212500002","4104553140","CSE","7453, 9150","7453, 7923, 9150, 9251","$0.00","This collaborative research brings together computer scientists from University of Maryland Baltimore County  (UMBC) and Brown University and neuroscientists from the University of Mississippi Medical Center (UMMC) to study the design of a scientific visualization language (SVL). Despite the numerous visualization approaches already devised, visualization remains more of an art than a science. Grounded in theories and methods from human-centered computing, machine learning, and cognitive psychology, this work is to develop and evaluate a scientific visualization language (SVL) to provide a principled way to help scientists understand how and why visualizations work. Tools and theories developed in this project can lead to efficient knowledge discovery to help neuroscientists study brains using diffusion tensor magnetic resonance imaging (DTI).<br/><br/>This work has the following specific objectives and outcomes: (1) close collaboration with scientists to discover, refine, and verify a symbol space, (2) a semantic space that describes the relationship among symbols, (3) a testbed that implements SVL for neuroscientists to compose visualizations, (4) development of new and enhanced courses at University of Southern Mississippi and Brown University, and (5) wide dissemination of the research outcomes through open-source software, experimental data, open labs, publications, and presentations.<br/><br/>This project is expected to have broad impact. It may lead to significantly better approaches to human knowledge discovery and decision making in many disciplines where visualizations have found successful application, including neuroscience, biomedicine, bioinformatics, biology, chemistry, geosciences, business, economics, and education. Undergraduate and graduate students are expected to participate in the research through our courses, and student exchanges are planned between USM and Brown. K-12 students can visit the USM lab while the project is in progress. Software and results will be disseminated via the project Web site (https://sites.google.com/site/simplevisualizationlanguage)."
"1149811","CAREER:  Exact and Approximate Algorithms for 3D Structure Modeling of Protein-Protein Interactions","CCF","CAREER: FACULTY EARLY CAR DEV, ALGORITHMIC FOUNDATIONS","07/01/2012","07/11/2016","Jinbo Xu","IL","Toyota Technological Institute at Chicago","Continuing grant","Mitra Basu","06/30/2018","$499,992.00","","j3xu@ttic.edu","6045 S Kenwood Ave","Chicago","IL","606372803","7738340409","CSE","1045, 7796","1045, 7931","$0.00","Protein-protein interactions (PPIs) play fundamental roles in all biological processes including the maintenance of cellular integrity, metabolism, transcription/translation, and cell-cell communication. High-throughput experimental approaches have been developed to systematically identify PPIs. However, these methods cannot produce atomic 3D models of PPIs, which hinder studying PPI molecular mechanisms at atomic level and understanding cellular processes at molecular level. Atomic structures of PPIs are also important for rational drug design. High-resolution methods for PPI 3D structure determination such as X-ray or NMR are time-consuming and sometimes technically challenging, so computational method is urgently needed for PPI structure modeling. <br/><br/>Intellectual Merit: This proposal studies exact/approximate algorithms for 3D structure modeling of PPIs, with the ultimate goal to enrich large-scale PPI networks with high-resolution 3D structure models. The proposal will study 1) simultaneous threading of all sequences of a target PPI to a complex template; 2) protein complex side-chain packing with a very large rotamer library and more realistic energy functions; and 3) simultaneous interface threading and side-chain packing to align distantly-related protein complexes. This proposal will apply several elegant and powerful techniques such as graph minor theory, probabilistic graphical models, dual relaxation and decomposition, which are not well-known in the field, to understanding the mathematical structure of the problem with more realistic and challenging settings and designing efficient algorithms. <br/><br/>The expected outcome includes theoretical analysis of protein interfaces and complexes by graph theory, efficient algorithms for PPI structure modeling and publicly available software and servers. The resulting software can be used to verify experimental PPIs and even predict novel PPIs missed by experimental approaches. The software will benefit a broad range of biological and biomedical applications, such as gene functional annotation, better understanding of disease processes, design of novel diagnostics and drugs, personalized medicine and even bio-energy development. The resulting algorithms and software will be communicated to the broader community and also be further developed and disseminated to industry by two companies.<br/><br/>Broader Impact: This work is expected to enrich and disseminate knowledge on systems biology and structure bioinformatics, machine learning, graph theory and optimization and further enrich the pedagogical literature. Contributions from this work to computer science are: understanding of protein graphs using graph minor theory and graph transformations and solving several computationally challenging problems by combining techniques from graph theory and continuous optimization. This research work will train minority students from two HBCU schools, future K-12 science teachers and students attending the first online bioinformatics program in Illinois. Students will receive training at the intersection of biology and computer science. The proposed course materials and book chapters will be freely available to the public."
"1148823","CAREER:   Image information extraction from heterogeneous populations of co-cultured cells","DBI","ADVANCES IN BIO INFORMATICS, CLB-Non-Career","03/01/2012","02/08/2016","Anne Carpenter","MA","Broad Institute, Inc.","Continuing grant","Jennifer Weller","02/28/2017","$850,352.00","","anne@broadinstitute.org","415 Main Street","Cambridge","MA","021421401","6177147000","BIO","1165, 9108","1045, 1165, CL10","$0.00","An award is made to the Broad Institute to identify and validate automated image analysis approaches to extract information from fluorescence microscopy images of co-cultured cell systems, while educating students, scientists, and the public about the theory, practice, and societal impact of biological image analysis. Biologists increasingly use co-culture systems, where two or more cell types are grown together in order to more accurately model their native environment. In order to use these powerful co-culture systems to tackle a wide range of basic biological research questions, the remaining challenge is to accurately extract quantitative measurements from each cell in microscopy images of such co-cultures, given that cell types with diverse morphologies are present. In close collaboration with researchers using co-culture systems, and building on the PI?s successful work in biological image analysis, this CAREER project integrates image analysis research and education. <br/><br/>The project will give the scientific community a validated, open-source software toolbox of image processing and machine learning algorithms readily usable by biologists. The education and outreach efforts of the project will produce validated, engaging educational materials that can be freely implemented by high school teachers around the world. More information about the project can be found at: http://www.broadinstitute.org/~anne/Carpenter_NSF_CAREER.html"
"1150062","CAREER: Metric Geometry Techniques for Approximation Algorithms","CCF","ALGORITHMIC FOUNDATIONS","07/01/2012","07/19/2016","Yury Makarychev","IL","Toyota Technological Institute at Chicago","Continuing grant","Tracy Kimbrel","06/30/2018","$499,988.00","","yury@ttic.edu","6045 S Kenwood Ave","Chicago","IL","606372803","7738340409","CSE","7796","1045, 7926, 7927","$0.00","Combinatorial optimization problems are of great importance to numerous applications. They arise in operations research, machine learning, VLSI design, computational biology and many other areas. Many optimization problems however are NP-hard and thus cannot be solved exactly in polynomial time unless P=NP. It is natural therefore to look for approximation algorithms, efficient algorithms that find only approximate solutions. Design and analysis of approximation algorithms is a very active research area. Various approaches for solving combinatorial optimization problems have appeared in the last three decades. However, despite significant progress, many important problems are still open. <br/><br/>This research project aims to advance the application of metric geometry techniques for solving combinatorial optimization problems, investigate new methods for designing approximation algorithms, and develop tools for analyzing the performance of approximation algorithms on real-life instances. This research project will be both theoretically important and practically relevant and it will lead to development of approximation algorithms for important applied problems that occur in many fields of science and engineering. Specifically, the PI will work on the following problems.<br/><br/>* Traditionally most research has focused on analyzing the worst case performance of approximation algorithms. However, practitioners observe that instances of combinatorial optimization problems that arise in practice are often not as hard as worst case instances. The PI will study semi-random models for various combinatorial optimization problems, and develop approximation algorithms that perform well on semi-random instances. This can perhaps explain what we see in practice.<br/><br/>* Recent research in the hardness of approximation initiated by Khot identified Unique Games Problem as a combinatorial obstacle to the development of approximation algorithms for many problems. The PI will study algorithmic techniques for solving the Unique Games Problem.<br/><br/>* The PI will study lift-and-project hierarchies of linear programming (LP) and semi-definite programming (SDP) relaxations, analyze their integrality gaps, and design subexponential approximation algorithms that use these hierarchies.<br/><br/>* There is a close connection between some areas of theoretical computer science and pure mathematics. To settle down some problems in combinatorial optimization, we need to resolve closely connected open problems in analysis. The PI will explore several problems with deep ties to computer science and mathematics."
"1215990","ICES: Small:  Collaborative Research:  Data-driven mechanisms in healthcare","CCF","Inter Com Sci Econ Soc S (ICE)","09/01/2012","08/22/2012","Mark Braverman","NJ","Princeton University","Standard Grant","Tracy Kimbrel","08/31/2015","$183,298.00","","mbraverm@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","8052","7923, 7932","$0.00","The main objective of the project is to develop and study new incentive and reimbursement mechanisms within the context of the delivery of medical care. The project takes a game-theoretic perspective, with the main players being the government, health plans, healthcare providers, and patients. Some of the design goals of the mechanisms are to motivate the parties involved to improve healthcare quality while controlling costs. Among others, the study will try to extend the algorithmic game theory framework to address the unique challenges that occur in the context of healthcare. A key goal of the study is understanding the extent to which utilizing electronically available medical records may enable new mechanisms in the context of healthcare. In particular, the project studies potential advantages of high quality risk-adjustment schemes that are empowered by the application of machine learning to medical data.<br/><br/>In recent years, much of the focus in the US public policy debate has been given to problems surrounding providing and sustaining healthcare services. One of the key problems the US healthcare system faces is that of aligning incentives among the various stakeholders - the government, health plans, care providers, and patients - to ensure favorable outcomes and efficient resource utilization. The study will use tools from algorithms and game theory to develop new alignment strategies. In particular, the study will look at ways in which the collection and data-mining of electronic health records may be used to provide better incentives for the stakeholders, and lead to better overall care at lower cost."
"1156487","REU Site: Bug Wars: A Collaborative Software Testing Research Experience for Undergraduates","CCF","RSCH EXPER FOR UNDERGRAD SITES","03/01/2012","02/23/2012","Renee Bryce","UT","Utah State University","Standard Grant","John Reppy","10/31/2012","$315,990.00","Daniel Bryce","Renee.Bryce@unt.edu","Sponsored Programs Office","Logan","UT","843221415","4357971226","CSE","1139","9150, 9250","$0.00","This funding establishes a new CISE Research Experiences for Undergraduates<br/>(REU) site at Utah State University.  The project, called<br/>Bug Wars, exposes students to research on software testing and AI<br/>planning through both competition and collaboration.  This project will<br/>create new knowledge about user-session-based testing, model-based<br/>testing with AI planning, and the combination of these two techniques<br/>as applied to web applications.   A novel feature of this REU is that<br/>it encourages both competition and collaboration. The students initially<br/>split into two teams that strive to find the most faults in web application<br/>systems under test. One team collects, reduces, and prioritizes<br/>user-session-based test suites. A second team uses machine learning to<br/>build models of the software and AI planning to generate test suites.<br/>Students compete to show the merits of their approach on the same systems<br/>by considering the sizes and fault detection effectiveness of their<br/>test suites.  The students then critically discuss their work and propose<br/>combining the different approaches to further improve effectiveness.  <br/><br/>The broader impacts of this research are that twenty-four students over a<br/>three-year period have the opportunity to participate in a supportive<br/>environment that encourages them to pursue graduate studies in Computer<br/>Science. The students are better prepared for graduate school as they gain<br/>basic research skills, including the formulation of research questions,<br/>design of experiments, critical evaluation, and written and oral<br/>communication."
"1228271","Solving large-scale eigen-related problems: Efficient and scalable algorithms","DMS","CDS&E-MSS, CDS&E","09/01/2012","08/29/2012","Yunkai Zhou","TX","Southern Methodist University","Standard Grant","Andrew D. Pollington","08/31/2015","$155,771.00","","yzhou@smu.edu","6425 BOAZ","Dallas","TX","752750302","2147682030","MPS","8069, 8084","9263","$0.00","The main objectives of this project focus on novel efficient algorithms for solving large-scale eigen-related problems. The term ""eigen-related"" refers to any problem for which a solution can be expressed using certain eigenvalues and/or eigenvectors. Such eigen-related problems are central to a wide range of scientific and engineering disciplines, including materials science, statistical computing, data mining and machine learning. The ever-increasing high dimensions and complexity of modern scientific problems bring overwhelming computational burden for eigen-related computations and render existing eigen-algorithms ineffective. There is great demand for eigen-algorithms that can take into account the dynamic and nonlinear nature of modern scientific problems, leading to algorithms that can handle massive datasets more efficiently.  The investigator proposes to systematically study and develop novel scalable approaches for alleviating the computational bottlenecks related to solving large-scale eigen-related problems. Approaches to be investigated include global spectrum filtering as well as multilevel sampling and clustering. Specifically, the investigator will develop methods (a) to reduce the diagonalization cost for density functional theory calculations; (b) to accelerate partial SVD calculations that are used in a significantly broad range of applications; and (c)to accelerate the low-rank matrix approximations via Nystrom-type methods.<br/><br/><br/>Large eigen-related problems are ubiquitous in modern science, engineering, and economics applications. This project will develop efficient scalable algorithms for solving large eigen-related problems.  One important broader impact of the proposed work is that the resulting methods will accelerate discoveries in a wide range of research fields, including material science and data mining, which are critical to national energy security and economic competitiveness. The project will also provide interesting and challenging topics for Ph.D. dissertation research, for undergraduate research, and for outreach activities."
"1152716","SBIR Phase II: Acoustoelastic Tissue Property Evaluation of Selected Tissue Region in Dynamic Ultrasound Images","IIP","SMALL BUSINESS PHASE II","04/15/2012","03/18/2015","Jeffrey Dalsin","WI","Echometrix, LLC","Standard Grant","Ruth M. Shuman","03/31/2015","$592,631.00","","jdalsin@echo-metrix.com","437 S. Yellowstone Drive","MADISON","WI","537190000","7089558058","ENG","5373","169E, 5373, 8038","$0.00","This Small Business Innovation Research (SBIR) Phase II project proposes to develop a real-time ultrasound system for evaluating musculoskeletal soft tissue conditions by implementing the novel ultrasound post-processing software developed in Phase I into a programmable platform ultrasound system. Today, radiologists diagnose most musculoskeletal diseases by observing static MRI or conventional ultrasound images and considering key factors that support only qualitative, subjective assessments. Developing an efficient, real-time, quantitative method for diagnosing soft tissue (e.g., tendons and ligaments) injuries and monitoring healing can lead to more accurate diagnoses and reduce re-injury of incompletely healed tissues. The project will enhance the novel software technology's clinical utility and workflow efficiency. The original software will be enhanced by improving the software to automatically detect a region of interest with the ultrasound image. The registered regions of interest can be matched precisely from one patient visit to the next. Developing data mining software will further increase efficiency and accuracy by leveraging machine learning to assist with diagnostic decisions. These software improvements will be integrated with the platform ultrasound system to improve clinical workflow. The integrated product will both match the work flow efficiency of standard ultrasound and dramatically advance the utility of ultrasound within the musculoskeletal arena. <br/><br/>The broader impact/commercial potential of this project, if successful, will dramatically improve clinicians' ability to care for soft tissue injuries and will position the company to capitalize on (1) pressure to reduce medical imaging costs, (2) musculoskeletal specialists' growing interest in ultrasound, especially portable instruments, (3) a major ultrasound manufacturer's focus on the large and relatively untapped musculoskeletal ultrasound market, and (4) the recent emergence of quantitative ultrasound for non-musculoskeletal applications. This Phase II project will produce an efficient, real-time, quantitative method for diagnosing soft tissue injuries and monitoring healing. In the US alone, overuse injuries (strains, sprains) are the most frequently reported musculoskeletal injuries. Annually, 18.4 million such injuries cost approximately $92 B. Patients suffering from musculoskeletal injuries currently face three challenges at diagnosis, care, and outcome. First, current diagnostic methods, including MRI, ultrasound, or physical manipulation, rely on highly subjective and observer-dependent interpretation, so accuracy varies. Second, MRI is still the standard of care, but is far more costly than ultrasound. Substituting ultrasound for MRI, where appropriate for initially diagnosing musculoskeletal conditions, could save Medicare $736 M/year. Third, the lack of an objective monitoring method to determine when a patient can safely return to activity means patients risk missing unnecessary work time or re-injury."
"1204347","II-NEW: Infrastructure for Change: From a Teaching Department to National Prominence","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","07/01/2012","06/29/2012","Ophir Frieder","DC","Georgetown University","Standard Grant","Maria Zemankova","06/30/2015","$460,000.00","Marcus Maloof, Thomas Shields, Nazli Goharian, Micah Sherr","of22@georgetown.edu","37th & O St N W","Washington","DC","200571789","2026250100","CSE","7359","7359","$0.00","Over the last several years, Georgetown University has espoused a<br/>policy of growth for its science departments.  This project supports<br/>Georgetown University's growth goals by positing a multifaceted<br/>research program that explores new techniques for processing large<br/>collections of text data.<br/><br/>The project examines how both new and existing text processing methods<br/>may be applied to a variety of open problems across different<br/>disciplines within computer science.  Research activities include text<br/>mining for large-scale document collections; machine learning<br/>techniques for detecting malicious executables; information retrieval<br/>methods for performing proactive digital forensic investigations; text<br/>processing approaches for preventing privacy breaches in encrypted<br/>voice-over-IP (VoIP) communication; and data mining and sentiment<br/>detection methods for automated content analysis of large quantities<br/>of political science texts.<br/><br/>To support these goals, the project includes the creation of a<br/>next-generation infrastructure for large-scale text processing.<br/>Data-intensive tasks will use interconnected front-end systems,<br/>connected via high-speed networking to scalable back-end storage<br/>solutions.  The infrastructure supports the development and testing of<br/>novel text processing algorithms on very large corpora.<br/><br/>The project's themes of text processing and information assurance have<br/>been integrated into the undergraduate and graduate curricula at<br/>Georgetown University.  More broadly, the project benefits society by<br/>improving the ability to (1) process large quantities of information,<br/>(2) identify malicious applications, and (3) improve the resiliency of<br/>digital communication networks to eavesdropping attacks.  The project<br/>broadens the participation in research through the University's Summer<br/>Programs for High School Students and the Center for Multicultural<br/>Equity and Access.<br/>"
"1161233","AF:  Medium:  Collaborative Research:  Sparse Approximation:  Theory and Extensions","CCF","Algorithmic Foundations","07/01/2012","05/02/2012","Anna Gilbert","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Rahul Shah","06/30/2017","$603,762.00","Martin Strauss","annacg@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7796","7924, 7926, 7933","$0.00","In the past ten years the theoretical computer science, applied math and electrical engineering communities have extensively studied variants of the problem of ``solving"" an under-determined linear system.  One common mathematical feature that allows us to solve these problems is sparsity; roughly speaking, as long as the unknown vector does not contain too many non-zero components (or has a few dominating components), we can ``solve'' the under-determined system for the unknown vector.  These problems are referred to as sparse approximation problems and have applications in diverse areas such as signal and image processing, biology, imaging, tomography, machine learning and others.<br/><br/>The proposed research project aims to develop a comprehensive, rigorous theory of sparse approximation, broadly defined.  The research proposal entails two complementary research directions: <br/>(1) a robust and more complete view of the combinatorial, algorithmic, and complexity-theoretic foundations of sparse approximations (including its generalization to functional sparse approximation where we want to ``solve"" for some function of the unknown vector instead of the vector itself),<br/>(2) coupled with either its interactions or direct applications in other areas of theoretical computer science, from complexity theory to coding theory, and of electrical engineering, from signal processing to analog-to-digital converters.<br/>A general theory of sparse approximation that concentrates both on the optimal tradeoffs between competing parameters and the computational feasibility of attaining such tradeoffs will not only help explore the theoretical limits and possibilities of sparse approximations, but also feed algorithmic techniques and theoretical benchmarks back to its application areas.  Sparse approximation already has been shown to have impact in a variety of fields, including imaging and signal processing, Internet traffic analysis, and design of experiments in biology and drug design."
"1150209","CAREER: Vibration and Texture Perception","IOS","Activation","03/15/2012","04/20/2016","Sliman Bensmaia","IL","University of Chicago","Continuing Grant","Sridhar Raghavachari","02/28/2017","$770,000.00","","sliman@uchicago.edu","6054 South Drexel Avenue","Chicago","IL","606372612","7737028669","BIO","7713","1045, 1096, 9178, 9179","$0.00","When we run our fingers across a finely textured surface, small, complex vibrations are elicited in the skin, and specialized receptors embedded in the skin allow us to sense these vibrations. Different surfaces elicit different vibrations, and it is on the basis of these differences that we are able to discriminate silk from, say, satin. The ability to extract meaningful information from skin vibrations implies neural mechanisms that have yet to be discovered. This project investigates the mechanisms by which information about surface texture is extracted from texture-elicited vibrations. To this end, the Principal Investigator records (using a laser Doppler vibrometer) the skin vibrations elicited by everyday textured surfaces and examines the information conveyed. He then records the responses evoked by various textured surfaces in the nerve of Rhesus macaques and has human participants perform discrimination experiments using these same textures. Exploiting the well-documented homology between human and Rhesus sensory systems, he uses a variety of computational and machine learning techniques to reveal which aspects of the neural response account for our ability to distinguish between textures. Ultimately, the processing of skin oscillations constitutes a distinct and poorly understood mode of somatosensory processing. Indeed, while analogies have been drawn between visual and tactile processing of shape and motion, this second processing mode draws strong analogies with the auditory system. In both systems, stimulus information is extracted from spectrally complex oscillations (of the ear drum or the skin for audition and touch, respectively). In addition to their important scientific implications, results from this project will inform the development of sensorized upper-limb neuroprostheses and of virtual haptics displays. Finally, the project comprises an important educational and outreach component, with a strong emphasis on undergraduate education, recruitment of under-represented groups, and outreach to the general public."
"1209194","Robust and Relevant Model Evaluation: Principles and Techniques for Handling Weak Prior Information and Contaminated Data","DMS","STATISTICS, Methodology, Measuremt & Stats","09/01/2012","06/02/2014","Steven MacEachern","OH","Ohio State University","Continuing Grant","Gabor Szekely","08/31/2016","$320,000.00","Yoonkyung Lee, Xinyi Xu","snm@stat.osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","MPS","1269, 1333","","$0.00","This research concerns the development of innovative model comparison and model evaluation methods that focus on the most relevant features of the data and that are robust to deficiencies of model and data.  The advent of modern, automated technology for data collection and of cheap, near-boundless storage capacity provides access to a previously undreamt wealth of data.  The parallel development of sophisticated models which allow one to combine many sources of information and the computational strategies and horsepower which allow one to fit the models would seem to facilitate near-perfect decision-making.  However, the wealth of data aggravates the problems caused by data contamination, and the complexity of model aggravates the difficulty of specifying the prior on the parameters.  Handling data contamination and constructing methods robust to the lack of prior information pose fundamental statistical challenges.  In this project, the investigators illustrate the deficiency of the current leading model evaluation/model comparison methods, and then propose a set of new tools to alleviate the problems.  The proposed research consists of the following two specific aims.  1.  To develop reliable methods for Bayesian model comparison when prior information is lacking.  The common practices of using an improper noninformative prior distribution or a vague proper prior distribution are effective in estimation, however they break down for Bayesian hypothesis testing problems where model choice is sensitive to details of the prior distribution.  To tackle this difficulty, the investigators propose a remedy, the calibrated Bayes factor.  The calibrated Bayes factor does not need extensive subjective evaluation, yields an analysis that better mimics the performance of the Bayes factor under a ''reasonable default'' prior, and is widely applicable in a large variety of model comparison problems.  2.  To develop robust methods for model evaluation and model fitting in the presence of contaminated data.  Contaminated data comes in many forms, including observations potentially from recording mistakes or from irrelevant populations.  The contaminating process might be unstable, which makes standard statistical modeling infeasible.  For model fitting, this project develops and implements restricted-likelihood, which leads to estimation strategies that focus on the most relevant features of the data and that are robust to ''bad data''.  For model evaluation, this project develops an adaptive loss (scoring) paradigm for cross-validation, which produces robust results and yields superior finite-sample performance by stabilizing the evaluation.  <br/><br/>Model evaluation and model comparison are used on a daily basis in both scientific and corporate decision-making settings.  These techniques help researchers judge which theory best describes the phenomenon, help health professionals identify which risk factors are related to disease incidence, and help corporate managers decide which business strategy results in increased sales or better customer retention.  However, most of the current model evaluation and model comparison methods neglect deficiencies in data or suffer from the lack of parameter information.  The proposed research provides powerful methodological tools for robust model preference, model evaluation, and model fitting in these difficult situations.  It can help people in various fields better extract information from massive data sets, and thus optimize their decision making.  Specific applications in health studies, psychological experiments and machine learning will proceed along with development of the new methodology.  The general methodology is also applicable to many other scientific and technical areas, such as genomics, climatology, and economics, where large data sets are collected and robust model evaluation is desirable.  The investigators are well-positioned to disseminate the project's results.  They have been actively involved in research groups at the intersection of Statistics and the social sciences, engineering/computer science, and marketing.  They are also key members of a joint industry-university center dedicated to provide and disseminate research relevant to the insurance industry.  Results from this project will be spread to other communities through the investigators' interactions with these groups."
"1249835","HCC: Workshop: Articulating the Computing Research Agenda in Social Computing Research","IIS","SOCIAL-COMPUTATIONAL SYSTEMS","08/01/2012","08/09/2012","David McDonald","WA","University of Washington","Standard Grant","William Bainbridge","07/31/2014","$43,773.00","","dwmc@u.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7953","7556, 7953","$0.00","This is funding for a two-day workshop entitled ""Articulating the Computing Research Agenda in Social Computing Research"" that will be held in the fall of 2012.  Social Computing is an important new area of research that has given rise to a number of new journals, conferences and funding opportunities.  A wide variety of topics and disciplinary approaches have been grouped under the label of ""social computing,"" some of which have strong computer and information science components but others of which contribute to other academic disciplines.  The goal of this workshop is to bring together experts in social computing to develop a framework for conceptualizing social computing as an area of computer and information sciences.  <br/><br/>Specifically, the workshop will (a) consider what constitutes a social computing contribution in the information and computational sciences; (b) consider intellectual relationships between types of social computing contributions; (c) identify standards necessary for advancing social computing research; and (d) identify infrastructures that would help develop and support social computing research as part of a computing disciplines. Outcomes will include a published overview of the current state of social computing research, a set of educational goals for developing a future community of social computing scholars, the identification of publishing and other venues that can stimulate the exchange of ideas among the social computing researchers, and the identification of infrastructure needs.<br/><br/>The workshop will be organized as a series of discussions around six themes in social computing research within the computer and information sciences: (a) AI, machine learning and data mining; (b) social networking and network science; (c) quantitative methods; (d) qualitative methods; (e) systems and experimentation; and (f) design and broader impacts.  One or more experts will lead each thematic discussion, during which the workshop attendees will consider issues such as social computing successes in that thematic area, critical research questions, and infrastructural needs.<br/><br/>Participants in the workshop will include invited experts in the thematic topic areas and researchers selected on the basis of position papers submitted in response to the call for participation. In addition, a number of slots will be reserved for advanced graduate students.  <br/><br/>Intellectual Merit:  The workshop will advance computer science and engineering by providing a framework for understanding current social computing research and for identifying future key research questions. It will also advance social computing research by identifying methods for community building and exchange of research findings.<br/><br/>Broader Impacts:  Social computing is central to many areas critical to society, including emergency response, collective action, crowd-sourcing, sustainability, and health applications.  The outputs of this workshop will help researchers identify computer and information science contributions that will advance these and related areas of public concern.  The workshop will also include outreach efforts to engage underrepresented groups in STEM research."
"1242802","EAGER: Nano-Patterned Coupled Spin Torque Oscillator (STO) Arrays - a Potentially Disruptive Multipurpose Nanotechnology","IIS","","10/01/2012","09/13/2012","Stuart Wolf","VA","University of Virginia Main Campus","Standard Grant","Sylvia Spengler","09/30/2014","$300,000.00","Mircea Stan, Jiwei Lu","saw6b@virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","CSE","L121, L122","170E, 7916","$0.00","As conventional CMOS technologies are running into multiple barriers that are likely to slow down or even stop Moore?s Law scaling sometime during the next decade, there is a need for new material discoveries and new nanodevice and circuit paradigms that allow new applications that would be impractical or even impossible using traditional methods.  This work focuses on modeling, fabrication and applications for spin torque oscillators (STOs), with an emphasis on nano-arrays. Using multiferroic materials and an electric field, the team will tune both the frequency of a spin torque oscillator and also the coupling between adjacent STOs. This electric field will change the coercive field of the free layer which, in turn, will adjust the precessional frequency of the STO and the coupling between STOs. When presented with a continuous stream of real-time disparate data records that could come from different types of environmental and/or biological sensors an STO array can extract knowledge by analyzing the stream and providing a degree of match information about various data instances in the stream and their class membership. The associative memory capabilities of STO arrays are agnostic to the source of data, thus they can be used for computation and composition of data streams from hybrid sources working in different modes of operation. This special form of machine learning can be done in real time with no need for excessive local buffering of data. It can also accommodate concept drift as the underlying statistics used for data mining can easily be changed over time by controlling the inputs to the individual STOs. Such applications as real-time data stream mining as enabled by the proposed technology are critical to the safety and security of the country. <br/><br/>The multidisciplinary activities exploit fundamental nanoelectronics and spintronics concepts through contributions in materials science, circuit design and novel nano-computing paradigms. The PIs have a strong track record of collaborating across disciplines and of involving their undergraduate and graduate students in these collaborative activities, and this project will enable further collaboration between material scientists, physicists, and electrical and computer engineers."
"1161079","SHF: MEDIUM: Achieving Software Reliability without True Test Oracles","CCF","Software & Hardware Foundation","09/01/2012","07/08/2014","Gail Kaiser","NY","Columbia University","Continuing Grant","Sol Greenspan","08/31/2016","$894,582.00","","kaiser@cs.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","7798","7924, 7944","$0.00","Conventional software testing checks whether each output is correct for the set of test inputs. But for some software, it is not known what the correct output should be for some inputs -- yet it is still important to detect coding errors in that software, so they can be fixed. This dilemma arises frequently for machine learning, simulation and optimization applications, often ""Programs which were written in order to determine the answer in the first place. There would be no need to write such programs, if the correct answer were known."" As these kinds of applications are frequently used in public infrastructure and biomedical research (domains targeted in this research), it is critical to detect and fix errors before a calamity occurs.<br/> <br/>Fortunately, many such applications reflect 'metamorphic properties' that define a relationship between pairs of inputs and outputs, such that for any previous input i with its already known output o, one can easily derive a test input i' and predict the expected output o'. If the actual output o'' is different from o', then there must be an error in the code. This project investigates methodology for determining the metamorphic properties of software and for devising good test cases from which the secondary tests can be derived. The project extends the inputs/outputs considered in previous work on metamorphic testing to focus on application state, before and after, rather than just functional parameters and results. The research also extends the pairwise relations implied by metamorphic properties to 'semantic similarity' for nondeterministic applications, applied to profiles from numerous executions, since an exact relation cannot be expected to hold for a single pair of test executions. These extensions enable treatment of more sophisticated properties that preliminary experiments have shown to reveal defects that were not detected otherwise."
"1242067","Center for Research and Education in Optical Sciences and Applications: Promoting Scientific Discoveries, Technological Advances, and STEM Education","HRD","Hist Black Colleges and Univ, Centers for Rsch Excell in S&T, EPSCoR Co-Funding","09/01/2012","10/17/2016","Mukti Rana","DE","Delaware State University","Continuing Grant","Victor Santiago","08/31/2019","$4,996,696.00","Noureddine Melikechi, Aristides Marcano, Renu Tripathi, Mukti Rana, Gary Holness","mrana@desu.edu","1200 N. Dupont Highway","Dover","DE","199012277","3028576001","EHR","1594, 9131, 9150","9150, 9178, 9179","$0.00","The Center for Research and Education in Optical Sciences and Applications (CREOSA) was established at Delaware State University in 2006 as a NSF Center of Research Excellence in Science and Technology (CREST).  The Phase II Center will build on accomplishments of the first 6 years of the program, with a continued focus on optical sciences and applied physics.  The CREOSA has been leading systemic and transformative educational, research, and outreach programs at Delaware State University, a Historically Black University.  A new doctoral program in Optics has been initiated (the only one at an HBCU), leveraged the CREST award to secure funds for construction of a new building dedicated to optics research, increased scholarly publications by a factor of 6, enabled the hiring of 9 new faculty members, licensed the first intellectual property for DSU, and provided graduate student training of a new generation of optical scientists, mostly from minority groups.  The Center also was instrumental in creating a culture of innovative integration to foster interdepartmental and multidisciplinary research and education at DSU. The continued support by the NSF for CREOSA has enable Delaware State to achieve national prominence in optical sciences while serving the minority population of students that will attend and will be drawn to DSU. <br/> <br/>Center Vision<br/>CREOSA will be a sustainable unit that seeks to achieve national prominence in research and education in optical sciences with a strong commitment to educate, train, and prepare traditionally underrepresented groups in science, technology, engineering, and mathematics (STEM).<br/><br/>Center Mission<br/>CREOSA will integrate multi- and inter-disciplinary approaches to (1) Perform cutting-edge research in optical sciences; (2) Provide world-class education in optical sciences with a focus on serving historically underrepresented groups in STEM; (3) Improve the research infrastructure in optical sciences at DSU; and (4) Enhance significantly the diversity of the workforce with backgrounds in optical sciences by broadening participation of underrepresented groups in research and education at all levels which includes the population groups from K-12, undergraduate, graduate students, post-doctoral researchers, and faculty.<br/><br/>Intellectual Merit<br/>CREOSA will exploit the interdisciplinary nature of research activities across fields of optics, biology, biochemistry, mathematics and computational sciences. Three research subprojects form the foundation of the Center's research and educational activities:<br/><br/>1. Spectroscopy and Imaging of Biomacromolecules in Crowded and Complex Media<br/>This subproject brings together optical sciences and biological sciences at the technological cutting edge.  The research has the potential to attain unprecedented limits for the accurate detection, identification, and classification of diverse bio-macromolecules in their host environments.  These biomolecules may serve as novel biomarkers for diagnosis and treatment of various diseases and are the key idea behind the rapidly developing research in the field of nanomedicine.<br/><br/>2. Spin Polarization Spectrosopy in Nanodiamond for Nanoscale Sensing and Imaging<br/>The subproject team will investigate spectroscopic methods of studying nanoscale materials.  The work complements the activity of the first subproject, in that it will provide avenues to probe single and/or complex biomacromolecules.  These materials may provide an important driver in the development of solid-state quantum information processing.<br/><br/>3. A System for Interactive Data Mining in Experimental Optics <br/>This subproject will link experimental optics with important elements of data mining. The research proposed in both of the first two subprojects will create a large amount of complex data, which will require an integrated approach in which data-driven machine learning techniques are coupled with idealized models and expert knowledge in the analysis of spectroscopic data. The techniques to be developed will help resolve the communication bottleneck that occurs when several scientists in multidisciplinary optics research collaborate and exchange experimental data and extracted information.  <br/><br/>Broader Impacts<br/>More than 65% of the students at Delaware State are African American, and the new doctoral program established from the Phase I award is an important resource for that population, both in terms of advanced graduate programs and as a model for the role of research in stimulating more students from underrepresented groups to pursue STEM degrees.  A partnership with the Fisk-Vanderbilt doctoral bridge program has been initiated, which will stimulate pathways IN BOTH DIRECTIONS for these institutions.  An active professional development program for graduate students will be initiated, concurrent with a well organized recruitment and mentoring activities.  The program will also be at the heart of recruitment efforts for undergraduate students to matriculate in STEM fields at DSU and from URM groups in Delaware elsewhere, because of the high visibility of the optical sciences program.<br/><br/>DSU will continue outreach activities for pre-college students and the general community, such as a summer program that has prepared 89 students from 23 universities during the first five years, sponsorship of events such as open houses for the Delaware Science Alliance and Physics Days.  The project will continue the enhancement of the curriculum at DSU, across several academic platforms.  These include new tracks in electrical engineering, optical engineering, and bio-engineering in the Department of Physics and Pre-Engineering, as well as updates to the curricula for physics and physics education.  New educational laboratories will be developed as part of the world-class imaging facilities that are part of a new building being constructed.<br/><br/>The research investigations have the potential for profound impact on human health and counterterrorism.  The work will provide a stronger scientific foundation that could yield new technologies for medical diagnostics of early signs of disease and to detect pathogens used to fight bio-terrorism.  <br/><br/>This award has been partially funded through the Experimental Program to Stimulate Competitive Research at the National Science Foundation."
"1228304","Understanding Data Through Mappings","DMS","OFFICE OF MULTIDISCIPLINARY AC, CDS&E-MSS, CDS&E","09/01/2012","08/31/2012","Leonidas Guibas","CA","Stanford University","Standard Grant","Yong Zeng","08/31/2017","$784,996.00","Gunnar Carlsson","guibas@cs.stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","MPS","1253, 8069, 8084","9263","$0.00","The investigators will study the problem of computing informative, structure-preserving mappings between related data sets, and especially data sets of a geometric character, such as images, videos, GPS traces, 3D scans, or microarray data. Unlike classical data fusion where the goal is to find exact correspondences, the main emphasis here is to be able to map data at different levels of abstraction and to incorporate uncertainty and ambiguity directly into the map formulation. This raises challenging issues, both at the representational and the algorithmic levels. The aim is to develop efficient multi-resolution techniques through which data set relationships can be compactly encoded, compared, etc. --- making data relationships into precise, tangible objects that can be explored, just like the data themselves. The work will involve tools from a wide variety of mathematical disciplines, including aspects of differential geometry and topology, functional and harmonic analysis, algebraic and computational topology, machine learning and statistics, discrete and continuous optimization, scientific computing, and finally discrete algorithms and data structures. <br/><br/>Across all human activities, from science and engineering to medicine, commerce, and defense, massive data sets are becoming more and more available and more and more crucial to improved efficiency and enhanced functionality.  As our data sets grow in size and number, they become increasingly interconnected and inter-related. This is because data is captured about the same or related entities in the physical or virtual words (e.g., different images of the same building, different logs of the same user), as well as because the data itself reflects an underlying reality that has symmetries, regularities, and other shared structure. Thus it makes sense to analyze data sets jointly, exploiting this shared structure to do individual operations on data sets better. Through the above mapping machinery it will be possible to organize data collections into (possibly overlapping) groups of related sets or parts thereof, separating what is common from what is variable within each group and across groups, and understanding the main axes of variability."
"1149677","CAREER: STATISTICAL INFERENCE FOR TOPOLOGICAL AND GEOMETRIC DATA ANALYSIS","DMS","STATISTICS","06/01/2012","05/26/2016","Alessandro Rinaldo","PA","Carnegie-Mellon University","Continuing grant","Gabor Szekely","05/31/2018","$400,000.00","","arinaldo@cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","MPS","1269","1045","$0.00","The research objective of this proposal is to develop new theories and methods for estimating topological and geometric features of lower-dimensional sets based on noisy high-dimensional data.  To this end, the investigator has formulated two separate but highly interdependent sets of research goals.  The first set of research goals is the integration of statistical theory with methods of topological data analysis. Recent breakthroughs in computational topology have made it possible to compute topological invariants of sets from a collection of points in Euclidean spaces. Though the potential for high-dimensional statistical inference  of these new types of data summaries is significant, their statistical properties are still largely unexplored.  The investigator proposes to 1) to develop a comprehensive theory of minimax (and adaptive) estimation of topological properties of sets and 2) to create statistical procedures for non-parametric testing and de-noising based on topological invariants. The second set of research goals pertains to the traditional geometric data-analytic task of clustering in high-dimensions, and it is aimed at advancing the theory and practice of high-density clustering. Recent progress in the theory of clustering has demonstrated  that clustering using density estimation can perform well in high-dimensional settings, and that the notion of high-density clustering provides a natural probabilistic framework for describing and analyzing clustering problems in great generality. Thus, the investigator intends 1) to generalize and refine the high-density clustering problem under weak conditions on the data-generating mechanism and 2) to investigate the theory and use of data resampling techniques for parameter tuning in high-density clustering and density estimation. A common thread in the proposed research is the reliance on density estimation, as a tool for both accurate high-dimensional clustering and smoothing/de-noising of topological features. <br/><br/><br/>In the last few decades, advances in data acquisition technologies have led to an explosion in the collection and diffusion of large-scale datasets, across a variety of scientific fields. The unprecedented magnitude and complexity of modern databases pose formidable challenges to statisticians, both of theoretical and methodological nature, and has required the development of new statistical tools for data analysis. Modern high-dimensional statistics is predicated on the key assumption that, while the data are observed in a high-dimensional space, the intrinsic complexity of the data-generating mechanism is in fact significantly smaller and, therefore, learnable in computationally efficient ways. This research proposal capitalizes on this premise, and describes an array of methods for summarizing, discriminating, visualizing and clustering high-dimensional noisy data and for extracting salient low-dimensional features. The proposed research encompasses several novel and open research problems at the interface of mathematics, computer science, statistics and machine learning. The procedures studied in the proposal are of broad applicability and promise to be used in a multitude of scientific areas, such as medical imaging, neuroscience, astrophysics, biology, genetics, geophysics and sensor networks, just to name a few. The broader impact of this project also includes interdisciplinary training of students in statistics, mathematics and computer science."
"1208354","Estimating Low Dimensional Structure in Point Clouds","DMS","STATISTICS","07/01/2012","05/29/2012","Isabella Verdinelli","PA","Carnegie-Mellon University","Standard Grant","Gabor Szekely","06/30/2016","$232,150.00","Larry Wasserman, Christopher Genovese","isabella@stat.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","MPS","1269","","$0.00","This project will develop computationally efficient estimation methods with accompanying theory for the problem of identifying low-dimensional structure in point-cloud data, both low and high dimensional.  A canonical example is a noisy sample from a manifold. The investigators will develop minimax lower bounds for the estimation problem and construct estimators that achieve these lower bounds. They will then implement these methods in a practically useful form nd apply them to several important scientific problems.<br/><br/>Datasets sometimes contain hidden, low-dimensional structure such as clusters, filaments and low dimensional surfaces. The goal of this project to develop rigorously justified, computationally efficient methods for extracting such structure from data.  The developed methods will be applied to a diverse set of problems in astrophysics,seismology, biology, and neuroscience.  The project will advance knowledge in several fields including computational geometry, machine learning, and statistics."
"1213151","AF: Large: Collaborative Research: Exploiting Duality between Meta-Algorithms and Complexity","CCF","ALGORITHMIC FOUNDATIONS","07/01/2012","07/10/2015","Russell Impagliazzo","CA","University of California-San Diego","Continuing grant","Tracy J. Kimbrel","06/30/2017","$1,250,000.00","Samuel Buss, Ramamohan Paturi","russell@cs.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7796","7925, 7926, 7927","$0.00","Meta-algorithms are algorithms that take  other algorithms as input.<br/>Meta-algorithms are important in a variety of applications, from<br/>minimizing circuits in VLSI to verifying hardware and software to<br/>machine learning. Lower bound proofs show that computational problems<br/>are difficult in the sense of requiring a prohibitive<br/>amount of time, memory, or other resource to solve.<br/>This is particularly important in the context of cryptography,<br/>where it is vital to ensure that no feasible adversary can break<br/>a code. Surprisingly, recent research by the PIs and others<br/>shows that designing meta-algorithms is, in a formal sense, <br/>equivalent to proving lower bounds. In other words, one can prove a <br/>negative (the non-existence of a small circuit to solve a problem) by <br/>a positive (devising a new meta-algorithm).  This was the key to a <br/>breakthrough by PI Williams, proving lower bounds on constant <br/>depth circuits with modular arithmetic gates.<br/><br/>The proposed research will utilize this connection both to<br/>design new meta-algorithms and to prove new lower bounds.<br/>A primary focus will be on meta-algorithms for<br/>deciding if a given algorithm is 'trivial' or not, such as algorithms<br/>for the Boolean satisfiability problem.  The proposed research will devise new<br/>algorithms that improve over exhaustive search for many variants<br/>of satisfiability.  On the other hand, it will also explore<br/>complexity-theoretic limitations on how much improvement is<br/>possible, using reductions and lower bounds for restricted<br/>models. Satisfiability will provide a starting point for a more<br/>general understanding of the exact complexities of other NP-complete<br/>problems such as the traveling salesman problem and k-colorability.<br/>The proposal addresses both worst-case performance and the use<br/>of fast algorithms as heuristics for solving this problem.<br/><br/>This exploration will be mainly mathematical.  However, when<br/>new algorithms and heuristics are developed, they will be<br/>implemented and the resulting software made widely available.<br/>This research will be incorporated in courses taught by<br/>the PI's, at both graduate and undergraduate levels.<br/>Both graduate and undergraduate students will perform research<br/>as part of the project."
"1202135","Modeling, Monitoring, and Optimization of Cognitive Networks","ECCS","COMMS, CIRCUITS & SENS SYS","05/01/2012","04/13/2012","Georgios Giannakis","MN","University of Minnesota-Twin Cities","Standard Grant","hao ling","04/30/2016","$370,567.00","","georgios@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","ENG","7564","153E","$0.00","The objective of this proposal is to establish transformative novel solutions to fundamental problems in cognitive network modeling, monitoring and optimization with significant impact to both theory, algorithms, and implementation. <br/><br/>The intellectual merit comprises the novel concept of network cartography as the tool for distilling and summarizing the ""network state"" for the purpose of network management. The resultant holistic approach to network monitoring offers modular and scalable statistical representations driven by past (and driving future) measurements and system design specifications. Further, it enables inference of global network behavior and adaptation of protocol designs to enhance network robustness and quality-of-service. Dramatic improvements in CR network performance will be attained by augmenting the vivid description of the network global state with models that account for the intrinsic non-tangible connectivity of cognitive wireless links, and the interference challenges arising from spectrum sharing hierarchies. <br/><br/>The broader impacts include advances of the IEEE 802.22 compliant cognitive radio technologies, and smart grid, surveillance, geo-monitoring, and institutional networks. Furthermore, tangible advances in the fundamental tools exploited will permeate benefits to bio-informatics, speech and image compression, machine learning and data mining for social networks, and the emerging smart grid communication network protocols. Through Honors Theses and Senior Design Projects, the proposed research will also impact student training with hands-on experience in state-of-the-art wireless cognitive systems and network optimization subjects that are not fully provided by coursework."
"1216318","Extending Sparse Optimization","DMS","COMPUTATIONAL MATHEMATICS, OPERATIONS RESEARCH","08/01/2012","07/24/2012","Stephen Wright","WI","University of Wisconsin-Madison","Standard Grant","rosemary renaut","07/31/2016","$240,999.00","","swright@cs.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1271, 5514","073E, 9263","$0.00","Rather than solving an optimization problem exactly, sparse optimization seeks approximate solutions that satisfy certain structural properties, such as few nonzeros in the solution vector. Sparse optimization problems and formulations are now recognized across a wide range of applications, and techniques for solving these problems draw on a large variety of algorithmic tools, old and new. This project aims to extend sparse optimization in two respects. First, work is proposed in application areas that can benefit from the sparse optimization perspective: machine learning and data mining at extreme scale, contact dynamics, object packing, medical image reconstruction, and derivative-free optimization. Algorithmic developments will target key problem formulations in these areas, paying particular attention to methods that can exploit parallel computer architectures and specialized hardware. Algorithmic techniques to be considered include stochastic approximation, randomized directions, augmented Lagrangian, and reduced-space search using higher-order information. Second, the project will use general frameworks to analyze such algorithmic ideas as manifold identification, continuation, first-order algorithms, inexactness, and convergence and complexity results. The general nature of these investigations will enable innovations to be spread across a wide range of formulations and applications.  <br/><br/>The field of optimization provides a vital framework for formulating, modeling, and solving problems in many application areas. In sparse optimization, we note that many applications require solutions with a special structure that is easy to specify, but hard to incorporate in traditional algorithms and models. Sparse optimization arises, for example, in reconstruction of signals and images, where we know that the signal should contain only a few frequencies, or that the image should look like a natural image rather than white noise.  Important developments of the past few years have shown that the requirement of structure in solutions, rather than being a hindrance to efficient solution, can actually lead to more efficient formulations and faster methods. Notable successes have been achieved in such areas as compressed sensing and image denoising. This project will build on these successes by developing algorithms that can be leveraged in many new and existing applications of sparse optimization. In keeping with modern optimization research, a bevy of algorithmic techniques will be considered. Theory will be developed to support the use of these techniques in a wide range of contexts."
"1156643","REU Site:   CSHL NSF-REU Bioinformatics and Computational Biology Summer Undergraduate Program","DBI","RSCH EXPER FOR UNDERGRAD SITES","04/01/2012","02/14/2013","Zachary Lippman","NY","Cold Spring Harbor Laboratory","Standard Grant","Sally E. O'Connor","03/31/2015","$350,854.00","Doreen Ware, Anne Churchland","lippman@cshl.edu","1 BUNGTOWN ROAD","COLD SPRING HARBOR","NY","117244220","5163678307","BIO","1139","9178, 9250","$0.00","A Research Experience for Undergraduates (REU) Sites award has been made to Cold Spring Harbor Laboratory (CSHL) that will provide research training for 8 students, for 10 weeks during the summers of 2012- 2014. The program trains participants on the present and growing need to integrate biological research with sophisticated computational tools and techniques. CSHL has over 40 faculty members, including members of a newly established Quantitative Biology Department, who will serve as bioinformatics and computational biology mentors in fields ranging from plant biology to machine learning for biology. Through this NSF-REU support, students are afforded the opportunity to conduct full-time research in an appropriately matched lab based on mutual interests and goals. CSHL REU participants have access to individual and shared laboratory facilities such as flow cytometry, high throughput sequencing and analysis, imaging, and proteomics facilities. Participants attend multiple seminars and workshops, such as the responsible conduct in research, professional communication skills, the graduate school application process, and introduction to science careers. REU participants also are invited to attend the CSHL summer courses or meetings, which cover a range of topics such as Computational Neuroscience and Single Cell Analysis. All students are housed on campus within walking distance of their laboratories and the CSHL cafeteria, where they receive the majority of their meals. The multilayer recruitment effort consists of both traditional and digital mailings to potential students and their professors, as well as recruitment visits to universities throughout the country. Students are selected based on academic record, motivation for the proposed program of study, and potential as future researchers. Alumni successes are monitored to determine their continued interest in their academic field of study, their career paths, and the long-term impact of their research experience. Information about the program will be assessed using faculty and student evaluations, as well as the use of an REU common assessment tool. More information is available by visiting http://www.cshl.edu/education/urp/nsf-sponsored-reu-in-bioinformatics-and-computational-biology, or by contacting the PI (Dr. Zachary Lippman at lippman@cshl.edu) or the co-PI (Dr. Doreen Ware at ware@cshl.edu)."
"1161151","AF: Medium: Collaborative Research: Sparse Approximation:  Theory and Extensions","CCF","ALGORITHMIC FOUNDATIONS","07/01/2012","05/02/2012","Shanmugavelayu Muthukrishnan","NJ","Rutgers University New Brunswick","Standard Grant","Rahul Shah","06/30/2016","$290,000.00","","muthu@cs.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7796","7924, 7926, 7933","$0.00","In the past ten years the theoretical computer science, applied math and electrical engineering communities have extensively studied variants of the problem of ``solving"" an under-determined linear system.  One common mathematical feature that allows us to solve these problems is sparsity; roughly speaking, as long as the unknown vector does not contain too many non-zero components (or has a few dominating components), we can ``solve'' the under-determined system for the unknown vector.  These problems are referred to as sparse approximation problems and have applications in diverse areas such as signal and image processing, biology, imaging, tomography, machine learning and others.<br/>The proposed research project aims to develop a comprehensive, rigorous theory of sparse approximation, broadly defined.  The research proposal entails two complementary research directions: <br/>(1) a robust and more complete view of the combinatorial, algorithmic, and complexity-theoretic foundations of sparse approximations (including its generalization to functional sparse approximation where we want to ``solve"" for some function of the unknown vector instead of the vector itself),<br/>(2) coupled with either its interactions or direct applications in other areas of theoretical computer science, from complexity theory to coding theory, and of electrical engineering, from signal processing to analog-to-digital converters.<br/>A general theory of sparse approximation that concentrates both on the optimal tradeoffs between competing parameters and the computational feasibility of attaining such tradeoffs will not only help explore the theoretical limits and possibilities of sparse approximations, but also feed algorithmic techniques and theoretical benchmarks back to its application areas.  Sparse approximation already has been shown to have impact in a variety of fields, including imaging and signal processing, Internet traffic analysis, and design of experiments in biology and drug design."
"1217857","CIF: Small: Mathematical Aspects of Sparse Recovery-Theory and Algorithms","CCF","COMM & INFORMATION FOUNDATIONS","10/01/2012","09/06/2012","Mihailo Stojnic","IN","Purdue University","Standard Grant","John Cozzens","03/31/2015","$131,832.00","","mstojnic@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7797","7923, 7936","$0.00","The motivation behind this research is to enable broader (and valid) usage of recent breakthroughs in the field of compressed sensing by improving the performance of sparse signal reconstruction algorithms. Compressed or sparse sensing is a new way to recover an analog signal (signal reconstruction) from a small number of measurements when the signal's support - points where the signal in question assumes nonzero values - is finite. Sparse signals are the counterpart to band-limited signals in conventional Nyquist sampling. What makes this approach so attractive is that the number of samples required to achieve ""perfect"" reconstruction is generally very small, and fast optimization algorithms can accomplish reconstruction. The first step in this research program will be to find ways to exploit prior knowledge about the nature of the support of the signal. The ultimate goal is to develop algorithmic approaches (theoretical/practical) to aid practitioners in formulating realistic assumptions in applications ranging from wireless communications, machine learning, DNA micro arrays, and various problems arising in image reconstruction. <br/><br/>The research focus is on designing polynomial reconstruction algorithms that could provably bridge the gap between sparsity to measurement density ratios (S/MDR) recoverable by the current state-of-the-art algorithms and the best possible situation where the S/MDR approaches 1. Specifically, the region where there has been a substantial lack of success for current state of the art approaches. More broadly, the objective is to obtain theoretical guarantees on the performance quality (complexity, precision etc.) of certain practically feasible optimization algorithms. Beyond the stated objectives, this work has the potential for creating powerful mathematical formalisms for solving other challenging fundamental optimization problems."
"1239341","CPS: Synergy: Collaborative Research: SensEye: An Architecture for Ubiquitous, Real-Time Visual Context Ssensing and Inference","IIS","INFORMATION TECHNOLOGY RESEARC, CYBER-PHYSICAL SYSTEMS (CPS)","10/01/2012","06/11/2014","Deepak Ganesan","MA","University of Massachusetts Amherst","Standard Grant","Sylvia J. Spengler","09/30/2015","$732,000.00","Benjamin Marlin, Marco Duarte","dganesan@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","1640, 7918","7918, 9178, 9251","$0.00","Continuous real-time tracking of the eye and field-of-view of an individual is profoundly important to understanding how humans perceive and interact with the physical world. This work advances both the technology and engineering of cyber-physical systems by designing an innovative paradigm involving next-generation computational eyeglasses that interact with a user's mobile phone to provide the capability for real-time visual context sensing and inference. This research integrates novel research into low-power embedded systems, image representation, image processing and machine learning, and mobile sensing and inference, to advance the state-of-art in continuous sensing for CPS applications. The activity addresses several fundamental research challenges including: 1) design of novel, highly integrated, computational eyeglasses for tracking eye movements, the visual field of a user, and head movement patterns, all in real-time; 2) a unified compressive signal processing framework that optimizes sensing and estimation, while enabling re-targeting of the device to perform a broad range of tasks depending on the needs of an application; 3) design of a novel real-time visual context sensing system that extracts high-level contexts of interest from compressed data representations; and 4) a layer of intelligence that combines contexts extracted from the computational eyeglass together with contexts obtained from the mobile phone to improve energy-efficiency and sensing accuracy.<br/><br/>This technology can revolutionize a range of disciplines including transportation, healthcare, behavioral science and market research. Continuous monitoring of the eye and field-of-view of an individual can enable detection of hazardous behaviors such as drowsiness while driving, mental health issues such as schizophrenia, addictive behavior and substance abuse, neurological disease progression, head injuries, and others. The research provides the foundations for such applications through the design of a prototype platform together with real-time sensor processing algorithms, and making these systems available through open source venues for broader use. Outreach for this project includes demonstrations of the device at science fairs for high-school students, and integration of the platform into undergraduate and graduate courses."
"1209057","Dimension Reduction Through Index Models","DMS","STATISTICS","06/01/2012","05/21/2012","Peter Radchenko","CA","University of Southern California","Standard Grant","Gabor J. Szekely","05/31/2015","$129,585.00","","radchenk@marshall.usc.edu","University Park","Los Angeles","CA","900890001","2137407762","MPS","1269","","$0.00","Historically statistics has dealt with extracting as much information as possible from a small data set. However, much of modern statistical research focusses on data sets that have enormous numbers of predictors. This phenomenon is a direct result of recent technological advances that have affected various fields of research, such as image processing, computational biology, and finance. This proposal addresses a very important question of fitting nonlinear regression models in high-dimensional situations, where the number predictors may be much larger than the number of observations. Unlike linear or generalized linear models, high-dimensional nonlinear regression is a very young research area that requires systematic and extensive development. Due to the curse of dimensionality, most of the work in this area has been conducted under the assumption that the regression function has a simple additive structure. The investigator proposes novel methodology for fitting index type regression models in high dimensions. The new methods cover models that are either complimentary or strictly more general than the additive models studied before. For each of the methods the proposal presents a computationally efficient fitting algorithm and lays out a plan for establishing theoretical results.<br/><br/>The proposed research is expected to have a broad impact on the practice and education of statistics and related fields. Disciplines such as Computational Biology, Finance, Marketing and Machine Learning are highly interested in the type of methodology that is targeted in this proposal. The investigator plans to systematically develop software for implementing the proposed methods through free software packages and then make them readily available to researchers in the aforementioned fields. The proposed research will also have an impact on the growth and development of the new USC Statistics Ph.D. program. Several students in this young program will be involved in methodology research, algorithm development, and theoretical investigation. They will get hands on experience and guidance in the very important field of high-dimensional statistical inference."
"1258335","EAGER: Data Analysis for Nursing Care Assistance","IIS","HCC-Human-Centered Computing","09/15/2012","09/12/2012","Jing Xiao","NC","University of North Carolina at Charlotte","Standard Grant","Ephraim Glinert","08/31/2015","$55,389.00","Srinivas Akella, Jianping Fan, Sonya Hardin","jxiao2@wpi.edu","9201 University City Boulevard","CHARLOTTE","NC","282230001","7046871888","CSE","7367","7367, 7916","$0.00","The rising cost of long-term patient care, the shortage of nurses, and the increasing number of seniors in the United States make it imperative to investigate the possibility of intelligent systems for elder/patient care.  One challenge is how to prevent falls, which often result in serious injury and considerable hospital and patient costs.   In this exploratory project the PI and her team will focus on analyzing sensory data of patient actions in an effort to develop algorithms for the automatic detection and prediction of falls among elderly patients.  Their goal is to gain a good understanding of how multimodal sensory data combined with domain knowledge of falls can be used to characterize pre-fall patient actions, in order to determine the feasibility of developing automatic alert systems that incorporate machine learning algorithms to assist human nurses and robotic caregivers by warning of potential falls.  <br/><br/>Broader Impacts:  Project outcomes will pave the way for future development of intelligent systems to reduce the incidence of patient falls, which is a major societal concern.  The project will provide a rich spectrum of interdisciplinary training for graduate student researchers, and will also strengthen UNC Charlotte's existing programs in broadening participation in computing and in research experiences for undergraduates (REU) by deepening involvement of women and minority undergraduate students in research."
"1219138","HCC: Small: Examining the Super User versus the Crowd in Human-Centered Computation","IIS","HCC-Human-Centered Computing","08/15/2012","07/23/2014","Albert Lin","CA","University of California-San Diego","Continuing Grant","William Bainbridge","07/31/2016","$497,250.00","Falko Kuester","a5lin@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7367","7367, 7923","$0.00","This project investigates the nature of crowd-based human analytics at various scales, specifically how the concentrated efforts of a few contributors differ from the summed micro contributions of many.  Automated approaches are good at handling huge amounts of data, but they lack the flexibility and sensitivity of human perception when making decisions or observations, especially when computational challenges revolve around visual analytics. Networks of humans, as an alternative, can scale up human perception by facilitating massively parallel computation through the distribution of micro-tasks, but human data interpretation is variant between individuals.  Wide variability in the amount of participation of individuals in crowd-based computation creates non-uniform representations of a crowd, which is an important discrepancy that could significantly impact the validity of the term ""crowd"" in crowdsourcing.  The research will explore data generated from the extreme ends of the participation curve and quantify the quality of data produced from a broad sampling of a crowd versus concentrated voice of the few ""super users."" <br/><br/>As one measure of comparison, the researchers will observe how characteristically variant samplings of human generated analysis alter the outcome when used as training data in a machine learning framework.  This investigation will utilize data generated from a crowdsourcing effort  that tapped over 10,000 volunteer participants to generate over 2 million human annotations on ultra-high resolution satellite imagery in search for tombs across Mongolia. Image tiles were distributed at random to participants who tagged anomalies of interest, while crowd consensus on points of interest provided a field survey team with locations to ground truth in Mongolia. Participation ranged widely, as illustrated by the fact that 20 percent of the data came from the most active 1 percent of participants, while at the other extreme 20 percent of the data came from the 80 percent of participants who were least active.  While consensus of the crowd provided one metric to measure the quality of anomaly identifications, ground truth observations showed actual validation tended to correspond with identifications made from higher interest participants.  This study will explore the nature of data generated from experts versus crowds of non-experts, starting from the discrepancies in participation levels.<br/><br/>Crowd-based human analytics has been welcomed as a potential solution to some of the world?s largest data challenges.  Examples of crowdsourcing have shown that the power of distributed microtasking can engage challenges as overwhelming as categorizing the galaxies, or as complicated as folding proteins.  However this concept depends upon the recruitment of human help, often at whatever levels of participation an individual is willing to contribute. The variation in contributions, and thus impact levels, between individuals can be staggering, with participation typically distributed across a longtail curve. That fundamental aspect of a recruited crowd should be recognized and understood when extracting knowledge from the data that is generated.  This project will contribute to the necessary understanding by determining how the distributed inputs from a crowd differ from the concentrated efforts of an individual.  Insight into the effects of crowd dynamics on results will determine how we pool and retain participation and, thus, have transformative impact on the development of crowdsourcing as a concept for analytics."
"1240394","Workshops for Creating a Community Roadmap for EarthCube Services for Data Discovery, Mining and Access: Data Mining Services","EAR","EarthCube","05/01/2012","04/17/2013","Rahul Ramachandran","AL","University of Alabama in Huntsville","Standard Grant","Barbara Ransom","10/31/2013","$99,394.00","Sara Graves","rahul.ramachandran@nasa.gov","301 Sparkman Drive","Huntsville","AL","358051911","2568242657","GEO","8074","7433, 9150","$0.00","EarthCube, a major new NSF initiative, is focused on community-driven development of an integrated and interoperable knowledge management system for data in the geo- and environmental sciences. By utilizing a cooperative, as opposed to competitive process like that which created the Internet and Open Source software, EarthCube will attack the recalcitrant and persistent problems that so far have prevented adequate access to and the analysis, visualization, and interoperability of the vast storehouses of disparate geoscience data and data types residing in distributed and diverse data systems.  This awards funds a series of broad, inclusive community interactions to gather adequate information and requirements to create a roadmap for a critical capability (data mining and algorithm/data analytic techniques) that will enable the development of EarthCube. The focus of the community conversations include exploration of present data analysis approaches and possible new novel approaches; data filtering; pattern recognition and machine learning as applied to geoscience datasets; and search algorithm identification and development.  Intensive interaction will also take place with awardees and participants in the other EarthCube community groups and concept development awards.  Broader impacts of the work include: helping EarthCube realize its potential to dramatically improve the infrastructure for science, actively engaging early career geoscience researchers in the process, and supporting investigators at an institution in an EPSCoR state."
"1239031","CPS: Synergy: Collaborative Research: SensEye: An Architecture for Ubiquitous, Real-Time Visual Context Sensing and Inference","IIS","Information Technology Researc, CPS-Cyber-Physical Systems","10/01/2012","09/10/2012","Prabal Dutta","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Sylvia Spengler","09/30/2016","$300,000.00","","prabal@berkeley.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","1640, 7918","7918","$0.00","Continuous real-time tracking of the eye and field-of-view of an individual is profoundly important to understanding how humans perceive and interact with the physical world.  This work advances both the technology and engineering of cyber-physical systems by designing an innovative paradigm involving next-generation computational eyeglasses that interact with a user's mobile phone to provide the capability for real-time visual context sensing and inference. This research integrates novel research into low-power embedded systems, image representation, image processing and machine learning, and mobile sensing and inference, to advance the state-of-art in continuous sensing for CPS applications. The activity addresses several fundamental research challenges including: 1) design of novel, highly integrated, computational eyeglasses for tracking eye movements, the visual field of a user, and head movement patterns, all in real-time; 2) a unified compressive signal processing framework that optimizes sensing and estimation, while enabling re-targeting of the device to perform a broad range of tasks depending on the needs of an application; 3) design of a novel real-time visual context sensing system that extracts high-level contexts of interest from compressed data representations; and 4) a layer of intelligence that combines contexts extracted from the computational eyeglass together with contexts obtained from the mobile phone to improve energy-efficiency and sensing accuracy.<br/><br/>This technology can revolutionize a range of disciplines including transportation, healthcare, behavioral science and market research. Continuous monitoring of the eye and field-of-view of an individual can enable detection of hazardous behaviors such as drowsiness while driving, mental health issues such as schizophrenia, addictive behavior and substance abuse, neurological disease progression, head injuries, and others. The research provides the foundations for such applications through the design of a prototype platform together with real-time sensor processing algorithms, and making these systems available through open source venues for broader use. Outreach for this project includes demonstrations of the device at science fairs for high-school students, and integration of the platform into undergraduate and graduate courses."
"1218791","AF: Small: Exploring Algebraic Methods in Computational and Combinatorial Geometry","CCF","Algorithmic Foundations","09/01/2012","08/09/2017","Boris Aronov","NY","New York University","Standard Grant","Joseph Maurice Rojas","08/31/2018","$348,272.00","","boris.aronov@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","7796","7923, 7929","$0.00","Building on recent progress in applying algebraic methods to traditional problems of computational and combinatorial geometry, the PI will explore further use of the tools of algebraic geometry in this subject.  In particular, the PI will develop novel algebraic techniques<br/>-- as an alternative to random sampling,<br/>-- for addressing geometric incidence problems,<br/>-- for tackling range searching questions,<br/>-- for making progress on the problem of repeated distances in the plane, and<br/>-- to facilitate multilevel partitioning schemes, <br/>by extending the ideas of Guth and Katz, and of Sharir and co-authors, combining algebraic tools such as Bezout's theorem, fast Fourier transform, and Veronese lifting with existing methods from computational and combinatorial geometry.<br/><br/>This work will significantly expand the arsenal of tools available to researchers in computational and combinatorial geometry.  The techniques of this field, in turn, have wide-ranging applications in a variety of practical subjects, such as computer graphics, geographic information systems, machine learning, and robotics path planning, amongst others.<br/><br/>PI's home institution (Polytechnic Institute of NYU) has historically had a very diverse student body.  Education is much more effective when in the classroom students encounter easily accessible research problems that remain open to this day.  Such problems have been and will be discussed in computational geometry and algorithms courses taught by PI, increasing accessibility and public appreciation of the somewhat nebulous ""research in theoretical computer science."""
"1158273","A Systems Approach to the NPK Nutriome and its Effect on Biomass","MCB","Systems and Synthetic Biology","04/01/2012","04/25/2014","Gloria Coruzzi","NY","New York University","Continuing Grant","Anthony Garza","03/31/2017","$1,185,284.00","Dennis Shasha, Alessia Para","gloria.coruzzi@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","BIO","8011","7465, 9104, 9109, 9178, 9179, 9183","$0.00","Intellectual Merit: This project aims to use a systems biology approach to identify the genes that control plant biomass production by sensing and integrating responses to nutrients in soil. The research focuses on the interactions of Nitrogen, Phosphorus and Potassium (NPK), the main nutrient chemicals used in commercial fertilizers. Optimizing plant growth in response to NPK nutrient interactions has the potential to increase biomass production, while decreasing the toxic leaching of these chemicals (especially nitrogen and phosphorus) into surface and ground waters, thus impacting the environment and human health. In now classic experiments, Murashige and Skoog (1962) showed that the interactions of NPK could lead to an increase in biomass under low N-input conditions. This project seeks to identify the gene networks underlying the ""NPK interaction effect"" on biomass by combining genomic, phenotyping, and network inference approaches. Our experimental and analytical strategy is the result of a highly successful collaboration between biologists and computer scientists, and involves an iterative cycle of experimentation and computation, a hallmark of systems biology. The aims of the project are to: 1. Identify combinations of NPK treatments that result in high biomass under low N-input; 2. Define ""early"" molecular marker genes that act as predictors of biomass; 3. Conduct genome-wide RNA analysis and modeling to identify gene regulatory networks associated with low-N High-biomass state; and 4. Test candidate regulatory genes. The ultimate goal is to uncover genes and pathways that control plant growth under NPK treatments and to manipulate them to optimize N-use efficiency and biomass production.<br/><br/>Broader Impacts: The long-term advantage of this project is to identify and target critical regulatory components controlling nutrient use efficiency to create crops that produce high biomass with a reduced amount of fertilizer, hence decreasing the health and ecological impacts of leached chemicals. In addition, the generation of nutrient-efficient crops would ameilorate their cultivation on impoverished or nutrient-poor soils. This project will involve the training of scientists at the graduate and postdoctoral level across computational and experimental biology. As the research entails the development of Systems Biology approaches, biologists will be engaged in teaching computer scientists about topics like genetics, experimental genomics, and the computational challenges of analyzing genomic data. In turn, computer scientists will be involved in developing and testing optimization as well as machine learning algorithms for network inference that predicting network states under untested conditions, the ultimate goal of systems biology. The project will also include an outreach program that provides high school students the opportunity to work in a laboratory environment at the interface of Biology and Computer Science. As part of this program, the NYU Center for Genomics and Systems Biology produced 4 semifinalists and 2 national finalists (out of 40) for the Intel Competition in 2012. One of these Intel finalists was mentored by the PI of this project.  The PIs of this project are committed to increasing diversity and will continue to actively seek out and recruit scientists from under-represented minorities to participate in the research and outreach components. The project has and will continue to involve a diverse team of researchers. The PI has served as a mentor for many women scientists and will continue this role in the future."
"1218929","NETS: Small: Exploiting Social Communication Channels Against Cyber Criminals","CNS","Secure &Trustworthy Cyberspace","09/01/2012","08/24/2012","A.L. Narasimha Reddy","TX","Texas A&M Engineering Experiment Station","Standard Grant","Deborah Shands","08/31/2016","$250,000.00","Guofei Gu","reddy@ece.tamu.edu","400 Harvey Mitchell Pkwy S","College Station","TX","778454645","9798626777","CSE","8060","7434, 7923","$0.00","Malware, especially botnets, have become the main source of most attacks and malicious activities on Internet. Bots communicate with each other and Command & Control servers to coordinate their malicious activities. This project is developing new techniques and tools to detect malicious activities and botnets through analyzing their communication channels. This project plans to investigate mechanisms for detecting these communication channels through several novel mechanisms: (i) through a graph analysis of social contacts, (ii) analyzing graph properties of communication to decipher Peer to peer communication properties of bots and (iii) machine learning based approaches to analyzing network traffic. The automation required to propagate malicious contents and malware will result in different behavior than human behavior in these communication channels. We expect the generated names, content, the time of contacts and communication, the social graph structures to be sufficiently different to enable us to develop techniques for detection of malicious entities. Our work focuses on developing robust techniques that are hard to evade or limit the botnet functions when they try to evade the detection mechanisms. Developed analysis tools will help in detecting botnets in networks and malicious entities in social networks. Educational impact will include training graduate and undergraduate students with valuable research skills while advancing the state of the art in network security and traffic analysis, contributing to the technology workforce. We will publish our results and enable technology transfer to industry."
"1218303","SHF: Small: Tactic-Centric Traceability Models for Preserving Architectural Quality","CCF","Software & Hardware Foundation, SOFTWARE ENG & FORMAL METHODS","08/01/2012","05/02/2014","Jane Huang","IL","DePaul University","Standard Grant","Sol Greenspan","07/31/2017","$522,193.00","","JaneClelandHuang@nd.edu","1 East Jackson Boulevard","Chicago","IL","606042287","3123627595","CSE","7798, 7944","7923, 7944, 9251","$0.00","Software-intensive systems are typically designed around a set of architectural decisions. These decisions are often based on well-known architectural tactics, which work together to shape the structure, behavior, properties, processes, and governance of the delivered solution. Unfortunately, architectural quality often degrades over time as ongoing maintenance activities are undertaken to correct faults, improve performance, or to adapt the system in response to changing requirements. This research presents a novel and practical approach for addressing the problem of architectural degradation based on a new concept of tactic Traceability Information Models (tTIMs). tTIMS provide semantically typed, reusable, and extensible traceability links, and are designed to significantly simplify the traceability process, reduce its costs, scale up to support traceability and architectural preservation in large and complex projects, and facilitate the visualization of underlying design rationales to the software maintainer. The proposed techniques will have the potential to impact a wide variety of high-performance and safety critical software systems.<br/><br/>The project makes several important contributions. First, it provides a potentially transformative solution for integrating existing techniques from the field of architectural assessment and trace retrieval into the maintenance process. Second it develops new algorithms and techniques for capturing and reusing traceability knowledge across projects in the form of a reusable infrastructure of semantically typed traceability links. Third, it provides new visualization techniques for communicating underlying architectural decisions and their interrelationships to developers and maintainers within the context of common software engineering tasks. Fourth, it advances cutting edge trace-retrieval solutions by utilizing a combination of trace retrieval, machine learning, and structural analysis techniques in order to detect classes related to architectural tactics and then to classify those classes according to specific roles in the tactic. Finally, it utilizes this information to automatically generate architecturally significant traceability links."
"1149885","CAREER:   Dissecting the Mechanisms of Genetic Control of Biological Systems via High-Dimensional Sparse Graphical Models","MCB","Information Technology Researc, Info Integration & Informatics, Systems and Synthetic Biology","06/01/2012","05/14/2016","Seyoung Kim","PA","Carnegie-Mellon University","Continuing grant","edward crane","05/31/2018","$933,210.00","","sssykim@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","BIO","1640, 7364, 8011","1045, 7465, 8750, 9102, 9179, 9251","$0.00","Since the completion of genome sequencing projects for various organisms including human and other model organisms, the fundamental goal of research in computational genomics, systems biology, and genetics has been to gain a complete understanding of how the instruction sets encoded in genomes get executed within a cell system and organism. The recent advances in the high-throughput technology and next-generation sequencing technology have allowed the researchers to collect a large amount of data for the genomes and various other aspects of a cell system. Such datasets hold the key to understanding the detailed mechanisms of the genetic control of a biological system and further deepening our knowledge of cell biology with the potential for broad application. This project will develop statistical machine learning methods based on high-dimensional sparse graphical models for integrative analysis of genomic datasets. As graphical models provide a powerful tool for representing the complex structure of the unknown biological processes that underlie the observed genomic data, the computational methods to be developed in this project will be able to extract rich information on the genetic control of gene regulation systems from genome-scale datasets.<br/><br/>This project will also include training the next-generation computational biologists by supervising graduate students and incorporating the research results into the curriculum. The project will involve collaboration between computational scientists and biologists to participate in outreach programs for high-school students to present them an alternative career path that combines biological and computer sciences. In addition, the project will contribute to increasing womens participation in science and engineering."
"1219023","III: Small: Issues in the Management of GeoMultimedia Data","IIS","Special Projects - CNS, Info Integration & Informatics, IIS Special Projects","09/01/2012","09/04/2013","Hanan Samet","MD","University of Maryland College Park","Standard Grant","Maria Zemankova","08/31/2016","$538,108.00","","hjs@cs.umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","1714, 7364, 7484","1714, 7364, 7484, 7923, 9251","$0.00","The growth of the Internet has led to a dramatic increase in the rate at which information is generated and delivered. This is especially true for the news cycle which has become instantaneous and often resulting in social networks, most notably Twitter, being the medium of choice to deliver late breaking news. This is usually in the form of links to more details which include news articles and multimedia data (news photos and videos). One problem lies in identifying reliable news gatherers which is done by noting if they are the first to report on a topic in contrast to being a re-poster and their frequency. Other issues in providing access to news content involve making the news automatically indexable (instead of by human tagging) both by content (regardless of the language of creation and the nature of the media) and by the location of the content (i.e., geotagging) rather than the location or affiliation of its creator. In the case of location, the challenge lies in devising language-independent geotagging techniques. Machine learning based methods are investigated and are expected to work better than rule-based methods due to a reduced reliance on language-specific rules and a greater reliance on examples. Access to the images is facilitated by indexing them by the words associated with the news articles containing them. This indexing technique is meant to be used as a filter for finding similar or near-duplicate images where the similarity is based on image features. The access by location is facilitated by the use of a map query interface. This is important as it corresponds to enabling the use of spatial synonyms thereby permitting a wider range of queries to be posed. A novel aspect of the research is the incorporation of non-English content which is facilitated by the use of computerized translation services which will be evaluated on their ability to capture the content on the basis of clustering similar articles in different languages rather than on the basis of factors such as proper grammar, etc. <br/><br/>In today's rapidly changing world, the tools that are developed in this project will make this information more accessible as users are enabled to focus on a geographical area of interest as well as have access to content in their own language. This is of utility to a number of organizations and attempts will be made to collaborate with potential users on tailoring the tools for their needs. In addition, the project will provide research experience to undergraduate and graduate students who will be involved in developing some of the components. These tools are also a step in the growth of the nascent field of computational journalism. The project web site (http://www.cs.umd.edu/~hjs/geomultimedia.html ) will provide access to results of this and related research."
"1216764","Strategic Workshop on Information Retrieval","IIS","Info Integration & Informatics","02/01/2012","01/25/2012","James Allan","MA","University of Massachusetts Amherst","Standard Grant","Maria Zemankova","01/31/2013","$20,000.00","","allan@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","7364","7364, 7556","$0.00","This award provides support for U.S. researchers to participate in the Strategic Workshop on Information Retrieval, to be held in Lorne, Victoria, Australia in February 2012. The goal of the workshop is to explore and define long-range research issues of general and personal information management in the context of information that is constantly changing, that is in a diverse set of formats, and that is stored in numerous locations. This workshop brings together about 40 junior and senior researchers from around the world with expertise spanning the information retrieval, information science, and search communities to develop common understandings and goals of the long-term challenges and opportunities.  The workshop will explore at least the following questions: (1) How can Information Retrieval algorithms be adapted to this new paradigm where information is distributed, multimedia, and dynamic? (2) In this setting, what are the key challenges that researchers, graduate students, and funding agencies should be aware of? (3) How can the successful evaluation models for Information Retrieval be extended to this setting where data availability and privacy are major challenges? (4) How should teaching about Information Retrieval be modified to reflect likely changes in the field?  <br/><br/>This workshop is expected to yield research directions that will impact the information retrieval and science research and development communities as well as research groups that tackle problems arising within this context (e.g., machine learning, human computer interaction). The immediate beneficiaries of the workshop's publications will include graduate students, faculty members, industrial researchers, and developers of search-related technology.  In addition, it is expected that general users of search tools will benefit from resulting advances in technology. Results addressing the questions above will be disseminated on a dedicated website (http://www.cs.rmit.edu.au/swirl12/) and will also be published in the widely distributed SIGIR Forum newsletter and its web site."
"1217418","Application-Specific Memory System Optimizations using Programmable Memory Controllers","CCF","Software & Hardware Foundation","07/01/2012","06/25/2012","Engin Ipek","NY","University of Rochester","Standard Grant","tao li","06/30/2015","$300,000.00","","ipek@cs.rochester.edu","518 HYLAN, RC BOX 270140","Rochester","NY","146270140","5852754031","CSE","7798","7923, 7941","$0.00","The off-chip memory subsystem is a significant performance and power bottleneck in modern computer systems, necessitating a memory controller that can overcome memory timing and resource constraints by carefully orchestrating data movement between the processor and main memory. The goal of this project is to address this need by enabling application-specific memory system optimizations using a programmable main memory controller, thereby improving the performance, energy-efficiency, and quality-of-service of future computer systems. To realize this vision of programmable main memory controllers, the project addresses challenges all the way from the hardware design of programmable processing units to the firmware implementation of novel memory management algorithms. Automated machine learning and search techniques are employed to quickly arrive at high-performance control algorithms customized to different applications, and to different phases of a single application. Application-specific memory management algorithms ranging from address mapping, command scheduling, and DRAM power management to error-correcting codes and memory compression are developed.<br/><br/>The flexibility of the resulting application-specific memory systems is expected to have a direct impact on the performance and energy-efficiency of future computer systems, with tremendous positive fallout to science, technology, and society as a whole. Architecture and software innovations are disseminated to the broader research community through published papers, as well as tutorials on programmable controllers and application-specific memory controller algorithms at major conferences. The educational component of the project involves (1) training both graduate and undergraduate students in computer architecture, (2) a memory systems course that integrates programmable memory controllers and next-generation memory systems into the syllabus, and (3) a memory controller design experience for undergraduates as part of the existing computer architecture curriculum."
"1152306","IDBR (Type A): Development of app and web interface for automated anuran recognition and mapping","DBI","INSTRUMENTAT & INSTRUMENT DEVP","03/01/2012","02/09/2017","Mark Bush","FL","Florida Institute of Technology","Standard Grant","Robert Fleischmann","02/28/2018","$368,675.00","Ronaldo Menezes, Eraldo Ribeiro","mbush@fit.edu","150 W UNIVERSITY BLVD","MELBOURNE","FL","329016975","3216748000","BIO","1108","7218, 9178, 9179, 9251","$0.00","A synergy of disease, climate change, pollution, exotic species and habitat loss, is pressuring anuran populations worldwide. In the USA, while overall anuran population declines are clearly evident, species that will be hardest hit are hard to predict. The spatial and temporal monitoring of anuran populations needs to be improved if the signs of further population failures are to be identified and remedies sought. The ideal means to increase the coverage of data is to put out sensors that would detect frogs, or have highly trained researchers conduct surveys. However, both these options are cost-prohibitive. A more cost-effective means to gather these data is to take advantage of the many volunteers interested in the fate of anurans. The limitation to date has been the time-consuming and inherently variable success in training those volunteers to recognize anuran vocalizations. In the proposed research, machine-learning algorithms and statistical analysis of sonograms will be used to provide reproducible, standardized identification of calling anurans in real time.<br/> A novel use of mobile devices (e.g. iPods, phones, and tablets) is proposed to identify and map anurans. The vocal recognition software will be developed as application (app) for mobile devices. By producing an efficient and intuitive app, that integrates with an accompanying social network an army of potential volunteers can be involved in mapping the abundance and range of anurans. While this proposal takes advantage of cutting-edge technology in terms of voice recognition, connectivity and social media, its purpose is not to advance any one of those branches of technology. Rather it applies existing technology in a novel way to increase participation in this aspect of citizen science by more than an order of magnitude, and bring the cost of automated voice recognition down by almost three orders of magnitude (from close to $1000 to ~$1). In providing a standardized means to recognize anuran calls, data quality and the reliability of data gathered by volunteers will be improved.<br/>    <br/>General statement of project importance<br/> Frogs and toads are excellent indicators of habitat quality, not just of their spawning sites, but also of adjacent uplands. These creatures are experiencing an unprecedented collapse in populations due to habitat loss, pollution, and novel diseases. As the faltering of amphibian populations may presage wider ecological problems, it is important to document where species are most affected. In the proposed research, an application (app) will be developed for smart phones and other mobile devices that will use cutting edge voice-recognition software to identify frogs and toads. Currently the average age for volunteers conducting anuran surveys is  about 50. By using modern social media and portable device technology the goal of this proposal is to engage a younger audience. The affordability of the system will make it appropriate for classroom use, and it has the potential to spur studies in ecology, physics, geography, and math, as the students consider the data, sonograms, and maps that the software will produce. Through active dissemination we believe this app could quickly be adopted by thousands, perhaps tens of thousands of users, many of whom would be new contributors to citizen science."
"1158697","Collaborative Research: Balancing the Portfolio: Efficiency and Productivity of Federal Biomedical R&D Funding","SMA","SciSIP-Sci of Sci Innov Policy, SciSIP Infrastructure","08/15/2012","08/13/2012","Margaret Blume-Kohout","NM","University of New Mexico","Standard Grant","Joshua Rosenbloom","09/30/2013","$385,447.00","","mblumeko@gettysburg.edu","1700 Lomas Blvd. NE, Suite 2200","Albuquerque","NM","871310001","5052774186","SBE","7626, 8075","7626, 9150, 9179","$0.00","This cross-disciplinary, cross-institution collaborative research project combines economic analysis with state-of-the-art methods from statistical machine learning, to assess the relative efficiency and efficacy of research and development expenditures across the U.S. National Institutes of Health (NIH) portfolio of extramural projects. The novel combination of econometrics, topic modeling, and document classification permits analysis of massive collections of grant abstracts and scientific publications, identification of latent research topics present in NIH-funded research, assessment of possible spillover effects across research topics, and evaluation of causal linkages between changes in NIH funding by research topic and scientific advances. Two specific research outcomes are considered: scientific publications, classified by topic; and pharmaceutical innovation, measured by drugs entering into clinical development to treat specific diseases.  Planned research also includes refinement of existing economic theory to produce normative evaluations of the allocation of public research spending.<br/><br/>Broader Impacts: This research will inform key policy questions related to federal funding of biomedical research. First, fiscal austerity requires careful attention to the nation's research portfolio and investments. This project will describe and evaluate the productivity of those investments, as a first step towards policy recommendations for rebalancing the portfolio, to maximize society's expected return on investment. Second, if NIH funding for basic research spurs increased pharmaceutical innovation, NIH possesses an important policy tool to promote pharmaceutical R&D in areas of high therapeutic importance or significant health disparity."
"1141488","COLLABORATIVE RESEARCH:  A Multiscale Analysis of Chemotactic Bacteria Transport in Heterogeneous Porous Media","EAR","Hydrologic Sciences","02/01/2012","01/30/2012","Brian Wood","OR","Oregon State University","Standard Grant","Thomas Torgersen","01/31/2016","$290,108.00","","brian.wood@orst.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","GEO","1579","","$0.00","COLLABORATIVE RESEARCH: A MULTISCALE ANALYSIS OF THE TRANSPORT OF CHEMOTACTIC BACTERIA IN HETEROGENEOUS POROUS MEDIA<br/><br/>Roseanne M. Ford Department of Chemical Engineering, University of Virginia<br/>Brian D. Wood  School of Chemical, Biological, and Environmental Engineering, Oregon State University<br/><br/>Chemotaxis is the ability of motile bacteria to sense chemical concentration gradients in their local surroundings and swim toward higher concentrations of pollutants that they degrade. In the subsurface, <br/>bioremediation is often hindered by the inability to achieve good mixing between injected substances and <br/>the resident contaminants. In such situations, chemotaxis might be exploited to enhance the mixing of <br/>bacterial populations within contaminated zones. The goal of this study is to connect direct experimental measurement of chemotaxis in physically and chemically heterogeneous porous media with appropriate mechanistic descriptive theory. Two main hypotheses will be used to organize the research effort. They will combine both (a) multiscale experimental observations, and (b) a multiscale upscaling analysis to develop a theoretical framework that can describe chemotaxis in chemically and physically heterogenous media at microscopic and macroscopic scales. The first hypotheses is that in porous media with trapped sources of nonaqueous phase liquid (NAPL) pollutants, chemotactic bacteria will have measurably slower transport than non-chemotactic bacteria.  In particular, chemotactic bacteria will exhibit greater retardation and tailing than will non-chemotactic mutants because of selective trapping of chemotactic bacteria at the fluid-NAPL interfaces. The second hypothesis is that transport of bacteria under chemotactic versus non-chemotactic conditions are dramatically different in the presence of large-scale heterogeneities.  Although chemotaxis is inherently a pore-scale process, the net influence of chemotactic bacteria on transport from high to low conductivity regions of a heterogeneous medium will be experimentally measurable, and will have relevance to bacterial transport in the field. This project will combine innovative experimental designs for data collection and upscaling to develop multiscale models for chemotaxis. <br/><br/>The purpose of this research is to develop an understanding of the macroscopic scale (bulk) transport behavior of chemotaxis in porous media.  This necessarily involves linking the macroscale transport to the essential microscale features of the system that control chemotaxis. Improvements in the ability to measure phenomena at small scales has lead to experimental data sets that contain detail that was nearly unimaginable even a decade ago. The development of such extraordinary data sets has also frequently promulgated the question ?how does one make sense of these data? Is there a way to search for essential features of behavior in them?? New archetypes for data analysis (e.g., machine learning algorithms, data mining) have been employed as methods to assess such data sets. When applied to the problem of large data sets arising from physical systems, upscaling methods are among these data analysis archetypes. Outcomes from this proposal will result in more robust models for predicting microbial transport in groundwater systems, which will lead to improved design and implementation of bioremediation schemes."
"1208339","Dynamical, computational and statistical aspects of stochastic processes on networks","DMS","PROBABILITY","08/01/2012","07/12/2012","Allan Sly","CA","University of California-Berkeley","Standard Grant","Tomek Bartoszynski","07/31/2015","$179,649.00","","allansly@princeton.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1263","","$0.00","The main theme of this proposal is the development of new theory and applications in the study of Markov random fields (MRFs) and other stochastic processes on networks.  One aspect involves studying dynamical processes on such systems, particularly the Glauber dynamics Markov chain. It will consider the question of universality of the cutoff phenomena at high temperatures as well pursuing a better understanding of the dynamics in the low temperature regime.  A second theme is the development of new tools for estimating the partition function of MRFs on locally tree-like graphs and inferring their local distributions. Understanding these distributions will enhance the understanding of links between phase transitions and transitions in the computational complexity of combinatorial counting, sampling and randomized optimization problems. The final topic of the proposal is to apply probabilistic theory and results for MRFs to widely used statistical estimators from phylogenetic reconstruction in computational biology and to develop new methods for detection of communities within social and other networks.<br/><br/><br/>Markov random fields (MRFs) are models of random processes on networks which play a major role in a wide range of areas including mathematical physics, machine learning and statistics, theoretical computer science and computational evolutionary biology.  This proposal seeks to develop new theory for MRFs to better understand models, algorithms and estimators and other probabilistic techniques in these areas.  It considers the problem of how much data is needed for standard statistical procedures, such as maximum likelihood estimation, to recover the underlying evolutionary tree for a collection of species.  The proposal addresses questions from computer science about when combinatorial counting problems, such as counting the number of independent sets in a graph, become computationally intractable and how is it related to the transitions of MRFs on trees.   It also studies questions of how long it takes random processes on networks, such as the Glauber dynamics Markov chain, to reach their equilibrium distribution and how does this depend on the local interactions of the model.    Each of these questions is related to probabilistic properties of MRFs and their transitions between different phases.  The proposal focuses on development of mathematical theory with a view to better understanding these problems."
"1158699","Collaborative Research: Balancing the Portfolio: Efficiency and Productivity of Federal Biomedical R&D Funding","SMA","SCIENCE OF SCIENCE POLICY, SciSIP Infrastructure","08/15/2012","07/27/2013","David Newman","CA","University of California-Irvine","Standard Grant","maryann feldman","07/31/2016","$297,331.00","","newman@uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","SBE","7626, 8075","7433, 7626, 9139","$0.00","This cross-disciplinary, cross-institution collaborative research project combines economic analysis with state-of-the-art methods from statistical machine learning, to assess the relative efficiency and efficacy of research and development expenditures across the U.S. National Institutes of Health (NIH) portfolio of extramural projects. The novel combination of econometrics, topic modeling, and document classification permits analysis of massive collections of grant abstracts and scientific publications, identification of latent research topics present in NIH-funded research, assessment of possible spillover effects across research topics, and evaluation of causal linkages between changes in NIH funding by research topic and scientific advances. Two specific research outcomes are considered: scientific publications, classified by topic; and pharmaceutical innovation, measured by drugs entering into clinical development to treat specific diseases.  Planned research also includes refinement of existing economic theory to produce normative evaluations of the allocation of public research spending.<br/><br/>Broader Impacts: This research will inform key policy questions related to federal funding of biomedical research. First, fiscal austerity requires careful attention to the nation's research portfolio and investments. This project will describe and evaluate the productivity of those investments, as a first step towards policy recommendations for rebalancing the portfolio, to maximize society's expected return on investment. Second, if NIH funding for basic research spurs increased pharmaceutical innovation, NIH possesses an important policy tool to promote pharmaceutical R&D in areas of high therapeutic importance or significant health disparity."
"1206522","First Conference of the International Society for NonParametric Statistics","DMS","STATISTICS","03/01/2012","02/14/2012","Dimitris Politis","CA","University of California-San Diego","Standard Grant","Gabor Szekely","02/28/2013","$18,000.00","","dpolitis@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","MPS","1269","7556","$0.00","A 5-day international conference,``The First Conference of the International Society for NonParametric Statistics'' is being planned to be held in Halkidiki, Greece, June 15-19, 2012.<br/><br/>NonParametric Statistics is not a new subject. Nevertheless, it has become more prominent in the 21st century since data have become abundant, and the computing power to analyze them is generally available. The aim of the PI and his collaborators is to bring together researchers from the U.S., Europe and other parts of the world in order to foster research on nonparametric inference with special emphasis on emerging fields such as:<br/>a) Nonparametric methodology for high dimensional data.<br/>b) Bootstrap, Empirical Likelihood, and related methods.<br/>c) Data depth and and related multivariate nonparametric methods.<br/>d) Dimension reduction, model selection, and related methods.<br/>e) Nonparametric machine learning.<br/>f) Nonparametric models for high dimensional mixed and random effects designs. <br/>g) Empirical process and semiparametrics.<br/>h) Bayesian Nonparametrics.<br/>i) Nonparametric methods for time series and Econometrics.<br/>j) Nonparametric methods for spatial processes.<br/>k) Wavelets and nonparametric functional estimation.<br/>l) Nonstandard Asymptotics in nonparametric inference.<br/><br/>The purpose of the conference is to facilitate the exchange of research ideas on all actively growing areas of nonparametric statistics, thereb giving the development of this important field of statistics a timely boost. This will also be the first conference of the newly formed International Society for NonParametric Statistics (ISNPS) whose mission is ""to foster the research and practice of nonparametric statistics, and to promote the dissemination of new developments in the field via conferences, books and journal publications."" ISNPS has a distinguished Advisory Committee that includes R.Beran, P.Bickel, R. Carroll, D. Cook, P. Hall, W. Hardle, R. Johnson, B. Lindsay, E. Parzen, P. Robinson, M. Rosenblatt, G. Roussas, T. SubbaRao, and G. Wahba, as well as a Charting Committee consisting of over 50 prominent researchers from all over the world. In particular, theprogram will provide a forum for junior and senior researchers alike to gain valuable experience interacting with leaders in their respective sub-areas,and for building new collaborations between researchers in Nonparametric Statistics from the US with researchers from all over the world."
"1141400","Collaborative Research:  A Multiscale Analysis of Chemotactic Bacteria Transport in Heterogeneous Porous Media","EAR","HYDROLOGIC SCIENCES","02/01/2012","01/30/2012","Roseanne Ford","VA","University of Virginia Main Campus","Standard Grant","Thomas Torgersen","07/31/2017","$303,045.00","","rmf3f@virginia.edu","P.O.  BOX 400195","CHARLOTTESVILLE","VA","229044195","4349244270","GEO","1579","","$0.00","COLLABORATIVE RESEARCH: A MULTISCALE ANALYSIS OF THE TRANSPORT OF CHEMOTACTIC BACTERIA IN HETEROGENEOUS POROUS MEDIA<br/><br/>Roseanne M. Ford Department of Chemical Engineering, University of Virginia<br/>Brian D. Wood  School of Chemical, Biological, and Environmental Engineering, Oregon State University<br/><br/>Chemotaxis is the ability of motile bacteria to sense chemical concentration gradients in their local surroundings and swim toward higher concentrations of pollutants that they degrade. In the subsurface, <br/>bioremediation is often hindered by the inability to achieve good mixing between injected substances and <br/>the resident contaminants. In such situations, chemotaxis might be exploited to enhance the mixing of <br/>bacterial populations within contaminated zones. The goal of this study is to connect direct experimental measurement of chemotaxis in physically and chemically heterogeneous porous media with appropriate mechanistic descriptive theory. Two main hypotheses will be used to organize the research effort. They will combine both (a) multiscale experimental observations, and (b) a multiscale upscaling analysis to develop a theoretical framework that can describe chemotaxis in chemically and physically heterogenous media at microscopic and macroscopic scales. The first hypotheses is that in porous media with trapped sources of nonaqueous phase liquid (NAPL) pollutants, chemotactic bacteria will have measurably slower transport than non-chemotactic bacteria.  In particular, chemotactic bacteria will exhibit greater retardation and tailing than will non-chemotactic mutants because of selective trapping of chemotactic bacteria at the fluid-NAPL interfaces. The second hypothesis is that transport of bacteria under chemotactic versus non-chemotactic conditions are dramatically different in the presence of large-scale heterogeneities.  Although chemotaxis is inherently a pore-scale process, the net influence of chemotactic bacteria on transport from high to low conductivity regions of a heterogeneous medium will be experimentally measurable, and will have relevance to bacterial transport in the field. This project will combine innovative experimental designs for data collection and upscaling to develop multiscale models for chemotaxis. <br/><br/>The purpose of this research is to develop an understanding of the macroscopic scale (bulk) transport behavior of chemotaxis in porous media.  This necessarily involves linking the macroscale transport to the essential microscale features of the system that control chemotaxis. Improvements in the ability to measure phenomena at small scales has lead to experimental data sets that contain detail that was nearly unimaginable even a decade ago. The development of such extraordinary data sets has also frequently promulgated the question ?how does one make sense of these data? Is there a way to search for essential features of behavior in them?? New archetypes for data analysis (e.g., machine learning algorithms, data mining) have been employed as methods to assess such data sets. When applied to the problem of large data sets arising from physical systems, upscaling methods are among these data analysis archetypes. Outcomes from this proposal will result in more robust models for predicting microbial transport in groundwater systems, which will lead to improved design and implementation of bioremediation schemes."
"1218981","III: Small: MapReduce Workload Management","CNS","SPECIAL PROJECTS - CISE, Computer Systems Research (CSR","09/01/2012","05/07/2014","Shivnath Babu","NC","Duke University","Standard Grant","M. Mimi McClure","08/31/2016","$315,971.00","","shivnath@cs.duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","1714, 7354","7923, 9178, 9251","$0.00","Researchers and decision makers in diverse fields such as <br/>fraud detection, genome sequencing, and datacenter<br/>management need to process many terabytes of data every day. <br/>Many fields are turning to MapReduce systems to process such <br/>growing datasets. Consequently, the relatively young MapReduce<br/>ecosystem has to support complex workloads that include <br/>declarative queries for report generation, MapReduce <br/>programs for machine learning tasks, and large job workflows.<br/>Furthermore, elastic and pay-as-you-go cloud platforms pose novel <br/>challenges and opportunities for MapReduce workload management.<br/><br/>This project is building the Hadoop AutoAdmin system for <br/>automating MapReduce workload management. To the PI's knowledge, <br/>Hadoop AutoAdmin is the first system to address this challenging <br/>problem that will become increasingly important as a broad class <br/>of users adopt MapReduce. Hadoop AutoAdmin has three research <br/>thrusts. The first thrust is to understand and characterize the <br/>behavior of MapReduce workloads based on a comprehensive empirical <br/>study involving workloads and data from multiple application domains <br/>as well as different cluster configurations on the cloud. The second <br/>thrust is to develop an easy-to-use and efficient warehouse to <br/>store, retrieve, and visualize the diverse forms of workload <br/>monitoring data. The models and insights from these activities will <br/>drive the third thrust of developing end-to-end algorithms for <br/>workload management.<br/><br/>This project can have significant impact in areas of national <br/>importance like security and healthcare that are inundated with <br/>data. Hadoop AutoAdmin will improve worker productivity, system <br/>utilization, and cost-effectiveness of cloud platforms. The <br/>technical contributions will be disseminated broadly and the <br/>system released publicly."
"1205296","Regularization for High Dimensional Inference and Sparse Recovery","DMS","STATISTICS","09/01/2012","07/30/2014","Jelena Bradic","CA","University of California-San Diego","Continuing grant","Gabor J. Szekely","08/31/2015","$120,000.00","","jbradic@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","MPS","1269","","$0.00","This proposal aims at answering pertinent questions to identifying sparse subsets of high dimensional covariate spaces, in the context of regularization methods, when simple least square loss functions are not well suited. The broad goal is to understand the fundamental interactions between nonlinear and/or censored structure of the statistical model, the regularization scheme and the intrinsic dimensionality of the problem. Specifically, the PI aims at (1) Identifying new statistical problems and regularization schemes, with complex nonlinear and time-to-event structure with high dimensional covariate space that are tuned to the characteristics of the data; (2) Developing novel non-asymptotic oracle bounds on the behavior of regularized estimators where techniques of random matrix theory, especially high probability bounds on various matrix norms, and approximation theory, will be utilized to enhance understanding of the effects of dimensionality on the non-asymptotic properties; (3) Investigating  new non-asymptotic  bounds on risk of  semiparametric methods  where the statistical model is possibly misspecified; (4) Analyzing and developing models that use special interplay between censoring rate, sample size and dimensionality of the problem and importantly  (5) Introducing new algorithms that optimally and efficiently solve the investigated large scale problems.<br/><br/><br/>Explosion of microarray technologies has lead to vast number of large-scale   genome-wide association studies where simultaneous analysis of a large number of SNPs is pertinent to discovering genetic identification of complex diseases. Presence and importance of time to event component calls for significant advances in statistical methodology for both NP dimensionality and censored structure. This research proposal aims at developing innovative and effective statistical methods for such complex data with special impact in genetic, public health and bioinformatic sciences, where censoring and vast number of gene interactions make identification of misbehaving genes very difficult. Moreover, the developments of this inter-disciplinary project will enhance new scientific discoveries, make new collaborative connections with practitioners and will promote teaching and training of graduate students on the contemporary state-of-the-art machine learning techniques applied to semiparametric models and censored data. To promote the progress of science, PI will make explicit collaborations of department of Mathematics, Biostatistics division of the Medical School at UCSD and Supercomputer Center in San Diego. Through dissemination of the results of this proposal, PI plans to expose biology to mathematics majors and promote science among underrepresented groups and women in mathematics."
"1205546","A complete sufficient dimension folding theory with novel methods","DMS","STATISTICS","08/01/2012","07/20/2012","Xiangrong Yin","GA","University of Georgia Research Foundation Inc","Standard Grant","Gabor J. Szekely","07/31/2015","$110,000.00","","yinxiangrong@uky.edu","310 East Campus Rd","ATHENS","GA","306021589","7065425939","MPS","1269","","$0.00","This proposal is aimed at developing a general formulation and the related methods for sufficient dimension folding where predictors are matrix-/array- valued, and where a specific functional (or parameter) of the conditional distribution is of interest. The past two decades have seen vigorous development of the sufficient dimension reduction methods for vector-valued predictors, and have accrued a striking record of their successful applications. However, many data are matrix-/array-valued, sufficient dimension reduction for vector-valued predictors applying to such data will lose its sufficiency and structure, resulting difficulties in interpretation, and to a large extent these methods treat the conditional distribution as the object of interest, without discriminating between parameter of interest and nuisance parameter. The investigator proposes a new paradigm for sufficient dimension folding for matrix-/array-valued predictors that focuses on a functional of the conditional distribution, which can be any one in a very wide class that covers most of applications. In addition, the investigator proposes to develop a coherent collection of associated techniques for estimation, computation, and asymptotic inference. <br/><br/>Recently, high throughput technologies that produce massive amount of complex and high-dimensional data are increasingly prevalent in such diverse areas as business, government administration, environmental studies, machine learning, and bioinformatics. These provide considerable momentum in the Statistics community to develop new theories and methodologies, that are capable of discovering critical evidence from high-dimensional, complex structural and massive data. Sufficient Dimension Folding is a new area of statistical research that arose amidst, and has been propelled by, these new demands. The investigator proposes to formulate the theories and methodologies of sufficient dimension folding so that they can be specifically tailored to target to be estimated. This new paradigm not only synthesizes, broadens, and deepens the recent advances in sufficient dimension folding, but brings the understanding of sufficient dimension folding on a par with classical statistical inference theory, by following the tradition of sufficiency, efficiency, information, parameter of interests, and nuisance parameters, which are the key ideas that had helped to propel classical inference to its maturity."
"1218145","AF: Small: RUI: A new and improved algorithm for fitting RNA backbone in crystallographic data","CCF","ALGORITHMIC FOUNDATIONS","08/01/2012","07/18/2012","Xueyi Wang","ID","Northwest Nazarene College","Standard Grant","Mitra Basu","07/31/2014","$187,934.00","","xwang@nnu.edu","623 Holly","Nampa","ID","836865897","2084678011","CSE","7796","7923, 7931, 9150","$0.00","Accurate details in 3D structures of RNA molecules are important for understanding RNA function, which can in turn help us understand biological systems, develop new medicines, and improve human health. One issue in RNA 3D structure analysis is that the structures obtained from biological experiments often contain errors and need to be corrected. The main reason for the errors is because RNA 3D structures are highly complex. While there are existing automatic tools for obtaining protein 3D structures from experimental data, such tools are not yet available for RNAs.<br/><br/>Previously the PI developed a program called RNA Backbone Correction (RNABC) that uses geometric algorithms and robotics to search for error-free RNA structures. While RNABC has been used to correct structural errors in existing RNA structures and has been integrated into the MolProbity web service for RNA structure validation, further advancement is needed. Research shows that RNABC corrects errors in 40% to 80% of RNA structures tested.<br/><br/>This project develops a new and improved computer program that scientists can use to correct structural errors and obtain accurate details of RNA 3D structures. The goal is to correct errors in over 80% of RNA structures and maintain the same running time. The new program will combine methods in machine learning, data mining, robotics and numerical analysis to search for error-free RNA structures. The project will advance algorithmic research in multiple ways and help us better understand the details of RNA structures. This project provides research opportunities to students interested in computer science, mathematics, and biology, and helps educate the next generation of scientists."
"1234451","Prognostic Methods for Battery Management Systems","CMMI","DYNAMICAL SYSTEMS","09/01/2012","11/15/2013","Michael Pecht","MD","University of Maryland College Park","Standard Grant","Massimo Ruzzene","08/31/2015","$240,000.00","Chaochao Chen","pecht@umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","ENG","7478","034E, 035E, 8024","$0.00","The objective of this research is to develop prognostic methods for improving the safety and availability of battery-powered systems such as electric vehicles and unmanned aerial vehicles. Battery-powered systems suffer from two problems. The first is ""range anxiety"" problem, which refers to the fear of running out of battery power during vehicle operation. The second problem is related to the safety of a battery pack, which can rupture or even explode under certain conditions. To address these problems, an approach for predicting the (1) end-of-discharge (the time at which a battery will run out of electrical charge), and (2) remaining-useful-performance of a battery with a known level of confidence will be developed. This approach involved machine learning algorithms to address future loading conditions, unit-to-unit variations, and modeling uncertainties. <br/><br/>This research will improve the operation readiness and safety of battery-powered systems that are used in applications ranging from commercial (electric vehicles) to defense (unmanned aerial vehicles) sectors. In 2011, President Obama announced his goal of having one million electric vehicles on the road by 2015. The proposed research will make significant contributions to reaching this target, since it can ease user concern about the safety and reliability of electric vehicles by providing robust battery state and health information in real-time, thus encouraging their widespread use. As a result, it will also decrease US dependence on foreign oil and reduce the emission of greenhouse gases. The content of this research will assist in the advancement of prognostics and health management techniques which will be disseminated to the engineering community through online graduate courses, seminars, workshops, and short courses thereby benefiting people from academia, industry and the military."
