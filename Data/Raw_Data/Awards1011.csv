"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"1005411","Cross-Cutting Research Workshops on Intelligent Information Systems","IIS","Info Integration & Informatics, Robust Intelligence","09/15/2010","06/29/2015","Sanjeev Khudanpur","MD","Johns Hopkins University","Continuing grant","Tatiana Korelsky","09/30/2015","$599,972.00","Gregory Hager, Sanjeev Khudanpur, David Yarowsky, Rene Vidal","khudanpur@jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","CSE","7364, 7495","7364, 7495, 7556","$0.00","A series of annual research workshops on Intelligent Information Systems, centered on machine learning for speech, language and vision technologies, are being organized at Johns Hopkins University to bring together diverse ?dream teams? of leading professionals, graduate students, and undergraduates, in a truly cooperative, intensive, and substantive effort to advance the state of the science.<br/>The primary goals of the proposed workshop series are to develop machine learning principles applicable to a broad spectrum of intelligent systems, to attract students to the field and to prepare them for research by putting them to work on exciting problems alongside senior researchers in a highly collaborative environment.  Creation of research infrastructure and lasting collaborations are secondary goals.<br/>An open call for workshop project proposals is being issued each year to researchers in the worldwide IIS community. Received proposals are competitively evaluated and cooperatively refined at interactive peer review meetings, where project proponents, government representatives, and experts from related fields meet to assess their scientific merit, viability and potential impact. The graduate students attending the workshop are familiar with the field and are selected in accordance with their demonstrated performance. The undergraduates are entering seniors who are new to the field and who have shown outstanding academic promise; they are selected through a national search. The participation of undergraduates in these research programs encourages talented young scholars to pursue graduate studies in IIS.<br/>By the end of this 3-year workshop series (beginning 2010), more than a hundred individuals will have conducted intensive collaborative research: about 30 academic and industry researchers, 20 researchers from government and national laboratories, 30 graduate students, and 20 undergraduates. Additional benefits of the workshops will be the collection or creation, and dissemination of valuable tools and data for IIS research, the establishment of fruitful and long-lasting collaborations, and the cross-fertilization of ideas among the participants."
"0958576","II-NEW: Acquiring infrastructure for Artificial Intelligence, Natural Language Processing and Information Retrieval","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","05/01/2010","03/03/2010","Jugal Kalita","CO","University of Colorado at Colorado Springs","Standard Grant","Vijayalakshmi Atluri","04/30/2012","$195,051.00","Lisa Hines, Carmen Stavrositu","jkalita@uccs.edu","1420, Austin Bluffs Parkway","Colorado Springs","CO","809183733","7192553153","CSE","7359","9218, HPCC","$0.00","This proposal seeks to obtain funding to acquire computing<br/>infrastructure to perform cross-disciplinary research in artificial<br/>intelligence, natural language processing, bioinformatics, social<br/>networks and related fields. In particular, the projects that will<br/>be supported by the infrastructure acquired include developing algorithms<br/>and architectures for large-scale ontology alignment, particularly in<br/>the biomedical domain; mining Wikipedia and similar Web-based sources<br/>for geographical, temporal and ontological information; performing<br/>named entity recognition in biomedical and other domains; performing<br/>computational motivational and content analysis of socially generated<br/>content such a blogs and micro-blogs; undertaking corpus-based computational<br/>linguistics research in under-studied and possibly endangered languages from<br/>India and other locations; developing better-performing algorithms for gene<br/>expression analysis; and developing, implementing and comparing algorithms<br/>for protein structure prediction, particularly proteins that contain<br/>coiled-coil structures. These projects deal with large amounts of data<br/>and information and processing such data and information requires large<br/>amounts of computing power. Our proposal seeks to acquire adequate and<br/>flexible computing hardware to facilitate problem-solving in these and other<br/>areas, so that current and future problems can be solved felicitously. <br/><br/>The infrastructure acquired will enable cutting-edge research by Ph.D.,<br/>Masters and undergraduate students, including REU site students, in a<br/>variety of cross-disciplinary topics that employ ideas and innovations<br/>in artificial intelligence, machine learning and information retrieval.<br/>For example, the results of our research may enable creations of systems<br/>that discover overlaps and matches among large medical ontologies so that<br/>painstakingly created domain-specific information can be fused, compared<br/>and utilized better; and may assist in creating programs that assist in<br/>automatically understanding and/or visualizing content of socially-generated<br/>Websites such as Wikipedia and Twitter.<br/><br/>For further information see the project web site at the URL:<br/>http://www.cs.uccs.edu/~kalita."
"1025375","EAAI-10: The First Symposium on Educational Advances in Artificial Intelligence; July 2010; Atlanta, Georgia","IIS","ROBUST INTELLIGENCE","05/15/2010","05/05/2010","Kiri Wagstaff","CA","Association for the Advancement of Artificial Intelligence","Standard Grant","Edwina L. Rissland","04/30/2011","$17,000.00","Mehran Sahami","Kiri.Wagstaff@jpl.nasa.gov","2275 E BAYSHORE RD STE 160","East Palo Alto","CA","943033224","6503283123","CSE","7495","7495","$0.00","This award supports participants to EAAI-10: The First Symposium on Educational Advances in Artificial Intelligence. EAAI-10 provides a venue for researchers and educators to discuss pedagogical issues and share resources related to teaching artificial intelligence (AI) and using AI in education across a variety of curricular levels (K-12 through postgraduate training), with a natural emphasis on undergraduate and graduate teaching and learning. The symposium will seek and disseminate contributions showing how to more effectively teach AI, as well as how themes from AI may be used to enhance education more broadly. Examples of this kind of broad AI impact include its use to motivate and inspire students in introductory computing courses or as a means for fostering computational thinking. We encourage the sharing of innovative educational approaches that convey or leverage AI and its many subfields, such as robotics, machine learning, natural language, and computer vision."
"0956881","SBIR Phase II: iGlasses: An Appliance for Improving Speech Understanding in Face-to-Face Communication and Classroom Situations","IIP","SMALL BUSINESS PHASE II","02/01/2010","06/10/2010","Michael Cohen","CA","Animated Speech Corporation","Standard Grant","Glenn H. Larsen","01/31/2012","$561,843.00","","michael@animatedspeech.com","851 Burlway Road","Burlingame","CA","940101715","8182122913","ENG","5373","115E, 1658, 5373, 7218, 9216, 9231, 9251, 9261, HPCC","$0.00","This Small Business Innovation Research (SBIR) Phase II project will complete the development of technology to supplement ordinary face-to-face language interaction for the millions of individuals who are deaf or hard of hearing or face other speech/language challenges. The goal of the project is to enable such individuals to fully participate in the spoken language community. The need for language and speech intelligibility aids is pervasive in today's world. Millions of individuals live with language and speech challenges (such as 36 million Americans with hearing deficits), and these individuals require additional support for communication and language learning. The Phase I research developed and tested the behavioral science and technology for iGlasses. Building on this research, the proposed research is to complete and bring to market an innovative intervention that can bring spoken language and culture into the lives of individuals who are currently marginalized because of hearing loss or other speech/language challenges. The proposed research will advance the state of the art in human machine interaction, speech, machine learning, and assistive technologies. <br/><br/>The broader/commercial impact of this project will benefit the deaf and hard-of-hearing populations as well as the scientific community by providing a research and theoretical foundation for a speech aid that would be naturally available to almost all individuals at a very low cost. It does not require literate users because no written information is presented as would be the case in a captioning system; it is age-independent in that it might be used by toddlers, adolescents, and throughout the lifespan; it is functional for all languages because it is language independent given that all languages share the same phonetic features with highly similar corresponding acoustic characteristics; it would provide significant help for people with hearing aids and cochlear implants; and it would be beneficial for many individuals with language challenges and even for children learning to read. Finally, regardless of the advances or lack of advances in speech recognition technology, it will always be more accurate and effective to pick off the fundamental acoustic features of speech than it is to recognize entire phonemes which are more complex combinations of these basic properties."
"1035151","RAPID: Harvesting Speech Datasets for Linguistic Research on the Web (Digging into Data Challenge)","BCS","Linguistics","08/01/2010","08/29/2010","Mats Rooth","NY","Cornell University","Standard Grant","Joan Maling","07/31/2013","$100,000.00","","mr249@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","SBE","1311","1311, 7914","$0.00","Distinctions of prosody (rhythm, stress, and intonation) are ubiquitous in spoken language. It often seems obvious to a native speakers of English what prosody is most appropriate in a given sentence and context, and researchers in Linguistics and related fields have proposed numerous formalized hypotheses about it. But establishing the validity of these hypotheses is remarkably elusive. Much of the problem is that it is difficult to observe enough examples of a given phenomenon to evaluate hypotheses. The project aims to address this problem of a dearth of data by collecting or ""harvesting"" examples of specific word sequences or word patterns from web sources. It is often possible to find hundreds or thousands of examples of people using the very same word pattern. If these examples are collected together into a dataset and made available to the research community, it will be possible to evaluate theories about the form and meaning of prosody on an unprecedented scale. Scaling up available data can be expected to have a transformative effect on our understanding of prosody.<br/><br/>Audio and audio-video recordings of spoken language, including podcasts, radio and television broadcasts, lectures, and much else, are pervasive on the web. This does not help in itself, because it is not possible to listen to tens of thousands of hours of speech in order to find a few hundred examples of a certain type. Fortunately, more sites are becoming available that provide text transcriptions obtained with automatic speech recognition (for instance Fox Business News, WNYC, Elections Video Search at Google, and university lectures at MIT). Industry blogs and newsletters indicate that more large sites will come online soon. By searching for a word pattern in the text transcription and subsequently retrieving an audio or video file, it becomes possible to find relevant data. <br/><br/>To construct datasets for prosody research from these web sources, the project team will implement software harvest engines that interact with the web through standard protocols. Datasets for eight to twelve specific phenomena will be collected. In order to demonstrate the impact of a data-intensive  methodology, the samples will be analyzed using techniques of statistics and formal linguistics. For instance, an approach known as machine learning classification will be used to identify the specific features of the sound signal (such as pitch, vowel duration, and intensity) that are responsible for the perception of prosody.<br/><br/>Prosody and intonation play an important role in making the discourse coherent, in signaling what part of the communicated information is foregrounded and backgrounded, and disambiguating speaker intention. Any advancement in understanding prosody will not only deepen our understanding of the human language capability, it also has implications in a wide range of areas, including language instruction, translation studies, speech therapy, improving comprehensibility of synthesized speech, and improving speech recognition systems."
"1014908","CIF:  Small:  Algorithms, Performance and Design for Sparsity-Enforced Learning","CCF","Comm & Information Foundations, SIGNAL PROCESSING","08/01/2010","05/03/2012","Arye Nehorai","MO","Washington University","Standard Grant","John Cozzens","07/31/2013","$327,217.00","","nehorai@ese.wustl.edu","CAMPUS BOX 1054","Saint Louis","MO","631304862","3147474134","CSE","7797, 7936","7936, 9218, 9251, HPCC","$0.00","Supervised learning is used to infer an unknown regression function from a set of training data. Applications include time-series prediction, remote sensing, medical image analysis, computer vision, face detection, text categorization, image scene classification, speech recognition, and bioinformatics. Existing learning algorithms have several drawbacks. For example, the set of basis functions usually involves unknown parameters that need to be determined empirically. The optimization problem defined by the learning task depends on a trade-off parameter that requires careful selection. Analytical performance of the existing learning methods is not well studied and the role of the set of basis functions is not yet clear. In addition, currently there is no effective way to select the set of basis functions.<br/><br/>The investigator develops a new framework of sparsity-enforced learning for regression function learning to overcome the drawbacks of existing approaches. This framework includes numerical algorithms for sparse vector estimation, tight bounds for performance analysis, and optimization procedures for dictionary design. A flexible form for the inferred function is a linear combination of basis functions, which are constructed by discretizing the unknown parameters. Algorithms are designed to learn the weight vector by convex sparsity enforcing, hierarchical Bayesian modeling and normalized maximum likelihood principle. The discretization and the sparsity enforcement enable automatic selection of most relevant basis functions with appropriate parameters. The investigator analyzes the performance of the learning framework by deriving tight bounds on the mean-squares error of the learned weights, in particular, the Cramer-Rao bound and Hammersley-Chapman-Rob bins bound. The performance bounds (and their simplifications) are employed to optimally design the overcomplete dictionary. The investigator uses the developed framework to model time series and extract knowledge from images."
"0958578","CI-P: The ""Poor Quality"" Meetings Corpus","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","08/01/2010","07/19/2010","Adam Janin","CA","International Computer Science Institute","Standard Grant","Tatiana Korelsky","07/31/2013","$99,308.00","","janin@icsi.berkeley.edu","1947 CENTER ST STE 600","Berkeley","CA","947044115","5106662900","CSE","7359","9218, HPCC","$0.00","Go to any meeting or lecture with the younger generation of researchers, business people, or government, and there is a laptop or smartphone at every seat. Each laptop and smartphone is capable not only of recording and transmitting video and audio in real time, but also of advanced analytics on the data (e.g. speech recognition, speaker identification, face detection, etc.). Yet this rich resource goes largely unexploited mostly due to lack of good training data for machine learning algorithms.<br/><br/>The first step in exploiting this resource is to collect a corpus of audio, video, and annotations of ""natural"" meetings using the participants' own laptops and cellphones, allowing both analysis of the meetings and training of machine learning algorithms. This one year planning project involves design of the corpus, including collection protocols, signals, formats, and annotations, collection of a small pilot corpus, and ongoing interaction with the community through mailing lists, forums, wikis, and a workshop hosted at ICSI in Berkeley, California. The annotations include the words spoken, events such as laughter, a telephone ringing, or a new participant entering the room, who is speaking, who appears on which camera, head and hand gestures, the participants' focus of attention, and summaries and topics.<br/><br/>Successful planning for the collection of a significant number natural meetings from a variety of settings using the participants' own laptops and cellphones allows for a new generation of analytic tools for meetings including browsing, search, collaboration tools, and teleremote aids."
"0957742","Collaborative Research:EAGER:Deep Architectures for Speech and Audio Processing","IIS","Robust Intelligence","01/01/2010","09/16/2009","Fei Sha","CA","University of Southern California","Standard Grant","Edwina L. Rissland","12/31/2011","$50,000.00","","feisha@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7495","7495, 7916, 9215, HPCC","$0.00","Recent studies have demonstrated the powerful abilities of deep <br/>architectures for statistical pattern recognition.  Deep <br/>architectures transform their inputs through multiple layers of <br/>nonlinear processing.  Inspired by the connectivity of biological <br/>neural networks, the hidden layers of deep architectures encode <br/>hierarchical, distributed representations of complex sensory input.  <br/>Theoretical results suggest that such representations are needed to <br/>solve the most difficult problems of artificial intelligence.<br/><br/>Previous applications of deep architectures include visual object <br/>recognition, statistical language modeling, and nonlinear <br/>dimensionality reduction.  Building on these successes, this project <br/>develops new applications of deep architectures for problems in <br/>speech and audio processing.  Current front ends for these problems <br/>are dominated by traditional methods in statistical modeling and <br/>signal processing.  Deep architectures have the potential to <br/>overcome many limitations of current approaches.<br/><br/>This project has two research components with interrelated and <br/>overlapping goals.  The project's first component explores <br/>unsupervised learning in convolutional neural networks.  <br/>The goal of learning in these networks is to discover new <br/>features for audio event detection and automatic speech <br/>recognition.  The project's second component investigates <br/>the possibility of deep learning in kernel machines.  <br/>This possibility is suggested by a recently discovered family <br/>of kernel functions that mimic the computation in large, <br/>multilayer networks.<br/><br/>The project's research components are tightly integrated with <br/>its educational activities.   The project supports two graduate <br/>students, including one female student.  An important goal <br/>is to develop publicly available software for use by other <br/>researchers."
"0957560","Collaborative Research:EAGER:Deep Architectures for Speech and Audio Processing","IIS","Robust Intelligence","01/01/2010","09/16/2009","Lawrence Saul","CA","University of California-San Diego","Standard Grant","Edwina L. Rissland","12/31/2011","$50,000.00","","saul@cs.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7495","7495, 7916, 9215, HPCC","$0.00","Recent studies have demonstrated the powerful abilities of deep architectures for statistical pattern recognition. Deep architectures transform their inputs through multiple layers of nonlinear processing. Inspired by the connectivity of biological neural networks, the hidden layers of deep architectures encode hierarchical, distributed representations of complex sensory input. Theoretical results suggest that such representations are needed to solve the most difficult problems of artificial intelligence. <br/><br/>Previous applications of deep architectures include visual object recognition, statistical language modeling, and nonlinear dimensionality reduction. Building on these successes, this project develops new applications of deep architectures for problems in speech and audio processing. Current front ends for these problems are dominated by traditional methods in statistical modeling and signal processing. Deep architectures have the potential to <br/>overcome many limitations of current approaches. <br/><br/>This project has two research components with interrelated and overlapping goals. The project's first component explores unsupervised learning in convolutional neural networks. The goal of learning in these networks is to discover new features for audio event detection and automatic speech <br/>recognition. The project's second component investigates the possibility of deep learning in kernel machines. This possibility is suggested by a recently discovered family of kernel functions that mimic the computation in large, <br/>multilayer networks. <br/><br/>The project's research components are tightly integrated with its educational activities. The project supports two graduate students, including one female student. An important goal is to develop publicly available software for use by other researchers."
"1007962","Emerging Research-Empirical Research--An Integrated Model of Cognitive and Affective Scaffolding for Intelligent Tutoring Systems","DRL","REAL","09/15/2010","07/26/2012","James Lester","NC","North Carolina State University","Continuing grant","Finbarr Sloane","08/31/2014","$1,542,275.00","Eric Wiebe, Kristy Boyer","lester@csc.ncsu.edu","2701 Sullivan DR STE 240","Raleigh","NC","276950001","9195152444","EHR","7625","9177, 9251, SMET","$0.00","One-on-one human tutoring is remarkably effective.  Seminal studies have shown that tutoring is significantly more effective than group instruction and may provide unparalleled opportunities for learning.  A central, unanswered research question is, ""How do expert tutors provide effective cognitive and motivational support over the course of long-term tutorial interactions to improve learning?"" With a curricular focus of college-level computer science education, this project will see the design and evaluation of a computer-based intelligent tutoring system, JavaTutor, which leverages artificial intelligence to provide both cognitive and motivational support.  The project will be conducted at North Carolina State University in conjunction with three partner institutions: Meredith College, Shaw University, and St.  Augustine's College.  <br/><br/>The project has three major thrusts.  First, the research team will conduct a semester-long observational study of cognitive and affective tutorial support provided by expert human tutors interacting with students in a fully-instrumented online tutoring environment.  The environment will log all tutorial conversations, problem-solving traces, and affective data streams including physiological signals, posture, and facial expressions.  Second, the research team will develop an empirically grounded, integrated model of cognitive and affective scaffolding using machine learning techniques including hidden Markov modeling.  Third, they will validate the integrated model of cognitive and affective scaffolding in a semester-long experiment with the JavaTutor intelligent tutoring system.  Four versions of the JavaTutor system will be deployed and compared.  It is hypothesized that over the course of a semester, the version with an integrated model of cognitive and motivational scaffolding will outperform each of the other models on both cognitive and affective student outcomes and yield differential effects across learner groups, accruing particularly significant benefit to low-performing and female students.<br/><br/>The products of this project include findings and technologies that will inform the future development of intelligent tutoring systems.  By promoting rich learning interactions through integrated cognitive and motivational scaffolding, the project will create new learning environment technologies that promote high levels of achievement and find broad application in STEM education.  It is anticipated that the resulting intelligent tutoring system technologies will serve as a foundation for the next generation of educational software that both complements and expands the impact of classroom teachers.  The impact should be significant given the effectiveness of human tutoring and the potential power of these new technologies to support learning."
"0917417","AF: Small:  Learnability, Randomness, and Lower Bounds","CCF","COMPLEXITY & CRYPTOGRAPHY","05/15/2010","05/19/2010","John Hitchcock","WY","University of Wyoming","Standard Grant","Tracy J. Kimbrel","04/30/2015","$300,000.00","","jhitchco@cs.uwyo.edu","1000 E. University Avenue","Laramie","WY","820712000","3077665320","CSE","7927","9150, 9216, 9217, 9218, HPCC","$0.00","This project is motivated by new connections between the research fields of computational complexity theory and machine learning theory.  Computational complexity theory aims to understand which problems can be solved efficiently on a computer by determining the amounts of computational resources such as CPU time, memory space, or circuit area that are required to solve problems.  At the center of this field is the famous P vs. NP question which impacts virtually every scientific and engineering discipline, given the thousands of diverse NP-complete problems that have been discovered.  Machine learning theory studies the extent to which computers can learn from data and their ability to make future predictions and classifications based on what has been learned.  Some powerful learning algorithms have been discovered, but whether computers can be programmed to accomplish many learning tasks remains an open question.<br/><br/>Both computational complexity and machine learning aim to understand the capabilities and limitations of computation, but the two fields study different types of problems and use different kinds of techniques.  This research will employ techniques and ideas from each of these two fields to impact the other field, specifically with the goal of proving ""lower bound"" results.  This research will be accomplished by making use of a new vantage point provided by algorithmic randomness to relate complexity and learning problems.  Learning algorithms will be utilized to establish lower bounds on the computational resources required to solve problems in computational complexity.  The converse direction will be investigated to apply techniques and ideas from computational complexity to show that ""attribute-efficient"" learning algorithms do not exist for certain concept classes.  Algorithmic randomness and Kolmogorov complexity will be used to improve our understanding of the capabilities and limitations of learning algorithms.<br/><br/>This research will improve our understanding of computational complexity, which is informative to many areas of science and engineering where computation plays a role.  This project aims to better understand what learning tasks can be accomplished efficiently by computers, which has applications to the foundations of artificial intelligence.  In particular, this research will identify new obstacles that must be overcome in order to design successful automatic learning systems.  A greater synergy will be developed between computational complexity theory and machine learning theory, with the benefit of laying a foundation for future collaboration and interdisciplinary work across these fields."
"0958482","II-EN: A compute cluster and software tools for Monte-Carlo methods in artificial intelligence","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","07/01/2010","03/10/2010","Thomas Dietterich","OR","Oregon State University","Standard Grant","Todd Leen","06/30/2014","$600,000.00","Prasad Tadepalli, Alan Fern, Weng-Keen Wong, Kagan Tumer","tgd@cs.orst.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","CSE","7359","9218, HPCC","$0.00","The project supports acquisition of a hardware cluster and development of software frameworks to support Monte Carlo methods in articial intelligence, for tasks such as model compilation through machine learning over the results of Monte Carlo simulation, application of Monte Carlo methods to solve single-agent and multi-agent sequential decision making problems, and integration of machine learning methods into Monte Carlo search to improve real-time decision making. These software frameworks will include implementations of baseline and state-of-the-art algorithms for each task, with a goal of release with generic APIs and open-source availability to make it easy for other researchers to add new methods and to connect external simulators to the frameworks.  <br/><br/>The algorithms and tools have many important applications, including (a) optimization of forest management to jointly minimize the risk of catastrophic fires and maximize biological and economic benefits, (b)design and validation of multiagent control methods for reducing congestion in air traffic control, (c) design and validation of multiagent control methods for micro air vehicles, and (d)modeling spatio-temporal distribution of species to support management of endangered and threatened species. The hardware cluster and software frameworks will be integrated into the undergraduate and graduate curriculum at Oregon State University, as well as various outreach beyond Oregon State."
"0963478","RI: Medium: Collaborative Research: Game Theory Pragmatics","IIS","Robust Intelligence","07/15/2010","07/14/2010","Yoav Shoham","CA","Stanford University","Standard Grant","Hector Munoz-Avila","06/30/2014","$590,272.00","Robert Wilson","shoham@cs.stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","7495","7924","$0.00","The past decade has seen unprecedented interaction between artificial intelligence and game theory, with exciting intellectual problems, technical results, and potentially important applications.  However, this thriving interaction has thus far not challenged in a fundamental way some of the basic assumptions of game theory, to include various forms of equilibrium as the fundamental strategic concept. Equilibrium specifies conditions under which the strategic choices of agents are in some sense stable. Equilibria are clever and beautiful constructs, but they embody strong idealized assumptions and as a result their applicability to complex, realistic games (i.e., formalizable 'social' interactions) is limited. Arguably computer science can provide alternative modeling foundations, or at least significantly contribute to them. <br/><br/>This project explores several complementary directions, to include: alternatives to equilibrium as game solution criteria; replacing analysis of large, complex games with analysis of their abbreviated or approximate versions; using machine-learning techniques to model the extent to which agent behavior is strategic, adaptive, or otherwise intelligent; investigating the role of strategic reasoning in controlled but rich environments, such as Computational Billiards, which involves continuous state and actions spaces as well as control uncertainty. One of the outreach and educational components of this project is organizing and participating in an annual Computational Billiards competition. Applications range from electronic commerce to social networks to peer-to-peer systems to online games, and in general all settings in which individual interests intertwine with computational elements."
"1050168","GV: EAGER: Innovative Analysis and Visualization Approaches for Understanding Model Uncertainty","IIS","GRAPHICS & VISUALIZATION","09/01/2010","12/02/2013","Marie desJardins","MD","University of Maryland Baltimore County","Standard Grant","Maria Zemankova","08/31/2014","$119,299.00","Penny Rheingans","mariedj@cs.umbc.edu","1000 Hilltop Circle","Baltimore","MD","212500002","4104553140","CSE","7453","7453, 7916, 9251","$0.00","This exploratory project strives to develop a new approach to support human understanding of the uncertainty that is inherent in the structure and predictions of complex models.Specifically, the focus in the project is on understanding several types of uncertainty that are associated with model predictions.<br/>Sample uncertainty occurs when regions of the instance space are not well represented in the training data, and predictions are therefore based on sparse information. Model instability occurs when model predictions vary, depending on the training data that was used to construct the model. Prediction variability occurs when a given observation may have noisy attributes, and this input uncertainty leads to uncertainty in the model's predictions. Novel analytical techniques are developed to create meta-models that characterize these three forms of uncertainty. To facilitate user understanding of the nature and distribution of these multiple types of uncertainty across the model space, novel visualization methods represent these meta-models in a display space. Finally, a novel evaluation methodology is used to measure whether, and in what ways, important characteristics of the meta-models are captured in the visualization display space.<br/><br/>This work develops novel techniques in the fields of machine learning and data visualization. Contributions in machine learning include more powerful methods for constructing and analyzing meta-models that characterize multiple types of uncertainty associated with predictive models. Data visualization research focuses on new approaches for representing multi-valued, probabilistic, and complex data, enabling the display of the nature and range of model predictions and uncertainty. An interdisciplinary contribution is the development of a novel methodology for evaluating the quality of model visualizations with respect to the preservation of important model and meta-model characteristics.<br/><br/>The broader impacts of this project may be grouped into three major clusters: a new model building paradigm; fostering scientific collaboration; and integrating research and education. The results are expected to provide foundations for further research is management of uncertainty in deriving models representing a wide range of phenomena. This project lays a technical groundwork that can contribute to new collaborations between the PIs and application domain experts, facilitating broad interdisciplinary collaborations. Project results will be widely disseminated via the project web site (http://maple.cs.umbc.edu/complexmodels/). Finally, through teaching and training activities, this research project is also well suited to include the introduction of undergraduates to the possibilities of research and the incorporation of project topics into the PIs' courses on visualization and artificial intelligence."
"1018967","RI: Small: Collaborative Research: A Scalable Architecture for Image Interpretation","IIS","Robust Intelligence","09/15/2010","09/13/2010","Melanie Mitchell","OR","Portland State University","Standard Grant","Kenneth Whang","08/31/2014","$341,269.00","","mm@pdx.edu","1600 SW 4th Ave","Portland","OR","972070751","5037259900","CSE","7495","7923, 9102","$0.00","Seamless understanding of the meaning of visual images is a key property of human cognition that is far beyond the abilities of current computer vision programs.  The purpose of this project is to build a computational system that captures the dynamical and interactive aspects of human vision by integrating higher-level concepts with lower-level visual perception.  If successful, this system will be able to interpret visual scenes in a way that scales well with the complexity of the scene. Current computer vision systems typically rely on relatively low-level visual information (e.g., color, texture, shape) to classify objects or determine the overall category of a scene.   Such categorization is typically done in a ""bottom-up"" fashion, in which the vision system extracts lower-level features from all parts of the scene, and subsequently analyzes the extracted features to determine which parts of the scene contain objects of interest and how those objects should be categorized.  Such systems lack the abilities to scale to large numbers of visual categories and to identify more complex visual concepts that involve spatial and abstract relationships among object categories.  Visual perception by humans is known to be a temporal process with feedback, in which lower-level visual features serve to activate higher-level concepts (or knowledge).  These active concepts, in turn, guide the perception of and attention given to lower-level visual features.  Moreover, activated concepts can spread activation to semantically related concepts (e.g., ""wheels"" might activate ""car"" or ""bicycle""; ""bicycle"" might activate ""road"" or ""rider"").    In this way there is a continual interaction between the lower and higher levels of vision, which allows the viewer to focus on and connect important aspects of a complex scene in order to perceive its meaning, without having to pay equal attention to every detail of the scene.   The system proposed here will model these aspects of human visual perception.  <br/><br/>The proposed system, called Petacat, will integrate and build on two existing projects:  the HMAX model of object recognition originally developed by Riesenhuber and Poggio, and the Copycat model of high-level perception and analogy-making, developed by Hofstadter and Mitchell.    HMAX models the ""what"" pathway of mammalian visual cortex via a feed-forward network that extracts increasingly complex textural and shape features from an image. (HMAX has been reimplemented, as the ""Petascale Artificial Neural Network"" or PANN, by the Synthetic Vision Group at Los Alamos to allow for high-performance computing on large numbers of neurons.)  Copycat implements a process of interaction between high-level concepts and lower-level perception, and has been used to model focus of attention, conceptual slippage, and analogy-making in several non-visual domains.  This project will marry the feature extraction abilities of HMAX/PANN with the higher-level interactive perceptual abilities of Copycat to build the Petacat architecture. The image interpretation abilities of Petacat will be evaluated on families of related semantic visual recognition tasks (e.g., recognizing, in a flexible, human-like way, instances of ""walking a dog"").    The evaluation part of the project will involve the creation of image databases for benchmarking semantic image-understanding systems.  The Petacat source code and benchmarking databases will be made publically available via the web."
"0968566","SoCS:  Creation of a Framework for Computational Gaming","SES","Information Technology Researc, Special Projects - CCF, SOCIAL-COMPUTATIONAL SYSTEMS","10/01/2010","06/16/2011","Anind Dey","PA","Carnegie-Mellon University","Standard Grant","Frederick Kronz","09/30/2014","$721,000.00","","anind@uw.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","SBE","1640, 2878, 7953","7953, 9178, 9215, 9251, SMET","$0.00","Leveraging crowdsourcing to collect data is becoming more common. Human Computation, in particular, has looked at how to use artificial intelligence on data collected from people playing games, to validate that useful data has been collected on a very large scale. This work will investigate a new form of artificial-intelligence based crowdsourced games called Computational Gaming, in which questions will be posed without knowing what the answers are beforehand. Questions that require human judgment will be posed in the context of a game, and machine learning will be used to determine what questions to pose to which players and how to determine whether the responses are valid.<br/><br/>Intellectual Merit. This project will demonstrate the validity of Computational Gaming through two examples in text and image labeling, delineating a set of guiding design principles for building and evaluating future Computational Gaming designs, and producing a toolkit that supports and encourages the use of these design principles for building Computational Gaming systems.<br/><br/>Potential Broader Impacts. The project will create, both more quickly and more cheaply, databases of human-labeled data; it will also do so for a wider variety of problems than currently exists. The framework and toolkit for Computational Gaming will be valuable for game designers, for researchers in many domains that need labeled data, and for the users for whom the research is being conducted."
"0953667","Career: An Adaptive Compiler for Multi-core Environments","CCF","International Research Collab, COMPILERS, Software & Hardware Foundation, EPSCoR Co-Funding","03/01/2010","07/16/2014","John Cavazos","DE","University of Delaware","Continuing Grant","Almadena Chtchelkanova","02/28/2015","$484,909.00","","cavazos@cis.udel.edu","210 Hullihen Hall","Newark","DE","197160099","3028312136","CSE","7298, 7329, 7798, 9150","1045, 1187, 5918, 5980, 7329, 7942, 9150, 9151, 9218, 9251, HPCC","$0.00","Compilers are a critical component between the software developer and the computer. They translate application written by software developers into machine code that is processed by the computer. An important task of a compiler is to optimize applications so that they run efficiently.  Traditional methods to develop optimizing compilers are ad-hoc, labor-intensive, and ineffective.  As a consequence, optimizing compilers for a new processor often produces code that achieves only a fraction of the machine?s available performance. This is especially true for today's multi-core architectures, which are parallel processors on a single chip.  This research will involve investigating techniques from the artificial intelligence community that will allow a compiler to automatically adapt and tune to new architectures.  In effect, this research will replace hand-tuning with self-tuning compilers that adapt software automatically to match the performance characteristics of each target architecture.<br/><br/>In this project, the PI proposes to explore the viability of developing adaptive compilers for multi-core environments (ACME) to allow application portability while still achieving high performance.  The PI will create a statistical auto-tuning framework to support the probabilistic representation of the following features: the benefit analysis of optimizations, the identification and prediction of the appropriate runtime environment for different optimizations, and the generation of executables that efficiently combine several optimized code versions. He will invent components to measure accurately the characteristics of applications and targeted computing systems. The PI hopes to discover techniques to replace ?traditional? optimization benefit analysis with powerful machine learning models. These models will address the broad spectrum of parallel applications and multi-core environments, and they will be able to analyze and predict benefit under different dynamic contexts."
"0958392","CI-ADDO-EN: Flexible Machine Learning for Natural Language in the MALLET Toolkit","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","06/01/2010","07/17/2013","Andrew McCallum","MA","University of Massachusetts Amherst","Continuing Grant","Aidong Zhang","05/31/2016","$650,000.00","","mccallum@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","7359","9218, HPCC","$0.00","Natural language processing, information extraction, information<br/>integration and other text processing solutions are central components<br/>of computer science, and key tools for addressing the ever-increasing<br/>problems in information overload.  Issues of information overload are<br/>not only personal problems, but critical for business productivity,<br/>national defense, and increasingly government decision-making and<br/>transparency.<br/><br/>State-of-the-art natural language processing is increasingly based on<br/>machine learning.  However, the methodologies can be complex, and<br/>software infrastructure necessary for such systems is generally<br/>difficult to develop from scratch.  To address this need we have<br/>created MALLET (MAchine Learning for LanguagE) and FACTORIE (Factor<br/>graphs, Imperative, Extensible), open-source software toolkit that run<br/>in the Java virtual machine.  They provide many modern<br/>state-of-the-art machine learning methods, specially tuned to be<br/>scalable for the idiosyncrasies of natural language data, while also<br/>applying well to many other discrete non- language tasks.<br/><br/>The project will fill three critical gaps: (1) broadening these<br/>toolkits' applicability to new data and tasks (with better end-user<br/>interfaces for labeling, training and diagnostics), (2) greatly<br/>enhancing their research-support capabilities (with infrastructure for<br/>flexibly specifying model structures), and (3) improving their<br/>understandability and support (with new documentation, examples,<br/>online community support).<br/><br/>The project will have a direct positive impact on NLP and other<br/>machine learning research, on teaching, and on collaborative research<br/>activities.  Well-designed toolkits not only help researchers avoid<br/>duplicate implementation effort, but (a) they encourage sharing of<br/>algorithms and code, and thus also cultivate increased collaboration<br/>and intellectual flow of ideas; (b) they foster the communication of<br/>detailed clarity of algorithms and scientific reproducibility; (c)<br/>they help ""level the playing field"" by providing state-of-the-art<br/>implementations of foundational building blocks and recent methods to<br/>top-tier and small institutions alike; (d) they supply a teaching<br/>tool, not only by making it easy for students to experiment with the<br/>supplied research methodologies.  Furthermore, by providing multiple<br/>ready-to-use systems, non-programmers will have access to modern,<br/>scalable implementations of text processing tools that will spread<br/>knowledge and use of these techniques across fields, to the social<br/>sciences, humanities, and bio-medical fields.<br/><br/>For further information see the project web site at the URL:<br/>http://www.cs.umass.edu/~mccallum/nsf-mallet"
"1018691","Collaborative Research: RI: Small: A Scalable Architecture for Image Interpretation","IIS","ROBUST INTELLIGENCE","09/15/2010","09/13/2010","Garrett Kenyon","NM","New Mexico Consortium","Standard Grant","Kenneth C. Whang","08/31/2013","$158,946.00","","garkenyon@gmail.com","4200 West Jemez Road, Suite 301","Los Alamos","NM","875442587","5054124200","CSE","7495","7923","$0.00","Seamless understanding of the meaning of visual images is a key property of human cognition that is far beyond the abilities of current computer vision programs.  The purpose of this project is to build a computational system that captures the dynamical and interactive aspects of human vision by integrating higher-level concepts with lower-level visual perception.  If successful, this system will be able to interpret visual scenes in a way that scales well with the complexity of the scene. Current computer vision systems typically rely on relatively low-level visual information (e.g., color, texture, shape) to classify objects or determine the overall category of a scene.   Such categorization is typically done in a ""bottom-up"" fashion, in which the vision system extracts lower-level features from all parts of the scene, and subsequently analyzes the extracted features to determine which parts of the scene contain objects of interest and how those objects should be categorized.  Such systems lack the abilities to scale to large numbers of visual categories and to identify more complex visual concepts that involve spatial and abstract relationships among object categories.  Visual perception by humans is known to be a temporal process with feedback, in which lower-level visual features serve to activate higher-level concepts (or knowledge).  These active concepts, in turn, guide the perception of and attention given to lower-level visual features.  Moreover, activated concepts can spread activation to semantically related concepts (e.g., ""wheels"" might activate ""car"" or ""bicycle""; ""bicycle"" might activate ""road"" or ""rider"").    In this way there is a continual interaction between the lower and higher levels of vision, which allows the viewer to focus on and connect important aspects of a complex scene in order to perceive its meaning, without having to pay equal attention to every detail of the scene.   The system proposed here will model these aspects of human visual perception.  <br/><br/>The proposed system, called Petacat, will integrate and build on two existing projects:  the HMAX model of object recognition originally developed by Riesenhuber and Poggio, and the Copycat model of high-level perception and analogy-making, developed by Hofstadter and Mitchell.    HMAX models the ""what"" pathway of mammalian visual cortex via a feed-forward network that extracts increasingly complex textural and shape features from an image. (HMAX has been reimplemented, as the ""Petascale Artificial Neural Network"" or PANN, by the Synthetic Vision Group at Los Alamos to allow for high-performance computing on large numbers of neurons.)  Copycat implements a process of interaction between high-level concepts and lower-level perception, and has been used to model focus of attention, conceptual slippage, and analogy-making in several non-visual domains.  This project will marry the feature extraction abilities of HMAX/PANN with the higher-level interactive perceptual abilities of Copycat to build the Petacat architecture. The image interpretation abilities of Petacat will be evaluated on families of related semantic visual recognition tasks (e.g., recognizing, in a flexible, human-like way, instances of ""walking a dog"").    The evaluation part of the project will involve the creation of image databases for benchmarking semantic image-understanding systems.  The Petacat source code and benchmarking databases will be made publically available via the web."
"1031505","A Hybrid Electronic Tongue for Geoenvironmental Site Characterization","CMMI","Geotechnical Engineering and M, EDA-Eng Diversity Activities","09/01/2010","03/05/2012","Pradeep Kurup","MA","University of Massachusetts Lowell","Standard Grant","Richard Fragaszy","08/31/2014","$421,859.00","Ramaswamy Nagarajan","Pradeep_Kurup@uml.edu","Office of Research Admin.","Lowell","MA","018543692","9789344170","ENG","1636, 7680","015E, 036E, 037E, 038E, 043E, 1057, 116E, 1636, 7680, 7974, 9178, 9179, 9231, 9251, CVIS","$0.00","This project will support the development of a novel ?electronic tongue cone penetrometer? for on-site characterization of heavy metals such as Arsenic, Cadmium, Copper, Chromium, Lead, Manganese, Mercury, Nickel, Selenium, Thallium and Zinc in soils and groundwater. The electronic tongue is a device that mimics the human gustatory system using microelectrode sensor arrays coupled with artificial intelligence for pattern recognition. The project will involve fundamental research to design and assemble materials for highly sensitive and broadly-selective microelectrode sensors. This will be followed by the development of conductometric and voltammetric techniques for the hybrid electronic tongue. In addition, intelligent machine learning models for multivariate data processing and interpretation will be developed for classification and quantification of heavy metals. Calibration chamber studies will be conducted to develop methods for analysis of heavy metals in aqueous soil samples. Finally, the microelectrode sensor arrays will be deployed in a field-rugged cone penetrometer to facilitate real-time geoenvironmental site characterization.<br/><br/> <br/><br/>The successful completion of this project would result in the development of a novel in situ tool and method, for rapid, safe, and cost effective characterization of heavy metal contaminated sites. This minimally invasive technology will limit potential personnel exposure to contaminated media, and reduce the amount of investigation-derived waste normally generated during conventional borehole drilling and sampling activities. It will also reduce the time-consuming laboratory analysis during initial site investigations, and will provide regulatory agencies with critical information that is necessary for taking appropriate steps such as communicating drinking water advisories in a timely manner. This technology can also be expanded further to detect other types of toxins, making this approach applicable to diverse fields such as biotechnology, pharmaceuticals and medical diagnostics, food industry, environmental monitoring, law enforcement and homeland security."
"1002507","Pilot:   Assisted Musical Composition through Functional Scaffolding","IIS","CreativeIT","08/15/2010","07/20/2010","Kenneth Stanley","FL","The University of Central Florida Board of Trustees","Standard Grant","Ephraim Glinert","07/31/2015","$295,229.00","","kstanley@cs.ucf.edu","4000 CNTRL FLORIDA BLVD","Orlando","FL","328168005","4078230387","CSE","7788","7788","$0.00","As a ubiquitous creative endeavor across all human cultures, musical composition is an effective microcosm for the study of creativity in general.  This project will impact computer science by showing through assisted musical composition how computers can genuinely improve upon the creative capabilities of humans alone.   By introducing an interactive framework that enables even inexperienced users to realize their creative vision, this project also helps pave the way for such systems to amplify our creative potential in other areas in the future, such as in engineering and design.  The technology that will be developed, called Functional Scaffolding for Musical Composition (FSMC), takes the unique approach of computing accompaniment for existing musical tracks (called the ?scaffold?) by generating special functions that take the scaffold as input and output accompanying tracks.  In this way, generated tracks are in effect transformations of the scaffold, allowing them to inherit the global structure and implicit nuance of the preexisting music.  The implication for computational creativity in general is thus to harness the richness of preexisting human-generated content as a seed for further elaboration.   Furthermore, the user will be provided an interactive evolutionary interface that makes it possible to search the space of such transforming functions, in effect allowing the user to continually breed and elaborate new concepts that build upon preexisting incomplete works.<br/><br/>The primary target audience for FSMC as a practical technology will be musicians who lack the resources, collaborators, or expertise to produce complete musical compositions.  For example, while a hobbyist with a keyboard might be able to compose a compelling melody, lack of expertise in other instruments may prohibit adding accompanying guitar or base.  In addition, even more experienced musicians may benefit from the capability to quickly propose accompaniment as a new means of concept generation.  In fact, existing computer programs that aid in musical composition often register millions of downloads online, demonstrating broad public interest in applications that enhance musical creativity.  In addition to dissemination through scientific conferences focusing on computational creativity, the results of this research will be released in a form compatible with such programs, thereby directly impacting the public with a practical utility and consequently raising awareness of the potential for artificial intelligence and machine learning to enhance creativity in general."
"1049363","RAPID:  Gulf Coast Oil Spill Biodiversity Tracker. A Volunteer-based Observation Network to Monitor the Impact of Oil on Organisms along the Gulf Coast","DBI","Global Systems Science","09/01/2010","08/17/2010","Steven Kelling","NY","Cornell University","Standard Grant","Julie Dickerson","08/31/2012","$195,595.00","","stk2@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","BIO","7978","1165, 5987, 7914, 9183, 9184, BIOT","$0.00","In response to the Deep Horizon oil spill in the Gulf of Mexico, Cornell University is awarded a RAPID grant to develop information infrastructure that enhances our ability to provide information on the impact of the oil spill.  The project uses Internet and information technologies to engage volunteers in providing critically needed data to assess the impact of the oil spill on wildlife and the environment through time. The Biodiversity Tracker will expand the capabilities currently provided by eBird (www.ebird.org), which uses a network of volunteers to survey beaches and marshes for birds. These data are displayed on real-time, interactive maps showing locations of reported birds in relation to current and forecasted oil slick locations. Expanded capabilities will include online data forms and maps to collect and display information on oiled birds, other wildlife affected by the oil spill, and beach conditions; outreach support to engage more participants; and open access to all data through the Avian Knowledge Network (www.avianknowledge.net) and DataONE (dataone.org).<br/><br/><br/>The oil spill disaster in the Gulf of Mexico will affect the region's biodiversity and ecology for years to come. Understanding this impact requires documenting the spill's effects on wildlife and the environment throughout the entire region, yet traditional biodiversity monitoring and inventorying approaches are inadequate to gather the vast amounts of data needed. The Gulf Coast Oil Spill Biodiversity Tracker engages citizen-science participants in gathering these data and makes the information publicly accessible through interactive online maps and databases. These data will enable quick responses for scientific and conservation efforts and will provide a fundamental data resource needed to monitor ongoing impacts from the disaster."
"0964681","RI: Medium: Learned Dynamic Prioritization","IIS","ROBUST INTELLIGENCE","08/15/2010","08/13/2012","Jason Eisner","MD","Johns Hopkins University","Continuing grant","Tatiana Korelsky","07/31/2014","$899,976.00","Hal Daume","jason@cs.jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","CSE","7495","7924","$0.00","This project uses machine learning to accelerate the execution of a class of computer programs relevant to AI.  Given a program and a class of inputs, the new methods automatically seek execution strategies that are fast while still achieving a high level of accuracy.<br/><br/>The project focuses on the main inference algorithms that underlie statistical AI: dynamic programming, belief propagation, Markov chain Monte Carlo, and backtracking search.  Each of these inference algorithms faces an enormous search space, iteratively extending or refining its picture of this space.  Each algorithm must continually choose which computational step to take next.<br/><br/>The opportunity is to learn a strategy for making these choices. Some choices are on the ""critical path"" and help the system find an accurate output, while others lead mainly to wasted work.  The learned strategy for evaluating choices in context may itself be computationally intensive, so the method learns to speed that up as well, within the same framework.<br/><br/>The project will disseminate software and will have broader impact on several fields.  The targeted algorithms are central to natural language processing, speech processing, machine vision, computational biology, health informatics and music processing.  Their ability to form a coherent global analysis of a set of observations is a hallmark of intelligence, and will enable artificial systems that aid human understanding and performance.  Speeding them up is critical as researchers develop increasingly sophisticated statistical models.<br/>Furthermore, the learning methodologies developed will be useful in other settings that attempt to learn computational or behavioral strategies."
"0963668","RI:  Medium:  Collaborative Research:  Unlocking Biologically-Inspired Computer Vision: A High-Throughput Approach","IIS","ROBUST INTELLIGENCE","09/01/2010","08/27/2010","David Cox","MA","Harvard University","Standard Grant","Kenneth C. Whang","08/31/2013","$410,000.00","","davidcox@fas.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","7495","7924","$0.00","This project exploits advances in parallel computing hardware and a neuroscience-informed perspective to design next-generation computer vision algorithms that aim to match a human's ability to recognize objects.  The human brain has superlative visual object recognition abilities -- humans can effortlessly identify and categorize tens of thousands of objects with high accuracy in a fraction of a second -- and a stronger connection between neuroscience and computer vision has driven new progress on machine algorithms.  However, these models have not yet achieved robust, human-level object recognition in part because the number of possible ""bio-inspired"" model configurations is enormous. Powerful models hidden in this model class have yet to be systematically characterized and the correct biological model is not known.<br/><br/>To break through this barrier, this project will leverage newly available computational tools to undertake a systematic exploration of the bio-inspired model class by using a high-throughput approach in which millions of candidate models are generated and screened for desirable object recognition properties (Objective 1).  To drive this systematic search, the project will create and employ a suite of benchmark vision tasks and performance ""report cards"" that operationally define what constitutes a good visual image representation for object recognition (Objective 2).  The highest performing visual representations harvested from these ongoing high-throughput searches will be used: for applications in other machine vision domains, to generate new experimental predictions, and to determine the underlying computing motifs that enable this high performance (Objective 3).   Preliminary results show that this approach already yields algorithms that exceed state-of-the-art performance in object recognition tasks and generalize to other visual tasks.<br/><br/>As the scale of available computational power continues to expand, this approach holds great potential to rapidly accelerate progress in computer vision, neuroscience, and cognitive science: it will create a large-scale ""laboratory"" for testing neuroscience ideas within the domain of computer vision; it will generate new, testable computational hypotheses to guide neuroscience experiments; it will produce a new kind of multidimensional image challenge suite that will be a rallying point for computer models, neuronal population studies, and behavioral investigations; and it could unleash a host of new applications."
"1103684","III-CXT-Small: Collaborative Research:  Automatic Geomorphic Mapping and Analysis of Land Surfaces Using Pattern Recognition","IIS","Info Integration & Informatics","09/01/2010","03/07/2011","Tomasz Stepinski","OH","University of Cincinnati Main Campus","Standard Grant","Maria Zemankova","08/31/2012","$268,523.00","","stepintz@uc.edu","University Hall, Suite 530","Cincinnati","OH","452210222","5135564358","CSE","7364","7364, 9216, HPCC","$0.00","Description<br/><br/>Advances in remote sensing techniques have made available large datasets of topographic measurements pertaining to terrestrial and planetary land surfaces. However, the scientific utilization of these datasets is hampered by a lack of tools for effective automated analysis. This project seeks to develop a system for fast, objective and transparent conversion of topographic data into knowledge about land surfaces. The project has two complementary goals: 1) to develop a tool that autonomously produces geomorphic maps mimicking traditional, manually derived maps in their appearance and content, and 2) to develop a tool that classifies entire topographic scenes into characteristic landscape categories. The mapping tool is based on the object-oriented supervised classification principle. A number of novel solutions, including semi-supervised learning, meta-learning, and a wrapping technique coupling classification and segmentation, are proposed to address challenges posed by the specificity of topographic data. The scene classification tool is based on information-theoretic metrics and incorporates novel solutions to problems posed by the raster character of topographic datasets.<br/><br/>Intellectual Merit<br/><br/>The project employs a novel fusion of machine learning and computer vision techniques to open new possibilities. In the process of constructing the mapping and classifying tools, novel machine learning methodologies will be developed and tested. The products of this research will enable a qualitatively new type of analysis of land surface topography: the large scale statistical comparison of spatial distribution of landforms.<br/><br/>Broad Impact<br/><br/>Successful mapping and classifying tools will have impact beyond the analysis of natural landscapes; they can be also be applied to the study of surface metrology (the numerical characterization of industrial surfaces). The nature of this project will attract interest and collaboration with specialists from diverse disciplines, such as computer science, remote sensing, geomorphology and hydrology. Such links will broaden the base of expertise for each discipline, as well as enrich participants from contributing domains."
"0959454","MRI-R2: Acquisition of a GPU cluster for solving n-body systems in science and engineering","OAC","MAJOR RESEARCH INSTRUMENTATION","04/01/2010","02/09/2012","Greg Walker","TN","Vanderbilt University","Standard Grant","Irene M. Qualters","03/31/2013","$390,423.00","Jens Meiler, Kelly Holley-Bockelmann, Thomas Palmeri, Robert Weller","greg.walker@vanderbilt.edu","Sponsored Programs Administratio","Nashville","TN","372350002","6153222631","CSE","1189","6890, 9150, 9215, HPCC","$390,423.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).<br/><br/>Graphics Processors (GPUs) are potentially a cost effective and low power vehicle for science and engineering research that requires high performance computation.  The primary challenge to the use of GPUs more broadly is the difficulty in programming.  Dr. Walker and a team of colleagues representing five different scientific and engineering disciplines propose to pursue research topics in each of the disciplines.  By selecting important research topics which require a fundamentally similar computational algorithm for a class of problems labelled ""n-body problems"", the project offers opportunity for meaningful interdisciplinary collaboration across scientific domains that are normally quite distinct.  Since, solutions to this class of problem are particularly well suited to GPUs, there is likelihood of advances in multiple areas of scientific interest at a fraction of alternative costs and power.  Therefore NSF's Office of Cyberinfrastructure (OCI) is supporting the acquisition of the instrument."
"0956817","SBIR Phase II: Automated Mining of Worker and HR Preferences for On-Demand Job Matching","IIP","SBIR Phase II","01/15/2010","10/21/2013","Paul Nemirovsky","NY","dMetrics Inc.","Standard Grant","Glenn H. Larsen","10/31/2013","$1,000,000.00","","paul.nemirovsky@gmail.com","181 North 11th St","Brooklyn","NY","112111175","6176427163","ENG","5373","1658, 165E, 5373, 9216, HPCC","$0.00","This Small Business Innovation Research (SBIR) Phase II project aims to improve the quality of on-demand job matching by applying data mining and machine learning techniques to natural language descriptions of job requests, worker reviews, and transaction history. The project will enable lasting job matches by predicting the needs, preferences and constraints of workers and human resource managers. Currently available methods of job matching rely primarily on keyword search, corporate personality assessment tests, or fixed ontologies. Such systems lack comprehensive learning and therefore have difficulty matching workers with jobs. This project approaches job matching with a bias-free learning model that learns from hiring successes, trains on real-world data, and adapts to new job verticals.<br/><br/>The broader/commercial impact of the project is a matching technology that optimizes workers' and employers' strengths, discovering matching opportunities overlooked by traditional search technologies. Online reputation-building through performance reviews can improve workers' ability to market themselves. The global matching technology permits nearly every skill to become marketable by matching workers with all features from every available job request. Natural language processing techniques, developed in the course of this project, have the potential to broaden the appeal of cell phone text-messaging as a comprehensive job-searching tool. Furthermore, the contextual approach to learning about workers and employers enables trends to be identified among  users, and has far-reaching commercial implications in fields as diverse as medical research and e-commerce."
"1017181","III: Small: Better Sentiment Analysis through Forecasting","IIS","Info Integration & Informatics","09/01/2010","03/16/2011","Steven Skiena","NY","SUNY at Stony Brook","Standard Grant","Maria Zemankova","08/31/2014","$423,164.00","","skiena@cs.sunysb.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","7364","7364, 7923, 9251","$0.00","The emerging field of sentiment analysis employs algorithmic methods to identify and summarize opinions expressed in text.  Both machine learning and ad-hoc approaches lie at the foundations of contemporary sentiment analysis systems, but progress on improving both precision and recall has been slowed by the expense and complexity of obtaining sufficiently broad, general sentiment training/validation data.<br/><br/>Recent work has established that fundamental economic variables can successfully be forecast by applying sentiment analysis methods to news-oriented text streams.  This project turns this relation on its head, using such forecasting approaches to improve both the precision and recall of general entity-oriented sentiment analysis methods. In particular, this project provides a three-pronged research effort into entity-level sentiment analysis, focusing on improved assessment and algorithms, with applications to the social sciences and forecasting.  In particular: <br/>(1) Developing a complete entity-level, text and language-independent sentiment evaluation environment, both to further the development of the Lydia system and for release to the international sentiment analysis community.<br/>(2) Building on this environment, to develop improved sentiment-detection methods for English news, foreign language news streams, social media such as blogs and Twitter, and historical text corpora.<br/>(3) Finally, applying improved sentiment analysis to a variety of challenges in the social sciences.  <br/><br/>This research promises to substantially improve both the precision and recall of sentiment detection methods, by focusing on the weakest link: rigorous yet domain-, source-, and language-independent assessment of sentiment.  Beyond improvements in natural language processing (NLP), this includes other issues in opinion mining, including article clustering and duplicate detection, entity-domain context, and combining opinions from large numbers of distinct sources.<br/><br/>The sentiment analysis methods and data developed under this research project are expected to have a broad impact, as the results will be directly applicable in a broad range of social sciences, including sociology, economics, political science, and media and communication studies.  The techniques will serve as both an educational and scholarly resource in these fields, empowering students and researchers to conduct their own primary studies on historical trends and social forces.  Results will be disseminated to the community through the project website (http://www.textmap.org/III)."
"1030472","Automated Vision-Based Sensing for Site Operations Analysis","CMMI","CIVIL INFRASTRUCTURE SYSTEMS","09/01/2010","03/27/2014","Jochen Teizer","GA","Georgia Tech Research Corporation","Standard Grant","Elise D. Miller-Hooks","08/31/2015","$299,966.00","Patricio Vela","teizer@ce.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","ENG","1631","023E, 036E, 039E, 1057, CVIS","$0.00","This research seeks to prove that it is possible to reliably and automatically track work progress and multiple resources with images (video and/or time-lapse) in order to reproduce the daily workflow activities associated to a construction worksite. The task of measuring the progress of construction site activities that involve workers, large machines, and materials, has often been a subjective and intensive manual process that is prone to error and, in real operations, frequently out-of-date. Demonstrating that an active vision system can effectively analyze and assess work-site progress will assist project managers by reducing the time spent monitoring and interpreting project status and performance, thus enabling increased attention to control of cost and schedule. By making project management and workforce more aware of the performance status of their project and their work environment, potential savings to the industry are envisioned. The track data will be interpreted and used to provide understanding of the spatio-temporal evolution of a worksite for automatically generating knowledge about worksite operations. In an information-based framework, much effort is spent acquiring and interpreting information. In a knowledge-based framework, efforts are allocated to making decisions based on the interpreted information.<br/><br/>If successful, this research will transform the review and management of construction operations from being information-based to knowledge-based, thus saving human resources and improving decision effectiveness. This research has broader appeal beyond construction. Research domains incorporating or requiring vision-based sensing, diverse resources (people, small to heavy machinery, goods, etc.), and processing of the visual data for awareness of operations and activities are additional investigation domains. Examples include airport ground operations and mining operations. Contributions are also expected in the fields of machine learning and computer vision. The proposed research will impact research into site operations by enabling the automated monitoring and tracking of site resources. Video-based monitoring and processing algorithms provide a non-intrusive, easy, and, rapid mechanism for generating a body of operational information and knowledge which, when made available, will enable inquiry into construction operations that is currently not possible. Longer term, this research will serve as a valuable aid to project management by enabling tighter control and greater efficiency. By making project management and workforce more aware of the performance status of their project and their work environment, potential savings to the construction and other industries are envisioned. This research will also actively include and drive the education of the next generation of engineers (civil, electrical, and computational engineering) and construction labor pool. The research has a dedicated outreach plan to involve in this research a broad spectrum of students from high schools and industry professionals who are interested in advanced hard- and software technology"
"1041411","Support for Student Participation in the Conference International Conference on Multimodal-Machine Learning for Multimodal Interaction (ICMI-MLMI) 2010","IIS","HCC-Human-Centered Computing","11/01/2010","06/23/2010","Louis-Philippe Morency","CA","University of Southern California","Standard Grant","Ephraim Glinert","10/31/2013","$24,592.00","","morency@cs.cmu.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7367","","$0.00","This is funding to support participation by about 10 graduate students in a Doctoral Consortium (workshop) to be held in conjunction with the 12th International Conference on Multimodal Interfaces and 7th Workshop on Machine Learning for Multimodal Interaction (ICMI-MLMI 2010), which will be held November 8-12, in Beijing, China, and which is organized by the Association for Computing Machinery (ACM) with co-sponsorship from the Institute of Electrical and Electronics Engineers (IEEE).   The conference will bring together researchers from North America, Europe, and Asia to present and discuss the latest multidisciplinary work on multimodal interfaces, systems, and applications.  Now in its second year, the combined ICMI-MLMI is the foremost international event representing the growing interest in next-generation perceptive, adaptive and multimodal user interfaces.  Such interfaces represent an emerging interdisciplinary research direction, involving spoken and natural language understanding, image processing, computer vision, pattern recognition, experimental psychology, etc.  They aim to promote efficient and natural interaction and communication between computers and human users, and represent a radical departure from previous computing that should ultimately enable users to interact with computers using everyday skills.  The main goals of ICMI-MLMI 20010 are to further scientific research within the broad field of multimodal interaction and systems, to focus on major trends and challenges, and to help identify a roadmap for future research and commercial success.  Topics of interest this year include: multimodal and multimedia processing; multimodal input and output interfaces; multimodal applications; human interaction analysis and modeling; and multimodal data, evaluation, and standards.  The three days of invited talks, panels, and single-track oral and poster presentations will facilitate interaction and discussion among researchers; the conference promises to be an international venue for brainstorming and coming up with creative directions for future research in multimodal interfaces. Participants in the Doctoral Consortium will get to showcase their ongoing thesis work, either orally or in a special ""doctoral spotlight"" poster session during which they will receive feedback from an invited committee composed of senior personnel, and including the Advisory Committee chair and the General and Program chairs.  As a further incentive for high-quality student participation, ICMI-MLMI will be awarding outstanding paper awards, with a special category just for student papers. More information about ICMI-MLMI is available online at http://www.acm.org/icmi/2010.<br/><br/>Broader Impacts: The Doctoral Consortium will give student participants exposure to their new research community, both by presenting their own work and by observing and interacting with established professionals in the field.  It will encourage students at this critical time in their careers to begin building a social support network of peers and mentors.  With the goal of increasing the breadth of participation at ICMIMLMI, selection of grantees will be done by the PI with oversight from the Advisory Committee Chair and the Conference General Chair; the organizers will make special efforts to encourage participation by a significant number of student researchers from less-well funded institutions, as well as by minority students, female students, and students from geographically under-represented states.  Students funded under this award will all be enrolled at U.S. institutions of higher education."
"1039741","MRI: Development of a Video-Based Robotic Instrument for Behavioral Analysis and Diagnosis of At-Risk Children","CNS","Major Research Instrumentation, Information Technology Researc, Special Projects - CNS, IUCRC-Indust-Univ Coop Res Ctr","10/01/2010","08/22/2018","Nikolaos Papanikolopoulos","MN","University of Minnesota-Twin Cities","Standard Grant","Rita Rodriguez","09/30/2019","$1,725,730.00","Kelvin Lim, Guillermo Sapiro","npapas@cs.umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","1189, 1640, 1714, 5761","1189, 5761, 9178, 9251","$0.00","Abstract <br/>Proposal #: 10-39741 <br/>PI(s): Papanikolopoulos, Nikolaos; Lim, Kelvin; Guillermo Sapiro <br/>Institution: University of Minnesota <br/>Title: MRI/Dev.: Video-Based Robotic Instrument for Behavioral Analysis and Diagnosis of At-Risk Children <br/><br/>Project Proposed: <br/>The proposed set of tools constitutes a video-based robotic instrument which targets the domain of early diagnosis for children at risk of developing psychiatric disorders. As such, this proposal is at the disciplinary boundaries between computer science, psychology and psychiatry, and medicine. Proposed is the development of a robotic instrument that could observe and automatically analyze abnormalities in children, thus introducing a novel technology which can help identifying children at risk.  Specific activities include: <br/>- Development and clinical verification of instrumentation and clinical protocols to quantify mental disorders in children; <br/>- Development and usage of computer vision and machine learning methodologies in the instrument; <br/>- Development of statistical models to evaluate the available related data sets; <br/>- Usage of a wide array of passive and active sensors and state-of-the-art 3D camera systems to collect and analyze the monitored data; <br/>- Usage of robots and robot pets as a means to detect and treat mental disorders; and, <br/>- Practical validation of the instrument at the Medical School. <br/><br/>Broader Impacts: <br/>The recent usage of computer vision methodologies/hardware and robotics for detection of mental disorders in children, in itself, constitutes strong broader impacts. Planned are also educational programs (workshops, tutorials, etc.) that will enable training gathering of physicians and psychologists to the aforementioned methods/procedures, which would otherwise not be possible. Moreover, significant planned curriculum development at the participating institutions revolves around the instrument. In addition, outreach activities for middle-school students from underrepresented groups will take place, and so will outreach to various pertinent patient groups. This truly interdisciplinary project also plans to include international partners."
"0905920","Postdoctoral Research Fellowships in Biology for FY 2009","DBI","BIO INFOR POSTDOCT RSCH FELLOW","01/01/2010","08/05/2009","Roy Wollman","CA","Wollman                 Roy","Fellowship Award","Julie Dickerson","12/31/2011","$123,000.00","","","","Mountain View","CA","940401078","","BIO","1398","1398, 9179, SMET","$0.00","This action funds an NSF Postdoctoral Research Fellowship for FY 2009. The fellowship supports a research and training plan entitled ""Cross talk within the macrophage-signaling network enhances its pathogen detection capabilities"" for Roy Wollman. The host institution for this research is Stanford University, and the sponsoring scientist is Tobias Meyer.<br/><br/>The immune system's role is to identify harmful pathogens and fight diseases. To do so, it uses specialized cell types called white blood cells, also known as macrophages. Macrophages detect invasion by pathogens and identify the class of pathogens that invades the body (e.g., fungi, bacteria, etc.).  This research focuses on understanding information processing within macrophages, which allows them to reliably distinguish between different classes of pathogens.  It utilizes robotic microscopy, computer vision, and machine learning tools to collect systematic data on macrophages' response to different pathogen-derived molecules. This resulting data is being used to develop a mathematical model describing the information processing encoded by the biochemical reactions inside a cell. This model provides a better understanding of how macrophages identify pathogens.<br/><br/>Training goals include computational training in the analysis of large sets of image data using computer vision algorithms, machine learning tools for reverse engineering of models from experimental data, and control theory for optimal experimental design. This interdisciplinary project emphasizes the integration of computational and experimental approaches. Educational impacts include training for undergraduate students and the dissemination of the results by publication this work in the scientific literature.  A better understanding of the classification capabilities of macrophages is essential for long-term progress in the development of new treatments for infectious diseases. In addition, understanding information processing and classification at the molecular level has important basic science implications."
"1027289","Workshop on NLP and Linguistics: finding the common ground","IIS","Linguistics, Robust Intelligence","10/01/2010","02/05/2014","Fei Xia","WA","University of Washington","Standard Grant","Tatiana Korelsky","09/30/2014","$16,996.00","","fxia@u.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","1311, 7495","1311, 7495","$0.00","Since early 1990s, with the advancement of machine learning methods and the availability of data resources such as treebanks and parallel corpora, data-driven approaches to Natural Language Processing (NLP) have made significant progress. The success of such data-driven approaches has cast doubt on the relevance of linguistics to NLP. Conversely, NLP techniques are rarely used to help linguistics studies. The goal of this NSF-sponsored workshop is to carefully examine the relationship between linguistics and NLP and determine how incorporating linguistic knowledge into NLP systems can advance the state of the art of NLP and how NLP can assist linguistic studies through automatic collection and analysis of linguistic data.<br/><br/> The workshop will bring together researchers from linguistics and NLP with diverse interests in and across both disciplines. The workshop is held in conjunction with ACL on July 16, 2010 in Sweden.  This award provides financial support that allows the workshop to attract top researchers in the US to attend the workshop in Sweden, and the support is crucial especially for linguists who normally do not attend ACL.<br/><br/> This workshop is intended to begin collaboration between linguists and NLP researchers that will continue long after the workshop has finished. The ultimate goals of the workshop and follow-up events are to accelerate work in NLP by bringing in important knowledge and information from linguistics, and to open the eyes of NLP researchers to the challenges within the field of linguistics that could benefit from cutting-edge, state-of-the-art NLP.<br/>The cross pollination between the disciplines can only push both forward and in directions that otherwise would come much later or not at all."
"0941463","CDI-Type I: Collaborative Research: A Bio-Inspired Approach to Recognition of Human Movements and Movement Styles","ECCS","CDI TYPE I","01/01/2010","02/22/2012","Rene Vidal","MD","Johns Hopkins University","Standard Grant","Zhi Tian","12/31/2012","$493,332.00","Charles Connor","rvidal@cis.jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","ENG","7750","0000, 6890, 7721, OTHR","$493,332.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).<br/><br/>The objective of this research is to develop bio-inspired algorithms for recognizing human movements and movement styles in videos of human activities.  The approach is based on a new representation of static three-dimensional (3D) shape structure in the ventral visual pathway consisting of configurations of 3D structural fragments.  This project uses neural experiments to validate an analogous four-dimensional (4D) representation of moving 3D shapes based on 4D (space and time) structure-in-motion (SiM) fragments.  These SiM fragment models are used to develop algorithms for automatically extracting SiM fragments from videos of human activities.  Hybrid system identification and clustering techniques are used to learn a dictionary of human movements used for recognition.  This dictionary, in turn, influences the design of the neural experiments.  The algorithms are evaluated using a real-time tele-immersion system.<br/><br/>The intellectual merit of this project is to substantially reduce the gap between human and machine perception of human movements through the interaction between computational and experimental analyses from neuroscience, computer vision, machine learning, and dynamical systems.  This project also has the potential to generate advances in machine learning and dynamical systems, by extending classification, clustering and system identification methods to time-series data generated by collections of hybrid dynamical models.<br/><br/>Potential broader impacts include applications in surveillance, security, assisted home living, infant care, tele-immersion, and athlete motion analysis.  The project also sponsors a competition where small robots are constructed by middle and high-school students from Baltimore, and the EL Alliance program for retaining underrepresented minorities at graduate schools."
"0941382","CDI-Type I: Collaborative Research: A Bio-Inspired Approach to Recognition of Human Movements and their Styles","ECCS","CDI TYPE I","01/01/2010","02/22/2012","Ruzena Bajcsy","CA","University of California-Berkeley","Standard Grant","Zhi Tian","12/31/2012","$246,666.00","","bajcsy@eecs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","ENG","7750","0000, 6890, 7721, OTHR","$246,666.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).<br/><br/>The objective of this research is to develop bio-inspired algorithms for recognizing human movements and movement styles in videos of human activities.  The approach is based on a new representation of static three-dimensional (3D) shape structure in the ventral visual pathway consisting of configurations of 3D structural fragments.  This project uses neural experiments to validate an analogous four-dimensional (4D) representation of moving 3D shapes based on 4D (space and time) structure-in-motion (SiM) fragments.  These SiM fragment models are used to develop algorithms for automatically extracting SiM fragments from videos of human activities.  Hybrid system identification and clustering techniques are used to learn a dictionary of human movements used for recognition.  This dictionary, in turn, influences the design of the neural experiments.  The algorithms are evaluated using a real-time tele-immersion system.<br/><br/>The intellectual merit of this project is to substantially reduce the gap between human and machine perception of human movements through the interaction between computational and experimental analyses from neuroscience, computer vision, machine learning, and dynamical systems.  This project also has the potential to generate advances in machine learning and dynamical systems, by extending classification, clustering and system identification methods to time-series data generated by collections of hybrid dynamical models.<br/><br/>Potential broader impacts include applications in surveillance, security, assisted home living, infant care, tele-immersion, and athlete motion analysis.  The project also sponsors a competition where small robots are constructed by middle and high-school students from Baltimore, and the EL Alliance program for retaining underrepresented minorities at graduate schools."
"0952943","CAREER: Learning Models for Scalable Content-Based Image Retrieval","IIS","Robust Intelligence","04/01/2010","03/08/2014","Lorenzo Torresani","NH","Dartmouth College","Continuing Grant","Jie Yang","03/31/2017","$494,964.00","","lorenzo@cs.dartmouth.edu","OFFICE OF SPONSORED PROJECTS","HANOVER","NH","037551421","6036463007","CSE","7495","1045, 1187, 9150","$0.00","This project addresses the design of machine learning algorithms enabling content-based image retrieval in Web-scale collections of photos. This research formulates image retrieval as a binary classification problem: decide which database images are the ""same"" as the user-provided photo. Efficiency and scalability to large collections are achieved by constraining the classifiers to be models supported by traditional text-search engines, which perform real-time search in databases of several billion documents. In order to implement search based on high-level notions of similarity, the research team develops methods to automatically localize the most content-relevant regions in the input photo and to extract from them semantically powerful classifiers combining appearance cues with robust geometric constraints. The algorithms learn from user-provided labels indicating the presence but not the location of similar visual content, thus requiring a minimal amount of human supervision. This research investigates also how this advanced form of similar-image search can be used to organize personal photos, provide semantic annotations, and support content-based clustering of pictures.  Furthermore, this work provides technical advances in a wide range of computer vision problems including object detection, visual saliency, and content-based clustering of photos. Moreover, the research team is collecting an unprecedentedly large image data set to evaluate the developed image retrieval system and to be  available to the community. Research is naturally integrated with education and outreach by means of related courses and out-of-classroom activities aimed at attracting students to this field and at encouraging interdisciplinary collaborations."
"1015930","RI: Small: Exploratory Data Analysis for Speech Recognition","IIS","ROBUST INTELLIGENCE","08/15/2010","08/20/2010","Steven Wegmann","CA","International Computer Science Institute","Standard Grant","Tatiana D. Korelsky","07/31/2012","$200,000.00","","swegmann@icsi.berkeley.edu","1947 CENTER ST STE 600","Berkeley","CA","947044115","5106662900","CSE","7495","7923","$0.00","Hidden Markov models (HMMs) have been successfully applied to automatic <br/>speech recognition for more than 35 years even though a key HMM <br/>assumption - the statistical independence of frames - is obviously <br/>violated by speech data. In fact, this data/model mismatch has inspired <br/>many attempts to modify or replace HMMs with alternative models that are <br/>better able to take into account the statistical dependence of frames. <br/>The scientific goal of this work is to discover predictable regions of <br/>statistical dependence in speech data and quantify their effect on <br/>HMM-based recognition accuracy. In contrast to previous studies of <br/>statistical dependency, this research uses the HMM to explore its <br/>departure from the data via exploratory data analysis (EDA). The <br/>methodology is to first analyze the data and its fit to the model, <br/>searching for regions of predictable statistical dependence - model/data <br/>mismatch. EDA is used again to develop simple models of the effect of <br/>the predictable mismatch on recognition accuracy. A key piece of this <br/>analysis is the development and use of graphical tools to visualize the <br/>statistical dependency, the recognition errors, and their relationship. <br/>The results of this research will provide important clues for the design <br/>of HMM generalizations. The analysis methodology is central to the field <br/>of statistics, but is rarely used in speech recognition research. <br/>Graduate students working on this project will learn its utility and <br/>how to use it on other problems. Open source versions of the software <br/>developed will be made available for free downloading."
"0905885","Postdoctoral Research Fellowships in Biology for FY 2009","DBI","BIO INFOR POSTDOCT RSCH FELLOW","01/01/2010","08/27/2009","Daniel Sheldon","NY","Sheldon                 Daniel         R","Fellowship Award","Julie Dickerson","12/31/2011","$123,000.00","","","","Ithaca","NY","148504474","","BIO","1398","1398, 6890, 9179, SMET","$123,000.00","This action funds an NSF Postdoctoral Research Fellowship in Biology for FY 2009 under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).  The fellowship supports a research and training plan entitled ""Algorithms for changing species distributions: tracking the world's tree swallows"" for Daniel Sheldon. The host institution for this research is Oregon State University, and the sponsoring scientist is Thomas Dietterich.<br/><br/>Understanding the geographical distributions of species and how they change is critical to conservation, ecological science, and global climate change research. This research develops computational methods to quantify patterns of change in the distribution of the Tree Swallow (Tachycineta bicolor), a migratory songbird that forms dense communal roosts during its eight-month non-breeding season. The early-morning departure of birds from the roost creates a distinctive ""roost ring"" pattern that is visible in NEXRAD weather radar images. The project employs techniques from computer vision, machine learning and probabilistic modeling to automatically detect and extract roost rings from radar, assembles daily estimates of the distribution of Tree Swallows throughout the United States, and then infers the patterns of migration and other events that cause distribution changes over time, both short-term and long-term. One of the great assets of NEXRAD radar is the ability to look fifteen years into the past with archived data. This project addresses significant questions about the roosting biology and population trends of swallows, advances techniques in radar ornithology, and lays an algorithmic foundation for identifying patterns of change in species distributions.<br/><br/>Training objectives include strengthening skills in computer vision, radar ornithology, and species distribution modeling. This research engages a broad audience of biologists, computer scientists, conservationists, and the general public by providing a complete and accurate model of songbird distribution on a large scale, with provisions to continuously monitor changes for the past fifteen years and into the future."
"1016029","RI:  Small:  Boosting, Optimality and Game Theory","IIS","ROBUST INTELLIGENCE","09/01/2010","07/01/2011","Robert Schapire","NJ","Princeton University","Continuing grant","Todd Leen","08/31/2014","$450,000.00","","schapire@cs.princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","7495","7923","$0.00","Boosting is a machine-learning method based on combining many<br/>carefully trained weak prediction rules into a single, highly accurate<br/>classifier.  Boosting has both a rich theory and a record of empirical<br/>success, for instance, to face detection and spoken-dialogue systems.<br/><br/>The theory of boosting is broadly connected to other research fields,<br/>but has only been fully developed for the simplest learning problems.<br/>Nevertheless, in practice, boosting is commonly applied in settings<br/>where the theory lags well behind.  We do not know if such practical<br/>methods are truly best possible; even for binary classification, it is<br/>not clear how to best exploit what is known about how boosting<br/>operates.  New challenges will demand an even greater widening of the<br/>foundations of boosting.<br/><br/>The goal of this project is to develop broad theoretical insights and<br/>versatile algorithmic principles.  The aim is to study<br/>game-theoretically how to design the most efficient and effective<br/>boosting algorithms possible.<br/><br/>Research on boosting is spread over many years. across multiple<br/>publications and disciplines.  To organize this body of work, a<br/>significant activity of this project is the completion of a book on<br/>boosting which will provide a valuable resource for students and<br/>researchers of diverse backgrounds and interests.<br/><br/>Boosting has historically had a major impact on areas outside machine<br/>learning, such as statistics, computer vision, and speech and language<br/>processing.  Thus, there is a strong potential for work at its<br/>foundations to have a broad impact on these other research and<br/>application areas as well."
"1035913","CPS: Medium: A Novel Human Centric CPS to Improve Motor/Cognitive Assessment and Enable Adaptive Rehabilitation","CNS","INFORMATION TECHNOLOGY RESEARC, SPECIAL PROJECTS - CISE, CYBER-PHYSICAL SYSTEMS (CPS)","09/15/2010","05/29/2015","Fillia Makedon","TX","University of Texas at Arlington","Standard Grant","David Corman","02/29/2016","$730,001.00","Heng Huang, Zhengyi Le, Vassilis Athitsos, Dan Popa","makedon@cse.uta.edu","701 S Nedderman Dr, Box 19145","Arlington","TX","760190145","8172722105","CSE","1640, 1714, 7918","116E, 7918, 7924, 9102, 9178, 9251","$0.00","The objective of this research is to develop methods and tools for a multimodal and multi-sensor assessment and rehabilitation game system called CPLAY for children with Cerebral Palsy (CP). CPLAY collects and processes multiple types of stimulation and performance data while a child is playing. Its core has a touch-screen programmable game that has various metrics to measure delay of response, score, stamina/duration, accuracy of motor/hand motion. Optional devices attached to extend CPLAY versions provide additional parallel measurements of level of concentration/participation/engagement that quantify rehabilitation activity. The approach is to model the process as a cyber-physical system (CPS) feedback loop whereby data collected from various physical 3D devices (including fNIR brain imaging) are processed into hierarchical events of low-to-high semantic meaning that impact/ adjust treatment decisions.<br/><br/>Intellectual Merit: The project will produce groundbreaking algorithms for event identification with a multi-level data to knowledge feedback loop approach. New machine learning, computer vision, data mining, multimodal data fusion, device integration and event-driven algorithms will lead towards a new type of cyber- physical rehabilitation science for neurological disorders. It will deliver fundamental advancements to engineering by showing how to integrate physical devices with a computationally quantitative platform for motor and cognitive skills assessment.<br/><br/>Broader Impacts: The project delivers a modular & expandable game system that has huge implications on the future of US healthcare and rehabilitation of chronic neurological disabilities. It brings hope to children with Cerebral Palsy via lower cost and remote rehabilitation alternatives. It brings new directions to human centered computing for intelligent decision-making that supplements evidence-based practices and addresses social and psychological isolation problems."
"1017256","III: Small: Privacy Preserving Techniques for Speech Processing","CNS","International Research Collab, Robust Intelligence, TRUSTWORTHY COMPUTING","09/01/2010","01/24/2012","Bhiksha Raj","PA","Carnegie-Mellon University","Standard Grant","Nina Amla","02/28/2015","$524,931.00","","bhiksha@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7298, 7495, 7795","5936, 5979, 7495, 7923","$0.00","Voice-processing systems that perform speaker verification, keyword spotting, speech recognition, etc. need complete access to the speech signal, albeit in parameterized form. These data could potentially be logged for future playback, analysis or even  malicious activities and represent a threat to the privacy and security of users.  This project aims to develop techniques that enable some key voice processing tasks, namely speaker identification or verification and keyword spotting, while preserving the privacy of the speaker?s voice. The techniques will perform their operations without observing any intelligible form of the speech signal from which one could glean any information about the speaker or what they said; yet at the end of the computation the results, which will only be delivered to an authorized party, will be indistinguishable from those that would be obtained if the system were not secured in this manner.<br/><br/>The proposed work draws upon approaches from cryptography and secure multiparty computation. It is explained how these techniques can be used to devise privacy-preserving algorithms for voice processing, and the development of such algorithms for the three problems mentioned, speaker identification and verification and keyword spotting, has been proposed.<br/><br/>For further information see http://mlsp.cs.cmu.edu/projects/secureaudio"
"0964102","RI: Medium: Collaborative Research:  Semi-Supervised Discriminative Training of Language Models","IIS","COLLABORATIVE RESEARCH, ROBUST INTELLIGENCE","06/01/2010","03/18/2014","Alexander Kain","OR","Oregon Health & Science University","Continuing grant","Tatiana Korelsky","05/31/2015","$519,050.00","Izhak Shafran, Richard Sproat","kaina@ohsu.edu","3181 S W Sam Jackson Park Rd","Portland","OR","972393098","5034947784","CSE","7298, 7495","5940, 7495, 7924","$0.00","This project is conducting fundamental research in statistical language modeling to improve human language technologies, including automatic speech recognition (ASR) and machine translation (MT).<br/><br/>A language model (LM) is conventionally optimized, using text in the target language, to assign high probability to well-formed sentences.  This method has a fundamental shortcoming: the optimization does not explicitly target the kinds of distinctions necessary to accomplish the task at hand, such as discriminating (for ASR) between different words that are acoustically confusable or (for MT) between different target-language words that express the multiple meanings of a polysemous source-language word.<br/><br/>Discriminative optimization of the LM, which would overcome this shortcoming, requires large quantities of paired input-output sequences: speech and its reference transcription for ASR or source-language (e.g. Chinese) sentences and their translations into the target language (say, English) for MT.  Such resources are expensive, and limit the efficacy of discriminative training methods.<br/><br/>In a radical departure from convention, this project is investigating discriminative training using easily available, *unpaired* input and output sequences: un-transcribed speech or monolingual source-language text and unpaired target-language text.  Two key ideas are being pursued: (i) unlabeled input sequences (e.g. speech or Chinese text) are processed to learn likely confusions encountered by the ASR or MT system; (ii) unpaired output sequences (English text) are leveraged to discriminate between these well-formed sentences from the (supposed) ill-formed sentences the system could potentially confuse them with.<br/><br/>This self-supervised discriminative training, if successful, will advance machine intelligence in fundamental ways that impact many other applications."
"0963898","RI:  Medium:  Collaborative Research:  Semi-Supervised Discriminative Training of Language Models","IIS","COLLABORATIVE RESEARCH, ROBUST INTELLIGENCE","06/01/2010","06/03/2014","Sanjeev Khudanpur","MD","Johns Hopkins University","Continuing grant","Tatiana D. Korelsky","08/31/2015","$518,250.00","Damianos Karakos, Chris Callison-Burch","khudanpur@jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","CSE","7298, 7495","5940, 7495, 7924","$0.00","This project is conducting fundamental research in statistical language modeling to improve human language technologies, including automatic speech recognition (ASR) and machine translation (MT). <br/><br/>A language model (LM) is conventionally optimized, using text in the target language, to assign high probability to well-formed sentences. This method has a fundamental shortcoming: the optimization does not explicitly target the kinds of distinctions necessary to accomplish the task at hand, such as discriminating (for ASR) between different words that are acoustically confusable or (for MT) between different target-language words that express the multiple meanings of a polysemous source-language word. <br/><br/>Discriminative optimization of the LM, which would overcome this shortcoming, requires large quantities of paired input-output sequences: speech and its reference transcription for ASR or source-language (e.g. Chinese) sentences and their translations into the target language (say, English) for MT. Such resources are expensive, and limit the efficacy of discriminative training methods. <br/><br/>In a radical departure from convention, this project is investigating discriminative training using easily available, *unpaired* input and output sequences: un-transcribed speech or monolingual source-language text and unpaired target-language text. Two key ideas are being pursued: (i) unlabeled input sequences (e.g. speech or Chinese text) are processed to learn likely confusions encountered by the ASR or MT system; (ii) unpaired output sequences (English text) are leveraged to discriminate between these well-formed sentences from the (supposed) ill-formed sentences the system could potentially confuse them with. <br/><br/>This self-supervised discriminative training, if successful, will advance machine intelligence in fundamental ways that impact many other applications."
"0953107","CAREER: Brain-Tongue-Computer Interfacing","IIS","HCC-Human-Centered Computing","03/01/2010","12/01/2016","Maysam Ghovanloo","GA","Georgia Tech Research Corporation","Continuing Grant","Ephraim Glinert","02/28/2018","$548,298.00","","mgh@getech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7367","1045, 1187, 7367, 9215, 9251, HPCC","$0.00","The PI's long-term research plans involve exploring new pathways to the human central nervous system (CNS) in order to expand our knowledge about this highly complex system and understand how it works, and developing innovative technologies and research tools that will enable direct or indirect communication with the CNS through such pathways.  In particular, he is keen to utilize and evaluate new interfacing technologies in devices that will help individuals who suffer from chronic disabilities and neurological diseases, such as blindness, deafness, and paralysis to improve and extend their quality of life.  With these general goals in mind, the PI will in this project focus on exploring the use of voluntary tongue motion as a substitute for some of the functions traditionally performed by the arms and hands in personal environmental control.  This has not been possible in the past absent access to tongue motion without impeding the tongue's key roles in swallowing, respiration, and speech.  The PI has previously developed and successfully tested a new wireless, unobtrusive, and wearable technology he calls the Tongue Drive System (TDS), to indicate tongue position in real time within certain user-defined locations in the oral space.  Building upon the TDS prototype, he will explore whether the inherent characteristics of the tongue and its rich motor capabilities can be harnessed as an intermediary pathway to the human brain.  In other words, he will seek to create a Brain-Tongue-Computer Interface (BTCI) by enhancing the functionality of the TDS hardware, signal processing algorithms, and GUI software to support a large number of choices that will be simultaneously available to users, in addition to the proportional control capability that is currently employed to facilitate navigation and computer access.  The PI will conduct experiments to evaluate the performance, usability, and acceptability of the BCTI platform, and will employ it to achieve a fundamental understanding of human factors associated with voluntary tongue motions.  Finally, the PI will combine his real time 3-D tongue tracking technology with multi-channel wireless neural recording to explore the relationship between unconstrained tongue movements and whole muscle/single motor unit activities in speech, respiration, and swallowing without any bodily restraints.<br/><br/>Broader Impacts:  Individuals who are severely disabled as a result of various causes from spinal cord injuries to stroke, cerebral palsy, and ALS find it extremely difficult to carry out everyday tasks without continuous help.  This research will ultimately transform the lives of many persons with severe disabilities, by helping them live active, self-supportive, and productive lives.  Solutions such as the BTCI may also help reduce healthcare and assisted-living costs by relieving the burden on family members and dedicated caregivers.  Utilization of the tongue's motion as an untapped human motor modality in command, control, and navigation tasks involves costs and benefits which are at present unknown; quantitative analysis of human performance in concurrently conducted sensory, motor, and cognitive tasks, both in the presence and absence of tongue motions, is likely to bring about new scientific discoveries in human system integration.  The PI's 3-D tongue tracking technology will also impact speech/language therapy, as well as the treatment of communication and sleep disorders that involve tongue motion.  The PI will explore use of the BTCI technology in educational settings for children with special needs through programs such as Tools for Life, and will also conduct outreach efforts to expose K-12 students to facts about the CNS, its associated impairments, and different ways to address those problems with engineering solutions."
"0951240","12th Conference on Laboratory Phonology","BCS","LINGUISTICS","03/15/2010","03/15/2010","Caroline Smith","NM","University of New Mexico","Standard Grant","William J. Badecker","02/29/2012","$19,999.00","Ian Maddieson","caroline@unm.edu","1700 Lomas Blvd. NE, Suite 2200","Albuquerque","NM","871310001","5052774186","SBE","1311","0000, 9150, OTHR","$0.00","The 12th Conference on Laboratory Phonology (LabPhon 12) will be held at the University of New Mexico on July 8-10, 2010. The LabPhon conference series brings together researchers from a variety of backgrounds who employ empirical methods to study the way that language is organized at the level of its most basic structural elements, the ""atoms"" of sound, sign or gesture. The conference series is distinctive in that each conference is organized around a different theme. The theme for LabPhon 12 is ""Gesture as language; gesture and language"". The objective is to connect the study of movements as linguistic units to the broader study of gesture as part of communication, including gestures that are not part of a linguistic system but nonetheless help us communicate. The conference will bring together researchers who have a gestural perspective on language and encourage cross-fertilization between different areas of research as these intersect at the level of phonological organization. This theme seems particularly timely, as sign language linguistics and gesture studies are increasingly integrated with longer-established fields of linguistic study, and recent technological developments in the study of movement are making possible more extensive and precise analysis of both speech and sign. Special sessions will center on six sub-topics related to the main theme, with an international group of invited speakers. Additional contributed papers addressing these themes will be included in the sessions, which conclude with an invited commentator who reflects on the contributions relating to each sub-topic. One or two non-thematic sessions will be scheduled to accommodate significant contributions on a wider range of topics, and three poster sessions are also planned. Sustained interaction among attendees is facilitated by the LabPhon tradition of having only plenary sessions, maintaining an ongoing dialogue during the three days of the conference.<br/><br/>LabPhon conferences enjoy strong participation by graduate students, many of whom present results of their ongoing research. Student participation will be encouraged through a favorable low student registration fee. The opportunities to present and discuss their work with peers and senior colleagues contribute to the integration of research with education. Further, the international and inter-disciplinary nature of the meeting is designed to enhance the kind of networking and partnership-building that often spawns the most creative new scientific work. Particular efforts will be made to attract participation by members of minorities under-represented in scientific research activities, most especially in this case members of the Native American and Deaf communities. The conference proceedings will be disseminated not only in formal academic publications but also on the web in a format designed to interest the lay reader and publicized via the university's web site. Among the central topics of the conference are several that have potential to bring direct societal benefits. Greater knowledge of how facial and manual gestures are coordinated with spoken language production promises to enhance automatic speech recognition, as well as training for the Deaf and hard-of-hearing. Deeper understanding of how linguistic gestures are produced provides a sounder foundation for clinical intervention and language instruction as well as for applications in speech technology. Study of the special features of individual languages increases awareness of the significance and value of every community's language and its contribution to overall human culture."
"0954737","CAREER: Statistical and Computational Complexities of Modern Learning Problems","DMS","STATISTICS","03/01/2010","02/21/2014","Alexander Rakhlin","PA","University of Pennsylvania","Continuing grant","Gabor Szekely","02/29/2016","$400,000.00","","rakhlin@mit.edu","Research Services","Philadelphia","PA","191046205","2158987293","MPS","1269","0000, 1045, 1187, OTHR","$0.00","The research objective of this proposal is to develop a mathematical theory relating statistical and computational complexities of learning from data. Through an integrated study of these complexities, the PI aims to fill the gap in the understanding of fundamental connections between Statistics and Computation. The problems considered in this proposal are aligned with the following overlapping directions: (1) effects of regularization on statistical and computational guarantees; (2) information-theoretic limitations of estimation and optimization; (3) trade-offs between statistical performance and computation time, as well as the effect of budget constraints; (4) sequential prediction methods as a link between optimization and statistical learning; and (5) limited-feedback models and the value of feedback in sequential prediction and optimization. Progress along these directions is of great significance from both theoretical and practical points of view. <br/><br/>Statistical Learning Theory has been successful in designing and analyzing algorithms that extract patterns from data and make intelligent decisions. Applications of learning methods are ubiquitous: they include systems for face detection and face recognition, prediction of stock markets and weather patterns, learning medical treatment strategies, speech recognition, learning user's search preferences, placement of relevant ads, and much more. As statistical learning methods become an essential part of many computerized systems, new challenges appear. These challenges include large amounts of data, high dimensionality, limited feedback, and a possibility of malicious behavior. All these challenges have a profound impact on (a) the statistical performance and (b) the computation time required to perform the task at hand. Little work exists on studying these two aspects simultaneously, and the goal of this project is to fill this gap. Better understanding of the interaction between Statistics and Computation is likely to lead to faster and more precise methods, thus positively impacting technology and society. The project's broader impact includes components for integration of interdisciplinary research and education through the development of new courses, seminars, workshops, and a summer school program. <br/>"
"1043341","Computational Dreaming","CNS","Networking Technology and Syst","09/01/2010","05/09/2014","JoAnn Paul","VA","Virginia Polytechnic Institute and State University","Standard Grant","Thyagarajan Nandagopal","08/31/2015","$191,709.00","","jmpaul@vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","CSE","7363","7916, 9102","$0.00","By focusing on a set of the physical-structural foundations of dreaming, this research will investigate: (1) new organizational principles for parallel computation and (2) why dreaming is critical to intelligence. <br/><br/>Seemingly against all survival instincts, all intelligent beings must sleep and dream, even if they are under duress, even if it endangers their very lives because they are in a hostile environment.  Sleep that includes dreaming is strongly related to efficient mental processes.  The thesis of this work is that the need to dream can be inferred from the brain's most striking physical and behavioral characteristics.  <br/><br/>The computer architecture that will be investigated is the DALI (Dream Architecture for Lateral Intelligence)  a true Multiple Instruction, Single Datastream (MISD) architecture in which multiple models process the same input stream in real-time.  While awake, some lateral processors are observers while one or more others are active.  A dream phase of computation resolves divergence between processors in a competitive feedback phase not dominated by a flow of logic. Reality contains multiple views of the same thing, between different individuals and within the same individual, with incongruence resolved over time. The goal of the DALI is to include multiple, lateral models that process what the system observes, and then to model the competitive process of model resolution during a dream phase so that the system may be more effective for the next day's real-time (awake) response. The initial problem which will be investigated is contextual partitioning for speech recognition in pervasive, portable computing devices."
"1026893","SBIR Phase II: High Performance Directional MEMS Microphones for Communication Devices","IIP","SMALL BUSINESS PHASE II","09/01/2010","08/30/2010","Caesar Garcia","TX","Silicon Audio, LLC","Standard Grant","Juan E. Figueroa","02/28/2013","$490,012.00","","caesar@siaudiolabs.com","3601 South Congress Avenue","Austin","TX","787040000","5127737684","ENG","5373","1517, 1775, 5373, 7257, 9139, HPCC","$0.00","This Small Business Innovation Research (SBIR) Phase II project will investigate a novel Micro-electro-mechanical systems (MEMS) microphone based on new design principles. By abandoning the design principles of traditional microphones (both MEMS and full-scale), a vastly superior acoustical design is being explored that has resulted in substantial improvements in fidelity and size reduction (15 dB signal to noise ration[SNR] improvement over existing commercial directional microphones, and roughly 100x smaller in volume). Furthermore, as demonstrated in Phase I, the microphones have an inherently directional response with the benefit of focusing on a speaker or event of interest while rejecting ambient background noise. These attributes make this innovation ideal for addressing an emerging need of high volume consumer communication device manufactures who are looking for acoustic sensing innovations with the unique combination of high performance + low manufacturing cost. The objective of this Phase II innovation is to continue prototyping efforts from Phase I to the point of pilot scale manufacture. This effort will entail finite element modeling and design optimization of the new device structure, fabrication of 2nd generation prototypes, and experimentation in collaboration with customers from several different microphone sectors including hearing aids and cellular phones. <br/><br/>The broader impact/commercial potential of this project is based on an enabling capability: the introduction of advanced audio features (e.g. directionality and high fidelity) into a suite of consumer communication devices. The primary customer focus for this innovation is high volume consumer communication device manufacturers. New applications on their horizon demand improvements in microphone component performance. There are presently several commercial suppliers of MEMS microphones. All use variations of a traditional microphone architecture which has proven incapable of addressing high SNR applications. Additional markets and applications for this innovation include acoustic instrumentation, performance audio, military and defense, intelligence gathering, speech recognition (e.g. in laptop computers), and hearing aids. Addressing hearing aid markets will have a societal impact as well, as patient satisfaction with hearing aid devices is presently very low. Innovations at the microphone and signal processing level have the potential to improve this greatly. The innovation is also expected to have other audiological applications including use in hearing health monitoring systems based on otoacoustic principles. Clinical tools and instruments based on this innovation will serve to enhance scientific and technological understanding in many fields of acoustics."
"1016713","RI: Small: Decision-Theoretic Control of Crowd-Sourced Workflows","IIS","HCC-Human-Centered Computing, Robust Intelligence","09/15/2010","04/11/2012","Daniel Weld","WA","University of Washington","Standard Grant","Todd Leen","08/31/2014","$320,669.00","Mausam Mausam","weld@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7367, 7495","7367, 7923, 9251","$0.00","Crowd-sourcing is a recent framework in which human intelligence tasks are outsourced to a crowd of unknown people as an open request for services. Requesters use crowd-sourcing for a wide variety of jobs like dictation-transcription, content screening, linguistic tasks, user-studies, etc. These requesters often use complex workflows to subdivide a large task into bite-sized pieces (including the management of these tasks), each of which is independently crowd-sourced. These workflows are paramount to the success of crowd-sourcing, still, there has been little attention paid to methods for dynamically optimizing the throughput of a workflow. Controlling and optimizing such a workflow is an excellent application for AI research for two reasons. First, it is challenging in that the agent has to understand the dynamics of an uncertain, real-time environment and reason about distinct choices for a decision. More importantly, the domain has significant economic value -- progress can potentially impact hundreds of thousands of people and spur economic development in a fast growing sector.<br/><br/>This project is investigating complex workflows using a decision-theoretic framework that optimizes for a quality/price trade-off, with aims of (1) building statistical models of worker behavior derived from a large corpus of online behavior, (2) defining a declarative representation language to describe a wide range of workflows, and (3) developing an automated scheme that optimizes a general workflow resulting in an automated controller for making informed decisions at various stages of the process and for monitoring worker accuracies and computing corrections based on them. In the longer term, perhaps beyond the scope of this project, is (4) development of an interface optimizer that automatically learns the best user interface for a task based on user behavior increasing throughput of the workflow, and (5) integrating these ideas in an open-source, software toolkit to directly benefit the various requesters in managing their tasks."
"1002748","MAJOR:  Assistive Artificial Intelligence to Support Creative Filmmaking in Computer Animation","IIS","HCC-Human-Centered Computing, CreativeIT","09/01/2010","08/31/2010","Mark Riedl","GA","Georgia Tech Research Corporation","Standard Grant","William Bainbridge","08/31/2014","$695,485.00","Michael Nitsche","riedl@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7367, 7788","7788","$0.00","This project will explore approaches to artificial intelligence that can support creative digital filmmaking, an extremely rich new form of expression and communication. The most accessible variant of digital filmmaking is ""machinima"" - cinematic movies created by manipulating avatars in 3D computer game worlds. Due to the allure of cheap, quick, and easy movie making, and the accessibility of high-fidelity graphics through video games technologies, machinima has grown into a mainstream form of creative expression and sharing. However, machinima has a high threshold of entry. This is due only partly to technical tools, which are cheap and easily acquired; digital filmmaking also has a high threshold of skill requirements. In general, creativity is collaborative, with creators often seeking feedback and critique from others. Intelligent systems can also participate in the feedback loop of creative practice by suggesting, autonomously creating, and critiquing digital media.<br/><br/>The goal of this research is to reduce the technological and skill barriers to complex, but rich forms of digital expression such as filmmaking, thereby increasing the creative productivity of amateur creators. Its approach is to develop digital media production tools that are instilled with computational models of creative practice and intuitive interfaces informed by empirical studies. The anticipated result is a greater understanding of creative processes involving feedback and critique, models of cognitive and emotive processes in human recipients of creative artifacts, and understanding about the tradeoffs of interface modalities involving intelligent participatory systems. The project is organized around two major, interrelated thrusts: (1) develop cognitive and computational models of feedback and critique as a means toward intelligent systems that participate in creative endeavors; (2) study how the creative abilities of amateur and expert digital filmmakers are affected by production interfaces along dimensions of (a) degree of constraint in cinematic control and (b) modes of intelligent participatory support.<br/><br/>It is anticipated that the resultant models and implementations will serve as next-generation creativity support tools to be adopted by the amateur digital filmmaking and machinima communities. By achieving its research goals, this project will demonstrate a technique for lowing the threshold of entry to a form of digital media creation. Lowering the threshold of machinima production, in particular, will open the practice to populations of users historically underrepresented in computing such as women, who are attracted to storytelling but often discouraged by highly technical ""hacker"" skills. As an expressive form, digital filmmaking is a powerful medium for communication, can be used as a draw to computing, and can be integrated into a wide repertoire of activities including entertainment and education.  Resultant models and implementations may also impact the growing practice of previsualization in the movie and television industries. The approach will result in a model for incorporating intelligent creative assistance into other forms of expressive digital media."
"0963404","Collaborative Research: Measuring and Modeling Collective Intelligence","IIS","HCC-Human-Centered Computing","01/01/2010","02/18/2015","Christopher Chabris","NY","Union College","Standard Grant","Ephraim Glinert","12/31/2015","$173,908.00","","chabrisc@union.edu","807 Union Street","Schenectady","NY","123083103","5183886101","CSE","7367","9215, HPCC","$0.00","The ""holy grail"" of artificial intelligence research for decades has been to design computers with robust, integrated, human-like intelligence. This goal has proven elusive, in spite of a massive amount of research.  But another goal is just now becoming feasible, and so has been the subject of much less research: using vast computer networks to create new kinds of intelligent entities that combine the best of both human and machine intelligence.  One key to designing such human-centered computing systems is better ways of measuring the collective intelligence they exhibit. That is the focus of this research, which represents a collaborative effort among researchers at MIT (lead institution), CMU and Union College.  The PIs will first use analogies with what is already known about measuring individual intelligence to suggest new ways of measuring the collective intelligence of complex human-machine systems.  For instance, they will determine whether the striking pattern of correlations across tasks that characterizes individual human intelligence even exists for human-machine groups.  Next, a series of statistically validated tests will be developed to measure the key components of collective intelligence in human-machine groups.  Then, to better understand the ""active ingredients"" of collective intelligence, the PIs will use what is already known about how groups of people interact effectively to measure micro-level behavior in human-machine groups.   A key goal will be to find critical factors (such as group size, technological support, or individual capabilities) that contribute to a human-machine group's adaptability across a wide range of tasks.<br/><br/>Most people and computers today are parts of larger human-machine systems that must cope with a wide range of problems.  This research will provide powerful new tools for managing and designing such systems.  Imagine, for instance, that one could give a short ""collective intelligence test"" to a top-management team, a product development team, or a collection of Wikipedia contributors.  Imagine that this test could predict the team's future performance on a wide range of important tasks.  And imagine that the test could also help suggest changes to the team that would improve its flexibility.  Or imagine that designers of new collaboration software tools could use a single test to predict how well their tools would improve a group's effectiveness on many different tasks.  From the smallest business work groups to our largest societal challenges, there are now many new opportunities for people and computers to solve problems together, not just more efficiently, but also more intelligently.  This work will help build a firmer scientific foundation for doing this.<br/><br/>Broader Impacts:  With individual humans, it is relatively easy to measure intelligence, but it is difficult to increase that intelligence or to observe the detailed events inside the brain that give rise to it.  With human-computer groups it is much easier to observe and change factors (such as group size, composition, and technological support) that are likely to determine the group's collective intelligence. Thus, there is a profound intellectual opportunity, not just to learn more about how to design intelligent human-computer systems but also to gain new insights into the very nature of intelligence in complex systems.  The results of this research, therefore, will be of interest not only to researchers in computer-supported cooperative work, human-computer interaction, and artificial intelligence, but also more broadly to fields such as cognitive science, social psychology, and organization theory."
"0963285","Collaborative Research:  Measuring Collective Intelligence","IIS","HCC-Human-Centered Computing","01/01/2010","01/11/2010","Thomas Malone","MA","Massachusetts Institute of Technology","Standard Grant","Ephraim Glinert","12/31/2014","$538,213.00","","malone@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7367","7924, 9215, HPCC","$0.00","The ""holy grail"" of artificial intelligence research for decades has been to design computers with robust, integrated, human-like intelligence. This goal has proven elusive, in spite of a massive amount of research.  But another goal is just now becoming feasible, and so has been the subject of much less research: using vast computer networks to create new kinds of intelligent entities that combine the best of both human and machine intelligence.  One key to designing such human-centered computing systems is better ways of measuring the collective intelligence they exhibit. That is the focus of this research, which represents a collaborative effort among researchers at MIT (lead institution), CMU and Union College.  The PIs will first use analogies with what is already known about measuring individual intelligence to suggest new ways of measuring the collective intelligence of complex human-machine systems.  For instance, they will determine whether the striking pattern of correlations across tasks that characterizes individual human intelligence even exists for human-machine groups.  Next, a series of statistically validated tests will be developed to measure the key components of collective intelligence in human-machine groups.  Then, to better understand the ""active ingredients"" of collective intelligence, the PIs will use what is already known about how groups of people interact effectively to measure micro-level behavior in human-machine groups.   A key goal will be to find critical factors (such as group size, technological support, or individual capabilities) that contribute to a human-machine group's adaptability across a wide range of tasks.<br/><br/>Most people and computers today are parts of larger human-machine systems that must cope with a wide range of problems.  This research will provide powerful new tools for managing and designing such systems.  Imagine, for instance, that one could give a short ""collective intelligence test"" to a top-management team, a product development team, or a collection of Wikipedia contributors.  Imagine that this test could predict the team's future performance on a wide range of important tasks.  And imagine that the test could also help suggest changes to the team that would improve its flexibility.  Or imagine that designers of new collaboration software tools could use a single test to predict how well their tools would improve a group's effectiveness on many different tasks.  From the smallest business work groups to our largest societal challenges, there are now many new opportunities for people and computers to solve problems together, not just more efficiently, but also more intelligently.  This work will help build a firmer scientific foundation for doing this.<br/><br/>Broader Impacts:  With individual humans, it is relatively easy to measure intelligence, but it is difficult to increase that intelligence or to observe the detailed events inside the brain that give rise to it.  With human-computer groups it is much easier to observe and change factors (such as group size, composition, and technological support) that are likely to determine the group's collective intelligence. Thus, there is a profound intellectual opportunity, not just to learn more about how to design intelligent human-computer systems but also to gain new insights into the very nature of intelligence in complex systems.  The results of this research, therefore, will be of interest not only to researchers in computer-supported cooperative work, human-computer interaction, and artificial intelligence, but also more broadly to fields such as cognitive science, social psychology, and organization theory."
"0963451","Collaborative Research: Measuring Collective Intelligence","IIS","HCC-Human-Centered Computing","01/01/2010","01/11/2010","Anita Woolley","PA","Carnegie-Mellon University","Standard Grant","Ephraim Glinert","12/31/2012","$187,633.00","","awoolley@cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7367","9215, HPCC","$0.00","The ""holy grail"" of artificial intelligence research for decades has been to design computers with robust, integrated, human-like intelligence. This goal has proven elusive, in spite of a massive amount of research.  But another goal is just now becoming feasible, and so has been the subject of much less research: using vast computer networks to create new kinds of intelligent entities that combine the best of both human and machine intelligence.  One key to designing such human-centered computing systems is better ways of measuring the collective intelligence they exhibit. That is the focus of this research, which represents a collaborative effort among researchers at MIT (lead institution), CMU and Union College.  The PIs will first use analogies with what is already known about measuring individual intelligence to suggest new ways of measuring the collective intelligence of complex human-machine systems.  For instance, they will determine whether the striking pattern of correlations across tasks that characterizes individual human intelligence even exists for human-machine groups.  Next, a series of statistically validated tests will be developed to measure the key components of collective intelligence in human-machine groups.  Then, to better understand the ""active ingredients"" of collective intelligence, the PIs will use what is already known about how groups of people interact effectively to measure micro-level behavior in human-machine groups.   A key goal will be to find critical factors (such as group size, technological support, or individual capabilities) that contribute to a human-machine group's adaptability across a wide range of tasks.<br/><br/>Most people and computers today are parts of larger human-machine systems that must cope with a wide range of problems.  This research will provide powerful new tools for managing and designing such systems.  Imagine, for instance, that one could give a short ""collective intelligence test"" to a top-management team, a product development team, or a collection of Wikipedia contributors.  Imagine that this test could predict the team's future performance on a wide range of important tasks.  And imagine that the test could also help suggest changes to the team that would improve its flexibility.  Or imagine that designers of new collaboration software tools could use a single test to predict how well their tools would improve a group's effectiveness on many different tasks.  From the smallest business work groups to our largest societal challenges, there are now many new opportunities for people and computers to solve problems together, not just more efficiently, but also more intelligently.  This work will help build a firmer scientific foundation for doing this.<br/><br/>Broader Impacts:  With individual humans, it is relatively easy to measure intelligence, but it is difficult to increase that intelligence or to observe the detailed events inside the brain that give rise to it.  With human-computer groups it is much easier to observe and change factors (such as group size, composition, and technological support) that are likely to determine the group's collective intelligence. Thus, there is a profound intellectual opportunity, not just to learn more about how to design intelligent human-computer systems but also to gain new insights into the very nature of intelligence in complex systems.  The results of this research, therefore, will be of interest not only to researchers in computer-supported cooperative work, human-computer interaction, and artificial intelligence, but also more broadly to fields such as cognitive science, social psychology, and organization theory."
"1016465","RI: Small: Integrating Paradigms for Approximate Stochastic Planning","IIS","Robust Intelligence","08/15/2010","09/09/2013","Mausam Mausam","WA","University of Washington","Standard Grant","Todd Leen","07/31/2014","$466,508.00","","mausam@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7495","7923, 9251","$0.00","A fundamental challenge for Artificial Intelligence is sequential decision making under uncertainty, a task where automated algorithms lag far behind human-level intelligence. The primary reason for the disparity is curse of dimensionality - the number of states is exponential in the problem features. Recent advances that restrict decision-theoretic computation to a reachable subset of state space have scaled to moderately-sized problems, but proven ineffective in scaling to real problems. On the other hand, probabilistic planners based on deterministic planning might scale up, but with a massive loss in solution quality.<br/><br/>This project is investigating several methods to scale probabilistic planning to real-sized problems. We combine decision-theoretic analysis, basis function approximation and the classical AI planning techniques, to develop a series of highly scalable planners. A common theme in our techniques is the use of deterministic plans to automatically obtain domain abstractions in the form of 'good' or 'bad' properties, or intermediate subgoals. The project introduces and exploits a principled collaboration between decision theory and classical planning techniques, thus retaining the benefits of both - high quality as well as high performance. Experiments show that our new planner solves difficult planning competition problems using orders of magnitude less memory outputting high quality policies.<br/><br/>Our research also proposes effective solutions to long-standing problems of generating a set of basis functions and computing a hierarchical problem decomposition. Both basis function approximation and hierarchical decomposition are popular in existing literature for speeding up planning, but they are not fully automated - a human is required to specify the basis functions and the hierarchy. We provide novel, domain-independent solutions that remove this additional human effort.  <br/><br/>Our research addresses several long standing challenges in AI, like scaling stochastic planning, and automatically generating basis functions and subgoal hierarchies. We expect to produce state-of-the-art planners that will be effective in large and complex real world scenarios, e.g., planetary exploration, military operations planning, and robotic decision making."
"1036017","The Fourth Northeast Student Colloquium on Artificial Intelligence","IIS","ROBUST INTELLIGENCE","05/15/2010","05/19/2010","Andrew McCallum","MA","University of Massachusetts Amherst","Standard Grant","Sven G. Koenig","04/30/2011","$16,181.00","Erik Learned-Miller","mccallum@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","7495","7495","$0.00","This award will help to subsidize the participation of graduate students in the fourth Northeast Student Colloquium on Artificial Intelligence (NESCAI) to be held April 16-18, 2010 at the University of Massachusetts in Amherst. This conference is to include oral and poster presentations by students,invited talks by senior AI researchers, and student-run tutorials. The conference will be largely run by a program committee consisting of doctoral students under the guidance of senior faculty. The program committee will conduct a review process to select the projects chosen for oral and poster presentations. In addition to graduate students, the conference plans to encourage attendance by outstanding senior undergraduates through a special undergraduate track, in the hope that it will increase undergraduate enthusiasm for research and thus the likelihood that they will go on to graduate work. The project integrates research and education and commits to broadening diversity."
"0942454","CCLI: Enhancing Expertise, Sociability and Literacy through Teaching Artificial Intelligence as a Lab Science","DUE","S-STEM-Schlr Sci Tech Eng&Math, CCLI-Type 1 (Exploratory)","09/15/2010","06/30/2016","Stephanie August","CA","Loyola Marymount University","Standard Grant","Victor Piotrowski","08/31/2016","$179,897.00","","saugust@lmu.edu","One LMU Drive","Los Angeles","CA","900452659","3103384599","EHR","1536, 7494","9178, SMET","$0.00","Computer Engineering (32)<br/><br/>This proposal develops a new approach of teaching introductory Artificial Intelligence (AI) concepts through a laboratory experimentation approach.  Each topic includes several components that range from ""the concept in the real world"" through detailed code implementation.  The laboratory setting supports communication skills as well as software engineering and process understanding, and the laboratories cover a wide range of AI applications. The laboratory approach supports students who learn better in a social setting, including women students, or who learn better by reading and developing mental models.<br/><br/>The project has several components, including developing laboratory materials to be used in a sequence of lab exercises as the student works through the laboratory (including the basic concept, some applications of the concept, sample processing concepts, design description, code hints, test suites, experiments, full source code, and a complexity analysis).  It extends beyond that available through current repositories of AI materials. The project includes three workshops, which are primarily assessment activities where participants review course materials.  <br/><br/>The project develops a repository linked to an appropriate element of the NSDL and appropriate development of metadata is included.  Along with the repository, a mix of papers, conference presentations, and conference workshops make this available to the broader community."
"1037866","Travel Support for 2010 Association for Advancement of Artificial Intelligence (AAAI) Robotics Workshop and Exhibition","IIS","ROBUST INTELLIGENCE","07/01/2010","05/19/2010","Ayanna Howard","CA","Association for the Advancement of Artificial Intelligence","Standard Grant","Richard Voyles","06/30/2011","$25,000.00","","ah260@gatech.edu","2275 E BAYSHORE RD STE 160","East Palo Alto","CA","943033224","6503283123","CSE","7495","7495","$0.00","The project serves to support team travel to the 2010 AAAI Robotics Exhibition and Workshop.  The exhibition and workshop overlaps with the 2010 AAAI Conference being held in Atlanta July 11-15, 2010.  The Exhibition and Workshop revolves around the theme of manipulation and learning.  Events include robot challenges, demonstrations and presentations.  Funds will be used to support about 40 students and their advisors for team travel."
"1020229","The Leonardo Project: An Intelligent Cyberlearning System for Interactive Scientific Modeling in Elementary Science Education","DRL","Discovery Research K-12","08/15/2010","09/23/2011","James Lester","NC","North Carolina State University","Continuing Grant","Helen Martin","07/31/2015","$3,499,410.00","Michael Carter, Eric Wiebe, Bradford Mott","lester@csc.ncsu.edu","2701 Sullivan DR STE 240","Raleigh","NC","276950001","9195152444","EHR","7645","9177, SMET","$0.00","The project designs and implements technologies that combine artificial intelligence in the form of intelligent tutoring systems with multimedia interfaces to support children in grades 4-5 learning science. The students use LEONARDO's intelligent virtual science notebooks to create and experiment with interactive models of physical phenomena. With this technology, students' models 'come alive' as interactive multimedia artifacts that combine animation, sound, and narration. The curricular focus is on physical and earth sciences, and the technology supports multimodal interactive scientific modeling for four curricular units: forces and motion, magnetism and electricity, landforms, and weather and climate.  A central feature of this environment is PadMates, which are intelligent virtual tutors that support science learning through interactive scientific modeling.<br/><br/>The PIs investigate the cognitive mechanisms by which learning occurs.  Specifically, they study the central issues of problem solving (strategy use, divergent thinking, and collaboration) and engagement (motivation, situational interest, presence) with respect to achievement as measured by both science content knowledge and transfer. With diverse student populations in 60 classrooms drawn from both urban and rural settings, the studies determine precisely which technologies and conditions contribute most effectively to learning processes and outcomes.<br/><br/>The products include technologies and findings that should be the basis of a framework to inform the future development of similar systems. The impact should be substantial on all learners given the potential power of the technology to scaffold learning at an important developmental stage."
"1014092","Student Support for the Tenth International Conference on Intelligent Tutoring Systems","IIS","HCC-Human-Centered Computing","02/01/2010","01/19/2010","David Mostow","PA","Carnegie-Mellon University","Standard Grant","Ephraim Glinert","01/31/2011","$25,000.00","Carolyn Rose","mostow@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7367","9215, HPCC","$0.00","This is funding to support attendance by approximately 25 advanced doctoral students from the United States and abroad at the 10th International Conference on Intelligent Tutoring Systems (ITS 2010), which will take place June 14-18, 2010, in Pittsburgh.  The first ITS conference was held in 1988 in Montreal, and they have continued every two years for the past 12 years in locations that include Brazil, Taiwan, France, and Canada as well as the United States.  The ITS conferences offer a rare professional opportunity for interdisciplinary researchers from around the world to converge and present cutting-edge results from the fields of artificial intelligence, computer science, cognitive and learning sciences, psychology, and educational technology.  The goal is to promote studies in advanced systems in computer science applied to education, cognitive science and human learning for learners of all ages.  To that end, the series provides a forum for the interchange of ideas in all areas of computer science and human learning, a unique environment in which researchers and practitioners exchange ideas, theories, experiments, techniques, applications and evaluations of initiatives supporting new developments relevant for the future.  Comments and feedback from each previous ITS conference indicate that the carefully structured conference format continues to be professionally rewarding and stimulating to all who attend.  ITS conferences are highly refereed international events and serve as reference guidelines for the research community; each paper is generally reviewed by 4 referees so that having a paper published at ITS is a reference of quality for any researcher evaluation.  The conferences operate under the auspices of an independent nonprofit organization and are funded entirely by registration fees.  More information about the conference is available at http://www.cmu.edu/its2010.<br/><br/>Students supported by NSF funds will have the opportunity to attend sessions with papers, posters, tutorials, workshops, and informal interactions with accomplished researchers, the latter within the framework of a Young Researchers Track that includes special sessions for the students to present their research ideas, meet peers who have related interests, and receive feedback and mentoring from senior members of the ITS community.   A structured program will be provided in which each student is matched with a mentor who will be encouraged to offer feedback and support to students as they prepare their presentations, during the doctoral consortium sessions, and in at least one 1-on-1 meeting.  The doctoral consortium will be situated within the main conference program in order to encourage maximal community involvement.  Its structure will facilitate as much discussion and feedback as possible.  With this goal in mind, students will present their work at lunchtime poster sessions open to all attendees.  To avoid competition with other events and to maximize attendance, no other talks will be scheduled at this time and posters will be in the same rooms as the buffet lunch for all conference attendees.  To acquaint attendees with student work, student poster sessions will be immediately preceded by ""fire-hose"" sessions where students summarize their work very briefly.  To enable poster presenters to see and discuss each other's posters, poster presentations will span all 3 days of the main conference so as to give students one day to present their posters and two days to see others.  Space and logistics permitting, presenters will be able to leave their posters up all 3 days of the conference, affording additional opportunities to discuss them with other researchers, for example during coffee breaks. <br/><br/>Broader Impacts:  This activity supports one of NSF's core missions, to train more advanced professionals in Science, Technology, Engineering, and Mathematics (STEM).  Participating in the conference will provide the selected students with a unique opportunity to be exposed to current research directions in different research communities both domestic and foreign.  This is important for the field, because it has been recognized that transformative advances in research tend to derive from the melding of cross-disciplinary knowledge and multinational perspectives.  Participants will be encouraged to create a social network both among themselves and with senior researchers at a critical stage in their professional development, to form collaborative relationships, and to generate new research questions to be addressed during the coming years.  The PI will place high priority on supporting young researchers (intermediate and advanced doctoral students) from degree-granting institutions that lack the funding necessary to support attendance by their students at international conferences such as ITS."
"1036262","AAAI/SIGART 2010 Doctoral Consortium","IIS","ROBUST INTELLIGENCE","07/01/2010","06/22/2010","Christopher Brooks","CA","Association for the Advancement of Artificial Intelligence","Standard Grant","Edwina L. Rissland","06/30/2011","$16,817.00","Bradley Clement","cbrooks@cs.usfca.edu","2275 E BAYSHORE RD STE 160","East Palo Alto","CA","943033224","6503283123","CSE","7495","","$0.00","This award supports participation of doctoral students in the Fourteenth SIGART/AAAI Doctoral Consortium to be held July 11-12, 2010 in Atlanta, Geargia in conjunction with the 2010 AAAI Conference on Artificial Intelligence. The Doctoral Consortium aims to: (1) provide a setting for feedback on participants' current research and guidance on future research directions; (2) develop a supportive community of scholars and a spirit of collaborative research; and (3) support a new generation of researchers. The Doctoral Consortium organizers strive to recruit and include students from underrepresented groups and smaller schools and schools with less established programs in artificial intelligence. Students will give presentations and participate in discussion; there are one-on-one meeting with a faculty mentor. There will also be opportunities to discuss career issues in both academic and other career pathways. A report on the Consortium will be published in the AI Magazine."
"0953756","CAREER: New Directions in Computing Game-Theoretic Solutions: Commitment and Related Topics","IIS","ROBUST INTELLIGENCE, ALGORITHMIC FOUNDATIONS, COMPUT GAME THEORY & ECON","03/01/2010","02/19/2014","Vincent Conitzer","NC","Duke University","Continuing grant","Hector Munoz-Avila","02/28/2015","$500,002.00","","conitzer@cs.duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","7495, 7796, 7932","1045","$0.00","Game Theory occupies an important place in the foundations of multi-agent systems in artificial intelligence. Research under this award focuses on settings where one agent can commit to her (possibly randomized) strategy before the other agent moves. We consider how to compute optimal strategies to commit to in games with a combinatorial structure, extensive-form games, and repeated/stochastic games. Among other topics are connections to learning in games and mechanism/environment design, and the implications of commitment for the efficiency of computing game-theoretic solutions more generally.<br/><br/>The main objective of the research is to make scientific contributions to artificial intelligence, multi-agent systems, and computational game theory, but to also advance real-world applications. For example, other researchers have expanded on the PI's prior theoretical research (with his PhD advisor) to apply the commitment framework to security applications, such as the placement of checkpoints and canine units at Los Angeles International Airport and the scheduling of Federal Air Marshals. The research performed under this award aims to provide a solid scientific foundation for improving and expanding this and related applications. The award also helps to build connections between computer science and economics, the traditional home of Game Theory. This interdisciplinary link can help diversify the computer science community, intellectually and demographically. Research is tightly integrated with the PI's educational efforts, which include the development of courses in Computational Microeconomics and Game Theory and an approved Computational Economics minor."
"0957820","Automata in Science","SES","SCIENCE, TECH & SOCIETY","07/01/2010","05/17/2010","Elly Truitt","PA","Bryn Mawr College","Standard Grant","Linda Layne","06/30/2012","$146,158.00","","etruitt@brynmawr.edu","101 N. Merion Avenue","Bryn Mawr","PA","190102899","6105265298","SBE","7603","0000, 1353, OTHR","$0.00","Automata-artificial objects that are, or appear to be, self-moving-were culturally significant in medieval Europe. They appear as diplomatic gifts from distant rulers to European courts; in stories and legends and chronicles of distant lands and times; as manifestations of esoteric and sometimes forbidden knowledge; in courtly settings of great luxury; attached to monumental clockworks; as examples of technological innovation, and in the service of the Church. This research project examines the presence of automata in visual, textual, and material form in medieval Europe and, in the course of this examination, traces the interpenetration of scientific ideas, technological developments, philosophical theories, and cultural history. By examining different types of primary sources, including philosophical treatises, historical chronicles, scientific texts, archival documents, visual representations of automata, technical drawings, and literary sources, and using close analysis of textual, visual, and material sources, this study examines the developments in how automata were created from the twelfth to the fifteenth centuries, and how these shifts relate to developments in medieval natural philosophy and technology. Additionally, a particular theme of this project is how automata, and the knowledge needed to create them, were initially believed to be from the Arabic-speaking world, and were thus viewed with mistrust, suspicion, and fear, as well as desire and wonder. Over time, however, automata were decoupled from these origins, and took on new significance.  <br/> <br/>This research will make significant intellectual contributions to several different fields. By revealing that automata were central to western medieval society, it will contribute to histories of science and technology as well as establish the importance of science and technology in medieval history. The project will also have a broader impact on scholars and scientists who work on robots, artificial life, and artificial intelligence. This project will place these scholars' interests and research on contemporary science into historical context, showing that current ideas about artificial, self-moving objects and the categories used to organize them have an intellectual, cultural, and scientific history that has been largely invisible. Lastly, this research will compellingly demonstrate that ideas often assumed to be novel developments of early modern natural philosophy are in fact rooted in medieval philosophy, scientific culture, and technological developments."
"0960061","MRI-R2: Development of Common Platform for Unifying Humanoids Research","CNS","Major Research Instrumentation","07/01/2010","04/17/2014","Youngmoo Kim","PA","Drexel University","Standard Grant","Rita Rodriguez","09/30/2015","$5,999,997.00","Stefan Schaal, Yury Gogotsi, William Regli, Dennis Hong","ykim@drexel.edu","1505 Race St, 10th Floor","Philadelphia","PA","191021119","2158955849","CSE","1189","6890","$5,999,997.00","""This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5)."" <br/>Proposal #: 09-60061<br/>PI(s):  Kim, Youngmoo E., Gogotsi, Yury, Hong, Dennis H., Regli, William C., Schaal, Stefan<br/>Institution: Drexel University<br/>Title:    Development of Common Platform for Unifying Humanoids Research<br/>Project Proposed: <br/>This project, developing and disseminating HUBO+, a new common humanoid research platform instrument, enables novel and previously infeasible capabilities for future research efforts while working with a common instrument. HUBO will be the first homogeneous, full-sized humanoid to be used as a common research and education platform. Eight universities (Drexel, CMU, MIT, Ohio State, Penn, Purdue, Southern California, and VaTech), representing a critical mass of humanoids research within US, participate in this development of the world's first homogeneous full-sized humanoid team. Building upon unique expertise, the work extends current capabilities, resulting in six identical units, facilitating the following potentially transformative advances in robotics:<br/>- A state-of-the-art, standardized humanoid platform instrument with embedded capabilities for sensing, manipulation, and rapid locomotion, ideal for a broad range of future humanoids research<br/>- The ability, for the first time, to directly compare and across validate algorithms and methodologies and consistently benchmark results across research teams<br/>- Novel energy storage technology for mobile robotics incorporating supercapacitors for operations requiring high power density, far exceeding the capabilities of traditional battery-only power sources<br/>- A widely distributed platform that motivates, recruits, and trains a broad range of students spanning multiple disciplines, including artificial intelligence, digital, signal processing, mechanics, and control<br/>Humanoids, robots engineered to mimic human form and motion, open broad avenues of cross disciplinary research spanning multiple fields, such as mechanical control, artificial intelligence, and power systems. Common humanoids are rarely autonomous and are not-ready for unconstrained interaction with humans. The most compelling demonstrations are meticulously pre-programmed and painstakingly choreographed. A few common platforms have already advanced some research. Hence, having a consistent platform should facilitate rapid progress in areas needed for autonomy and natural interaction, including mobility, manipulation, robot vision, speech communication, and cognition and learning. However, although currently Japan and Korea are considered world leaders in design and construction of humanoids, best practices have not been developed for constructing multiple, identical humanoids. These conditions call for the making of an urgently needed benchmark providing evaluations and cross-validation of results. With this development and the servicing of 6 humanoids, this project aims to create knowledge and best practices contributing to robotics research, possibly leading to the standardization needed for ubiquity.<br/>Broader Impacts: <br/>The instrument enables US researchers to develop expertise in the design and construction of humanoids, while the distribution of the work activities ensures the broad dissemination of the knowledge. Humanoids research, inherently interdisciplinary and integrative, inspires young students. The graduate and undergraduates students participating are likely to receive a world-class training in robotics. Outreach partners, including several high-profile museums will introduce people of all ages to the exciting technologies of robotics, particularly useful in recruiting K-12 students into science, engineering, mathematics, etc. A partnership with the Science Leadership Academy (SLA), a magnet school with more than 63% underrepresented students, assures their involvement. With SLA, the project initiates an annual program modeled on a NASA-style experiment design competition, in which students use simulation tools to propose humanoids projects and activities. Selected winner(s) will have their proposed projects implemented on HUBO."
"1019841","Collaborative Research: INK-12:  Teaching and Learning Using Interactive Ink Inscriptions in K-12","DRL","Discovery Research K-12","09/01/2010","02/03/2017","Andee Rubin","MA","TERC Inc","Standard Grant","Margret Hjalmarson","06/30/2017","$1,065,548.00","","andee_rubin@terc.edu","2067 Massachusetts Avenue","Cambridge","MA","021401339","6178739600","EHR","7645","7645, 9177, SMET","$0.00","The research project continues a collaboration between MIT's Center for Educational Computing Initiatives and TERC focusing on the enhancement of K-12 STEM math and science education by means of technology that supports (1) creation of what are termed ""ink inscriptions""--handwritten sketches, graphs, maps, notes, etc. made on a computer using a pen-based interface, and (2) in-class communication of ink inscriptions via a set of connected wireless tablet computers. The project builds on the PIs' prior work, which demonstrated that both teachers and students benefit from such technology because they can easily draw and write on a tablet screens, thus using representations not possible with only a typical keyboard and mouse; and they can easily send such ink inscriptions to one another via wireless connectivity. This communication provides teachers the opportunity to view all the students' work and make decisions about which to share anonymously on a public classroom screen or on every student's screen in order to support discussion in a ""conversation-based"" classroom.  Artificial intelligence methods are used to analyze ink inscriptions in order to facilitate selection and discussion of student work.<br/><br/>The project is a series of design experiments beginning with the software that emerged from earlier exploratory work. The PIs conduct two cycles of experiments to examine how tablets affect students learning in 4th and 5th grade mathematics and science.  The project research questions and methods focus on systematic monitoring of teachers' and students' responses to the innovation in order to inform the development process. The PIs collect data on teachers' and students' use of the technology and on student learning outcomes and use those data as empirical evidence about the promise of the technology for improving STEM education in K-12 schools.  An external evaluator uses parallel data collection, conducting many of the same research activities as the core team and independently providing analysis to be correlated with other data.  His involvement is continuous and provides formative evaluation reports to the project through conferences, site visits, and conference calls.  <br/><br/>The primary products are substantiated research findings on the use of tablet computers, inscriptions, and networks in 4th and 5 grade classrooms.  In addition the PIs develop models for teacher education and use, and demonstrate the utility of artificial intelligence techniques in facilitating use of the technology.  With the addition of Malden Public Schools to the list of participating districts, which includes Cambridge Public Schools and Waltham Public Schools from earlier work, the project expands the field test sites to up 20 schools' classrooms."
"0939454","BEACON:  An NSF Center for the Study of Evolution in Action","DBI","STC Integrative Partnrshps Adm, CYBERINFRASTRUCTURE, STCs - 2010 Class, BEACON","08/01/2010","02/20/2020","Charles Ofria","MI","Michigan State University","Cooperative Agreement","Samuel Scheiner","07/31/2021","$48,035,209.00","Erik Goodman, Richard Lenski, Kay Holekamp, Charles Ofria, Robert Pennock","ofria@msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","BIO","1297, 7231, 8005, 8017","019Z, 7317, 7433, 7634, 9171, 9251","$0.00","The Bio/computational Evolution in Action CONsortium (BEACON) is a Science and Technology Center (STC) that enables research on evolutionary dynamics in natural and artificial systems and training of multi-disciplinary scientists in bio/computation, with a unique focus on the intersection of evolutionary biology, computer science, and engineering. The Center will enhance the development of applications of computational methods in biology, the use of artificial intelligence in computer science, and the enhancement of genetic algorithms in engineering design. Evolution by natural selection defines an algorithmic approach to finding solutions for complex problems; computer scientists and engineers have harnessed similar algorithms to a diversity of challenges that require optimization over multiple competing dimensions. Likewise, biologists have begun to employ digital modeling of the evolutionary process to examine evolution of complex biological structures and patterns in areas such as paleontology and the gene networks, which defy experimental manipulation in vivo. The Center will promote these interdisciplinary efforts by coordinating activities through three thrust groups: (1) Evolution of Genomes, Networks, and Evolvability, (2) Evolution of Behavior and Intelligence, and (3) Evolution of Communities and Collective Dynamics. <br/><br/>This center has the potential to transform both biology and computational sciences by developing digital experiments to test and apply fundamental principles of evolutionary biology. The possible impacts will be far reaching: from cyber-security to everyday computing applications, from the evolution of disease resistance to the self-organization of social behavior. The BEACON center will train the next generation of interdisciplinary scientists and educate the public about evolution and its role in solving real-world problems through significant educational outreach for K12 students and science museums."
"1018954","RI: Small: A Human-Level, Real-Time, Integrated Agent","IIS","Robust Intelligence","09/01/2010","04/19/2011","Arnav Jhala","CA","University of California-Santa Cruz","Standard Grant","Weng-keen Wong","01/31/2015","$464,090.00","Michael Mateas","ahjhala@ncsu.edu","1156 High Street","Santa Cruz","CA","950641077","8314595278","CSE","7495","7495, 7923, 9251","$0.00","This project is developing and integrating statistical and symbolic methods of Artificial Intelligence in an agent architecture and evaluating the agent in a competitive domain, notably the real-time strategy game StarCraft. Real-time strategy (RTS) games provide several interesting research challenges including real-time decision making, enormous state spaces and imperfect information. StarCraft is a popular commercial RTS game that has several professional gaming leagues, and therefore ideal for evaluating the performance of AI agents. Professional StarCraft players reason about and react to strategic decisions at multiple levels of abstraction, sometimes executing over 300 game actions per minute, so developing competition-level StarCraft agents presents extraordinary challenges.  <br/><br/>More specifically, the project is using novel supervised and unsupervised learning algorithms to automatically learn domain knowledge from collections of professional gameplay traces; the agent is being implemented within the reactive planning architecture ABL (A Behavior Language). The ABL reactive planner provides the glue for integrating multiple, heterogeneous reasoners within a real-time execution environment. <br/><br/>This work is expected to make significant contributions to the understanding of decision making processes  in a complex, real-time domain. This understanding will contribute to the development of robust, intelligent systems that can be deployed within real-world environments. This work will motivate AI researchers to build integrated agent architectures. As a well-known game with very high-level professional play, research in StarCraft AI has the potential to attract significant attention to AI research. The StarCraft competition being hosted by our lab  has attracted significant interest both within and outside academia, and at the high-school, undergraduate and graduate level. Thus, this work has the potential to raise general public awareness in research in human-level AI, and will encourage high-school students to pursue careers in computer science and game design."
"1020152","Collaborative Research: INK-12: Teaching and Learning Using Interactive Ink Inscriptions in K-12","DRL","Discovery Research K-12","09/01/2010","08/31/2016","Kimberle Koile","MA","Massachusetts Institute of Technology","Standard Grant","Margret Hjalmarson","02/28/2017","$1,891,343.00","","kkoile@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","EHR","7645","7645, 9177, SMET","$0.00","The research project continues a collaboration between MIT's Center for Educational Computing Initiatives and TERC focusing on the enhancement of K-12 STEM math and science education by means of technology that supports (1) creation of what are termed ""ink inscriptions""--handwritten sketches, graphs, maps, notes, etc. made on a computer using a pen-based interface, and (2) in-class communication of ink inscriptions via a set of connected wireless tablet computers. The project builds on the PIs' prior work, which demonstrated that both teachers and students benefit from such technology because they can easily draw and write on a tablet screens, thus using representations not possible with only a typical keyboard and mouse; and they can easily send such ink inscriptions to one another via wireless connectivity. This communication provides teachers the opportunity to view all the students' work and make decisions about which to share anonymously on a public classroom screen or on every student's screen in order to support discussion in a ""conversation-based"" classroom.  Artificial intelligence methods are used to analyze ink inscriptions in order to facilitate selection and discussion of student work.<br/><br/>The project is a series of design experiments beginning with the software that emerged from earlier exploratory work. The PIs conduct two cycles of experiments to examine how tablets affect students learning in 4th and 5th grade mathematics and science.  The project research questions and methods focus on systematic monitoring of teachers' and students' responses to the innovation in order to inform the development process. The PIs collect data on teachers' and students' use of the technology and on student learning outcomes and use those data as empirical evidence about the promise of the technology for improving STEM education in K-12 schools.  An external evaluator uses parallel data collection, conducting many of the same research activities as the core team and independently providing analysis to be correlated with other data.  His involvement is continuous and provides formative evaluation reports to the project through conferences, site visits, and conference calls.  <br/><br/>The primary products are substantiated research findings on the use of tablet computers, inscriptions, and networks in 4th and 5 grade classrooms.  In addition the PIs develop models for teacher education and use, and demonstrate the utility of artificial intelligence techniques in facilitating use of the technology.  With the addition of Malden Public Schools to the list of participating districts, which includes Cambridge Public Schools and Waltham Public Schools from earlier work, the project expands the field test sites to up 20 schools' classrooms."
"1048632","HCC: EAGER:  Authoring Game AIs by Demonstration for Real-Time Strategy Games","IIS","HCC-Human-Centered Computing","09/01/2010","04/14/2011","Ashwin Ram","GA","Georgia Tech Research Corporation","Standard Grant","William Bainbridge","02/29/2012","$315,898.00","","ashwin.ram@parc.com","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7367","7367, 7916, 9251","$0.00","This research will explore novel ""authoring by demonstration"" techniques for real-time strategy (RTS) games. Creating rich artificial intelligence (AI) behavior sets for complex computer games requires significant engineering effort. Developers need to anticipate all imaginable circumstances that the AI may encounter within the game world. The resulting AI is often static and results in predictable behaviors, detracting from the player experience. In addition, it is difficult for average players to create AI behaviors, without significant expertise in both AI and scripting. Modeling human-like goals and behaviors required for multiplayer games with semi-autonomous avatars adds additional complexity. This potentially transformative project will develop novel learning techniques that allow users to create intelligent behaviors simply by demonstrating them. The research will be done within the domain of RTS games, as these domains pose significant challenges that must be tackled in order to scale up the learning techniques to real-world tasks.<br/><br/>Case-based planners, hierarchical task network planners, or industry-standard behavior-tree execution engines require a library of base behaviors or methods in order to generate complete plans, which traditionally are coded by hand. The project will investigate ways to automate the process of generating such behavior libraries based on novel methods for learning strategic plans from user demonstrations. The techniques will be evaluated in the context of a case-based planning system for RTS games. RTS games are complex and involve strategic decision-making, multi-agent coordination, real-time interaction, and partially-observable environments. These properties pose significant challenges to existing AI methods for planning and learning. This research will make fundamental scientific contributions to learning, case-based reasoning, and AI for real-time strategic domains, addressing key problems in goal recognition, plan learning, and authoring support. <br/><br/>This research will enable game designers and other non-programmers to create the behavior sets for RTS games without requiring programming knowledge. This capability has two main consequences: first, it allows game developers to create games with less effort, and second it will enable a new genre of games where players would be able to create their own AIs as part of the game play. Additionally, as RTS games are essentially domain-specific simulations, the research will support authoring of behavior sets for domains such as simulation environments for training, real-time robotic control, organizational modeling for business decision-making, or sophisticated market simulations for economics strategy or public policy. The educational impact of the project is twofold. First, the project will constitute an important advance towards easy authoring of training simulators for educational applications that require environment with complex AI behaviors. This will enable development of new educational technologies with simulators or virtual worlds. Second, the project will involve undergraduate and graduate students in all phases of the work."
"1023246","Doctoral Consortium Travel Support: International Conference on Automated Planning and Scheduling","IIS","ROBUST INTELLIGENCE","05/01/2010","04/28/2010","Daniel Bryce","UT","Utah State University","Standard Grant","Edwina L. Rissland","04/30/2011","$18,000.00","","daniel.bryce@usu.edu","Sponsored Programs Office","Logan","UT","843221415","4357971226","CSE","7495","7495","$0.00","This award gives travel, housing, and registration-cost support to selected doctoral students from U.S. universities for their participation in the Doctoral Consortium of the 20th International Conference on Automated Planning and Scheduling (ICAPS-10) held May 13 in Toronto, Canada. ICAPS is the premier conference for research in artificial intelligence planning and scheduling, with relevance to a wide variety of applications such as software engineering, manufacturing, transportation, and robotics. The ICAPS-10 Doctoral Consortium includes a poster session, where students present their research, and a mentoring program that pairs senior scientists with doctoral students."
"0958298","Collaborative Research:II-NEW: A Digital/VLSI Test and Reliable Computing Research Laboratory","CNS","Special Projects - CNS, CCRI-CISE Cmnty Rsrch Infrstrc","06/01/2010","04/10/2012","Mohammed Niamat","OH","University of Toledo","Standard Grant","Almadena Chtchelkanova","05/31/2014","$167,730.00","Mansoor Alam, Rashmi Jha","mniamat@utnet.utoledo.edu","2801 W Bancroft St., MS 218","TOLEDO","OH","436063390","4195302844","CSE","1714, 7359","7359, 9218, 9251, HPCC","$0.00","Testing represents one of the major manufacturing costs in the semiconductor industry. Designing circuits with testability features significantly reduces testing costs and time. Thus, it is important for designers to be exposed to the concepts in testing which can help them design better and reliable products.<br/>In this collaborative project between the University of Toledo (UT) and Ohio Northern University (ONU), a ""Digital/VLSI Test and Reliable Computing Research Laboratory"" is being established for the development of computationally intensive algorithms and use of commercial CAD tools for testing digital, VLSI, and advanced semiconductor devices.<br/><br/>Some of the research projects being carried out include: Novel Testing Techniques for Quantum Cellular Automata (QCA) circuits; Built In Self Test (BIST) for Embedded SRAMS in System on Chips; Testing Look-Up-Table (LUT) Delay Aliasing Faults in SRAM Based FPGAs Using Half-Frequencies;  Analysis and Testing of Electromigration Failures; Testing and Modeling Soft Errors in FPGAs; Reliability Analysis and Device Failures in Advanced Semiconductor Devices; Reliability Issues Related to Power Consumption in VLSI Chips during Test; and Small Delay Defects and Test Generation.<br/><br/>Educational modules developed from the research carried out in this project are integrated into a number of graduate and undergraduate courses at ONU and UT. Since applications of semiconductor chips are far and wide, many different industries including auto, aerospace, defense and healthcare may benefit from this project."
"1036000","US-Turkey Advanced Study Institute on Biomedical Engineering Grand Challenges, September 20 -27, 2009,  Antalya, Turkey","OISE","Engineering of Biomed Systems, OTHER GLOBAL LEARNING & TRNING","01/18/2010","05/10/2010","Metin Akay","TX","University of Houston","Standard Grant","Carleen Maitland","08/31/2011","$49,919.00","","makay58@gmail.com","4800 Calhoun Boulevard","Houston","TX","772042015","7137435773","O/D","5345, 7731","0000, 004E, 5940, 5976, 7237, OTHR","$0.00","<br/>Proposal Number:  OISE-0908947<br/><br/>Principal Investigator:  Metin Akay<br/><br/>Institution:  Arizona State University<br/><br/>Title: US-Turkey Advanced Study Institute on Biomedical Engineering Grand Challenges, Anatalya, Turkey<br/><br/>The PI is organizing an advanced study institute to highlight the use of artificial intelligence in understanding brain function.  The National Academy of Engineering announced the 14 Grand Challenges for Engineering at the annual meeting of the American Association for the Advancement of Science. Included among these 14 challenges were Reverse?Engineer the Brain, Engineer better Medicines, and Advance Health Informatics.  Reverse-Engineering the Brain is an emerging discipline that promotes understanding of how the brain works.  Advanced computer intelligence should enable automated diagnosis and prescriptions for treatment.  To highlight these emerging disciplines, the First Advanced Study Institute on Biomedical Engineering will be devoted to Grand Challenges-Reverse Engineering the Brain and related research. It will be held Istanbul, Turkey and will introduce the attendees with biology backgrounds to the latest developments in the neural engineering. It will also be helpful to students in computer science and mathematics who are interested in doing research in neuroscience and neural engineering, as the advanced study institute will provide insights into the fundamental challenges in neuroscience and biology.   In terms of the broader impacts, further development of these topics may lead to the development of methods that allow doctors to forecast the benefits and side effects of potential treatments or cures.  The Advanced Study Institute is an opportunity for students to meet with leaders in the field.  This study institute will further stimulate interdisciplinary research and collaborations among engineers, mathematicians, computer scientists, neuroscientists and medical researchers.  In addition the study institute will aid in the understanding of complex neural systems and signals and in identifying the new directions of neuroscience and neural engineering. Options will be discussed for collaboration among researchers and students internationally.  This award is co-funded by Office of International Science and Engineering and the Biomedical Engineering Program.<br/><br/>"
"1018152","RI:  Small:  Understanding Value-based Multiagent Learning and Its Applications","IIS","ROBUST INTELLIGENCE","08/15/2010","08/01/2010","Michael Littman","NJ","Rutgers University New Brunswick","Standard Grant","James Donlon","03/31/2014","$450,000.00","","mlittman@cs.brown.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7495","7923, 9150","$0.00","This project explores the behavior of value-based learning methods in multi-agent environments. Value-based methods make decisions by using experience to estimate the utility impact of alternatives and choosing those with high predicted value. Because they evaluate components of behavior instead of treating behaviors as atomic units, they are computationally and statistically efficient. While these methods have been used in computational experiments for many years, only recently have researchers begun to formally characterize their behavior. Our own preliminary work is finding that some value-based methods exhibit super-Nash behavior, making them particularly worthy of study.<br/><br/>More specifically, we are analyzing, mathematically and experimentally, how value-based algorithms perform in several classes of simulated games of varying complexity from the artificial intelligence community, multi-agent engineering applications drawn from the wireless networking area, and as models of human and animal decision making in collaboration with cognitive neuroscientists. Where possible, we are refining existing value-based algorithms to work more efficiently, robustly, and generally than existing algorithms. We are also designing educational outreach activities, including creating entertaining instructional videos on how to promote cooperative behavior in real-life social dilemmas."
"1038216","Million Book Project Partners Meeting 2010","IIS","HCC-Human-Centered Computing","08/01/2010","07/27/2010","Gloriana St. Clair","PA","Carnegie-Mellon University","Standard Grant","William Bainbridge","07/31/2011","$39,970.00","","gstclair@andrew.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7367","","$0.00","This proposal requests funds for a ""Million Book Project"" partners research and coordination meeting.  Begun in 2000, the Million Book Project has scanned over 1.6 million books in China, India, Egypt and Australia and made great strides in research areas relevant to large-scale, multi-lingual database storage and retrieval. Project partners intend to continue to work together on issues related to human computer interactions, usability, automatic metadata detection and correction using artificial intelligence, intellectual property, machine translation and summarization, improving and providing centralized access to metadata, long term data storage and access issues, diversity and education.  Funding provided by the National Science Foundation has attracted international partners and matching funds exceeding $100 million U.S. dollars."
"0969430","AIS: Nonlinear Statistical Control Using Neural Networks","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","06/01/2010","05/02/2012","Chang-hee Won","PA","Temple University","Continuing grant","Paul Werbos","05/31/2014","$356,000.00","","cwon@temple.edu","1801 N. Broad Street","Philadelphia","PA","191226003","2157077547","ENG","7607","090E, 093E, 094E, 096E, 1653, 9251","$0.00","Abstract<br/><br/>The objective of this research is to discover new fundamental theories and algorithms for stochastic optimal control with application in solar energy transmission.  The approach is based on novel statistical control with cost cumulants and neural networks.  In statistical control, one views the performance index as a random variable and shapes the distribution of the performance index.  By shaping the performance index, more accurate control performance is achieved.<br/><br/>Intellectual Merit.  A new optimal control paradigm of performance shaping is proposed.   This paradigm will lead to a new direction of research in optimal control for the general nonlinear stochastic systems.  By using artificial neural networks, this project proposes to solve the nonlinear optimal control problems.  Nonlinear statistical control will allow high performance controllers for various applications.  This transformative optimal control theory is applied to a high societal impact application of space solar power harvesting.  Statistical control will significantly improve the solar power transmission performance.<br/><br/>Broader Impacts.  This project will advance the fundamental knowledge in optimal stochastic control.  By applying a new optimal control method to the space solar power transmission application, the energy will be transmitted more accurately, which means that the energy is harvested more economically.  This research will allow efficient space solar energy transmission by addressing the antenna pointing control problem.  Space solar energy harvesting is an issue of national strategic importance.  Furthermore, through this project and spacecraft systems engineering course, students of underrepresented groups will be educated about engineering and state-of-the-art research."
"1059577","EAGER:  Modeling and Visualization of Latent Communities","IIS","","09/15/2010","09/14/2010","Peter Brusilovsky","PA","University of Pittsburgh","Standard Grant","Sylvia Spengler","12/31/2011","$155,882.00","","peterb@mail.sis.pitt.edu","300 Murdoch Building","Pittsburgh","PA","152603203","4126247400","CSE","J394","7484, 7916","$0.00","In many areas of human professional and social life, people tend to form more or less clearly defined communities.   The main problem of these hidden or latent communities is that they are really hard to discover since the borders of these communities cut through various professional and organizational borders. The modern social Web, however, provides a huge volume of alternative data sources for discovering latent communities.  The goal of this proposal is to explore a range of promising approaches that can be used to elicit latent communities from various kinds of data about individuals available in the modern social Web and deliver the results for human thinking and interactive exploration through interactive visualizations. The visualization provided will allow humans explore and manipulate the results delivered by the new algorithms.  This will deliver results that are produced by the joint power of human and artificial intelligence. In the course of the project, the team will build several data sets combining data of several social Web systems and use these data sets to develop, evaluate, and compare several elicitation and visualization approaches.  The work will advance the research on latent communities, community and user modeling, and interactive social visualization. At the same time, the work will constitute one of the first attempts to use a variety of social Web data and a variety of approaches for community modeling. To increase the broader impact of the project, the researcher will apply the latent community knowledge to several practical tasks, such as identifying proper academic mentors and forming coherent collaboration groups.  They will also engage a number of students in the research advancing their training into this emerging field."
"0957438","Collaborative  Research: CI-ADDO-NEW: *-EXEC: A Cross-Community Solver Execution Service","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","05/01/2010","05/05/2010","Geoffrey Sutcliffe","FL","University of Miami","Standard Grant","Edwina L. Rissland","04/30/2012","$15,750.00","","geoff@cs.miami.edu","1320 S. Dixie Highway Suite 650","CORAL GABLES","FL","331462926","3052843924","CSE","7359","9218, HPCC","$0.00","Ongoing breakthroughs in nationally important research areas like Verification and Artificial Intelligence depend on continuing advances in high-performance automated theorem proving tools. The typical use of these tools is as backends: application problems are translated by an application tool into (typically very large and complex) logic formulas, which are then handed off to a logic solver. Different tradeoffs between linguistic expressiveness and the difficulty of solving the resulting problems give rise to different logics. Solver communities, formed around these different logics, have developed their own community research infrastructures to encourage innovation and ease adoption of their solver technology. Such infrastructure includes standard formats for logic formulas, libraries of benchmark formulas, and regular solver competitions to spur solver advances. <br/><br/>Currently, these different infrastructures are all developed separately, at significant cost in equipment and support. These costs are paid again and again for the different services, since there is currently no global piece of computing infrastructure suitable for the logic solving domain, which all these communities can use. This project is building a simplified proof-of-concept of a single piece of shared computing infrastructure, which could eventually be used by many different logic solver communities. The award also provides other support for soliciting research community feedback on the prototype and the design of a comprehensive infrastructure."
"1057624","EAGER:  Long-term View on Nanotechnology R&D as Reflected in Scientific Papers, Patents, and NSF Awards","CBET","SOCIETAL IMPLICATIONS OF NANO","09/15/2010","09/02/2010","Hsinchun Chen","AZ","University of Arizona","Standard Grant","Nora Savage","08/31/2014","$279,522.00","","hchen@eller.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","ENG","7702","011E, 014E, 080E, 7702, 7735, 7916","$0.00","The National Nanotechnology Initiative (NNI) has been underway since 2001, with attempts at coordinating federal work dating back even earlier. Over its history, NNI has been instrumental in establishing more than 60 state-of-the-art interdisciplinary research and education centers working in fields as diverse as health, aeronautics, energy, and defense. In this award, researchers at the Artificial Intelligence Laboratory at the University of Arizona will conduct a longitudinal examination of the output of the twenty years of nanotechnology research and development (1991-2010), including statistical trends and topic analysis. <br/><br/>This will be accomplished using a information diffusion model to understand the influences on nanotechnology R&D and the outcomes resulting from the many billions spent on these efforts over the past two decades. Sources of data will include articles and scientific papers, United States Patent and Trademark Office (USPTO) patents, and NSF awards. This modeling technique, typically used in the context of epidemics and the spread of infectious diseases, will be used to analyze emerging topics and possible future knowledge patterns. By examining the patterns by which information diffuses from scientific discovery into patents and commercial products, the model can be used to help estimate the probability of a granted patent document to be cited by others in the future. This is important as in a rapidly changing field such as nanotechnology, past performance is not necessarily the best predictor of future success.<br/><br/>The results of this analysis can help stakeholders, policymakers, and funding agencies such as NSF understand what the impact of funding and knowledge diffusion is on nanotechnology research and development. This understanding can then in turn be used as a tool to help influence future policies, procedures, and R&D funding."
"0966963","Intent Seeking Algorithms for New Human-Machine Interface","CBET","Disability & Rehab Engineering","08/01/2010","07/27/2010","Sanjay Joshi","CA","University of California-Davis","Standard Grant","alexander leonessa","07/31/2015","$283,362.00","","maejoshi@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","ENG","5342","010E","$0.00","PI: Joshi, Sanjay S.<br/>Proposal Number: 0966963<br/><br/>Project Summary: We have created a novel human-machine interface (HMI) technology for paralyzed persons which uses the surface electromyography (sEMG) signal of a single, facial muscle for simultaneous multidimensional control of external devices. Our new controller has the potential to significantly increase the quality of life for its users. Unlike many existing humancomputer interfaces for this population, our interface is: unobtrusive & inconspicuous, noninterfering with eyes/mouth/tongue, continuously available when needed, multifunctional, easy-to-use in almost any head position, and portable. We have recently discovered that humans can learn how to simultaneously manipulate power levels in two separate frequency-bands of a sEMG power spectrum (simply by contracting the muscle). Each frequency band becomes a separate control channel, which can simultaneously control different aspects of a device. Thus, we may exploit a single muscle?s natural electrical signals in far more complex ways than previously known. Using this underlying discovery, we have developed a new user interface that relies on a single head muscle?s surface EMG signal, which is easy to obtain and restricted to a small non-descript area near the ear. Our system is somewhat similar to some electroencephalographic (EEG) based brain-computer interfaces (BCI) in which a person learns to ?guide? a cursor to certain positions on a computer screen. These positions on the screen could be virtual buttons that open computer applications (human-computer interfaces), turn on/off lights (environmental control units), or control wheelchairs (mobility applications). Two central challenges in all ?cursor-guided? HMI systems are 1) intent (how does the computer know where the user intended to place the cursor?), and 2) speed at which the cursor can achieve the intended position. These two questions are intertwined in that earlier knowledge of intent can lead to faster systems. We propose to develop new ?intent-seeking? algorithms that could make our HMI much faster than our currently instantiated system. In addition, in order to conduct evaluation studies on subjects with disabilities who cannot leave either home or hospital, we will develop a new smaller mobile version of our hardware that is very easy to transport, setup, and use anywhere.<br/>Intellectual Merit: The use of a single sEMG signal for simultaneous multidimensional control in human-computer interfaces is potentially transformative. The notion of predicting the future location of a target (in this case a computer cursor) arises in many different applications (e.g. aerospace engineering, robotics, brain-computer interfaces). These applications employ a combination of mathematical and computer-science techniques including statistical decision making, optimal filtering, and artificial intelligence. We intend to draw from these fields to develop accurate, fast algorithms for our human-computer interface application. From a hardware perspective, entire new classes of computing devices are appearing that can perform complex computations and run graphics-intensive applications from a hand-held (or smaller) footprint.<br/>Designing our interface around these operating platforms will advance the area of highly portable and easy-to-use assistive interfaces.<br/>Broader Impact: A recent study initiated by the Reeve Foundation (2009) estimates that more than 5.5 million people live with paralysis in the United States. Many of the most severely paralyzed use ventilators to breathe, and are confined to certain head/body positions at different times during the day. Our goal is for severely paralyzed persons to regain some control of their surroundings and some basic independence. We are committed to including disabled persons in our research, not only as subjects but also as researchers themselves. As such, our work will create an additional broader impact in terms of research inclusiveness. In terms of intellectual broader impact, our new intent-seeking algorithms could have applications for many computer operating systems/programs for which disabled or non-disabled persons use various devices to guide cursors on a screen."
"1025682","VOSS: Culture and Coordination in Global Engineering Teams","OAC","ENGINEERING EDUCATION, INNOVATION & ORG SCIENCES(IOS), VIRTUAL ORGANIZATIONS","09/01/2010","07/07/2015","Catherine Cramton","VA","George Mason University","Standard Grant","Rajiv Ramnath","08/31/2016","$400,000.00","Tine Koehler, Raymond Levitt","ccramton@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","CSE","1340, 5376, 7642","110E, 7969","$0.00","Construction engineering provides the foundation for society: the buildings, bridges, roads and other infrastructure we use every day. Engineering work has become increasingly complex, and big engineering projects are almost always undertaken by teams of engineers whose members are multicultural and distributed around the globe.  Effective coordination is crucial for success, yet recent research suggests that team coordination practices vary with national culture. Experts have warned that engineering education fails to prepare engineers for these differences.<br/><br/>We will use scripts theory, which lies at the intersection of the fields of social psychology and artificial intelligence, to characterize the culturally-specific coordination practices of engineering teams, introducing the notion of ""cultural coordination scripts."" We will 1) characterize and understand the coordination scripts of several specific cultures, 2) show how cultural coordination scripts are intertwined with technical training and technical tasks and 3) determine how differences in cultural coordination scripts in multi-cultural globally-distributed teams affect their capacity to coordinate highly precise work and achieve innovative, safe, timely and cost-effective outcomes. We seek to predict likely points of coordination failure. Data collection will include unobtrusive longitudinal observation of the coordination activities of mono-cultural and multi-cultural collocated and distributed engineering design teams.<br/><br/>This research program will help engineers -- and others who work in multicultural, multi-national project teams -- become more adept in anticipating and working across different cultural expectations related to project coordination. Recommendations will be developed for engineering education and practice. Cultural differences in coordination practices are understudied across domains of application, so our work will provide new knowledge concerning this central team process. We also offer a new approach to capturing the complexity of teamwork ""an approach grounded in scripts theory"" that has the potential to influence research across disciplines."
"0953495","CAREER: Evolutionary Computation and Bioinformatics","IIS","Info Integration & Informatics, EPSCoR Co-Funding","06/01/2010","07/18/2014","Clare Congdon","ME","University of Southern Maine","Continuing grant","Sylvia Spengler","05/31/2016","$496,000.00","","congdon@gmail.com","96 Falmouth St","Portland","ME","041049300","2072288536","CSE","7364, 9150","1045, 1187, 7364, 9102, 9150, 9215, 9251, HPCC","$0.00","The research focus of this project is to develop a novel evolutionary computation-based approach for identifying candidate modules in non-coding DNA that respond to environmental toxins (such as arsenic) and that alter gene expression. These modules are composed of short pieces of DNA that are binding sites for proteins; the cooperative and combinatorial interactions are believed to contribute to the inducibility and specificity of environmentally responsive genes. Since each gene has an enormous number of possible modules, searching for them in the laboratory is untenable; even an exhaustive computational search for candidate modules is impractical, given the large space. Thus, the development of artificial intelligence techniques is called for.<br/><br/>This is an interdisciplinary proposal that makes contributions in both computer science and biology. The computational contributions include designing an effective search through the large and complex space of possible modules. While a few existing tools have been designed to search the thousand base pair region immediately upstream of the gene, the work here is designed to search significantly longer sections, 1 million base pairs and longer in length. The existing approaches cannot be expected to scale to the larger search, requiring the development of a novel approach.<br/><br/>The PI has plans for introducing undergraduates to research, both through coursework and in supervised research projects. This proposal will support and encourage the creation of a new upper-level course in informatics as well as the development of informatics-themed exercises to be incorporated at the introductory level. The project will further directly support undergraduate researchers who will contribute to the core research project.<br/>This project will result in a well integrated program of research and teaching for the PI, contribute to the available tools and our understanding of evolutionary computation approaches for informatics work, and introduce scores of students to this work."
"1040683","Promoting Expertise in Computational Cognitive Science","BCS","PERCEPTION, ACTION & COGNITION, ROBUST INTELLIGENCE","08/15/2010","08/08/2010","Tim Rogers","WI","University of Wisconsin-Madison","Standard Grant","Betty H. Tuller","07/31/2011","$25,000.00","","ttrogers@wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","SBE","7252, 7495","","$0.00","This award is to support tutorials at the intersection of artificial intelligence and cognitive psychology at the 2010 meeting of the Cognitive Science Society. Encouraging computational sophistication in the next generation of cognitive scientists, and encouraging interdisciplinarity in researchers at all career stages, are key elements of the training mission of the Cognitive Science Society. Access to information presented at the conference will also be provided through Web broadcasts of the symposia and a special issue of the Society journal with an emphasis on the teaching mission of workshop attendees."
"0966086","Attracting and Retaining Promising Transfer Students into STEM Fields","DUE","S-STEM-Schlr Sci Tech Eng&Math","07/01/2010","06/17/2011","Jaime Davila","MA","Hampshire College","Continuing Grant","Elizabeth Teles","06/30/2017","$441,550.00","Christopher Jarvis","jdavila@hampshire.edu","893 West Street","Amherst","MA","010023372","4135595378","EHR","1536","9178, SMET","$0.00","The effort of the college to open access to transfer students comes at a time when increasingly sophisticated methodological methods are being developed to forge connections across science, technology, engineering, and mathematics (STEM) fields. This project acts as a catalyst for these developments and offers up to 20 transfer students a pathway to achieve bachelor degrees. This project is motivated by the institutional objective to better attract and retain science concentrators in STEM fields. Additionally, a core team of faculty, whose work investigates questions in computational science, artificial intelligence, and the biological sciences, are interested in building structural and cultural cohesion around concentrators who may already be motivated by work and life experiences to excel in STEM fields. <br/><br/>Intellectual Merit: While the college has done well attracting underrepresented students in STEM fields, the overall attrition rate for underrepresented groups is somewhat high. The college has begun to take deliberate and careful measures to address these attrition rates. This project allows the college to offer a more robust financial aid package to help to attract and retain transfer students. These transfer students provide diversification, which strengthens the overall demographic makeup of the college and offers a more diverse range of perspectives in the classroom.<br/><br/>Broader Impact: The inquiry-driven, cross-disciplinary approach to teaching science at the college has demonstrated success in attracting students from diverse backgrounds to STEM fields. According to recent admissions data, the college graduates more science concentrators (15%) than it initially attracts (10%). Moreover, from 2005-08, women and students from underrepresented groups comprised 70% of science concentrators. Many of these students enter a range of scientific and medical careers, or pursue graduate study upon matriculating. This project allows faculty to work with a greater number of students and is expected to result in higher numbers of talented graduates prepared to make meaningful contributions to national and international scientific challenges."
"0954138","CAREER: Reasoning under Uncertainty in Cybersecurity","CNS","TRUSTWORTHY COMPUTING, Secure &Trustworthy Cyberspace, EPSCoR Co-Funding","03/01/2010","03/10/2014","Xinming Ou","KS","Kansas State University","Continuing grant","Sylvia J. Spengler","03/31/2016","$457,373.00","","xou@usf.edu","2 FAIRCHILD HALL","Manhattan","KS","665061100","7855326804","CSE","7795, 8060, 9150","1045, 7434, 7923, 9150, 9168, 9178, 9251","$0.00","Cyber security, like security in the physical world, relies upon investigation methodologies that piece together dispersed evidence spread across multiple places, and come to a conclusion on what security breaches have happened and how they happened. While effective evidential reasoning based on manual analysis are used in the physical world by law-enforcement agencies, in the cyber world we need automated reasoning methodologies to handle the automated cyber attacks against our nation's information infrastructures every day. This research aims at discovering and developing such automated reasoning methodologies. The problem is  difficult due to the uncertain nature of such reasoning, which is compounded by the characteristics of cyber attacks.<br/><br/>The uncertainty in cyber security comes from two sources. The first is the uncertainty from not knowing the attacker's actions and choices. Since hackers are essentially invisible in the cyberworld, we have to rely upon various types of sensors that report symptoms of potential attacks. The second source of uncertainty comes from these sensors. Since in most cases the symptoms of cyber<br/>attacks significantly overlap with symptoms from benign network activities, it is not possible to rely on a single sensor to give an absolutely correct judgment on whether an attack has happened and succeeded. A key question is how to use these imperfect sensors to conduct reasoning so that one can come up with almost certain conclusions regarding a system's security status. <br/><br/>This challenge of reasoning under uncertainty is not new. In the past four decades computer science researchers have developed an array of reasoning models and methods for uncertainty, especially in the area of artificial intelligence. However, the emergence of cyber threats poses a new<br/>challenge to this problem. The existing methodologies typically require a knowledge-engineering process to build a knowledge model for the problem domain. This has worked reasonably well with the more static and well-behaved problem domains such as disease diagnosis. A key difference between these problem domains and cyber security is that the latter has to deal with an active<br/>malicious attacker who will try to break whatever assumptions made in the reasoning model. For this reason, the knowledge model for cyber security cannot be static because then they can be easily evaded. What will be an effective and practical knowledge engineering approach to handle the uncertainty in cyber security is the biggest open problem that needs to be answered from the<br/>research.<br/><br/>This research adopts an empirical, bottom-up approach to tackle the above challenges. Instead of starting from the existing theories, the PI will start from empirical study on how a human security analysts would reason about cyber events and try to capture the essence of the reasoning in the process. Then, the PI will carry out this empirical study by running intrusion detection sensors on production networks and work with system administrators to understand and reason about the alerts. The next step is to develop a reasoning model that simulates the human reasoning process, and apply the automated reasoning engine on fresh new data to see how it fares. In this spiral theory development process the PI can always make sure that the methodologies are applicable to real cyber-security analysis and constantly find gaps in the model that reveal what will be the most appropriate theories and how to apply them in this problem. The eventual goal is to find the right theoretical framework for reasoning under uncertainty in cyber-security, and validate such theories through repeatable experiments on data from production systems.<br/><br/>This research is tightly integrated into the PI?s education efforts both for students and targeted at the society at large. The empirical nature of the research provides a valuable venue for dialogue between security practitioners and researchers, which will result in a two-way education process: students working on the project can acquire the essential skills of applying advanced knowledge to a practical problem; and security practitioners like system administrators can learn the state-of-the art in cyber security technology through collaborative work with the research team. The empirical study carried out from the research will provide endless data and examples to refresh the materials of the cyber-security courses taught by the PI. New courses with a focus on uncertainty in cyber security defense will be developed. There will be a number of undergraduate students who take part in the research efforts, which will provide a unique education experience for them. Moreover, the test-bed infrastructure produced from the research will also be used as an education platform for the general public about cyber-security problems, with the help of the out-reach programs already established at Kansas State University."
"1025453","CMG Collaborative Research: Non-assimilation Fusion of Data and Models","DMS","OPPORTUNITIES FOR RESEARCH CMG","08/01/2010","08/26/2010","Leonid Piterbarg","CA","University of Southern California","Standard Grant","Junping Wang","07/31/2014","$265,841.00","","piter@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","MPS","7215","1303, 7215, 7232","$0.00","The PIs will develop a methodology for improving estimate and prediction of the state of a dynamical system, with particular focus on analyzing ocean dynamics. The primary goals of this project are thus to develop innovative approaches for representation and manipulation of data uncertainty and model error using a fuzzy set formulation and to then apply these approaches for the data and model fusion formulated as the global optimization problem. Convenient and fast numerical algorithms will be developed to solve the problem using high-performance parallel computing. Such an approach differs from usual statistical estimates but with advantages and drawbacks of its own. The general mathematical theory will be applied to a long-standing but important problem of improving estimates and prediction of the state of the ocean. In particular, the proposed study targets a synthesis of submesoscale/mesoscale fronts, jets and eddies by fusing satellite observations, float and shipboard data of lower resolution, as well as ROMS simulation results for Central California. The theory should provide new tools to be applied in oceanography, meteorology, climatology, artificial intelligence, computer science, control engineering, decision theory, expert systems, operational research and pattern recognition. As the first step in using these tools for broader oceanography community goals, the fusion approach will be applied to different data bases to understand and quantify heat storage and carbon content of the North Atlantic in collaboration with scientists from Great Britain and Germany and to allow junior scientists to obtain excellent training and learning in cross disciplinary/multi-disciplinary areas of great scientific and practical importance. <br/><br/>The PIs will address a long-standing but important problem involved with improving the estimation and prediction of the state of the ocean. The primary goals of this project are to develop an innovative approach for representation and manipulation of uncertainty coming from a wide variety of sources such as sensor outputs, model outputs, aggregating expert opinions as well as merging different databases and data even when distinct pieces of information are contradictory, and to suggest methods to fuse this information in decision making goals. The study will provide new mathematical theory and tools relevant for this problem, but also for more general applications in oceanography, meteorology and climatology. Mathematically the approach uses a fuzzy set formulation which originated in pure mathematics and which will be adapted for representing and manipulating data uncertainty and ocean model error. Results of the work will advance development of new forecast metrics in terms of fuzzy sets as well as new methods for quantification of model predictability through data-model and model-model comparisons at weather and climatic scales. As the first step in using these tools for broader oceanography community goals, the approach will be applied to different data bases which relate to quantifying heat storage and carbon content of the North Atlantic. The PIs will collaborate with scientists from Great Britain and Germany. Junior scientists involved in the project will obtain excellent training and learning in cross disciplinary/multi-disciplinary areas of great scientific and practical importance."
"0958160","Collaborative  Research: CI-ADDO-NEW: *-EXEC: A Cross-Community Solver Execution Service","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","05/01/2010","05/05/2010","Aaron Stump","IA","University of Iowa","Standard Grant","Todd Leen","04/30/2012","$84,197.00","Cesare Tinelli","aaron-stump@uiowa.edu","2 GILMORE HALL","IOWA CITY","IA","522421320","3193352123","CSE","7359","9218, HPCC","$0.00","Ongoing breakthroughs in nationally important research areas like Verification and Artificial Intelligence depend on continuing advances in high-performance automated theorem proving tools. The typical use of these tools is as backends: application problems are translated by an application tool into (typically very large and complex) logic formulas, which are then handed off to a logic solver. Different tradeoffs between linguistic expressiveness and the difficulty of solving the resulting problems give rise to different logics. Solver communities, formed around these different logics, have developed their own community research infrastructures to encourage innovation and ease adoption of their solver technology. Such infrastructure includes standard formats for logic formulas, libraries of benchmark formulas, and regular solver competitions to spur solver advances.<br/><br/>Currently, these different infrastructures are all developed separately, at significant cost in equipment and support. These costs are paid again and again for the different services, since there is currently no global piece of computing infrastructure suitable for the logic solving domain, which all these communities can use. This project is building a simplified proof-of-concept of a single piece of shared computing infrastructure, which could eventually be used by many different logic solver communities. The award also provides other support for soliciting research community feedback on the prototype and the design of a comprehensive infrastructure."
"1016509","AF:  Small:  Beyond Worst-Case Analysis in Approximation Algorithms, Algorithmic Mechanism Design and Online Algorithms","CCF","ALGORITHMS, COMPUT GAME THEORY & ECON","08/01/2010","07/23/2010","Anna Karlin","WA","University of Washington","Standard Grant","Balasubramanian Kalyanasundaram","07/31/2014","$499,847.00","","karlin@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","7926, 7932","9218, HPCC","$0.00","In theoretical computer science, algorithms are usually evaluated with respect to their worst-case performance, whereas in other areas, average-case analysis is often used. Both of these approaches have drawbacks: worst-case analysis is overly pessimistic and average-case analysis often rests on unrealistic assumptions. To address these issues, a number of other analysis frameworks have been proposed including self-improving algorithms, smoothed analysis, instance-optimality, and algorithmic design based on a variety of data models. The objective of the project is to continue this line of research and develop techniques that go beyond worst-case analysis in the areas of approximation algorithms, algorithmic mechanism design and online algorithms.  In the area of approximation algorithms for NP-hard problems, the project focuses on the development of approximation algorithms that achieve a kind of instance optimality. In the area of algorithmic mechanism design, the PI will continue to study the design and analysis of profit maximizing auctions in single-parameter environments and beyond. In the area of online algorithms, the PI will work to develop effective online algorithms for a fundamental and practical self-organizing data structure problem.<br/><br/>Through the development of more effective and practical algorithms and a deeper understanding of the performance of these algorithms in practice, this research has the potential to impact a variety of subfields of computer science including artificial intelligence, systems and networking, data mining, and electronic commerce."
"1004842","REU Site:   Undergraduate Research in Computational Biology at Mississippi State University","DBI","RSCH EXPER FOR UNDERGRAD SITES, EPSCoR Co-Funding","03/01/2010","01/27/2014","Andy Perkins","MS","Mississippi State University","Standard Grant","Sally E. O'Connor","05/31/2014","$286,250.00","Susan Bridges","perkins@cse.msstate.edu","PO Box 6156","MISSISSIPPI STATE","MS","397629662","6623257404","BIO","1139, 9150","9150, 9178, 9250, 9251, SMET","$0.00","An award has been made to Mississippi State University that will provide research training for 10 weeks for 10 students, during the summers of 2010-2012. This project is supported by the Directorates for Biological Sciences (BIO) and Computer Information Science and Engineering (CISE). Undergraduate students will be recruited from institutions throughout Mississippi and the surrounding area to work on interdisciplinary research projects in computational biology. Students will work closely with their faculty mentors on projects in the areas of genome analysis, functional genomics, artificial intelligence, scientific visualization, and bio-ontologies, among others. Recruitment will be targeted at colleges and universities within Mississippi and the surrounding states with the goal of providing a meaningful research experience to promising undergraduate students who otherwise would not have had the opportunity to engage in research. There is a particular focus on increasing the participation of underrepresented groups in computational biology research. Educational and professional development sessions will be conducted to train students in the basics of computational biology and to provide information on graduate school and pursuing scientific careers. Students will also receive training and mentoring in the responsible conduct of research. The program will culminate with a poster session where the students will present their research findings to the computational biology community at Mississippi State University. Program objectives will be assessed through the use of a common web-based assessment tool. More information about this program can be obtained by contacting the project PI, Dr. Andy Perkins (perkins@cse.msstate.edu) at (662) 325-0004, or by visiting the project website at http://www.cse.msstate.edu/~compbio/."
"1029082","The Emergence of Cognitive Flexibility in Neural-Behavioral Systems","BCS","DS -Developmental Sciences, Perception, Action & Cognition, EPSCoR Co-Funding","10/01/2010","08/13/2015","Richard Hazeltine","IA","University of Iowa","Continuing Grant","Betty Tuller","06/30/2016","$1,130,883.00","Gregor Schoner, Richard Hazeltine, Rodica Curtu, Vincent Magnotta","eliot-hazeltine@uiowa.edu","2 GILMORE HALL","IOWA CITY","IA","522421320","3193352123","SBE","1698, 7252, 9150","1698, 7956, 7969","$0.00","The goal of this project is to understand how the brain realizes the impressive flexibility that is a hallmark of human cognition.  For example, adults can flexibly shift from conversing about current events, to feeding the cats, to cooking dinner, all without getting mixed up and cooking cat food as the main course. This ability is thought to rely on the actions and interactions of multiple brain areas. A central challenge in understanding cognitive flexibility is to understand how these brain networks change with learning and how they organize and re-organize ""on-the-fly"" depending on the situation. More generally, how does the brain keep track of events as they unfold but at the same time retain flexibility?<br/><br/>In this project, a multidisciplinary team of investigators will test a new theory of cognitive flexibility using computer modeling and functional neuroimaging data. The theory is implemented using dynamic neural fields -a specific type of neural network that can be simulated on a computer and that specifies how different areas of the cortex interact during complex tasks. Some cortical areas actively maintain neural patterns related to the lower-level details of what is happening, for instance, maintaining information about critical visual features of the pan, the items you are cooking for dinner, and so on. Other cortical areas modulate what these systems are up to. Critically, ""higher-level"" systems do not need to understand all the details of what is going on. They just need to help decide which general patterns of information are important in the context. It is the dialog between these different neural patterns that makes cognition flexible.<br/><br/>The success of this project would have far-reaching effects. The ability to modulate behavior in context-specific ways is a central achievement that impacts language skills, mathematical abilities, school and work performance, and IQ. Moreover, deficits in cognitive flexibility and so-called executive functions underlie different forms of psychopathology, such as schizophrenia, as well as many of the challenges faced by aging adults.  The investigators have also established a collaboration with the Iowa Children's Museum where they will work with museum staff to design an interactive display around the theme of ""Brain Play,"" in which children construct simple solar-powered robots that embody how the brain and body realize flexibility."
"0949051","Probing Motoneuron Dendritic Integration during Locomotion with Targeted Ion Channel Manipulation in Drosophila","IOS","Activation","08/15/2010","05/31/2012","Carsten Duch","AZ","Arizona State University","Continuing grant","David Coppola","07/31/2013","$523,766.00","Richard Levine","carsten.duch@asu.edu","ORSPA","TEMPE","AZ","852816011","4809655479","BIO","7713","1096, 9178, 9179","$0.00","In all animals, motor behaviors such as walking, breathing, swimming, or flying rely upon the coordinated patterns of muscle contractions, which in turn, depend on information processing in neural circuits located in the nervous system. Motoneurons transfer neural information from the central neural networks to the muscles. Although the neural network plays critical roles in generating the coordinated information output to the muscles, motoneurons are equipped with intrinsic membrane currents that also sculpt the final motor output. Motoneuron membrane properties are further influenced by neuromodulatory substances which can be released during motor activity to shape patterned motor output. Despite a half century of studying motor pattern generation, our knowledge of how motoneuron membrane currents shape patterned motor output remains fragmentary. One major experimental problem in studying the specific functions of motoneuron membrane currents has been that most pharmacological or genetic loss-of function experiments target all neurons of the network. This project will use the genetic tools available in Drosophila to target genetic manipulations specifically to motoneurons without affecting the rest of the network.  In a comparative approach the experiments will focus on two locomotor activities with different requirements for motoneuron activation; larval crawling and adult flight. In the first step, the study will determine which genes code in motoneurons for calcium, calcium-activated potassium, and hyperpolarization activated currents. Based on this, specific currents will be manipulated genetically only in these motoneurons to test their functions by recordings during motor behavior. The investigators expect to unravel new motor strategies and cellular properties that are relevant to normal motoneuron function and to understanding defects of motoneurons perturbed by disease or injury.  The Broader Impacts of this work include interdisciplinary training of undergraduates and graduate students.  Additional educational activities will include training of students from underrepresented groups; for instance, the PI will recruit students from the local Bioscience High School and mentor these students while the students are involved in the proposed research.  Finally, the PI will contribute to the University?s Minority Access to Research Careers program to increase participation by underrepresented undergraduate students in STEM."
"1037829","CMG COLLABORATIVE RESEARCH: Development of New Statistical Learning Theory and Techniques for Improvement of Convection Parameterization in Climate Models","DMS","OPPORTUNITIES FOR RESEARCH CMG, MATHEMATICAL GEOSCIENCES","03/16/2010","04/11/2012","Philip Rasch","WA","Battelle Memorial Institute","Standard Grant","Junping Wang","09/30/2012","$128,630.00","","phil.rasch@pnnl.gov","Battelle Blvd","Richland","WA","993541794","5093717608","MPS","7215, 7232","0000, 7232, 7303, OTHR","$0.00","This proposal focuses upon two interconnected and equally important problems. The first of them is developing a new Statistical Learning Theory (SLT) dedicated to modeling specific complex systems. The second one is to develop a new convection representation for numerical climate models. Understanding climate and weather is important to science, society and the economy. The processes we focus upon (clouds, and particularly convection) are critical to climate and weather. The proposal involves a novel approach to improving the representation of those processes. Our goal is to combine a team of mathematical scientists with expertise in SLT, and atmospheric scientists with expertise in cloud modeling and climate system modeling to produce an innovative representation for convection in the atmospheric models used for numerical weather prediction and climate change studies. The project will develop an SLT system that emulates the statistical behavior of a more realistic but very expensive high resolution Cloud System Resolving Model (CSRM) in a variety of cloud regimes. Employing even the simplest of these CSRM frameworks in a large scale model increases the cost of today?s atmospheric models by factors of thousands, which make their use impractical for many studies. By emulating the behavior of these more realistic frameworks in a large scale model we develop a new SLT parameterization, dramatically reducing the cost of the more realistic representations of model convection, and providing an opportunity to address problems currently viewed as critical within the scientific community. By developing the application-oriented SLTs we hope to make the more realistic cloud and convective formulations currently being explored, computationally feasible and use them in climate models. <br/><br/>This proposal combines research used in the computational statistics scientific community with climate science. One of the most important components of the climate system is the representation of clouds. They control many aspects of the energy and heat that enter and leave the climate system, and they interact with many components of the earth system (agriculture, weather, society, and the economy). But clouds are so complex that they can not be treated very precisely in models that are used for understanding climate and weather. The equations required to represent clouds are so complex that a precise treatment would slow down current models by factors of thousands or millions. Current computational climate and weather models cannot afford a precise representation of clouds so faster approximate treatments of clouds are needed. Traditional representations for clouds in climate and weather models are not sufficiently accurate, and progress has been slow in improving these model components. This proposal employs advanced statistical-mathematical methods to try and improve the situation. These methods (called Statistical Learning Theory or SLT) allow one to represent very complex systems with accurate, and very fast approximations. We are going to try to approximate very detailed, complex and expensive models of convective clouds using SLT to produce an accurate approximation for clouds with the goal of using this approximation (these approximations are frequently called a parameterization in climate and weather models). This research will push forward the knowledge base used in both the SLT community, and the climate community."
"1002333","Integrating Dynamic Decision Making with Neurocontrollers by Combining System and Cognitive Sciences","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","09/15/2010","09/24/2010","Sivasubramanya Balakrishnan","MO","Missouri University of Science and Technology","Standard Grant","Radhakisan Baheti","08/31/2014","$219,247.00","","bala@mst.edu","300 W 12th Street","Rolla","MO","654096506","5733414134","ENG","7607","","$0.00","Project Summary<br/><br/>The objective of this research is to develop new neural network structures to solve optimal control problems with dynamic decision making. These problems are quite complex since the system dynamics could switch modes at unknown times based on event based decision making. The approach is to develop the decision-making paradigms from cognitive science principles but their mathematical representations will use Decision Field Theory. Their solutions contained in neural networks will interact with another set of networks that embed solutions to the related optimal control problem formulated in an approximate dynamic programming framework.<br/><br/>Intellectual Merit <br/><br/>This research seeks to find unified controller solutions to problems which have both continuous and discrete elements in them. It is expected that the mathematical cognitive science ideas developed will lead to new representations and problem solving structures in computational neuroscience and control. The work proposed in this effort seeks to accomplish these objectives by offering a transformative approach that integrates concepts from system science and cognitive science. <br/><br/>Broader Impact <br/><br/>Abstractions and solution structures developed through this research can be used in consequence or emergency management systems like managing the aftermath of an earthquake, retrieving an impaired aircraft to stability and sustainable motion and landing, and managing multiple assets and allocation in striking responses to threats. Decision making structures resulting from this research can make tremendous impact on human-machine interactions too. For example, driver aid systems can be developed to augment human perception and enhance their cognition when they drive under impaired conditions."
"1048366","EAGER: Persuasive Sensing Networks: A New Frontier to Changing Human Behavior","CNS","Networking Technology and Syst","08/01/2010","07/23/2010","Samir Chatterjee","CA","Claremont Graduate University","Standard Grant","Thyagarajan Nandagopal","06/30/2012","$149,619.00","","samir.chatterjee@cgu.edu","150 East Tenth Street","Claremont","CA","917115909","9096079296","CSE","7363","7916","$0.00","This project explores a new frontier called Persuasive Sensing that brings together advances happening in two fields, namely sensor networks and persuasive technology. Today, advances in sensor networks are making it possible to capture, detect, and analyze data. However what is missing is to present relevant information mined from sensor data to subjects about their daily life and activity rhythms and using feedback mechanisms to alter human behavior. It is now demonstrated that human beings normally follow an approximately 24-hour fluctuating rhythm known as circadian activity rhythm. This is an exploratory research project to design and engineer such a prototype system. The data obtained from environmental sensors as well as body-wearable sensors are fused together to generate meaningful feedback to persuade end-users.<br/><br/>This research project is novel in several ways. First, the idea to fuse environmental sensor data that detects activity in space (or location) along with body-wearable sensors that collects physiological health data and utilize both to detect circadian activity rhythms has not be done before. Second, while sensor networks have been used in healthcare applications before, our persuasive feedback based on mining the data and benchmarking it against normal activity and the ability to detect patterns that identify onset of diseases or pathologies is novel. Third, the artificial neural network data mining algorithms that can identify patterns from circadian activity rhythms and then match those against a database of known rhythms will be a significant novel contribution. Finally, people are very different when taking suggestions. In order to achieve effectiveness, the system needs to monitor and learn from human reactions from previous suggestions and adapt. Broader impact includes the use of wireless sensor networks along with persuasive technology design that will open up new possibilities for prevention and help address chronic diseases such as obesity and diabetes. Employing post docs and graduate students, this project will train and educate the next generation of workforce in Healthcare IT and integrate research findings into classroom. Findings from the research will be incorporated into a graduate level course at Claremont Graduate University."
"1021646","The function of glycine in modulation of cone visual sensitivity","IOS","Activation","08/15/2010","06/03/2014","Wen Shen","FL","Florida Atlantic University","Standard Grant","Sridhar Raghavachari","07/31/2015","$416,467.00","","wshen@fau.edu","777 GLADES RD","BOCA RATON","FL","334316424","5612970777","BIO","7713","1096, 9178, 9179","$0.00","This project studies the potential role of glycine in fine-tuning cone visual sensitivity within retinas.  Cone photoreceptors are sensitive in bright light and less sensitive in dim light conditions.  Flickering light can enhance cone visual sensitivity in dim light conditions, and changing cone visual sensitivity is a neural modulatory process occurring within retina.  The mechanism behind this altered cone sensitivity is unknown.  Preliminary studies show that glycine released as a neurotransmitter in the retina results in an increase of cone signaling in dim flickering light. The goal of the proposed research is to discover the novel mechanism(s) involved in modulation of neurotransmission from cones to second-order retinal neurons in dim light.  State-of-the-art techniques, including electrophysiological recording and fluorescence imaging, will be used to investigate the affects of flickering light-induced glycine release in the retinal neural network and the action of glycine in enhancing cone signaling to the second-order neurons.  The experiments will be carried out in salamander retinas commonly used as a model system to study retinal network modulation.  The results from the proposed project will serve for better understanding the functional interactions of neural networks and their behavioral adaptations.  There will also be training of undergraduate and graduate students in the technically demanding areas of electrophysiology, in vivo imaging, and immunocytochemistry. This research and broader impacts will enable the important mission of training future scientists, many from underrepresented populations, in these technically demanding areas of science. Also, this work will promote teaching and learning in the lifelong learning program for seniors in southern Florida."
"1004422","PostDoctoral Research Fellowship","DMS","WORKFORCE IN THE MATHEMAT SCI","08/01/2010","05/14/2010","Zachary Kilpatrick","UT","Kilpatrick Zachary P","Fellowship","Bruce P. Palka","07/31/2014","$135,000.00","","","","Salt Lake City","UT","841120090","","MPS","7335","9219","$0.00",""
"1002188","Collaborative Research: Integrating dynamic decision making with neurocontrollers by combining system and cognitive sciences","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","09/15/2010","09/24/2010","Jerome Busemeyer","IN","Indiana University","Standard Grant","Paul Werbos","08/31/2014","$136,645.00","","jbusemey@indiana.edu","509 E 3RD ST","Bloomington","IN","474013654","3172783473","ENG","7607","","$0.00","Project Summary<br/><br/>The objective of this research is to develop new neural network structures to solve optimal control problems with dynamic decision making. These problems are quite complex since the system dynamics could switch modes at unknown times based on event based decision making. The approach is to develop the decision-making paradigms from cognitive science principles but their mathematical representations will use Decision Field Theory. Their solutions contained in neural networks will interact with another set of networks that embed solutions to the related optimal control problem formulated in an approximate dynamic programming framework.<br/><br/>Intellectual Merit <br/><br/>This research seeks to find unified controller solutions to problems which have both continuous and discrete elements in them. It is expected that the mathematical cognitive science ideas developed will lead to new representations and problem solving structures in computational neuroscience and control. The work proposed in this effort seeks to accomplish these objectives by offering a transformative approach that integrates concepts from system science and cognitive science. <br/><br/>Broader Impact <br/><br/>Abstractions and solution structures developed through this research can be used in consequence or emergency management systems like managing the aftermath of an earthquake, retrieving an impaired aircraft to stability and sustainable motion and landing, and managing multiple assets and allocation in striking responses to threats. Decision making structures resulting from this research can make tremendous impact on human-machine interactions too. For example, driver aid systems can be developed to augment human perception and enhance their cognition when they drive under impaired conditions."
"1018786","NeTS: Small: Dynamic Spectrum Access for Mission Critical Wireless Networks","CNS","Networking Technology and Syst","08/01/2010","07/20/2010","Dennis Roberson","IL","Illinois Institute of Technology","Standard Grant","Thyagarajan Nandagopal","07/31/2014","$450,000.00","","dennis.roberson@iit.edu","10 West 35th Street","Chicago","IL","606163717","3125673035","CSE","7363","7923, 9150","$0.00","Public safety first responders have a need for increased access to radio spectrum to improve interoperability between agencies in natural and human-caused emergencies, and to accommodate bandwidth intensive applications such as mission critical video surveillance. Dynamic Spectrum Access, a new paradigm that promises improved RF spectrum access and efficiency, has been hindered in its application to mission critical networks by a lack of the detailed understanding of the spectrum utilization characteristics necessary to drive system development.  This project is generating the fundamental long-term and high-resolution spectrum measurements needed to characterize the time, frequency, energy, and spatial dynamics of mission critical wireless networks in a dense urban environment (Chicago). Empirical and analytical models that characterize public safety spectrum utilization are being created from these measurements.  The empirical and analytical models are being used in turn as inputs to discrete-event simulations of candidate mission critical Dynamic Spectrum Access approaches in order to assess the ability of these approaches to improve capacity, while maintaining the necessary performance as measured by access delay and interference tolerance. Results in the form of the RF measurements, empirical and analytical models of mission critical spectral utilization, and system models are being disseminated via a secure download server linked to the Illinois Institute of Technology website. This research will impact Public Safety stakeholders including government agencies, public safety standards bodies, the Federal Communications Commission, and equipment providers by assessing the feasibility of applying to mission critical public safety wireless networks and facilitating system development."
"1010172","CRCNS: Long Term Reactivations in Cortex and Hippocampus","IIS","CRCNS-Computation Neuroscience, Robust Intelligence","09/15/2010","06/16/2013","Jean-Marc Fellous","AZ","University of Arizona","Continuing Grant","Kenneth Whang","08/31/2015","$468,334.00","Masami Tatsuno","fellous@email.arizona.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","CSE","7327, 7495","7327, 7495, 9251","$0.00","Understanding how memory is encoded and maintained in our brain is paramount to understanding cognitive functions. Unlike in a computer, human memories are continuously consolidated, reconsolidated, and integrated within the context of what has already been learned. This process is thought to involve exchanges of information between the cortex and the hippocampus during sleep. The investigators will study the ability of small groups of cells in the rodent hippocampus and medial prefrontal cortex (mPFC) to become transiently co-active during sleep periods occurring many hours after learning has taken place. Rats will be engaged in learning tasks aimed at selectively activating one or both of these areas. It is expected that the activity recorded during post-task sleep will be correlated with the activity of the same cells during the task in a manner compatible with the nature of the task and the specifics of the learning. This type of reactivation is considered to be a basic mechanism for memory consolidation.<br/><br/>The investigators have developed new analytical tools based on fuzzy clustering and information geometry. Preliminary data show that short episodes of reactivation occur with different time courses in these two structures, as is often proposed on theoretical grounds. In this project, the investigators will study how this reactivation is coordinated across two connected brain areas (CA3-CA1, CA1-mPFC) on very long time scales, and how single neurons contribute to single reactivating episodes.<br/><br/>These studies will yield insights into the long-term temporal and spatial dynamics of reactivation in the adult rodent. They will also contribute to a better understanding of the neural basis of memory consolidation and reconsolidation in cortex and hippocampus, and the relationship between memory consolidation and sleep."
"0953870","CAREER:  Rational Language Processing with Uncertain and Noisy Input","IIS","PERCEPTION, ACTION & COGNITION, ROBUST INTELLIGENCE","04/15/2010","08/21/2012","Roger Levy","CA","University of California-San Diego","Continuing grant","Tatiana D. Korelsky","03/31/2016","$501,529.00","","rplevy@mit.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7252, 7495","1045, 1187, 7495, 9215, HPCC","$0.00","This CAREER award investigates how humans integrate a wide variety of  <br/>information sources to achieve rapid, accurate natural language  <br/>comprehension subject to the physical and cognitive constraints under  <br/>which it takes place.  The project's primary empirical focus is on  <br/>reading, a mode of information exchange of unexceeded importance in  <br/>literate societies.  Reading involves a rapid sequence of targeted eye  <br/>movements throughout a text -- recordable through modern eye-tracking  <br/>technology -- from which noisy sensory input are obtained and  <br/>integrated with prior knowledge to resolve perceptual and linguistic  <br/>uncertainty.  The central goal of this project is thus to develop,  <br/>implement, and test a computational model of language comprehension  <br/>and eye movement control in reading built on principles of  <br/>probabilistic inference and rational action, using the tools of  <br/>natural language processing (NLP) technology, reinforcement learning,  <br/>and behavioral psycholinguistic experimentation.<br/><br/>The success of this project is likely to have major impact in the  <br/>field of human sentence processing, bringing a new level of nuance and  <br/>detail to both theory and data analysis, and will bear on broad  <br/>current debates in cognitive science regarding rationality in  <br/>cognition. Additionally, the results of this basic research project  <br/>have a wide range of potential applications ranging from intelligent  <br/>tutoring technology to language-impairment diagnosis to cognitive  <br/>ergonomics. Together with this research program, the project involves  <br/>an educational program including a new textbook on probabilistic  <br/>models in the study of language, new undergraduate and graduate  <br/>courses, and tutorials and courses on computational psycholinguistics  <br/>at major conferences and summer institutes.<br/><br/>This CAREER award is co-funded by two directorates:: CISE/IIS and SBE/BCS."
"1017143","SHF: Small: Fusion of Quantum Dot/Nanowire Based Sensors and Processors in Ultra-low-energy, Distributed-Intelligence Sensing Network","CCF","NANOCOMPUTING","09/01/2010","07/09/2015","Pinaki Mazumder","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Sankar Basu","08/31/2016","$400,281.00","","mazum@eecs.umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7947","9150, 9218, HPCC","$0.00","The purpose of the award is to develop a new nano-circuit architecture, novel circuit realization using quantum-electronic devices, and a comprehensive, vertically integrated design methodology for real-time electronic vision applications.  The architecture integrates nano-optical sensors and active quantum dot processing elements into hybrid, ultra efficient, distributed intelligence systems with meta-level decision making agents.  This is in contrast with conventional electronic vision systems where all decision related processing is done by an algorithmic agent only after digitization of the input from nanowire sensors.  The approach employs a smooth analog-to-digital processing transition to allow en-route, hierarchical transformation of the input into simpler, digital representations of multidimensional, abstract input characteristics while the signal is on its way to the central decision making agent.  A more rapid decision-making can be achieved because the agent can now directly correlate just the key features without first identifying and extracting these features or filtering out extraneous details.  Such close-to-source processing also includes all other desirable benefits like high energy-efficiency, low signal degradation and small area requirement for chip implementation, in addition to high speed. Specifically, fanning-out the input to a cellular-neural-network-like architecture of active quantum-electronic analog functional units will extract and encode the key features that can then be fanned in to one or more decision agents. These units include spatio-temporal filters, velocity estimators, and image processing elements. <br/><br/>The intellectual merits of this research include the construction of nanoscale quantum dots based cellular logic arrays capable of performing neuromorphic computation like spatiotemporal signal processing, video and image processing; and the design of a new CAD tool for optimizing the 3D nanostructures of quantum tunneling devices while performing the system-level optimization in an augmented circuit simulator developed by the principal investigator's research group.  The broader impacts include development of pedagogical interdisciplinary training to the next generation of circuit engineers and supplementary didactic material---two new, definitive textbooks on nanoelectronics."
"0954938","CAREER:  Stochastic Optimization and Coordinating Control for the Next-Generation Electric Power System with Significant Wind Penetration","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","08/15/2010","05/17/2013","Wei Qiao","NE","University of Nebraska-Lincoln","Standard Grant","eyad abed","07/31/2016","$407,999.00","","wqiao@engr.unl.edu","151 Prem S. Paul Research Center","Lincoln","NE","685031435","4024723171","ENG","7607","1045, 155E, 9102, 9150, 9251","$0.00","Objectives:<br/><br/>The objective of this research is to study the use of stochastic optimization and coordinating control to improve the stability, reliability, and economic efficiency of the next-generation power system with significant wind penetration.  The approach is to (1) develop novel modeling and optimization frameworks using probability-based decision-making approaches for security-constrained stochastic optimization of the power system; (2) enhance steady-state and transient performance of wind plants through novel wind-plant supervisory control; and (3) enhance overall power system dynamic performance and stability through novel wide-area coordinating control.<br/><br/>Intellectual Merit:  <br/><br/>Through the creation of new philosophies for optimization and control design, this project will advance understanding of the critical operational issues of the next-generation power system and will lead to smarter power grids to provide better electric energy security, efficiency, and sustainability.  In addition to power systems, this research will create several fundamental frameworks and methodologies applicable to optimization and control of other interconnected, large-scale, stochastic, dynamical, complex systems.<br/><br/>Broader Impacts:  <br/><br/>This project will build a thriving and diversified power program through the integration of graduate and undergraduate research and teaching with K-12 outreach, which will provide a unique platform of learning for young individuals entering the power engineering profession.  This project will build multiple foreign research training and exchange programs to promote international collaborations in power engineering education and research.  The outcomes of this project will further exploit the benefits of wind power in reaching the goal of supplying 20% of the nation's electricity by wind and will benefit various sectors of the nation's economy."
"1013935","SBIR Phase I: Building a Flexible, Technology Adaptive Architecture to Support Processing of Content by Knowledge Workers","IIP","SMALL BUSINESS PHASE I","07/01/2010","05/24/2010","Eric Koefoot","VA","PublicRelay, Inc.","Standard Grant","Errol Arkilic","12/31/2010","$150,000.00","","ekoefoot@yahoo.com","12310 Pinecrest","Reston","VA","201910000","7034728337","ENG","5371","5371, 6850, 9139, HPCC","$0.00","This Small Business Innovation Research (SBIR) Phase I Project addresses the gap between the capabilities of today's natural language processing systems and the accuracy requirements of knowledge workers (analysts and researchers) in language-sensitive fields such as public relations, foreign affairs, and crisis management. Knowledge workers in many organizations monitor and analyze print and web coverage for content of interest. When the volume of search results is large, some filter, classify and score the results with Natural Language Processing (NLP) systems using complex libraries of words, patterns, and context-specific algorithms. However, users complain that these systems fall short of desired accuracy, missing rhetorical devices such as irony, sarcasm, metaphors, double entendre, and improperly interpreting references. Users with high thresholds for accuracy thus turn to manual processes to either supplement or substitute for technology. This project will test a prototype architecture allowing rapid insertion, testing, and adaptation of text analysis algorithms and a workflow process efficiently integrating human review judgment. <br/><br/><br/>Once commercialized, the system will enable more rapid adoption of technology by knowledge workers. In fields with high accuracy requirements, the need for human judgment has constrained technology use to discrete areas like search, while in subsequent processing steps, analysts must manually capture, classify, score, analyze, and report on the output."
"0953837","CAREER:  Investigating the Ultimate Mechanisms of Embodied Cognition","IIS","ROBUST INTELLIGENCE","05/15/2010","04/16/2014","Joshua Bongard","VT","University of Vermont & State Agricultural College","Continuing grant","Kenneth C. Whang","04/30/2017","$499,999.00","","jbongard@uvm.edu","85 South Prospect Street","Burlington","VT","054050160","8026563660","CSE","7495","1045, 1187, 9150","$0.00","To date, relatively little success has been achieved in realizing machines that continually perform simple yet adaptive behaviors in unstructured environments (compared to a structured environment such as a factory). The prevailing approach to create such machines is to copy physiological and neurological systems observed in animals, and build them into robots. This raises the issue however of what from among the infinitude of existing biological structures should be copied. Research under this award is pursuing an alternative approach: rather than copy existing biological systems, evolutionary dynamics are copied and connected in a virtual space. The resulting evolutionary algorithm optimize virtual robots' neurological structures that control behavior and their body plans. Importantly, evolution in these studies is task and behavior specific.<br/><br/>The research is intended to make important contributions to robotics and biology. For roboticists, this work will enable computers to automatically design the body plans and neural controllers for robots that are more adaptive and robust than robots designed manually. Automatically-designed virtual robots can then be built as physical devices and deployed into real-world environments, to include those that are dangerous to humans. For biologists, our studies will provide insight into why and how particular structures evolved in nature. For example, if legged robots originally evolved for locomotion are then selected to locomote and grasp objects, computational evolution may re-purpose the robot's front legs into arms and grippers; or, it may add manipulatory appendages onto the existing body plan. Either outcome would be of great interest to evolutionary biologists.<br/><br/>Finally, experiments are being housed in online tools that will allow graduate, undergraduate and K-12 students to run evolutionary simulations passively on their own machines, as well as actively participate in the process: they may design novel virtual environments in which the robots must evolve. This active participation is intended to motivate students to understand the physics, biology, engineering and computational processes underlying evolution."
"1000744","New Methodologies for System-Level Electromagnetic Compatibility (EMC) Analysis of Electronic Systems","CMMI","ESD-Eng & Systems Design, CCSS-Comms Circuits & Sens Sys","09/01/2010","06/13/2013","Vijaya Kumar Devabhaktuni","OH","University of Toledo","Standard Grant","Christiaan Paredis","08/31/2014","$389,997.00","Charles Bunting","vjdev@pnw.edu","2801 W Bancroft St., MS 218","TOLEDO","OH","436063390","4195302844","ENG","1464, 7564","067E, 068E, 073E, 116E, 1464, 7564, 9148, 9178, 9251, MANU","$0.00","The research objective of this award is to create new computer aided design (CAD) methodologies for modeling and design of electronic systems exposed to electromagnetic interference (EMI). EMI continues to be a key design challenge as it could lead to severe reliability issues in modern electronic products. The methodologies offer a generalized perspective in terms of analyzing EMI coupling scenarios faced by electromagnetic compatibility (EMC) design engineers. The underlying framework is broad-based and can be applied to the EMC design of a wide range of electronic systems. The research formulates system-level modeling approaches, which allow optimized selection of different CAD models for different sub-systems, e.g. batteries, cables, and large enclosures. Circuits and structures representing typical EMI scenarios will be designed, fabricated, and tested, to validate the CAD methodologies, identify limitations and demonstrate reliable EMC solutions. Deliverables include system-level CAD methodologies, subsystem-level neural network and statistical models, well-documented research results, a project website, and state-of-the-art curriculum for industry engineers and university students.<br/><br/>If successful, the results of this research will have both scientific and educational impact. The research advances the fundamental design practices, e.g. design of highly-overmoded aircraft and automotive structures, with a potential for breaking the barriers in terms of simulation accuracies, speeds, and other capabilities. CAD techniques enabled by this research will offer significant cost and time reduction in engineering innovative and reliable electronic products. The research outcomes will lead to industry-oriented curriculum components at The University of Toledo and at Oklahoma State University. Engineering students and postdoctoral fellows will benefit through classroom instruction and involvement in research. Through a constantly updated website, efforts will be made to offer equal opportunities to students belonging to minority and underrepresented groups. The website will also enhance the public awareness on various aspects of EMI/EMC."
"0952497","Collaborative Research: Semantics and Pragmatics of Projective Meaning across Languages","BCS","Linguistics","06/15/2010","06/22/2010","Mandy Simons","PA","Carnegie-Mellon University","Standard Grant","William Badecker","11/30/2013","$118,081.00","","simons@andrew.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","SBE","1311","1311","$0.00","The term ""projection"" describes cases where some element of meaning unexpectedly escapes from the scope of another expression, such as a negation or conditional. For example, ""John didn't see his sister"" is typically understood as denying that there was an event of John seeing his sister, but not as denying that John has a sister -- even though syntactically the expression ""his sister"" is under the scope of the negation. An enormous range of expressions yield projection, including presupposition triggers, conventional implicature triggers, approximatives, and some inferences associated with Gricean implicatures. This project will carry out the first systematic empirical and theoretical analysis of the full range of projective meanings, to establish why projection occurs, and how subclasses of projective meaning differ. To this end, it will develop templates for diagnostic tests that can be used to study properties of projective meanings cross-linguistically, with theoretically naive consultants. The researchers will conduct detailed investigations of projective meanings in English and Paraguayan Guarani, and disseminate the techniques to fieldworkers studying other languages, through publications and workshops. <br/><br/>Modern linguistic semantics centers around Fregean compositionality, the way in which the meanings of parts are combined to give the meanings of larger expressions. While there has been great progress in compositional semantics, projective meaning does not obey ordinary compositional rules. Until now, there has been no uniform account of why projective meanings behave differently from ordinary content. This project seeks to explain projection, based on the working hypothesis that aspects of meaning project if and only if they are not at-issue. The notion of at-issueness is a pragmatic one, based on what questions are under discussion in a discourse. Thus the project will place a relatively little-studied pragmatic notion at the heart of work on linguistic meaning. A first area where the innovations in the project are of broad significance is in its cross-linguistic focus: a theory of human language cannot be based on one language alone, and the project will apply its empirical techniques cross-linguistically. Beyond linguistic semantics, the issues studied are fundamental to Philosophy of Language, since the work subtly redraws the boundary between Semantics and Pragmatics. Further the project is of practical significance in the field of Natural Language Processing (specifically, the subfield of Computational Semantics). Text processing systems, e.g. computer systems for automatically answering a user's questions, must take account of inferences which arise independently of standard composition. A question answering system which does not take account of what is projected will not be able to identify what the user already knows, and will not be able to identify what question needs to be answered."
"0952571","Collaborative Research: Semantics and Pragmatics of Projective Meaning across Languages","BCS","LINGUISTICS","06/15/2010","10/03/2014","Craige Roberts","OH","Ohio State University","Standard Grant","William J. Badecker","11/30/2015","$190,484.00","Judith Tonhauser","roberts.21@osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","SBE","1311","1311","$0.00","The term ""projection"" describes cases where some element of meaning unexpectedly escapes from the scope of another expression, such as a negation or conditional. For example, ""John didn't see his sister"" is typically understood as denying that there was an event of John seeing his sister, but not as denying that John has a sister -- even though syntactically the expression ""his sister"" is under the scope of the negation. An enormous range of expressions yield projection, including presupposition triggers, conventional implicature triggers, approximatives, and some inferences associated with Gricean implicatures. This project will carry out the first systematic empirical and theoretical analysis of the full range of projective meanings, to establish why projection occurs, and how subclasses of projective meaning differ. To this end, it will develop templates for diagnostic tests that can be used to study properties of projective meanings cross-linguistically, with theoretically naive consultants. The researchers will conduct detailed investigations of projective meanings in English and Paraguayan Guarani, and disseminate the techniques to fieldworkers studying other languages, through publications and workshops. <br/><br/>Modern linguistic semantics centers around Fregean compositionality, the way in which the meanings of parts are combined to give the meanings of larger expressions. While there has been great progress in compositional semantics, projective meaning does not obey ordinary compositional rules. Until now, there has been no uniform account of why projective meanings behave differently from ordinary content. This project seeks to explain projection, based on the working hypothesis that aspects of meaning project if and only if they are not at-issue. The notion of at-issueness is a pragmatic one, based on what questions are under discussion in a discourse. Thus the project will place a relatively little-studied pragmatic notion at the heart of work on linguistic meaning. A first area where the innovations in the project are of broad significance is in its cross-linguistic focus: a theory of human language cannot be based on one language alone, and the project will apply its empirical techniques cross-linguistically. Beyond linguistic semantics, the issues studied are fundamental to Philosophy of Language, since the work subtly redraws the boundary between Semantics and Pragmatics. Further the project is of practical significance in the field of Natural Language Processing (specifically, the subfield of Computational Semantics). Text processing systems, e.g. computer systems for automatically answering a user's questions, must take account of inferences which arise independently of standard composition. A question answering system which does not take account of what is projected will not be able to identify what the user already knows, and will not be able to identify what question needs to be answered."
"1034594","Workshop on Applications of Computer Vision in Archaeology ACVA?10 -- Vision, Visualization, and Computational  Methods to Cultural Heritage Needs.","IIS","HCC-Human-Centered Computing","06/01/2010","06/10/2010","Fernand Cohen","PA","Drexel University","Standard Grant","William Bainbridge","05/31/2012","$31,576.00","","fscohen@coe.drexel.edu","1505 Race St, 10th Floor","Philadelphia","PA","191021119","2158955849","CSE","7367","9150, 9215","$0.00","This proposal is to support a workshop on Applications of Computer Vision in Archaeology.  The workshop will bring together about 100 archaeologists, cultural heritage preservationists, computer vision, visualization, graphics, and new media research and practitioners to discuss the state-of-the art in current research.  The event will also provide a forum for planning and coordination of future efforts. Since the first workshop on Applications of Computer Vision in Archaeology was held in June 2003 the domain of research interest has broadened significantly to include research in graphics and visualization.  At present, nearly all phases of archaeological practice, from discovery in the field, through artifact analysis and conservation, to the presentation of new findings to the public are in a period of radical change.  Computer vision research combined with graphics, visualization and computational methods have made available to archaeology and other interdisciplinary research dealing visual artifacts a rich set of tools and methods to extend research capabilities.   New efforts range from the creation of virtual libraries (digital publishing of field records) to computer-assisted artifact mending technologies to 3D presentations of historical site interpretations."
"0952862","Collaborative Research: Semantics and Pragmatics of Projective Meaning across Languages","BCS","LINGUISTICS","06/15/2010","06/22/2010","David Beaver","TX","University of Texas at Austin","Standard Grant","William J. Badecker","02/28/2013","$104,558.00","","dib@mail.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","SBE","1311","1311","$0.00","The term ""projection"" describes cases where some element of meaning unexpectedly escapes from the scope of another expression, such as a negation or conditional. For example, ""John didn't see his sister"" is typically understood as denying that there was an event of John seeing his sister, but not as denying that John has a sister -- even though syntactically the expression ""his sister"" is under the scope of the negation. An enormous range of expressions yield projection, including presupposition triggers, conventional implicature triggers, approximatives, and some inferences associated with Gricean implicatures. This project will carry out the first systematic empirical and theoretical analysis of the full range of projective meanings, to establish why projection occurs, and how subclasses of projective meaning differ. To this end, it will develop templates for diagnostic tests that can be used to study properties of projective meanings cross-linguistically, with theoretically naive consultants. The researchers will conduct detailed investigations of projective meanings in English and Paraguayan Guarani, and disseminate the techniques to fieldworkers studying other languages, through publications and workshops. <br/><br/>Modern linguistic semantics centers around Fregean compositionality, the way in which the meanings of parts are combined to give the meanings of larger expressions. While there has been great progress in compositional semantics, projective meaning does not obey ordinary compositional rules. Until now, there has been no uniform account of why projective meanings behave differently from ordinary content. This project seeks to explain projection, based on the working hypothesis that aspects of meaning project if and only if they are not at-issue. The notion of at-issueness is a pragmatic one, based on what questions are under discussion in a discourse. Thus the project will place a relatively little-studied pragmatic notion at the heart of work on linguistic meaning. A first area where the innovations in the project are of broad significance is in its cross-linguistic focus: a theory of human language cannot be based on one language alone, and the project will apply its empirical techniques cross-linguistically. Beyond linguistic semantics, the issues studied are fundamental to Philosophy of Language, since the work subtly redraws the boundary between Semantics and Pragmatics. Further the project is of practical significance in the field of Natural Language Processing (specifically, the subfield of Computational Semantics). Text processing systems, e.g. computer systems for automatically answering a user's questions, must take account of inferences which arise independently of standard composition. A question answering system which does not take account of what is projected will not be able to identify what the user already knows, and will not be able to identify what question needs to be answered."
"0959985","MRI-R2: Development of an Instrument for Information Science and  Computing  in Neuroscience","CNS","Major Research Instrumentation","06/01/2010","03/05/2014","Malek Adjouadi","FL","Florida International University","Standard Grant","Rita Rodriguez","09/30/2015","$2,939,515.00","Naphtali Rishe, Armando Barreto, Prasanna Jayakar, William Gaillard","adjouadi@fiu.edu","11200 SW 8TH ST","Miami","FL","331990001","3053482494","CSE","1189","6890","$2,939,515.00","""This award is funded under the American Recovery and Reinvestment Act of 2009(Public Law 111-5).""<br/>Proposal #: 09-59985<br/>PI(s):  Adjouadi, Malek; Barreto, Armando B.; Gaillard,William; Jayakar,Prasanna; Rishe, Naphtali D.<br/>Institution: Florida International University <br/>Title:          MRI-R2/Dev: Instrument for Information Science and Computing in Neuroscience<br/>This project, developing an instrument for information processing and computing that enables cohesive study of the brain, involves the new concept of a 5-D brain processing platform while addressing the challenge of finding the best way to put together five dimensions to provide a complete picture of brain dynamics. This unified approach brings together fields of Computer Engineering, Computer Science, Electrical Engineering, and Bioengineering to create an instrument for precisely measuring and visualizing significant information and results across the five dimensions, three from spatial data, time, and an imaging modality that serves as the fifth dimension. Accomplishing this mission involves advanced designs, hardware-software integration mechanisms, and novel interfaces that bring competing, and sometimes diverging, technologies into a unified brain research platform. Combining a Multi-site Data Repository, Modality Integration and Computational, Visualization, and Operation Support Units, this new instrument is expected to bring:<br/>- New insights into brain structure, functional correlations and dynamics, both in its normal state and under specific pathological conditions,<br/>- Far improved mapping of patterns of brain activity,<br/>- Integration of multimodal technologies in order to augment their capabilities with new insights while consolidating high spatial resolution with high temporal resolution,<br/>- Database design and management augmented with mechanisms for fast user interaction and visualization to meet the challenge of managing complex spatio-temporal datasets, posing complex queries, and establishing effective methods for data representation and visualization, and<br/>- Resolution of those paradigms that confront heavy computational requirements and compatibility problems that arise from the use of different recording modalities and diverse software platforms.<br/>The project aims to collectively overcome the primary barriers in identifying the different factors that influence the functional organization of the brain and its underlying pathology. As an example, it delves in the epileptic seizure that can be mapped over time as it moves along specific fiber tracts that may enable better identification of specific areas of treatment and consequently protect functionally important parts of the brain during surgery that may lead to better and safer outcomes.<br/>Broader Impacts: <br/>Fostering an environment that supports cross disciplinary initiatives, joint collaboration and programs, the instrument establishes a research platform that enables academic institutions and hospitals to investigate multi-site collaborative studies in accordance to systematically administered standardize protocols to a database of common assessments and measures. The project extends the breadth and depth of multidisciplinary efforts including new paradigms and findings. Moreover, the project advances the education, research, and training of many students in a minority serving university."
"1053105","WORKSHOP: Froniers in Computer Vision","IIS","Robust Intelligence","09/01/2010","08/22/2010","Aude Oliva","MA","Massachusetts Institute of Technology","Standard Grant","Jie Yang","08/31/2012","$50,000.00","","oliva@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7495","7495","$0.00","Computer vision started with the goal of building machines that can see like humans. Nowadays, computer vision has expended to numerous applications such as image database search in the world wide web, computational photography, reconstruction of three-dimensional scenes, surveillance, assistive systems, vision for graphics and nanotechnology, etc. More domains and applications keep arising as computer vision technology develops. <br/><br/> The goal of the workshop in Frontiers in Computer Vision is to bring together national and international experts, from academia and industry, to identify the future impact of computer vision on the economic, social, educational and security needs of the nation and outline the scientific and technological challenges to address the issues: how can computer vision build on the success and enthusiasm of its growing participants? how can the academic community make connections to industry? how to better foster scholarship and improve communication both within computer vision itself and to related disciplines and application areas? How can computer vision best interact with related fields? how can the importance and promise of computer vision be communicated to the general public? <br/><br/>The deliverables of the event  include, among others, videos of the presentations available on the Frontiers in Computer Vision website, together with a roadmap to outline the scientific and technological challenges to address."
"0964269","RI:  Medium:  Collaborative Research:  Unlocking Biologically-Inspired Computer Vision:  A High-Throughput Approach","IIS","ROBUST INTELLIGENCE","09/01/2010","08/27/2010","James DiCarlo","MA","Massachusetts Institute of Technology","Standard Grant","Kenneth C. Whang","08/31/2013","$410,000.00","David Cox","dicarlo@MIT.EDU","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7495","7495, 7924","$0.00","This project exploits advances in parallel computing hardware and a neuroscience-informed perspective to design next-generation computer vision algorithms that aim to match a human's ability to recognize objects.  The human brain has superlative visual object recognition abilities -- humans can effortlessly identify and categorize tens of thousands of objects with high accuracy in a fraction of a second -- and a stronger connection between neuroscience and computer vision has driven new progress on machine algorithms.  However, these models have not yet achieved robust, human-level object recognition in part because the number of possible ""bio-inspired"" model configurations is enormous. Powerful models hidden in this model class have yet to be systematically characterized and the correct biological model is not known.<br/><br/>To break through this barrier, this project will leverage newly available computational tools to undertake a systematic exploration of the bio-inspired model class by using a high-throughput approach in which millions of candidate models are generated and screened for desirable object recognition properties (Objective 1).  To drive this systematic search, the project will create and employ a suite of benchmark vision tasks and performance ""report cards"" that operationally define what constitutes a good visual image representation for object recognition (Objective 2).  The highest performing visual representations harvested from these ongoing high-throughput searches will be used: for applications in other machine vision domains, to generate new experimental predictions, and to determine the underlying computing motifs that enable this high performance (Objective 3).   Preliminary results show that this approach already yields algorithms that exceed state-of-the-art performance in object recognition tasks and generalize to other visual tasks.<br/><br/>As the scale of available computational power continues to expand, this approach holds great potential to rapidly accelerate progress in computer vision, neuroscience, and cognitive science: it will create a large-scale ""laboratory"" for testing neuroscience ideas within the domain of computer vision; it will generate new, testable computational hypotheses to guide neuroscience experiments; it will produce a new kind of multidimensional image challenge suite that will be a rallying point for computer models, neuronal population studies, and behavioral investigations; and it could unleash a host of new applications."
"1010463","CRCNS: US-German Collaboration: Auditory and Spatial Sequence Encoding in the Hippocampus","IIS","COLLABORATIVE RESEARCH, CRCNS","09/15/2010","09/04/2012","Stefan Leutgeb","CA","University of California-San Diego","Continuing grant","Kenneth C. Whang","08/31/2014","$349,982.00","","sleutgeb@ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","CSE","7298, 7327","5936, 5979, 7327","$0.00","Complex cognitive functions such as spatial cognition, language development, and episodic memory require associations between multiple sensory experiences, such as images and sound, and require that associations are remembered in sequence. The rodent hippocampus has been one of the leading models for understanding neuronal mechanisms for remembering a sequence of spatial locations. Yet, it remains unknown whether the mechanisms that are used for encoding a path through space are the same as those that are used for encoding sequences that contain multiple modalities. To address this question, the auditory modality is of particular interest since it provides a mean to present a sequence of stimuli with high temporal precision and thus a mean to investigate time constraints for sequence learning. The proposed research investigates auditory sequence learning in a rodent species that is a hearing-specialist and tests whether the hippocampus of Mongolian gerbils can encode sequences of auditory stimuli with mechanisms similar to those used for spatial sequences. <br/><br/>It will be tested whether place fields and theta phase precession exist in Mongolian gerbils, whether complex sound stimuli are encoded in the gerbil hippocampus in a spatially-independent manner or in association with the location of the animal, and whether sound sequences are encoded with network mechanisms related to those that are used for encoding spatial sequences. These questions will be addressed with single-unit recording from hippocampal principal cell populations of behaving animals, and experiments will be conducted in a virtual reality setup that allows for the precise delivery of auditory stimuli. Performing these experiments in a rodent species with an auditory specialization might result in important advances in addressing how the hippocampus encodes multimodal sequences. This research can provide important insight into neural network mechanisms for sequence coding and can lead to a better understanding of the contribution of the hippocampus to language development in humans.<br/><br/>This project is jointly funded by Collaborative Research in Computational Neuroscience and the Office of International Science and Engineering.  A companion project is being funded by the German Ministry of Education and Research (BMBF)."
"1011778","EAGER: Phylo: Phylogenetic Reconstruction of Textual Histories","IIS","ROBUST INTELLIGENCE","02/01/2010","01/22/2010","David Chiang","CA","University of Southern California","Standard Grant","Tatiana D. Korelsky","01/31/2012","$75,000.00","","dchiang@nd.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7495","7495","$0.00","This project, supported by an EArly-concept Grant for Exploratory Research (EAGER), is developing computational models of how manuscripts of premodern texts changed over time due to copying with errors, intentional editing, and translation into different languages. The purpose of these models is to reconstruct the original texts and to better understand the forces that shaped them. We are building on work applying ideas from computational evolutionary biology to the task, but the main focus of the project is to explore whether cutting-edge ideas from computational linguistics and natural language processing are better suited for modeling the evolution of natural-language texts. In particular we are exploring the use of techniques from nonprojective dependency parsing to model the tree of relationships among manuscripts and statistical machine translation to model the relationship between pairs of manuscripts.<br/><br/>The tools that result from the project will be made publicly available in order to foster cross-disciplinary research. These tools will enable scholars of ancient and medieval literature to use our models to analyze collections of manuscripts that may not have been possible to analyze by hand before. The techniques explored will shed light on computationally hard learning and search problems such as those that frequently arise in natural language processing."
"1017190","HCC: Small: Collaborative Research:  Analysis of Language Samples for Detecting Language Impairment in Monolingual and Bilingual Children","IIS","HCC-Human-Centered Computing","09/01/2010","08/19/2010","Yang Liu","TX","University of Texas at Dallas","Standard Grant","Ephraim Glinert","08/31/2015","$195,726.00","","yangl@hlt.utdallas.edu","800 W. Campbell Rd., AD15","Richardson","TX","750803021","9728832313","CSE","7367","7923, 9102, 9150","$0.00","It is widely recognized that language impairment can have a negative effect on literacy skills, and that children suffering language impairment are at a higher risk of academic under-achievement and lower overall social development.  Hence, early and accurate language assessment for children is critical, especially for those with non-mainstream linguistic backgrounds.  Spontaneous language samples are commonly used in communication disorders to measure the speaker's competence across a range of complementary language skills.  These elicitation tasks allow clinicians and clinical researchers to analyze speech fluency by looking at the patterns of disfluencies and other speech disruptions.  Language productivity can be gauged by computing mean length of utterance, along with measures of vocabulary and total utterances produced.  Morpho-syntactic skills can also be analyzed from these data, by manually coding for specific grammatical constructions that are known to signal developmental milestones.  At present, use of the information contained in these language samples is restricted to the capacity of human experts to manually analyze the data, since little has been done to use computational models for this task   In this collaborative effort by PIs in the University of Alabama at Birmingham and the University of Texas at Dallas, the objective is to address this problem by developing computational approaches for scoring samples from children along different language dimensions, including speech fluency, syntactic structure, content, and coherence, with the long term goal of building robust computational linguistic approaches for identifying language impairments in children.   With these ends in mind, the PIs will investigate a number of core research questions, including measuring syntactic complexity in children's language, evaluating content in story retelling and play sessions, and detecting disfluencies in children's transcripts.  Moreover, this research will focus on analyzing samples from children with three different language backgrounds: English monolinguals, Spanish monolinguals, and Spanish-English bilinguals of Mexican descent (the latter representing the fastest growing minority in this country).  Since their models will be data driven, the PIs expect to be able to evaluate empirically the differences in developmental patterns of speech in children across these linguistic diversities.   Addressing the bilingual population involves modeling code-switching behavior; thus, additional core research questions include measuring syntactic complexity in code-switched data, and identification and categorization of code-switching patterns in bilingual children.  <br/><br/>Broader Impacts:  This research will contribute to developing more accurate and practical tools for assessing language development in children, a field to which little attention has been paid to date.  Addressing the challenges involved in the automated analysis of children's speech will also advance the field of Natural Language Processing (NLP) in general.  Moreover, since the project involves children with three different linguistic backgrounds, the new technology will have low language dependency and so should be easily portable to other languages and domains.  In the field of communication disorders, applying corpus-based approaches to language assessment is still in its infancy; project outcomes will have a direct impact on this field, by providing new metrics for scoring spontaneous language samples of children that can complement the battery of assessment tools currently used."
"1018124","HCC: Small: Collaborative Research:  Analysis of Language Samples for Detecting Language Impairment in Monolingual and Bilingual Children","IIS","HCC-Human-Centered Computing, EPSCoR Co-Funding","09/01/2010","08/19/2010","Thamar Solorio","AL","University of Alabama at Birmingham","Standard Grant","Ephraim Glinert","10/31/2014","$301,055.00","","thamar.solorio@gmail.com","AB 1170","Birmingham","AL","352940001","2059345266","CSE","7367, 9150","7923, 9102, 9150","$0.00","It is widely recognized that language impairment can have a negative effect on literacy skills, and that children suffering language impairment are at a higher risk of academic under-achievement and lower overall social development.  Hence, early and accurate language assessment for children is critical, especially for those with non-mainstream linguistic backgrounds.  Spontaneous language samples are commonly used in communication disorders to measure the speaker's competence across a range of complementary language skills.  These elicitation tasks allow clinicians and clinical researchers to analyze speech fluency by looking at the patterns of disfluencies and other speech disruptions.  Language productivity can be gauged by computing mean length of utterance, along with measures of vocabulary and total utterances produced.  Morpho-syntactic skills can also be analyzed from these data, by manually coding for specific grammatical constructions that are known to signal developmental milestones.  At present, use of the information contained in these language samples is restricted to the capacity of human experts to manually analyze the data, since little has been done to use computational models for this task   In this collaborative effort by PIs in the University of Alabama at Birmingham and the University of Texas at Dallas, the objective is to address this problem by developing computational approaches for scoring samples from children along different language dimensions, including speech fluency, syntactic structure, content, and coherence, with the long term goal of building robust computational linguistic approaches for identifying language impairments in children.   With these ends in mind, the PIs will investigate a number of core research questions, including measuring syntactic complexity in children's language, evaluating content in story retelling and play sessions, and detecting disfluencies in children's transcripts.  Moreover, this research will focus on analyzing samples from children with three different language backgrounds: English monolinguals, Spanish monolinguals, and Spanish-English bilinguals of Mexican descent (the latter representing the fastest growing minority in this country).  Since their models will be data driven, the PIs expect to be able to evaluate empirically the differences in developmental patterns of speech in children across these linguistic diversities.   Addressing the bilingual population involves modeling code-switching behavior; thus, additional core research questions include measuring syntactic complexity in code-switched data, and identification and categorization of code-switching patterns in bilingual children.  <br/><br/>Broader Impacts:  This research will contribute to developing more accurate and practical tools for assessing language development in children, a field to which little attention has been paid to date.  Addressing the challenges involved in the automated analysis of children's speech will also advance the field of Natural Language Processing (NLP) in general.  Moreover, since the project involves children with three different linguistic backgrounds, the new technology will have low language dependency and so should be easily portable to other languages and domains.  In the field of communication disorders, applying corpus-based approaches to language assessment is still in its infancy; project outcomes will have a direct impact on this field, by providing new metrics for scoring spontaneous language samples of children that can complement the battery of assessment tools currently used."
"1002634","Student Research Workshop in Computational Linguistics at the ACL 2010 Conference","IIS","Robust Intelligence","02/01/2010","01/08/2010","Tomasz Strzalkowski","NY","SUNY at Albany","Standard Grant","Tatiana Korelsky","01/31/2012","$17,700.00","","tomek@rpi.edu","1400 WASHINGTON AVE MSC 100A","Albany","NY","122220100","5184374974","CSE","7495","7495","$0.00","The Association for Computational Linguistics (ACL) is the primary international organization in the field of natural language processing. The ACL's annual conference is the major international conference in this field. This project is to subsidize travel, conference, and housing expenses of students selected to participate in the Association for Computational Linguistics (ACL) Student Research Workshop, which is part of the ACL conference on July 11-16, 2010 in Uppsala, Sweden. The workshop consists of two tracks: paper presentations and poster presentations. Each paper presentation is followed by a discussion chaired by respected researchers in the field. The poster session provides a further opportunity for extended one-on-one interaction with the members of the CL research community. All selected work has only student authors and the workshop is organized and run by students.<br/><br/>The Student Research Workshop provides a valuable opportunity for the next generation of natural language processing researchers to enter the computational linguistics community. It allows the best students in the field to take their first important step toward becoming professional computational linguists by receiving critical feedback on their work from external experts, and by making contacts with other students and senior researchers in their field. The students who are involved in running and selecting papers for the workshop also gain valuable opportunities for professional growth and interaction with the researchers on the organizing committee of the main conference. The workshop contributes to the maintenance and development of a skilled and diverse computational linguistics and natural language processing research community."
"1022697","Student Research Workshop in Computational Linguistics at the NAACL HLT 2010 Conference","IIS","Robust Intelligence","05/01/2010","03/10/2010","Diane Litman","PA","University of Pittsburgh","Standard Grant","Tatiana Korelsky","04/30/2011","$18,000.00","","litman@cs.pitt.edu","300 Murdoch Building","Pittsburgh","PA","152603203","4126247400","CSE","7495","7495, 9102","$0.00","The NAACL HLT conference is the major international conference in North America in the field of natural language processing. This project is to subsidize travel, conference, and housing expenses of students selected to participate in the NAACL HLT Student Research Workshop which will be held during the conference June 1-6, 2010 in Los Angeles, California. The workshop consists of two tracks: full paper presentations and poster presentations. All selected work has only student primary authors. The full paper sessions are composed of paper presentations followed by a discussion panel led by respected researchers in the field. The workshop is organized and run by students.<br/><br/>The Student Research Workshop provides a valuable opportunity for the next generation of natural language processing researchers to enter the computational linguistics community. It allows the best students in the field to take their first important step toward becoming professional computational linguists by receiving critical feedback on their work from external experts, and by making contacts with other students and senior researchers in their field. The students who are involved in running and selecting papers for the workshop also gain valuable opportunities for professional growth and interaction with the researchers on the organizing committee of the main conference. The workshop contributes to the maintenance and development of a skilled and diverse computational linguistics and natural language processing research community."
"1014231","SBIR Phase I: Wearable Augmented Perception for Environmental Recognition","IIP","SMALL BUSINESS PHASE I","07/01/2010","05/25/2010","Eric Huber","TX","TRACLabs Inc.","Standard Grant","Muralidharan S. Nair","12/31/2010","$149,939.00","","huber@traclabs.com","100 N.E. Loop 510","San Antonio","TX","782164727","2814617884","ENG","5371","5371, 6840, 9139, HPCC","$0.00","This Small Business Innovation Research (SBIR) Phase I project focuses on reducing the footprint of machine vision and human interface solutions in support of a wearable apparatus that will improve environmental awareness of the visually impaired. Computer vision has provided significant capabilities in the robotics domain including object tracking, facial recognition, environmental localization, and hazard detection. In the past, computer vision sensor/software systems have been slow, bulky, and power-hungry. Recent advances in imaging hardware and embedded processing now provide an opportunity to shrink vision systems, including stereo vision and other complex operations, to a size that would allow them to be embedded in a wearable apparatus, similar to wraparound sunglasses. Using an auditory signal to feedback environmental information to the wearer, this device will provide valuable, and previously unimaginable, visual sensing capabilities to the visually impaired. These include: 1) determining distance traveled even in GPS-denied environments; 2) detecting and classifying obstacles, drop-offs, overhangs, and other nearby hazards; and 3) detecting the presence and relative location of nearby people. <br/><br/>The broader impact/commercial potential of this project will be a significant breakthrough in the compact combination of computer vision and human interface technologies. The technology developed in this proposal has considerable impact for the visually impaired and strong commercial potential. The needs of the visually impaired are not being met by existing technology. The proposed technology will increase the independence of the visually impaired and improve their quality of life, especially with respect to social interaction. The technology, produced initially to help the visually impaired, has the potential for a much broader scientific and commercial impact. Commercial potential for this product includes robotics, military ground forces, augmented reality, and surveillance. The augmented reality market has particular broad impact beyond the visually impaired. First responders such as fire fighters and police officers can receive additional information via a computer vision prosthetic that enhances their existing perception. Additionally, there exists a strong demand by human interface researchers for this technology in a commercially available device."
"1040711","Workshop/Tutorial/Competition:  Computational Symmetry in Computer Vision","IIS","ROBUST INTELLIGENCE","06/15/2010","06/10/2010","Yanxi Liu","PA","Pennsylvania State Univ University Park","Standard Grant","Jie Yang","05/31/2012","$28,517.00","","yanxi@cse.psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","7495","7495","$0.00","Humans, animals and insects have an innate ability to perceive and take <br/>advantage of symmetry, which is a pervasive phenomenon presenting itself <br/>in all forms and scales in natural and man-made environments. Although <br/>our understanding of repeated patterns is generalized by the mathematical concept of symmetries and group theory, perception and recognition of symmetry has yet to be fully explored in machine intelligence and computer vision, and few effective computational methods are available today. <br/><br/>In response to a resurging interest in computational symmetry with the vision community, this timely and unique workshop/tutorial/competition is organized to investigate this potentially powerful intermediate level tool.<br/><br/>The event has three main parts:<br/>(1) a multidisciplinary perspective on the importance and lasting impact of symmetry, presented by a worldwide group of distinguished speakers;<br/><br/>(2) a detailed summary of the mathematical theory, state of the art algorithms and a diverse set of applications (successes and failures); and<br/><br/>(3) the algorithms and the outcome of the symmetry detection competition, presented by the top three performers on the benchmarked symmetry detection algorithm competition.<br/><br/>Active participations by computer vision researchers, especially graduate students, in this event are expected, leading to broadened understanding and appreciation of symmetry in all participants, as well as an acute and lasting impact to their research and their use of computational symmetry tools. A website with the content of the workshop/tutorial, the competition process and final results is set up before and augmented after the workshop, for public access."
"1049032","EAGER: A New Framework for Balancing Deformability and Discriminability in Computer Vision","IIS","Robust Intelligence","09/01/2010","08/22/2010","Haibin Ling","PA","Temple University","Standard Grant","Jie Yang","08/31/2012","$68,863.00","","hling@cs.stonybrook.edu","1801 N. Broad Street","Philadelphia","PA","191226003","2157077547","CSE","7495","7495, 7916","$0.00","Deformability and discriminability are often two ""conflicting"" factors in computer vision problems such as shape matching and object recognition. For example, it has been observed that strong deformation invariant descriptors often suffer from low discriminative powers for category recognition. This EAGER project explores a new framework for balancing deformability and discriminability for computer vision tasks. The framework uniformly embeds an object, which can be a 2D shape, a point set, an image, a 3D volume or a surface, in a high dimensional space named aspect space. The embedding parameter is then used to control the degree of deformation insensitivity. Both the theoretic and application sides of the proposed framework are investigated. Based on the framework, the project aims to develop three additional research goals: robust shape matching methods by selecting deformability adaptively, robust point set registration methods by dealing with articulation in the framework, and robust image matching by extracting features in the embedded aspect space. These goals are planned to be evaluated on real applications including silhouette-based foliage data retrieval, 3D marker matching in computer-based physical therapy, and image-based disease screening. <br/>The project aims to bridge the two main problems, handling deformation and improving discriminability, which relate to many subfields inside and outside computer vision. The interdisciplinary applications are expected to generate significant contributions to various fields including biodiversity studies, biomedical study, etc. The research results, including code and data, are made public available through the project website."
"1028048","Recurrent Deep Learning Machines","ECCS","EPCN-Energy-Power-Ctrl-Netwrks","09/01/2010","08/27/2010","James Lo","MD","University of Maryland Baltimore County","Standard Grant","Paul Werbos","08/31/2014","$295,151.00","","jameslo@umbc.edu","1000 Hilltop Circle","Baltimore","MD","212500002","4104553140","ENG","7607","","$0.00","The objective of this research is to develop a new paradigm of deep learning machine - those with a feedback structure. Feedbacks bring to computing nodes current or past information contained in neighboring or larger receptive fields of other computing nodes from the same or higher layers for forming better local representations or features. Such information is required for processing dynamical data and for maximizing generalization capabilities on static data. The approach of this research is to select or design deep and recurrent architectures, develop generative and discriminative learning techniques, and integrating the risk-averting method of convexifying training criteria into training recurrent deep learning machines. <br/><br/>    Intellectual Merit: Recurrent neural networks are irreplaceable for applications involving dynamical data and are fundamentally better than feedforward networks even on static data. However, difficulty in training recurrent networks has stifled development and understanding of them. The proposed research is expected to help remove this difficulty, bring forth the full power of recurrent neural networks, and boost interests in neural networks in general, which have unfortunately and undeservedly fallen out of favor in recent years.<br/><br/>    Broader Impact: Recurrent deep learning machines are powerful for static and dynamical classification and regression, including image and video recognition, analysis and compression; nonlinear system identification/control; signal processing/filtering; and critical system health/fault monitoring/detection. Therefore, the proposed work will contribute greatly to medical instrumentation, computer/robot/information technology, wireless telecommunication, national defense, and homeland security. Recurrent deep learning machines will ecome an important component in the graduate education in engineering and computer science"
"1118355","II-NEW: Collaborative Research: Spam Processing, Archiving, and Monitoring Community Facility (SPAM Commons)","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","12/01/2010","03/11/2011","Brent Kang","VA","George Mason University","Standard Grant","Marilyn McClure","08/31/2012","$50,391.00","","bkang5@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","CSE","7359","7359, 9218, HPCC","$0.00","In this project, the PIs propose to construct and develop a shared infrastructure to support the collection and maintenance of realistic, large scale spam data sets, <br/>referred as SPAM Commons.<br/><br/>Spam is a problem in many important communications media such as email and web. A sub-problem of spam, phishing (a form of online pretexting), caused an estimated $3.2B in damages in 2007. The broad impact of effective spam filtering methods can be estimated in billions of dollars in several communications media such as email and web.<br/><br/>Spam has also invaded other media, with concrete attack examples in social networks, blogosphere, Internet telephony (VoIP), instant messaging, and click fraud. <br/><br/>Unfortunately, spam research has been hampered by the lack of published real world data sets due to concerns with privacy and company intellectual property. This project team develops a shared infrastructure to support the collection and maintenance of realistic, large scale spam data sets, called Spam Processing, Archiving, and Monitoring Community Facility (SPAM Commons). <br/><br/>The main goals of SPAM Commons are: <br/>(1) to facilitate remedial research that will stem the wastes and losses caused by spam, and <br/>(2) enable revolutionary research that aim for stopping certain kinds of spam attacks altogether. <br/><br/>SPAM Commons is divided into a Public Partition and a Protected Partition.<br/><br/>The Public Partition is a direct analog of standard corpora for speech and image recognition research, consisting of a systematic and regular collection of both spam and legitimate data in the various communications media, starting from email and web spam, and expanding into other communications media as spam becomes a serious threat in each area and data become available. <br/><br/>The Protected Partition consists of a combined data and processing facility that makes private data or near real-time spam data available for experimental evaluation of spam defense mechanisms in a protected testbed. Access to such protected data will enable new spam research on real-time evolving spam and real world data sets that is infeasible today. <br/><br/>The intellectual challenges of the SPAM Commons project extend beyond the new research on various abovementioned spam areas enabled by the availability of data sets. The construction of both partitions of SPAM Commons includes significant intellectual challenges of their own. First, the isolation of Protected Partition addresses partially the concerns of privacy, which remains a general research problem. Second, useful spam and legitimate data sets require automated distinction of spam from legitimate documents with certainty, which remains an open research question in email, web, and other media. Third, the adversarial and mutual evolution of spam producers and defenders require continuous collection of fresh data for further study. Finally, the collection and streaming of near-real-time spam data represent research resources currently unavailable to spam researchers. Advances in these areas will spur the growth and evolution of SPAM Commons that will enable new research on the evolving and growing spam problem.<br/><br/>The impact of SPAM Commons data sets on experimental spam research may be similar to the impact of large corpora in disciplines such as speech/image recognition and natural language processing, which achieved a level of scientific result reproducibility and comparativeness after the use of such corpora became standard requirements. The proposed data repository will be supported and used by 9 university partners (Clayton State, Emory, Georgia Tech, NC A&T, Northwestern, Texas A&M, UC Davis, U. Georgia, UNC Charlotte), and several industry partners (IBM, PureWire, Secure Computing)."
"1027886","CDI-Type I: Collaborative Research: Gaining Knowledge from Other Patients: Structuring and Searching the Content of Health-Related Web Posts","BCS","CDI TYPE I","10/01/2010","09/27/2010","Noemie Elhadad","NY","Columbia University","Standard Grant","Joan Maling","09/30/2014","$427,288.00","","noemie.elhadad@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","SBE","7750","7721","$0.00","Individuals with chronic diseases rely more and more on online forums, blogs, and mailing lists to exchange information, practical tips, and stories about their conditions and to get emotional support from their peers. While this type of social networking has become central to the daily lives and decision-making processes of many patients, there has been little research on the quality of the content it conveys, as well as its use and impact in the fields of medicine and public health. On the patients' side, forums are surprisingly technologically poor: users have often no choice but to browse through massive numbers of posts while looking for a particular piece of information. The lack of appropriate tools to organize, analyze and ultimately understand the overwhelming number of health-related, patient-written posts hinders researchers from investigating this medium and hinders patients from using this medium to its full potential. <br/><br/>This project aims at helping both patients and health professionals access online patient-authored information by creating tools to search for information in patient forums. The proposed work spans several fields: natural language processing, data management, information retrieval, public health and behavioral medicine, and it will build the foundations for understanding peer patient posts available through online forums and mailing lists.<br/><br/>This proposal aims at bringing together information processing and medical understanding of patient-centric resources. The work in this project will process texts from an emerging medium, which directly addresses the immediate concerns of patients. The tools designed as part of this project will benefit the researchers who study the behaviors and information needs of patients online. These tools, such as the intelligent search engine for posts, will also enhance the experience of the patients themselves, who are avid users of this medium. In addition to the research agenda, this proposal presents an education plan consistent with the overall goal of bridging the gap between researchers in computer science and researchers in medical fields. In particular, a course is presented that introduces methods of intelligent information processing in the context of research questions important to the fields of public health and medicine."
"1027801","CDI-Type I:  Collaborative Research:   Gaining Knowledge from Other Patients:  Structuring and Searching the content of Health-Related Web Posts","BCS","CDI TYPE I","10/01/2010","09/27/2010","Amelie Marian","NJ","Rutgers University New Brunswick","Standard Grant","Joan Maling","09/30/2014","$302,268.00","","amelie@cs.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","SBE","7750","7721","$0.00","Individuals with chronic diseases rely more and more on online forums, blogs, and mailing lists to exchange information, practical tips, and stories about their conditions and to get emotional support from their peers. While this type of social networking has become central to the daily lives and decision-making processes of many patients, there has been little research on the quality of the content it conveys, as well as its use and impact in the fields of medicine and public health. On the patients' side, forums are surprisingly technologically poor: users have often no choice but to browse through massive numbers of posts while looking for a particular piece of information. The lack of appropriate tools to organize, analyze and ultimately understand the overwhelming number of health-related, patient-written posts hinders researchers from investigating this medium and hinders patients from using this medium to its full potential. <br/><br/>This project aims at helping both patients and health professionals access online patient-authored information by creating tools to search for information in patient forums. The proposed work spans several fields: natural language processing, data management, information retrieval, public health and behavioral medicine, and it will build the foundations for understanding peer patient posts available through online forums and mailing lists.<br/><br/>This proposal aims at bringing together information processing and medical understanding of patient-centric resources. The work in this project will process texts from an emerging medium, which directly addresses the immediate concerns of patients. The tools designed as part of this project will benefit the researchers who study the behaviors and information needs of patients online. These tools, such as the intelligent search engine for posts, will also enhance the experience of the patients themselves, who are avid users of this medium. In addition to the research agenda, this proposal presents an education plan consistent with the overall goal of bridging the gap between researchers in computer science and researchers in medical fields. In particular, a course is presented that introduces methods of intelligent information processing in the context of research questions important to the fields of public health and medicine."
"0968583","SoCS:  Collaborative Research:  Leveraging Others' Insights to Improve Collaborative Analysis","IIS","Information Technology Researc, SOCIAL-COMPUTATIONAL SYSTEMS","07/01/2010","04/06/2011","Sara Kiesler","PA","Carnegie-Mellon University","Standard Grant","Tatiana Korelsky","06/30/2015","$265,915.00","","kiesler@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","1640, 7953","7953, 9215, 9251","$0.00","This research seeks to improve collaborative analysis in fields such as criminal justice, intelligence, and epidemiology by developing tools that capture and represent essential elements of analytic conversations. The work has three major aims: (a) understanding how team hypotheses, insights, and problem orientation are reflected in their conversations; (b) developing and testing natural language processing (NLP) techniques to detect teams? hypotheses, insights, and problem orientation; and (c) developing and testing methods to communicate the results of NLP analyses to provide teams with feedback on their own and other teams? reasoning processes. These goals are addressed through behavioral studies, NLP research, and tool development and evaluation.<br/>Intellectual merit: The project provides unique contributions in four areas: (a) understanding how team analytical processes are evidenced in team communication; (b) advancing the state-of-the art in NLP by developing discourse analysis techniques for use in collaborative analysis applications; (c) applying NLP techniques to the support of team analytical processes; and (d) designing interfaces to provide feedback within and across teams. The research also furthers the training and education of undergraduate and graduate students. <br/>Broader impact: The project has the potential to improve collaborative investigative analysis in many fields of critical importance to society, including criminal justice, intelligence, science, and epidemiology. The results will provide new tools for analysts, recommendations for organizational practices to improve the quality of collaborative analysis, new methods for training professional analysts, and new learning tools for graduate programs in fields such as epidemiological analysis and criminal justice."
"1000700","Collaborative Research:  Machine Vision Enhanced Post Earthquake Inspection and Rapid Loss Estimation","CMMI","Structural and Architectural E","08/15/2010","06/25/2010","Reginald DesRoches","GA","Georgia Tech Research Corporation","Standard Grant","Kishor Mehta","01/31/2014","$185,032.00","Ioannis Brilakis","rdr@rice.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","ENG","1637","036E, 039E, 040E, 1057, 9102","$0.00","The objective of this project is to develop and validate a computational framework for post-earthquake inspection of reinforced concrete (RC) frame buildings that will enable rapid, automated assessment of the damage state of the structure and of the cost and time required to repair the structure. The proposed automated procedure will start with collection of video frames using a high-resolution video camera mounted on an inspector?s hardhat. Then, state-of-the-art detection and extraction algorithms will be employed to detect RC columns and identify and characterize column damage. Component damage will be classified using empirically based models, and component damage will be used to determine the damage state of the building. Building damage state, configuration and type will be used to query a set of fragility curves defining the likelihood of building collapse during an aftershock and, thereby, provide an improved understanding of risk.  Deliverables include a catalog of fundamental visible damage characteristics, model-based recognition and analysis tools, demonstration and validation via hardware, documentation of research results, engineering student education, and outreach seminars to building evaluators.<br/><br/>If successful, the results of this research will provide the first robust method in the area of structural member and damage recognition from video. This scientific breakthrough will allow researchers to integrate this work in as-built building information modeling, project monitoring, virtual and augmented reality and other applications of importance to the engineering community. Also, this will be the first known study to quantitatively link visual damage in a building component (column or wall), to the likelihood of building collapse using robust probabilistic methods.  The discoveries sought in this project are expected to serve as a foundation for a new knowledge base in damage assessment and to promote intellectual cross-pollination among the fields of computer vision and structural engineering. The results will be disseminated to allow the creation of commercial software that have increased precision, reduced cost, and work with reduced weight devices. Graduate and undergraduate engineering students and K-12 students will benefit through classroom instruction and involvement in the research."
"1018590","III: Small: Collaborative Research: Building a Large Multilingual Semantic Network for Text Processing Applications","IIS","Info Integration & Informatics","09/15/2010","09/13/2010","Razvan Bunescu","OH","Ohio University","Standard Grant","Sylvia Spengler","08/31/2014","$224,540.00","","bunescu@ohio.edu","108 CUTLER HL","ATHENS","OH","457012979","7405932857","CSE","7364","7923","$0.00","This project is devoted to building a large multilingual semantic network<br/>through the application of novel techniques for semantic analysis<br/>specifically targeted at the Wikipedia corpus. The driving hypothesis of<br/>the project is that the structure of Wikipedia can be effectively used to<br/>create a highly structured graph of world knowledge in which nodes<br/>correspond to entities and concepts described in Wikipedia, while edges<br/>capture ontological relations such as hypernymy and meronymy. Special<br/>emphasis is given to exploiting the multilingual information available in<br/>Wikipedia in order to improve the performance of each semantic analysis<br/>tool. Significant research effort is therefore aimed at developing tools<br/>for word sense disambiguation, reference resolution and the extraction of<br/>ontological relations that use multilingual reinforcement and the<br/>consistent structure and focused content of Wikipedia to solve these tasks<br/>accurately. An additional research challenge is the effective integration<br/>of inherently noisy evidence from multiple Wikipedia articles in order to<br/>increase the reliability of the overall knowledge encoded in the global<br/>Wikipedia graph. Computing probabilistic confidence values for every piece<br/>of structural information added to the network is an important step in<br/>this integration, and it is also meant to provide increased utility for<br/>downstream applications. The proposed highly structured semantic network<br/>complements existing semantic resources and is expected to have a broad<br/>impact on a wide range of natural language processing applications in need<br/>of large scale world knowledge.<br/><br/>For further information, please see the project website:<br/>http://lit.csci.unt.edu/index.php/Mu.Se.Net"
"0968450","SoCS:  Collaborative Research:  Leveraging Others' Insights to Improve Collaborative Analysis","IIS","INFORMATION TECHNOLOGY RESEARC, SOCIAL-COMPUTATIONAL SYSTEMS","07/01/2010","06/06/2012","Claire Cardie","NY","Cornell University","Standard Grant","Tatiana D. Korelsky","06/30/2014","$519,471.00","Claire Cardie","cardie@cs.cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","1640, 7953","7953, 9215, 9251","$0.00","This research seeks to improve collaborative analysis in fields such as criminal justice, intelligence, and epidemiology by developing tools that capture and represent essential elements of analytic conversations. The work has three major aims: (a) understanding how team hypotheses, insights, and problem orientation are reflected in their conversations; (b) developing and testing natural language processing (NLP) techniques to detect teams? hypotheses, insights, and problem orientation; and (c) developing and testing methods to communicate the results of NLP analyses to provide teams with feedback on their own and other teams? reasoning processes. These goals are addressed through behavioral studies, NLP research, and tool development and evaluation.<br/>Intellectual merit: The project provides unique contributions in four areas: (a) understanding how team analytical processes are evidenced in team communication; (b) advancing the state-of-the art in NLP by developing discourse analysis techniques for use in collaborative analysis applications; (c) applying NLP techniques to the support of team analytical processes; and (d) designing interfaces to provide feedback within and across teams. The research also furthers the training and education of undergraduate and graduate students. <br/>Broader impact: The project has the potential to improve collaborative investigative analysis in many fields of critical importance to society, including criminal justice, intelligence, science, and epidemiology. The results will provide new tools for analysts, recommendations for organizational practices to improve the quality of collaborative analysis, new methods for training professional analysts, and new learning tools for graduate programs in fields such as epidemiological analysis and criminal justice."
"1018314","RI:  Small:  Acquiring Domain Knowledge from Text through Cooperative Bootstrapping","IIS","Info Integration & Informatics, Robust Intelligence","07/01/2010","05/06/2011","Ellen Riloff","UT","University of Utah","Continuing grant","Tatiana Korelsky","06/30/2015","$391,845.00","","riloff@cs.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","7364, 7495","7923, 9150, 9251","$0.00","Some of the most pressing needs for natural language processing (NLP) technology come from specialized domains where broad-coverage solutions are not sufficient, such as clinical medicine and molecular biology.  This project focuses on the development of bootstrapped learning techniques to rapidly create domain-specific semantic analyzers, and the automatic harvesting of domain knowledge from unstructured text.<br/><br/>This project establishes a new cooperative bootstrapping paradigm to learn semantic analyzers for different tasks simultaneously by allowing classifiers for different tasks to learn from each other.<br/>These analyzers then populate a domain event graph with semantic information extracted from a domain-specific text collection. New knowledge harvesting algorithms acquire domain-specific facts and inference rules from the graph. This project explores the domain of veterinary medicine using message board posts by veterinarians to acquire real-world knowledge for the purposes of animal health surveillance.<br/><br/>This work will advance the state-of-the-art in natural language technology by developing a new bootstrapping framework to rapidly create semantic analysis tools for specialized domains.  This technology will impact many NLP applications, including information extraction, question answering, and summarization.  The knowledge harvesting tools will be made publicly available to allow for direct impact across many disciplines that have a need to extract knowledge from unstructured text collections.  This project will also benefit society by creating new tools for animal health surveillance, which could provide early warning signs of zoonotic disease outbreaks (such as bird flu and mad cow disease), exposures to toxic substances, and contamination in the food chain."
"1037944","Collaborative Research:  Sino-USA Summer School in Vision, Learning, Pattern Recognition VLPR 2010","IIS","Catalyzing New Intl Collab, Robust Intelligence","07/15/2010","07/15/2010","Ying Wu","IL","Northwestern University","Standard Grant","Jie Yang","09/30/2012","$25,000.00","","yingwu@eecs.northwestern.edu","750 N. Lake Shore Drive","Chicago","IL","606114579","3125037955","CSE","7299, 7495","5978, 7299, 7495, 9200","$0.00","The recent decade has witnessed rapid advances in computer vision research, not only in its fundamental studies but also its emerging applications. This Sino-USA summer school in Vision, Learning and Pattern Recognition (VLPR 2010) is held in Xi'an City, China. It brings together a high-quality team of leading American and Chinese researchers in computer vision to offer a one-week educational program to students and junior scholars from both US and China. This education program provides an important opportunity to discuss recent advance in Perception, Motion and Events, and allows technical and culture exchanges between researchers from two countries. Such interactions are important for fostering new understanding and new collaborations in science, education, and culture. <br/><br/>Summer School web site: http://vlpr2010.eecs.northwestern.edu/"
"1037845","Collaborative Research:  Sino-USA Summer School in Vision, Learning, Pattern Recognition VLPR 2010","IIS","Catalyzing New Intl Collab, ROBUST INTELLIGENCE","07/15/2010","07/15/2010","James Rehg","GA","Georgia Tech Research Corporation","Standard Grant","Jie Yang","06/30/2011","$25,000.00","","rehg@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7299, 7495","5978, 7299, 7495, 9200","$0.00","The recent decade has witnessed rapid advances in computer vision research, not only in its fundamental studies but also its emerging applications. This Sino-USA summer school in Vision, Learning and Pattern Recognition (VLPR 2010) is held in Xi'an City, China. It brings together a high-quality team of leading American and Chinese researchers in computer vision to offer a one-week educational program to students and junior scholars from both US and China. This education program provides an important opportunity to discuss recent advance in Perception, Motion and Events, and allows technical and culture exchanges between researchers from two countries. Such interactions are important for fostering new understanding and new collaborations in science, education, and culture. <br/><br/>Summer School web site: http://vlpr2010.eecs.northwestern.edu/"
"1018613","III: Small: Collaborative Research: Building a Large Multilingual Semantic Network for Text Processing Applications","IIS","Info Integration & Informatics","09/15/2010","09/11/2013","Rada Mihalcea","TX","University of North Texas","Standard Grant","Sylvia Spengler","08/31/2014","$275,336.00","","mihalcea@umich.edu","1155 Union Circle #305250","Denton","TX","762035017","9405653940","CSE","7364","7923","$0.00","This project is devoted to building a large multilingual semantic network<br/>through the application of novel techniques for semantic analysis<br/>specifically targeted at the Wikipedia corpus. The driving hypothesis of<br/>the project is that the structure of Wikipedia can be effectively used to<br/>create a highly structured graph of world knowledge in which nodes<br/>correspond to entities and concepts described in Wikipedia, while edges<br/>capture ontological relations such as hypernymy and meronymy. Special<br/>emphasis is given to exploiting the multilingual information available in<br/>Wikipedia in order to improve the performance of each semantic analysis<br/>tool. Significant research effort is therefore aimed at developing tools<br/>for word sense disambiguation, reference resolution and the extraction of<br/>ontological relations that use multilingual reinforcement and the<br/>consistent structure and focused content of Wikipedia to solve these tasks<br/>accurately. An additional research challenge is the effective integration<br/>of inherently noisy evidence from multiple Wikipedia articles in order to<br/>increase the reliability of the overall knowledge encoded in the global<br/>Wikipedia graph. Computing probabilistic confidence values for every piece<br/>of structural information added to the network is an important step in<br/>this integration, and it is also meant to provide increased utility for<br/>downstream applications. The proposed highly structured semantic network<br/>complements existing semantic resources and is expected to have a broad<br/>impact on a wide range of natural language processing applications in need<br/>of large scale world knowledge.<br/><br/>For further information, please see the project website:<br/>http://lit.csci.unt.edu/index.php/Mu.Se.Net"
"0958266","SBIR Phase II:   Knowledge Discovery based on Personal Web Content Annotation","IIP","SBIR Phase II","03/01/2010","02/17/2010","Victor Karkar","MA","skribel, Inc.","Standard Grant","Juan E. Figueroa","10/31/2012","$500,000.00","","victor@skribel.com","19 Ivy Lane","Burlington","MA","018031332","7815529851","ENG","5373","5373, 6850, 9139, HPCC","$0.00","This Small Business Innovation Research (SBIR) Phase II project aims to develop a Recommendation System that offers users links to relevant pages as they browse the Web. As users interact with a Web page annotation platform, they use tools, such as highlighters and sticky notes, to annotate pages. The System is able to leverage these annotations to accurately model the user's information need, and to deliver high-quality recommendations. This Phase II project builds upon a prototype developed in Phase I, applying techniques from the information retrieval and natural language processing research communities to improve recommendation quality. This project encompasses primary research in document modeling, index representations and retrieval models. Further, the project proposes interesting synergies by drawing in methods from the text categorization, topic detection and tracking and collaborative filtering communities. <br/><br/>The Broader Impact of this work lies in its potential to positively impact the task of doing research on the web. The company's nascent Web annotation Platform promises to save users time, reducing cost and frustration by providing content management and organizational structures that allow them to preserve state between web research sessions. The next step is to deploy the Recommendation System to bring users the next page they need before they even realize they need it. Individual users and businesses alike will derive value from the time savings provided by the company's Platform and its Recommendation System."
"0941950","Scaffolding Wiki Use in Engineering Courses","DUE","S-STEM-Schlr Sci Tech Eng&Math, CCLI-Type 1 (Exploratory)","03/15/2010","03/10/2010","Jihie Kim","CA","University of Southern California","Standard Grant","Don Millard","02/28/2013","$170,000.00","Erin Shaw","jihie@isi.edu","University Park","Los Angeles","CA","900890001","2137407762","EHR","1536, 7494","9178, SMET","$0.00","Computer Science (31) <br/><br/>Wikis, known as collaborative knowledge building environments, are being used to facilitate computer mediated collaboration among undergraduate students working in project-based settings. As educational technology becomes an integral part of the instructional engineering landscape and distance education becomes accepted practice, understanding how students learn engineering becomes intertwined with understanding how computer mediated communication helps and hinders learning. <br/><br/>The Pedagogical Wiki project is creating a novel framework for assessing and scaffolding collaborative learning within Wiki environments. Research being undertaken includes (a) studying student adoption and interaction in new and ongoing Wiki-based engineering courses, and comparing Wiki adoption in non-engineering (education) courses;(b) developing instructional assessment tools based on discourse analysis and course topic ontology for qualitatively evaluating student Wiki interactions; and (c) identifying scaffolding opportunities, such as topic-based material sharing, to promote student engagement and communication. Activity Theory is being used as a framework to study Wiki adoption.<br/><br/>Ultimately, a key component of this work lies in the new discourse and topic-based instruments that enable instructors and educational researchers to better assess student learning within Wiki environments.  Qualitative as well as quantitative metrics are being developed, some with respect to the particular domains being studied. The combined use of new natural language processing techniques and traditional instruments to study Wiki adoption in an engineering context contributes to an understanding of how students learn engineering in a collaborative, ""real-world"" medium.  Codifying best practices for computer mediated collaboration is impacting the way engineering is being taught."
"1036868","Collaborative Research:  Workshop for Women in Machine Learning","IIS","Info Integration & Informatics, Robust Intelligence","09/01/2010","06/28/2010","Jennifer Vaughan","CA","University of California-Los Angeles","Standard Grant","Todd Leen","08/31/2014","$41,824.00","","jenn@cs.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","7364, 7495","7495","$0.00","Since 2006, the annual workshop for Women in Machine Learning (WiML) has brought together female<br/>researchers in industry and academia, postdoctoral fellows, and graduate students from the machine<br/>learning community to exchange research ideas and build mentoring and networking relationships. The<br/>one-day workshop has been especially beneficial for junior graduate students, giving them a supportive<br/>environment in which to present their research (in many cases, for the first time) and enabling them to<br/>meet peers and more senior researchers in the field of machine learning. The networking opportunities<br/>provided by the workshop have also helped senior graduate students find jobs following graduation.<br/>Intellectual Merit: This workshop will advance machine learning knowledge and foster collaboration<br/>within the machine learning community. As invited speakers, established researchers at top universities<br/>and research labs will teach workshop participants about cutting-edge ideas from diverse areas of<br/>machine learning. Students will present their own research and receive valuable feedback from both<br/>senior researchers and their peers. By enabling women at all stages of their careers in machine learning to<br/>exchange research ideas and form new relationships, we expect that new connections and research<br/>collaborations will be established, thereby advancing the state-of-the-art of the field.<br/>Broader Impact: This workshop will provide a forum for female graduate students, postdoctoral fellows,<br/>junior and senior faculty, and industry and government research scientists to exchange research ideas and<br/>establish networking and mentoring relationships. Undergraduates, particularly those who are interested<br/>in pursuing graduate school or industry positions in machine learning, are also welcome to attend.<br/>Bringing together women from different stages of their careers gives established researchers the<br/>opportunity to act as mentors, and enables junior women to find female role models working in the field<br/>of machine learning. The workshop will also benefit the wider machine learning community: Firstly, the<br/>WiML website, which lists all previous workshop presenters, serves as a useful resource for organizations<br/>looking for female invited speakers. Secondly, co-locating with a major machine learning conference<br/>enhances the visibility of female researchers among the wider machine learning community. Thirdly,<br/>travel funding provided to workshop participants also facilitates their travel to the co-located conference,<br/>which for some participants would otherwise not be possible. Finally, all workshop materials (slides,<br/>abstracts, etc.) will be made available on the workshop website in order to ensure broad dissemination."
"1027989","CDI Type I: Collaborative Research: Machine Learning in Taxonomic Research","MCB","CDI TYPE I","10/01/2010","09/30/2010","Yixin Chen","MS","University of Mississippi","Standard Grant","Engin Serpersu","09/30/2015","$285,455.00","","ychen@cs.olemiss.edu","100 BARR HALL","UNIVERSITY","MS","386771848","6629157482","BIO","7750","7721, 9150","$0.00","Intellectual Merit<br/>It is estimated that less than 10 percent of the world's species have been described, yet species are being lost daily due to human destruction of natural habitats. Considering the fast pace of habitat destruction, experts fear that many species will become extinct before they can be discovered and formally described. The job of describing the earth's remaining species is exacerbated by the shrinking number of practicing taxonomists and the very slow pace of traditional taxonomic research. In describing new species of animals, taxonomists typically rely on specimens deposited in natural history museums. They have to make careful counts and measurements on large numbers of specimens from multiple populations across the geographic ranges of both known and newly discovered species, in order to diagnose the new species as distinct from all of its known relatives. The process is laborious and can take years or even decades to complete, depending on the geographic range of the species. In this project, the research team will develop new machine learning methods for taxonomic research, with the specific aim of fundamentally increasing the pace of taxonomic revision.<br/><br/>The scientific research will focus on two areas: species identification and new species discovery. In distinguishing a species from others, taxonomists must identify a set of diagnostic characters that distinguishes the species in question from all of its known relatives. To automate and expedite this laborious process, the team will explore existing feature subset selection techniques, and will develop new ones. Images of categorized specimens will be used to train a collection of statistical models representing the known taxonomic grouping of organisms. An ""optimal"" set of body shape characters will be automatically identified. New species discovery is the most important research objective in taxonomy. From a machine learning point of view, detecting new species is fundamentally different from the problem of recognizing known species because by definition, the training set does not contain prior knowledge of a new species. The research team will formulate new species discovery as a novelty detection problem, and will develop an efficient novelty detection framework for taxonomic tasks.<br/><br/>Broader Impact<br/>The project, if carried out successfully, will demonstrate the fruitfulness of fusing technologies from different fields. From the biology side, the project will demonstrate that machine learning techniques can assist taxonomists and evolutionary biologists in various research tasks, hence fundamentally accelerate the pace of taxonomic revision. From the computer science side, researchers will benefit from the computational challenges motivated from real-world biological problems and the database created in the project. New machine learning algorithms will be developed to impact taxonomic research. The PIs will integrate the research with their educational activities at both University of Mississippi and Tulane University. The PIs will devote additional efforts to mentoring female and underrepresented minority students involved in exploring cutting-edge interdisciplinary research."
"1037002","Collaborative Research:  Workshop for Women in Machine Learning","IIS","ROBUST INTELLIGENCE","09/01/2010","06/28/2010","Hanna Wallach","MA","University of Massachusetts Amherst","Standard Grant","Todd Leen","08/31/2014","$6,330.00","","wallach@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","7495","7495","$0.00","Since 2006, the annual workshop for Women in Machine Learning (WiML) has brought together female<br/>researchers in industry and academia, postdoctoral fellows, and graduate students from the machine<br/>learning community to exchange research ideas and build mentoring and networking relationships. The<br/>one-day workshop has been especially beneficial for junior graduate students, giving them a supportive<br/>environment in which to present their research (in many cases, for the first time) and enabling them to<br/>meet peers and more senior researchers in the field of machine learning. The networking opportunities<br/>provided by the workshop have also helped senior graduate students find jobs following graduation.<br/>Intellectual Merit: This workshop will advance machine learning knowledge and foster collaboration<br/>within the machine learning community. As invited speakers, established researchers at top universities<br/>and research labs will teach workshop participants about cutting-edge ideas from diverse areas of<br/>machine learning. Students will present their own research and receive valuable feedback from both<br/>senior researchers and their peers. By enabling women at all stages of their careers in machine learning to<br/>exchange research ideas and form new relationships, we expect that new connections and research<br/>collaborations will be established, thereby advancing the state-of-the-art of the field.<br/>Broader Impact: This workshop will provide a forum for female graduate students, postdoctoral fellows,<br/>junior and senior faculty, and industry and government research scientists to exchange research ideas and<br/>establish networking and mentoring relationships. Undergraduates, particularly those who are interested<br/>in pursuing graduate school or industry positions in machine learning, are also welcome to attend.<br/>Bringing together women from different stages of their careers gives established researchers the<br/>opportunity to act as mentors, and enables junior women to find female role models working in the field<br/>of machine learning. The workshop will also benefit the wider machine learning community: Firstly, the<br/>WiML website, which lists all previous workshop presenters, serves as a useful resource for organizations<br/>looking for female invited speakers. Secondly, co-locating with a major machine learning conference<br/>enhances the visibility of female researchers among the wider machine learning community. Thirdly,<br/>travel funding provided to workshop participants also facilitates their travel to the co-located conference,<br/>which for some participants would otherwise not be possible. Finally, all workshop materials (slides,<br/>abstracts, etc.) will be made available on the workshop website in order to ensure broad dissemination."
"1027830","CDI-Type I: Collaborative Research: Machine Learning in Taxonomic Research","MCB","CDI TYPE I","10/01/2010","09/30/2010","Henry Bart","LA","Tulane University","Standard Grant","Engin Serpersu","09/30/2015","$285,497.00","Nelson Rios","hbartjr@tulane.edu","6823 ST CHARLES AVENUE","NEW ORLEANS","LA","701185698","5048654000","BIO","7750","7721, 9150","$0.00","Intellectual Merit<br/>It is estimated that less than 10 percent of the world's species have been described, yet species are being lost daily due to human destruction of natural habitats. Considering the fast pace of habitat destruction, experts fear that many species will become extinct before they can be discovered and formally described. The job of describing the earth's remaining species is exacerbated by the shrinking number of practicing taxonomists and the very slow pace of traditional taxonomic research. In describing new species of animals, taxonomists typically rely on specimens deposited in natural history museums. They have to make careful counts and measurements on large numbers of specimens from multiple populations across the geographic ranges of both known and newly discovered species, in order to diagnose the new species as distinct from all of its known relatives. The process is laborious and can take years or even decades to complete, depending on the geographic range of the species. In this project, the research team will develop new machine learning methods for taxonomic research, with the specific aim of fundamentally increasing the pace of taxonomic revision.<br/><br/>The scientific research will focus on two areas: species identification and new species discovery. In distinguishing a species from others, taxonomists must identify a set of diagnostic characters that distinguishes the species in question from all of its known relatives. To automate and expedite this laborious process, the team will explore existing feature subset selection techniques, and will develop new ones. Images of categorized specimens will be used to train a collection of statistical models representing the known taxonomic grouping of organisms. An ""optimal"" set of body shape characters will be automatically identified. New species discovery is the most important research objective in taxonomy. From a machine learning point of view, detecting new species is fundamentally different from the problem of recognizing known species because by definition, the training set does not contain prior knowledge of a new species. The research team will formulate new species discovery as a novelty detection problem, and will develop an efficient novelty detection framework for taxonomic tasks.<br/><br/>Broader Impact<br/>The project, if carried out successfully, will demonstrate the fruitfulness of fusing technologies from different fields. From the biology side, the project will demonstrate that machine learning techniques can assist taxonomists and evolutionary biologists in various research tasks, hence fundamentally accelerate the pace of taxonomic revision. From the computer science side, researchers will benefit from the computational challenges motivated from real-world biological problems and the database created in the project. New machine learning algorithms will be developed to impact taxonomic research. The PIs will integrate the research with their educational activities at both University of Mississippi and Tulane University. The PIs will devote additional efforts to mentoring female and underrepresented minority students involved in exploring cutting-edge interdisciplinary research."
"0965338","TLS and DAT: Construct Utilization in the Behavioral Sciences","SBE","SCIENCE OF SCIENCE POLICY","05/15/2010","05/10/2010","Kai Larsen","CO","University of Colorado at Boulder","Standard Grant","Joshua Rosenbloom","04/30/2013","$358,622.00","Jintae Lee, Eliot Rich","kai.larsen@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","SBE","7626","0000, OTHR","$0.00","While it is natural for research disciplines to split into more specialized areas, the resulting divide complicates efforts to share the benefits of related research. Constructs are important underpinnings of a research method used in many behavioral and social sciences, and virtually identical constructs have been developed to support different theories, frequently under different names. As a result, useful connections are missed, constructs and theories are reinvented, and little knowledge exists about the construct origins and flow among disciplines. <br/><br/>Intellectual Merit: This project addresses the problem by investigating the following research question: is it possible to identify closely related constructs, including those in different disciplines and use this information to measure the extent to which existing constructs are utilized effectively? The question is examined in the domain of 'latent construct research using scales,' which is typified by the studies using questionnaire scales to assess latent constructs, thus spanning multiple disciplines in the behavioral and social sciences. For this domain, the notion of a Closely Related Construct (CRC), is formulated and operationalized using a method that integrates automated text analysis, citation analysis, and meta-analysis. Using CRC, a science metric termed Construct Utilization Ratio, is formulated that measures the extent to which closely related constructs are recognized between two units, where the unit may be chosen at different collective levels, such as between theories, journals, or research areas. <br/><br/>Broader Impact: The project examines how this metric can be used to quantitatively and qualitatively assess the extent to which CRCs for a given construct are recognized and utilized within and across different research areas. It can also reveal where opportunities and redundancies reside."
"1027985","CDI-Type I: Using Machine Learning to Develop New Approaches to Semiempirical Quantum Chemistry","CHE","CDI TYPE I","10/01/2010","09/20/2010","David Yaron","PA","Carnegie-Mellon University","Standard Grant","Evelyn Goldfield","09/30/2014","$670,947.00","Geoffrey Gordon","yaron@cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","MPS","7750","7721, 7752, 9216, 9263","$0.00","The proposed work melds quantum chemistry with machine learning to develop efficient computational methods for predicting the electronic structure of chemical systems. The past decades have brought quantum chemistry to a point where highly accurate results can be routinely generated for small molecules. However, the computational cost increases rapidly with molecular size, making calculations on proteins or complex nanostructures challenging. This project takes advantage of molecular similarity, whereby molecular fragments behave similarly in different environments, to substantially lower the computational cost. First, a database of accurate but computationally expensive high-level results on the electronic structure of a molecular fragment in a range of environments is generated. This data is then used to develop a machine learning algorithm that uses information about the molecular fragment and its environment to predict the behavior of the fragment. The challenge for machine learning is to generalize to new fragments and environments, to integrate this generalization into the larger molecular simulation, and finally to characterize the performance to allow reporting of the confidence in the eventual simulation results.  For example, if the learning algorithm works by breaking chemical space into regions that can be well described with low-cost approximating functions, the approach must characterize the boundaries of these regions and handle the transitions between the regions. This challenge will be addressed by a close integration of the chemistry and machine learning portions of the project, such that design decisions regarding the form of the approximating function and learning algorithm are made together.<br/><br/>The ability to quickly and accurately generate the energy of a molecular system would have broad impact in domains such as biology and nanotechnology. Current computational approaches to large molecular systems rely on greatly simplified models of the energy, such as the ball and stick models of molecular mechanics. While such models are useful for structure, functional predictions often require breaking and formation of chemical bonds, which requires more realistic electronic structure approaches. The approaches developed here are designed to make realistic functional predictions for large systems computationally feasible. The close integration of chemistry and machine learning also provides excellent interdisciplinary training opportunities for both graduate and undergraduate students.<br/><br/>This is a Cyber-Enabled Discovery and Innovation Program award and is co-funded by the Division of Chemistry and the Office of Multidisciplinary Activities."
"1000440","Collaborative Research:  Machine Vision Enhanced Post Earthquake Inspection and Rapid Loss Estimation","CMMI","Structural and Architectural E","08/15/2010","06/25/2010","Laura Lowes","WA","University of Washington","Standard Grant","Kishor Mehta","07/31/2014","$175,000.00","","lowes@u.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","ENG","1637","036E, 039E, 040E, 1057, 9102","$0.00","The objective of this project is to develop and validate a computational framework for post-earthquake inspection of reinforced concrete (RC) frame buildings that will enable rapid, automated assessment of the damage state of the structure and of the cost and time required to repair the structure. The proposed automated procedure will start with collection of video frames using a high-resolution video camera mounted on an inspector?s hardhat. Then, state-of-the-art detection and extraction algorithms will be employed to detect RC columns and identify and characterize column damage. Component damage will be classified using empirically based models, and component damage will be used to determine the damage state of the building. Building damage state, configuration and type will be used to query a set of fragility curves defining the likelihood of building collapse during an aftershock and, thereby, provide an improved understanding of risk.  Deliverables include a catalog of fundamental visible damage characteristics, model-based recognition and analysis tools, demonstration and validation via hardware, documentation of research results, engineering student education, and outreach seminars to building evaluators.<br/><br/>If successful, the results of this research will provide the first robust method in the area of structural member and damage recognition from video. This scientific breakthrough will allow researchers to integrate this work in as-built building information modeling, project monitoring, virtual and augmented reality and other applications of importance to the engineering community. Also, this will be the first known study to quantitatively link visual damage in a building component (column or wall), to the likelihood of building collapse using robust probabilistic methods.  The discoveries sought in this project are expected to serve as a foundation for a new knowledge base in damage assessment and to promote intellectual cross-pollination among the fields of computer vision and structural engineering. The results will be disseminated to allow the creation of commercial software that have increased precision, reduced cost, and work with reduced weight devices. Graduate and undergraduate engineering students and K-12 students will benefit through classroom instruction and involvement in the research."
"0953181","CAREER: Socially Guided Machine Learning","IIS","HCC-Human-Centered Computing","01/01/2010","02/07/2014","Andrea Thomaz","GA","Georgia Tech Research Corporation","Continuing Grant","Ephraim Glinert","12/31/2015","$551,063.00","","athomaz@ece.utexas.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7367","1045, 1187, 7218, 7367, 9102, 9215, HPCC","$0.00","There is currently a surge of interest in service robotics, a desire to have robots leave the labs and factory floors to help solve issues facing society.  But if robots are to play a useful role in domains ranging from eldercare to education, they will need the ability to interact with ordinary people and to acquire new relevant skills after they are deployed; we cannot pre-program these robots with every skill they might conceivably need.  The PI's approach to solving this critical issue is Socially Guided Machine Learning (SG-ML).  In this project she will explore ways in which machine learning agents can exploit principles of human social learning.  An important question for SG-ML is ""What is the right level of human involvement?""  Previous efforts in machine learning systems that use human input have tended to hold this level constant (e.g., guidance oriented approaches that are completely dependent on human instruction in order to learn, and exploration oriented approaches with limited input from a human partner).  The PI, taking her inspiration from human learning and from her prior work in robot learning, posits that a robot should be able to explore and learn on its own, while also taking full advantage of a human partner's guidance when available.  The PI's goal in this work is to successfully incorporate self and social learning within a single SG-ML framework, enabling a robot learner to dynamically adjust to varying levels of human involvement in the learning process.  To this end, the PI will seek to make progress toward four main objectives:<br/><br/>1) Motivations for learning: Typically machines learn because they are programmed to do so, unlike children and animals who learn because they are motivated to master their environment.  A key component of this work is computational motivations that drive a robot to good learning opportunities.<br/><br/>2) Multiple learning strategies: As mentioned above, an SG-ML framework should have a repertoire of self and social learning mechanisms working together. A central issue of this research is how the robot should best arbitrate and manage the use of multiple strategies.<br/><br/>3) Transparency devices: Learning is a collaborative activity. The learner's behavior has to be understandable, and has to express a certain level of internal state to help the teacher guide the learning process.  Transparency is a fundamental issue of this work, developing robot behaviors that successfully communicate the progress of the learning process.<br/><br/>4) Engagement mechanisms: In human social learning, teaching is a rewarding process for both the learner and the teacher.  This is a positive feedback loop from which a machine learner could benefit.  A primary component of this work is to develop mechanisms that make the teaching process rewarding.<br/><br/>Broader Impacts:  The long-term promise of this research is robots in society able to adapt and learn from everyday people.  The core principles developed in this work will one day enable robots to adapt and learn about the changing needs of people in their homes, or staff in a hospital.  The lessons learned about social learning with robots will be relevant both to computational devices and to human-computer interaction in general.  The PI will exploit the fact that social robot projects like this one generate particular interest in the community to conduct outreach programs in local area high schools, to raise awareness about the work of women in science, and to stimulate the American public's interest in science."
"0953219","CAREER: Using Machine Learning to Understand and Enhance Human Learning Capacity","IIS","Robust Intelligence","06/01/2010","05/05/2014","Xiaojin Zhu","WI","University of Wisconsin-Madison","Continuing grant","Weng-keen Wong","05/31/2017","$465,586.00","","jerryzhu@cs.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","7495","1045, 1187","$0.00","Understanding and enhancing human learning are important challenges in the 21st century.  Existing human category learning models cannot quantify important capacities such as people's (in)ability to generalize from training to test, to learn from imperfect data, or to learn by actively asking questions.  <br/><br/>This research project studies human learning using machine learning.  It first develops machine learning theory and algorithms to quantify these human learning capacities: It establishes learning-theoretic error bounds on human generalization performance; It models human learning from an imperfect teacher with non-parametric Bayesian methods; It models human's ability to ask informative questions with active learning theory.  The project then studies computational approaches to enhance human learning: It develops ""machine teaching"" algorithms when the computer knows the target concept, and selects the optimal training examples to teach a human learner; It develops ""human machine co-learning"" algorithms when the computer does not know the target concept, but instead learns alongside the human and suggests better learning strategies to her.  Each topic is verified by human experiments.<br/><br/>The project advances machine learning with new learning theory and algorithms on tasks where humans excel.  It advances cognitive psychology with new models of human learning.  It has broader impacts in understanding human intelligence, and in benefiting students with new educational tools.  This research project is integrated with an educational plan that incorporates undergraduate and graduate teaching and mentoring, developing a new course and a book on machine and human learning, organizing seminars, tutorials and workshops, and sharing all results on a website."
"0964037","AF:  Medium:  Collaborative Research:  Solutions to Planar Optimization Problems","CCF","Algorithmic Foundations, ALGORITHMS","08/01/2010","03/06/2015","Philip Klein","RI","Brown University","Standard Grant","jack snoeyink","06/30/2015","$636,988.00","Claire Kenyon-Mathieu","klein@brown.edu","BOX 1929","Providence","RI","029129002","4018632777","CSE","7796, 7926","7924, 7926, 9216, 9218, 9251, HPCC","$0.00","The aim of this research is to develop new algorithms and algorithmic<br/>techniques for solving fundamental optimization problems on planar<br/>networks.  Many optimization problems in networks are considered<br/>computationally difficult; some are even difficult to solve<br/>approximately.  However, problems often become easier when the input<br/>network is restricted to be planar, i.e. when it can be drawn on the<br/>plane so that no edges cross each other.  Such planar instances of<br/>optimization problems arise in several application areas, including<br/>logistics and route planning in road maps, image processing and<br/>computer vision, and VLSI chip design.  <br/><br/>The investigators plan to develop algorithms that achieve faster<br/>running times or better approximations by exploiting the planarity of<br/>the input networks.  In addition, in order to address the use of<br/>optimization in the discovery of some ground truth, the investigators<br/>will develop algorithms not just for the traditional worst-case input<br/>model but also for models in which there is an unusually good planted<br/>solution; for a model of this kind, the investigators expect to find<br/>algorithms that produce even more accurate answers.<br/><br/>The research will likely uncover new computational techniques whose<br/>applicability goes beyond planar networks.  In the recent past, once a<br/>technique has been developed and understood in the context of planar<br/>networks, it has been generalized to apply to broader families of<br/>networks.<br/><br/>In addition, new algorithms and techniques resulting from this<br/>research might enable people to quickly compute better solutions to<br/>problems arising in diverse application areas.  For example, research<br/>in this area has already had an impact in the computer vision<br/>community.  Further research has the potential to be useful, for<br/>example, in the design of networks, the planning of routes in road<br/>maps, the processing of images."
"1004447","Montclair REU Site in Imaging and Computer Vision  (iMagine)","IIS","RSCH EXPER FOR UNDERGRAD SITES, , , ","04/01/2010","03/26/2012","Stefan Robila","NJ","Montclair State University","Continuing grant","Maria Zemankova","03/31/2014","$287,760.00","Jing Peng","robilas@mail.montclair.edu","1 Normal Avenue","Montclair","NJ","070431624","9736556923","CSE","1139, J103, J243, k629","7736, 9218, 9250, HPCC","$0.00","The aims of this renewal REU project are to improve the understanding and processing of imaging data and to develop of computer- based solutions to current imaging and vision problems.  Student research projects will include: spectral data processing, perceptual, ubiquitous, and mobile user interfaces, content based media retrieval, multi- approach integration of image processing, and inverse problems using scattering theories. While designed for undergraduate research, the projects are relevant to current problems in the imaging field and thus seek to contribute to its advancement in a real way.<br/><br/>This site is supported by the Department of Defense in partnership with the NSF REU program."
"0959979","MRI-R2: Development of an Immersive Giga-pixel Display","CNS","Major Research Instrumentation","05/01/2010","02/09/2012","Arie Kaufman","NY","SUNY at Stony Brook","Standard Grant","Rita Rodriguez","04/30/2014","$1,400,000.00","Amitabh Varshney, Hong Qin, Dimitrios Samaras, Klaus Mueller","ari@cs.stonybrook.edu","WEST 5510 FRK MEL LIB","Stony Brook","NY","117940001","6316329949","CSE","1189","6890","$1,400,000.00","""This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5)."" <br/>Proposal #: 09-59979<br/>PI(s):  Kaufman, Arie E., Mueller, Klaus, Qin, Hong, Samaras, Dimitrios, Varshney, Amitabh<br/>Institution: SUNY at Stony Brook<br/>Title:    MRI-R2: Development of an Immersive Giga-pixel Display<br/>Project Proposed: <br/>This project, developing a next generation of immersive display instrument (called 'Reality Deck'), aims to explore and visualize data from many fields. To satisfy the need driven by the explosive growth of data size and environments already at the institution, the work builds on the existing experience with immersive environments (e.g., the 'Immersive Cabin' a current generation device using projectors). This unique project generates a one-of-a-kind exploration theater, using 308 high-resolution 30 LCD display monitors, by contributing an environment whose visual resolution is at the limit of the human eye's acuity. Within this environment investigators can interact with the data/information displayed.  <br/>The instrument services many groups, including visual computing, virtual and augmented reality, human computer interfaces, computer vision and image processing, data mining, physics, scientific computing, chemistry, marine and atmospheric sciences, climate and weather modeling, material science, etc.<br/>Collaborating scientist's applications will be ported to the RealityDeck, including applications in nanoelectronics, climate and weather modeling, biotoxin simulations, microtomography, astronomy, atmospheric science, G-pixel camera for intelligence gathering, architectural design and disaster simulations, smart energy grid, and many others.<br/>A unique assembly of displays, GPU cluster, sensors, communication/networking, computer vision and human computer interaction technologies, RealityDeck is an engineering feat with user studies to deliver a holistic system with a significant societal and research value. It is a one-of-a-kind pioneering G-pixel display approaching the limits of visual cognition that provides functionalities to a diverse user base. Its resolution at the eye's visual acuity and its field of view will always exceed that of a human user (wherever a human chooses to look), satisfying visual queries into the data in a very intuitive way. This visual interaction is tightly coupled with physical navigation.<br/>This surround virtual environment consists of inertial sensors and six cameras mounted around the top corners of the RealityDeck room to allow interaction with the displays. The display system is driven by a cluster of about 80 high-end computer nodes, each equipped with two high end GPUs. A small-scale video-wall has already been constructed as an experimental platform for the RealityDeck consisting of 9 high-resolution 30 LCD panels in a 33 configuration. <br/>Broader Impacts: <br/>The instrument will be used for research, education, and outreach across many departments at the institution, the University of Maryland, and Brookhaven National Laboratory (BNL). It fosters collaborations across disciplines attracting faculty, researchers, and students. RealityDeck significantly enriches the quality of visual thinking and data exploration. It substantially enhances the infrastructure of research and education and has the potential to alter the way computer graphicists, engineers, and scientists work and/or conduct scientific discoveries."
"0963921","AF:  Medium:  Collaborative Research:  Solutions to Planar Optimization Problems","CCF","ALGORITHMS","08/01/2010","07/26/2010","Glencora Borradaile","OR","Oregon State University","Standard Grant","Balasubramanian Kalyanasundaram","07/31/2014","$175,008.00","","glencora@eecs.oregonstate.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","CSE","7926","9218, HPCC","$0.00","The aim of this research is to develop new algorithms and algorithmic<br/>techniques for solving fundamental optimization problems on planar<br/>networks. Many optimization problems in networks are considered<br/>computationally difficult; some are even difficult to solve<br/>approximately. However, problems often become easier when the input<br/>network is restricted to be planar, i.e. when it can be drawn on the<br/>plane so that no edges cross each other. Such planar instances of<br/>optimization problems arise in several application areas, including<br/>logistics and route planning in road maps, image processing and<br/>computer vision, and VLSI chip design.<br/><br/>The investigators plan to develop algorithms that achieve faster<br/>running times or better approximations by exploiting the planarity of<br/>the input networks. In addition, in order to address the use of<br/>optimization in the discovery of some ground truth, the investigators<br/>will develop algorithms not just for the traditional worst-case input<br/>model but also for models in which there is an unusually good planted<br/>solution; for a model of this kind, the investigators expect to find<br/>algorithms that produce even more accurate answers.<br/><br/>The research will likely uncover new computational techniques whose<br/>applicability goes beyond planar networks. In the recent past, once a<br/>technique has been developed and understood in the context of planar<br/>networks, it has been generalized to apply to broader families of<br/>networks.<br/><br/>In addition, new algorithms and techniques resulting from this<br/>research might enable people to quickly compute better solutions to<br/>problems arising in diverse application areas. For example, research<br/>in this area has already had an impact in the computer vision<br/>community. Further research has the potential to be useful, for<br/>example, in the design of networks, the planning of routes in road<br/>maps, the processing of images."
"0953373","CAREER: A New Statistical Framework for Natural Images with Applications in Vision","IIS","ROBUST INTELLIGENCE","06/01/2010","03/12/2014","Siwei Lyu","NY","SUNY at Albany","Continuing grant","Jie Yang","09/30/2016","$499,596.00","","lsw@cs.albany.edu","1400 WASHINGTON AVE MSC 100A","Albany","NY","122220100","5184374974","CSE","7495","1045","$0.00","This project studies natural image statistics, and their applications in diverse fields such as computational neuroscience, image processing, computer vision, and graphics. The centerpiece of this project is a new image representation based on a simple nonlinear transform that is statistically justified and biologically inspired. This representation provides a new language to describe image signals, and forms the basis to build statistical models to more effectively capture statistical properties of natural image. Built upon this new image representation, this project explores new paradigms to model and interpret visual neural responses and high-level perceptual properties, and provides new tools for image restoration, analysis and synthesis. On the other hand, by applying natural image statistics to the forensic analysis of digital images, this project facilitates forensic practitioners in criminal investigations, and contributes to national security and public safety. Moreover, this project contributes to education by making the learning of Computer Science fun and useful for undergraduate students, promoting the participation of women and undergraduate students in research, and improving the early learning of mathematics and sciences for local high school students."
"1018771","CSR: Small: Learning-Assisted Parallelization","CNS","SPECIAL PROJECTS - CISE, Computer Systems Research (CSR","08/01/2010","03/09/2012","Ronald Barnes","OK","University of Oklahoma Norman Campus","Continuing grant","M. Mimi McClure","07/31/2015","$515,882.00","Amy McGovern","ron@ou.edu","201 Stephenson Parkway","NORMAN","OK","730199705","4053254757","CSE","1714, 7354","7923, 9150, 9178, 9251","$0.00","The number of processing cores in everyday computer systems is quickly outpacing programmers' and compilers' ability to create many-threaded applications capable of taking advantage of the cores.  Instead of requiring applications to be rewritten to take advantage of the many-core architectures, this project aims to create automated parallelization approaches that utilize machine learning to improve traditional parallelization methods to create more effective (and often speculative) threads.  These techniques will be applied to the arrangement of the parallel execution of tasks (whether coarse- or fine-grained), yielding a systematic learning approach that will outperform static parallelization attempts.   This approach assists both programmer identified parallelism as well as that exposed by the compiler.  This project is not limited to applications with regular loop-level parallelism, but targets irregular integer applications which have proven difficult to statically parallelize.  <br/><br/>This project will result in the development of a high performance dynamic parallelization system based on critical information gathered from machine learning and related approaches.  By creating intelligent parallelization approaches, we enable the multicore machines of the future to be used to their full advantage by both scientists and the general public.  Although many of today's applications may not require the full computational power of a many-core machine, the effective utilization of such machines will enable new, more-powerful applications than are currently possible."
"1004902","REU Site: Applied Computing Research in Wireless Sensing of Marine Data","CNS","RSCH EXPER FOR UNDERGRAD SITES, CPATH","06/01/2010","09/16/2010","Dulal Kar","TX","Texas A&M University Corpus Christi","Continuing grant","Harriet Taylor","05/31/2014","$269,293.00","Long-zhuang Li, Ahmed Mahdy","dulal.kar@tamucc.edu","6300 Ocean Drive, Unit 5844","Corpus Christi","TX","784125844","3618253882","CSE","1139, 7640","9250","$0.00","This CISE REU Site award provides funding to Texas A&M University-Corpus Christi, a Hispanic Serving Institution on the Texas Gulf Coast, to offer research experience in applied computing to 24 undergraduate students of primarily Hispanic descent.   The students will participate in deriving solutions for technological challenges faced by researchers in the collection, storage, and analysis of data from marine and aquatic environments of the coastal region. The primary objective of the program is to provide participants unique research experiences in a stimulating environment that will involve designing, developing, adapting, and testing wireless sensor networks for data gathering operations from marine and aquatic environments.  Activities include training sessions and social events designed to enhance group cohesiveness and to hone technical writing, communication, and leadership skills. Primary research activities will occur in the areas of design of marine sensor networks, reliability and security of marine sensor networks, and integration of marine sensor data from a variety of domains on the Gulf of Mexico. The participants will write and present research papers on their work in local, regional, and national symposiums. <br/><br/>The intellectual merit comes from student participation in a unique marine-based research program that seeks solutions for survivable, secured, and reliable design of sensor networks for marine environment; energy-efficient sensor data gathering for marine applications; and integration of marine sensor data from a variety of domains on the Gulf of Mexico. The students participate in projects that have the potential to impact the field and to make a contribution to the university's efforts to become a global leader in research on the Gulf of Mexico and its resources. <br/><br/>The broader impacts of the proposed program will extend beyond the project in a number of ways.  These experiences should instill in the students a passion for research in applied computing for environmental applications and should motivate them for further learning and discovery in the computing disciplines. The project has the potential to increase retention rates of science, technology, engineering, and mathematics (STEM) majors. The project should also broaden inter-institutional collaboration, particularly among Hispanic serving institutions nationwide and will advance science and technical knowledge on sustainable marine resource utilization and preservation globally."
"0964481","AF: Medium:Smoothed Analysis in Multi-Objective Optimization, Machine Learning, and Algorithmic Game Theory","CCF","Algorithmic Foundations, ALGORITHMS, COMPUT GAME THEORY & ECON","03/01/2010","01/17/2012","Shanghua Teng","CA","University of Southern California","Continuing grant","Balasubramanian Kalyanasundaram","02/28/2015","$1,099,858.00","","shanghua@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","7796, 7926, 7932","9218, HPCC","$0.00","Technical description of the project:<br/><br/>Smoothed analysis has been introduced to study the behavior of algorithms when their good practical performance cannot be predicted by the traditional frameworks such as the worst-case and average-case analyses. In the original paper that introduced smoothed analysis, Spielman and Teng proved that the simplex method, which was one of the amazingly practical heuristics known to have poor worst-case performance, had good smoothed complexity. Since its introduction, smoothed analysis has been applied to heuristics in areas that include mathematical programming, numerical analysis, and combinatorial optimization.<br/><br/>To follow up on this breakthrough in linear programming, whose primary concern is to optimize a single linear objective function, the goal of this project is to extend smoothed analysis to optimization problems involving multiple objective functions and multiple agents, as often considered in Computational Game and Economics Theory. The plan of the project will also include the design and analysis of machine learning algorithms in the smoothed setting.<br/><br/>The plan for multiobjective optimization is to improve the recent work which show that the Pareto set of any binary linear optimization problem with a constant number of objective functions has a polynomial size in the smoothed model. In game theory, the research will focus on the smoothed complexity of several potential games for resource allocation and scheduling. For both of these problems, a new analytical approach for the performance of an iterative process is needed.  In machine learning, the goal is to extend the results on the smoothed analysis of PAC and agnostic learning from product distributions to other more practical distributions.<br/><br/><br/>The broader significance and importance of the project:<br/><br/>This research will provide insight into why heuristics in multi-objective optimization, local search, and machine learning are successful and allow researchers in Theoretical Computer Science to incorporate these heuristics into the canon of Algorithms. It will also provide algorithmic insights into the differences between game-theoretic problems involving multiple agents and multi-objective optimization problems concerning only a single agent. By developing a theory that better predicts the performance of algorithms in practice, this research has the promise to allow researchers to develop more powerful algorithms. When one designs an algorithm, one does so with some reason in mind as to why it should work. By providing a new analytical notion of what it means for an algorithm to ""work in practice,"" this research can shape the intuition of algorithm designers to include algorithms that might otherwise have been discarded for being ""bad in the worst-case.""<br/><br/>A primary objective of this project is to build bridges between the discipline of Theoretical Computer Science (TCS) and communities within the disciplines of Operations Research, Game Theory, and Mathematical Economics. If successful, the project will help to provide a framework within which researchers in TCS can understand some of the great practical achievements of these disciplines. In turn, by providing analyses that respect the sensibilities of researchers in these disciplines as to what constitutes a ""practical input,"" this project will increase the value of theoretical analyses to researchers in these fields. As this project combines ideas from many disciplines within one coherent research effort, lectures and tutorials presented on the fruits of the project will help cross-fertilize the disciplines within its scope. The development of theoretical explanations for the success of the heuristics examined in this project should simplify education in algorithms and allow discussion of practically important heuristics at earlier stages of a computer scientist's education. These explanations also simplify the task of designing easily digestible lectures on these topics, and such lecture notes inspired by this research will be made available by the PI and collaborators on the internet."
"1029035","Collaborative Research:  Computational Behavioral Science:  Modeling, Analysis, and Visualization of Social and Communicative Behavior","IIS","INFORMATION TECHNOLOGY RESEARC","09/01/2010","09/11/2012","David Forsyth","IL","University of Illinois at Urbana-Champaign","Continuing grant","Ephraim P. Glinert","08/31/2017","$1,500,000.00","Karrie Karahalios","daf@cs.uiuc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","1640","1640, 7723, 7969, 9218, HPCC","$0.00","Computational Behavioral Science: Modeling, Analysis, and Visualization of Social and <br/>Communicative Behavior<br/>Lead PI/Institution: James M. Rehg, Georgia Institute of Technology<br/>This Expedition will develop novel computational methods for measuring and analyzing the behavior of children and adults during face-to-face social interactions. Social behavior plays a key role in the acquisition of social and communicative skills during childhood. Children with developmental disorders, such as autism, face great challenges in acquiring these skills, resulting in substantial lifetime risks. Current best practices for evaluating behavior and assessing risk are based on direct observation by highly-trained specialists, and cannot be easily scaled to the large number of individuals who need evaluation and treatment. For example, autism affects 1 in 110 children in the U.S., with a lifetime cost of care of $3.2 million per person. By developing methods to automatically collect fine-grained behavioral data, this project will enable large-scale objective screening and more effective delivery and assessment of therapy. Going beyond the treatment of disorders, this technology will make it possible to automatically measure behavior over long periods of time for large numbers of individuals in a wide range of settings. Many disciplines, such as education, advertising, and customer relations, could benefit from a quantitative, data-drive approach to behavioral analysis. <br/>Human behavior is inherently multi-modal, and individuals use eye gaze, hand gestures, facial expressions, body posture, and tone of voice along with speech to convey engagement and regulate social interactions.  This project will develop multiple sensing technologies, including vision, speech, and wearable sensors, to obtain a comprehensive, integrated portrait of expressed behavior. Cameras and microphones provide an inexpensive, noninvasive means for measuring eye, face, and body movements along with speech and nonspeech utterances. Wearable sensors can measure physiological variables such as heart-rate and skin conductivity, which contain important cues about levels of internal stress and arousal that are linked to expressed behavior. This project is developing unique capabilities for synchronizing multiple sensor streams, correlating these streams to measure behavioral variables such as affect and attention, and modeling extended interactions between two or more individuals. In addition, novel behavior visualization methods are being developed to enable real-time decision support for interventions and the effective use of repositories of behavioral data. Methods are also under development for reflecting the capture and analysis process to users of the technology.<br/>The long-term goal of this project is the creation of a new scientific discipline of computational behavioral science, which draws equally from computer science and psychology in order to transform the study of human behavior. A comprehensive education plan supports this goal through the creation of an interdisciplinary summer school for young researchers and the development of new courses in computational behavior. Outreach activities include significant and on-going collaborations with major autism research centers in Atlanta, Boston, Pittsburgh, Urbana-Champaign, and Los Angeles."
"0953330","CAREER: Machine Learning and Event Detection for the Public Good","IIS","Info Integration & Informatics, SciSIP-Sci of Sci Innov Policy","07/01/2010","03/31/2010","Daniel Neill","PA","Carnegie-Mellon University","Standard Grant","Maria Zemankova","06/30/2016","$529,962.00","","daniel.neill@nyu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7364, 7626","0000, 1045, 1187, 7364, 7626, 9215, HPCC, OTHR","$0.00","The goal of this research is to create and explore novel methods for detection of emerging events in massive, complex real-world datasets.  The approach consists of new algorithms to efficiently and exactly find the most anomalous subsets of a large, high-dimensional dataset, as well as methodological advances to incorporate incremental model learning from user feedback into event detection, incorporate society-scale data from emerging, transformative technologies such as cellular phones and user-generated web content, and augment event detection by creating methods and tools for event characterization, explanation, visualization, investigation and response.  <br/><br/>The experimental research is integrated with a multi-pronged educational initiative to incorporate machine learning into the public policy curriculum through development of courses and seminars, workshops in machine learning and policy research and education, and establishment of a new Joint Ph.D. Program in Machine Learning and Policy.  The results of this project will be incorporated into deployed event surveillance systems and applied to the public health, law enforcement, and health care domains, enabling more timely and accurate detection of emerging outbreaks of disease, prediction of emerging hot-spots of violent crime, and identification of anomalous patterns of patient care.  Project results, including publications, software, and datasets, will be disseminated via project web site (http://www.cs.cmu.edu/~neill/CAREER)."
"1017199","RI: Small: 3D Nonrigid Object Reconstruction from Large-Scale Unorganized 2D Images","IIS","ROBUST INTELLIGENCE","09/01/2010","02/18/2011","Song Wang","SC","University South Carolina Research Foundation","Standard Grant","Jie Yang","08/31/2015","$216,000.00","","songwang@cec.sc.edu","1600 Hampton Street","COLUMBIA","SC","292080001","8037777093","CSE","7495","7923, 9251","$0.00","Reconstructing the 3D shape of an object from multiple 2D images is a fundamental problem in computer vision. Prior work on this problem usually requires the object of interest to be rigid or the available 2D images to be well organized, such as consecutive frames in a video. This project investigates the challenging problem of reconstructing a nonrigid 3D object from a large number of unorganized 2D images, which may be taken at different times, with different backgrounds, from different perspectives, under different lighting conditions, and/or using different cameras.<br/><br/>The research team develops new algorithms of combining object localization, feature matching, and partial shape matching across the images to segment the 2D object of interest from the input images. The segmented 2D objects are organized into clusters to recover the underlying 3D nonrigid deformation. Pieces of the 3D object are reconstructed from these clusters and finally assembled to obtain the complete 3D object by removing the in-between nonrigid deformations. An image database with 2D images of selected nonrigid objects is constructed for performance evaluation.<br/><br/>This research benefits many applications in computer vision, computer graphics, computer gaming, zoology, microbiology, marine science, and medical research, which all involve the modeling of 3D norigid objects. Progress made on object localization, feature matching and partial shape matching has immediate applications in object detection, object recognition, image search, surveillance, tracking, and segmentation. This research also provides an excellent setting for the training of both undergraduate and graduate students."
"1016395","RI: Small:Differential Ray Geometry for Surface Reconstruction and Modeling","IIS","GRAPHICS & VISUALIZATION","09/01/2010","08/26/2010","Jingyi Yu","DE","University of Delaware","Standard Grant","Jie Yang","08/31/2014","$370,940.00","","yu@cis.udel.edu","210 Hullihen Hall","Newark","DE","197160099","3028312136","CSE","7453","7923, 9150","$0.00","This project focuses on developing the theoretical foundation of differential ray geometry. The PI first derives a comprehensive ray geometry framework including the ray-distortion, ray-caustics, and ray-curvature theories as well as ray differential operators. This new framework is widely applicable to real-world problems. On the computer vision front, the PI explores robust and efficient schemes to infer ray structures from distortions or from caustics patterns and then recover surface geometry from ray differential attributes. This leads to a new class of specular (reflective and refractive) surface reconstruction algorithms. On the computer graphics front, the PI employs a novel normal-ray representation that converts a smooth 3D surface into a 2D ray manifold so that surface differential attributes can be directly derived from normal-ray geometry. The PI further develops new subdivision, re-meshing, and mesh simplification schemes for generating surfaces consistent with the underlying normal ray structures. <br/><br/>This research benefits many computer vision and graphics applications by providing a differential ray geometry model for cameras, light sources, and surfaces. It also benefits shape designs in aircraft, automobile, and many other industries, where higher-order shape consistencies are required. This project contributes to education through the development of new differential geometry courses and seminars and by involving women and under-represented students in mathematical and computer science research. The PI further seeks to build strong connections with the fields of mathematics and physics, optical engineering, and mechanical engineering through the project."
"0963071","Divvy: Robust and Interactive Cluster Analysis","SES","METHOD, MEASURE & STATS","06/01/2010","06/01/2010","Virginia de Sa","CA","University of California-San Diego","Standard Grant","Cheryl L. Eavey","05/31/2015","$310,000.00","","vdesa@cogsci.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930621","8585344896","SBE","1333","0000, OTHR","$0.00","This project will develop software for the application of rapid, robust, and interactive dimensionality reduction and clustering algorithms to real-world datasets.  The software, called Divvy, will provide parallel visualization of multiple dimensionality reduction and clustering techniques, flexible domain knowledge integration, customizable exemplar and outlier visualization, and dynamic indicators of cluster quality using theoretically sound cluster quality measures.  Divvy will leverage recent advances in parallel and graphics processing unit computing in order to deliver near real-time calculation of partitions on many datasets.  Divvy also will be used as a platform for psychophysical studies that investigate the role and behavior of human researchers in the data-analysis process.<br/><br/>Machine learning techniques are increasingly essential for scientific analysis in many different fields.  As datasets increase in size and dimensionality, scientists need access to tools that can help them quickly and easily perform exploratory data analysis and visualization.  Divvy will allow a user to rapidly interact with and visualize the results of many different dimensionality-reduction and clustering algorithms through an intuitive interface.  By collecting a broad set of cutting-edge machine-learning tools in one user-friendly interface, Divvy will enable substantial improvements in data analysis methodology for researchers outside of machine learning and related fields.  This project will support workshops and tutorials at conferences outside the machine learning field in order to evangelize recent machine learning techniques and encourage adoption of Divvy."
"1027965","CDI-Type II: Collaborative Research: Joint Image-Text Parsing and Reasoning for Analyzing Social and Political News Events","CNS","CDI TYPE II","10/01/2010","09/09/2010","ChengXiang Zhai","IL","University of Illinois at Urbana-Champaign","Standard Grant","Fahmida N. Chowdhury","09/30/2015","$500,001.00","","czhai@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7751","7721","$0.00","Summary: Rapidly changing technologies of multi-modal communication, from the global reach of international satellite TV, the proliferation of Internet news outlets, to YouTube, are transforming the news industry. In parallel, ?citizen journalism? is on the rise, enabled by smart phones, social networks, and blogs. The Internet is becoming a vast information ecosystem driven by mediated events ? elections, social movements, natural disasters, disease epidemics ? with rich heterogeneous data: text, image, and video. Meanwhile, the tools and methodologies for users and researchers are not keeping pace: it remains prohibitively labor-intensive to systematically access and study the vast amount of emerging news data. <br/><br/>Leveraging UCLA's ongoing digital collection of 85,000 hours of news videos, including 8.1 billion image frames and 530 million words of closed captioning, the research team is developing a new computational paradigm for analyzing massive datasets of social and political news events: (i) Studying joint image-text parsing to categorize news by topics and events, and analyzing selection and presentation biases across networks and media spheres in a statistical and quantitative manner never before possible; (ii) Studying by joint image-text mining to reason the persuasion intents, and modeling the techniques of verbal and visual persuasions; (iii) Discovering spatio-temporal patterns in the interactions of multiple mediated events, and analyzing agenda setting patterns; and (iv) Developing an interactive multi-perspective news interface, vrNewsScape, for visualizing and interacting with our computational and statistical results. <br/><br/>Intellectual merit: This interdisciplinary project makes innovative contributions to three disciplines. Transforming social science research. The project develops a data-driven paradigm for transforming communication research in the social sciences. By enabling quantitative studies of massive visual datasets, the research team identifies and characterizes large-scale patterns of news mediation and persuasion currently inaccessible to researchers, due to the prohibitive cost of manual analysis. The research team goes beyond traditional object detection, segmentation, and recognition by studying framing and persuasion techniques in images, an untouched topic in computer vision. The team studies semantic associations and meanings for object and scene categories in their social context. Also, the team is studying image parsing to fill the semantic gap ? a long standing technical barrier in image retrieval, and will generate narrative text descriptions from the parse trees so that they can be fused with the input text and closed captioning for topic mining. <br/><br/>The research goes beyond conventional topic mining from text to perform integrative text-image mining, bias detection, and pattern discovery in the spatio-temporal evolution of mediated news events. The research detects and summarizes controversy and mine user-generated content for analyzing communicative intent and persuasive effects. <br/><br/>Broader impacts: vrNewsScape is being made publicly available to researchers and graduate students. Because the news media report on events in multiple different expert domains ? including congressional and presidential politics, international relations, war and public uprisings, natural disasters and humanitarian aid missions, disease epidemics and health initiatives, criminal activity and court cases, celebrities and cultural events ? the analytical tools in development are not limited to a particular research domain in social, political and computer sciences, but permit for the first time a systematic and quantitative examination of the massive datasets required to understand today?s mediated society. <br/><br/>In education, the project extends UCLA?s Digital Civic Learning initiative (dcl.sscnet.ucla.edu), a program involving college and high-school students in the analysis of news, thus delivering education benefits to potentially a huge number of students nationwide in Communication Studies (in 2004, 433,000 college students were enrolled in Communication and Journalism and 209,000 in Political Science[153]), exposing them to a new generation of high-level tools for handling multimodal data and inspiring them to pursue computational thinking, in line with the NSF?s objectives."
"1016018","CIF: Small: Visual Recognition and Restoration In Concert","CCF","COMM & INFORMATION FOUNDATIONS, SIGNAL PROCESSING","08/01/2010","05/14/2012","Peyman Milanfar","CA","University of California-Santa Cruz","Continuing grant","John Cozzens","07/31/2013","$443,957.00","","milanfar@ee.ucsc.edu","1156 High Street","Santa Cruz","CA","950641077","8314595278","CSE","7797, 7936","9218, ","$0.00","Project Abstract for NSF Proposal 1016018<br/> Visual Recognition and Restoration In Concert<br/> Peyman Milanfar<br/> Electrical Engineering Department<br/> University of California at Santa Cruz<br/><br/>In this research effort a central challenge in computer vision is addressed: Namely, to recognize and enhance objects in complex visual scenes given imperfect images, and more generally, video data. This effort strengthens the theoretical and practical foundations for generic visual object recognition systems that can deal with significant variations in visual appearance, a large number of categories, and stochastically and systematically degraded data. Data imperfections can include random noise, blur, and environmental degradations. The approach has transformative potential for a broad range of practical applications such as scalable image search and retrieval, automatic annotation, surveillance and security, video forensics, and medical image analysis for computer-aided<br/>diagnosis.<br/><br/>The research advances the state-of-the-art in two important ways: (a) a unified and robust framework is derived for both (2-D) object and (3-D) action recognition, even when the data is subject to significant distortions, and (b) recognition and restoration from degraded data are treated in a common, statistically optimal setting. Traditionally, recognition and restoration have been addressed with limited awareness of each other?s techniques and of potential commonalities in approach. By improving, generalizing, and refining previously separate approaches to recognition with degraded data in an adaptive, non-parametric setting, for both 2-D and 3-D, this project contributes to the technical foundations and toolkits that can connect computer vision and image processing<br/>intelligently."
"1028163","CDI-Type II: Collaborative Research: A Paradigm Shift in Ecosystem and Environmental Modeling: An Integrated Stochastic, Deterministic, and Machine Learning Approach","IIS","CDI TYPE II","09/15/2010","09/16/2010","John Reilly","MA","Massachusetts Institute of Technology","Standard Grant","Kenneth C. Whang","08/31/2015","$150,000.00","","jreilly@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7751","7721, 7722, 7751","$0.00","This project will advance systems modeling approaches by developing a suite of stochastic modeling approaches, coupled with geostatistical and machine learning techniques.  The new system modeling approach will utilize both in situ and satellite remotely sensed data to improve system model parameters and model structure.  These novel developments, together with observed data, will advance ecosystem and environmental sciences through computational thinking.  The proposed approach will be used to develop a cyber-enabled stochastic carbon-weather system to provide more adequate quantification of regional carbon exchanges, which is critical to better understanding carbon-climate-atmosphere feedbacks and facilitating climate-policy making.   <br/><br/><br/>The proposed approach will transform the current system modeling approach by (1) developing a stochastic version of the deterministic differential equation models of ecosystems and environmental systems; (2) developing geospatial statistical techniques to fully exploit multifaceted observational data to improve model parameterization; (3) developing advanced statistical and machine learning techniques to further utilize observational data to improve model structure; and (4) applying the improved model to examine the societal and biogeochemical impacts of land use change.  Advantages of the proposed cyber-enabled terrestrial ecosystem model will include: (1) Efficiently quantifying regional net carbon exchanges and associated uncertainty and (2) Improving system model parameters and structure using advanced statistical and machine learning techniques and spatiotemporal data acquired over the U.S.   Project deliverables include: (1) An innovative, cyber-enabled carbon-weather system that can quantify net carbon exchanges and associated probabilistic information at high spatial and temporal resolution for the continental U.S. and (2) a suite of transformative advanced mathematical, statistical and system modeling techniques that could be applied to other complex modeling fields (e.g., hydrological modeling).  This project will significantly advance ecosystem sciences with computational thinking and will provide a unique opportunity to train a new generation of scientists in a highly interdisciplinary research environment."
"1028291","CDI-Type II: Collaborative Research: A Paradigm Shift in Ecosystem and Environmental Modeling: An Integrated Stochastic, Deterministic, and Machine Learning Approach","IIS","CDI TYPE II","09/15/2010","09/16/2010","Qianlai Zhuang","IN","Purdue University","Standard Grant","Kenneth C. Whang","08/31/2016","$1,591,428.00","Hao Zhang, Jian Zhang, Melba Crawford, Dongbin Xiu","qzhuang@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7751","7721, 7722, 7751","$0.00","This project will advance systems modeling approaches by developing a suite of stochastic modeling approaches, coupled with geostatistical and machine learning techniques.  The new system modeling approach will utilize both in situ and satellite remotely sensed data to improve system model parameters and model structure.  These novel developments, together with observed data, will advance ecosystem and environmental sciences through computational thinking.  The proposed approach will be used to develop a cyber-enabled stochastic carbon-weather system to provide more adequate quantification of regional carbon exchanges, which is critical to better understanding carbon-climate-atmosphere feedbacks and facilitating climate-policy making.   <br/><br/><br/>The proposed approach will transform the current system modeling approach by (1) developing a stochastic version of the deterministic differential equation models of ecosystems and environmental systems; (2) developing geospatial statistical techniques to fully exploit multifaceted observational data to improve model parameterization; (3) developing advanced statistical and machine learning techniques to further utilize observational data to improve model structure; and (4) applying the improved model to examine the societal and biogeochemical impacts of land use change.  Advantages of the proposed cyber-enabled terrestrial ecosystem model will include: (1) Efficiently quantifying regional net carbon exchanges and associated uncertainty and (2) Improving system model parameters and structure using advanced statistical and machine learning techniques and spatiotemporal data acquired over the U.S.   Project deliverables include: (1) An innovative, cyber-enabled carbon-weather system that can quantify net carbon exchanges and associated probabilistic information at high spatial and temporal resolution for the continental U.S. and (2) a suite of transformative advanced mathematical, statistical and system modeling techniques that could be applied to other complex modeling fields (e.g., hydrological modeling).  This project will significantly advance ecosystem sciences with computational thinking and will provide a unique opportunity to train a new generation of scientists in a highly interdisciplinary research environment."
"1005369","Topics on Computational Algebra","DMS","Information Technology Researc, Special Projects - CCF, MSPA-INTERDISCIPLINARY","09/01/2010","08/24/2010","Shuhong Gao","SC","Clemson University","Standard Grant","Tie Luo","08/31/2014","$210,000.00","","sgao@clemson.edu","230 Kappa Street","CLEMSON","SC","296345701","8646562424","MPS","1640, 2878, 7454","9150","$0.00","Computing Groebner bases and finding primary decomposition of polynomial ideals are two closely related topics that are fundamental in computational algebraic geometry. Groebner bases provide an essential tool for computation in algebra, especially in solving systems of multivariate polynomials. The primary decomposition theorem was proved by the late chess Master Emanuel Lasker in 1905 for polynomial rings and Emmy Noether in early 1920s for general Noetherian rings. Primary decomposition is a crucial step in computerizing schemes in algebraic geometry, yet it is still a big challenge to provide efficient algorithms for reasonable sized systems of polynomials. The major bottleneck is  in computing Groebner bases for systems of polynomials  that appear in the process of computing primary decomposition. The main goal of the project is to develop new efficient algorithms for computing Groebner bases and for finding primary decomposition.<br/><br/><br/>Solving polynomial systems is ubiquitous in sciences and engineerings. <br/>Its applications include, but not limited to, computer vision, computer-aided designs, coding theory, cryptography, robot kinematics, computational biology, etc. Work in this project  would benefit major computer algebra systems and their users in education and industry. It also bears direct applications in reliable and secure communications from Internet commercial to military combats and from cell phones to outer space explorations."
"1005117","REU Site: Computational Science Training at Marshall University for Undergraduates in the Mathematical and Physical Sciences","OAC","RSCH EXPER FOR UNDERGRAD SITES","04/01/2010","05/24/2011","Howard Richards","WV","Marshall University Research Corporation","Standard Grant","Almadena Chtchelkanova","03/31/2013","$326,484.00","Maria Babiuc Hamilton","howard.richards@marshall.edu","One John Marshall Dr.","Huntington","WV","257550002","3046964837","CSE","1139","9150, 9250","$0.00","Over the summers of 2010-2012, the Departments of Mathematics, Physics, and Chemistry at Marshall University will jointly host twelve students for ten weeks of instruction and research in computational science. Each student will extend a carefully selected and delimited aspect of his or her mentor's research. Research fields include: 1) the characterization of beta-compounded statistical distributions; 2) computer vision with gray-scale images; 3) visualizations of general relativity; 4) potential energy surfaces for van der Waals interactions; 5) ion-molecule complexation in the zero pressure limit; and 6) the dynamics of metastable decay for the Ising model on tree-like networks. <br/><br/>In addition to performing research in a specific area, students will be instructed in practices and issues that are common to all areas of computational science. They will also receive the equivalent of a one semester hour course in ethics for scientists and engineers. Experts will be brought in to discuss computational science from the more applied perspective of industry and government service, as well as potential career trajectories. <br/><br/>This program will focus on students in the EPSCoR states of West Virginia and Kentucky, as well as students from historically black colleges and universities. Students will present the results of their research in a symposium to conclude the summer program and at a professional conference as appropriate. Students will emerge from the program with skills and confidence that can carry them into postgraduate studies, as well as an awareness of possible careers using computational science."
"1013936","SBIR Phase I:  Computer Generation and Optimization of Image Processing Functions","IIP","SMALL BUSINESS PHASE I","07/01/2010","04/21/2010","Yevgen Voronenko","PA","SpiralGen Inc","Standard Grant","Errol Arkilic","12/31/2010","$150,000.00","","yvoronen@cmu.edu","201 S. Craig St.","Pittsburgh","PA","152133732","4125671010","ENG","5371","5371, 6850, 9139, HPCC","$0.00","This Small Business Innovation Research (SBIR) Phase I project proposes to enable computer generation of high performance software for image processing problems. For computing intensive functions, the by far most common approach is hand-optimization for every new platform.  SpiralGen's technology has the potential to demonstrate that it is possible to automatically produce performance-competitive code (code that is a good as human-written code and takes optimal advantage of multiple cores) for an important class of image processing functions.  The approach is based on a blend of mathematics, programming languages, symbolic computation, compilers, and optimization.<br/> <br/>Image processing (including its close cousin video processing) is one of the most demanding computing domains due to its higher-dimensional nature.  Key application domains that can benefit from the proposed technology include military applications (surveillance, target recognition), consumer multimedia processing, medical imaging (computer tomography), oil exploration, and computer vision.  The company's technology has the ability to provide a longterm solution to the porting and optimization problem: applications are ported by regenerating performance critical components optimized for the new platform. As a result, new more capable platforms can be deployed faster."
"1028381","CDI-Type II: Collaborative Research: Joint Image-Text Parsing and Reasoning for Analyzing Social and Political News Events","CNS","CDI TYPE II","10/01/2010","09/09/2010","Song-Chun Zhu","CA","University of California-Los Angeles","Standard Grant","Fahmida N. Chowdhury","09/30/2016","$1,299,998.00","Tim Groeling, Francis Steen","sczhu@stat.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","7751","7721","$0.00","Summary: Rapidly changing technologies of multi-modal communication, from the global reach of international satellite TV, the proliferation of Internet news outlets, to YouTube, are transforming the news industry. In parallel, ?citizen journalism? is on the rise, enabled by smart phones, social networks, and blogs. The Internet is becoming a vast information ecosystem driven by mediated events ? elections, social movements, natural disasters, disease epidemics ? with rich heterogeneous data: text, image, and video. Meanwhile, the tools and methodologies for users and researchers are not keeping pace: it remains prohibitively labor-intensive to systematically access and study the vast amount of emerging news data.<br/><br/>Leveraging UCLA's ongoing digital collection of 85,000 hours of news videos, including 8.1 billion image frames and 530 million words of closed captioning, the research team is developing a new computational paradigm for analyzing massive datasets of social and political news events: (i) Studying joint image-text parsing to categorize news by topics and events, and analyzing selection and presentation biases across networks and media spheres in a statistical and quantitative manner never before possible; (ii) Studying by joint image-text mining to reason the persuasion intents, and modeling the techniques of verbal and visual persuasions; (iii) Discovering spatio-temporal patterns in the interactions of multiple mediated events, and analyzing agenda setting patterns; and (iv) Developing an interactive multi-perspective news interface, vrNewsScape, for visualizing and interacting with our computational and statistical results.<br/><br/>Intellectual merit: This interdisciplinary project makes innovative contributions to three disciplines. Transforming social science research. The project develops a data-driven paradigm for transforming communication research in the social sciences. By enabling quantitative studies of massive visual datasets, the research team identifies and characterizes large-scale patterns of news mediation and persuasion currently inaccessible to researchers, due to the prohibitive cost of manual analysis. The research team goes beyond traditional object detection, segmentation, and recognition by studying framing and persuasion techniques in images, an untouched topic in computer vision. The team studies semantic associations and meanings for object and scene categories in their social context. Also, the team is studying image parsing to fill the semantic gap ? a long standing technical barrier in image retrieval, and will generate narrative text descriptions from the parse trees so that they can be fused with the input text and closed captioning for topic mining.<br/><br/>The research goes beyond conventional topic mining from text to perform integrative text-image mining, bias detection, and pattern discovery in the spatio-temporal evolution of mediated news events. The research detects and summarizes controversy and mine user-generated content for analyzing communicative intent and persuasive effects.<br/><br/>Broader impacts: vrNewsScape is being made publicly available to researchers and graduate students. Because the news media report on events in multiple different expert domains ? including congressional and presidential politics, international relations, war and public uprisings, natural disasters and humanitarian aid missions, disease epidemics and health initiatives, criminal activity and court cases, celebrities and cultural events ? the analytical tools in development are not limited to a particular research domain in social, political and computer sciences, but permit for the first time a systematic and quantitative examination of the massive datasets required to understand today?s mediated society.<br/><br/>In education, the project extends UCLA?s Digital Civic Learning initiative (dcl.sscnet.ucla.edu), a program involving college and high-school students in the analysis of news, thus delivering education benefits to potentially a huge number of students nationwide in Communication Studies (in 2004, 433,000 college students were enrolled in Communication and Journalism and 209,000 in Political Science[153]), exposing them to a new generation of high-level tools for handling multimodal data and inspiring them to pursue computational thinking, in line with the NSF?s objectives."
"1016862","RI:  Small:  Hierarchical Visual Scene Understanding","IIS","ROBUST INTELLIGENCE","09/01/2010","08/27/2010","Aude Oliva","MA","Massachusetts Institute of Technology","Standard Grant","Jie Yang","08/31/2014","$449,184.00","","oliva@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7495","7923, 9102","$0.00","Intelligent systems, both artificial and biological, must find effective ways to organize a complex visual world. The cross-disciplinary field of scene understanding is in need of a comprehensive framework in which to integrate cognitive, computational and neural approaches to the organization of knowledge. <br/><br/>This research program aims to create a framework for organizing knowledge of visual environments that human and artificial systems encounter when navigating in the world or browsing visual databases. The aim is to determine which taxonomies are best suited for solving different visual tasks, and use computer vision algorithms to organize visual environments as humans do. For example, semantic relationships between scenes are well captured by a hierarchical tree (e.g. a basilica is a type of church, which is a type of building) but functional similarities between different environments may be best represented as clusters  (e.g. restaurants, kitchens and picnic areas clustered as places to eat; offices and internet cafs as places to work). <br/><br/>Because hierarchies and taxonomies provide a way of formalizing many types of contextual information (spatial, temporal, and semantic), they can be used to enhance the performance of computer vision systems at object and scene recognition, and aid in the development of smarter image search algorithms. <br/><br/>Besides serving as a unified benchmark for comparing different models and theories, this enterprise offers new teaching and applied tools for research and courses, which will be made available through websites and symposia."
"1027955","CDI-Type II: Collaborative Research: A Paradigm Shift in Ecosystem and Environmental Modeling: An Integrated Stochastic, Deterministic, and Machine Learning Approach","IIS","CDI TYPE II","09/15/2010","09/16/2010","Jerry Melillo","MA","Marine Biological Laboratory","Standard Grant","Kenneth Whang","08/31/2015","$199,996.00","David Kicklighter","jmelillo@mbl.edu","7 M B L ST","WOODS HOLE","MA","025431015","5082897243","CSE","7751","7721, 7722, 7751","$0.00","This project will advance systems modeling approaches by developing a suite of stochastic modeling approaches, coupled with geostatistical and machine learning techniques.  The new system modeling approach will utilize both in situ and satellite remotely sensed data to improve system model parameters and model structure.  These novel developments, together with observed data, will advance ecosystem and environmental sciences through computational thinking.  The proposed approach will be used to develop a cyber-enabled stochastic carbon-weather system to provide more adequate quantification of regional carbon exchanges, which is critical to better understanding carbon-climate-atmosphere feedbacks and facilitating climate-policy making.   <br/><br/><br/>The proposed approach will transform the current system modeling approach by (1) developing a stochastic version of the deterministic differential equation models of ecosystems and environmental systems; (2) developing geospatial statistical techniques to fully exploit multifaceted observational data to improve model parameterization; (3) developing advanced statistical and machine learning techniques to further utilize observational data to improve model structure; and (4) applying the improved model to examine the societal and biogeochemical impacts of land use change.  Advantages of the proposed cyber-enabled terrestrial ecosystem model will include: (1) Efficiently quantifying regional net carbon exchanges and associated uncertainty and (2) Improving system model parameters and structure using advanced statistical and machine learning techniques and spatiotemporal data acquired over the U.S.   Project deliverables include: (1) An innovative, cyber-enabled carbon-weather system that can quantify net carbon exchanges and associated probabilistic information at high spatial and temporal resolution for the continental U.S. and (2) a suite of transformative advanced mathematical, statistical and system modeling techniques that could be applied to other complex modeling fields (e.g., hydrological modeling).  This project will significantly advance ecosystem sciences with computational thinking and will provide a unique opportunity to train a new generation of scientists in a highly interdisciplinary research environment."
"1027812","CDI-Type I: Modeling Quantum Tunnel Current to Statistically Sequence Biomolecules","CHE","CDI TYPE I","09/15/2010","09/11/2010","Manjeri Anantram","WA","University of Washington","Standard Grant","Evelyn Goldfield","08/31/2015","$576,001.00","Maya Gupta","anant@ee.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","MPS","7750","1366, 7237, 7721, 7722, 9216, 9263","$0.00","This paradigm shifting project advances the ability to read DNA and proteins and builds on designing nanotechnology systems using machine learning. The goal is to use computational thinking for changing the way biomolecules are designed by providing a synergistic combination of physical modeling, computational data, and adapted statistical learning methods that proactively account for the randomness. Currently all-electronic sequencing of DNA and proteins is a speculative dream that could revolutionize biochemical sciences and medicine. This project investigates theoretically and algorithmically the fundamentals of all-electronic sequencing using a computational framework. Currently, the field of machine learning treats the extraction of useful features from data as largely independent from the design of learning algorithms that act on the extracted features. This project aims at discovering whether significant performance gains can be obtained by designing learning algorithms that explicitly account for the physical variations and randomness in electronic sequencing. The underlying computational thinking can potentially lead to insights and methods leading to transformative advances on all-electronic sequencing of biomolecules. <br/><br/>The research direction of this project can impact biology, medicine, nanoelectronics, and the design of machine learning systems.  The training of high school, undergraduate and graduate students in quantum devices and statistical theory and methods in the design and understanding of nanoscale and machine learning systems are an integral part of the activities spanned by this project.<br/><br/>This award is part of the Cyber-Enabled Discovery and Innovation program, and the recipients are Professors M. P. Anantram and Maya Gupta of the University of Washington."
"1018433","IIS: RI:  Small: Nonlinear Dynamical System Theory for Machine Learning","IIS","ROBUST INTELLIGENCE","09/01/2010","08/18/2010","Max Welling","CA","University of California-Irvine","Standard Grant","Todd Leen","08/31/2014","$450,000.00","Anton Gorodetski","welling@ics.uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","CSE","7495","7923","$0.00","Learning complex statistical models from data is intractable for many models of interest. The PIs are studying a new approach to learning from data that formulates learning as a weakly chaotic nonlinear dynamical system. They show that this dynamical system, which they call ?herding?, combines learning and inference into one tractable forward mapping. They study the abstract mathematical properties of this nonlinear mapping, such as the properties of its attractor set and the topological and metric entropy of the mapping. They then relate these to properties of learning systems. <br/><br/>The PIs apply herding systems to a wide range of applications in machine learning. In supervised learning they show that herding suggests a natural extension to the ?voted perceptron algorithm? by including hidden variables. In unsupervised learning, herding is used to train Markov random field models from data. Herding is also extended to Hilbert spaces where it naturally leads to a deterministic sampling algorithm. Due to negative autocorrelations, this ?kernel herding? generates samples that have superior convergence properties than random sampling. They also apply herding to active learning problems. <br/><br/>Herding has the potential to radically transform the way we view learning systems. It connects learning to the vast field of nonlinear dynamical systems and chaos theory. As such the impact on machine learning is significant. Scientific results will be disseminated through journal publications and conference proceedings. The PIs also introduce a new course on learning, chaos and fractals to expose students to the intriguing connections between these fields."
"1007593","Conditional Modeling and Conditional Inference","DMS","STATISTICS","09/15/2010","09/09/2010","Stuart Geman","RI","Brown University","Standard Grant","Gabor Szekely","08/31/2013","$245,999.00","Matthew Harrison","Stuart_Geman@Brown.edu","BOX 1929","Providence","RI","029129002","4018632777","MPS","1269","9150","$0.00","In many applications, the complexity and dimensionality of the data preclude nonparametric inference, despite the availability of massive data sets. At the same time, it is usually true that too little is known about the detailed mechanisms generating the data to meaningfully specify parametric models. Conditional modeling and conditional inference are semi-parametric approaches to complex high-dimensional data in which attention is focused on manageable low-dimensional statistical modeling and estimation. Applications include efficient feature estimation and data classification (e.g. in computer vision), exact tests for broad and scientifically relevant hypotheses (e.g. in the statistical analysis of multi-electrode neuronal recordings), exploration of time scale in non-stationary processes (e.g. in the study of market dynamics), and construction of complex distributions through successive low-dimensional perturbations (e.g. in the study of probabilistic context-sensitive grammars). <br/> <br/>High-dimensional data are ubiquitous. Sources include molecular biology, finance, neurophysiological recordings, and the imagery and text of the Internet. Despite the availability of almost unlimited amounts of these data, their complexity and high dimensionality challenge existing statistical models and represent a bottleneck to successful applications. Oftentimes the complexity and dimensionality can be finessed through mathematical methods that select and focus on a collection of low-dimensional characteristics of the data. The approach avoids untenable or un-testable model assumptions without necessarily compromising the information content and power of the data. The research is at the interface between statistical theory and scientific application, with potential impact in technology (e.g. through computer vision) and, more broadly, society (e.g. through neuroscience and better financial modeling)."
"0963835","CIF: Medium:  Collaborative Research:  Advances in the Theory and Practice of Low-Rank Matrix Recovery and Modeling","CCF","Comm & Information Foundations, SIGNAL PROCESSING","05/01/2010","08/13/2013","Emmanuel Candes","CA","Stanford University","Continuing Grant","John Cozzens","04/30/2016","$490,324.00","","candes@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","7797, 7936","9218, HPCC","$0.00","This project concerns one of the fundamental challenges facing<br/>contemporary science and engineering today, namely, the efficient<br/>processing and analysis of massive amounts of high-dimensional data,<br/>such as images, videos, web pages, and bioinformatics data. In short,<br/>data now routinely lie in thousands or even billions of dimensions. On<br/>the one hand, massive data collection is motivated by 1) scientific<br/>discovery and 2) the need for better engineering systems. On the other<br/>hand, the difficult task now is to conduct meaningful inference in<br/>such high dimensions, and draw correct conclusions from limited<br/>amounts of sample data and with limited computational<br/>resources. Fortunately, scientific or engineering data often have very<br/>low intrinsic complexity and dimensionality.  This project addresses<br/>the opportunities offered by this common situation, establishes<br/>conditions under which reliable inference is actually possible, and<br/>develops computational tools for extracting key information from huge<br/>data sets.<br/><br/>This interdisciplinary project is expected to have three outcomes: 1)<br/>the development of innovative mathematics needed to study the recovery<br/>of data matrices from partial and corrupted information 2) the<br/>development of effective algorithms for recovering low-rank matrices<br/>and performing accurate dimensionality reduction with corrupted data<br/>and 3) the development of novel applications in which these techniques<br/>are expected to considerably advance the state-of-the-art.  With these<br/>new tools, scientists and engineers will be able to efficiently<br/>extract correct information from data, which was previously<br/>inaccessible or intractable by conventional techniques. This will<br/>enable the development of far better computer vision systems for face<br/>recognition, better compression schemes of video sequences, a better<br/>understanding of gene expression data, or better search engines for<br/>web documents and images."
"1022653","Collaborative Research: Automated Analysis of Constructed Response Concept Inventories to Reveal Student Thinking: Forging a National Network for Innovative Assessment Methods","DUE","S-STEM-Schlr Sci Tech Eng&Math, CCLI-Type 2 (Expansion), TUES-Type 2 Project","09/01/2010","08/11/2011","Mark Urban-Lurain","MI","Michigan State University","Standard Grant","Myles Boylan","08/31/2014","$458,575.00","John Merrill, Julie Libarkin, Tammy Long","urban@msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","EHR","1536, 7492, 7511","9178, SMET","$0.00","Numerous reports on the effectiveness of U.S. higher education in the Science, Technology, Engineering and Mathematics (STEM) disciplines call for increased emphasis on conceptual learning, rather than rote memorization. Suitable assessments (tests) of conceptual learning (often referred to as concept inventories or diagnostic question clusters), however, are few and are constrained by the ability to score the outcomes of the tests in a cost-effective manner. Multiple-choice assessments (selected responses) are more widespread in higher education, especially at medium to large institutions where class sizes are large, and where automated scoring provides the essential cost-effectiveness. Conversely, written response assessments (constructed responses), which are widely held to be superior at revealing actual student thinking, are quite rare in practice given the time and effort required for manual scoring. This project leverages the latest computerized tools and statistical techniques to make constructed response assessments more broadly available. Computer automation allows the use of these more insightful conceptual questions and tests with much larger numbers of students, thereby providing an enhanced understanding of students' conceptual learning. Project personnel work with developers of conceptual testing instruments to create constructed response versions of the tests coupled with the necessary computerized scoring tools with the eventual goal of providing computer-automated evaluation of conceptual thinking. The project is a collaboration among three major public universities."
"0964215","CIF: Medium: Collaborative Research:  Advances in the Theory and Practice of Low-Rank Matrix Recovery and Modeling","CCF","COMM & INFORMATION FOUNDATIONS, SIGNAL PROCESSING","05/01/2010","06/05/2013","Minh Do","IL","University of Illinois at Urbana-Champaign","Continuing grant","John Cozzens","12/31/2014","$368,875.00","","minhdo@illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7797, 7936","9218, HPCC","$0.00","This project concerns one of the fundamental challenges facing<br/>contemporary science and engineering today, namely, the efficient<br/>processing and analysis of massive amounts of high-dimensional data,<br/>such as images, videos, web pages, and bioinformatics data. In short,<br/>data now routinely lie in thousands or even billions of dimensions. On<br/>the one hand, massive data collection is motivated by 1) scientific<br/>discovery and 2) the need for better engineering systems. On the other<br/>hand, the difficult task now is to conduct meaningful inference in<br/>such high dimensions, and draw correct conclusions from limited<br/>amounts of sample data and with limited computational<br/>resources. Fortunately, scientific or engineering data often have very<br/>low intrinsic complexity and dimensionality.  This project addresses<br/>the opportunities offered by this common situation, establishes<br/>conditions under which reliable inference is actually possible, and<br/>develops computational tools for extracting key information from huge<br/>data sets.<br/><br/>This interdisciplinary project is expected to have three outcomes: 1)<br/>the development of innovative mathematics needed to study the recovery<br/>of data matrices from partial and corrupted information 2) the<br/>development of effective algorithms for recovering low-rank matrices<br/>and performing accurate dimensionality reduction with corrupted data<br/>and 3) the development of novel applications in which these techniques<br/>are expected to considerably advance the state-of-the-art.  With these<br/>new tools, scientists and engineers will be able to efficiently<br/>extract correct information from data, which was previously<br/>inaccessible or intractable by conventional techniques. This will<br/>enable the development of far better computer vision systems for face<br/>recognition, better compression schemes of video sequences, a better<br/>understanding of gene expression data, or better search engines for<br/>web documents and images."
"1028076","CDI-Type II: Computational Tools for Behavioral Analysis, Diagnosis, and Intervention of at Risk Children","SMA","CROSS-DIRECTORATE ACTIV PROGR, SOCIOLOGY, INFORMATION TECHNOLOGY RESEARC, Computer Systems Research (CSR, RSCH EXPER FOR UNDERGRAD SITES","09/15/2010","07/29/2015","Nikolaos Papanikolopoulos","MN","University of Minnesota-Twin Cities","Standard Grant","Fahmida N. Chowdhury","08/31/2016","$1,578,897.00","Guillermo Sapiro, Kelvin Lim, Arindam Banerjee","npapas@cs.umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","SBE","1397, 1331, 1640, 7354, 1139","7721, 7752, 9178, 9251, 9250","$0.00","This project will develop algorithms to assist with the early diagnosis of children who are at risk of developing behavioral disorders. Previous research has indicated that two critical areas of behavioral investigation for use in identifying at-risk children have been abnormalities in motor activities and emotional range displays, especially of the face. Motor abnormalities are based on the observation that motor control involves the circuits of the brain associated with dopamine; these are also implicated in behavioral disorders. Many different disorders share the observation of disruption in the emotional range regulation, so facial expressions are included in the study.<br/><br/>To date, assessments of motor and emotional range have been done by the experts who view and rate videos of an individual. However, these expert, subjective ratings limit the analysis of behavioral conditions to only a narrow range of behaviors, work only for small populations of individual subjects, and are both costly and dependent on the observer's particular expertise. In order to enable wider population screening, automation is required. Innovative ways of capturing and quantifying the expertise of experts will be accompanied by metrics for assessing the evolution of the behavior. In addition, new computational tools will support evaluation of the effectiveness of interventions.<br/><br/>The broader impacts of the proposed work will involve improved mental health levels across the populations by providing a systematic approach for enhancing early detection, prevention, or mitigation of behavioral disorders and likely reduce the long-term costs of missed or late diagnosis. The research results will be blended with the educational process through inclusion of project themes in the curricula at the Institute of Technology, the Medical School, and the College of Education and Human Development at the University of Minnesota and the creation of a program with annual workshops, tutorials, web pages and a wiki on knowledge discovery and behavioral analysis. The team will develop an interactive exhibit for children at the Bakken Museum, and create of new instructional material for student teachers at the Institute of Child Development and similar institutions. Development of a central web repository will insure that the algorithms and the data will be readily available for appropriate research."
"1017156","NeTS: Small: Distributed Solutions to Smart Camera Networks","CNS","Special Projects - CNS, Networking Technology and Syst","07/01/2010","06/07/2011","Hairong Qi","TN","University of Tennessee Knoxville","Standard Grant","Thyagarajan Nandagopal","06/30/2014","$411,000.00","Qing Cao","hqi@utk.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","CSE","1714, 7363","7363, 7923, 9150, 9251","$0.00","Smart camera networks (SCNs) merge computer vision, distributed<br/>processing, and sensor network disciplines to solve problems in<br/>multi-camera applications by providing valuable information through<br/>distributed sensing and collaborative in-network processing.<br/>Collaboration in sensor networks is necessary not only to compensate<br/>for the processing, sensing, energy, and bandwidth limitations of each<br/>sensor node but also to improve the accuracy and robustness of the<br/>network. Collaborative processing in SCNs is more challenging than in<br/>conventional scalar sensor networks (SSNs) because of three unique<br/>features of cameras, including the extremely higher data rate, the<br/>directional sensing characteristics with limited field of view (FOV),<br/>and the existence of visual occlusion. An integrated research is<br/>carried out to tackle the unique challenges presented by SCNs where<br/>collaboration is the key. Three aspects of collaborative processing<br/>are investigated, 1) coverage estimation in the presence of visual<br/>occlusions to provide adequate redundancy in sensing coverage, and to<br/>enable collaboration where the statistics of visual coverage blends<br/>the statistics of camera nodes and targets, 2) clustering to<br/>schedule an efficient sleep-wakeup pattern among neighbor nodes formed<br/>by image comparison-based semantic neighbor selection algorithm for<br/>more efficient collaboration, and 3) distributed optimization, for<br/>in-network data processing that concerns how to effectively obtain<br/>robust and accurate integration results from multiple distributed<br/>sensors for challenging vision tasks like target detection,<br/>localization, and tracking in crowds."
"0962119","The Perceptual Identification and Representation of Image Contours","BCS","PERCEPTION, ACTION & COGNITION","09/01/2010","08/24/2010","James Todd","OH","Ohio State University","Standard Grant","Betty H. Tuller","08/31/2015","$436,101.00","","todd.44@osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","SBE","7252","","$0.00","It has long been recognized that a convincing pictorial representation of an object can sometimes be achieved by drawing just a few salient contours in an image. This phenomenon is really quite remarkable, given that a line drawing effectively strips away almost all of the variations in color and shading that are ordinarily available in natural scenes. Somehow the artists who create such drawings are able to capture the essential information for perceptual recognition with just a few simple strokes. Although a well structured line drawing is easily interpreted by human observers, the ability to create these drawings can require considerable artistic skill. Indeed, despite almost a half century of research in the field of computer vision, there are no existing algorithms that can duplicate the performance of a competent human artist. In this project, Dr. James Todd and his students at the Ohio State University will investigate how human observers perceptually identify different types of image contours, such as shadows, corners or occlusion. The group will also examine which contours in an image are perceptually most important for creating pictorial representations of objects. The stimuli in these studies will include drawings by artists with varying amounts of training, who will be asked to produce line drawings of objects with known 3D structures. The drawings will be ranked by human observers to assess their relative perceptual effectiveness. The contours in the drawings will also be compared with different aspects of the depicted surface geometry in order to determine which specific aspects of a surface are most important for its pictorial depiction. <br/><br/>A better understanding of how human observers perceptually determine the 3D shapes of surfaces from 2D image data has many possible applications, including the design of more robust and effective algorithms in machine vision, improved techniques for 3D visualization in computer graphics and design, and the potential development of more functional prosthetic devices for the blind. This work may also have a significant impact on how students are taught to draw in art or design courses."
"0958442","Collaborative Research:  II-EN:  Development of Publicly Available, Easily Searchable, Linguistically Analyzed, Video Corpora for Sign Language and Gesture Research","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","04/01/2010","03/31/2010","Carol Neidle","MA","Trustees of Boston University","Standard Grant","Ephraim Glinert","03/31/2012","$70,000.00","Stan Sclaroff","carol@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","7359","9102, 9218, HPCC","$0.00","American Sign Language (ASL) is used by as many as two million people in the United States, with additional users elsewhere in North America.  The purpose of this ""planning grant"" is to enable the PI and her multi-institutional team to explore the case for a possible future NSF investment in an annotated, publicly available, and easily searchable corpus consisting of terabytes of ASL video data (deriving in part from prior work by the PI and her colleagues), including diverse types of content such as dialogues, narratives, elicited sentences illustrating specific grammatical constructions, and isolated signs.  The PI contends such a resource would constitute an important infrastructure that would be exploited by a broad research community to advance the fields of linguistics (the structure of ASL), computer vision (machine recognition of gestures), indexing of visual information (through the expansion of mark up vocabularies), and education.  The PI notes that the potential value of the existing corpora remains largely untapped, notwithstanding their extensive and productive use by her team and others, due to hardware and software limitations that make it cumbersome to search, identify, and share data of interest.  <br/><br/>Broader Impacts:  The new resource would be easily accessible by the research community and the broader public, via a user-friendly Web-based interface.  Availability of the resource online would allow ASL teachers and users, and others, to access the data directly.  Users would be able to look up an unknown sign by submitting a video example of that sign.  Students of ASL would be able to retrieve video showing examples of a specific sign used in actual sentences, or examples of a grammatical construction.  ASL instructors and teachers of the Deaf would have easy access to video examples of lexical items and grammatical constructions as used by a variety of native signers, for use in language instruction and evaluation."
"1017017","RI: Small: The Shape of Visual Motion","IIS","ROBUST INTELLIGENCE","08/15/2010","05/31/2011","Carlo Tomasi","NC","Duke University","Continuing grant","Jie Yang","07/31/2015","$457,499.00","","tomasi@cs.duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","7495","7923, 9251","$0.00","This project studies methods for describing motion in video. All visible points in the world are tagged by their identity, and trajectories of their projections on the image plane are tracked through space and time.  This computation is performed globally, both in space and time, and motion discontinuities are explicitly delineated in the output. In contrast with previous techniques, which estimate motion primarily from the bottom up, starting with two frames at a time, the box of data from a video camera is carved up into tube-like regions whose shapes capture information about the motion and deformation of the objects visible in the scene. Novel methods include the projection of all visual motion onto a sparse basis of point trajectories through low-rank matrix data imputation; the use of L1 regularization in a function space that preserves boundaries; the generalization of robust estimation methods from variational calculus and quadratic programming for the efficient computation of tubes and occlusions in the multi-frame case; and several domain-specific techniques for initializing general but local optimization methods close to the global solution. The resulting descriptors enable video retrieval, medical diagnosis of heart rhythm anomalies, assessment of performance in sports, sign language recognition, traffic monitoring, surveillance, and more. The project also forms the basis for a new class on experimental methods for computer vision, the materials of which are made available online."
"1049694","III: EAGER: Learning Evaluation Metrics for Information Retrieval","IIS","Info Integration & Informatics","09/01/2010","03/01/2012","Hongyuan Zha","GA","Georgia Tech Research Corporation","Standard Grant","Maria Zemankova","08/31/2013","$206,000.00","","zha@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7364","7364, 7916, 9251","$0.00","Information retrieval (IR) performance is typically measured in terms of relevancy: every document is known to be either relevant or non-relevant to a particular query. Furthermore, more relevant documents are expected to receive a higher rank than lower less relevant documents. However, determination of relevance and rank by users is not practical. Therefore, it is crucial to develop evaluation metrics and ranking functions that can be derived automatically from judgment data and user behavior data, rather than ad-hoc heuristics. This exploratory project investigates machine learning approaches for constructing evaluation metrics for Web search and information retrieval that consider along important directions other than relevance such as diversity, balance and coverage. <br/><br/>The approach is based on fundamentally extending the popular evaluation metric Discounted Cumulated Gains (DCG). Research focuses on developing optimization methods for learning DCG that can incorporate the degree of difference in pair-wise comparison of ranking lists. Machine learning methods that can learn DCG for the more realistic scenarios where the relevance grades are not readily available are explored, and nonlinear utility functions as evaluation metrics that can accurately capture the quality of search result sets in terms of relevance, diversity, coverage, balance and novelty are investigated.<br/><br/>The project has a number of broad impacts. Research results are expected to provide foundations for further research in evaluation metrics. Active collaborations with industry leaders in Web search will enable the resulting methods to have real impacts on search engine as well as large IR system performance improvements. Improving the quality of search results will have significant impacts on satisfying people's information needs as well as their quality of life in general. The set of research topics lies at the interface between information retrieval and machine learning applications and it provides an ideal setting for training undergraduate and graduate students in the emerging interdisciplinary field of Web of science and engineering research. The project Web site (http://www.cc.gatech.edu/~zha/metrics.html) will be used for results dissemination."
"1018470","HCC: Small: Sketching Architectural Designs in Context","IIS","HCC-Human-Centered Computing","08/15/2010","03/21/2011","Julie Dorsey","CT","Yale University","Standard Grant","Ephraim Glinert","07/31/2014","$516,000.00","","dorsey@cs.yale.edu","Office of Sponsored Projects","New Haven","CT","065208327","2037854689","CSE","7367","7367, 7923, 9251","$0.00","In this research the fusion of data from different sources will be used to describe an existing site. The fused data will be incorporated into a lightweight interactive design system that facilitates conceptual design. A method for making the transition from the output of the conceptual design phase to a full 3D model suitable for a CAD system used for construction documentation will also be developed.  The work will introduce new methods for gathering and fusing data at an appropriate level of detail for design, rather than following a more traditional approach of creating detailed models and simplifying them for use in an interactive system. The proposed work can have a direct impact on the creative design process by offering a novel approach for creating and editing 3D form. This work is interdisciplinary and brings together research from computer graphics and architecture, computer vision, cognitive science, psychology and design and engineering."
"1049080","EAGER:   VizWiz - Enabling Blind People to Answer Visual Questions On-the-Go with Remote Automatic and Human-Powered Services","IIS","Information Technology Researc, HCC-Human-Centered Computing","09/01/2010","07/19/2010","Jeffrey Bigham","NY","University of Rochester","Standard Grant","Ephraim Glinert","08/31/2011","$49,999.00","","jbigham@cmu.edu","518 HYLAN, RC BOX 270140","Rochester","NY","146270140","5852754031","CSE","1640, 7367","7916","$0.00","The lack of access to visual information like text labels, icons, and colors frustrates blind people and severely decreases their independence.  Current access technology uses fully-automatic approaches to address some problems in this space but is error-prone, limited in scope, and expensive.  Blind people who can afford to do so must carry multiple special-purpose portable devices with different audio and tactile interfaces in order to access critical data about their environment such as product information from bar codes and location information via GPS.  These devices would likely be used more often if they had greater functionality and failed less often.  Providing a fallback by making it easy to consult a human assistant could be part of the solution.  The PI's talking VizWiz application for mobile phones is one such prototype that connects blind people to remote human workers who answer general questions about the users' visual environments.  VizWiz currently allows blind users to take a picture, speak a question, and receive answers back quickly.  Preliminary findings have demonstrated the potential advantages of including humans in the loop to help overcome visual problems that are still too difficult to be solved by automatic approaches alone, but questions remain about the efficacy, privacy, speed, and cost of these approaches.  In this project the PI will seek answers to some of these questions, by conducting a longitudinal study of VizWiz with blind people to better understand how the application might fit into their everyday lives.  He will endeavor to determine how users' existing social networks might be employed as a source of answers using applications for Facebook and Twitter.  And he will seek to define new services with appropriate interfaces that let users mediate between automatic and human-powered remote sources for answers.  A mobile accessibility solution using both automated and human Web services represents a significant advance in accessibility but presents challenging user interface questions.  Understanding issues such as those enumerated above is necessary for human-powered services to be accepted as part of assistive technology.  <br/><br/>Broader Impacts:  This exploratory research represents a new paradigm in human-computer interaction in which humans are both clients and providers.  VizWiz has the potential to improve the independence of blind people, and may be both less expensive and more sustainable than current accessibility solutions.  This project will improve our understanding of the types of tools that would be useful for blind people regardless of what is possible today with automatic computer vision, and will help us better understand how to recruit people to answer questions while respecting the asker's values.  The research will involve blind people throughout; the resulting interfaces and functionality will be evaluated by blind people in the world going about their everyday lives.   The interfaces, applications, and framework created and improved as part of this project will be released as open source so other researchers may build on the PI's results.  Project outcomes will be broadly applicable to other problems where automated solutions may occasionally need human intervention."
"0958247","Collaborative Research:  II-EN:  Development of Publicly Available, Easily Searchable, Linguistically Analyzed, video Corpora for Sign Language and Gesture Research","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","04/01/2010","03/31/2010","Dimitris Metaxas","NJ","Rutgers University New Brunswick","Standard Grant","Ephraim Glinert","03/31/2011","$20,000.00","","dnm@cs.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7359","9218, HPCC","$0.00","American Sign Language (ASL) is used by as many as two million people in the United States, with additional users elsewhere in North America.  The purpose of this ""planning grant"" is to enable the PI and her multi-institutional team to explore the case for a possible future NSF investment in an annotated, publicly available, and easily searchable corpus consisting of terabytes of ASL video data (deriving in part from prior work by the PI and her colleagues), including diverse types of content such as dialogues, narratives, elicited sentences illustrating specific grammatical constructions, and isolated signs.  The PI contends such a resource would constitute an important infrastructure that would be exploited by a broad research community to advance the fields of linguistics (the structure of ASL), computer vision (machine recognition of gestures), indexing of visual information (through the expansion of mark up vocabularies), and education.  The PI notes that the potential value of the existing corpora remains largely untapped, notwithstanding their extensive and productive use by her team and others, due to hardware and software limitations that make it cumbersome to search, identify, and share data of interest.  <br/><br/>Broader Impacts:  The new resource would be easily accessible by the research community and the broader public, via a user-friendly Web-based interface.  Availability of the resource online would allow ASL teachers and users, and others, to access the data directly.  Users would be able to look up an unknown sign by submitting a video example of that sign.  Students of ASL would be able to retrieve video showing examples of a specific sign used in actual sentences, or examples of a grammatical construction.  ASL instructors and teachers of the Deaf would have easy access to video examples of lexical items and grammatical constructions as used by a variety of native signers, for use in language instruction and evaluation."
"1036462","Frontiers of Activity Recognition","IIS","ROBUST INTELLIGENCE","05/15/2010","05/19/2010","Stefano Soatto","CA","University of California-Los Angeles","Standard Grant","Jie Yang","04/30/2011","$20,000.00","","soatto@ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","7495","7495","$0.00","This award is made in support of a collaborative project called ""Frontiers in Activity Recognition"" whereby a group of experts from different fields of computer science, engineering, mathematics and statistics convene in a workshop to be held in the vicinity of UCLA.  <br/>One component of the workshop consists in interactive break-out sessions where different approaches to activity representation (descriptors) and recognition will be analyzed. A second component consists in a competition, announced to the broad public ahead of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), whereby an extensive dataset provided by a third party will be released, with benchmarks, and contestants will be invited to submit their best results in the detection of a number of action categories. The proposers of high-ranking approaches will be invited to the workshop to present their results and discuss it in the context of the analysis of the state of the art to be performed as part of the field assessment. The workshop can have broad impact to many applications ranging from security (surveillance, monitoring) to environmental science (habitat monitoring, global warming), to industrial operations (factory floor optimization), to multi-media and information retrieval (content-based video meta-data extraction), to entertainment (input devices for games), and to transportation (driver assistance)."
"0943577","EMSW21-RTG: Statistics and Machine Learning for Scientific Inference","DMS","STATISTICS","06/01/2010","05/12/2010","Robert Kass","PA","Carnegie-Mellon University","Standard Grant","Gabor Szekely","05/31/2012","$349,996.00","William Eddy, Kathryn Roeder, Larry Wasserman, Christopher Genovese","kass@stat.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","MPS","1269","0000, 7301","$0.00","Statistics curricula have required excessive up-front investment in statistical theory, which many quantitatively-capable students in ``big science'' fields initially perceive to be unnecessary.  A training program at Carnegie Mellon will expose students to cross-disciplinary research early, showing them the scientific importance of ideas from statistics and machine learning, and the intellectual depth of the subject. Graduate students will receive instruction and mentored feedback on cross-disciplinary interaction, communication skills, and teaching. Postdoctoral fellows will become productive researchers who understand the diverse roles and responsibilities they will face as faculty or members of a research laboratory.<br/><br/>The statistical needs of the scientific establishment are huge, and growing rapidly, making the current rate of workforce production dangerously inadequate.  The Department of Statistics at Carnegie Mellon University will train undergraduates, graduate students, and postdoctoral fellows in an integrated program that emphasizes the application of statistical and machine learning methods in scientific research. The program will build on existing connections with computational neuroscience, computational biology, and astrophysics.Carnegie Mellon will recruit students from a broad spectrum of quantitative disciplines, with emphasis on computer science.  Carnegie Mellon already has an unusually large undergraduate statistics program. New efforts will strengthen the training of these students, and attract additional highly capable students to be part of the pipeline entering the mathematical sciences."
"1031917","Student Poster Program and Travel Scholarships for International Conference on Machine Learning (ICML) 2010; Haifa, Israel","IIS","Robust Intelligence","07/01/2010","07/06/2010","Alan Fern","OR","Oregon State University","Standard Grant","Sven G. Koenig","06/30/2011","$30,000.00","","afern@eecs.oregonstate.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","CSE","7495","7495","$0.00","The project supports graduate student participation in the 27th International Conference on Machine Learning (ICML 2010). Specifically, the project supports travel to the conference for those who might not otherwise be able to attend for financial reasons and organizes a student poster-presentation program that will facilitate one-on-one discussions and other mentoring with the world's leading researchers in machine learning. Students are exposed to state-of-the-art work by other researchers and have the opportunity to attend tutorials on material that is not taught at their home institutions. Participating students receive feedback from senior researchers beyond their institutional and national boundaries. Furthermore, participation in the poster session and conference helps to integrate these students into the research community and represents a natural integration of research and education."
"0958286","Collaborative:  II-EN:  Development of Publicly Available,  Easily Searchable, Linguistically Analyzed, Video Corpora for Sign Language  and Gesture Research","CNS","CCRI-CISE Cmnty Rsrch Infrstrc","04/01/2010","03/31/2010","Vassilis Athitsos","TX","University of Texas at Arlington","Standard Grant","Ephraim Glinert","03/31/2011","$10,000.00","","athitsos@uta.edu","701 S Nedderman Dr, Box 19145","Arlington","TX","760190145","8172722105","CSE","7359","9218, HPCC","$0.00","American Sign Language (ASL) is used by as many as two million people in the United States, with additional users elsewhere in North America.  The purpose of this ""planning grant"" is to enable the PI and her multi-institutional team to explore the case for a possible future NSF investment in an annotated, publicly available, and easily searchable corpus consisting of terabytes of ASL video data (deriving in part from prior work by the PI and her colleagues), including diverse types of content such as dialogues, narratives, elicited sentences illustrating specific grammatical constructions, and isolated signs.  The PI contends such a resource would constitute an important infrastructure that would be exploited by a broad research community to advance the fields of linguistics (the structure of ASL), computer vision (machine recognition of gestures), indexing of visual information (through the expansion of mark up vocabularies), and education.  The PI notes that the potential value of the existing corpora remains largely untapped, notwithstanding their extensive and productive use by her team and others, due to hardware and software limitations that make it cumbersome to search, identify, and share data of interest.  <br/><br/>Broader Impacts:  The new resource would be easily accessible by the research community and the broader public, via a user-friendly Web-based interface.  Availability of the resource online would allow ASL teachers and users, and others, to access the data directly.  Users would be able to look up an unknown sign by submitting a video example of that sign.  Students of ASL would be able to retrieve video showing examples of a specific sign used in actual sentences, or examples of a grammatical construction.  ASL instructors and teachers of the Deaf would have easy access to video examples of lexical items and grammatical constructions as used by a variety of native signers, for use in language instruction and evaluation."
"1007808","Modeling Higher-Order Dependence With Cumulant Tensors","DMS","STATISTICS","09/01/2010","08/16/2010","Jason Morton","PA","Pennsylvania State Univ University Park","Standard Grant","Gabor Szekely","08/31/2014","$125,000.00","","morton@math.psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","MPS","1269","","$0.00","The second cumulant tensor  of a multivariate distribution is its<br/>covariance matrix, which provides a partial description of its<br/>dependence structure (complete in the Gaussian case). Innumerable<br/>successful statistical methods are based on analyzing the covariance<br/>matrix, e.g. imposing rank restrictions as in principal component<br/>analysis or zeros in its inverse as in Gaussian graphical models.<br/>Moreover, the covariance matrix plays a critical role in optimization<br/>in finance and other areas involving optimization of risky payoffs,<br/>since it is the bilinear form yielding  the variance of a linear<br/>combination of variables.  For multivariate, non-Gaussian data, the<br/>covariance matrix is an incomplete description of the dependence<br/>structure.  Cumulant tensors are the multivariate generalization of<br/>univariate skewness and kurtosis and the higher-order generalization<br/>of covariance matrices, and allow a more complete description of<br/>dependence.  The research investigates a number of problems in theory,<br/>estimation, algorithms, and applications around modeling higher-order<br/>non-Gaussian dependence with cumulant tensors.<br/><br/>Data arising from modern applications like computer vision, finance,<br/>and computational biology are rarely well described by a normal<br/>distribution, though  analysis often proceeds as if they were.  For<br/>example, one cause of the financial crisis and the damage it did to<br/>many investors was an over-reliance on the variance-based risk<br/>measures appropriate primarily for normal distributions. This can<br/>allow risk to be in a sense hidden in the higher-order structure,<br/>where it can be ignored or even made worse by application of<br/>traditional risk metrics.  Cumulant tensors provide a promising avenue<br/>for modeling higher-order dependence.  Success in developing these<br/>models will have broad impacts in the analysis of real-world data with<br/>complex dependence, particularly in modeling and managing financial<br/>risk and in dimension reduction, and could help improve the robustness<br/>of parts of the financial system."
"0954083","CAREER:  Discriminative Spatiotemporal Models for Recognizing Humans, Objects, and their Interactions","IIS","ROBUST INTELLIGENCE","06/01/2010","05/19/2014","Deva Ramanan","CA","University of California-Irvine","Continuing grant","Jie Yang","10/31/2015","$444,511.00","","deva@cs.cmu.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","CSE","7495","1045, 1187","$0.00","One of the goals of computer vision is to build a system that can see people and recognize their activities. Human actions are rarely performed in isolation -- the surrounding environment, nearby objects, and nearby humans affect the nature of the performed activity.<br/>Examples include actions such as ""eating"" and ""shaking hands."" The research goal of this project is to approach human performance in understanding videos of activities defined by human-object and human-human interactions.<br/><br/>This project makes use of structured, contextual representations to make predictions given spatiotemporal data. It does so by extending recent successful work on object recognition to the space-time domain, introducing extensions for spatiotemporal grouping and contextual modeling. Video enables the extraction of additional dynamic cues absent in static images, but this poses additional computational burdens that are addressed through algorithmic innovations for approximate parsing and large-scale discriminative learning.<br/><br/>To place activity recognition on firm quantitative ground, the proposed models are evaluated using concrete metrics based on activities of daily living (ADL) and human proxemic models from the medical and anthropological communities. Examples include systems for automated monitoring of stroke patients interacting with everyday objects and automated analysis of crisis response team interactions during emergency drills. This project produces non-scripted, real-world, labeled action recognition datasets, of benefit to the research community as a whole."
"1017828","DC:Small: Collaborative Research: Data Intensive Computing for General Relational Data Learning","CCF","DATA-INTENSIVE COMPUTING","09/01/2010","08/10/2010","Zhongfei Zhang","NY","SUNY at Binghamton","Standard Grant","Jie Yang","08/31/2015","$250,000.00","","zhongfei@cs.binghamton.edu","4400 VESTAL PKWY E","BINGHAMTON","NY","139026000","6077776136","CSE","7793","7793, 9218, HPCC","$0.00","It is well-observed that the whole world is full of data that are highly related and of diverse data object types such as people, organizations, and events. In many applications, it is intended to discover the hidden structures through such relationships involving different types of data objects in the world, in addition to ""clusters"" of the same type of data objects. On the other hand, relational data learning typically involves a large collection of data objects and thus algorithms for relational data learning are computation-intensive as well as data intensive. This calls for massively parallel solutions in order to make the algorithms scalable to large collections of data. This project addresses a three year integrated research and education program focusing on engaging in-depth research in developing novel parallel frameworks for a wide spectrum of state-of-the-art solutions to a series of fundamental problems in relational data learning. This research promotes the revolutionized understanding of relational data learning in the context of distributed computation environment. The project addresses fundamental problems in the literature of relational data learning as well as the expected breakthrough in the interdisciplinary and multidisciplinary research communities including parallel computation and scheduling, data mining and machine learning, and pattern analysis. The technologies generated from the research can be immediately deployed in important applications such as social network analysis, biological information discovery, financial and economic development analysis and prediction, natural disaster prediction, as well as military intelligence analysis.<br/><br/>Project url:<br/><br/>http://www.fortune.binghamton.edu/nsf-iis-1017828.htm"
"1017672","RI: Small: Towards Portable Navigational Devices for the Visually Impaired","IIS","ROBUST INTELLIGENCE, EPSCoR Co-Funding","08/01/2010","08/08/2010","Cang Ye","AR","University of Arkansas Little Rock","Standard Grant","gregory chirikjian","07/31/2015","$320,389.00","","cye@vcu.edu","2801 South University","Little Rock","AR","722041000","5015698474","CSE","7495, 9150","7923, 9150","$0.00","The objective of this project is to devise computer vision methods that enable a Portable Blind Navigational Device (PBND) to guide a visually impaired person in unstructured environments. The main research question of this project is to answer if the approach of employing a single perception sensor can solve blind navigation problem, including localization of the PBND and object recognition. A distinctive feature of this work is that it addresses blind navigation problem by simultaneously processing the visual and range information of a 3D imaging sensor. <br/><br/>The project consists of four related research endeavors. First, it investigates techniques for accurate and precise pose estimation of the PBND in a GPS-denied environment.  Second, it develops an effective 3D data segmentation method to allow scene recognition for wayfinding. Third, it applies the pose estimation method to register 3D range data and devises methods to reduce registration error. Four, it addresses real-time implementation of the methods in the PBND with limited computing power.    <br/><br/>The research will result in new algorithms that can improve the lives of the visually impaired in the near term. They will also enable the autonomy of small robots that have wider applications in military situational awareness, firefighting, and search and rescue. The discoveries will revolutionize small robot autonomy and impact the robotics research community as a whole. Broader impacts also include training of undergraduate and graduate students, and educating the public on robotics through workshops and robot exhibits in science museums and technology showcases."
"1025120","Manifold Alignment of High-Dimensional Data Sets","CCF","MSPA-INTERDISCIPLINARY, FOUNDATIONS VISUAL ANALYTICS","09/01/2010","07/22/2010","Sridhar Mahadevan","MA","University of Massachusetts Amherst","Standard Grant","Tie Luo","08/31/2014","$499,909.00","Rui Wang","mahadeva@cs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","7454, 7703","7923","$0.00","As the availability and size of digital information repositories continues to burgeon, the problem of extracting deep semantic structure from high-dimensional data becomes more critical. This project addresses the fundamental problem of transfer learning, in particular it investigates methods for aligning multiple heterogeneous data sets to find correspondences and extract shared latent semantic structure. Domains of applicability include automatic machine translation, bioinformatics, cross-lingual information retrieval, perceptual learning, robotic control, and sensor-based activity modeling.  The proposed research will investigate a geometric framework for transfer learning based on finding correspondences between data by aligning their projections onto lower dimensional manifolds. The proposed research will investigate a broad spectrum of approaches to manifold alignment, including one-step vs. two-step alignment, instance-based vs. feature-based alignment, semi-supervised vs. unsupervised alignment, and finally one-level vs. multi-scale alignment. Visualization tools that use alignment information will be developed to facilitate interactive learning from data analysis. To aid the processing of large data sets, the parallel computational power of modern graphics processing units (GPUs) will be exploited.<br/><br/>Given the rapidly increasing availability of digital data sets from a diverse variety of domains, the scientific question of extracting knowledge from massive unstructured information repositories is becoming ever more critical. The proposed research combines the study of machine learning algorithms for discovering latent correspondences between seemingly disparate data sets, and the development of visualization tools to aid human interpretation of high-dimensional data.  Empirical studies on a variety of real-world applications will be carried out, ranging from bioinformatics, Internet web archives, multilingual text, and sequential time-series data sets. The broader impacts of the proposed research include algorithmic advances in the analysis and visualization of high-dimensional data, and empirical studies on a variety of real-world applications. The data sets and software developed in this research will be disseminated through the web. The research will be communicated through a variety of conferences, workshops and seminars in several disciplines ranging from computer science, engineering, mathematics, and statistics. The PIs will make significant efforts to recruit underrepresented groups, including women and other minorities, in this research.  New course material on advanced data analysis and visualization will be developed based on the proposed research."
"1018651","RI: Small: Spectral Methods for Learning Time Series and Graphical Models","IIS","ROBUST INTELLIGENCE","08/01/2010","03/15/2013","Sham Kakade","PA","University of Pennsylvania","Continuing grant","Todd Leen","07/31/2013","$224,998.00","","sham@cs.washington.edu","Research Services","Philadelphia","PA","191046205","2158987293","CSE","7495","7923, 9150","$0.00","The investigators study a new class of statistical methods for learning time series and graphical models. Their approach is based on spectral analysis and matrix decomposition methods that have enjoyed tremendous success in applications, but their use in graphical models has drawn less attention. The goal of this investigation is to extend the enormous previous successes of matrix decomposition methods to the realm of more complicated time series and certain graphical models, which will lead to new statistical machine learning algorithms with important practical applications.<br/><br/>In the information age, an important measure of computer intelligence is the ability to analyze huge amount of data that become available electronically, and make critical decisions under uncertain environment. Statistical machine learning is the main technique for analyzing electronic data, and graphical models are mathematical tools for understanding these complex data both by computer systems and by human operators in order to facilitate decision making. However, traditional algorithms for learning graphical models have limitations that restrict capabilities of modern computing systems. The current research attempts a new class of mathematical algorithms that can be used to design more effective graphical models, which in turn allows modern computers to analyze data more accurately and achieve higher level of intelligence."
"1001157","Mori Dream Spaces and Rational Curves","DMS","ALGEBRA,NUMBER THEORY,AND COM","08/15/2010","08/05/2010","Ana-Maria Castravet","AZ","University of Arizona","Standard Grant","Tie Luo","10/31/2011","$105,903.00","","a.castravet@neu.edu","888 N Euclid Ave","Tucson","AZ","857194824","5206266000","MPS","1264","","$0.00","The project aims at understanding different aspects of the geometry of algebraic varieties and their moduli. There are two main topics:<br/>(1) Effective and ample cones of moduli spaces of stable curves. This sequence of projects is focused on the Grothendieck-Knudsen moduli space of stable rational curves. The goal is to investigate the Mori Dream Space structure of the moduli space; in particular, give modular interpretations for its birational contractions and give a presentation for its total coordinate ring. A new point of view is the interpretation of the moduli space as a Brill-Noether locus of a reducible curve associated to new combinatorial structures called hypertrees. (2) A study of higher Fano varieties using minimal dominating families of rational curves. The main focus is on the classification of 2-Fano varieties and generalizations of Tsen's theorem.<br/><br/>The broader context of the project is the area of algebraic geometry, one of the oldest and currently one of the most active branches of mathematics, with widespread applications throughout mathematics and reaching into physics and engineering. Algebraic geometry is the study of algebraic varieties, which are geometric objects defined by the zeros of systems of polynomial equations. The variation of algebraic varieties is captured by the so-called moduli spaces, which are themselves varieties with a very rich structure. The project aims at revealing the intriguing structure of various moduli spaces of curves (which are fundamental in many areas of mathematics and in theoretical physics). The project impacts arithmetic and computational algebraic geometry, areas which have increasing applications in coding theory, robotics, computer vision, phylogenetics, statistics, etc."
"1016061","RI: Small: Spectral Methods for Learning Time Series and Graphical Models","IIS","ROBUST INTELLIGENCE","08/01/2010","06/16/2011","Tong Zhang","NJ","Rutgers University New Brunswick","Continuing grant","Todd Leen","07/31/2014","$224,646.00","","tzhang@stat.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","7495","7923","$0.00","The investigators study a new class of statistical methods for learning time series and graphical models. Their approach is based on spectral analysis and matrix decomposition methods that have enjoyed tremendous success in applications, but their use in graphical models has drawn less attention. The goal of this investigation is to extend the enormous previous successes of matrix decomposition methods to the realm of more complicated time series and certain graphical models, which will lead to new statistical machine learning algorithms with important practical applications. <br/><br/>In the information age, an important measure of computer intelligence is the ability to analyze huge amount of data that become available electronically, and make critical decisions under uncertain environment. Statistical machine learning is the main technique for analyzing electronic data, and graphical models are mathematical tools for understanding these complex data both by computer systems and by human operators in order to facilitate decision making. However, traditional algorithms for learning graphical models have limitations that restrict capabilities of modern computing systems. The current research attempts a new class of mathematical algorithms that can be used to design more effective graphical models, which in turn allows modern computers to analyze data more accurately and achieve higher level of intelligence."
"1010818","Crowd ID: Collaborative Tools Connecting People to Biodiversity through Social Networks and Machine Learning (Preproposal #0947206)","DRL","AISL","09/15/2010","08/11/2012","Miyoko Chu","NY","Cornell University","Continuing grant","Arlene de Strulle","10/31/2014","$1,895,101.00","Richard Bonney, Steven Kelling, Mirek Riedewald","mcc37@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","EHR","7259","9177, SMET","$0.00","The Cornell Lab of Ornithology (CLO) is creating a new type of interactive, question-driven, online bird-identification tool called ""Merlin,"" along with associated games, social networking tools, and other media. Unlike existing bird-identification guides, which are based on traditional taxonomic keys written by scientists, Merlin uses machine learning algorithms and crowd-sourced data (information provided by thousands of people) to identify birds and improve Merlin's performance with each interaction. The tool will help more than twelve million people a year identify birds and participate in a collective effort to help others. The Crowd ID project will make it easier for people to discover the names of birds, learn observation and identification skills, find more information, and appreciate Earth's biodiversity. The summative evaluation plan is measuring increases in participants' knowledge, engagement, and skills, as well as changes in behavior. Impacts on participants will be compared to a control group of users not using Merlin.<br/><br/>Crowd ID tools will be integrated into the CLO's citizen science and education projects, which reach more than 200,000 participants, schoolchildren, and educators across the nation. Merlin will be broadly adapted for use on other websites, social networking platforms, exhibits, mobile devices, curricula, and electronic field guides. Once developed, Merlin can be modified to identify plants, rocks, and other animals. Merlin will promote growth of citizen science projects which depend on the ability of participants to identify a wide range of organisms."
"0964797","EAGR:  Perceptually Optimized Semantic Media Adaptation for Mobile Device Access","IIS","Robust Intelligence","04/01/2010","03/23/2010","Chang Wen Chen","NY","SUNY at Buffalo","Standard Grant","Jie Yang","03/31/2011","$59,999.00","","chencw@buffalo.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","CSE","7495","7495, 7916","$0.00","This project develops a semantic media adaptation scheme for mobile access of information that is perceptually optimized for users to enjoy semantically relevant media content using small display mobile devices. The research team explores seamless integration of multidisciplinary technologies from computer vision, media coding and transmission, wireless networking, mobile device, and human visual perception to tackle the challenges in closing the gap between rich content in high resolution and size limited mobile device access.<br/><br/>The intellectual merit of this project lies in the exploration and development of several relevant techniques in (1) Limited user interface media semantic extraction; (2) Capacity and resource constrained media content adaptation; (3) Perceptually optimized delivery and display of adapted media to small sized mobile devices. The research team addresses these issues by seamless integration of technologies from different research fields that traditionally have less interaction. <br/><br/>The project bridges both semantic gap and user intention gap in mobile multimedia search and access. First, the innovative scheme of semantic adaptation can be extended for any media search application based on semantically relevant characteristics.  Second, the adaptation of high resolution media content for small sized mobile device plays a key role in media gateway for wireless mobile access. Finally, the investigation of perceptual optimized display on mobile devices shall open up a new research avenue to understand how mobile users perceive rich media content with small displays."
"1017626","RI: Small: Probabilistic Latent Variable Models for Sparse Data","IIS","ROBUST INTELLIGENCE","12/01/2010","12/03/2010","Raquel Urtasun","IL","Toyota Technological Institute at Chicago","Standard Grant","Jie Yang","11/30/2011","$149,988.00","","rurtasun@ttic.edu","6045 S Kenwood Ave","Chicago","IL","606372803","7738340409","CSE","7495","7923","$0.00","This project focuses on learning parsimonious representations of complex high-dimensional data. In recent years, the PI has developed probabilistic latent variable models based on Gaussian processes that are able to capture complex interactions and perform state of the art prediction in diverse applications such as 3D human body tracking and collaborative filtering. However, in real-word applications such as the analysis of human motion or object recognition from images, the data is structured (e.g., the correlations in a video sequence can be captured with tensors), can come from different sources of information (e.g., video and audio), can be generated from a complex dynamical process or additional information might be available (e.g., labels). <br/><br/>To address these real world problems, this project investigates extensions of the aforementioned probabilistic latent variable models to learn parsimonious representations of this complex data, focusing on the development of efficient learning algorithms that are able to handle large and sparse datasets. This research is strongly tied to an empirical performance goal, consisting of improving the state-of-the-art in both pose estimation and object recognition applications through the modeling of such complex interactions.  The proposed research has broad impact in several areas of computer vision in particular human body motion estimation and tracking."
"1018751","RI:  Small:  Learning  and Inference with And-Or Graphs for Image Understanding","IIS","ROBUST INTELLIGENCE","08/01/2010","11/13/2014","Song-Chun Zhu","CA","University of California-Los Angeles","Continuing grant","Jie Yang","06/30/2015","$450,000.00","Yingnian Wu","sczhu@stat.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","CSE","7495","7923, 9150","$0.00","In this project, the PIs and students study a probabilistic and graphical representation, called the And-or graph (AoG) for visual knowledge representation.  This AoG model embodies hierarchical and contextual models for visual objects and scenes and is the key to robust object and scene recognition. More specifically, the project addresses two major technical challenges: (i)  Learning the AoG for representing objects and scenes in an unsupervised way; and (ii)  Developing effective inference algorithm by scheduling top-down and bottom-up processes to extract semantic contents in a parse graph under the guidance of the AoG. The extracted semantics include the hierarchical decomposition of the image from scene to objects, and parts, as well as the contextual relations. These contents are crucial for filling in the semantic gap in large scale image search and retrieval.  The technologies studied in this project are key to a number of applications, such as image content extraction for security surveillance, information gathering, Internet image search, and situation awareness. One specific application studied in this project is autonomous driving assistant for designing safer vehicles and reducing car accidence. The project also supports the training of 3 graduate students over the three year period. Research results are disseminated through public publications in major computer vision conferences and journals, institutional webpages, and shared data sets and code in the Internet."
"1017344","RI: Small: Theory and Experiments with Tumbling Robots","IIS","ROBUST INTELLIGENCE","08/01/2010","06/19/2015","Nikolaos Papanikolopoulos","MN","University of Minnesota-Twin Cities","Standard Grant","Jie Yang","07/31/2016","$497,995.00","","npapas@cs.umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","7495","7495, 7923, 9251","$0.00","This project revolves around tumbling which is an exciting area of robotic locomotion that takes advantage of ground-body interactions to achieve high mobility on smaller scales when compared to conventional methods. Additionally, the required hardware complexity to produce such locomotion is very low. In this respect, tumbling can be viewed as a minimalistic approach to producing miniature mobile robots capable of traversing complex and dynamic terrains. Due to the nature of tumbling however, the added mobility comes at the price of increased control complexity. The minimalistic nature of tumbling robots generally results in underactuated systems that exhibit nonholonomic constraints which greatly complicate the motion planning problem. Additionally, tumbling often involves time-varying supports and sliding contacts with the ground. Ultimately, this research views tumbling as a largely unexplored yet promising area of research. This work addresses the intricacies of tumbling locomotion. Specifically, we are developing general planning algorithms for tumbling robots and identify important design characteristics of tumbling robots that lead to simplified control. <br/><br/>Seminars and workshops to bring together practitioners, end-users, researchers, and policy makers will be organized to have the maximal impact. Web-based dissemination of the algorithms and rapid prototyping/simulation tools ensure that the results of this project reach all communities. Students trained in this project participate in the US FIRST competitions, summer mentoring programs for high school students, summer schools in robotics, and other outreach programs."
"1026141","Middle East Robotics Research Collaboration Planning","IIS","Catalyzing New Intl Collab, Robust Intelligence","06/01/2010","06/07/2010","Rajiv Dubey","FL","University of South Florida","Standard Grant","Richard Voyles","05/31/2012","$49,440.00","Redwan Alqasemi","dubey@usf.edu","4019 E. Fowler Avenue","Tampa","FL","336172008","8139742897","CSE","7299, 7495","5976, 7495, 7506, 7534, 7542, 9150","$0.00","The objective of this planning visit is to explore and identify potential collaborations in robotics with specific emphasis on rehabilitation robotics, humanoids and medical robotics, in the United Arab Emirates, Saudi Arabia and Qatar, and to meet with the researchers, administrators and government officials there to discuss future research collaborations.   Rehabilitation and assistive robotics are active research fields that can significantly change the lives of the elderly and individuals with physical disabilities.   A U.S. team of 4 faculty members has been assembled.  The team is well qualified to address the objective and has expertise in broad robotics areas including rehabilitation robotics, assistive technology, kinematic and dynamic synthesis, smart and intelligent systems, computer vision and sensing, robotic control, and mechanism design.  Immediate outcome of this Planning Visit will be a report summarizing the findings and recommendations for potential collaborations.  We anticipate proposal submissions (e.g. NSF PIRE) aimed toward long-term goals to continue international collaborations. Our visit is expected to be ""ambassador-like"" and not just purely academic.  It is a ?fact finding"" mission to potentially pave the way for increased US-Gulf collaborations in areas of NSF CISE interest. This planning visit is funded jointly by IIS and the Office of International Science and Engineering (OISE)."
"1058444","MRI RAPID: ACQUISITION OF A FIELD SPECTROSCOPY ENVIRONMENTAL ANALYSIS SYSTEM FOR GULF OIL SPILL RESEARCH","CBET","Major Research Instrumentation, EnvS-Environmtl Sustainability","09/15/2010","05/24/2012","Daan Liang","TX","Texas Tech University","Standard Grant","Bruce Hamilton","08/31/2012","$104,908.00","Philip Smith, Stephen Cox, Brian Nutter","daan.liang@ua.edu","349 Administration Bldg","Lubbock","TX","794091035","8067423884","ENG","1189, 7643","015E, 5987, 7914, 9102","$0.00","This MRI-RAPID award is in response to events subsequent to an explosion on the British Petroleum (BP) Deepwater Horizon Oil Rig in the Gulf of Mexico off the coast of Louisiana.  The objective of this proposal is to acquire a Field Spectroscopy Environmental Analysis System to enable collection of highly perishable data on the Deepwater Horizon oil spill. More specifically, the instrument will be used to take in-situ measurements of the spectral response of oil spill and vegetation under stress between 350 nm and 2500 nm. The need for collecting these field samples is urgent due to the rapid evolution of spatial distribution and physical-chemical composition of leaked oil as it begins interacting with the environment and oil collection efforts. The transient and dynamic nature of such a large-scale disaster calls for both immediate and continuous efforts in data collection, benchmarking, and quantitative analysis for monitoring, assessment, and management of the impact of this and future spills.  The new instrument will enable the team to take time-sensitive, in-situ measurements of solar reflectance of oil spill in various physical forms (e.g. sheen, patch, tar balls) at various sites (e.g. deep water, shallow water, marshes, and beaches) while recording their geospatial locations. In conjunction with hyperspectral images acquired by satellites and/or aircraft, the team will be able to pursue potentially transformative research in 1) seeking a spectral-spatial solution for achieving more accurate oil distribution mapping in open water; 2) estimating oil slick thickness based on its spectrum and water conditions; 3) assessing the quantity of tar balls onshore using spectral un-mixing to support clean-up activities; and 4) detecting the presence of oil in complex, environmentally sensitive ecosystems.  Environmental toxicologists on the team will use the instrument to perform spectral analysis of ecosystems stressed by crude oil.  The acquisition of this instrument will significantly enhance the capabilities of the Computer Vision and Image Analysis Laboratory (CVIAL) at Texas Tech University to enable hands-on research experience for graduate and undergraduate students in imaging spectroscopy and hyperspectral remote sensing with broad applications in disaster response, environmental monitoring, and national defense."
"1001621","Networking STEP Project Leadership for Sharing Best Practices","DUE","STEP-STEM Talent Expansn Pgm","03/01/2010","02/26/2010","Jorja Kimball","TX","Texas A&M Engineering Experiment Station","Standard Grant","Lee Zia","05/31/2012","$49,780.00","Judy Kelley, Margaret Hobson","j-kimball@tamu.edu","400 Harvey Mitchell Pkwy S","College Station","TX","778454645","9798626777","EHR","1796","9178, SMET","$0.00","A series of three workshops is being held in 2010 to address challenges specific to projects in Texas that are supported by the Science, Technology, Engineering, and Mathematics Talent Expansion Program (STEP). Workshop I has a primary focus on informing participants of the goal of each of the fifteen STEP projects funded in the state of Texas, and providing introductions and social networking opportunities for project personnel.  A part of the workshop is devoted to using evidence-based decision making to inform projects, and a panel of senior PIs provides a question and answer session to field questions from new projects and to offer lessons learned.  Workshop II focuses on a theme of best practices in retention for minority students, first generation students, women, and community college students. Presenters use their project data and tie their findings/recommendations to current literature.  Workshop III focuses on mathematics as a critical factor in STEM student success and learning of other concepts. Topics include math preparedness, math placement, and promoting deep learning, especially for transfer and minority students.  At least 28 participants are expected to attend each workshop. The product of each workshop will be a document of ""Key Findings"" that summarizes key conclusions and next steps, with an emphasis on information that will be useful to the project teams at the workshops and to the broader national set of STEP projects."
"1007527","High Dimensional Statistical Theory for Sparse Regularization","DMS","STATISTICS","07/01/2010","05/03/2012","Tong Zhang","NJ","Rutgers University New Brunswick","Continuing grant","Gabor J. Szekely","06/30/2014","$250,000.00","","tzhang@stat.rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","MPS","1269","","$0.00","High Dimensional Statistical Theory for Sparse Regularization<br/><br/><br/>The investigator studies statistical machine learning with sparse regularization in the setting of high dimensional statistical estimation. A number of research directions will be explored, including improved performance bounds for sparse regularization, new sparse learning formulations, and the statistical theory for several important computational algorithms.<br/><br/><br/>In the information age, more and more data become available electronically, and these data need to be automatically analyzed by computers in order to filter out the most important information. Statistical machine learning is the main technical tool for analyzing electronic data. Many modern applications involve data in very high dimension that cannot be handled by traditional algorithms. Sparse regularization is an important new statistical machine learning technique that can deal with this issue by effectively identifying the most significant patterns from a vast amount of available information. This research develops new sparse regularization algorithms that will significantly enhance the capability for modern computer systems to find critical information from available electronic data."
"1000255","Collaborative Research: Motion Control of Bacteria-Powered Microrobots","CMMI","CONTROL SYSTEMS","07/01/2010","05/12/2012","MinJun Kim","PA","Drexel University","Standard Grant","Jordan Berg","06/30/2014","$213,387.00","","mjkim@lyle.smu.edu","1505 Race St, 10th Floor","Philadelphia","PA","191021119","2158955849","ENG","1632","030E, 031E, 034E, 036E, 1057, 116E, 9178, 9231, 9251, CVIS","$0.00","The research objective of this award is to understand the control of bacterial propulsion systems, and to demonstrate the enabling technologies necessary for the feedback control of bacteria-actuated microstructures. Bacteria are ideal systems for many microbiorobotic systems, because of the ease of their ?gproduction and refueling?h. For this purpose, microbiorobots (MBRs) are constructed, which consist of flagellated bacteria integrated with fabricated microstructures. The bacterial cells propel the microstructures in fluidic environments.  In this project, some design aspects for the MBRs are explored, including the effects of bacterial density, distribution and orientation on the surface of the MBRs, as well as various modalities to control the bacteria. A number of stimuli, including ultraviolet light, electromagnetic field, chemicals, and thermal stimuli are used as control inputs to the MBRs, while measurement feedback is provided by a computer vision-based system. Deliverables include mathematical models of the system behavior, experimental results on the bacteria morphology, microscopy visualization, prototypical demonstration of motion control, documentation of research results, engineering student education, and engineering research experiences for high school students and teachers.<br/><br/>If successful, the outcome of this research will represent a critical step toward understanding how to control bacterial propulsion systems to manipulate larger engineered elements, for example in microassembly and micromanipulation scenarios. Graduate and undergraduate engineering students will benefit from this project through classroom instruction and involvement in the research.  The program will also be integrated with various outreach activities, including (i) microbiorobotics workshops at Drexel and RPI, (ii) active recruitment and training of women and under?]represented minority engineers by leveraging and expanding existing and proven programs already in place at Drexel and RPI, (iii) outreach to high school students and teachers at RPI, and (iv) interactive web?]based tutorials and exhibits."
"1007563","Eigenvectors of random graphs & diffusions on simplices","DMS","PROBABILITY","07/01/2010","05/21/2010","Soumik Pal","WA","University of Washington","Standard Grant","Tomek Bartoszynski","06/30/2014","$147,424.00","","soumikpal@gmail.com","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","MPS","1263","","$0.00","The PI proposes two long term directions in his research in Probability theory. The first one involves study of spectral and eigenvectors properties of sparse random graphs. This is a joint work with Ioana Dumitriu. Eigenvectors of graph adjacency or laplacian matrices have natural significance as solutions of combinatorial optimization problems. However, very little theoretical results are known about eigenvectors of random graphs. Our ongoing investigation combines methods and intuition from the Random Matrix Theory with graph combinatorics. We have so far succeeded in obtaining interesting results for random regular graphs where we show that, when the degree grows poly-logarithmically with the order, the adjacency matrix displays some of the properties of the Gaussian Orthogonal Ensembles. Eventually we plan to  study ``real world'' graphs, particularly the ones with a given degree distribution. The second direction is the construction and study of a diffusion on the space of continuum trees whose reversible measure is the Brownian CRT, as conjectured by David Aldous. This appears as a limit of natural discrete Markov chains on finite phylogenetic trees. Continuum trees are of great significance as they appear as the limiting state space of several families of random trees and random planar maps (via Schaeffer bijection). It appears from our ongoing study that this limiting diffusion is a new kind of measure-valued process on the space of such trees, reminiscent of the classical Fleming-Viot model. The proposed method is a continuation of ideas developed by the PI in related recent work.<br/><br/><br/>Random graphs and networks are popular in diverse areas such as social networks, models for the internet, computer vision, and number theory. Several natural optimization problems on graphs (e.g., figuring out clusters, or ranking algorithms such as Google PageRank) involve what are called eigenvectors of the graph. If the network grows randomly, its eigenvectors are random, and it is of interest to study its properties. A study of such properties is listed as one of our main research areas. The other main area involves the structure of phylogenetic (or evolutionary) trees. A lot of recent interest in Biology and related mathematics is in the structure of the collection of all possible evolutionary trees. This is an enormous space very different from the usual Euclidean (say, three dimension) space. One way to explore this set is to let a Markov chain jump around from tree to tree randomly in a ``nice'' way. A few such models have been proposed in the literature. We take up the study of one of them which is related to other areas of Probability. The analysis and results are expected to be quite novel in the subject area."
"0954361","CAREER: Dynamics and Control of Motion Coordination for Information Transmission in Groups","CMMI","DYNAMICAL SYSTEMS","04/01/2010","03/26/2010","Derek Paley","MD","University of Maryland College Park","Standard Grant","Massimo Ruzzene","03/31/2015","$400,000.00","","dpaley@umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","ENG","7478","034E, 036E, 1045, 1057, 1187, CVIS","$0.00","The long-term goal of this CAREER project is to improve our understanding of information transmission in biological groups and apply this understanding to synthesize bio-inspired motion- coordination algorithms for autonomous vehicles. How do schooling fish respond collectively to an external stimulus that is perceived by only a few individuals? How might we enable a fleet of autonomous vehicles to coordinate their movement using only sensory perception? In pursuit of this goal, the research objectives are (1) to use tools from estimation theory and computer vision to perform quantitative analysis of collective behavior in schooling fish; (2) to conduct laboratory experiments to test the hypothesis that schooling fish use motion coordination to amplify external signals and attenuate noise; (3) to construct an analytical modeling framework for information transfer in a kinetic interaction network; and (4) to synthesize bio-inspired motion-coordination algorithms using nonlinear-control tools.<br/><br/>The long-term educational goal of the PI is to cultivate an engaging environment for creative, rigorous application of dynamics and control theory that propels a diverse population of students to pursue STEM education and careers. In pursuit of this goal, the educational objectives are (1) to conduct K-12 outreach activities that broaden the STEM participation of minorities and women; (2) to pioneer a K-12 outreach framework that uses the topic of collective behavior in fish to create a partnership with a middle-school educator; and (3) to provide research experiences for aerospace undergraduate students.<br/><br/>The broader significance and importance of the proposed research activities lie in the promise of improved understanding of collective behavior in biological groups and improved capacity to model and synthesize collective motion in engineered networks. Improved tools for tracking biological aggregations can be used to understand mosquito swarms in malarial regions; to monitor the role of krill swarms in carbon sequestration; and to assess the impact of bacterial aggregations on human disease processes. Research in biologically inspired coordination of unmanned systems has applications in the inspection of aging civil infrastructure; improved forecasts of hurricane intensity; and environmental monitoring of climate variability. It is not presently understood what motion coordination strategies yield fast and accurate information transmission in biological systems because sufficient data to quantitatively analyze and model three-dimensional motion in large groups are not available. The proposed fish-tracking research will yield new tools for automated, three-dimensional trajectory reconstruction of the position, orientation, and shape of individual fish in a high-density school."
"1000284","Collaborative Research: Motion Control of Bacteria-Powered Microrobots","CMMI","CONTROL SYSTEMS","07/01/2010","06/14/2010","Anak Agung Julius","NY","Rensselaer Polytechnic Institute","Standard Grant","Jordan Berg","06/30/2014","$192,600.00","","agung@ecse.rpi.edu","110 8TH ST","Troy","NY","121803522","5182766000","ENG","1632","030E, 036E, 1057, CVIS","$0.00","The research objective of this award is to understand the control of bacterial propulsion systems, and to demonstrate the enabling technologies necessary for the feedback control of bacteria-actuated microstructures. Bacteria are ideal systems for many microbiorobotic systems, because of the ease of their ?gproduction and refueling?h. For this purpose, microbiorobots (MBRs) are constructed, which consist of flagellated bacteria integrated with fabricated microstructures. The bacterial cells propel the microstructures in fluidic environments.  In this project, some design aspects for the MBRs are explored, including the effects of bacterial density, distribution and orientation on the surface of the MBRs, as well as various modalities to control the bacteria. A number of stimuli, including ultraviolet light, electromagnetic field, chemicals, and thermal stimuli are used as control inputs to the MBRs, while measurement feedback is provided by a computer vision-based system. Deliverables include mathematical models of the system behavior, experimental results on the bacteria morphology, microscopy visualization, prototypical demonstration of motion control, documentation of research results, engineering student education, and engineering research experiences for high school students and teachers.<br/><br/>If successful, the outcome of this research will represent a critical step toward understanding how to control bacterial propulsion systems to manipulate larger engineered elements, for example in microassembly and micromanipulation scenarios. Graduate and undergraduate engineering students will benefit from this project through classroom instruction and involvement in the research.  The program will also be integrated with various outreach activities, including (i) microbiorobotics workshops at Drexel and RPI, (ii) active recruitment and training of women and under?]represented minority engineers by leveraging and expanding existing and proven programs already in place at Drexel and RPI, (iii) outreach to high school students and teachers at RPI, and (iv) interactive web?]based tutorials and exhibits."
"0941610","CDI-Type II: Zooniverse - Conquering the Data Flood with a Transformative Partnership between Citizen Scientists and Machines","DRL","CDI TYPE II","01/01/2010","02/19/2013","Geza Gyuk","IL","Adler Planetarium","Standard Grant","Ellen McCallie","09/30/2013","$1,889,993.00","John Wallin, Michael Raddick, Pamela Gay, Lucy Fortson, Christopher Lintott","ggyuk@adlerplanetarium.org","1300 S. Lake Shore Drive","Chicago","IL","606052403","3123220325","EHR","7751","6890, 9177, SMET","$1,889,993.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5). <br/><br/>Scientists and researchers from fields as diverse as oceanography and ecology, astronomy and classical studies face a common challenge. As computer power and technology improve, the sizes of data sets available to us increase rapidly. The goal of this project is to develop a new methodology for using citizen science to unlock the knowledge discovery potential of modern, large data sets. For example, in a previous project Galaxy Zoo, citizen scientists have already made major contributions, lending their eyes, their pattern recognition skills and their brains to address research questions that need human input, and in so doing, have become part of the computing process. The current Galaxy Zoo project has recruited more than 200,000 participants who have provided more than 100 million classifications of galaxies from the Sloan Digital Sky Survey. This project builds upon early successes to develop a mode of citizen science participation which involves not only simple ""clickwork"" tasks, but also involves participants in more advanced modes of scientific thought. As part of the project, a symbiotic relationship with machine learning tools and algorithms will be developed, so that results from citizen scientists provide a rich training set for improving algorithms that in turn inform citizen science modes of participation. The first phase of the project will be to develop a portfolio of pilot projects from astrophysics, planetary science, zoology, and classical studies. The second phase of the project will be to develop a framework - called the Zooniverse - to facilitate citizen scientists. In particular, research and  machine-learning communities will be engaged to identify suitable projects and data sets to integrate into Zooniverse.<br/><br/>The ultimate goal with the Zooniverse is to create a sustainable future for large-scale, internet-based citizen science as part of every researcher?s toolkit, exemplifying a new paradigm in computational thinking, tapping the mental resources of a community of lay people in an innovative and complex manner that promises a profound impact on our ability to generate new knowledge. The project will engage thousands of citizens in authentic science tasks leading to a better public understanding of science and also, by the engagement of students, leading to interest in scientific careers."
"1017903","III: Small: Collection Construction Methodologies for Learning-to-Rank","IIS","Info Integration & Informatics","09/01/2010","09/02/2010","Javed Aslam","MA","Northeastern University","Standard Grant","Maria Zemankova","08/31/2013","$488,723.00","","jaa@ccs.neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","7364","7923","$0.00","Modern search engines, especially those designed for the World Wide Web, commonly analyze and combine hundreds of features extracted from the submitted query and underlying documents (e.g., web pages) in order to assess the relative relevance of a document to a given query and thus rank the underlying collection. The sheer size of this problem has led to the development of learning-to-rank algorithms that can automate the construction of such ranking functions: Given a training set of (feature vector, relevance) pairs, a machine learning procedure learns how to combine the query and document features in such a way so as to effectively assess the relevance of any document to any query and thus rank a collection in response to a user input. Much thought and research has been placed on feature extraction and the development of sophisticated learning-to-rank algorithms. However, relatively little research has been conducted on the choice of documents and queries for learning-to-rank data sets nor on the effect of these choices on the ability of a learning-to-rank algorithm to ""learn"", effectively and efficiently.<br/><br/>The proposed work investigates the effect of query, document, and feature selection on the ability of learning-to-rank algorithms to efficiently and effectively learn ranking functions.  In preliminary results on document selection, a pilot study has already determined that training sets whose sizes are as small as 2 to 5% of those typically used are just as effective for learning-to-rank purposes. Thus, one can train more efficiently over a much smaller (though effectively equivalent) data set, or, at an equal cost, one can train over a far ""larger"" and more representative data set.  In addition to formally characterizing this phenomenon for document selection, the proposed work investigate this phenomenon for query and feature selection as well, with the end goals of (1) understanding the effect of document, query, and feature selection on learning-to-rank algorithms and (2) developing collection construction methodologies that are efficient and effective for learning-to-rank purposes.<br/><br/>In addition to characterizing and developing collection construction methodologies, the project plan includes development and release of new, efficient, and effective learning-to-rank data sets for use by academia and industry.  In fostering this effort, the project team has close ties with the National Institute of Standards and Technology (NIST) and Microsoft Research, two of the premier organizations that develop and release Information Retrieval data sets.  All research results and data sets developed as part of this project will be made available at the project website (http://www.ccs.neu.edu/home/jaa/IIS-1017903/). The project provides an educational and training experience for students."
"0953274","CAREER: Combinatorial Online Learning and its Applications","IIS","Robust Intelligence","04/01/2010","04/30/2013","Arindam Banerjee","MN","University of Minnesota-Twin Cities","Continuing grant","Weng-keen Wong","03/31/2017","$495,804.00","","banerjee@cs.umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","7495","1045","$0.00","Several important problems in machine learning, such as maximum<br/>aposteriori (MAP) inference in graphical models, are inherently combinatorial. While extensive research has been devoted to designing approximation algorithms for such problems, existing algorithms do not scale well to large problems. This project focuses on leveraging ideas from online learning with expert advice to develop a novel family of online learning algorithms for combinatorial optimization problems. <br/><br/>Algorithms for combinatorial online learning are efficient and simple to analyze in order to establish guarantees. Unlike existing literature on approximation algorithms for combinatorial problems which rely on suitable real relaxations of the original problem, combinatorial online learning algorithms never use relaxations; they work directly with binary/integer solutions and have global approximation guarantees.  The project investigates generalizations of the framework to solve online and batch binary quadratic programming problems, yielding approximation algorithms for a variety of combinatorial problems, including NP-complete problems, and MAP inference in directed and undirected graphical models. The project considers three important real life applications: portfolio selection for effectively investing in the stock market, automating surgical pathology by expediting disease detection in tissue images, and climate change detection for discovering abrupt climate changes from spatiotemporal climate data. <br/><br/>The project is expected to be transformative, especially in the context of surgical pathology and climate change detection, yielding significant long term societal benefits. The research results will be disseminated to the community through research papers, tutorials, open source software, and outreach activities using games based on mock stock markets."
"1016312","RI:  Small:  Perceptually Grounded Learning of Instructional Language","IIS","Robust Intelligence","09/01/2010","06/10/2011","Raymond Mooney","TX","University of Texas at Austin","Continuing grant","Tatiana Korelsky","08/31/2014","$450,000.00","","mooney@cs.utexas.edu","3925 W Braker Lane, Ste 3.340","Austin","TX","787595316","5124716424","CSE","7495","7923","$0.00","This project is developing methods that allow a computer to automatically learn to understand and generate instructions in human language. Traditional approaches to natural-language learning require linguistic experts to laboriously annotate large numbers of sentences with detailed information about their grammar and meaning. In this project, instructional language is initially learned by simply observing humans following instructions given by other humans.  Once the system has learned reasonably well from observation, it also actively participates in the learning process by following human-given instructions itself, or giving its own instructions to humans and observing their behavior. The approach is being evaluated on its ability to interpret and generate English instructions for navigating in a virtual environment (e.g. ""Go down the hall and turn left after you pass the chair."").  A novel machine learning method infers a probable formal meaning for a sentence from the resulting actions performed by a human follower, and then existing language-learning methods are used to acquire a language interpreter and generator.  The learned system is being evaluated in a range of virtual environments, testing its ability to follow human-provided natural language instructions to achieve prescribed goals, as well as to generate natural language instructions that humans can successfully follow to find specific destinations. The methods developed for this project will contribute to the development of virtual agents in games and educational simulations that learn to interpret and generate English instructions, and eventually aid the development of robots that can learn to interpret human language instruction from observation."
"1018114","DC:Small:Collaborative Research:Data Intensive Computing  for General Relational Data Learning","CCF","DATA-INTENSIVE COMPUTING","09/01/2010","03/23/2011","Lixin Gao","MA","University of Massachusetts Amherst","Standard Grant","Jie Yang","08/31/2015","$257,997.00","","lgao@ecs.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","CSE","7793","7793, 9218, 9251, HPCC","$0.00","It is well-observed that the whole world is full of data that are highly related and of diverse data object types such as people, organizations, and events. In many applications, it is intended to discover the hidden structures through such relationships involving different types of data objects in the world, in addition to ""clusters"" of the same type of data objects. On the other hand, relational data learning typically involves a large collection of data objects and thus algorithms for relational data learning are computation-intensive as well as data intensive. This calls for massively parallel solutions in order to make the algorithms scalable to large collections of data. This project addresses a three year integrated research and education program focusing on engaging in-depth research in developing novel parallel frameworks for a wide spectrum of state-of-the-art solutions to a series of fundamental problems in relational data learning. This research promotes the revolutionized understanding of relational data learning in the context of distributed computation environment. The project addresses fundamental problems in the literature of relational data learning as well as the expected breakthrough in the interdisciplinary and multidisciplinary research communities including parallel computation and scheduling, data mining and machine learning, and pattern analysis. The technologies generated from the research can be immediately deployed in important applications such as social network analysis, biological information discovery, financial and economic development analysis and prediction, natural disaster prediction, as well as military intelligence analysis. <br/><br/>Project url: <br/><br/>http://www.fortune.binghamton.edu/nsf-iis-1017828.htm"
"1023875","Causal Learning As Sampling","BCS","DS -Developmental Sciences","09/15/2010","09/12/2010","Alison Gopnik","CA","University of California-Berkeley","Standard Grant","Laura Namy","08/31/2014","$323,030.00","Thomas Griffiths","gopnik@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","SBE","1698","","$0.00","In the course of development, children change their beliefs, moving from a less to more accurate picture of the world. How do they do this when there are apparently an infinite variety of beliefs from which to choose? And how can we reconcile children's cognitive progress with the apparent irrationality of many of their explanations and predictions? In computer science, probabilistic models have provided a powerful framework for characterizing beliefs, and can tell us when beliefs are justified by the evidence. But they face similar questions: how can one actually get from less warranted beliefs to more accurate ones given a vast space of possibilities? This project brings these threads together, suggesting a possible solution to both challenges. The solution is based on the idea that children may form their beliefs by randomly sampling from a probability distribution of possible hypotheses, testing those sampled hypotheses, and then moving on to sample new possibilities. This ""Sampling Hypothesis"" provides a natural bridge between understanding how children actually do learn and reason and how computers can be designed to learn and reason optimally. These experiments will provide an important first step in exploring the Sampling Hypothesis: how do evidence and prior beliefs shape the samples of possible beliefs that children generate and evaluate, and how do developmental changes lead to differences in the samples of possible beliefs generated and evaluated.  <br/><br/>A relatively immediate contribution of this work will be to connect state-of-the-art methods from machine learning and data analysis in computer science and statistics with accounts of belief acquisition in developmental psychology and educational psychology. In the longer run, the proposed projects have the potential to inform education, early intervention programs, and the study of cognitive deficits; by precisely characterizing how learning should proceed in typically developing children, this project can illustrate when and how developmental limitations impact learning and suggest a framework of ways of helping children with such disorders. The research also supports an ambitious training plan for post-doctoral and graduate student researchers, requiring the development of a nuanced understanding of both computational approaches and developmental experiments."
"0968487","SoCS:  Effectively Leveraging Contributions in Human Computation Systems","IIS","Special Projects - CCF, SOCIAL-COMPUTATIONAL SYSTEMS","08/01/2010","04/20/2011","Luis von Ahn","PA","Carnegie-Mellon University","Standard Grant","Tatiana Korelsky","07/31/2013","$753,500.00","Tom Mitchell","biglou@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","2878, 7953","7953, 9251","$0.00","Human computation studies how to collect useful data as a by-product of another activity in which people are interested (e.g., playing games). A popular example is the ESP Game, where two players are shown the same image and must independently generate tags; tags that match become labels for the image. ESP Game players have generated millions of labels that help improve image search engines.<br/><br/>Currently, little is understood about how to capitalize on each person's individual expertise to produce the best results in human computation systems. For example, the ESP Game could generate better results if automotive enthusiasts labeled images of cars while biologists labeled images of animals. This project aims to better understand how each individual's different capabilities can be assessed, dynamically leveraged, and even improved for the purposes of human-driven data collection. <br/><br/>Intellectual Merit: Improved understanding of the strengths and weaknesses of human users as teachers and data sources; an intelligent new objective-driven model of data collection; novel opportunities to study machine learning algorithms that capitalize on human teachers' abilities; and an analysis of learning opportunities as incentives for people to participate in human computation systems.<br/><br/>Broader Impact: Distribution of large new data sets (e.g., Wikipedia articles in multiple languages); several Internet-based human computation systems for large-scale evaluation of machine learning and other algorithms; a new course called ?Human-in-the-Loop Systems?; workshops held in conjunction with major conferences; and outreach activities (e.g., summer projects) that introduce female undergraduate students to interdisciplinary research."
"1009542","Text, Neuroimaging, and Memory: Unified Models of Corpora and Cognition","IIS","Engineering of Biomed Systems, ROBUST INTELLIGENCE","08/01/2010","06/19/2015","Kenneth Norman","NJ","Princeton University","Standard Grant","aude oliva","12/31/2015","$732,296.00","Kenneth Norman","knorman@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","CSE","5345, 7495","004E, 7327","$0.00","The PIs will develop new machine learning algorithms to explore how meaning is represented in the brain and how meaning representations shape human memory.  Current neuroscientific theories of memory posit that forming a memory for a particular event involves associating the details of that event with the person's current mental context, i.e., everything else that she is thinking about at the time.  When trying to remember the event, the person can access stored details by reinstating the mental context that was present when the memory was formed. This fits with the intuition that forgotten details (e.g., the location of misplaced house keys) can be retrieved by mentally ""re-tracing steps"", i.e., trying to reinstate the mindset that was present at the time of the original event.  With these theories in mind, the goal of this work is to develop machine learning algorithms that make it possible to track, based on fMRI brain data and behavioral memory data, the process of ""mentally re-tracing steps""---the proposed algorithms will be able to decode the state of a person's mental context as she forms memories and (later) as she searches for these memories.<br/><br/>The proposed work uses two fundamental ideas about memory and meaning: The first idea is that mental context is shaped by the meanings of recently encountered stimuli.  The second idea is that semantic relationships between concepts in the brain mirror statistical relationships between words in naturally occurring language. The developed algorithms will bring together data from three sources---behavioral data from subjects performing memory recall tasks, fMRI neuroimaging data collected while subjects performed these tasks, and large collections of documents---to discover a latent meaning space that can simultaneously describe all three types of information.  Each point in this space describes a mental context. Thus the core of the proposed work is to develop latent variable models and algorithms that can infer from data how the mental context moves through meaning space as a person stores and searches for memories.<br/><br/>The proposed work will lead to fundamental advances in machine learning (new algorithms for inferring hidden variables based on multiple, heterogeneous data types) and neuroscience (more refined theories of how memory search is accomplished in the brain). Furthermore, this work will catalyze the development of new technologies for diagnosing and remediating memory problems, by making it possible to track how the contextual reinstatement process is going awry in people experiencing memory retrieval failure."
"1017967","III:  Small:  Representation, Modeling and Inference for Large Biological and Information Networks","IIS","Info Integration & Informatics","08/01/2010","05/04/2012","Edoardo Airoldi","MA","Harvard University","Continuing grant","Sylvia Spengler","07/31/2014","$513,780.00","Jun Liu","airoldi@temple.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","CSE","7364","7364, 7923, 9251","$0.00","Modern technology has completely transformed the concept of data in the biological and information sciences. Data collections about the flow of information on the web, for instance, or about regulatory and metabolic dynamics that drive cellular functionality are extremely large and heterogeneous. These collections are often characterized as networks of websites, or proteins, where directed edges denote information flow, or chemical reactions, and with node information described in terms of web pages, or chains of amino acids. Knowledge discovery and management is key. The goal of this proposal is to create novel computational and statistical approaches to store, search, and quantify patterns in large networks efficiently, and to explore the extent to which these new tools help address a number of important open problems and computational issues. The research plan includes theoretical, methodological, data analysis, and dissemination aspects.<br/><br/>The approach is to develop new models, methods and algorithms for analyzing large biological and information networks with rich node information. New tools will be developed: to assess the complexity of networks; to compare the fit of alternative network models; to store information about both connectivity and nodes in a network efficiently; to calibrate informative priors for networks that reflect the reality of signaling both in metabolic networks and  in the spread of news on the web for empirical Bayesian analyses; to estimate the effects of node information on the local connectivity in a network; and to infer influence potentials and diffusion channels in online information networks. The proposed research is focused on three specific technical tasks: (1) establishing a new representation of valued, multivariate networks based on a statistical models; (2) developing a flexible family or probabilistic graphical models to link local connectivity in the network to high-dimensional node attributes; and (3) developing scalable algorithms to infer a non-observable network structure from multiple trails of informational artifacts on the network itself. In addition, two in-depth case studies will be developed to illustrate the potential of the proposed methodology. The first is an analysis of the effects of local influence patterns among online newspapers, news collectors and blogs on the diffusion of news and information items. The second is an analysis of the effects of local perturbations of signaling in regulatory networks on global cellular responses, for many known functions, from bacteria to human. Insights gained in tackling the case studies will in turn generalize and foster the development of the next wave of core methodology and theory in machine learning.<br/><br/>The proposed work meets an urgent need for the development of new and principled methods for analyzing massive amounts of network data, as well as the creation of large-scale data sets for testing and benchmarking, to the benefit of the community at large. The research plan is tightly integrated with an interdisciplinary educational program and with the development of a statistical machine learning curriculum, which will attract many undergraduates to research at the intersection of machine learning and the sciences, and will provide opportunities to actively encourage students from underrepresented groups to pursue careers in computer science and statistics. The team will distribute open source software and set-up websites to enable the community to use and build upon the tools."
"1029260","What Do Customers Like: A New Approach That Lets The Data Decide","CMMI","OPERATIONS RESEARCH","10/01/2010","09/20/2010","Devavrat Shah","MA","Massachusetts Institute of Technology","Standard Grant","Donald Hearn","09/30/2015","$305,000.00","Vivek Farias","devavrat@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","ENG","5514","073E, 9147, MANU","$0.00","The research objective of this award is to establish theoretical foundations and build algorithms for the problem of learning non-parametric models of customer choice from limited data. Existing tools for this task posit parametric models of choice which are subsequently fit to data. In contrast, the starting point for this research will be a characterization of the set of all possible choice models consistent with any data that might be available. Given such a characterization, the research will produce methodologies for, and establish the merits of, several distinct criteria for selecting an appropriate choice model from this set. In the spirit of Occam's razor, one criterion that will be considered is finding the ""sparsest"" model from this set. The study of this criterion will establish the limits to choice model identification, a key component of any theory of learning. This will be accomplished via the use and development of tools from the area of compressive sensing. A second criterion for model selection will be derived from a certain function of the choice model sought that arises in numerous revenue optimization applications. Algorithms that utilize this criterion to select a choice model will require new optimization techniques for robust optimization. <br/><br/>If successful, this research will advance the fundamental techniques used in obtaining choice models from available partial data. Choice models are probabilistic descriptions of how consumers choose from options presented to them. This research will make possible a ""black box"" approach to the problem of learning such choice models that will be valuable in applications where expert input is difficult or expensive to obtain. Applications in this genre include problems of inventory and assortment optimization faced by large auto companies and online retailers alike. This research will provide a viable methodology for such problems wherein the complex nature of consumer demand is explicitly accounted for. The implementation of such methodologies will lead to increased revenues and efficiencies. At the same time, this investigation represents a promising direction within the statistical inference, signal processing and machine learning communities, which have recently begun to explore the methodological challenges herein under the umbrella of compressive sensing. In that sense, this work will advance the theory of compressed sensing and non-parametric learning."
"1012017","RI-Large: Activity Learning and Recognition for a   Cognitive Assistant","IIS","Info Integration & Informatics, Robust Intelligence","08/15/2010","08/03/2012","Henry Kautz","NY","University of Rochester","Continuing grant","Tatiana Korelsky","01/31/2014","$749,988.00","","kautz@cs.rochester.edu","518 HYLAN, RC BOX 270140","Rochester","NY","146270140","5852754031","CSE","7364, 7495","7925","$0.00","This project addresses a key problem in advancing the state of the art in cognitive assistant systems that can interact naturally with humans in order to help them perform everyday tasks more effectively. Such a system would help not only people with cognitive disabilities but all individuals as they perform complex tasks they are unfamiliar with. The research focuses on structured activities of daily living that lend themselves to practical experimentation, such as meal preparation and other kitchen activities.<br/><br/>Specifically, the core focus of the research is activity recognition, i.e., systems that can identify the goals and individual actions a person is performing as they work on a task. Key innovations of this work are 1) that the activity models are learned from the user via intuitive natural demonstration, and 2) that the system is able to reason over activity models to generalize and adapt them. In contrast, current practice requires specialized training supervised by the researchers and supports no reasoning over the models. This advance is accomplished by integrating capabilities that are typically studied separately, including activity recognition, knowledge representation and reasoning, natural language understanding and machine learning. The work addresses a significant step towards the goal of building practical and flexible in-home automated assistants."
"1017429","HCC: Small: Learning Routines to Support People's Activities","IIS","HCC-Human-Centered Computing","08/15/2010","04/25/2011","Anind Dey","PA","Carnegie-Mellon University","Standard Grant","William Bainbridge","07/31/2014","$507,592.00","John Zimmerman","anind@uw.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7367","7367, 7923, 9150, 9251","$0.00","People construct routines as they repeatedly perform the same sequence of actions. Routines provide a huge benefit by freeing people?s attention, allowing them to carry out their daily tasks without constantly thinking about every little thing they must do. Problems begin to arise when people must deviate from their routines. Families rely heavily on their routines to address the complex logistics and conflicting agendas of work, school, family, and enrichment activities. However, families often deviate from their routines, and when breakdowns in the plans occur, they feel their lives are out of control.<br/><br/>This research will develop a system that learns the routine movements of family members, and a planning system that leverages this model in order to generate a speculative plan for future days.  The system will also predict conflicts with scheduled deviations and detect when plans begin to breakdown, such as when someone forgets to deviate from a routine. A calendar interface that displays the routine movements of family members along with their scheduled deviations and a small set of reminder applications that help people enact their plans and that support them when plans breakdown will form the basis for evaluating the underlying systems. This research is transformative in the novel integration of machine learning and planning techniques, and its application to a real-world and complex problem. Finally, this research provides insights on how intelligent, ubiquitous computing technology influences families? feelings of control and their quality of life. <br/><br/>The proposed work has the potential to significantly improve the quality of life for millions of families by reducing stress caused from breakdowns in plans and routines. Lowering stress can improve the quality of marriages, the quality of parenting, and the physical and mental health of children. We will involve undergraduate and graduate students in our research and will incorporate our findings into our courses on ubiquitous computing, interaction design, and on smart homes. We expect that our focus on a social problem will attract non-science-focused students to science and expose science-focused students to design methods of inquiry."
"0941533","CDI Type I: Collaborative Research: Integration of relational learning with ab-initio methods for prediction of material properties","ECCS","CDI TYPE I","01/01/2010","09/15/2009","Yuan Qi","IN","Purdue University","Standard Grant","Radhakisan S. Baheti","12/31/2014","$324,600.00","","alanqi@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","ENG","7750","0000, 7721, 7722, 7752, OTHR","$0.00","<br/>The objective of this collaborative research is to apply computational thinking to materials science with the goals of revealing hidden rules about materials structure and properties and providing efficient computational and statistical tools for modeling large systems with interacting elements.  The approach combines materials science with statistical learning.  The research is driven by two key problems in materials development, crystal structure prediction and the inverse problem in materials science whereby one postulates the desired properties and finds the composition and arrangement of atoms that result in those properties.  The investigators seek to design principled Bayesian models for relational data, coupled with efficient inference methods.<br/><br/>With respect to intellectual merit, the integrative approach is a significant departure from current methods for materials research.  Ab initio computation has begun to show promise for materials development, but its integration with statistical learning holds the promise of leading to novel approaches that can utilize massive amounts of materials data.  Further, extracting knowledge from massive relational data presents opportunities for machine learning research.  The research addresses common challenges in many disciplines and provides new mathematical frameworks and computational tools.<br/><br/>With respect to broader impacts, the research has the potential to enhance materials research and, ultimately, lead to the development of better materials.  The application focus on materials for energy is timely and important.  The investigators plan to recruit women and other students from underrepresented groups into their research teams.  Results will be disseminated through education and a cyber-based platform, exposing computer science students to engineering applications."
"0941043","CDI Type I: Collaborative Research: Integration of relational learning with ab-initio methods for prediction of material properties","ECCS","CDI TYPE I","01/01/2010","09/15/2009","Gerbrand Ceder","MA","Massachusetts Institute of Technology","Standard Grant","hao ling","08/31/2013","$305,382.00","","gceder@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","ENG","7750","0000, 7721, 7722, 7752, OTHR","$0.00","<br/>The objective of this collaborative research is to apply computational thinking to materials science with the goals of revealing hidden rules about materials structure and properties and providing efficient computational and statistical tools for modeling large systems with interacting elements.  The approach combines materials science with statistical learning.  The research is driven by two key problems in materials development, crystal structure prediction and the inverse problem in materials science whereby one postulates the desired properties and finds the composition and arrangement of atoms that result in those properties.  The investigators seek to design principled Bayesian models for relational data, coupled with efficient inference methods.<br/><br/>With respect to intellectual merit, the integrative approach is a significant departure from current methods for materials research.  Ab initio computation has begun to show promise for materials development, but its integration with statistical learning holds the promise of leading to novel approaches that can utilize massive amounts of materials data.  Further, extracting knowledge from massive relational data presents opportunities for machine learning research.  The research addresses common challenges in many disciplines and provides new mathematical frameworks and computational tools.<br/><br/>With respect to broader impacts, the research has the potential to enhance materials research and, ultimately, lead to the development of better materials.  The application focus on materials for energy is timely and important.  The investigators plan to recruit women and other students from underrepresented groups into their research teams.  Results will be disseminated through education and a cyber-based platform, exposing computer science students to engineering applications."
"1010109","US-German Collaboration: Integration of Bottom-Up and Top-Down Signals in Visual Recognition","BCS","COGNEURO, COLLABORATIVE RESEARCH, CRCNS","10/01/2010","09/26/2010","Gabriel Kreiman","MA","Children's Hospital Corporation","Standard Grant","Akaysha Tang","09/30/2013","$305,969.00","","gabriel.kreiman@tch.harvard.edu","300 LONGWOOD AVENUE","Boston","MA","021155737","6179192729","SBE","1699, 7298, 7327","7327, 5936, 5979","$0.00","Visual cognition is orchestrated by the interaction of 'bottom-up' (feed-forward) processes that carry sensory information and 'top-down' (feed-back) processes that modulate the incoming input in the context of goals, tasks, emotions and stored information. At the anatomical level, each area within the cerebral cortex is heavily innervated by both feed-forward signals and feed-back signals. With funding from the National Science Foundation, Gabriel Kreiman, Ph.D. of Children's Hospital Corporation (Boston, Massachusetts) in collaboration with Andreas Schulze-Bonhage, Ph.D., of the Freiburg University Hospital (Freiburg, Germany), is investigating the dynamical integration of bottom-up and top-down neural signals, by combining computational models and machine learning techniques for data analysis with high-resolution neurophysiological recordings from the human temporal lobe. Researchers have long recognized that top-down and bottom-up signals play a key role in visual recognition, however, the relative contribution and interactions between these signals remain unclear. The research project is focused on a particular aspect of cognition, namely our ability to visually recognize patterns, which is central to most everyday tasks. Even the best machine computational models available today only provide a coarse approximation to the complex neurophysiological responses found in higher visual cortex. Not surprisingly, a three-year-old can outperform sophisticated computational algorithms in recognition tasks, such as navigation in complex environments or recognizing objects in cluttered scenes. The research project focuses on three progressively more complex tasks that rely increasingly on top-down influences. The first research aim involves top-down influences during recognition of objects in a cluttered visual stimulus. The second aim examines whether neurophysiological responses in the human temporal lobe can support recognition from partial object information. This question is being approached through studying the phenomenon of object completion. The third aim combines visual stimulus clutter and occlusion in a complex realistic recognition scenario. For this aim, the researchers are examining the influences of attention and task-related goals on neurophysiological activity while epilepsy patients play a custom-designed video game. These neurophysiological data take advantage of the rare opportunity to combine high-resolution neurophysiology, computational models, and behaviorally complex tasks to carry out research that would be difficult with non-human animals. <br/><br/>By furthering the understanding of the transformation of perceptual information into cognition, the researchers are contributing to two broader goals: The goal to help alleviate the challenging conditions involved in cognitive disorders through the development of interfaces between brains and machines, and the goal to apply knowledge about neuronal circuits to develop computational algorithms that automatically extract cognitive information from sensory data. Building a fast, robust, and reliable artificial vision system would have profound repercussions in many areas of science and engineering, including pattern recognition, surveillance and security, automatic navigation and clinical image analysis. These scientific and engineering advances could in turn translate into important real-world applications of interest for industrial partnerships. Understanding the visual system relies on many skills ranging from computer science to physics, neuroscience, and psychology. The research efforts are complemented by educational and outreach initiatives aimed at training interdisciplinary scientists. The training is producing multidisciplinary students who can build on their fundamental scientific skills and apply this knowledge to challenging clinical and engineering problems. This project is jointly funded by the Cognitive Neuroscience Program, the Social Behavioral and Economics Division, Collaborative Research in Computational Neuroscience, and the Office of International Science and Engineering. A companion project is being funded by the German Ministry of Education and Research (BMBF)."
"1012205","RI-Large: Activity Learning and Recognition for a   Cognitive Assistant","IIS","Robust Intelligence","08/15/2010","08/03/2012","James Allen","FL","Florida Institute for Human and Machine Cognition, Inc.","Continuing Grant","Tatiana Korelsky","01/31/2014","$750,000.00","","jallen@ihmc.us","40 S. Alcaniz St.","Pensacola","FL","325026008","8502024473","CSE","7495","7925","$0.00","This project addresses a key problem in advancing the state of the art in cognitive assistant systems that can interact naturally with humans in order to help them perform everyday tasks more effectively. Such a system would help not only people with cognitive disabilities but all individuals as they perform complex tasks they are unfamiliar with. The research focuses on structured activities of daily living that lend themselves to practical experimentation, such as meal preparation and other kitchen activities.<br/><br/>Specifically, the core focus of the research is activity recognition, i.e., systems that can identify the goals and individual actions a person is performing as they work on a task. Key innovations of this work are 1) that the activity models are learned from the user via intuitive natural demonstration, and 2) that the system is able to reason over activity models to generalize and adapt them. In contrast, current practice requires specialized training supervised by the researchers and supports no reasoning over the models. This advance is accomplished by integrating capabilities that are typically studied separately, including activity recognition, knowledge representation and reasoning, natural language understanding and machine learning. The work addresses a significant step towards the goal of building practical and flexible in-home automated assistants."
"1017938","HCC: Small: Body Language Animation for Virtual Worlds and Computer-Mediated Communication","IIS","HCC-Human-Centered Computing","09/01/2010","09/02/2010","Vladlen Koltun","CA","Stanford University","Standard Grant","William Bainbridge","08/31/2013","$495,208.00","","vladlen@stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","7367","7923","$0.00","This project will enable full-body nonverbal communication in virtual worlds, applying state-of-the-art machine learning and computer animation techniques to the synthesis of nonverbal communication, advancing and refining these techniques in the process. Virtual worlds are an emerging computer-mediated communication medium that situates geographically distributed participants in a shared communication space and enables embodied interaction with others in a simulated environment. Participants are represented as animated virtual humans that can convey both speech and body language. Yet no viable technology exists that can animate the virtual human's body during a live conversation without resorting to esoteric hardware or brittle algorithmic techniques.<br/><br/>This research will develop an approach that can convey body language through virtual humans in real time, using a natural control interface: the speech and motion of the participants. The proposal is based on, and sometimes advances, the state of the art in machine learning, computer animation, and the relevant aspects of linguistics and cognitive psychology. Body language animation based purely on visual tracking of the participant's motion is prone to significant defects due to tracking noise and failure. Thus this approach analyzes the speech of the participant together with the motion. This principled integration of live speech and motion input constitutes a fundamentally new approach to the control of nonverbal expression of human self-representations in virtual worlds.<br/><br/>The ability to convey rich nonverbal communication in virtual worlds will advance the capabilities of computer-mediated communication as a whole. This will provide basic infrastructure for distributed collaboration in science and engineering. Powerful forms of situated learning and social-scientific inquiry in education will be enabled, with positive impact on the self-efficacy of students who traditionally underperform in science curricula. Social science will be enriched with a new medium for the study of human interaction."
"0953149","CAREER:  Cross-Document Cross-Lingual Event Extraction and Tracking","IIS","Info Integration & Informatics","03/01/2010","06/27/2014","Heng Ji","NY","CUNY Queens College","Continuing grant","Maria Zemankova","03/31/2015","$543,384.00","","jih@rpi.edu","65 30 Kissena Blvd","Flushing","NY","113671575","7189975400","CSE","7364","1045, 7364, 9102, 9215, 9251, HPCC","$0.00","The goal of this research project is advance the Information Extraction (IE) paradigm beyond ""slot filling"", and achieve more accurate, salient, complete, concise and coherent extraction results by exploiting dynamic background knowledge and cross-document cross-lingual event ranking and tracking. The approach consists of cross-document inference, unknown implicit event time prediction and reasoning, cross-document entity coreference resolution with global contexts, centroid entity detection, event attribute extraction and graph-based clustering algorithms for redundancy and contradiction detection, automatic new event clustering and active learning, abstractive summary generation based on extraction results, name translations with comparable corpora and cross-lingual co-training.<br/><br/>The experimental research is integrated with educational activities, including project-related curriculum development. The project involves PhD students as well as undergraduate students, engages non-Computer Science undergraduate students in utility evaluation and corpus annotation, and attracts elementary school and high school students by tutorials, regular research seminars and an extensive summer workshop. The results of this project will also have a benefit in E-Science and E-Learning by extracting and tracking the related knowledge from scientific literature and learning materials used in elementary schools and high schools.<br/><br/>Project results, including open source software, task definition guidelines, annotated corpora, scoring metrics will be disseminated via project Web site<br/>(http://nlp.cs.qc.cuny.edu/blendeet.html)."
"0955316","CAREER: High Dimensional Variable Selection and Risk Properties","DMS","STATISTICS","09/01/2010","08/08/2014","Jinchi Lv","CA","University of Southern California","Continuing grant","Gabor J. Szekely","08/31/2015","$400,000.00","","jinchilv@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","MPS","1269","0000, 1045, 1187, OTHR","$0.00","High dimensional variable selection plays a pivotal role in contemporary statistical modeling, learning and scientific discoveries. Long-standing theoretical questions in the literature include how high dimensionality regularization methods with general penalties can handle, what the role of penalty functions is, and how to characterize the optimality of variable selection procedures. The investigator proposes to study four interrelated research topics. First, the investigator studies penalized likelihood methods with general penalties, which are widely applied for simultaneously selecting important variables and estimating their effects in high dimensional statistical inference, where the dimensionality can be much larger than sample size. Second, various contexts of high dimensional variable selection beyond penalized likelihood methods including penalized empirical risk and hunting for interactions are investigated. Third, the investigator proposes new principles for model selection when models are possibly misspecified and studies the robustness of various regularization methods for high dimensional variable selection under model misspecification. Fourth, the risk properties and optimality of various high dimensional regularization methods in the contexts of penalized least squares and penalized likelihood are further investigated.<br/><br/>The analysis of vast data sets now commonly arises in diverse fields of sciences, engineering and humanities ranging from genomics and health sciences to economics, finance and machine learning. High dimensional data analysis poses numerous challenges to statistical theory, methods and implementations that are not present in smaller scale studies. A major goal of this proposal is to make theoretical and methodological contributions to the important and challenging topic of high dimensional variable selection and statistical inference. These new developments provide unified and systematic understandings of various regularization methods in high dimensions, and allow scientists to analyze high dimensional data with increased efficiency, expediency and interpretability. The proposed work is incorporated into new courses on the state-of-the-art high dimensional statistical learning, and will benefit the training and learning of undergraduates, graduate students, and underrepresented minorities. The proposed work on variable selection in high dimensions will not only help better identify factors that are important to, for example, public health and market risk, but also benefit a broad range of scientists and researchers in various fields.<br/>"
"1026435","SBIR Phase II:  Sound-object Recognition for Real-time or Offline Systems","IIP","SBIR Phase II","08/15/2010","07/24/2014","Jason LeBoeuf","CA","Imagine Research, Inc","Standard Grant","Glenn H. Larsen","07/31/2014","$1,016,000.00","","leboeuf.jay@gmail.com","2660 Garfield Street","San Mateo","CA","944030000","4155965392","ENG","5373","116E, 165E, 5373, 6850, 9139, 9231, 9251, HPCC","$0.00","This Small Business Innovation Research (SBIR) Phase II project includes research and development of audio recognition and analysis software for offline and real-time sound recognition.  Musicians and audio engineers have access to terabytes of music loops and sound effects. However, musicians are limited to searching for sounds using text-only keyword searches. This is a mundane, inaccurate, and exhausting process that ignores the files' actual audio content. The proposed solution provides a unique ""find-something-that-sounds-like-this"" search engine. Media production software and hardware is too complex, tedious, and labor-intensive for both novice and advanced users. The proposed sound platform adds capability that was previously missing - recognizing an input sound and automatically choosing the best parameters for the user. This project uses a signal processing and machine learning platform to perform novel experiments for classifying audio streams in real-time, improving recognition accuracy, and retrieving sounds from large collections. Commercial-quality software development kits for offline and real-time sound recognition will be developed. This project integrates state-of-the-art machine learning, digital signal processing, and information retrieval techniques. <br/><br/>If successful, the platform will be able to listen to an audio signal and understand what it is listening to - as human listeners can identify and classify sounds. This innovative technology will be licensed to audio and music technology software and hardware manufacturers. The platform is suited for long-term discoveries and innovation, with demonstrated commercial interest from biomedical signal processing, security/surveillance, and interactive gaming companies. In the first chosen market, (sound engineering) the platform will have direct cultural benefits for musicians, music hobbyists, and audio engineers. It will allow music creation and audio production to become a completely creative task - minimizing the tedious technical issues that hinder the creative process, and lowering the barriers to entry for novice musicians and creative professionals."
"1016571","Fast First-Order Methods for Large-Scale Structured and Sparse Optimization","DMS","COMPUTATIONAL MATHEMATICS, OPERATIONS RESEARCH","09/01/2010","08/16/2010","Donald Goldfarb","NY","Columbia University","Standard Grant","Junping Wang","08/31/2014","$450,000.00","Garud Iyengar, Katya Scheinberg","goldfarb@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","MPS","1271, 5514","9263","$0.00","Algorithms for large-scale optimization have traditionally exploited<br/>sparsity and structure in problem data. Many important optimization <br/>problems today, such as those that arise in  statistical machine <br/>learning (ML) and in compressive sensing (CS) are extremely large-scale convex<br/>problems with completely dense and/or unstructured problem data. <br/>However, there is often sparsity and structure in the solutions to <br/>these problems. The goal of this research project is the development of<br/>first-order algorithms, including gradient methods for non-smooth functions,<br/>smoothed penalty methods for constrained problems, multiple splitting methods,<br/>alternating-direction augmented-Lagrangian methods, and<br/>block coordinate descent methods, for extremely large-scale convex <br/>optimization problems that take advantage of solution structure and/or <br/>sparsity. Rigorous convergence analysis for these methods will be provided and<br/>robust software implementations will be developed. Although these <br/>methods are expected to have wide applicability, the focus will be on <br/>applications in CS and ML. Specifically, the investigators propose to <br/>develop  and analyze new scalable algorithms for (i) CS signal <br/>recovery, including algorithms that are able to exploit more detailed <br/>a priori knowledge in addition to sparsity; (ii) matrix rank minimization, the <br/>matrix analog of CS, and its variants; and (iii) a broad array of ML problems that exploit <br/>the special sparsity/structure of the solutions to these <br/>problems.<br/><br/>The research that is proposed under this grant is focused on the development of <br/>algorithms with provable performance guarantees that are capable of <br/>solving extremely large scale optimization problems whose solutions are <br/>either sparse or have special structure.  Such problems arise under the paradigm of <br/>compressive sensing, which allows signals (e.g., radar) and images <br/>(e.g., CT and MRI scans) to be obtained with far fewer measurements <br/>than predicted by traditional theory, various extensions of CS, and in <br/>a broad array of problems in machine learning. All of these problems are <br/>aimed at extracting a ""sparse"" or low-dimensional true model from a <br/>high dimensional or dense empirical model or data. They have important applications <br/>in extracting information from surveillance video and hyper-spectral images, face <br/>recognition, medical imaging and data mining,as well as many other areas <br/>of strategic interest such as national security and biotechnology."
"1036311","BETTER PLUS: Building Excellence in Teaching, Training, Education and Research","HRD","HIST BLACK COLLEGES AND UNIV","09/15/2010","09/14/2010","Mickey Burnim","MD","Bowie State University","Standard Grant","Claudia M. Rankins","08/31/2012","$349,979.00","Sadanand Srivastava, Nelson Petulante, Elaine Davis, Joan Langdon","mlburnim@bowiestate.edu","14000 Jericho Park Road","BOWIE","MD","207159465","3018604399","EHR","1594","9178","$0.00","Bowie State University's (BSU) project entitled: Building Excellence in Teaching, Training, Education and Research, will incorporate improvements and enhancements of STEM undergraduate education  based on lessons learned,  an evaluation of the strengths and accomplishments of the existing program, and an assessment of where BSU  needs to go to meet its existing needs. The project will expand outreach, teaching, and mentoring activities through the following three objectives: 1) redesigning gate keeper courses, curricular development, revision and enhancement reform, 2) expanding pathways for undergraduate recruitment, enrichment, retention and research experiences, and 3) strengthening faculty development activities. The goal is to improve all STEM programs at BSU by addressing existing weaknesses and by strengthening the research infrastructure <br/>The project will establish partnerships with industries and other academic institutions to increase research opportunities for BSU faculty and students.  The project will continue to serve as a beacon to the community, school teachers, students, and industry, putting the spotlight on the acute need for highly qualified STEM graduates. Partnerships with local industries, K-12 teachers, parents, and students, and community agencies will be established, to broaden the participation of minority students in STEM.  The interdisciplinary teaching curriculum and research infrastructure will be improved, which will increase the production of STEM graduates."
"1013624","SBIR Phase I: System for Location-Based Mobile Consumer Analytics","IIP","SMALL BUSINESS PHASE I","07/01/2010","12/06/2010","Thaddeus Fulford-Jones","MA","Cadio Inc","Standard Grant","Errol Arkilic","06/30/2011","$180,000.00","","thaddeus@locately.com","38 Ossipee Rd, Suite 2","Somerville","MA","021441610","6174478340","ENG","5371","5371, 6850, 9139, HPCC","$0.00","This Small Business Innovation Research (SBIR) Phase I project aims to develop a data mining system to analyze semi-continuous GPS data generated by consumer mobile devices. The system will thereby detect emergent patterns and draw inferences about each consumer's behavior, preferences, and lifestyle for market research. The proposed data mining system would utilize state-of-the-art pattern recognition and machine learning techniques to dynamically process and interpret GPS data. The objectives of this proposal are, first, to develop and deploy a scalable, extensible database of collected and processed location data from opted-in mobile consumer devices, second, to develop a machine learning system to classify consumer behaviors, third, to develop a real-time visit detection engine that triggers an action based in part on an individual's dwell time within a geofenced zone, and fourth, to evaluate and refine this system by conducting a one-month pilot study with GPS data. If successful, this research will prove the feasibility of a system that can draw inferences about consumer behavior by analyzing semi-continuous GPS data.<br/><br/>Analysis of consumer behavior using electronically-derived location data can both supplement and contextualize existing market research methods, thereby providing quantitative actionable inferences to retailers and brands. If this research effort is successful, the proposed system would allow businesses to more efficiently and accurately conduct consumer-focused market research. Such a system would address a broad range of market research opportunities, from shopper loyalty research to store siting to marketing effectiveness measurements. Recent changes in the marketplace indicate that market and technology conditions are now favorable for the development of the proposed data mining system. Specifically, the accelerating penetration of GPS-equipped mobile phones is accompanied by a growing need for brands and retailers to more robustly justify marketing spend and business decisions by using verifiable analytics that go beyond self-reported survey data. Additional future impacts of the proposed effort include the ability to combine GPS-derived mobile consumer analytics with Geographic Information Systems for improved public safety, municipal planning and transit systems design."
"1048515","EAGER: Investigating Diversity in Online Community Filtering","IIS","HCC-Human-Centered Computing","08/01/2010","08/07/2010","Rachel Greenstadt","PA","Drexel University","Standard Grant","William Bainbridge","07/31/2012","$95,822.00","Jennifer Rode","greenstadt@nyu.edu","1505 Race St, 10th Floor","Philadelphia","PA","191021119","2158955849","CSE","7367","7916","$0.00","A transition is occurring from a world in which gatekeepers and editors filter content before it is published to a world full of user-generated content in which information filtering is done after publication. Today's online communities have developed a variety of community-based filtering and rating mechanisms to help maintain quality and manageability. However, it is an open question  whether these filtering mechanisms represent ""the wisdom of crowds"" or ""the censoring mob.""<br/><br/>This project will apply statistical machine learning and ethnographic studies to understand the mechanisms by which online communities censor content from the bottom up. This understanding will provide insight into how values are and can be embedded into these large, socially intelligent systems. Ultimately, the goal is to design socially intelligent community filtering systems in which individuals, communities, and intelligent software agents collaborate, to explain the mechanisms behind social, bottom-up filtering, and expand the range of the possible in terms of the values these systems can reflect and the communities it can serve. This project will study the mechanisms through which the social construction of gender impacts community filtering systems. This will be done via an in-depth study of two online communities that have vigorous community policed comment filtering; one whose participants are predominantly male and another whose participants are predominantly female.<br/><br/>Online communities are rapidly becoming the modern public square and community filtering has the potential to make the space vibrant and useful and/or degenerate into a form of censorship. The health of our civil society and its ability to address large challenges depends on the health of its public discourse. By creating systems for socially intelligent filtering that reflect the community we facilitate diversity, in that minority positions are protected and preserved, while at the same time majority positions have the opportunity to develop and refine cogent arguments necessary for a well reasoned debate."
"1045306","The Second Workshop on Large-Scale Data Mining: Theory and Applications","IIS","Info Integration & Informatics","07/15/2010","07/13/2010","Christos Faloutsos","PA","Carnegie-Mellon University","Standard Grant","Sylvia Spengler","06/30/2011","$10,000.00","","christos@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7364","7364","$0.00","This project will provide travel fellowships to support US students to <br/>attend the 2nd workshop on Large-scale Data Mining: Theory and Applications <br/>(LDMTA) in conjunction with KDD 2010 on July 25, 2010 in Washington DC. The <br/>LDMTA workshop will cover topics on  scalable machine learning and data mining <br/>algorithms, and their applications, such as medical informatics, <br/>telecommunications, social network analysis, and e-commerce. PIs aim at <br/>investigating the scalability and efficiency of existing machine learning and <br/>data mining algorithms with respect to both theoretical and experimental <br/>perspectives. <br/><br/>Due to the recent data explosion in many applications, governments and <br/>companies can easily collect data spanning terabytes, petabytes or more. <br/>Several traditional data mining algorithms need to be replaced, or drastically <br/>re-designed, to handle such volumes through parallel architecture such as <br/>MapReduce/Hadoop. The focus of the workshop is exactly to bridge the gap <br/>between theory and practice of data mining, focusing on the fundamental <br/>research on large-scale data mining theory and applications. The goal of this <br/>workshop is to assemble the leaders in data mining research and industry to <br/>present their views on the need and challenges for large-scale data mining and<br/>also attract graduate students to study on large-scale data mining.<br/>The proposed student support will attract the US students to participate in <br/>this workshop and present their works on topics related to large-scale data <br/>mining, to encourage them to focus on the extremely promising research <br/>direction of large-scale data mining as their thesis research. In particular,<br/>PIs will try to broaden the involvement of female and minority students by<br/>prioritizing the award to them. <br/><br/>For further information about this project see the project website at <br/>http://arnetminer.org/LDMTA2010"
"0965616","Collaborative Research: GOSTRUCT: modeling the structure of the Gene Ontology for accurate protein function prediction","DBI","ADVANCES IN BIO INFORMATICS","06/01/2010","02/15/2012","Karin Verspoor","CO","University of Colorado at Denver","Standard Grant","Peter McCartney","05/31/2015","$288,384.00","Karin Verspoor","Karin.Verspoor@ucdenver.edu","MS F428, AMC Bldg 500","Aurora","CO","800452570","3037240090","BIO","1165","9178, 9179, 9183, 9184, BIOT","$0.00","Colorado State University is awarded a grant to develop machine learning methods for predicting protein function.  The availability of protein function annotations supports the everyday work of biologists in multiple areas---from biomedical discovery to the study of plant drought resistance, and the design of bacteria useful in biofuel production.  Assigning function to proteins in sequenced genomes is a major undertaking, and with new organisms being sequenced daily, experimentally determining the function of all the proteins in those organisms is not practical, requiring computational assignment of function to proteins that have not been studied in the lab.  Computational scientists have been considering the problem of function prediction for over two decades.  Yet, the basic methodology for protein function prediction has not changed much during this time and remains that of ""annotation transfer"" from proteins with a known function using a method for sequence comparison such as BLAST.  Protein function prediction has several properties that make it difficult to apply state-of-the-art machine learning methods to this problem, such as the large number of potential functions (thousands of possible terms), the fact that proteins can have multiple functions, and the hierarchical relationship between terms in the Gene Ontology (GO), which is the standard system of keywords used to describe protein function.  In this work the problem of annotating proteins with GO terms will be explicitly modeled as a hierarchical classification problem using the methodology of ""kernel methods for structured outputs"", which allows the modeling of complex prediction problems.  This methodology will allow the PIs to integrate a variety of genomic information - sequence data, gene expression, protein-protein interactions, and information mined from the biological literature.  The award will lead to a function prediction method with state-of-the-art accuracy.  The project will have broad impact by providing the GOstruct method to the bioinformatics and biology communities in the form of downloadable software and an online-accessible function prediction server. Education will be impacted through the incorporation of the tool into new courses in programming for biologists and on kernel methods."
"0965768","Collaborative Research:  GOSTRUCT:  modeling the structure of the Gene Ontology for accurate protein function prediction","DBI","ADVANCES IN BIO INFORMATICS","06/01/2010","06/10/2010","Asa Ben-Hur","CO","Colorado State University","Standard Grant","Peter McCartney","05/31/2015","$523,303.00","","asa@cs.colostate.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","BIO","1165","9178, 9179, 9183, 9184, BIOT","$0.00","Colorado State University is awarded a grant to develop machine learning methods for predicting protein function.  The availability of protein function annotations supports the everyday work of biologists in multiple areas---from biomedical discovery to the study of plant drought resistance, and the design of bacteria useful in biofuel production.  Assigning function to proteins in sequenced genomes is a major undertaking, and with new organisms being sequenced daily, experimentally determining the function of all the proteins in those organisms is not practical, requiring computational assignment of function to proteins that have not been studied in the lab.  Computational scientists have been considering the problem of function prediction for over two decades.  Yet, the basic methodology for protein function prediction has not changed much during this time and remains that of ""annotation transfer"" from proteins with a known function using a method for sequence comparison such as BLAST.  Protein function prediction has several properties that make it difficult to apply state-of-the-art machine learning methods to this problem, such as the large number of potential functions (thousands of possible terms), the fact that proteins can have multiple functions, and the hierarchical relationship between terms in the Gene Ontology (GO), which is the standard system of keywords used to describe protein function.  In this work the problem of annotating proteins with GO terms will be explicitly modeled as a hierarchical classification problem using the methodology of ""kernel methods for structured outputs"", which allows the modeling of complex prediction problems.  This methodology will allow the PIs to integrate a variety of genomic information - sequence data, gene expression, protein-protein interactions, and information mined from the biological literature.  The award will lead to a function prediction method with state-of-the-art accuracy.  The project will have broad impact by providing the GOstruct method to the bioinformatics and biology communities in the form of downloadable software and an online-accessible function prediction server. Education will be impacted through the incorporation of the tool into new courses in programming for biologists and on kernel methods."
"1059283","EAGER: Foundations for Predictive Resource Management in Next-Generation Multicore Processor Systems","CCF","COMPUTER ARCHITECTURE","09/15/2010","09/15/2010","Sangyeun Cho","PA","University of Pittsburgh","Standard Grant","Hong Jiang","08/31/2012","$100,000.00","","cho@cs.pitt.edu","300 Murdoch Building","Pittsburgh","PA","152603203","4126247400","CSE","7941","7916, 9218, HPCC","$0.00","Future multicore processor systems will have a growing amount of system-wide shared resources. However, shared resources will present significant undesirable asymmetry. For example, the capabilities of processor cores, cache access latency, and memory access cost will differ depending on the time and the location of their usage. If such asymmetry is not properly managed, the full potential of the multicore computing paradigm will not be achieved. This exploratory research will investigate a novel predictive resource management framework called MAESTRO. The proposed framework automatically learns asymmetry in the system and useful application behavior; the learned knowledge is accumulated and refined; and resource management decisions, such as cache capacity allocation, are made in a predictive manner by exploiting the accumulated knowledge. It is expected that MAESTRO's predictive strategies with detailed system and application knowledge will be a more effective solution to new multicore resource management problems than conventional reactive strategies with limited knowledge. The PI will validate this expectation with solid system prototyping and by studying two target resource management problems.<br/> <br/>The project has the potential to impact the way future computer systems are designed and managed. It is inter-disciplinary by nature and requires understating of applications, computer architecture, OS and machine learning. Students working on this project will receive rigorous inter-disciplinary training."
"1050448","III: EAGER: Data Integration as a Dialogue with the User","IIS","Info Integration & Informatics","09/01/2010","08/24/2010","Zachary Ives","PA","University of Pennsylvania","Standard Grant","Frank Olken","08/31/2012","$150,000.00","","zives@cis.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","CSE","7364","7916","$0.00","This work establishes a new approach to providing ad hoc (""discovery"") queries requiring integration and structuring:  such queries help scientists learn possible relationships between topics, and help decision-makers or consumers explore options.  The work develops a new system and underlying architecture based on an iterative process, where the system and user engage in a dialogue until the user has answers meeting his or her information need. <br/><br/>The resulting system takes sources on the Web, discovers semantic relationships among them, and allows users to pose discovery queries.   It leverages existing extraction, matching, and recommendation algorithms as sources of evidence to generate hypotheses and corresponding queries, and adjusts these hypotheses based on user feedback over the query results.  Innovations include scalable models for combining features and learning to re weight hypotheses; query and source recommendation techniques; and means of generalizing tuple-based feedback to support or refute hypotheses. <br/><br/>The research impact is a new paradigm for data integration by end users, which scalably combines machine learning and database concepts.  The broader impact includes better discovery tools for scientific users and other users who sorely need them; improved integration of existing Web data resources; and new educational material on how networks of data can be as important as networks of systems and people.  The PI is incorporating the research concepts into courses in the University of Pennsylvania's new Market and Social Systems Engineering Program, focused on the interface between people, protocols, and systems on the Internet, especially through social and data networks, as well as markets.  More information on the project can be found on the project website at http://www.cis.upenn.edu/~zives/dialogue/"
"1002180","Sparsity-Aware RF Cartography for Cognitive Networks","ECCS","COMMS, CIRCUITS & SENS SYS","06/15/2010","06/21/2010","Georgios Giannakis","MN","University of Minnesota-Twin Cities","Standard Grant","Lawrence S. Goldberg","08/31/2014","$391,707.00","","georgios@umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","ENG","7564","","$0.00","ECCS-1002180<br/>Georgios Giannakis<br/>University of Minnesota<br/>Sparsity-Aware RF Cartography for Cognitive Networks<br/><br/>ABSTRACT<br/><br/>Intellectual Merit: Wireless cognitive radio (CR) technology holds great promise to address fruitfully the perceived dilemma of bandwidth under-utilization versus spectrum scarcity, which has rendered fixed-access communication networks inefficient. Accordingly, the need arises for fundamental research in critical cognition infrastructure to sense, learn, and adapt to the environment CR networks operate. This proposal aims to develop this infrastructure for comprehensive situation awareness through the novel notion of RF cartography. Paralleling the success of routing tables, the vision is to jointly utilize interference and channel gains maps to identify opportunistically available bands for re-use, and handoff operation; localize and track user activities; as well control resource allocation and routing decisions.  The approach draws from contemporary advances in sparsity-aware regression, compressive sampling, basis expansions, spline interpolation, and kriged Kalman filtering. The project leverages these tools to investigate: (a) distributed, online, and adaptive algorithms for map estimation and tracking; (b) training-based and blind cartography options; (c) spatio-temporal spectrum re-use, and localization in the presence of multipath and shadowing effects; and, (d) cartography-driven network utility maximization for optimal cross-layer design of CR networks.<br/><br/>Broader Impacts: This research is of interest to software radio designs with IEEE 802.11 compliant standards. In a broader sense, advances in sparsity-aware regression, kriging, splines, and spectrum cartography will permeate benefits to a gamut of areas as diverse as machine learning and data mining for social networks, dynamic magnetic resonance imaging, surveillance using wireless sensor networks, as well as navigation and safety systems. Cognition in networking and localization applications will further provide meaningful experiences to undergraduates and integration of the expertise gained to enhance the content of graduate classes. Outreach to the government and industrial sectors will be possible through short courses, tutorials in workshops and conferences, and student-faculty-staff collaboration."
"1025177","Multi-Source Visual Analytics","CCF","GRAPHICS & VISUALIZATION, FOUNDATIONS VISUAL ANALYTICS, MSPA-INTERDISCIPLINARY","08/01/2010","07/31/2010","Jieping Ye","AZ","Arizona State University","Standard Grant","Sankar Basu","07/31/2014","$498,485.00","Anshuman Razdan, Peter Wonka","jpye@umich.edu","ORSPA","TEMPE","AZ","852816011","4809655479","CSE","7453, 7703, 7454","7703, 9218, 9215, HPCC","$0.00","Abstract<br/><br/>Data visualization forms an important aspect of analysis in the field of visual analytics. Analysts rely on visual tools to process massive data sets and discover meaningful patterns in the data. A common strategy for many visualization tools is to transform high-dimensional data to an intermediate lower-dimensional space and then project to screen space using a visualization transformation. For example, a data set with 200 dimensions can be transformed to an intermediate 4D representation and then mapped to screen space by using two-dimensions<br/>for the location and two dimensions to determine shape and color. Therefore, the mathematical foundations of visualization are closely related to the problem of dimensionality reduction.<br/>While dimensionality reduction is a necessary step to visualize the data, the final goal of visual analytics is data analysis, such as searching, clustering, and the detection of outliers. Therefore, there is an urgent need to study dimensionality reduction techniques that are especially useful for data analysis. <br/><br/>This research involves the development and implementation of linear and nonlinear dimensionality reduction algorithms for the transformation and visualization of high-dimensional data. The novel aspect of the transformation is that dimensionality reduction and clustering are performed simultaneously in a joint framework. In addition, this research involves the development and implementation of novel algorithms for multi-source data transformations based on multiple kernel learning (MKL). This addresses the question of fusing a multitude of heterogeneous independently collected data. In the past, most research on MKL has focused on supervised learning. One major contribution of this research is to extend MKL to the unsupervised case. This research presents visual analytics as a bridge between theoretical foundations in machine learning and real-world applications. This research is utilizing two testbed data bases, one consisting of printed documents as might be used by the intelligence community and one based on public health information."
"0952918","CAREER: The Dynamics of Collective Intelligence","IIS","Robust Intelligence","06/01/2010","01/25/2010","Sanmay Das","NY","Rensselaer Polytechnic Institute","Continuing Grant","Edwina L. Rissland","11/30/2012","$288,082.00","","sanmay@wustl.edu","110 8TH ST","Troy","NY","121803522","5182766000","CSE","7495","1045","$0.00","This project studies the design of information systems like wikis and information markets. Research in social science has established that often there is a ""wisdom of the crowd"" -- i.e., collectives can display more intelligence than the individuals they are composed of. When such collective information systems work, they serve as superb aggregators and disseminators of information. However, fundamental computational challenges remain in understanding how to design them optimally.<br/><br/>This research is advancing along several lines, including<br/><br/>(1) general theories of how information is aggregated in different social media, developed and validated using real data gathered from existing databases and generated from user experiments; <br/><br/>(2) algorithms for facilitation of user interactions so that the medium in question can deliver the promised results (for example, market-making algorithms for liquidity provision in information markets);<br/><br/>(3) theoretical and practical characterization of the possibilities for rogue users to manipulate collective wisdom systems;<br/><br/>(4) algorithms for detecting malicious users, and mechanisms that thwart miscreants. <br/><br/>The research is naturally interdisciplinary in nature, drawing from machine learning and probabilistic reasoning, data mining and social networks, as well as finance and economics. It contributes to our understanding of complex social phenomena like the growth of information in wikis and blogs, as well as to the development of intelligent reasoning algorithms for agents in complex, uncertain multi-agent environments like markets.<br/><br/>The design of agents that participate in markets and social systems improves the quality of online markets and improves information flow in virtual spaces. Further, insights gained from modeling market structures and social spaces can tell us how to design them better. For example, understanding the impact of different levels of central control on wiki articles or open source software projects yields guidelines for how much central control is optimal in different settings.<br/><br/>In a world where computation and social systems are increasingly intertwined, the PI's research and education program exposes students to multidisciplinary ideas through the introduction of a new class on collective intelligence, social networks and e-commerce, and the development and extensive use of the very objects of study -- information markets and wikis -- in classroom and lab settings. The PI is also developing an experimental project for putting freely accessible course wikis online, similar to online course materials at other universities, but open to editing by the community."
"1026078","Workshop: 10th International Workshop on Tree Adjoining Grammars and Related Formalisms (TAG+10)","IIS","ROBUST INTELLIGENCE","06/01/2010","06/07/2010","Robert Frank","CT","Yale University","Standard Grant","Tatiana D. Korelsky","05/31/2011","$15,000.00","","bob.frank@yale.edu","Office of Sponsored Projects","New Haven","CT","065208327","2037854689","CSE","7495","7495","$0.00","The past two decades have witnessed the exploration of a range of grammatical formalisms by computational, theoretical, and psycho linguists, for their utility in building natural language interfaces and machine translation systems, characterizing the nature of human linguistic knowledge, and constructing models of language processing.  Because many of these formalisms share important formal and linguistic properties (most prominently, mild context-sensitivity and lexicalization), there are many potential synergies, computational, theoretical and psychological, that can be gotten by considering ideas that stem from work outside of particular formalism.   Moreover, though theoreticians, computationalists, and psychologists are concerned with solving different problems, ideas that derive from one community often turn out to have a significant consequences for the others.  <br/><br/>The goal of this NSF-sponsored workshop, which will take place at Yale University on June 10-12, 2010, is to foster both of these types of connections: across the formalism divide and the theoretical-computational-psycho divide.   The Tree-Adjoining Grammar community has a history of exploring these connections, and this workshop aims to expand the community of researchers involved in such cross-pollination even further.  The workshop will bring together researchers from the Tree-Adjoining Grammar, Minimalism, Categorial Grammar, Dependency Grammar, HPSG,  and LFG  communities to look at the similarities and differences of the formalisms, with the goals of developing shared, broad-coverage grammars, transferring parsing and machine learning algorithms from one formalism to another. and gaining new insights into the properties of different formalisms and their capacity for linguistic and psycholinguistic explanation. <br/><br/>This award provides support crucial to attract to the workshop not only prominent researchers, who will give invited presentations and tutorial lectures, but also, and perhaps even more importantly, PhD students.  By introducing junior researchers to the fruitfulness of cross-framework and cross-disciplinary interactions at an early stage in their careers, our hope is that the award will have a transformative effect on the kind of work they will engage in during their entire careers, potentially leading to a broader, more integrated perspective in the field at large."
"0968841","Collaborative Research: PSERC Collaborative Proposal for a Phase III Industry University Cooperative Research Center Program","IIP","INDUSTRY/UNIV COOP RES CENTERS, ","03/01/2010","12/10/2013","Venkataramana Ajjarapu","IA","Iowa State University","Continuing grant","Raffaella Montelli","02/29/2016","$703,000.00","","vajjarap@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","ENG","5761, J415","1049, 122E, 170E, 5761","$0.00","PSERC is proposing a Phase III support (third five-year period) for the Center, including support for center personnel, evaluation and research.  The proposed Center is a multi-university Center comprised of the following universities: Arizona State University (lead), Cornell University, Texas A & M, Howard University, Washington State University, the University of Wisconsin-Madison, Iowa S ate University, Wichita State University, Georgia Tech, the University of Illinois at Urbana Champaign, the University of California-Berkeley, and Colorado School of Mines.<br/> <br/>The proposal from PSERC seeks funding for a Phase III (3rd five-year period) led by Arizona State University (ASU). PSERC had its origin as a multi-university IUCRC in 1996, and was created with the vision that collaboration amongst a large group of academic, industrial, and governmental institutions can develop solutions to the complex problems in electric energy. PSERC (currently comprised of 12 universities) will conduct research organized under three primary research stem areas: markets stem, system stem, and transmission and distribution technologies stem. PSERC leverages the expertise of some 40 multidisciplinary researchers. The PI and all of the other site directors are very well-qualified and have access to appropriate and adequate resources.<br/><br/>The proposed work addresses important improvements and advancements in the transmission and delivery of electricity. PSERC has been a leader in in developing several key concepts with regard to the development of electricity markets. PSERC researchers also developed one of the first large scale visualization tools for the electric power industry, and this effort has resulted in the creation of a company that has commercially developed the tool. Over the past six years, PSERC member companies have employed an average of 80 graduate and undergraduate students per year from PSERC universities. In the area of diversity, one of the PSERC member universities is Howard University, a historically black university. PSERC will continue to seek women and minorities through students and faculty at member universities as well as on an individual basis as research associates."
"1018733","RI: Small: Probabilistic Models for Reconstructing Ancient Languages","IIS","LINGUISTICS, ROBUST INTELLIGENCE","08/15/2010","08/16/2010","Dan Klein","CA","University of California-Berkeley","Standard Grant","Tatiana Korelsky","07/31/2014","$460,143.00","Thomas Griffiths","klein@cs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","1311, 7495","7923","$0.00","One of the oldest problems in linguistics is to reconstruct ancient protolanguages on the basis of their modern descendants. Identifying ancestral word forms makes it possible to evaluate proposals about the nature of language change and to draw inferences about human prehistory.  Currently, linguists painstakingly reconstruct protolanguages by hand, using knowledge of the relationships between languages and the plausibility of sound changes.  This research project develops statistical, computational methods that automate or augment the reconstruction process.  Unlike past computational approaches, these new models use detailed phonological representations to infer hidden sound changes. Moreover, they automatically infer which words are co-descendent (cognates).  <br/><br/>These advances, combined with new algorithms for large-scale statistical inference, enable the analysis of orders of magnitude more data than prior work.  The models from this project significantly expand the computational tools available to linguists; large-scale reconstructions make it possible to collect quantitative data to help answer long-standing questions about language change.  Beyond word reconstruction, the models and tools from this project will be useful for other related applications, such as machine translation, where reconstructions can be used to fill gaps in the mapping between the vocabularies of different languages, and the alignment of biological sequences, which requires considering which regions in those sequences are co-descendent.  In addition, the technical advances in probabilistic modeling and approximate inference methods will have cross-cutting implications for a range of modeling problems in computational linguistics, bioinformatics, statistics, machine learning, and cognitive science."
"1007801","Statistical Methods for High Dimensional Discrete Data","DMS","STATISTICS","06/01/2010","06/18/2012","Naomi Altman","PA","Pennsylvania State Univ University Park","Continuing grant","Gabor Szekely","05/31/2015","$200,000.00","","nsa1@psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","MPS","1269","","$0.00","Very high dimensional count and binary data are now common in many fields including machine learning, imaging and marketing.  In high-throughput biology, ultra-high thoughput sequencing technologies which produce count and categorical data are displacing microarrays and other ""omics"" measurement devices. The output  of these measurement devices are counts per gene or other biological subunit for tens of thousands of responses per sample, or presence/absence for features such as single nucleotide polymorphisms (SNPs), for possibly millions of responses per sample.  Similar data can be derived on for features on satellite images, medical scans, monitoring devices and other very high dimensional measurement devices.  The investigator will extend highly multivariate and multiple testing methods developed for continuous (primarily normally distributed) data to discrete data.   New methods will be developed in four areas: A)  analyses for differences in distribution for discrete data that can accommodate complex experimental designs using generalized linear mixed models with overdispersion and Bayesian or empirical Bayes shrinkage.  B)  methods for supervised clustering of samples and variables in the discrete data setting taking into account the error structure of the discrete predictors.  C) classical and sufficient dimensions reduction methods such as canonical correlation and sliced inverse regression for discrete data.  D) extension of concepts and methods in multiple testing, such as false discovery rate estimation to the discrete setting in which the p-values from independent or weakly dependent tests may have different null distributions using conditional mixture modeling.  The methods will be tested on genomics and imaging data.<br/><br/>Very highly multivariate data are now the norm in fields as diverse as cell biology, marketing, medical and satellite imaging, meteorology, epidemiology,<br/> fraud detection and cancer research.  These data may include thousands or millions of measurements on each item in the sample.  For example, genotyping <br/>services provide individuals with information on hundreds of thousands of genetic variants in their cells and retailer databases may have information on <br/>the sales of tens of thousands of items for each store in the chain. Many of these data come in the form of counts (such as number of items of each type <br/>in inventory, number of mRNA molecules encoding a particular protein) or in the form of categories (such as on/off, present/absent, or genotype AA, aa <br/>or Aa).  Methodology for highly multivariate continuous measurements such as blood pressure and temperature are well-developed but do not apply directly <br/>to count and categorical data.  The investigator will develop statistical methodology and software to improve analysis and summary of count and categorical data.  Four main areas of research are proposed:  A) statistical models and tests to determine if the variables are associated with differences among groups; B) statistical methods for prediction or classification of group membership; C) methods to summarize the data with a much smaller set of derived variables which preserve the predictive power of the full data and D) multiple comparisons methods to estimate the error rates.   For example, in a study of the genes associated with metastatic versus non-metastatic cancer, the methods could be used to determine which genes express differently in tumors which did or did not advance to metastasis, select a smaller set of genes which could be used as a diagnostic tool and then provide convenient  summaries which can readily be interpreted by clinicians.  In a study of stresses on a machine part, the pixels of scans of the part before and during  the application of the stresses could be used to determine precise locations at which the part might fail and differences among features of the scan  between parts which fail at low versus high stress.  In studies in which a large number of models are fitted or tests conducted, it is necessary to tolerate a small percentage of errors.  Concepts and methods in multiple testing which have been developed for continuous data will be extended to assist in estimating and controlling the number of false conclusions with count and categorical data."
"1017961","SHF: Small: Exploring Statistical Models to Optimize Hardware and Software under Processor Reliability Constraints","CCF","COMPUTER ARCHITECTURE, EPSCoR Co-Funding","08/01/2010","08/10/2010","Lu Peng","LA","Louisiana State University","Standard Grant","tao li","07/31/2015","$374,424.00","Bin Li","lpeng@lsu.edu","202 Himes Hall","Baton Rouge","LA","708032701","2255782760","CSE","7941, 9150","9150, 9215, 9218, HPCC","$0.00","With technology scaling coupled with increasing power densities, modern processors suffer from potential soft errors and hard errors. The reliability analysis of such multi-threaded processors, e.g. Simultaneous Multithreading (SMT) and Chip-Multiprocessors (CMP), where inter-thread resource contention exists, is a relatively unexplored area. Furthermore, the modeling complexity is exacerbated by two additional factors: (1) increasing number of cores in a chip; and (2) heterogeneity brought by manufacturing process variation. Software wise, traditional compiler designs are aimed at providing high performance and recently low power when generating object codes. With increasing hardware vulnerabilities, however, high performance computing programs suffer from unexpected errors and exceptions, which might be mitigated by using fault-tolerance techniques such as error detections and check pointing, but still eventually hurt their performance. Apart from a reliable hardware platform, software designers can further improve system reliability by generating error resilient codes. Moreover, analysis of software's architectural vulnerability is still in an ad hoc stage. Therefore, this project proposes a predictive framework to handle the above challenges by employing modern statistical and machine learning methods. The outcomes of this project include a predictive framework which guides for reliable software and hardware optimization and its applications to high performance computing.<br/><br/>The broader impact plans include outreach activities and undergraduate and graduate training. The interdisciplinary nature of the proposed work allows students to learn cutting-edge knowledge from different areas to broaden their scope of training as well as to enhance their productivity. Students from the under-represented groups will be encouraged and given priorities for joining the project."
"1005175","REU Site:  EcoInformatics Summer Institute","OAC","RSCH EXPER FOR UNDERGRAD SITES, , , ","09/01/2010","09/07/2012","Desiree Tullos","OR","Oregon State University","Continuing Grant","Almadena Chtchelkanova","08/31/2013","$296,784.00","Julia Jones","desiree.tullos@oregonstate.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","CSE","1139, J103, J243, K629","7736, 9215, 9250, HPCC","$0.00","The objectives of this REU Site are to (1) Train students from ecosystem, earth,  engineering, mathematics, and computer sciences in collaborative interdisciplinary research and professional development, (2) Forestall the flight of US citizens from STEM (science, technology, engineering, and mathematics) disciplines by recruiting qualified, diverse students to engage in ecological problems requiring computer, mathematical sciences and engineering input, and (3) Conduct research to develop and apply novel techniques in computer science, engineering, and mathematics to solve natural resource management problems, allowing informatics to enable the science and the science to enable tool development.  We will achieve these objectives through structured research experiences, thematic and professional development seminars, exchanges with other REU sites, immersive field experiences, and mentoring and development of collegial relationships.  Students will be recruited from ecology, geosciences, engineering, mathematics, and computer science. We will actively recruit Native, Black, and Hispanic Americans and will provide all students with high quality mentoring, graduate school fellowship opportunities, and, and career counseling so they will enter the work force equipped to make key contributions to natural resources management and policy. We intend to provide students with skills for today?s science environment where information technology and models are essential parts of ecosystem management.  <br/> <br/>Mentored by teams of interdisciplinary scientists, participants will address three key research themes: (1) regional climate change and vegetation ""landform"" atmosphere interactions using sensor networks, computer visualization, mathematical models, and field studies of climate and vegetation; (2) landscape-scale changes in species distributions using bioacoustics, machine learning, and large datasets of birds and insects; and (3) network-scale river restoration using fiber optic temperature sensors, physical models of channel hydraulics, and mathematical modeling.   Each theme integrates ecological principles, field experiences, novel engineering techniques, new developments in computer science, and mathematical modeling and problem solving.    <br/><br/>This REU site will serve as a national model for transforming the education of undergraduates to embrace Ecosystem Informatics (EI) as both a set of tools and a mindset for approaching and solving natural resources problems in an interdisciplinary context.   The activities will advance discovery while promoting training and learning in key social issues including climate change, ecological change, and restoration.  It will recruit and train underrepresented minorities, enhance infrastructure and programs at the Andrews Forest LTER and OSU Wave Lab, link with an IGERT and REUs at OSU, and disseminate management and policy implications to USFS partners.    <br/>"
"0964416","RI: Medium:  Active Scene Interpretation by Entropy Pursuit","IIS","ROBUST INTELLIGENCE","07/01/2010","06/15/2011","Donald Geman","MD","Johns Hopkins University","Continuing grant","Jie Yang","06/30/2014","$794,824.00","Rene Vidal, Laurent Younes","geman@jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","CSE","7495","7495, 7924","$0.00","This project develops a new strategy for scene interpretation, especially for annotating cluttered scenes with instances from many object categories (e.g., a kitchen scene) and videos of people interacting with objects in everyday life (e.g., cooking). The research team develops a statistical model for scene interpretations and image measurements. One component of the model is a prior distribution on a huge interpretation vector. Each bit of this vector represents a high-level scene attribute with widely varying degrees of specificity and resolution ? some are very coarse (general hypotheses) and some are very fine (specific hypotheses). The other component is a simple conditional data model for a corresponding family of learned binary classifiers, one per bit. The scene interpretation is then computed by assessing hypotheses in a highly coarse-to-fine manner, using an image parsing algorithm called ?entropy pursuit? based on stepwise uncertainty reduction, and classifiers for detecting events in spatiotemporal volumes which leverage on recent advances at the intersection of machine learning and dynamical systems.  The computational models and scene parsing algorithms developed in this project are broadly applicable to scene interpretation problems arising in many areas of science and engineering. Specific applications include home surveillance and security, assisted home living, infant and elderly care, etc. The project also provides research opportunities for graduate students in underrepresented minorities and even high school students."
"1017149","TC: Small: Collaborative Research: User-Centric Privacy Control for Collaborative Social Media","CNS","TRUSTWORTHY COMPUTING","09/01/2010","03/19/2013","H. Jagadish","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Sylvia Spengler","08/31/2015","$226,654.00","","jag@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7795","7923","$0.00","Social-networking sites (e.g., Facebook, MySpace, LinkedIn, etc.) and other online collaborative tools have emerged as places where people can post and share information.  This information-sharing has many benefits, ranging from practical  (e.g., sharing a business document) to purely social (e.g., communicating with distant friends). At the same time, information sharing inevitably poses significant threats to user privacy. In social-networking sites, for example, documented threats range from identity theft to digital stalking and personalized spam.  As a result, a growing number of such sites allow individual users to specify fine-grained policies that indicate who can access their data, and to what extent. However, studies have consistently shown that most end-users find the task of specifying access-control policies for their own data overwhelming; as a result, users often skip the process altogether.<br/><br/><br/>The goal of this project is to help collaborative and social-media users gain control of their data.  To that end, the project will include three main components: assisted specification, feedback, and refinement recommendations.  To assist users in initially specifying access-control policies for their data, the project will develop a ""privacy wizard,"" which employs data mining and machine learning methods, including active learning, to construct an accurate policy, with minimal input from the user.  To provide feedback regarding existing privacy settings, the project will pursue two approaches: aggregate scores and visualizations.  For example, an aggregate score can be used to concisely explain to the user how her settings differ from those of other users.   Preliminary work found that Item Response Theory (IRT) can be used effectively for this purpose.  Finally,  the project will consider how aggregate scores and visual feedback can be enriched with recommendations for refinements to help the user achieve an expressed level of social exposure.<br/><br/>Online collaborative tools and social media offer great promise in a number of arenas. In addition to communicating with friends via social networking sites, collaborative tools are now used in fields as diverse as business, medicine and education. However, the absence of usable privacy and access control prevents such tools from realizing their full potential.  Results of this project will be disseminated via prototype implementations, as well as research publications. New undergraduate and graduate curriculum modules will also increase awareness of the importance of policy-specification and emerging research in this area."
"0945236","SBIR Phase I:  Visual Information Delivery Robot for Visually Impaired Children","IIP","SBIR Phase I","01/01/2010","06/07/2010","Hyukseong Kwon","IN","EN'URGA INC","Standard Grant","Glenn H. Larsen","12/31/2010","$200,000.00","","hyukseon@purdue.edu","1201 CUMBERLAND AVE, Suite R","WEST LAFAYETTE","IN","479061359","7654973269","ENG","5371","1658, 5371, 9216, HPCC","$0.00","This Small Business Innovation Research (SBIR) Phase I project will evaluate the feasibility of a robotic system to detect and track a child's finger so that the scene that the child wishes to see can be displayed at high magnification on a monitor. There are two issues of intellectual merit that will be addressed during the Phase I work. The first issue is analyzing the pointing pattern of children. Since each person has his or her own pattern for raising an arm and pointing with a finger, the system requires a machine learning algorithm to adjust the decision from the system. This adaptive algorithm forms one of the key innovations that will be evaluated during the Phase I work. The second issue will be an adaptive zooming and scene segmentation algorithm. <br/><br/>There are two primary commercial applications for the proposed visual information delivery robot. The first involves the education of visually impaired children. Most of research and products currently available for visually impaired children are focused on learning while they are sitting on the chair in front of the computer monitor. However, the proposed system captures the scene that the child points to in any location, thus bringing a dynamic tool to education. It is anticipated that such tools will have a significant commercial potential in schools for the visually impaired. The second commercial application is in assisting visually impaired adults with enhanced dynamic information when they are in a wheelchair. The commercial and societal impact of the proposed project is that it will enable visually impaired children and adults to enhance their quality of life by adding a dynamic tool for visualizing near and far off objects. The algorithms developed during the Phase I research will also aid researchers in industrial automation with advanced robots."
"0964302","III:  Medium:  Collaborative Research:  Integrating Behavioral, Geometrical and Graphical Modeling to Simulate and Visualize Urban Areas","IIS","GRAPHICS & VISUALIZATION","09/01/2010","06/14/2011","Daniel Aliaga","IN","Purdue University","Continuing grant","Ephraim Glinert","08/31/2014","$449,818.00","Bedrich Benes","aliaga@cs.purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","7453","7924","$0.00","In this project, the PI and his team will develop a new simulation framework to interactively model and visualize socio-economic and geometric characteristics of urban areas.  The framework will consist of a synergistic collaboration of three different areas: behavioral urban modeling, probabilistic graphical modeling, and visualization and computer graphics.  In machine learning and statistics, the area of probabilistic graphical modeling offers a flexible framework to build, estimate and simulate from models of substantial complexity and scale, with partially observed data.  By accounting for uncertainty and interdependencies, including aspects of dynamic equilibrium that arise in modeling the complex spatio-temporal dynamics of urban areas, the PI argues there is significant potential for breakthroughs in modeling large-scale urban systems.  Similarly, by integrating behavioral and geometrical dimensions of urban areas, he expects to exploit the power of behavioral simulations more effectively by filling in geometric details that behavioral models are not well suited to manage, and at the same time provide a powerful framework to generate 2D and 3D geometric representations of urban areas that are behaviorally and geometrically consistent.  The PI will take advantage of massive datasets available for urban areas, including parcel and building inventories, business establishment inventories, census data, household surveys, and GIS data on physical and political features, and will fuse these data into a coherent and consistent database to support his modeling objectives.  This data fusion will address imputation of missing data, accounting for complex spatial and relational connections among the data sources.  The PI will evaluate the accuracy and usability of his system through several deployments in diverse contexts.  The PI has elicited engagement from the Urban Land Institute, the European Research Council, and the Council for Scientific and Industrial Research.  Several organizations in the San Francisco Bay Area in California and the Puget Sound region in Washington will serve as testbeds for the research. Finally, the PI will collaborate with other NSF-funded research projects, such as the Drought Research Initiative Network, in order to investigate correlations between urban development and water/drought. <br/><br/>Broader Impacts:  The results of this multidisciplinary project will have a transformative effect on the area of urban simulation, in that they will enable non-professionals as well as the general public to better understand urban phenomena.  City planners, researchers, students, and citizens will be able to efficiently simulate urban processes not previously possible, and to visualize the effects of adopting different urban policies on urban livability and sustainability outcomes, and to address local and global concerns regarding equity, infrastructure, and economic development.  The framework will provide interactive desktop and web-based interfaces for configuring urban scenario inputs to a simulation that may reach petabytes in data size, and to visualize the simulation results using 2D aerial views, 3D city walkthroughs, and choroplethic maps and tables of indicators portraying the simulated area.  Thus, the work will also advance the fields of visualization and computer graphics, through development of new techniques for large-scale urban modeling and rendering.  The PI will develop an open-source system to make the results of this research widely available."
"1031914","Doctoral Dissertation Research: The Development and Integration of Spatial Analyses for Search and Rescue Operations in Yosemite National Park","BCS","Geography and Spatial Sciences","09/01/2010","08/27/2010","Qinghua Guo","CA","University of California - Merced","Standard Grant","Antoinette WinklerPrins","08/31/2012","$12,000.00","Paul Doherty","qguo@ucmerced.edu","5200 North Lake Road","Merced","CA","953435001","2092012039","SBE","1352","1352, 9179, SMET","$0.00","Doctoral student Paul Doherty, under the supervision of Professor Qinghua Guo in the School of Engineering at the University of California Merced will develop a system designed to estimate the location of injured and/or lost people in Yosemite National Park based on historical data of where people have been injured and/or found in past rescue events.  This project has three research components.  The first component involves the process of georeferencing historic incidents from text-based information. This is a common challenge for users of geographic information (ecologists, historians, curators) who rely on spatial data to make decisions. A probability-field georeferencing technique will be used to map historic search and rescue incidents in Yosemite National Park. The second component of this project evaluates the geographic one-class data issue faced by geographers with presence-only test data. More specifically, a model whose parameters are defined by expert knowledge (helicopter managers) will be compared to a model that uses presence-only test points (known landing areas) to develop helicopter landing area suitability layers in Yosemite National Park. This will project will provide a novel testing scenario for dealing with presence-only or geographic one-class data using machine-learning algorithms. The third component of this project will address the problem of locating moving objects in spatiotemporal space. This entails creating a travel-cost layer that uses slope, vegetation density, and path network presence to estimate the time required to cross a known distance within Yosemite National Park. Once this has been completed and tested, an object-oriented model will be constructed to use this travel-cost layer to generate isochrones (time-distance rings). This will allow a user to enter a geographic point and time elapsed to determine the maximum distance a human can travel from a known location. <br/> <br/>The logical problem to be addressed by this research is an entirely spatial one, ""how can developments in geography and spatial sciences help rescuers find and rescue visitors in Yosemite National Park more effectively?"" In essence, this research will develop tools for emergency service providers to better do their job in three ways: enable institutional knowledge from incident history in a spatially-enabled digital library format, generate a visualization tool for emergency communication dispatchers and helicopter pilots to locate landing areas, and give search managers a tool for defining the outer limits of their search area when persons go missing. The classic dilemma of search and rescue is a puzzle that professional rescuers, their rescuees, and loved-ones take very seriously. Geographic Information Systems can provide a platform for spatial analyses to help solve problems so that others may live. This research will introduce science-based techniques to provide society with safer and more effective rescue techniques."
"0940575","BPC-LSA: Scaling and Adapting Computing Alliance of Hispanic-Serving Institutions (CAHSI) Initiatives","CNS","SPECIAL PROJECTS - CISE, BROADENING PARTIC IN COMPUTING","02/01/2010","12/21/2011","Miguel Alonso Jr","FL","Miami Dade College","Continuing grant","Janice Cuny","01/31/2014","$970,219.00","Rocio Guillen-Castrillo, Danmary Albertini, Andres Figueroa, James Poe","malonsoj@fiu.edu","300 NE 2nd Avenue","Miami","FL","331322204","3052373910","CSE","1714, 7482","9218, HPCC","$0.00","Miami Dade College, California State University San Marco, and the University of Texas Pan-American propose a program-called SACI for ""Scaling and Adapting Computing Alliance of Hispanic-Serving Institutions (CAHSI) Inititatives"" which will join with CAHSI and apply CAHSI best practices to advance student success among Hispanics and other groups underrepresented in computing fields. CAHSI is an NSF-funded Alliance with seven universities that has developed and demonstrated the effectiveness of a series of initiatives. The SACI partners will adapt five CAHSI strategies: (1) a new introductory course designed to attract students and prepare them for majors in computing fields, (2) a peer-led team learning structure, to provide academic support and motivation for students to persist in computing disciplines, (3) an affinity research group model, to promote undergraduate retention through early research experiences and prepare students for success in upper-division and graduate studies, (4) faculty and peer mentoring, to improve student retention and success, and (5) outreach workshops, to stimulate interest in computing careers among high school and undergraduate students and to disseminate project research and evaluation findings.  SACI will join CAHSI, significantly extending the Alliance and its impact, and leveraging the Alliance's educational and research strengths; SACI will participate in the CAHSI Alliance-wide evaluation."
"1017529","TC: Small: Collaborative Research: User-centric Privacy Control for Collaborative Social Media","CNS","TRUSTWORTHY COMPUTING","09/01/2010","08/02/2010","Evimaria Terzi","MA","Trustees of Boston University","Standard Grant","Sylvia J. Spengler","08/31/2014","$247,323.00","","evimaria@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","7795","7923","$0.00","Social-networking sites (e.g., Facebook, MySpace, LinkedIn, etc.) and other online collaborative tools have emerged as places where people can post and share information. This information-sharing has many benefits, ranging from practical (e.g., sharing a business document) to purely social (e.g., communicating with distant friends). At the same time, information sharing inevitably poses significant threats to user privacy. In social-networking sites, for example, documented threats range from identity theft to digital stalking and personalized spam. As a result, a growing number of such sites allow individual users to specify fine-grained policies that indicate who can access their data, and to what extent. However, studies have consistently shown that most end-users find the task of specifying access-control policies for their own data overwhelming; as a result, users often skip the process altogether.<br/><br/><br/>The goal of this project is to help collaborative and social-media users gain control of their data. To that end, the project will include three main components: assisted specification, feedback, and refinement recommendations. To assist users in initially specifying access-control policies for their data, the project will develop a ""privacy wizard,"" which employs data mining and machine learning methods, including active learning, to construct an accurate policy, with minimal input from the user. To provide feedback regarding existing privacy settings, the project will pursue two approaches: aggregate scores and visualizations. For example, an aggregate score can be used to concisely explain to the user how her settings differ from those of other users. Preliminary work found that Item Response Theory (IRT) can be used effectively for this purpose. Finally, the project will consider how aggregate scores and visual feedback can be enriched with recommendations for refinements to help the user achieve an expressed level of social exposure.<br/><br/>Online collaborative tools and social media offer great promise in a number of arenas. In addition to communicating with friends via social networking sites, collaborative tools are now used in fields as diverse as business, medicine and education. However, the absence of usable privacy and access control prevents such tools from realizing their full potential. Results of this project will be disseminated via prototype implementations, as well as research publications. New undergraduate and graduate curriculum modules will also increase awareness of the importance of policy-specification and emerging research in this area."
"0959958","MRI-R2: Acquisition of Robots and Robot Accessories for Interdisciplinary Faculty and Student Research at Fayetteville State University","CNS","MAJOR RESEARCH INSTRUMENTATION","05/01/2010","02/08/2012","Sambit Bhattacharya","NC","Fayetteville State University","Standard Grant","Rita V. Rodriguez","04/30/2013","$175,092.00","Daniel Montoya, Michael Almeida, Bogdan Czejdo","sbhattac@uncfsu.edu","1200 Murchison Road","Fayetteville","NC","283014252","9106721141","CSE","1189","6890","$175,092.00","""This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).""<br/>Proposal #: 09-59958<br/>PI(s):  Bhattacharya, Sambit; Almeida, Michael; Czejdo, Bogdan, D.; Montoya, Daniel<br/>Institution: Fayetteville State University <br/>Title:  MRI-R2: Acq. of Robots and Robot Accessories for Interdisciplinary Faculty and Student Research<br/>Project Proposed:<br/>This project, acquiring four research grade robot platforms along with accessories required for navigation, robot vision, and robot-human interaction within a hybrid architecture, enables research in mobile reactive and hybrid robots. It also forms the basis of a robotics laboratory for conducting such research.<br/>The work addresses research in the following areas: spatial and temporal representation and reasoning using cognitive maps, stereo vision and object recognition, rapid modification and testing for correctness of reactive behaviors, and human experience of embodiment using robots.  Although the institution currently utilizes educational robots in teaching activities, a robot platform suitable for human-robot interaction research is needed.  Currently, no advanced robotics research facility exists at FSU or other local universities.<br/>The acquisition of this equipment for a Robotics Lab strengthens interdisciplinary collaboration between the Departments of Psychology and Mathematics and Computer Science. Students in Psychology benefit from learning about computer hardware components and programming algorithms, while students in Computer Science learn about and work with concepts of cognitive psychology. A room within the science building at FSU is currently being refurbished to conduct the research projects described in this proposal, and store the robots.  The PI and a co-PIs are responsible for installation of the robots at FSU.  Most accessory and software installation in the robot bases will mainly be done by the vendor before shipping, so very little time and effort will be required to get started using the robots.  Since one robot base will be used per project described in the proposal, there is no initial allocation needed.  A long term maintenance contract from the vendor is included as part of the proposal.<br/>Broader Impacts: <br/>The equipment enables research activities involving undergraduate and graduate students, contributing to train them in advanced scientific research methodology and application development.  This training benefits students from segments of population who are traditionally underrepresented in the STEM disciplines since FSU is a minority serving institution. The existing facilities at FSU are adequate for introductory programming and behavior based robotics courses.  This new equipment and the space committed by the university to support the project enables advanced research and training.  The activities supported by the equipment strengthen research at FSU.  The new equipment will be available to regional faculty who are interested in conducting research in robotics and related fields."
"1016668","III: Small: An Automatic Framework for Processing Drosophila Embryonic Images","IIS","Info Integration & Informatics, EPSCoR Co-Funding","10/01/2010","09/16/2010","Qi Li","KY","Western Kentucky University Research Foundation","Standard Grant","Sylvia Spengler","09/30/2013","$78,557.00","","qi.li@wku.edu","Western Kentucky University","Bowling Green","KY","421011016","2707454652","CSE","7364, 9150","7923, 9150","$0.00","High resolution embryonic images, e.g., the data set BDGP (Berkley Drosophila Genome Project), have been introduced as an important tool for the discovery of gene-gene interaction. These images contain not only temporal information of a gene but also precise spatial information of expression regions of genes. So the biologic problem of the discovery of gene-gene interaction can be characterized as a computational problem of matching expression patterns of embryos at the same developmental stage. It is, however, very challenging to design a fully automatic computational system due to severe imaging and artificial variations in embryonic images. Current research on embryonic image processing involves significant manual manipulation or addresses only a small subset of variations. In this proposal, I propose a comprehensive automatic framework to achieve the three fundamental tasks: image standardization, stage determination, and expression pattern modeling. The proposed project will essentially advance the integration of biologic, image processing and pattern recognition, and machine learning. The PI will develop a series of analysis modules to standardize the variation across images, provide for inpainting, and provide estimates of the boundaries of embryos.  The expression pattern modeling will develop discriminate features to address issues of distinguishing specific pixels in varied refraction circumstances.  A key concept is to develop an imbalance point detection scheme that will minimize the occurrences of edge points and provide a measure of the imbalance degree.<br/><br/>These methods should be adaptable for analysis of images from other model species, e.g., mouse. The proposed work will directly facilitate basic and applied research on image processing crucial to biological image analysis. The challenges in analyzing developmental biological images as compared to natural images have created increasing demands on and opportunities for developing novel image processing techniques. The algorithms and tools developed in this project will have made available to the community. This project will also facilitate the development of new courses and laboratory infrastructure for knowledge discovery from biological data."
"1047839","Participant Support: 2011 Mittag-Leffler Institute","DMS","ALGEBRA,NUMBER THEORY,AND COM, Other Global Learning & Trng","09/15/2010","09/07/2010","Elizabeth Allman","AK","University of Alaska Fairbanks Campus","Standard Grant","Tie Luo","08/31/2011","$48,515.00","Seth Sullivant","e.allman@alaska.edu","West Ridge Research Bldg 008","Fairbanks","AK","997757880","9074747301","MPS","1264, 7731","5914, 5937, 5979, 7556, 9150","$0.00","This grant will enable young mathematicians -- graduate students, post-doctoral candidates, and junior faculty members -- to attend and participate in satellite conferences held in conjunction with the Mittag-Leffler Institute's 2011 program ``Algebraic geometry with a view towards applications."" The satellite conferences will take place in Oslo, Norway and Stockholm, Sweden, and the organization and venues will permit ample time for young researchers to meet, interact, and share mathematical ideas in an international setting. The focus of the conferences include applications of algebraic geometry in the sciences, solving polynomial equations, and algorithm development.<br/><br/><br/>Although algebraic geometry has traditionally been considered a core area of pure mathematics, in recent years its techniques have been utilized to advance methods in applied mathematics. For example, algebraic geometric techniques have been used to prove identifiability results for parameters in statistical models which are parameterized by polynomials. In other areas also -- biology, coding theory, cryptology, quantum computing, machine learning, and optimization for example -- valuable contributions have been made recently by researchers employing methods from algebraic geometry."
"0941268","CDI-Type II: Collaborative Research: Groupscope: Instrumenting Research on Interaction Networks in Complex Social Contexts","BCS","Information Technology Researc","09/15/2010","09/22/2011","Marshall Poole","IL","University of Illinois at Urbana-Champaign","Standard Grant","Amber L. Story","08/31/2015","$1,697,482.00","David Forsyth, Feniosky Pena-Mora, Mark Hasegawa-Johnson, Peter Bajcsy, Kenton McHenry","mspoole@uiuc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","SBE","1640","7721","$0.00","Many of the most important functions in society are undertaken by large groups or teams.  Emergency response, product development, health care, education, and economic activity are pursued in the context of large, dynamic, interacting networks of groups.  Theory and research on such networks of groups is much less developed than research on isolated small groups or formal organizations.  A major challenge for research on networks of groups is the difficulties that accompany the collection and analysis of the huge bodies of high resolution, high volume, observational data necessary to study these large, dynamic networks of groups.  The goal of this project is to address this challenge by applying advanced computing applications to capture, manage, annotate and analyze these massive observational sets of video, audio, and other data.  The resulting data analysis system, GroupScope, will enable breakthrough research into social interaction in large, dynamic groups to be conducted much more quickly and with much higher reliability than was previously possible.  It will do this by automating as many functions as possible to the highest degree possible, including managing huge volumes of video, audio, and sensor data, transcription, parsing audio for critical discourse events, annotation and indexing of video streams, and coding interaction.  These first pass analyses can then be supplemented by human analysts (and their analyses in turn will feed into machine learning that will improve the computerized analysis).  <br/><br/>GroupScope will be developed with the collaboration of social scientists studying emergency response teams, children's playground behavior, distributed teams, and product development teams.  When developed, GroupScope will be deployed in a cyberenvironment, a Web 2.0 based cyberinfrastructure that enables a community of researchers to collaborate on common problems.  The cyberenvironment will enable multiple researchers to analyze and code the same group data for both small groups and large dynamic groups and networks.  Multiple analyses and codings working from diverse perspectives will enable discovery of previously unsuspected relationships among different levels and layers of human interaction.  They can also be linked to survey responses from participants, enabling linkage to the realm of perceptions and traits.<br/><br/>Many of the most fundamental advances in science have come through the development of new instruments, such as more powerful telescopes or microscopes that can allow scientists to view molecules.  In the same way GroupScope will shed light on the workings of critical functions performed by real world groups such as emergency response units, health care teams, stock exchanges, and military units.  GroupScope will also have applications in the training of those working in multi-team systems, such as first responders to disasters.  It can be used to record and ""grade"" training sessions, giving participants feedback on both strengths and weaknesses of their approaches."
"0953943","CAREER: A Multi-Disciplinary Approach to the Next Generation of Collaborative Technologies","IIS","HCC-Human-Centered Computing","01/01/2010","04/15/2014","Darren Gergle","IL","Northwestern University","Continuing Grant","William Bainbridge","06/30/2015","$501,918.00","","dgergle@northwestern.edu","750 N. Lake Shore Drive","Chicago","IL","606114579","3125037955","CSE","7367","1045, 1187, 7367, 9215, 9251, HPCC","$0.00","This research aims to enable the development of the next generation of collaborative technologies and to study their effects on human collaboration. It does so by improving our theoretical understanding of how various features of shared visual context affect communication, coordination and collaboration. The research develops and makes available the code for a new dual eye tracking methodology that can be used to study coordination and collaboration. It also develops a new set of metrics that can be more generally used to understand coordinated eye movements as they relate to dialogue and conversation. The results of this work will add to knowledge in a number of disciplines, including human-computer interaction, language technologies, computer science, computational linguistics, communication studies, cognitive science, and social and cognitive psychology.<br/><br/>As recent efforts in telemedicine, distance education, and remote training and repair attest, there exists enormous potential for technologies to support these activities at a distance, ultimately resulting in widespread benefits such as equal access to quality education and medical care. The education activities in this research include: (1) developing a new Ph.D. course that teaches a theory-driven design approach; (2) developing a series of course modules that use the dual eye tracking methodology to teach behavioral coding, statistics and machine learning; (3) developing a publicly available multimodal corpus with a set of tutorials, and (4) developing a series of international workshops on dual eye tracking. Finally, the research will simultaneously advance discovery and understanding while training both graduate and undergraduate students in interdisciplinary research methods and will contribute to the development of traditionally underrepresented individuals in the fields of computer and information sciences.<br/>"
"1018321","III: Small: Modeling and Inferring Searcher Intent by Mining User Interactions","IIS","Info Integration & Informatics","09/01/2010","08/01/2010","Yevgeny Agichtein","GA","Emory University","Standard Grant","Maria Zemankova","08/31/2014","$500,000.00","","eugene.agichtein@emory.edu","1599 Clifton Rd NE, 4th Floor","Atlanta","GA","303224250","4047272503","CSE","7364","7923","$0.00","Inferring searcher intent is a central problem in information retrieval and web search: for effective ranking and result presentation, the search engine must know what the user is looking for. Yet, expressing a searcher information need currently relies on entering the ?right? search keywords, which can require multiple rounds of trial-and-error from the searcher. The goal of this project is to develop effective methods for a search engine to automatically infer searcher intent and information needs from the searcher interactions and behavior data. Specifically, the project addresses two main challenges of search intent inference: developing accurate and robust models of searcher intent and behavior, and exploiting these models to infer search intent for each individual user. This project significantly advances previous efforts on implicit feedback and search modeling, by considering a wide range of user interaction and contextual features, and by developing novel techniques for mining and exploiting these signals to improve web search and information access.<br/><br/>To develop robust search intent and behavior models, the project uses machine learning and data mining techniques to model the connection between search actions and result page behavior and the searcher intent. The first stage of the project develops and evaluates these models in controlled lab environments, by combining eye tracking and search interface instrumentation data. The second stage of the project empirically validates the intent inference models through a large-scale collection of search behavior data using a variety of remote user studies with instrumented search interfaces. Finally, the project applies the resulting models and algorithms to improve performance on key information retrieval tasks including result ranking, automatic query expansion, and search result presentation. <br/><br/>The techniques developed in this project are expected to make web search and information access more intuitive and effective for millions of users through collaboration with major search engine companies. Additional broader impacts will be achieved through domain-specific applications of the developed techniques, ranging from improved library search to web-based diagnostics of cognitive impairment. All aspects of the project will involve graduate and undergraduate students, and the resulting tools and datasets are to be integrated into undergraduate course instruction and projects, thus broadening participation in computer science research. The resulting publications, software, and datasets will be made publicly available on the project website (http://ir.mathcs.emory.edu/intent/)."
"1052997","Collaborative Research: Biological Shape Spaces, Transforming Shape into Knowledge","DBI","ADVANCES IN BIO INFORMATICS, Cross-BIO Activities","09/15/2010","09/10/2010","Surangi Punyasena","IL","University of Illinois at Urbana-Champaign","Standard Grant","Anne Maglia","08/31/2014","$297,007.00","","punyasena@life.illinois.edu","1901 South First Street","Champaign","IL","618207406","2173332187","BIO","1165, 7275","1165, 1719, 1729, 7275, 7969, 9178, 9183, 9184, BIOT","$0.00","Collaborative Research: Biological Shape Spaces, Transforming Shape into Knowledge<br/><br/>This project will develop a framework to represent, analyze and interpret shapes extracted from images, supporting a wide range of biological investigations. The primary objectives are: (1) to develop a mathematical framework and computational tools for the quantification and analysis of shapes; (2) to integrate these computational models with machine learning and statistical inference methods to enable new discoveries, transforming imaging data into biological knowledge; (3) to deliver novel quantitative methodologies for shape analysis that start from a biological premise, rather than a purely geometric one. The aim is thus not only to quantitatively describe shape, but to develop methods for linking morphological variation to its underlying biological causes.  To ensure that the project focuses on methods that are most promising to biology with significant breadth of application, model and tool development will be guided and supported by a set of diverse case studies, ranging from the sub-cellular to organismal scales. <br/><br/>Shape represents a complex and rich source of biological information that is fundamentally linked to underlying mechanisms and function. However, shape is still often examined on a qualitative basis in many disciplines in biology, an approach that is time consuming and prone to human subjectivity. While ad hoc quantitative methods do exist, they are often inaccessible to non-experts and do not easily generalize to a wide variety of problems. The inability of biologists to systematically link shape to genetics, development, environment, function and evolution often precludes advances in biological research spanning diverse spatial and temporal scales, from the movement of molecules within a cell to adaptive changes in organismal morphology. The primary goal of this project is to develop a new suite of widely applicable quantitative methods and tools into the study of biological shape to address the significant need for consistent and repeatable analysis of shape data."
"1052942","Collaborative Research: Biological Shape Spaces, Transforming Shape into Knowledge","DBI","ADVANCES IN BIO INFORMATICS, Cross-BIO Activities","09/15/2010","09/10/2010","Washington Mio","FL","Florida State University","Standard Grant","Anne Maglia","08/31/2014","$296,130.00","","mio@math.fsu.edu","874 Traditions Way, 3rd Floor","TALLAHASSEE","FL","323064166","8506445260","BIO","1165, 7275","1165, 1719, 1729, 7275, 7969, 9183, 9184, BIOT","$0.00","Collaborative Research: Biological Shape Spaces, Transforming Shape into Knowledge<br/><br/>This project will develop a framework to represent, analyze and interpret shapes extracted from images, supporting a wide range of biological investigations. The primary objectives are: (1) to develop a mathematical framework and computational tools for the quantification and analysis of shapes; (2) to integrate these computational models with machine learning and statistical inference methods to enable new discoveries, transforming imaging data into biological knowledge; (3) to deliver novel quantitative methodologies for shape analysis that start from a biological premise, rather than a purely geometric one. The aim is thus not only to quantitatively describe shape, but to develop methods for linking morphological variation to its underlying biological causes.  To ensure that the project focuses on methods that are most promising to biology with significant breadth of application, model and tool development will be guided and supported by a set of diverse case studies, ranging from the sub-cellular to organismal scales. <br/><br/>Shape represents a complex and rich source of biological information that is fundamentally linked to underlying mechanisms and function. However, shape is still often examined on a qualitative basis in many disciplines in biology, an approach that is time consuming and prone to human subjectivity. While ad hoc quantitative methods do exist, they are often inaccessible to non-experts and do not easily generalize to a wide variety of problems. The inability of biologists to systematically link shape to genetics, development, environment, function and evolution often precludes advances in biological research spanning diverse spatial and temporal scales, from the movement of molecules within a cell to adaptive changes in organismal morphology. The primary goal of this project is to develop a new suite of widely applicable quantitative methods and tools into the study of biological shape to address the significant need for consistent and repeatable analysis of shape data."
"1018463","AF: Small: Algorithm Design Using Spectral Graph Theory","CCF","ALGORITHMS","09/01/2010","11/20/2014","Gary Miller","PA","Carnegie-Mellon University","Standard Grant","Balasubramanian Kalyanasundaram","08/31/2014","$498,228.00","David Tolliver, Ioannis Koutis","glmiller@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7926","9150, 9218, HPCC","$0.00","Spectral Graph Theory or Algebraic Graph Theory, as it is also known, is the study of the relationship between the eigenvalues and eigenvectors of graphs and their combinatorial properties.  The project will focus on furthering our understanding of this relationship and exploit this understanding to design new and efficient algorithms.  Included in this list of algorithmic problems will be fast and reliable linear system solvers and graph partitioners.  These new algorithms will in turn be used to find better and more efficient algorithms for problems in image processing, medical imaging, machine learning, and linear and non-linear optimizations. Enabling technology will include linear-work or O(m log m)-work algorithms for computing extreme eigenvalues of symmetric diagonally dominate systems.  The project will uses ideas and techniques from graph theory such as graph sparsifiers, graph cuts, and Steiner trees.  These graph theoretic ideas will be combined with numerical methods such as Krylov subspaces methods, interior point methods, and preconditioning methods to design and analyze these new algorithms.  When possible, code for the basic algorithms and their applications will be made available over the web to researchers.<br/><br/>The use of Spectral Graph Theory in computer science applications has become  increasingly important and popular. A notable application is the algorithm patented by Google to rank order web pages.  Other applications include image processing, in particular,  medical image segmentation and  denoising.  This project will further contribute to the design of better algorithms for these problem domains by combining the best ideas from numerical analysis and graph theory.  The goal is to design algorithms with very strong guarantees for both run time and robustness so that these algorithms will be appropriate for critical applications such as real-time image processing in a clinician's office. Dissemination will not only include journal and conference publications, but also giving a biannual spectral graph theory class and spectral graph theory lectures in both undergraduate and graduate algorithm classes."
"0968842","Collaborative Research: Mathematical Programming for Streaming Data","CMMI","OPERATIONS RESEARCH","06/01/2010","06/26/2012","Alexandre d'Aspremont","NJ","Princeton University","Standard Grant","michael fu","08/31/2012","$169,851.00","","aspremon@princeton.edu","Off. of Research & Proj. Admin.","Princeton","NJ","085442020","6092583090","ENG","5514","073E, 9147, MANU","$0.00","A large amount of data is now easily accessible in real-time in a streaming fashion: news, traffic, temperature or other physical measurements sent by sensors on cell phones. Applying statistical and machine learning methods to these streaming data sets represents tremendous opportunities for a better real-time understanding of complex physical, social or economic phenomena. These algorithms could be used, for example, to understand trends in how news media cover certain topics, and how these trends evolve over time, or to track incidents in transportation networks.<br/><br/>Unfortunately, most algorithms for large-scale data analysis are not designed for streaming data; typically, adding data points (representing, say, today's batch of news articles from the Associated Press) requires re-solving the entire problem. In addition, many of these algorithms require the whole data set under consideration to be stored in one place. These constraints make classical methods impractical for modern, live data sets.<br/><br/>This project's focus is on optimization algorithms designed to work in online mode, allowing for faster, possibly real-time, updating of solutions when new data or constraints are added to the problem. Efficient online algorithms are currently known for just a few special cases. Using homotopy methods and related ideas, this work will seek to allow online updating for a host of modern data analysis problems. A special emphasis will be put on problems involving sparsity or grouping constraints; such constraints are important for example to understand how a few key features in the data set that explain most of the changes in the data. These new online algorithms will be amenable to distributed implementations to allow for parts of the data to be stored on different servers.<br/><br/>These methods will be applied to streaming news data coming from major US media, and also to the problem of online detection, which arises when tracking some important signal over, say, a communication network, in an online fashion."
"1011228","AF: Large: Collaborative Research:  Compact Representations and Efficient Algorithms for Distributed Geometric Data","CCF","COMPUTATIONAL GEOMETRY","09/01/2010","08/22/2010","Leonidas Guibas","CA","Stanford University","Standard Grant","Tracy Kimbrel","08/31/2015","$432,364.00","","guibas@cs.stanford.edu","450 Jane Stanford Way","Stanford","CA","943052004","6507232300","CSE","7929","9218, HPCC","$0.00","Across many fields of science, engineering, and business, massive data sets are being generated at unprecedented rate by high-bandwidth sensors and cameras, large-scale simulations, or web-enabled large scale data collection.  Much of this data has a geometric character, either directly or indirectly.   For example, second generation LiDARs can map the earth's surface at 15-20 cm resolution; the Large Synoptic Telescope is set to produce about 30 terabytes of data each night; thirteen hours of video are uploaded to YouTube every minute; Facebook manages over 40 billion photos requiring more than one petabyte of data.<br/><br/>These data sets provide tremendous opportunities to enable novel capabilities that were unimaginable a few years ago.  Capitalizing on these opportunities, however, and transforming these massive amounts of heterogeneous data into useful information for vastly different types of applications and users requires solving challenging algorithmic problems.  An effective way of addressing this challenge is by designing efficient methods for producing informative yet succinct summaries of such geometric data sets.  These summaries must work at multiple scales, and allow a wide variety of queries to be answered approximately but efficiently.  The goal of this project is to study the theoretical underpinnings of compact representations and efficient algorithms for organizing, summarizing, cross-correlating, interlinking, and querying large distributed geometric data sets.<br/><br/>This project will design methods for computing summaries of many kinds of flavors, all with provable properties.  Summaries can be combinatorial and metric (core sets and kernels), algebraic (linear sketches), topological (persistence diagrams), feature-based, and structural (encoding self-similarities in the data).  The properties they aim to capture extend from low-level metric attributes, such as the diameter or width of a point set, to higher-level attributes revealing the internal structure of the data, as in the detection of symmetries and repeated patterns.  This processing must be done in the presence of uncertainty in data coming from sensors, and optimize multiple performance measures, including communication cost for data distributed across multiple locations in a network.  Another key aspect of this project is that it aims to understand not individual data sets in isolation but rather the inter-relationships and correspondences among different data sets, and to do so by communicating only summary information, without even having all the data in one place. <br/><br/>This work touches upon many topics in theoretical computer science and applied mathematics including low-distortion embeddings, compressive sensing, transportation metrics, spectral graph theory or harmonic analysis, machine learning, and computational topology."
"1017362","III-Core:Small: MoveMine: Mining Sophisticated Patterns and Actionable Knowledge from Massive Moving Object Data","IIS","Info Integration & Informatics","09/01/2010","07/23/2015","Jiawei Han","IL","University of Illinois at Urbana-Champaign","Continuing grant","Maria Zemankova","08/31/2016","$500,000.00","","hanj@cs.uiuc.edu","1901 South First Street","Champaign","IL","618207406","2173332187","CSE","7364","7923","$0.00","This research project is to investigate principles and methods for uncovering sophisticated patterns and actionable knowledge from massive moving object data.  Thanks to the rapid progress and broad adoption of sensor, GPS, wireless network, and other advanced technologies, moving object data have been accumulating in unprecedented scale. However, moving object data could be dynamic, sparse, scattered, and noisy, and patterns and knowledge to be mined could be deeply hidden, sophisticated, and subtle.  The MoveMine project investigates effective and scalable methods for mining various kinds of complex patterns from dynamic and noisy moving object data, finding multiple interleaved periodic patterns, and performing in-depth multidimensional analysis of moving object data.  It integrates and extends multiple disciplinary approaches derived from spatiotemporal data analysis, data mining, pattern recognition, statistics, and machine learning.  The study takes bird and animal movement data and traffic data as the major sources of data for investigation.  However, developed methods can be applied to the analysis of many other kinds of moving object data for environmental study, traffic control, law enforcement, and protection of homeland security.  The study also addresses the issue of ensuring privacy and security protection while developing powerful pattern and knowledge discovery mechanisms.  The research results are to be published in various research and application forums and be integrated into the educational programs at UIUC.  The progress of the project and the research results are also disseminated via the project Web site (http://www.cs.uiuc.edu/homes/hanj/projs/movemine.htm)."
"1018769","GV: Small: Collaborative Research: Supporting Knowledge Discovery through a Scientific Visualization Language","IIS","GRAPHICS & VISUALIZATION, EPSCoR Co-Funding","11/01/2010","09/09/2010","Jian Chen","MS","University of Southern Mississippi","Standard Grant","Maria Zemankova","02/28/2013","$205,001.00","","chen.8028@osu.edu","2609 WEST 4TH ST","Hattiesburg","MS","394015876","6012664119","CSE","7453, 9150","7453, 7923, 9150","$0.00","This collaborative research brings together computer scientists from University of Southern Mississippi (USM) and Brown University and neuroscientists from the University of Mississippi Medical Center (UMMC) to study the design of a scientific visualization language (SVL). Despite the numerous visualization approaches already devised, visualization remains more of an art than a science. Grounded in theories and methods from human-centered computing, machine learning, and cognitive psychology, this work is to develop and evaluate a scientific visualization language (SVL) to provide a principled way to help scientists understand how and why visualizations work. Tools and theories developed in this project can lead to efficient knowledge discovery to help neuroscientists study brains using diffusion tensor magnetic resonance imaging (DTI).<br/><br/>This work has the following specific objectives and outcomes: (1) close collaboration with scientists to discover, refine, and verify a symbol space, (2) a semantic space that describes the relationship among symbols, (3) a testbed that implements SVL for neuroscientists to compose visualizations, (4) development of new and enhanced courses at University of Southern Mississippi and Brown University, and (5) wide dissemination of the research outcomes through open-source software, experimental data, open labs, publications, and presentations.<br/><br/>This project is expected to have broad impact. It may lead to significantly better approaches to human knowledge discovery and decision making in many disciplines where visualizations have found successful application, including neuroscience, biomedicine, bioinformatics, biology, chemistry, geosciences, business, economics, and education. Undergraduate and graduate students are expected to participate in the research through our courses, and student exchanges are planned between USM and Brown. K-12 students can visit the USM lab while the project is in progress. Software and results will be disseminated via the project Web site (https://sites.google.com/site/simplevisualizationlanguage)."
"1024713","BECS: Pattern-Steering in Nonlinear Dynamical Networks","EFMA","OFFICE OF MULTIDISCIPLINARY AC, Proc Sys, Reac Eng & Mol Therm, DYNAMICAL SYSTEMS, COFFES, EFRI Research Projects","09/01/2010","07/14/2013","Nathalia Peixoto","VA","George Mason University","Standard Grant","Eduardo Misawa","08/31/2013","$315,971.00","John Cressman, Nathalia Peixoto, Nathalia Peixoto","npeixoto@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","ENG","1253, 1403, 7478, 7552, 7633","034E, 035E, 116E, 7722, 8024, 9178, 9251","$0.00","We investigate computational methods for moving the activity of a  complex dynamical network from one state to another. Complicated  nonlinear networks appear to be multistable and multirhythmic, a mix  of chaotic and periodic stable attractors, together with other  metastable transients that may be effectively stable for the time  interval of an engineering application. Computational methods will be  developed for steering between basins of attractors, and tested in  three different experimental platforms: primary neuronal cell cultures  from vertebrates and invertebrates, hippocampal slices from rodents,  and defect activity in convection rolls in nematic liquid crystals. Better understanding of these test cases could be a first step to new  approaches to neural engineering, machine learning, and optimization  of energy transfer and production. The experimental examples have in  common a lack of known equations of motion, which severely limits the  use of classical control theory and motivates development of our new techniques.<br/><br/>The hallmark of complex systems behavior in engineering devices is the  appearance of dynamical patterns that vary in time and space. In this  exploratory study, we will expose the main problems and bottlenecks  that need to be overcome to enable low-power steering between  different spatiotemporal patterns in engineered networks, and to begin  to develop efficient computational methods to accomplish this  switching quickly and automatically. The project includes theoretical,  computational, and experimental components. Methods will be developed  and tested experimentally in systems that have nonlinear, complex  network characteristics, such as primary neuronal cell cultures,  hippocampal slices from rodents, and nematic liquid crystals."
"1041673","CAMEO: Multiscale modeling of Hawaii's coral reef communities","OCE","BIOLOGICAL OCEANOGRAPHY","07/15/2010","07/09/2010","Megan Donahue","HI","University of Hawaii","Standard Grant","David L. Garrison","06/30/2014","$364,541.00","Paul Jokiel","donahuem@hawaii.edu","2440 Campus Road, Box 368","HONOLULU","HI","968222234","8089567800","GEO","1650","9150, 9169, EGCH","$0.00","A key challenge in the effective management of marine ecosystems is translating from small scale studies of distribution and dynamics to the regional scale of management action. In many marine ecosystems, including the Hawaiian Archipelago, there are extensive survey data of nearshore communities from multiple investigators, representing a huge investment of resources. Often, these data are underutilized and remain of limited use to managers. In the Hawaiian Archipelago, at least seven separate entities are engaged in surveys of coral reef communities, with varying degrees of coordination. The synthesis of these data requires integrated modeling approaches at multiple scales. This study builds on an existing database and extends two existing models: the Coral Recovery Model (CRM) of stochastic coral recovery after disturbance and the COMBO model of the synergistic impacts of increasing acidification and temperature on coral reefs. Extending from this prior work is the application of two innovative modeling approaches (scale transition theory and fundamental niche modeling) to predict coral community composition and dynamics at the regional scale. Fundamental niche modeling uses multiple data fitting approaches (regression, machine learning, etc) to describe the relationship between species and their environments, using a split dataset for training and validation. This approach can generate a predictive and validated spatially continuous model of species distribution from discrete data points. The scale transition modeling will use the completed database of species distributions as the landscape on which species interactions occur. These interactions are described by a local model, here, based on recruitment, growth, and mortality from the Coral Recovery Model. In scale transition theory, the local model plus landscape information on the distribution and co-distribution of organisms and their environments predicts how a species assemblage responds (locally and regionally) to changes in biotic and abiotic factors on the landscape.<br/><br/>This project will generate four products relevant to ecosystem-based management of the Hawaiian Archipelago, resulting in significant impacts beyond the research community: (1) A Hawaiian Archipelago-wide GIS database of coral distribution, benthic community data, fish surveys, and other data gathered by CRAMP, NPS (National Park Service), various divisions in NOAA, the Hawaii Division of Aquatic Resources, and other sources into a single GIS database; (2) Validated, predictive, and spatially continuous maps of coral species distribution throughout the Archipelago; (3) A validated Coral Recovery Model for coral reef mitigation in the Main Hawaiian Islands; (4) Prediction of coral community response to climate change throughout the Hawaiian Archipelago, based on known and predicted coral distributions and the COMBO model."
"1012254","AF: Large: Collaborative Research: Compact Representations and Efficient Algorithms for Distributed Geometric Data","CCF","ALGORITHMIC FOUNDATIONS, COMPUTATIONAL GEOMETRY","09/01/2010","05/13/2013","Pankaj Agarwal","NC","Duke University","Continuing grant","Tracy J. Kimbrel","08/31/2015","$448,539.00","","pankaj@cs.duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","7796, 7929","7925, 9251, 9218, HPCC, 7929","$0.00","Across many fields of science, engineering, and business, massive data sets are being generated at unprecedented rate by high-bandwidth sensors and cameras, large-scale simulations, or web-enabled large scale data collection. Much of this data has a geometric character, either directly or indirectly. For example, second generation LiDARs can map the earth's surface at 15-20 cm resolution; the Large Synoptic Telescope is set to produce about 30 terabytes of data each night; thirteen hours of video are uploaded to YouTube every minute; Facebook manages over 40 billion photos requiring more than one petabyte of data.<br/><br/>These data sets provide tremendous opportunities to enable novel capabilities that were unimaginable a few years ago. Capitalizing on these opportunities, however, and transforming these massive amounts of heterogeneous data into useful information for vastly different types of applications and users requires solving challenging algorithmic problems. An effective way of addressing this challenge is by designing efficient methods for producing informative yet succinct summaries of such geometric data sets. These summaries must work at multiple scales, and allow a wide variety of queries to be answered approximately but efficiently. The goal of this project is to study the theoretical underpinnings of compact representations and efficient algorithms for organizing, summarizing, cross-correlating, interlinking, and querying large distributed geometric data sets.<br/><br/>This project will design methods for computing summaries of many kinds of flavors, all with provable properties. Summaries can be combinatorial and metric (core sets and kernels), algebraic (linear sketches), topological (persistence diagrams), feature-based, and structural (encoding self-similarities in the data). The properties they aim to capture extend from low-level metric attributes, such as the diameter or width of a point set, to higher-level attributes revealing the internal structure of the data, as in the detection of symmetries and repeated patterns. This processing must be done in the presence of uncertainty in data coming from sensors, and optimize multiple performance measures, including communication cost for data distributed across multiple locations in a network. Another key aspect of this project is that it aims to understand not individual data sets in isolation but rather the inter-relationships and correspondences among different data sets, and to do so by communicating only summary information, without even having all the data in one place. <br/><br/>This work touches upon many topics in theoretical computer science and applied mathematics including low-distortion embeddings, compressive sensing, transportation metrics, spectral graph theory or harmonic analysis, machine learning, and computational topology."
"1010614","SBIR Phase I: ZOOZbeat - An Interactive Music Analysis, Re-synthesis and Distribution Engine","IIP","SMALL BUSINESS PHASE I","07/01/2010","12/21/2010","Scott Geller","GA","ZOOZ Mobile","Standard Grant","Errol Arkilic","06/30/2011","$173,250.00","","scott@zoozmobile.com","325 Trowbridge Walk","Atlanta","GA","303506876","7703788115","ENG","5371","5371, 6850, 9139, HPCC","$0.00","This Small Business Innovation Research (SBIR) Phase I project aims at developing a computational music system that will automatically analyze, re-synthesize and distribute digital music with the goal of transforming a linear and passive music listening practice into an interactive, expressive and viral music performance experience. The system will utilize approaches for temporal, tonal and harmonic analysis as well as collaborative interactive performance and distribution, in an effort to create an engaging musical experience that is rewarding and viral. Mobile and web-based applications will be developed supporting cloud-based multi-player interactions, allowing music fans to get expressively involved in creating, modifying, and personalizing their favorite music. The project will investigate how temporal structures in music are represented and processed by human listeners and will design new models for analysis and re-synthesis of music in an effort to facilitate interactive and collaborative musical experiences. The project will advance current knowledge in areas such as music information retrieval, music theory, music perception, machine learning, mobile interaction, signal processing, instrument design, and cloud computing. <br/><br/>If successful, the project will lead to broad impact in the public sphere by creating engaging and rewarding musical experience for users at all skill levels. By providing easy-to use tools for music creation and sharing, the project would encourage creativity and expression for the general population, facilitating growth in user-generated content in an area that is usually not accessible to the masses. From a business perspective, the broad impact of the project will be in providing a novel solution to the significant problems faced by the music industry today. The industry is looking for new ways to monetize content by engaging fans with music games and personalization tools (e.g. ring tones). Once deployed, the platform proposed will address these needs by providing an intelligent system that will allow fans to interact, personalize and share their favorite music in an expressive and viral manner, using mobile devices and online."
"1018006","SHF:  Small:  An Integrated Parallel Constraint Programming Platform for Combinatorial Search Problems","CCF","PROGRAMMING LANGUAGES","08/01/2010","07/28/2010","Neng-Fa Zhou","NY","CUNY Brooklyn College","Standard Grant","Anindya Banerjee","07/31/2015","$277,065.00","","zhou@sci.brooklyn.cuny.edu","Office of Research & Sponsored P","Brooklyn","NY","112102889","7189515622","CSE","7943","9150, 9215, HPCC","$0.00","Many real-world problems, ranging from scheduling in industrial production lines, planning for intelligent robots, protein structure predication, resource allocation, to various network optimization problems are combinatorial search problems. Constraint Programming (CP) and Answer Set Programming (ASP) are emerging techniques for solving these problems. CP over Finite Domains (FD) has had great successes in many application areas, such as scheduling, where use of global constraints is very effective. ASP has been found amenable to knowledge-intensive search problems such as planning and configuration problems. Recently, there has been great interest in parallelizing CP and ASP solvers to take advantage of the power provided by multi-core processors. <br/><br/>This research aims to develop an integrated parallel constraint programming platform for combinatorial search problems. It entails three tasks. Firstly, this research will enhance the power of CLP(FD) (Constraint Logic Programming over FD) by enabling constraints over Composite Finite Domains (CFD). The resulting language, CLP(CFD), allows for natural and efficient modeling of problems with multi-attributed objects. Action Rules (AR), a successful language developed by the PI, will be enhanced and used to implement CLP(CFD). Secondly, this research will develop a compiler to translate ASP programs into AR. For an ASP program, the generated program maintains a partial answer set as a pair of disjoint tuple sets and uses labeling and propagation to compute answer sets. Unlike most ASP solvers, the AR-based solver requires no prior grounding of programs. Thirdly, this research will parallelize AR. Since AR is used as a common intermediate language for both CLP(CFD) and ASP, a parallel implementation of AR will directly result in parallel solvers for CLP(CFD) and ASP. This research will advance the implementation techniques for constraint languages and the resulting system will benefit a wide range of real-world applications."
"1053024","Collaborative Research: Biological Shape Spaces, Transforming Shape into Knowledge","DBI","ADVANCES IN BIO INFORMATICS, Cross-BIO Activities","09/15/2010","09/10/2010","Chi-Ren Shyu","MO","University of Missouri-Columbia","Standard Grant","Anne Maglia","08/31/2014","$266,241.00","","shyuc@missouri.edu","115 Business Loop 70 W","COLUMBIA","MO","652110001","5738827560","BIO","1165, 7275","1719, 1729, 7275, 7969, 9183, 9184, BIOT","$0.00","Collaborative Research: Biological Shape Spaces, Transforming Shape into Knowledge<br/><br/>This project will develop a framework to represent, analyze and interpret shapes extracted from images, supporting a wide range of biological investigations. The primary objectives are: (1) to develop a mathematical framework and computational tools for the quantification and analysis of shapes; (2) to integrate these computational models with machine learning and statistical inference methods to enable new discoveries, transforming imaging data into biological knowledge; (3) to deliver novel quantitative methodologies for shape analysis that start from a biological premise, rather than a purely geometric one. The aim is thus not only to quantitatively describe shape, but to develop methods for linking morphological variation to its underlying biological causes.  To ensure that the project focuses on methods that are most promising to biology with significant breadth of application, model and tool development will be guided and supported by a set of diverse case studies, ranging from the sub-cellular to organismal scales. <br/><br/>Shape represents a complex and rich source of biological information that is fundamentally linked to underlying mechanisms and function. However, shape is still often examined on a qualitative basis in many disciplines in biology, an approach that is time consuming and prone to human subjectivity. While ad hoc quantitative methods do exist, they are often inaccessible to non-experts and do not easily generalize to a wide variety of problems. The inability of biologists to systematically link shape to genetics, development, environment, function and evolution often precludes advances in biological research spanning diverse spatial and temporal scales, from the movement of molecules within a cell to adaptive changes in organismal morphology. The primary goal of this project is to develop a new suite of widely applicable quantitative methods and tools into the study of biological shape to address the significant need for consistent and repeatable analysis of shape data."
"1053171","Collaborative Research: Biological Shape Spaces, Transforming Shape into Knowledge","DBI","ADVANCES IN BIO INFORMATICS, Cross-BIO Activities","09/15/2010","09/10/2010","Cindy Grimm","MO","Washington University","Standard Grant","Julie Dickerson","01/31/2013","$213,923.00","","grimmc@onid.orst.edu","CAMPUS BOX 1054","Saint Louis","MO","631304862","3147474134","BIO","1165, 7275","1165, 1719, 1729, 7275, 7969, 9183, 9184, BIOT","$0.00","Collaborative Research: Biological Shape Spaces, Transforming Shape into Knowledge<br/><br/>This project will develop a framework to represent, analyze and interpret shapes extracted from images, supporting a wide range of biological investigations. The primary objectives are: (1) to develop a mathematical framework and computational tools for the quantification and analysis of shapes; (2) to integrate these computational models with machine learning and statistical inference methods to enable new discoveries, transforming imaging data into biological knowledge; (3) to deliver novel quantitative methodologies for shape analysis that start from a biological premise, rather than a purely geometric one. The aim is thus not only to quantitatively describe shape, but to develop methods for linking morphological variation to its underlying biological causes.  To ensure that the project focuses on methods that are most promising to biology with significant breadth of application, model and tool development will be guided and supported by a set of diverse case studies, ranging from the sub-cellular to organismal scales. <br/><br/>Shape represents a complex and rich source of biological information that is fundamentally linked to underlying mechanisms and function. However, shape is still often examined on a qualitative basis in many disciplines in biology, an approach that is time consuming and prone to human subjectivity. While ad hoc quantitative methods do exist, they are often inaccessible to non-experts and do not easily generalize to a wide variety of problems. The inability of biologists to systematically link shape to genetics, development, environment, function and evolution often precludes advances in biological research spanning diverse spatial and temporal scales, from the movement of molecules within a cell to adaptive changes in organismal morphology. The primary goal of this project is to develop a new suite of widely applicable quantitative methods and tools into the study of biological shape to address the significant need for consistent and repeatable analysis of shape data."
"1019104","EAGER: Exploratory Evaluation: Scalability and Effectiveness of Data-Intensive Table-based Computing Software Systems","CCF","HECURA, ","08/01/2010","08/10/2010","Garth Gibson","PA","Carnegie-Mellon University","Standard Grant","Almadena Chtchelkanova","07/31/2012","$300,000.00","","garth@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7952, J147","7916, 9218, HPCC","$0.00","Science and analysis today are increasingly tackled by systematic exploration of high-resolution captured, or simulated, data. With a more expansive sample of raw data, more detailed models and precise questions of the meaning of the data can be formed.  However, massively parallel systems for processing massive data sets render traditional programming, storage and fault tolerance strategies ineffective.  <br/><br/>Table-based or column-oriented distributed data storage systems are being developed to support such large scale data analysis, led by Google?s BigTable and including open source variations such as Apache Hbase.  These new systems have the flavor of database row and column organization, but have simpler semantics, weaker isolation, and non-SQL interfaces, for example. The effectiveness of these new systems for applications other than internet search support is not well understood.<br/><br/>This exploratory project is developing an evaluation framework and exploring a set of these new table-based storage systems, with the goal of capturing an understanding of the state of the art, how they perform and scale, and their reliability and usability.<br/><br/>In addition to benchmarks focussing on key metrics, the project's evaluation framework includes real world applications drawn from machine learning approaches to understanding streams of events, such as internet blog publications, and approaches to understanding complex interrelationships, such as social networking graphs, in order to extract insight about the requirements needed to enable these emerging types of knowledge discovery applications."
"0940851","CDI-Type II: Collaborative Research: Groupscope: Instrumenting Research on Interaction Networks in Complex Social Contexts","BCS","Information Technology Researc","09/15/2010","09/14/2010","Noshir Contractor","IL","Northwestern University","Standard Grant","Amber L. Story","08/31/2014","$252,518.00","","nosh@northwestern.edu","750 N. Lake Shore Drive","Chicago","IL","606114579","3125037955","SBE","1640","7721","$0.00","Many of the most important functions in society are undertaken by large groups or teams. Emergency response, product development, health care, education, and economic activity are pursued in the context of large, dynamic, interacting networks of groups. Theory and research on such networks of groups is much less developed than research on isolated small groups or formal organizations. A major challenge for research on networks of groups is the difficulties that accompany the collection and analysis of the huge bodies of high resolution, high volume, observational data necessary to study these large, dynamic networks of groups. The goal of this project is to address this challenge by applying advanced computing applications to capture, manage, annotate and analyze these massive observational sets of video, audio, and other data. The resulting data analysis system, GroupScope, will enable breakthrough research into social interaction in large, dynamic groups to be conducted much more quickly and with much higher reliability than was previously possible. It will do this by automating as many functions as possible to the highest degree possible, including managing huge volumes of video, audio, and sensor data, transcription, parsing audio for critical discourse events, annotation and indexing of video streams, and coding interaction. These first pass analyses can then be supplemented by human analysts (and their analyses in turn will feed into machine learning that will improve the computerized analysis). <br/><br/>GroupScope will be developed with the collaboration of social scientists studying emergency response teams, children's playground behavior, distributed teams, and product development teams. When developed, GroupScope will be deployed in a cyberenvironment, a Web 2.0 based cyberinfrastructure that enables a community of researchers to collaborate on common problems. The cyberenvironment will enable multiple researchers to analyze and code the same group data for both small groups and large dynamic groups and networks. Multiple analyses and codings working from diverse perspectives will enable discovery of previously unsuspected relationships among different levels and layers of human interaction. They can also be linked to survey responses from participants, enabling linkage to the realm of perceptions and traits. <br/><br/>Many of the most fundamental advances in science have come through the development of new instruments, such as more powerful telescopes or microscopes that can allow scientists to view molecules. In the same way GroupScope will shed light on the workings of critical functions performed by real world groups such as emergency response units, health care teams, stock exchanges, and military units. GroupScope will also have applications in the training of those working in multi-team systems, such as first responders to disasters. It can be used to record and ""grade"" training sessions, giving participants feedback on both strengths and weaknesses of their approaches."
"1049427","EAGER: Optimization in Wireless Mobile and Sensor Networks: A Novel Paradigm Based on Differential Evolution","CNS","Special Projects - CNS, Networking Technology and Syst","09/01/2010","05/16/2011","Uday Chakraborty","MO","University of Missouri-Saint Louis","Standard Grant","Thyagarajan Nandagopal","08/31/2012","$137,137.00","","chakrabortyu@umsl.edu","ONE UNIVERSITY BLVD","SAINT LOUIS","MO","631214400","3145165897","CSE","1714, 7363","7916, 9178, 9251","$0.00","This EAGER proposal seeks to develop new, improved approaches, based on single- and multi-objective differential evolution, to the following important problems:<br/>(i) QoS-based multicast routing in mobile networks; (ii) Energy-efficient routing in hierarchical (two-tiered) wireless sensor networks; (iii) Stability-aware clustering in mobile ad hoc networks with special consideration of group mobility; and (iv) Cross-layer optimization in wireless sensor networks by joint routing and link scheduling in the presence of energy constraints, link interference and noise.<br/><br/>The goal is to achieve higher energy saving, better network performance and extended network lifetimes.<br/><br/>The novelty of this proposal is that it brings the power of differential evolution, a cutting-edge strategy in present-day computational intelligence research, to a group of outstanding, NP-hard problems in computer networks. It presents novel schemes for encoding of trial solutions and also for designing the differential operator for these problems. This research cuts across conventional subject lines ? it embodies an interdisciplinary and transformative application of ideas from electrical engineering, computer communications, computational intelligence, and statistical machine learning. This has the potential to open up a radically new direction in networking research.<br/><br/>The broader impact of this research is far-reaching in this era of ubiquitous and pervasive computing. The efficiency, flexibility, and controllability provided in the proposed methods can be used to save costs and improve the quality of the final products in the industry. The proposal also includes well-thought out plans for integrating research and education. The PI will use this project to involve high-school students, women, and undergraduate/graduate students in computer science research."
"0969923","Collaborative Research: Mathematical Programming for Streaming Data","CMMI","OPERATIONS RESEARCH","06/01/2010","05/05/2010","Laurent El Ghaoui","CA","University of California-Berkeley","Standard Grant","Sheldon Jacobson","05/31/2013","$250,000.00","","elghaoui@eecs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","ENG","5514","073E, 9147, MANU","$0.00","A large amount of data is now easily accessible in real-time in a streaming fashion: news, traffic, temperature or other physical measurements sent by sensors on cell phones. Applying statistical and machine learning methods to these streaming data sets represents tremendous opportunities for a better real-time understanding of complex physical, social or economic phenomena. These algorithms could be used, for example, to understand trends in how news media cover certain topics, and how these trends evolve over time, or to track incidents in transportation networks.<br/><br/>Unfortunately, most algorithms for large-scale data analysis are not designed for streaming data; typically, adding data points (representing, say, today's batch of news articles from the Associated Press) requires re-solving the entire problem. In addition, many of these algorithms require the whole data set under consideration to be stored in one place. These constraints make classical methods impractical for modern, live data sets.<br/><br/>This project's focus is on optimization algorithms designed to work in online mode, allowing for faster, possibly real-time, updating of solutions when new data or constraints are added to the problem. Efficient online algorithms are currently known for just a few special cases. Using homotopy methods and related ideas, this work will seek to allow online updating for a host of modern data analysis problems. A special emphasis will be put on problems involving sparsity or grouping constraints; such constraints are important for example to understand how a few key features in the data set that explain most of the changes in the data. These new online algorithms will be amenable to distributed implementations to allow for parts of the data to be stored on different servers.<br/><br/>These methods will be applied to streaming news data coming from major US media, and also to the problem of online detection, which arises when tracking some important signal over, say, a communication network, in an online fashion."
"1049139","Support for US-Based Students to Attend the 2010 IEEE International Conference on Data Mining (ICDM 2010), December 13-17, 2010, Sydney, Australia","IIS","Info Integration & Informatics","09/01/2010","07/15/2010","Xindong Wu","VT","University of Vermont & State Agricultural College","Standard Grant","Maria Zemankova","08/31/2011","$22,500.00","","xwu@louisiana.edu","85 South Prospect Street","Burlington","VT","054050160","8026563660","CSE","7364","7556, 9150","$0.00","This grant provides international travel support for U.S. based graduate student participants to attend the 2010 International Conference on Data Mining (ICDM 2010), which will be held in Sydney, Australia, on December 13-17, 2010. ICDM has established itself as the world's premier research conference in data mining. It provides an international forum for presentation of original research results, as well as exchange and dissemination of innovative, practical development experiences.<br/><br/>The conference seeks to continuously advance the state-of-the-art in data mining, including algorithms, software and systems, as well as related areas such as data management, machine learning and their use in a wide range of applications. With the growth of the Web, wireless communication and data intensive technologies such as sensor networks, social media, multimedia information systems, cloud computing, and application domains such as bioinformatics, climate change, or security, advances in data mining have a significant impact. <br/><br/>A strong representation of U.S. researchers at the Conference is useful in maintaining U.S. competitiveness in this important area. The total number of ICDM participants in the past has been in excess of 300, with a majority of the participants from the U.S., then Europe and Asia. It is expected to provide scholarships to 15 U.S. based graduate student participants. This grant will partially support the travel costs for the U.S. based graduate student participants. <br/><br/>The ICDM proceedings are published by IEEE. The student award results will be announced at the ICDM 2010 conference website (http://datamining.it.uts.edu.au/icdm10/)."
"0968481","Collaborative Research: SoCS:  Analysis of Social Media Driven By Theories of Political Psychology","IIS","Information Technology Researc, Special Projects - CCF","08/15/2010","08/18/2010","William Cohen","PA","Carnegie-Mellon University","Standard Grant","William Bainbridge","07/31/2013","$356,349.00","","wcohen@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","1640, 2878","9215","$0.00","Understanding how people make decisions in complex settings is crucial in many application areas, including marketing, intelligence analysis, and political decision making. Traditionally, human decision-makers are modeled as rational agents seeking to maximize some mathematical measure of utility. In fact, however, people are overwhelmed in information-rich environments, and have developed cognitive and emotional strategies to navigate such environments.  One such strategy is ""motivated reasoning"", where information is first evaluated subconsciously for emotional content, with the goal of maintaining an existing emotional commitment, and cognitive processing of the information is then conditioned on this emotional evaluation. Political scientists have demonstrated motivated reasoning in evaluation of both candidates and issues. In general, decision-making and information-gathering are strongly influenced by emotion, prior knowledge, and the social communities to which a person belongs. Evidence suggests that accurate models of human decision-making must be complex enough to model not only utility, but prior knowledge and beliefs, human cognitive abilities, and social context.  Building such cognitive models requires substantially extending the state-of-the-art in machine learning.<br/><br/>In the past, the ability of researchers in political psychology to develop such complex models of decision-making was limited by the amount of data obtainable obtain from surveys or human-subject experiments. The recent explosion of on-line political communities provides an opportunity to overcome this limitation. We will model human behavior for socially-driven information gathering and decision-making tasks - specifically for political decisions - by combining human-subject experiments with analysis of large datasets of social media and social interactions."
"1016828","CSR: Small: Adaptive Synchronization for Multicore Systems","CNS","CSR-Computer Systems Research","08/01/2010","07/20/2010","Michael Spear","PA","Lehigh University","Standard Grant","Marilyn McClure","07/31/2014","$250,025.00","","spear@cse.lehigh.edu","Alumni Building 27","Bethlehem","PA","180153005","6107583021","CSE","7354","7923","$0.00","Transactional Memory (TM) is among the most promising techniques for simplifying the development of correct parallel programs.  Dozens of different techniques have been developed to implement TM, with each appearing ideally suited to some combination of application and hardware characteristics.  Unfortunately, when the wrong algorithm is chosen for a workload, pathologically bad performance can result.  <br/><br/>The problem is particularly acute in programs whose behavior varies over<br/>time: the best TM implementation for one phase of execution may be unacceptable during another phase.  This research will create new algorithms for dynamic adaptivity, so that the implementation of TM can be changed repeatedly during program execution, thereby maximizing performance during each program phase.<br/><br/>The research will explore both mechanism and policy.  It will create new algorithms and heuristics for selecting TM implementations.  It will also model the behavior of a wide array of TM algorithms and workloads on multiple architectures, in order to create a knowledge base suitable for guiding adaptivity.  To achieve maximum performance, the research will consider a broad set of characteristics, to include TM implementation details, bottlenecks, and overheads; risk of pathology; properties of the hardware environment; and workload requirements such as frequency of inter-thread synchronization, I/O, and nontransactional access to shared data.  This research will also develop novel static analyses and a dynamic optimization framework to avoid any overhead while providing robust, adaptive TM.  Prototypes and source code will be distributed as open-source software."
"1017921","GV: Small: Collaborative Research: Supporting Knowledge Discovery Through a Scientific Visualization Language","IIS","GRAPHICS & VISUALIZATION, EPSCoR Co-Funding","11/01/2010","05/24/2011","Alexander Auchus","MS","University of Mississippi Medical Center","Standard Grant","Maria Zemankova","10/31/2014","$32,976.00","","aauchus@umc.edu","2500 North State Street","Jackson","MS","392164505","6018155000","CSE","7453, 9150","7453, 7923, 9150","$0.00","This collaborative research brings together computer scientists from University of Southern Mississippi (USM) and Brown University and neuroscientists from the University of Mississippi Medical Center (UMMC) to study the design of a scientific visualization language (SVL). Despite the numerous visualization approaches already devised, visualization remains more of an art than a science. Grounded in theories and methods from human-centered computing, machine learning, and cognitive psychology, this work is to develop and evaluate a scientific visualization language (SVL) to provide a principled way to help scientists understand how and why visualizations work. Tools and theories developed in this project can lead to efficient knowledge discovery to help neuroscientists study brains using diffusion tensor magnetic resonance imaging (DTI).<br/><br/>This work has the following specific objectives and outcomes: (1) close collaboration with scientists to discover, refine, and verify a symbol space, (2) a semantic space that describes the relationship among symbols, (3) a testbed that implements SVL for neuroscientists to compose visualizations, (4) development of new and enhanced courses at University of Southern Mississippi and Brown University, and (5) wide dissemination of the research outcomes through open-source software, experimental data, open labs, publications, and presentations.<br/><br/>This project is expected to have broad impact. It may lead to significantly better approaches to human knowledge discovery and decision making in many disciplines where visualizations have found successful application, including neuroscience, biomedicine, bioinformatics, biology, chemistry, geosciences, business, economics, and education. Undergraduate and graduate students are expected to participate in the research through our courses, and student exchanges are planned between USM and Brown. K-12 students can visit the USM lab while the project is in progress. Software and results will be disseminated via the project Web site (https://sites.google.com/site/simplevisualizationlanguage)."
"0964412","III:  Medium:  Collaborative Research:  Integrating Behavioral, Geometrical and Graphical Modeling to Simulate  and Visualize Urban Areas","IIS","GRAPHICS & VISUALIZATION","09/01/2010","08/12/2011","Paul Waddell","CA","University of California-Berkeley","Continuing grant","Ephraim Glinert","08/31/2014","$450,000.00","Michael Jordan","waddell@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","CSE","7453","7924","$0.00","In this project, the PI and his team will develop a new simulation framework to interactively model and visualize socio-economic and geometric characteristics of urban areas.  The framework will consist of a synergistic collaboration of three different areas: behavioral urban modeling, probabilistic graphical modeling, and visualization and computer graphics.  In machine learning and statistics, the area of probabilistic graphical modeling offers a flexible framework to build, estimate and simulate from models of substantial complexity and scale, with partially observed data.  By accounting for uncertainty and interdependencies, including aspects of dynamic equilibrium that arise in modeling the complex spatio-temporal dynamics of urban areas, the PI argues there is significant potential for breakthroughs in modeling large-scale urban systems.  Similarly, by integrating behavioral and geometrical dimensions of urban areas, he expects to exploit the power of behavioral simulations more effectively by filling in geometric details that behavioral models are not well suited to manage, and at the same time provide a powerful framework to generate 2D and 3D geometric representations of urban areas that are behaviorally and geometrically consistent.  The PI will take advantage of massive datasets available for urban areas, including parcel and building inventories, business establishment inventories, census data, household surveys, and GIS data on physical and political features, and will fuse these data into a coherent and consistent database to support his modeling objectives.  This data fusion will address imputation of missing data, accounting for complex spatial and relational connections among the data sources.  The PI will evaluate the accuracy and usability of his system through several deployments in diverse contexts.  The PI has elicited engagement from the Urban Land Institute, the European Research Council, and the Council for Scientific and Industrial Research.  Several organizations in the San Francisco Bay Area in California and the Puget Sound region in Washington will serve as testbeds for the research. Finally, the PI will collaborate with other NSF-funded research projects, such as the Drought Research Initiative Network, in order to investigate correlations between urban development and water/drought. <br/><br/>Broader Impacts:  The results of this multidisciplinary project will have a transformative effect on the area of urban simulation, in that they will enable non-professionals as well as the general public to better understand urban phenomena.  City planners, researchers, students, and citizens will be able to efficiently simulate urban processes not previously possible, and to visualize the effects of adopting different urban policies on urban livability and sustainability outcomes, and to address local and global concerns regarding equity, infrastructure, and economic development.  The framework will provide interactive desktop and web-based interfaces for configuring urban scenario inputs to a simulation that may reach petabytes in data size, and to visualize the simulation results using 2D aerial views, 3D city walkthroughs, and choroplethic maps and tables of indicators portraying the simulated area.  Thus, the work will also advance the fields of visualization and computer graphics, through development of new techniques for large-scale urban modeling and rendering.  The PI will develop an open-source system to make the results of this research widely available."
"1005539","High Dimensional Inference and Signal Recovery","DMS","STATISTICS","07/01/2010","06/10/2010","Lie Wang","MA","Massachusetts Institute of Technology","Standard Grant","Gabor J. Szekely","06/30/2013","$150,000.00","","liewang@math.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","MPS","1269","","$0.00","This research project is to reconstruct high-dimensional sparse signals based on a small number of measurements, possibly corrupted by noise. More precisely, four of the main objectives of the program are: 1) further weaken the conditions and strengthen the results of current methods. The investigator aims to push the boundary of the field by developing new theoretical tools to analyze the current algorithms; 2) analyze the connections among different methods to get a deeper understanding of the nature of sparse signal recovery problem; 3) extend the research to multichannel setup which involves simultaneously recovery of a set of signals; 4) develop courses for both graduate student and undergraduate student. Build research projects for graduate students. Make the undergraduate students at least be aware of the possible limitation of classical methods and suitable alternatives.<br/><br/>Due to advances in science and technology, scientists and engineers are now able to collect and process enormously large data sets of all kinds. Such data sets pose many statistical challenges not encountered in smaller scale studies. One of the key problems in this area is the reconstructing of high-dimensional sparse signals, which is a fundamental problem in signal processing. This and other related problems have attracted much interest in a number of fields including applied mathematics, electrical engineering, statistics, finance, and bioinformatics. The proposed research will benefit applications in these scientific areas, for instance the compression of audio, images, and video signals and the analysis of microarray data. It is also of critical importance in linear regression, signal modeling, and machine learning."
"1012042","AF: Large: Collaborative Research:  Compact Representations and Efficient Algorithms for Distributed Geometric Data","CCF","COMPUTATIONAL GEOMETRY","09/01/2010","08/22/2010","Piotr Indyk","MA","Massachusetts Institute of Technology","Standard Grant","Tracy J. Kimbrel","08/31/2014","$433,000.00","","indyk@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","7929","9218, HPCC","$0.00","Across many fields of science, engineering, and business, massive data sets are being generated at unprecedented rate by high-bandwidth sensors and cameras, large-scale simulations, or web-enabled large scale data collection.  Much of this data has a geometric character, either directly or indirectly.   For example, second generation LiDARs can map the earth's surface at 15-20 cm resolution; the Large Synoptic Telescope is set to produce about 30 terabytes of data each night; thirteen hours of video are uploaded to YouTube every minute; Facebook manages over 40 billion photos requiring more than one petabyte of data.<br/><br/>These data sets provide tremendous opportunities to enable novel capabilities that were unimaginable a few years ago.  Capitalizing on these opportunities, however, and transforming these massive amounts of heterogeneous data into useful information for vastly different types of applications and users requires solving challenging algorithmic problems.  An effective way of addressing this challenge is by designing efficient methods for producing informative yet succinct summaries of such geometric data sets.  These summaries must work at multiple scales, and allow a wide variety of queries to be answered approximately but efficiently.  The goal of this project is to study the theoretical underpinnings of compact representations and efficient algorithms for organizing, summarizing, cross-correlating, interlinking, and querying large distributed geometric data sets.<br/><br/>This project will design methods for computing summaries of many kinds of flavors, all with provable properties.  Summaries can be combinatorial and metric (core sets and kernels), algebraic (linear sketches), topological (persistence diagrams), feature-based, and structural (encoding self-similarities in the data).  The properties they aim to capture extend from low-level metric attributes, such as the diameter or width of a point set, to higher-level attributes revealing the internal structure of the data, as in the detection of symmetries and repeated patterns.  This processing must be done in the presence of uncertainty in data coming from sensors, and optimize multiple performance measures, including communication cost for data distributed across multiple locations in a network.  Another key aspect of this project is that it aims to understand not individual data sets in isolation but rather the inter-relationships and correspondences among different data sets, and to do so by communicating only summary information, without even having all the data in one place. <br/><br/>This work touches upon many topics in theoretical computer science and applied mathematics including low-distortion embeddings, compressive sensing, transportation metrics, spectral graph theory or harmonic analysis, machine learning, and computational topology."
"1031095","US-Mexico Workshop on Optimization and its Applications","OISE","Catalyzing New Intl Collab","11/01/2010","10/27/2010","Stephen Wright","WI","University of Wisconsin-Madison","Standard Grant","Harold Stolberg","10/31/2011","$21,525.00","","swright@cs.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","O/D","7299","5922, 5977, 7299","$0.00","This award provides funding for a joint research workshop on optimization to be held in Oaxaca, Mexico, in January 2011. The workshop is being organized jointly by Stephen Wright (University of Wisconsin-Madison) and Jose Luis Morales (Instituto Tecnologico Autonomo de Mexico). Its objectives include promotion of research collaboration between researchers in the two countries and providing an opportunity for students to interact with leading researchers and practitioners in the field.  The program will focus on topics such applications in energy and health care as well as methodologies and various theoretical aspects of optimization. The meeting will have strong participation from industry and will also include a short course for attending students and young researchers on imaging, machine learning, and energy systems. Mexican participants will be supported by a grant from CONACyT, the Mexican science agency.<br/>In addition to addressing theoretical topics at the frontier of optimization research, this workshop will also consider new computational applications to critical societal problems in the areas of energy and health-care systems. The participation of students from both countries will contribute to future developments in the field and establish personal relationships for future collaboration."
"0968295","Collaborative Research:  SoCS: Analysis of Social Media Driven By Theories of Political Psychology","IIS","INFORMATION TECHNOLOGY RESEARC, SPECIAL PROJECTS - CCF","08/15/2010","08/18/2010","David Redlawsk","NJ","Rutgers University New Brunswick","Standard Grant","William Bainbridge","07/31/2014","$388,111.00","","redlawsk@udel.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","1640, 2878","9215","$0.00","Understanding how people make decisions in complex settings is crucial in many application areas, including marketing, intelligence analysis, and political decision making. Traditionally, human decision-makers are modeled as rational agents seeking to maximize some mathematical measure of utility. In fact, however, people are overwhelmed in information-rich environments, and have developed cognitive and emotional strategies to navigate such environments. One such strategy is ""motivated reasoning"", where information is first evaluated subconsciously for emotional content, with the goal of maintaining an existing emotional commitment, and cognitive processing of the information is then conditioned on this emotional evaluation. Political scientists have demonstrated motivated reasoning in evaluation of both candidates and issues. In general, decision-making and information-gathering are strongly influenced by emotion, prior knowledge, and the social communities to which a person belongs. Evidence suggests that accurate models of human decision-making must be complex enough to model not only utility, but prior knowledge and beliefs, human cognitive abilities, and social context. Building such cognitive models requires substantially extending the state-of-the-art in machine learning. <br/><br/>In the past, the ability of researchers in political psychology to develop such complex models of decision-making was limited by the amount of data obtainable obtain from surveys or human-subject experiments. The recent explosion of on-line political communities provides an opportunity to overcome this limitation. We will model human behavior for socially-driven information gathering and decision-making tasks - specifically for political decisions - by combining human-subject experiments with analysis of large datasets of social media and social interactions."
"0957099","ChemMine Tools: an Open Source Framework for Chemical Genomics","DBI","ADVANCES IN BIO INFORMATICS","05/15/2010","04/09/2012","Thomas Girke","CA","University of California-Riverside","Continuing grant","Peter H. McCartney","04/30/2015","$601,032.00","","thomas.girke@ucr.edu","Research & Economic Development","RIVERSIDE","CA","925210217","9518275535","BIO","1165","1228, 1729, 9179, 9183, 9184, BIOT","$0.00","The University of California, Riverside is awarded a grant to develop ChemMineTools, an environment  to efficiently analyze and model large sets of small molecules along with their bioactivity data. It will provide unrestricted access to a scalable set of open source tools that integrates novel and existing algorithm. To maximize its utility spectrum for experimental and computational scientists, the analysis modules will be available from the powerful R environment, as well as an intuitive-to-use web interface. The specific objectives of this project are: (1) The development and implementation of accelerated compound search and clustering algorithms that scale to today's large databases with millions entries. This will focus on the expansion of the ultrafast EI-Search and EI-Clustering algorithms by embedding and indexing (EI). These multipurpose algorithms will be adopted to advanced similarity measures that can currently not be used for processing large databases due their insufficient speed performance. (2) The R package ChemMine R Tools will be developed. It will offer access to advanced clustering, machine learning and visualization functionalities along with interactive visualization tools. (3) User-friendly access to all analysis and visualization tools will be provided by the ChemMine Web Tools interface. (4) An educational outreach program will be offered to provide extensive training opportunities and to integrate underrepresented groups into this project. <br/><br/>By integrating novel and existing analysis routines in an efficient data mining environment, the project will disseminate and transform multidisciplinary concepts of powerful chemical approaches in modern biology.  Moreover, it will encourage young scientists to incorporate cheminformatics strategies into their daily research. Substantial educational resources for interdisciplinary training at the intersect of computational biology and cheminformatics will be provided by this project. Workshops will be offered to scientists, postdoctoral researchers, graduate, undergraduate, high school and other K-12 students. Members of underrepresented groups will participate in all aspects of this project. Extensive online workshop and software manuals will be provided to maximize the educational outreach of the activities. Further information about this project may be found at its website: http://cmtools.ucr.edu."
"0958508","Giving Zooniverse a Face: A Citizen Science Facebook Application","AST","EXTRAGALACTIC ASTRON & COSMOLO","03/01/2010","03/03/2010","Pamela Gay","IL","Southern Illinois University at Edwardsville","Standard Grant","Nigel Sharp","08/31/2012","$97,638.00","Christopher Impey","pgay@siue.edu","Campus Box 1046","Edwardsville","IL","620250001","6186503010","MPS","1217","1207, 7569, 7916","$0.00","The Internet is systematically changing how professional scientists and the public engage with astronomical data and with one another.  One of the most striking examples of new ways astronomers and the public are collaborating is the ""Galaxy Zoo.""  This online citizen science project has attracted over 230,000 people from around the globe to make over 100 million classifications of the morphology of galaxies.  These classifications, taken in aggregate, have been shown to be as accurate as the work of professional galaxy classifiers and have led to the publication or submission of 14 papers on topics as diverse as galaxy mergers, active galactic nuclei behavior, and machine learning.<br/><br/>The Galaxy Zoo is now transforming into a project called Zooniverse that will provide a framework for a diverse set of citizen science tasks.  As the Zooniverse grows, we need to find effective ways to expand the Galaxy Zoo user base, and this project uses Facebook to address this fundamental need.  One of the fastest growing virtual communities in the world, Facebook potentially offers exceptional leverage to scale up the number of research questions that can quickly be answered in the realms of both astronomy and education.<br/><br/>This project will develop the first two Facebook applications to engage citizen scientists.  The first will focus on getting people to do science by inviting them to pick from a ""Rogue's Gallery"" the galaxy that best matches a ""Most Wanted"" galaxy.  The second will use social networking to build the user base of the main Zooniverse website by linking users' Facebook and Galaxy Zoo profiles, allowing users to share socially the results of their citizen science work, to play trivia games, and to give friends ""galaxies"" to build their own ""clusters"".<br/><br/>The results of these galaxy classifications will be used in a Principal Component Analysis looking for new ways of categorizing galaxies.  The sample of roughly 250,000 galaxies provides a valid space in which to define meaningful categories of galaxy morphology, deriving for the first time natural classification classes from the appearance of a large sample of galaxies, instead of sorting them into categories defined a priori.<br/><br/>This project will also pave the way for future applications to migrate into Facebook by defining what characteristics do and do not attract users to engage in citizen science online.  This will allow other projects to engage users in astronomical research and will inform those working on non-astronomy citizen science about how better to create future applications."
"0945936","SBIR Phase I: Blue Shift Communities - Improving Online Technical Support","IIP","SMALL BUSINESS PHASE I","01/01/2010","11/17/2009","Franklin Harper","MN","Blue Shift Software Laboratory Inc","Standard Grant","Errol Arkilic","11/30/2010","$150,000.00","","max@harp3r.com","3120 E 25th St","Minneapolis","MN","554061450","6123878003","ENG","5371","5371, 6850, 9139, HPCC","$0.00","This Small Business Innovation Research (SBIR) Phase I project identifies a commercial opportunity to improve online technical support software. The company propose two complementary innovations that facilitate scalable, high-quality technical support. First, is a new system for building technical support communities that generate informational artifacts of lasting value by encouraging synthesis and reducing duplication of effort. The system combines the features from question and answer (Q&A) sites with wiki-inspired mechanisms for content synthesis and refinement. Second, is the development of machine learning-based methods for detecting low-quality user contributions, such as flames and angry rants. The company will experiment with both context-free models of text processing and context-rich user models to determine the most robust feature set for algorithmically-predicting the quality of a user contribution.<br/><br/>The complementary innovations share one primary goal: to improve the user experience in online technical support communities. By providing tools that are easier to use, the company will make it easier to ask questions and search for answers to technical problems. By making it easier to moderate low quality posts, the company will ease the burden on administrators, and increase the signal-to-noise ratio for regular users. By building tools that encourage synthesis and discourage duplication, the company has the potential to create high quality repositories of information that benefit a wide array of constituents."
"0945307","SBIR Phase I: Seamless mobile health monitoring platform using novel informatics-based methods","IIP","SMALL BUSINESS PHASE I","01/01/2010","11/17/2009","Steven LeBoeuf","nc","Valencell Inc.","Standard Grant","Muralidharan S. Nair","12/31/2010","$150,000.00","","leboeuf@valencell.com","920 Main Campus Drive","raleigh","nc","276065219","9194244432","ENG","5371","1357, 4096, 5371, 9139, HPCC, 1367","$0.00","This Small Business Innovation Research (SBIR)Phase I project will demonstrate the feasibility of a wireless, wearable, noninvasive platform for monitoring fitness and health that is virtually unnoticed by the user for maximum performance and convenience. Generating mobile health assessments, such as cardiovascular status, blood pressure, and metabolic activity, requires advanced algorithms that can extract meaningful correlations from orthogonal, low-acuity sensors within a mobile platform. However, commercially available health and fitness monitors cannot generate these assessments during everyday life activities. This limitation is caused by motion-related noise which convolutes sensor data and destroys sensor accuracy and reliability. This Phase I program will demonstrate the feasibility of a novel method for generating mobile health assessments within a mobile platform using machine learning probabilistic models. These models will then be programmed into algorithms that can be implemented in real-time via a single, wearable device in wireless communication with a smartphone. <br/><br/>The broader impact/commercial potential of this project will address critical problems across multiple industries. In the consumer fitness markets, mobile fitness assessments will promote healthier lifestyles and enhanced athletic performance. In mobile health (mHealth) markets, mobile health assessments will substantially reduce medical costs through prevention. In the military/first responder market, wireless vital status assessments will enable advanced performance training and maximization of triage resources. For clinical research markets, the mobile platform will enable new scientific studies correlating physiological reactions to new therapies in a real-world environment. Additionally, novel algorithms demonstrated in this Phase I program can eventually be applied towards wireless mortality monitoring in hospitals as well has home health monitoring for the outpatients and the elderly."
"0964429","RI: Medium: Collaborative Research: Recognition of Materials","IIS","ROBUST INTELLIGENCE","07/01/2010","06/18/2013","Shree Nayar","NY","Columbia University","Continuing grant","Kenneth C. Whang","06/30/2014","$285,959.00","","nayar@cs.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","7495","7924","$0.00","We live in a world made of diverse materials whose variations in appearance enrich our visual experience. It is also this variability of materials that adds daunting complexity to image understanding. This research program aims to establish the theoretical and computational foundation for automatic visual understanding and recognition of real-world materials. The program tackles this challenging problem from three key aspects, namely, deriving 1) novel hybrid physically-based and data-driven representations of the spatial, angular, spectral, temporal, and scale variations of material appearance, 2) active and passive methods for estimating the values of physically-based parameters that govern material appearance, and 3) single-image material recognition methods that leverage physically-based optical parameters as priors or invariants to guide machine learning techniques. These research thrusts lead to a comprehensive set of computational tools to recognize materials in real-world images despite their complex appearance variations, such as recognizing rusted metals, discerning soft cloth from hard concrete, identifying different fat content of milks, and labeling image regions with material traits like soft, hard, rough, and heavy.<br/><br/>The capabilities resulting from this program are crucial to a broad range of scenarios, for instance, to enable humanoid robots to understand that it should not squeeze the soft hands of a child, autonomous vehicles to understand what regions to avoid in a rugged terrain, visual analyses of tissues to help medical diagnosis, and automated inspection systems to reliably discover sub-standard quality food to prevent ill-health. The PIs work with research groups in these specific application areas to closely integrate the results from this project into their efforts. The results from this research are also broadly disseminated via publications, websites, databases, new courses and symposiums."
"1052925","Collaborative Research: Biological Shape Spaces, Transforming Shape into Knowledge","DBI","ADVANCES IN BIO INFORMATICS, Cross-BIO Activities","09/15/2010","09/10/2010","Ge Yang","PA","Carnegie-Mellon University","Standard Grant","Anne Maglia","08/31/2014","$203,193.00","","geyang@andrew.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","BIO","1165, 7275","1165, 1719, 1729, 7275, 7969, 9179, 9183, 9184, BIOT","$0.00","Collaborative Research: Biological Shape Spaces, Transforming Shape into Knowledge<br/><br/>This project will develop a framework to represent, analyze and interpret shapes extracted from images, supporting a wide range of biological investigations. The primary objectives are: (1) to develop a mathematical framework and computational tools for the quantification and analysis of shapes; (2) to integrate these computational models with machine learning and statistical inference methods to enable new discoveries, transforming imaging data into biological knowledge; (3) to deliver novel quantitative methodologies for shape analysis that start from a biological premise, rather than a purely geometric one. The aim is thus not only to quantitatively describe shape, but to develop methods for linking morphological variation to its underlying biological causes.  To ensure that the project focuses on methods that are most promising to biology with significant breadth of application, model and tool development will be guided and supported by a set of diverse case studies, ranging from the sub-cellular to organismal scales. <br/><br/>Shape represents a complex and rich source of biological information that is fundamentally linked to underlying mechanisms and function. However, shape is still often examined on a qualitative basis in many disciplines in biology, an approach that is time consuming and prone to human subjectivity. While ad hoc quantitative methods do exist, they are often inaccessible to non-experts and do not easily generalize to a wide variety of problems. The inability of biologists to systematically link shape to genetics, development, environment, function and evolution often precludes advances in biological research spanning diverse spatial and temporal scales, from the movement of molecules within a cell to adaptive changes in organismal morphology. The primary goal of this project is to develop a new suite of widely applicable quantitative methods and tools into the study of biological shape to address the significant need for consistent and repeatable analysis of shape data."
"1052688","Collaborative Research: Biological Shape Spaces, Transforming Shape into Knowledge","DBI","ADVANCES IN BIO INFORMATICS, Cross-BIO Activities","09/15/2010","09/10/2010","Sandra Rugonyi","OR","Oregon Health & Science University","Standard Grant","Anne Maglia","08/31/2014","$352,654.00","","rugonyis@bme.ogi.edu","3181 S W Sam Jackson Park Rd","Portland","OR","972393098","5034947784","BIO","1165, 7275","1165, 1719, 1729, 7275, 7969, 9183, 9184, BIOT","$0.00","Collaborative Research: Biological Shape Spaces, Transforming Shape into Knowledge<br/><br/>This project will develop a framework to represent, analyze and interpret shapes extracted from images, supporting a wide range of biological investigations. The primary objectives are: (1) to develop a mathematical framework and computational tools for the quantification and analysis of shapes; (2) to integrate these computational models with machine learning and statistical inference methods to enable new discoveries, transforming imaging data into biological knowledge; (3) to deliver novel quantitative methodologies for shape analysis that start from a biological premise, rather than a purely geometric one. The aim is thus not only to quantitatively describe shape, but to develop methods for linking morphological variation to its underlying biological causes.  To ensure that the project focuses on methods that are most promising to biology with significant breadth of application, model and tool development will be guided and supported by a set of diverse case studies, ranging from the sub-cellular to organismal scales. <br/><br/>Shape represents a complex and rich source of biological information that is fundamentally linked to underlying mechanisms and function. However, shape is still often examined on a qualitative basis in many disciplines in biology, an approach that is time consuming and prone to human subjectivity. While ad hoc quantitative methods do exist, they are often inaccessible to non-experts and do not easily generalize to a wide variety of problems. The inability of biologists to systematically link shape to genetics, development, environment, function and evolution often precludes advances in biological research spanning diverse spatial and temporal scales, from the movement of molecules within a cell to adaptive changes in organismal morphology. The primary goal of this project is to develop a new suite of widely applicable quantitative methods and tools into the study of biological shape to address the significant need for consistent and repeatable analysis of shape data."
"1053130","Collaborative Research: Biological Shape Spaces, Transforming Shape into Knowledge","DBI","ADVANCES IN BIO INFORMATICS, Cross-BIO Activities","09/15/2010","09/10/2010","Rolf Mueller","VA","Virginia Polytechnic Institute and State University","Standard Grant","Anne Maglia","08/31/2014","$260,611.00","","rolf.mueller@vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","BIO","1165, 7275","1165, 1719, 1729, 7275, 7969, 9183, 9184, BIOT","$0.00","Collaborative Research: Biological Shape Spaces, Transforming Shape into Knowledge<br/><br/>This project will develop a framework to represent, analyze and interpret shapes extracted from images, supporting a wide range of biological investigations. The primary objectives are: (1) to develop a mathematical framework and computational tools for the quantification and analysis of shapes; (2) to integrate these computational models with machine learning and statistical inference methods to enable new discoveries, transforming imaging data into biological knowledge; (3) to deliver novel quantitative methodologies for shape analysis that start from a biological premise, rather than a purely geometric one. The aim is thus not only to quantitatively describe shape, but to develop methods for linking morphological variation to its underlying biological causes.  To ensure that the project focuses on methods that are most promising to biology with significant breadth of application, model and tool development will be guided and supported by a set of diverse case studies, ranging from the sub-cellular to organismal scales. <br/><br/>Shape represents a complex and rich source of biological information that is fundamentally linked to underlying mechanisms and function. However, shape is still often examined on a qualitative basis in many disciplines in biology, an approach that is time consuming and prone to human subjectivity. While ad hoc quantitative methods do exist, they are often inaccessible to non-experts and do not easily generalize to a wide variety of problems. The inability of biologists to systematically link shape to genetics, development, environment, function and evolution often precludes advances in biological research spanning diverse spatial and temporal scales, from the movement of molecules within a cell to adaptive changes in organismal morphology. The primary goal of this project is to develop a new suite of widely applicable quantitative methods and tools into the study of biological shape to address the significant need for consistent and repeatable analysis of shape data."
"1053036","Collaborative Research: Biological Shape Spaces, Transforming Shape into Knowledge","DBI","ADVANCES IN BIO INFORMATICS, Cross-BIO Activities","09/15/2010","09/10/2010","Charless Fowlkes","CA","University of California-Irvine","Standard Grant","Anne Maglia","08/31/2014","$284,388.00","","fowlkes@ics.uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","BIO","1165, 7275","1165, 1719, 1729, 7275, 7969, 9183, 9184, BIOT","$0.00","Collaborative Research: Biological Shape Spaces, Transforming Shape into Knowledge<br/><br/>This project will develop a framework to represent, analyze and interpret shapes extracted from images, supporting a wide range of biological investigations. The primary objectives are: (1) to develop a mathematical framework and computational tools for the quantification and analysis of shapes; (2) to integrate these computational models with machine learning and statistical inference methods to enable new discoveries, transforming imaging data into biological knowledge; (3) to deliver novel quantitative methodologies for shape analysis that start from a biological premise, rather than a purely geometric one. The aim is thus not only to quantitatively describe shape, but to develop methods for linking morphological variation to its underlying biological causes.  To ensure that the project focuses on methods that are most promising to biology with significant breadth of application, model and tool development will be guided and supported by a set of diverse case studies, ranging from the sub-cellular to organismal scales. <br/><br/>Shape represents a complex and rich source of biological information that is fundamentally linked to underlying mechanisms and function. However, shape is still often examined on a qualitative basis in many disciplines in biology, an approach that is time consuming and prone to human subjectivity. While ad hoc quantitative methods do exist, they are often inaccessible to non-experts and do not easily generalize to a wide variety of problems. The inability of biologists to systematically link shape to genetics, development, environment, function and evolution often precludes advances in biological research spanning diverse spatial and temporal scales, from the movement of molecules within a cell to adaptive changes in organismal morphology. The primary goal of this project is to develop a new suite of widely applicable quantitative methods and tools into the study of biological shape to address the significant need for consistent and repeatable analysis of shape data."
"0960480","RUI:  New Tools for Characterizing Protein Dynamics","DBI","ADVANCES IN BIO INFORMATICS","09/01/2010","06/06/2012","Shawn Newsam","CA","University of California - Merced","Continuing Grant","Peter McCartney","08/31/2015","$647,841.00","Michael Colvin, Ajay Gopinathan","snewsam@ucmerced.edu","5200 North Lake Road","Merced","CA","953435001","2092012039","BIO","1165","1719, 9178, 9179, 9183, 9184, 9229, BIOT","$0.00","The University of California at Merced has been awarded a grant to develop new tools and protocols for characterizing protein dynamics. While the focus of protein biophysics has traditionally been on the structure and interactions of the natively folded protein, there is now rapidly growing interest in the properties of denatured protein structures as related to the mechanisms of protein folding and in the function of so-called intrinsically disordered proteins. Molecular simulations have great promise in helping to understand disordered proteins; however, new tools are needed to characterize the defining property of these constantly changing systems--their dynamics. The long-term goal of this research is to develop new tools and protocols for determining the two most fundamental descriptors of protein dynamics, the effective dimensionality of the motion and the volume of structural space sampled by the protein. The tools will help answer the many fundamental, unresolved questions about the denatured or disordered states of proteins that are central to understanding protein function and folding. Specifically, these tools will determine to what extent denatured or intrinsically disordered proteins behave as random polymers or have motions constrained to lower dimensional dynamics. Additionally these tools will elucidate whether there are differences between the dynamics of intrinsically disordered proteins and natively folded proteins in the early stages of folding. Finally, these tools will help in determining the efficiency of different molecular simulations in sampling the denatured or disordered states of proteins. <br/><br/>This project will be an ideal multidisciplinary training experience for the students involved since it brings together three scientific disciplines: machine learning, molecular simulation, and polymer biophysics. The tools developed in this project will be used by graduate and undergraduate students in both molecular simulation and computer sciences courses at UC Merced and by students in summer research projects. The molecular dynamics community has a well established tradition of software tools that are developed and maintained by individual research groups and freely distributed to very large user bases. These established channels will be used to freely distribute all of the software tools developed in this project in source code format along with documentation and test data sets. More information can be found at the project website: http://vision.ucmerced.edu/projects/protein.<br/>"
"0964420","RI: Medium: Collaborative Research:  Recognition of Materials","IIS","Robust Intelligence","07/01/2010","06/18/2013","Ko Nishino","PA","Drexel University","Continuing grant","Kenneth Whang","06/30/2015","$392,638.00","","kon@drexel.edu","1505 Race St, 10th Floor","Philadelphia","PA","191021119","2158955849","CSE","7495","7924","$0.00","We live in a world made of diverse materials whose variations in appearance enrich our visual experience. It is also this variability of materials that adds daunting complexity to image understanding. This research program aims to establish the theoretical and computational foundation for automatic visual understanding and recognition of real-world materials. The program tackles this challenging problem from three key aspects, namely, deriving 1) novel hybrid physically-based and data-driven representations of the spatial, angular, spectral, temporal, and scale variations of material appearance, 2) active and passive methods for estimating the values of physically-based parameters that govern material appearance, and 3) single-image material recognition methods that leverage physically-based optical parameters as priors or invariants to guide machine learning techniques. These research thrusts lead to a comprehensive set of computational tools to recognize materials in real-world images despite their complex appearance variations, such as recognizing rusted metals, discerning soft cloth from hard concrete, identifying different fat content of milks, and labeling image regions with material traits like soft, hard, rough, and heavy.<br/><br/>The capabilities resulting from this program are crucial to a broad range of scenarios, for instance, to enable humanoid robots to understand that it should not squeeze the soft hands of a child, autonomous vehicles to understand what regions to avoid in a rugged terrain, visual analyses of tissues to help medical diagnosis, and automated inspection systems to reliably discover sub-standard quality food to prevent ill-health. The PIs work with research groups in these specific application areas to closely integrate the results from this project into their efforts. The results from this research are also broadly disseminated via publications, websites, databases, new courses and symposiums."
"1032683","SDCI Data: Improvement: Java Graphical Authorship Attribution Program (JGAAP)","OAC","SOFTWARE DEVELOPEMENT FOR CI","09/01/2010","08/30/2010","Patrick Juola","PA","Duquesne University","Standard Grant","Amy Walton","08/31/2014","$1,622,036.00","","juola@mathcs.duq.edu","Room 310 Administration Building","Pittsburgh","PA","152193016","4123961537","CSE","7683","7683","$0.00","Recent developments in machine learning and corpus linguistics have shown it to be possible to make automatic determinations about authorship using statistics; the NSF- funded JGAAP (Java Graphical Authorship Attribution Program) system has been part of these developments.  JGAAP has helped support the emerging authorship attribution community and create a useful tool for a wide variety of scholastic specialties. <br/><br/>Although JGAAP incorporates thousands of possible methods, there are many more in the literature that have been proposed but not rigorously tested. Comparative testing on a large scale will require the development of new methods and test corpora. In addition, there are many key problems to address to meet the needs of the community, such as the open class problem, the adversarial problem, and the coauthorship problem. Finally, we will examine applications of JGAAP and similar systems to key areas in linguistic profiling, such as determining gender, education, native language, psychological profile, medical condition, age (of document or writer), or even attempted deceptiveness.  Again, by applying a rigorous testing method to these new problems and corpora, the project can establish accuracy benchmarks for various techniques (under the various testing conditions), find new combinations resulting in improved techniques, and establish a recommendation for 'best practices.' <br/><br/>Improved authorship attribution will be immediately useful both to scholars and in broader social contexts, such as law enforcement and forensics where there are direct demands for this kind of security technology. The historical/social analysis will also provide better access between the related disciplines of digital humanities, sociology, history, and computer science, providing the basis for a better understanding of traditional humanities issues. Profiling work can help medical and psychological practitioners by providing a non-invasive method to detect certain aspects of a person's mind. The software developed (and the planned development/distribution process) will help improve the effectiveness of both digital humanities scholarship and computer science, especially through the establishment of software review standards and processes. In particular, by providing direct evidence of the conditions and expected error rates involved in various techniques, the information gained will help authorship attribution meet the Daubert criteria for expert evidence, allowing authorship attribution to be used in a formal legal setting. Finally, the funding of this research will help support the unique interdisciplinary Duquesne University Computational Mathematics program, providing a broader access to an unusual and atypical audience for technological education."
"0964562","RI: Medium: Collaborative Research:  Recognition of Materials","IIS","Robust Intelligence","07/01/2010","07/15/2013","Srinivasa Narasimhan","PA","Carnegie-Mellon University","Continuing grant","Kenneth Whang","06/30/2015","$406,581.00","","srinivas@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","7495","7495, 7924, 9251","$0.00","We live in a world made of diverse materials whose variations in appearance enrich our visual experience. It is also this variability of materials that adds daunting complexity to image understanding. This research program aims to establish the theoretical and computational foundation for automatic visual understanding and recognition of real-world materials. The program tackles this challenging problem from three key aspects, namely, deriving 1) novel hybrid physically-based and data-driven representations of the spatial, angular, spectral, temporal, and scale variations of material appearance, 2) active and passive methods for estimating the values of physically-based parameters that govern material appearance, and 3) single-image material recognition methods that leverage physically-based optical parameters as priors or invariants to guide machine learning techniques. These research thrusts lead to a comprehensive set of computational tools to recognize materials in real-world images despite their complex appearance variations, such as recognizing rusted metals, discerning soft cloth from hard concrete, identifying different fat content of milks, and labeling image regions with material traits like soft, hard, rough, and heavy.<br/><br/>The capabilities resulting from this program are crucial to a broad range of scenarios, for instance, to enable humanoid robots to understand that it should not squeeze the soft hands of a child, autonomous vehicles to understand what regions to avoid in a rugged terrain, visual analyses of tissues to help medical diagnosis, and automated inspection systems to reliably discover sub-standard quality food to prevent ill-health. The PIs work with research groups in these specific application areas to closely integrate the results from this project into their efforts. The results from this research are also broadly disseminated via publications, websites, databases, new courses and symposiums."
"1016623","GV: Small: Collaborative Research: Supporting Knowledge Discovery through a Scientific Visualization Language","IIS","GRAPHICS & VISUALIZATION, EPSCoR Co-Funding","11/01/2010","04/26/2011","David Laidlaw","RI","Brown University","Standard Grant","Maria Zemankova","10/31/2014","$268,596.00","","dhl@cs.brown.edu","BOX 1929","Providence","RI","029129002","4018632777","CSE","7453, 9150","7453, 7923, 9150, 9251","$0.00","This collaborative research brings together computer scientists from University of Southern Mississippi (USM) and Brown University and neuroscientists from the University of Mississippi Medical Center (UMMC) to study the design of a scientific visualization language (SVL). Despite the numerous visualization approaches already devised, visualization remains more of an art than a science. Grounded in theories and methods from human-centered computing, machine learning, and cognitive psychology, this work is to develop and evaluate a scientific visualization language (SVL) to provide a principled way to help scientists understand how and why visualizations work. Tools and theories developed in this project can lead to efficient knowledge discovery to help neuroscientists study brains using diffusion tensor magnetic resonance imaging (DTI).<br/><br/>This work has the following specific objectives and outcomes: (1) close collaboration with scientists to discover, refine, and verify a symbol space, (2) a semantic space that describes the relationship among symbols, (3) a testbed that implements SVL for neuroscientists to compose visualizations, (4) development of new and enhanced courses at University of Southern Mississippi and Brown University, and (5) wide dissemination of the research outcomes through open-source software, experimental data, open labs, publications, and presentations.<br/><br/>This project is expected to have broad impact. It may lead to significantly better approaches to human knowledge discovery and decision making in many disciplines where visualizations have found successful application, including neuroscience, biomedicine, bioinformatics, biology, chemistry, geosciences, business, economics, and education. Undergraduate and graduate students are expected to participate in the research through our courses, and student exchanges are planned between USM and Brown. K-12 students can visit the USM lab while the project is in progress. Software and results will be disseminated via the project Web site (https://sites.google.com/site/simplevisualizationlanguage)."
"0963988","IDBR: Development of CytoIQ, an Adaptive Cytometer to Measure the Noisy of Dynamics of Gne Expression in Individual Live Cells","DBI","INSTRUMENTAT & INSTRUMENT DEVP","06/15/2010","05/23/2011","Jean Peccoud","VA","Virginia Polytechnic Institute and State University","Standard Grant","Joyce Fernandes","05/31/2012","$176,373.00","","jean.peccoud@colostate.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","BIO","1108","9178, 9184, 9251, BIOT","$0.00","The Virginia Bioinformatics Institute at Virginia Tech is awarded a grant to develop Cyto.IQ, an adaptive imaging system specifically designed to characterize the noisy dynamics of gene expression and other molecular interactions in individual live cells. Cyto.IQ analyzes microscopic images on the fly to produce statistical plots and other quantitative indicators capturing important parameters of the cell physiology. The instrument has the capability to optimize the frequency of image acquisition and the total number of images taken using machine learning algorithms. It finds an optimal tradeoff between the cost of an experiment and the information it generates using a priori knowledge of the expected gene expression dynamics and previously acquired data. The control software is able to determine what cells to observe and when to observe them to ensure a fast convergence of statistical estimators while minimizing adverse effects of light exposure, and the overall duration of the experiment. <br/>Cyto.IQ is specifically designed to meet the needs of systems biologists, bioengineers, or biophysicists who are developing quantitative models of gene networks. Due to the noise affecting gene expression mechanisms, this rapidly growing community of users needs an instrument to observe the state of many individual cells over time. Current methods used to extract this type of data out of time-series of images collected using standard imaging platforms are inherently inefficient. They represent a major obstacle to the refinement of our understanding of the dynamics of cellular processes. Cyto.IQ increases the productivity of scientists working in this field by reducing the time it takes to perform an experiment and the number of experiments needed to collect suitable data sets. <br/>The adaptive control software is open source and available from www.cytoiq.org."
"0960298","MRI-R2:  Acquisition of Airborne Remote Sensing System for Oceanographic, Terrestrial, and Environmental Research","OCE","MAJOR RESEARCH INSTRUMENTATION","03/01/2010","02/25/2010","W Melville","CA","University of California-San Diego Scripps Inst of Oceanography","Standard Grant","Kandace Binkley","02/29/2012","$841,848.00","Daniel Cayan, Paul Linden, Robert Guza, Mati Kahru","kmelville@ucsd.edu","8602 La Jolla Shores Dr","LA JOLLA","CA","920930210","8585341293","GEO","1189","0000, 1189, 6890, OTHR","$841,848.00","""This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).""<br/><br/>The PI?s request funding for the acquisition, integration and testing of the components of an airborne remote sensing system comprised of a waveform airborne LIDAR, a hyperspectral camera, a high-resolution video camera, a GPS/inertial motion unit, data acquisition and post-processing hardware and software, an existing IR video camera, and the labor and supplies to integrate and test the system over two years. <br/><br/>Satellite remote sensing has enabled remarkable progress in the ocean, earth, atmospheric and environmental sciences through its ability to provide global coverage with ever increasing spatial resolution. However, the temporal coverage of low earth orbiting satellites is not optimal. This temporal coverage may be sufficient for mesoscale ocean processes with time scales of a month, but is not sufficient for ocean processes that respond to atmospheric forcing with time scales of hours to days and other submesoscale ocean processes, especially coastal processes, both physical and biological, and air-sea-land interactions in the coastal zone. In the hydrological sciences the time scales can range from hours for flash floods, to days for snowfall on mountain ranges, to months for the snowmelt into the river systems. On an even smaller scale, remote sensing of the built environment catalyzes research into more resource-efficient and sustainable cities but requires building-resolving thermal resolution of a few meters. For this range of phenomena, satellite data are very useful but not optimal, and need to be supplemented with higher resolution airborne data that are not tied to the strict schedule of a satellite orbit. This proposal addresses these needs, in research and training in these areas of science and engineering.<br/><br/>Broader Impacts<br/><br/>The data provided by the system should well support the stated research areas of coastal process and oceanography, bio, hydrology, and built areas. These are all relevant to California and to larger issues in climate and environmental change. The combination of LIDAR, a visual camera, and hyperspectral imaging is important to really get the whole story. Aside from the science that this system will enable, the data sets will be of huge interest to machine learning and robotics researchers as well. Outreach and education efforts benefit from programs in place, and seem strong. Students will love the flyovers of their schools and neighborhoods - this simple demo should be a high priority. The proposed collaboration with COSEE CA should prove fruitful and will provide the opportunity to excite a new generation of young scientists.<br/>"
"0922738","Genomics of Comparative Seed Evolution","IOS","Plant Genome Research Resource","08/01/2010","08/14/2013","Gloria Coruzzi","NY","New York University","Continuing grant","Anne W. Sylvester","07/31/2015","$3,762,501.00","Dennis Stevenson, Robert DeSalle, Dennis Shasha, W. Richard McCombie","gloria.coruzzi@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","BIO","7577","1329, 7218, 9109, BIOT","$0.00","PI: Gloria M. Coruzzi (New York University)<br/><br/>CoPIs: Robert DeSalle (American Museum Natural History), W. Richard McCombie (Cold Spring Harbor Laboratories), Dennis E. Shasha (New York University), and Dennis W. Stevenson (New York Botanical Garden)<br/><br/>Senior Personnel: Eric Brenner (New York Botanical Garden), Manpreet Katari (New York University), Ernest Lee (American Museum of Natural History), and Robert Martienssen (Cold Spring Harbor Laboratory)<br/><br/>The goal of this project is to exploit plant genome diversity to discover new genes involved in the development of seeds. This project combines the expertise of scientists from four research/educational institutions specializing in evolution and genomics to build upon a previously funded pilot project to generate new data, resources, and bioinformatic tools and analytical pipelines to enable functional trait-to-gene predictions for any species based on phylogeny and/or machine learning approaches.  It is envisioned that the data and software resources generated will empower Comparative Genomic researchers to exploit plant diversity to identify genes associated with any trait of interest or economic value.  All sequence data will be available at GenBank and all germplasm through the New York Botanical Gardens.  In addition, the public can access data, tools, and resources generated from this project at the following links: OrthologID (http://nypg.bio.nyu.edu/orthologid), BIGPLANTv1 (http://nypg.bio.nyu.edu/orthologid/bigplant) and ViCoGenTA (http://nypg.bio.nyu.edu/vicogenta)."
"1017719","TC: Small: THWART: Trojan Hardware in Wireless ICs - Analysis and Remedies for Trust","CNS","SPECIAL PROJECTS - CISE, TRUSTWORTHY COMPUTING","09/01/2010","06/06/2011","Yiorgos Makris","CT","Yale University","Standard Grant","carl landwehr","09/30/2011","$507,877.00","","yiorgos.makris@utdallas.edu","Office of Sponsored Projects","New Haven","CT","065208327","2037854689","CSE","1714, 7795","7923, 9178, 9251","$0.00","Towards enhancing trustworthiness of wireless integrated circuits, this project investigates the problem of hardware Trojans in the analog/RF domain. Hardware Trojans are maliciously-intended modifications to fabricated integrated circuits, making them capable of additional functionality which is unknown to the designer and user, but which can be exploited by the perpetrator after chip deployment to sabotage or incapacitate it, or to steal sensitive information. The motivation for this research is two-fold: First, partly because of design outsourcing and migration of fabrication to low-cost areas across the globe, and partly because of increased reliance on external intellectual property and design automation software, the integrated circuit supply chain is now considered far more vulnerable to such malicious modifications than ever before. Second, wireless integrated circuits constitute an indispensable part of modern electronic systems and their ability to communicate data (possibly encrypted) over public channels makes them a prime attack candidate. To address this problem, this project focuses on (i) delineating the threat and potential impact of hardware Trojans in wireless cryptographic ICs, (ii) elucidating the shortcomings of existing test methods in exposing them, (iii) developing preventive countermeasures for obfuscating the chip design and complicating the development of hardware Trojans, and (iv) devising efficient hardware Trojan detection methods based on statistical analysis and machine learning. The anticipated impact of this research lies in the attainment of a better understanding of the hardware Trojan threat and in the development of appropriate remedies, thus enabling secure deployment of wireless integrated circuits and fostering technology trustworthiness."
"1003220","NSF Postdoctoral Fellowship in Biology FY 2010","DBI","BIO INFOR POSTDOCT RSCH FELLOW","09/01/2010","07/22/2010","Jeff Clune","MI","Clune Jeff","Fellowship","Carter Kimsey","08/31/2012","$123,000.00","","","","Lansing","MI","489123016","","BIO","1398","1398, 9179, ","$0.00","This action funds an NSF Postdoctoral Research Fellowship for FY 2010. The fellowship supports a research and training plan entitled ""Identifying the Environmental Attributes that Drive the Evolution of Complexity in Organisms"" for Jeff Clune. The host institution for this research is Cornell University, and the sponsoring scientist is Lipson Hod.<br/><br/>One of the major open questions in biology is how evolution produces the complexity seen in natural forms, such as the structural organization (regularity, modularity, and hierarchy) of bodies and brains. The environmental pressures that drive the evolution of such structural organization are poorly understood largely because they are difficult to study experimentally in natural systems. Understanding structural organization is important, however, because it is a fundamental property of biological systems, because it affects how organisms will respond to novel environments, and because it increases the speed of evolutionary adaptation, which opens possibilities for improving bioengineering. The Fellow is taking advantage of computational systems that exhibit evolutionary dynamics to identify the environmental attributes that drive the evolution of structural organization. Specifically, the research (1) develops novel statistical algorithms to quantify structural organization, (2) uses evolution experiments in computers to test whether eight different environmental attributes drive the evolution of structural organization, (3) quantifies structural organization in biological phenotypes, (4) tests if the environmental drivers of structural organization in computer evolution experiments (identified in 2) also drive structural organization in biological phenotypes (using data from 3). <br/><br/>The training goals include enhancing mathematical and machine learning skills and better understanding computational models of brains. The broader impacts include development of a website that allows visitor participation in evolving structurally-organized phenotypes, and publishing open-source software tools for quantifying structural organization."
"0968480","SoCS: Collaborative Research: Conversational Dynamics in Online Support Groups","IIS","Information Technology Researc, SOCIAL-COMPUTATIONAL SYSTEMS","09/15/2010","09/17/2010","John Levine","PA","University of Pittsburgh","Standard Grant","Betty Tuller","08/31/2015","$112,229.00","","jml@pitt.edu","300 Murdoch Building","Pittsburgh","PA","152603203","4126247400","CSE","1640, 7953","7752, 7953, 9215, 9251","$0.00","Health support groups, including those on the internet, can substantially benefit participants, but the social processes responsible for these benefits are unclear.  A team of researchers led by Robert Kraut at Carnegie-Mellon University will explore how the conversational dynamics of online cancer support groups influence group functioning and participant quality of life and will develop computational tools that can be used to analyze online conversations and improve their effectiveness. The research project has four specific goals. (1) To understand how conversational episodes in online support groups facilitate social support. For example, what must a person say to get others to respond empathically? (2) To understand how support in these groups influences group commitment and affects health outcomes. (3) To develop computational tools to make the analysis of large datasets of health conversations tractable. (4) To use these tools to improve the training of support group facilitators.<br/><br/>Online health support groups are popular, being used by about 58% of American adults. Identifying the role of communication in online cancer support groups will provide valuable information to users and facilitators of these groups and will enhance their training. Moreover, a tool for analyzing large corpora of conversational data will facilitate the work of researchers who are interested in conversational behavior in other kinds of online groups"
"1018374","SHF:  Small:  Open Source Software Components:  Utilization Assessment and Automatic Retrieval","CCF","Software & Hardware Foundation, SOFTWARE ENG & FORMAL METHODS","08/01/2010","06/01/2012","Cristina Lopes","CA","University of California-Irvine","Continuing Grant","Sol Greenspan","07/31/2015","$499,619.00","","lopes@ics.uci.edu","141 Innovation Drive, Ste 250","Irvine","CA","926173213","9498247295","CSE","7798, 7944","9150, 9215, HPCC","$0.00","Software quality has been an important but illusive concept for several decades, with experts different, sometimes conflicting, guidelines. Using an ecosystem of about 20,000 open source Java projects, this study will to try to discover correlations between open source component utilization and software quality metrics, in order to provide the strongest empirical evidence yet as to how the several metrics pertaining to software quality correlate with actual utilization of reusable components. This study will provide a scientific basis to some of the existing guidelines and to the dismissal of some others. If no correlations are found, this result will disrupt current conceptualizations of component quality, forcing researchers and developers to reassess their understanding of software quality and reusable components. Software being a foundation of modern society, and Open Source development being a significant movement in society at large, it is critical to gain a deeper understanding of software quality on a global scale, leading to the development of innovative tools and methods.<br/><br/>The metrics used in this study are those defined by the SQO-OSS Quality Model. To understand correlations,  the following method will be used. First the dependency graph will be built, capturing software dependencies at the global scale. This requires overcoming technical challenges in cleaning up and clustering the data, as real world projects contain all sorts of idiosyncrasies related to the use of external components. Second, a suite of utilization metrics will be developed using this global dependency graph that capture the depth and breadth of component usage by these projects. Finally, the SQO-OSS Quality metrics for a significant subset of projects in the data set will be computed and compared with the projects' utilization metrics in order to reveal the correlations."
"0964350","NeTS: Medium: Collaborative Research: A Comprehensive Approach for Data Quality and Provenance in Sensor Networks","CNS","Networking Technology and Syst, TRUSTWORTHY COMPUTING, Secure &Trustworthy Cyberspace","06/01/2010","05/31/2012","Murat Kantarcioglu","TX","University of Texas at Dallas","Continuing grant","Thyagarajan Nandagopal","05/31/2014","$150,000.00","","muratk@utdallas.edu","800 W. Campbell Rd., AD15","Richardson","TX","750803021","9728832313","CSE","7363, 7795, 8060","7924","$0.00","Sensor networks enable real-time gathering of large amounts of data that can be mined and analyzed for taking critical actions. As such, sensor networks are a key component of decision-making infrastructures. A critical issue in this context is the trustworthiness of the data being collected. Data integrity and quality decide the trustworthiness of data.  Data integrity can be undermined not only because of errors by users, measurement devices and applications, but also because of malicious subjects who may inject inaccurate data with the goal of deceiving the data users. A fundamental tradeoff exists between data quality and the cost to gather and protect this data, e.g., in terms of sensor node energy. This project focuses on a multi-faceted solution to the problem of assessing integrity of data streams in sensor networks, taking into account cost and energy constraints. Key elements of the solution are: (a) a cyclic framework supporting the assessment of sensor data trustworthiness based on provenance, and sensor trustworthiness based on data that sensors provide; (b) strategies for continuously updating trust scores of sensor data and nodes; (c) a game-theoretic model to analyze and mitigate the risks due to active adversaries that try to undermine data integrity; (d) protocols for sensor network sleep/wake scheduling and routing that balance the data quality and energy efficiency tradeoff. The project also includes the development of tools for assessing data trustworthiness, and experimental evaluation of the system performance.  The research has impact on healthcare, homeland security, and applications in several other domains."
"0968485","SoCS: Collaborative Research: Conversational Dynamics in Online Support Groups","IIS","Information Technology Researc, SOCIAL-COMPUTATIONAL SYSTEMS","09/15/2010","06/19/2012","Robert Kraut","PA","Carnegie-Mellon University","Standard Grant","Betty Tuller","08/31/2014","$667,444.00","Carolyn Rose","kraut@andrew.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122688746","CSE","1640, 7953","7752, 7953, 9215, 9251","$0.00","Health support groups, including those on the internet, can substantially benefit participants, but the social processes responsible for these benefits are unclear.  A team of researchers led by Robert Kraut at Carnegie-Mellon University will explore how the conversational dynamics of online cancer support groups influence group functioning and participant quality of life and will develop computational tools that can be used to analyze online conversations and improve their effectiveness. The research project has four specific goals. (1) To understand how conversational episodes in online support groups facilitate social support. For example, what must a person say to get others to respond empathically? (2) To understand how support in these groups influences group commitment and affects health outcomes. (3) To develop computational tools to make the analysis of large datasets of health conversations tractable. (4) To use these tools to improve the training of support group facilitators.<br/><br/>Online health support groups are popular, being used by about 58% of American adults. Identifying the role of communication in online cancer support groups will provide valuable information to users and facilitators of these groups and will enhance their training. Moreover, a tool for analyzing large corpora of conversational data will facilitate the work of researchers who are interested in conversational behavior in other kinds of online groups"
"1035866","CPS: Medium: Collaborative Research: Dynamic Routing and Robotic Coordination for Oceanographic Adaptive Sampling","CNS","Information Technology Researc, CPS-Cyber-Physical Systems","09/15/2010","09/07/2010","Gaurav Sukhatme","CA","University of Southern California","Standard Grant","Radhakisan Baheti","08/31/2015","$345,000.00","","gaurav@cs.usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","1640, 7918","7918, 7924","$0.00","The objective of this research is the design of innovative routing,<br/>planning and coordination strategies for robot networks, and their<br/>application to oceanography.  The approach is organized in three<br/>synergistic thrusts: (1) the application of queueing theory and<br/>combinatorial techniques to networked robots performing sequential tasks,<br/>(2) the design of novel distributed optimization and coordination schemes<br/>relying only on asynchronous and asymmetric communication, (3) the design<br/>of practical routing and coordination algorithms for the USC Networked<br/>Aquatic Platforms.  In collaboration with oceanographers and marine<br/>biologists, the project aims to design motion, communication and<br/>interaction protocols that maximize the amount of scientific information<br/>collected by the platforms.<br/><br/>This proposal addresses multi-dimensional problems of relevance in<br/>Engineering and Computer Science by unifying fundamental concepts from<br/>multiple cyberphysical domains (robotics, autonomy, combinatorics, and<br/>network science).  Our team has expertise in a broad range of scientific<br/>disciplines, including control theory and theoretical computer science and<br/>their applications to multi-agent systems, robotics and sensor networks.<br/><br/>The proposed research will have a positive impact on the emerging<br/>technology of autonomous and reliable robotic networks, performing a broad<br/>range of environmental monitoring and logistic tasks.  Our educational and<br/>outreach objectives are manifold and focus on (1) integrating the proposed<br/>research themes into undergraduate education and research, e.g., via the<br/>existing NSF REU site at the USC Computer Science Department, and (2)<br/>mounting a vigorous program of outreach activities, e.g., via a<br/>well-developed collaboration with the UCSB Center for Science and<br/>Engineering Partnerships."
